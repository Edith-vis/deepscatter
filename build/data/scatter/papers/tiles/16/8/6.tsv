id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
42c581653ebd9217060dc9c281518adb020190fb	decision making using rating systems: when scale meets binary	and scale;asymmetric information;ranking;rating;reputation systems	Rating systems measuring quality of products and services (i.e., the state of the world) are widely used to solve the asymmetric information problem in markets. Decision makers typically make binary decisions such as buy/hold/sell based on aggregated individuals’ opinions presented in the form of ratings. Problems arise, however, when different rating metrics and aggregation procedures translate the same underlying popular opinion to different conclusions about the true state of the world. This paper investigates the inconsistency problem by examining the mathematical structure of the metrics and their relationship to the aggregation rules. It is shown that at the individual level, the only scale metric (1,. . . ,N) that reports people’s opinion equivalently in the a binary metric (-1, 0, 1) is one where N is odd and N-1 is not divisible by 4. At aggregation level, however, the inconsistencies persist regardless of which scale metric is used. In addition, this paper provides simple tools to determine whether the binary and scale rating systems report the same information at individual level, as well as when the systems differ at the aggregation level. 1 2 JEL: D82, D70.	bitwise operation;mathematical structure	Anna E. Bargagliotti;Lingfang Li	2013	Decision Sciences	10.1111/deci.12049	information asymmetry;mechanism design;decision-making;economics;ranking;artificial intelligence;marketing;operations management;data mining;mathematical economics;management;statistics	Web+IR	-3.8327198487745098	-10.753435949999362	43598
0889a9415cc7e3be4b3559a25f96880753806b4a	a consistency and consensus based method for group decision making with intuitionistic fuzzy preference relations		This paper addresses the consistency and consensus of group decision making with intuitionistic fuzzy preference relations (IFPRs). To check the consistency of IFPRs and the consensus among experts, a new consistency index and a consensus index are introduced, respectively. By minimizing deviations of initial IFPRs and corresponding improved IFPRs, a goal program is built to improve the consistency of IFPRs and group consensus simultaneously. Finally, a novel algorithm is proposed for group decision making with IFPRs. An example of a supplier selection is provided to show the application of the proposed method.	algorithm;cladogram;database index;fuzzy set;goal programming;intuitionistic logic;preference learning	Gaili Xu	2017	2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)	10.1109/ISKE.2017.8258733	fuzzy logic;group decision-making;artificial intelligence;pattern recognition;mathematics	Robotics	-2.9076030004102744	-20.22990791117127	43642
38f72b843047709adae6a3b1c050023d04af5957	coherent correction for conditional probability assessments with r	conditional probability assessments;r.;non- linear optimization;coherent correction	We present an R implementation for a procedure proposed to correct incoherent conditional probability assessments. We obtain a coherent correction q for the initial assessment p given on a family E of conditional events by minimizing a discrepancy measure between p and the set of all probability distributions over the sample space spanned by E .	coherent;discrepancy function	A. Brozzi;A. Morelli;Francesca Vattari	2010		10.1007/978-3-642-14746-3_7	conditional probability distribution;discrete mathematics;conditional independence;conditional probability;probability measure;conditional expectation;conditional variance;regular conditional probability;data mining;mathematics;law of total probability;posterior probability;conditional mutual information;statistics;conditional probability table	Vision	1.8380140845128594	-18.470259872089002	43762
eb44b663888f30877738170fd6698e83688b133e	decision aid system founded on nonlinear valuation, dispersion-based weighting and correlative aggregation for wire rope selection in slope stability cable nets	prioritisation;slope stability;beta cumulative distribution function;wire rope net;dispersion based weighting;correlation coefficient	This paper presents a decision aid system to address hierarchically structured decisionmaking problems based on the determination of the satisfaction provided by a group of alternatives in relation to multiple conflicting subcriteria grouped into criteria. The system combines the action of three new methods related to the following concepts: nonlinear valuation, dispersion-based weighting and correlative aggregation. The first includes five value functions that allows the conversion of the ratings of the alternatives regarding the subcriteria into the satisfaction they produce in a versatile and simple manner through the Beta Cumulative Distribution Function. The use of measures of dispersion to weight the subcriteria by giving more importance to those factors that can make a difference due to their heterogeneity is revised to validate it when the values are not normally distributed. Dependencies between subcriteria are taken into account through the determination of their correlation coefficients, whose incorporation adjusts the results provided by the system to favour those alternatives having a balanced behaviour with respect to conflicting aspects. The overall satisfaction provided by each alternative is determined using a prioritisation operator to avoid compensation between criteria when aggregating the subcriteria. The system was tested through a novel field of application such as the selection of wire rope to form slope stability cable nets.	coefficient;nonlinear system;review aggregator;value (ethics);wire wrap	Daniel Jato-Espino;Elena Blanco-Fernandez;Jaime Carpio-García;Daniel Castro-Fresno	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.01.023	slope stability;statistics	ML	-2.573732297599163	-17.08941483688242	44027
a07582c4acfbb8d85e0190cc2f224e84ce2dd222	modelling the impact of government policies on import on domestic price of indian gold using arima intervention method		The study attempts to determine the impact of government policies of import of gold in India on the domestic price of gold during 2013 using Autoregressive Integrated Moving Average (ARIMA) intervention model. 2013 was an amazing year for Indian gold market where the price had reached its zenith. In April 2013, to curb a record trade deficit, India imposed an import duty of 10 percent on gold and tied imports for domestic consumption to exports, creating scarce supply of the yellow metal and boosting premiums to curtail the Current Account Deficit (CAD). The objective of the paper is to model the impact of this intervention by the government on the domestic price of Indian gold. Suitable ARIMAmodel is fit on the preintervention period and thereafter the effects of the interventions are analysed.The results indicate that ARIMA (1, 1, 1) is themost suitable model during preintervention period. Intervention analysis reveals that there is significant decrease in domestic price of gold by 56% from 2013. The model may be used by policymakers to analyse the future of gold before framing regulations and policies.		Jyothi Unnikrishnan;Kodakanallur Krishnaswamy Suresh	2016	Int. J. Math. Mathematical Sciences	10.1155/2016/6382926	mathematical analysis;mathematics;economic policy;public policy;gold as an investment;government;current account;balance of trade;autoregressive integrated moving average	Web+IR	0.7732285425058025	-10.880546686552755	44051
acf1f3ccc68e49cbda4450bfe648b963929ffdc9	analyzing preference rankings when there are too many alternatives	consumer preference;statistical analysis;ranking;ties;preferences	Consumer preferences can be measured by rankings of alternatives. When there are too many alternatives, this consumer task becomes complex. One option is to have consumers rank only a subset of the available alternatives. This has an impact on subsequent statistical analysis, as now a large amount of ties is observed. We propose a simple methodology to perform proper statistical analysis in this case. It also allows to test whether (parts of the) rankings are random or not. An illustration shows its ease of application.	randomness	Kar Yin Lam;Alex Koning;Philip Hans Franses	2008		10.1007/978-3-642-01044-6_51	economics;marketing;potentially all pairwise rankings of all possible alternatives;data mining;welfare economics	Crypto	-2.3839947915183006	-11.269387557811195	44052
cf853b3210ae1d510b1356517deb12cf73ddabd5	a new wrapped ensemble approach for financial forecast		The financial market is a highly complex and dynamic system that has great commercial value; thus, many financial elite are drawn to research on the subject. Recent studies show that machine learning methods perform better than traditional statistical ones. In our study, based on the characteristics of financial sequence data, we propose a wrapped ensemble approach using a supervised learning algorithm to predict stock price volatility of China’s stock markets. To check our new approach, we developed an intelligent financial forecast system and used the Hushen 300 index data to test our model; it proves that our model performs better than a single algorithm. We also compared our model with the famous ensemble approach of bagging, and the result shows that our model is better.	algorithm;bootstrap aggregating;dynamical system;machine learning;supervised learning;volatility	Yun Ling;BaoLong Yue;Huaguang Zhang	2014	J. Intelligent Systems	10.1515/jisys-2013-0007	finance;economics	AI	6.69663193354478	-19.551334053539332	44268
ad156286407b3c43940f7ced50235f893f9e85c2	a clustering based forecast engine for retail sales		Efficient and accurate sales forecasting is a vital part of creating an efficient supply chain in enterprises. Times series methods are a popular choice for forecasting demand sales. A major challenge is to develop a relatively inexpensive and automated forecasting engine that guarantees a desired forecasting accuracy. Times series decomposition and Forecast combination have been two classes of methods that have attracted the interest of recent researchers. One solution has been to use decomposition followed by recombining to form a very large number of forecasting models Further many recent papers present Data Mining based methods to intelligently discover a subset of methods from a large list that can be used in combination for sales demand forecasting. These methods are computationally expensive and prohibitive if they are applied to each individual time series in a retail organization. In this paper we present a novel technique to identify similar sales series and efficiently use the best combination of methods learnt for one series to forecast for the entire set of similar series.	analysis of algorithms;data mining;time series	Vijayalakshmi Murlidhar;Bernard L. Menezes;Mihir Sathe;Goutam Murlidhar	2012	JDIM		demand forecasting;forecasting;machine learning;data mining	ML	4.930214729345902	-18.004382429874365	44315
799083421cff839fb5b0b5685327737b8d0081a3	deriving priorities from fuzzy pairwise comparison judgements	optimisation;analytic hierarchy process;comparaison par paire;fuzzy mathematical programming;fuzzy programming;optimizacion;processus hierarchie analytique;multiple criteria decision making;rangement;simulacion numerica;critere multiple;prise decision;multiple criteria;optimal priority;fuzzy and interval comparisons;programacion lineal;comparaison intervalle et floue;comparacion por pares;ranking;priorite optimale;mathematical programming;fuzzy and interval comparison;simulation numerique;proceso jerarquia analitico;linear programming;programmation lineaire;paired comparison;fuzzy linear programming;optimization;programmation floue;toma decision;ordenamiento;programmation mathematique;programacion matematica;programacion difusa;numerical simulation	A new approach for deriving priorities from fuzzy pairwise comparison judgements is proposed, based on -cuts decomposition of the fuzzy judgements into a series of interval comparisons. The assessment of the priorities from the pairwise comparison intervals is formulated as an optimisation problem, maximising the decision-maker’s satisfaction with a speci4c crisp priority vector. A fuzzy preference programming method, which transforms the interval prioritisation task into a fuzzy linear programming problem is applied to derive optimal crisp priorities. Aggregating the optimal priorities, which correspond to di5erent -cut levels enables overall crisp scores of the prioritisation elements to be obtained. A modi4cation of the linear fuzzy preference programming method is also proposed to derive priorities directly from fuzzy judgements, without applying -cut transformations. The formulation of the prioritisation problem as an optimisation task is similar to the previous approach, but it requires the solution of a non-linear optimisation program. The second approach also derives crisp priorities and has the advantage that it does not need additional aggregation and ranking procedures. Both proposed methods are illustrated by numerical examples and compared to some of the existing fuzzy prioritisation methods. c © 2002 Elsevier Science B.V. All rights reserved.	fuzzy set;linear programming;mathematical optimization;maxima and minima;nonlinear system;numerical analysis	Ludmil Mikhailov	2003	Fuzzy Sets and Systems	10.1016/S0165-0114(02)00383-4	pairwise comparison;mathematical optimization;discrete mathematics;analytic hierarchy process;ranking;fuzzy classification;computer science;linear programming;fuzzy number;mathematics;fuzzy set operations;algorithm	AI	-0.4278614411325716	-16.381469503003704	44457
f25b718b454006e007930dfe3962322ed509a624	the role of contextual information in demand forecasting	contextual information;probability calculations;judgemental forecasting;judgemental adjustment;forecast accuracy;demand forecasting	The paper deals with clarifying the role of contextual information in demand forecasting. Both judgemental and statistical forecasting methods are often needed to provide accurate forecasts. However, in practice it is often difficult to tell when judgemental intervention is needed and when it is not. This paper presents a case example about judgemental forecasting, in which the forecaster has different pieces of information available for the basis of a forecast. The paper provides some guidelines on how to evaluate the value of contextual information with probability calculations. The calculations show that in some situations, it is impossible to improve forecast accuracy, even though the contextual information is seemingly valuable. With probability calculations, it is possible to give more objective and specific rules on when contextual information is useful in forecasting and when it is not. This can help in selecting proper forecasting methods, and setting more realistic accuracy targets.		Annastiina Kerkkänen;Janne Huiskonen	2014	IJIDS	10.1504/IJIDS.2014.061765	econometrics;economics;demand forecasting;marketing;data mining;operations research	Vision	1.1618044212173335	-13.346118773362658	44586
e724ac340dc8a91d3ce0463b4856fe6c59948edc	integration of multi-time-scale models in time series forecasting	time series forecasting;time scale;forecasting model;fuel consumption;electricity consumption;state space;long range;structural time series model;state space model	A solution to the problem of producing long-range forecasts on a short sampling interval is proposed. It involves the incorporation of information from a long sampling interval series, which could come from an independent source, into forecasts produced by a state-space model based on a short sampling interval. The solution is motivated by the desire to incorporate yearly electricity consumption information into weekly electricity consumption forecasts. The weekly electricity consumption forecasts are produced by a state-space structural time series model. It is shown that the forecasts produced by the forecasting model based on weekly data can be improved by the incorporation of longer-time-scale information, particularly when the forecast horizon is increased from 1 year to 3 years. A further example is used to demonstrate the approach, where yearly UK primary fuel consumption information is incorporated into quarterly fuel consumption forecasts.	sampling (signal processing);state space;time series	Fiona T. Murray;John V. Ringwood;Paul C. Austin	2000	Int. J. Systems Science	10.1080/00207720050165753	econometrics;state space;state-space representation;time series;mathematics;fuel efficiency;consensus forecast;statistics	AI	8.685317376475211	-15.137703731283995	44820
13156f02a0d61883b909b7c549f9d9609e159033	a discrete time variable index for supporting dynamic multi-criteria decision making	multi criteria decision making;bipolar aggregation operator;dynamic decision making	While Multi-Criteria Decision Making (MCDM) models are focused on selecting the best alternative from a finite number of feasible solutions according to a set of criteria, in Dynamic Multi-Criteria Decision Making (DMCDM) the selection process also takes into account the temporal performance of such alternatives during different time periods. In this paper a new discrete time variable index is proposed, to handling differences in temporal behavior of alternatives, which are not discriminated in preceding dynamic approaches, also considering rating-based perspectives for discrimination of the decision maker by modeling different attitudes to deal with the rating changes along different time periods. Moreover a DMCDM for supplier selection example is provided to illustrate the feasibility and effectiveness of the proposed index.	common criteria;selection algorithm	Yeleny Zulueta;Juan Martínez-Moreno;Rafael Bello;Luis Martínez-López	2014	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488514500019	optimal decision;influence diagram;decision analysis;decision field theory;weighted product model;computer science;decision analysis cycle;dynamic decision-making;management science;weighted sum model	Web+IR	-3.334466391414808	-18.325263345996998	45001
df8d63b875aa25664360dd075efaa4cd4d76471e	beating the baseline prediction in food sales: how intelligent an intelligent predictor is?	sales prediction;time series categorization;meta features	Sales prediction is an essential part of stock planning for the wholesales and retail business. It is a complex task because of the large number of factors affecting the demand. Designing an intelligent predictor that would beat a simple moving average baseline across a number of products appears to be a non-trivial task. We present an intelligent two level sales prediction approach that switches the predictors depending on the properties of the historical sales. First, we learn how to categorize the sales time series into ‘predictable’ and ‘random’ based on structural, shape and relational features related to the products and the environment using meta learning approach. Next, for the products identified as ‘predictable’ we apply an intelligent base predictor, while for ‘random’ we use a moving average. Using the real data from a food wholesales company we show how the prediction accuracy can be improved using this strategy, as compared to the baseline predictor as well as an ensemble of predictors. In our study we also show that by applying an intelligent predictor for the most ‘predictable’ products we can control the risk of performing worse than the baseline.	baseline (configuration management);categorization;kerrison predictor;meta learning (computer science);network switch;time series	Indrė Žliobaitė;Jorn Bakker;Mykola Pechenizkiy	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.07.078	simulation;artificial intelligence;machine learning;data mining	ML	5.955015379787697	-18.641639959288806	45091
3cbe78224f27821133a2acb3fb0a7885ec8769ac	a system for forecasting corporate-tax revenue based on fuzzy logic and fuzzy set theory	tasacion;modelo prevision;sistema experto;north america;america del norte;amerique du nord;amerique;sistema informatico;logique floue;econometria;computer system;economic model;etats unis;logica borrosa;raisonnement;estados unidos;fuzzy set theory;forecast model;fuzzy logic;modelo economico;modele economique;estudio caso;corporate taxes;revision economique;razonamiento;etude cas;taxation;systeme informatique;econometrics;systeme expert;reasoning;america;econometrie;expert system;modele prevision	Abstract   A system based on fuzzy set theory for forecasting the Florida corporate income tax revenue has been developed in response to several years of unsuccessful forecasts generated by conventional econometric methods. The system consists of two subsystems. The first utilizes time series of revenue and of the real per capita GNP. Both time series are transformed into trend vectors through the moving-average technique. Both vectors are divided into strings of growth patterns such as “accelerating growth,” “decelerating growth,” and “negative growth.” The system checks for several indications of systematic relationship between such strings in the GNP vector and the tax revenue vector. Based on that relationship, the forecast of the corporate tax revenues is generated in fuzzy terms, such as “very rapid growth,” “slightly negative growth,” etc. The fuzzy forecast from the first subsystem of the system constitutes input into the second subsystem, which in turn generates the range of forecasted revenue in millions of dollars. A control mechanism, which is built into the system, continuously checks forecasted rates of change in tax revenues against the actual throughout the history of the time series to make sure that the cumulative forecasting error will not reach an unacceptable magnitude.	fuzzy logic;fuzzy set;set theory	Eliahu Shnaider;Abraham Kandel	1992	Inf. Sci.	10.1016/0020-0255(92)90060-L	fuzzy logic;computer science;artificial intelligence;economic model;fuzzy set;operations research;expert system;reason	AI	2.881467926825034	-16.181736169151375	45108
615f449ba4041483c84fe255e77ca821a4701cb5	some new shapley 2-tuple linguistic choquet aggregation operators and their applications to multiple attribute group decision making	2 tuple linguistic;shapley 2 tuple linguistic choquet aggregation operators;multiple attribute group decision making magdm;shapley index;choquet integral	In this paper, we investigate themultiple attribute group decision making (MAGDM) problems with 2-tuple linguistic information. Firstly, motivated by the ideas of Choquet integral and Shapley index, we propose three 2tuple linguistic aggregation operators called Shapley 2-tuple linguistic Choquet averaging operator, Shapley 2-tuple linguistic Choquet geometric operator and generalized Shapley 2-tuple linguistic Choquet averaging operator. Then we discuss someproperties of these operators, such as idempotency, monotonicity, boundary and commutativity. Secondly, if the information about the weights of decision makers (DMs) and attributes is incompletely known, we build two models to determine the optimal fuzzy measures on DM set and attribute set, respectively. Furthermore, we develop a new method formultiple attribute group decisionmaking under 2tuple linguistic environment based on the proposed operators. Finally, we apply the developed MAGDM method to select the most desirable emergency alternative and the validity of the developed method is verified by comparing the evaluation results with those obtained from the existing 2-tuple correlated aggregation operators.	aggregate data;decision support system;entity–relationship model;fuzzy measure theory;idempotence;interaction;stable marriage problem;yang	Yanbing Ju;Xiaoyue Liu;Aihua Wang	2016	Soft Comput.	10.1007/s00500-015-1740-3	mathematical optimization;discrete mathematics;mathematics;choquet integral	AI	-2.9762057839618823	-20.786456290268493	45170
4881b13757e00899e172cb6566e617302f7dc465	data envelopment analysis in dynamic framework	analisis envolvimiento datos;matematicas aplicadas;mathematiques appliquees;data envelopment analysis;efficacite dynamique;dynamic efficiency;efficiency measurement;applied mathematics;data envelope analysis;convex combination;analyse enveloppement donnee	The purpose of this paper is to develop a new DEA with dynamic revenue efficiency. The proposed dynamic DEA model not only measures the efficiency of the whole periods, but also it provides the efficiency measure for each of the periods. It is proven that the aggregate efficiency of the whole periods is a convex combination of the dynamic efficiency of each period. The particular application area investigated is that involving the gas companies of Iran.	data envelopment analysis	Alireza Amirteimoori	2006	Applied Mathematics and Computation	10.1016/j.amc.2006.01.003	econometrics;data envelopment analysis;mathematics;operations research	Vision	0.6789723909297862	-14.19201860666051	45223
3487fd57a2c4e3412fdcbb1e086ea354f760b6ac	research note - should consumers use the halo to form product evaluations?	consumer choice;compra;halo effect;informacion incompleta;heuristic method;effet halo;metodo heuristico;psychology;teoria decision;utility maximization;decision theoretic consumer choice;utility preference;incomplete information;james stein estimator;theorie decision;decision theory;decision theoretic;cognition;information incomplete;preferencia;estimation risk;cognicion;achat;psychologie;efecto halo;preference;methode heuristique;purchases;psicologia;formative evaluation	I purchase situations where attribute information is either missing or difficult to judge, a well-known heuristic that consumers use to form evaluations is the halo effect. The psychology literature has widely considered the halo a reflection of consumers’ inability to discriminate between different attributes and have therefore labeled it the “halo error” or the “logical error.” The objective of this paper is to offer a rationale for the halo effect. We use a decision-theory framework to show that the halo is consistent with the goal of minimizing estimation risk. Contrary to conventional wisdom, we demonstrate that a decision using the halo has lower estimation risk compared to not using the halo heuristic. Therefore, using the halo results in utility maximization and is indicative of rational behavior.	decision theory;design rationale;expectation–maximization algorithm;heuristic	Peter Boatwright;Ajay Kalra;Wei Zhang	2008	Management Science	10.1287/mnsc.1070.0742	econometrics;james–stein estimator;cognition;decision theory;halo effect;marketing;welfare economics;formative assessment;complete information;statistics	HCI	-1.5725480831205352	-14.789899305717618	45271
e69f73fff3f9dfc1067a8d08696b544a442b68d5	granular structures of fuzzy rough sets based on general fuzzy relations	upper approximation granular structures fuzzy rough sets general fuzzy relations t similarity relations lower approximation;fuzzy relation;granular structure;rough set theory approximation theory fuzzy set theory;granular structure fuzzy relation fuzzy rough sets;fuzzy rough sets	Existing studies on the granular structures of fuzzy rough sets are mainly proposed based on the T-similarity relations which are only a special kind of fuzzy relations. Therefore, it is of interest to extend the study to more general situations in order to expand the application scope of the fuzzy rough set theory. In this paper, the granular structures of the S-lower, T-upper, #-lower and crupper approximations of the fuzzy sets are established based on more general fuzzy relations.	approximation;fuzzy set;rough set;set theory	Xiao Zhang;Changlin Mei;Degang Chen	2015	2015 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2015.7340661	fuzzy logic;mathematical analysis;discrete mathematics;rough set;topology;membership function;defuzzification;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	DB	-1.2988715890152527	-23.78863130860771	45404
edb03f6e6edbd0d8aa0bedef72b3a897f601f251	selecting the best supply chain by goal programming and network data envelopment analysis	data envelopment analysis;network dea;goal programming;supply chain	Today, one of the most important problems of decision makers in most organizations is to choose the best supply chain. The main objective of this paper is to choose the best supply chain. To select the best supply chain this paper presents a model based on goal programming and network data envelopment analysis (NDEA). The proposed model enables decision makers to compare supply chains with predetermined goals. A case study is presented to validate the proposed	data envelopment analysis;goal programming;markov chain	Saeed Yousefi;Hadi Shabanpour;Reza Farzipoor Saen	2015	RAIRO - Operations Research	10.1051/ro/2014059	goal programming;data envelopment analysis;mathematics;supply chain	Robotics	-4.399394092065091	-15.517056401699893	45575
1bbcf4f34398b5876ef81271548624e19abedecd	multicriteria support of choosing a group decision	computers;optimisation group decision support systems;information systems;yttrium;yttrium decision making optimization computers computer science information systems life sciences;life sciences;group decision making interactive method reference point method multicriteria optimization task group decision selection process;optimization;computer science	The paper presents the method of a group decision making in a competitive environment. We deal with a group decision when the group of people with different preferences are to make one single decision. The group decision selection process is modeled with the use of multi-criteria optimization task. It is solved with the use of reference point method. This method is an interactive method in which every person specifies its requirements in the form of a reference point, expressing the desired values for its evaluation function. On the basis of the provided reference point, a scalar achievement function is built. Maximization of this function generates a solution of the multi-criteria task. This solution is presented to every person for acceptance or as a basis for the modification of the reference point. The paper contains the example of application of the proposed method to support a group decision by three people with different preferences.	evaluation function;expectation–maximization algorithm;interaction;mathematical optimization;requirement	Andrzej Lodzinski	2015	2015 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2015F58	r-cast;optimal decision;influence diagram;intelligent decision support system;decision analysis;decision engineering;computer science;artificial intelligence;decision analysis cycle;yttrium;decision tree;decision rule;management science;evidential reasoning approach;operations research;information system;weighted sum model;business decision mapping	Vision	-3.8333791649975844	-17.334950901238045	45778
e8ce37a7316e5358b54824030f4b61831aa5d9fd	forecasting traffic time series with multivariate predicting method	k nearest neighbor knn nonparametric regression model;traffic time series;forecast accuracy measure;multivariate predicting method;univariate predicting method	Scalar time series considered in most studies may be not sufficient to reconstruct the dynamics, while using multivariate time series may demonstrate great advantages over scalar time series if they are available. Multivariate time series are available in the traffic system and we intend to examine the issue for the real data in the traffic system. In this paper, we propose the multivariate predicting method and discuss the prediction performance of multivariate time series by comparison with univariate time series and K-nearest neighbor (KNN) nonparametric regression model. The three kinds of forecast accuracy measure for multivariate predicting method are smaller than those for the other two methods in all cases, which suggest the predicting results for traffic time series by multivariate predicting method are better and more accurate than those based on univariate time series and KNN model. It demonstrates that the proposed multivariate predicting method is more successful in predicting the traffic time series than univariate predicting method and KNN method. The multivariate predicting method has a broad application prospect on prediction because of its advantage on recovering the dynamics of nonlinear system.	time series	Yi Yin;Pengjian Shang	2016	Applied Mathematics and Computation	10.1016/j.amc.2016.07.017	econometrics;multivariate statistics;computer science;machine learning;statistics	ML	7.657663421865933	-20.291063598334844	45781
a3a4a96f65d01ae0c4a57a1e242298552e398346	the relevance of trends for predictions of stock returns	stock returns		relevance	Thomas Hellström;Kenneth Holmström	2000	Int. Syst. in Accounting, Finance and Management	10.1002/(SICI)1099-1174(200003)9:1%3C23::AID-ISAF178%3E3.0.CO;2-U	financial economics;stock market bubble;computer science;finance;financial system	DB	4.237321494836791	-13.91081068322922	45819
37f27532a14496f21f7f3ddacc7a65172976f8e8	deeptravel: online transport mode identification based on low-rate sampling sensors through deep neural network		Transportation Mode Identification (TMI) has been studied for over 10 years, but few researches focused on datasets sampled at low sampling rates with low localization rates as a result of energy-efficient data collection. To fill this gap, we consider such a topic in this paper and present DeepTravel, an online TMI algorithm using a deep neural network with only ten features for each data sample, to classify four transportation modes: walking/stationary, bus, car and metro. We trained and evaluated our model on a dataset with an average sampling rate of 0.067Hz and localization rate less than 50%, and still managed to achieve an overall classification accuracy of 87%, which indicates its effectiveness in energy-efficient TMI tasks.	algorithm;algorithmic inference;artificial neural network;data point;deep learning;duty cycle;real-time clock;sampling (signal processing);sensor;smartphone;sparse matrix;stationary process	Zhongjing Wang;Qingmei Zhao;Chen Wang	2017	2017 4th International Conference on Systems and Informatics (ICSAI)	10.1109/ICSAI.2017.8248520	artificial neural network;control engineering;computer science;data collection;sampling (signal processing);feature extraction;sampling (statistics);sample (statistics);pattern recognition;artificial intelligence	Robotics	9.526697865094127	-23.32665042822999	45829
e0fe9f7236ed6cf3d6d9954d6bf94c629f980a79	a linear programming model dealing with ordinal ratings in policy capturing of performance appraisal	performance appraisal;judgement;linear programming;linear program;policy capturing;management;management policy	Linear programming (LP) has been utilized as an alternative to policy capturing (PC) for inferring criteria weights in a performance appraisal. Previous studies for policy capturing LP (PCLP) focused only on the criteria of numerical ratings while excluding ordinal ratings. This study presents an extended LP model to deal with ordinal ratings. Since LP can handle only numerical ratings, a set of possible ratings for each ordinal criterion is transformed into a set of unknown numerical ratings by introducing a function based on elements' relations of ordinal ratings. Then, the unknown numerical ratings are connected to a `weak relation' or `strong relation'. Solving our model gives the optimal values for the criteria weights and the unknown numerical ratings. A simple example is given to illustrate our model.	linear programming;ordinal data;programming model	Ho-Won Jung	2001	European Journal of Operational Research	10.1016/S0377-2217(00)00270-8	mathematical optimization;economics;linear programming;operations management;mathematics;management science;ordinal data;welfare economics	ECom	-2.587213800945236	-17.81266540634806	45833
5618a3cfd39ce9d11804796b78fb1f4ecaf82ec7	an exploration of functional size based effort estimation models	functional similarity;base functional components;effort estimation model;artificial neural network	A number of methods have been proposed to build a relationship between effort and size. These models are generally based on regression analysis and a widely accepted model is not yet available. Although in some sizing methods, such as MKII and IFPUG, different multipliers for the base functional components (BFC) exist, their origin and the purpose of their usage are undefined. The COSMIC method does not treat components separately and assigns the same measurement unit to each of them. In this study we used the Artificial Neural Network and regression based methods to create effort estimation models that take the four components of the COSMIC method into consideration. In the research we compared several functional size based effort models in terms of accuracy using a reliable company dataset. These models comprised not only the generic models proposed in the literature or currently in use, but also specific models that we generated using our dataset with a single and multi-variate regression analysis and the ANN method. We also explored the effect of functional similarity (FS) using our specific models. We found that using BFC instead of total size improved effort estimation models and the ANN method is a useful approach to calibrate these components according to the company characteristics.	artificial neural network;base one foundation component library (bfc);cosmic;cost estimation in software engineering;mortal kombat ii;software development effort estimation;undefined behavior	Seçkin Tunalilar;Onur Demirörs	2011	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194011005347	computer science;artificial intelligence;data mining;artificial neural network	SE	4.89275036682221	-20.297264004874986	45867
9fd846c7651993d60d1c9c410b02b94af097b651	a prediction interval-based approach to determine optimal structures of neural network metamodels	model selection;080108 neural;structural complexity;evolutionary and fuzzy computation;mean absolute percentage error;coverage probability;complex system;mean square error;simulation metamodeling;network structure;neural network model;prediction interval;selection criteria;neural network;discrete event simulation	Neural networks have been widely used in literature for metamodeling of complex systems and often outperform their traditional counterparts such as regression-based techniques. Despite proliferation of their applications, determination of their optimal structure is still a challenge, especially if they are developed for prediction and forecasting purposes. Researchers often seek a tradeoff between estimation accuracy and structure complexity of neural networks in a trial and error basis. On the other hand, the neural network point prediction performance considerably drops as the level of complexity and amount of uncertainty increase in systems that data originates from. Motivated by these trends and drawbacks, this research aims at adopting a technique for constructing prediction intervals for point predictions of neural network metamodels. Space search for neural network structures will be defined and confined based on particular features of prediction intervals. Instead of using traditional selection criteria such as mean square error or mean absolute percentage error, prediction interval coverage probability and normalized mean prediction interval will be used for selecting the optimal network structure. The proposed method will be then applied for metamodeling of a highly detailed discrete event simulation model which is essentially a validated virtual representation of a large real world baggage handling system. Through a more than 77% reduction in number of potential candidates, optimal structure for neural networks is found in a manageable time. According to the demonstrated results, constructed prediction intervals using optimal neural network metamodel have a satisfactory coverage probability of targets with a low mean of length.		Abbas Khosravi;Saeid Nahavandi;Douglas C. Creighton	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.07.059	structural complexity;probabilistic neural network;mean absolute percentage error;prediction interval;computer science;artificial intelligence;discrete event simulation;machine learning;data mining;mean squared error;artificial neural network;model selection;statistics	ML	7.157818900678565	-22.617454366112177	45950
90694a832a312a11b38fc0679d240c5bac54cf2c	neural network research progress and applications in forecast	modeling and forecasting;activation function;parameter selection;network structure;neural network	This paper roughly reviews the history of neural network, briefly introduces the principles, the features, and the applied fields of neural network, and puts emphasis on discussing the current situation of the latest research from parameters selection, algorithms improvement, network structure improvement and activation function; expounds the effect of neural network in modeling and forecast, from forward direction modeling and reverse direction modeling, describes the principles of modeling and forecast based on neural network in details, analyzes the basic steps of forecast using neural network, then discusses the latest research progress and the facing problems in this field, at last looks forward to the developing trend of this advancing front theory and its applied prospect in forecast.		Shifei Ding;Weikuan Jia;Chunyang Su;Liwen Zhang;Zhongzhi Shi	2008		10.1007/978-3-540-87734-9_89	computer science;artificial intelligence;machine learning;data mining;activation function;operations research;artificial neural network	Crypto	6.887291416648866	-23.85444137964128	46256
7e178c50a889298626c4df271b15628a5ab18d97	efficiency analysis of non-homogeneous parallel sub-unit systems for the performance measurement of higher education		Conventional Data Envelopment Analysis (DEA) models focus only on initial inputs and final outputs for efficiency evaluation. Thus, these models treat the production process as a ‘black box’, i.e., they do not take into account how exactly inputs are related to outputs. Various models that came later take care of internal processes of DMU. The existing models of internal processes for parallel sub-units consist of three stages: the first stage calculates the relative weights of sub-units, the second stage calculates the efficiencies of sub-units, and in the third stage efficiencies of sub-units are aggregated as the efficiency of DMU. It is observed that when existing models of internal processes are applied to nonhomogeneous parallel sub-units, in the first stage, the weight assigned to the maximum efficient sub-unit is one and to other sub-units is zero. This implies that the efficiency of a DMU is equal to the maximum of efficiencies of its sub-units indicating that the efficiency of a DMU is not sensitive to the efficiencies of sub-units other than the sub-unit with maximum efficiency. This paper proposes a single stage DEA approach where the efficiency of a DMU and its sub-units can be measured simultaneously. The advantage of the proposed approach is that the efficiency of a DMU is sensitive to the changes in the efficiency of its sub-units, and weights of sub-units can be assigned a priori by the decision maker. The development of the proposed approach is inspired from the growing interest in evaluating efficiency of higher education system in India. In the proposed application, states are considered as DMUs and universities, colleges and stand-alone institutions are taken as three non-homogeneous parallel sub-units of DMUs.	care-of address;data envelopment analysis;digital mockup	Sanjeet Singh;Prabhat Ranjan	2018	Annals OR	10.1007/s10479-017-2586-0	mathematical optimization;scheduling (production processes);mathematics;econometrics;performance measurement;homogeneous;data envelopment analysis	SE	-2.0733856780315856	-15.293137547628584	46336
b1acf6e530aa220a358fac0ef5145c6d089386fc	detecting novelties in time series through neural networks forecasting with robust confidence intervals	forecasting;confidence intervals;failure detection;anomaly detection;novelty detection;forecasting model;time series;confidence interval;financial system;fraud detection;neural network	Novelty detection in time series is an important problem with application in a number of different domains such as machine failure detection and fraud detection in financial systems. One of the methods for detecting novelties in time series consists of building a forecasting model that is later used to predict future values. Novelties are assumed to take place if the difference between predicted and observed values is above a certain threshold. The problem with this method concerns the definition of a suitable value for the threshold. This paper proposes a method based on forecasting with robust confidence intervals for defining the thresholds for detecting novelties. Experiments with six real-world time series are reported and the results show that the method is able to correctly define the thresholds for novelty detection. r 2006 Elsevier B.V. All rights reserved.	artificial neural network;experiment;novelty detection;realization (probability);sensor;simulation;time series	Adriano Lorena Inácio de Oliveira;Silvio Romero de Lemos Meira	2006	Neurocomputing	10.1016/j.neucom.2006.05.008	anomaly detection;confidence interval;computer science;machine learning;data mining;artificial neural network;statistics	AI	6.1023963752936705	-21.873126319260923	46498
31c453c389e300d30d94bc4de4baace34bf129fd	an empirical comparison of the validity of a neural net based multinomial logit choice model to alternative model specifications	model specification;empirical study;neural networks;market share;multinomial logit;utility function;330 wirtschaft;ddc 330;posterior probability;multilayer perceptron;neural net;choice models;marketing;generalized additive model;taylor series expansion;interaction effect;choice model;neural network;multinomial logit model;brand choice	Applications of choice models to brand purchase data as a rule specify a linear deterministic utility function. We estimate deterministic utility by means of a neural net able to approximate any continuous multivariate function and its derivatives to a desired level of precision. We compare this model to related alternatives both with linear and nonlinear utility functions. Alternatives with nonlinear utility functions are based on generalized additive modeling and Taylor series expansion, respectively. We analyze purchase data of the six largest brands in terms of market share for two product groups. Neural choice models outperform the alternative models studied w.r.t. posterior probabilities. They also attain the best crossvalidated log–likelihood values. These results demonstrate that the increase in complexity caused by the neural choice model is justified by higher validity. In the empiricial study the neural choice models imply elasticities different from those obtained by linear utility multinomial logit models for several predictors. Neural choice models discover inversely S–shaped, saturation and interaction effects on utility.	approximation algorithm;artificial neural network;choice modelling;multinomial logistic regression;nonlinear system;series expansion;utility functions on indivisible goods	Harald Hruschka;Werner Fettes;Markus Probst	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00410-7	financial economics;econometrics;mixed logit;economics;computer science;mathematics;choice set;artificial neural network;multinomial logistic regression;statistics	ML	2.9708606405456734	-10.761988026348964	46823
6461ee4c79d39de398d69f6beff547a86ed7b07f	a new method of fuzzy linear programming problems	acceptable solution;numerical method;particle measurements;fuzzy number;probability density function;fuzzy variable;decision maker;data mining;delta modulation;fuzzy set theory;fuzzy sets;vectors;chromium;business;linear programming national electric code fuzzy sets chromium educational institutions business particle measurements vectors;linear programming;fuzzy linear programming;national electric code;linear programming fuzzy set theory;decision maker fuzzy linear programming fuzzy number acceptable solution;numerical method fuzzy linear programming problem decision maker;fuzzy linear programming problem	This paper proposes a method for solving fuzzy linear programming problems where all the coefficients are fuzzy numbers. Firstly, this paper use the expected value of fuzzy variable presented by Baoding Liu to compare two fuzzy numbers. Secondly, this paper offer the acceptable solution of fuzzy linear programming for the decision-maker in different degrees of feasiblity. Lastly, we solve one numerical example using this method.	linear programming	Wenguang Tang;Yunling Luo	2009		10.1109/BIFE.2009.50	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Theory	-2.1482483442278775	-19.614448803235618	46974
d5f4ea8e337a904c359857f71d955a14d618dd5e	a method for intrusion detection in web services based on time series	web services autoregressive moving average processes security of data service oriented architecture time series;intrusion detection;web services time series analysis intrusion detection predictive models data models mathematical model xml;anomaly detection intrusion detection web services time series service oriented architecture autoregressive integrated moving average model arima model;time series analysis;web services;xml;mathematical model;predictive models;data models	A prevalent issue in today's society that has attracted much attention is anomaly detection in time series. Service-oriented architecture (SOA) and web services are considered as one of the most important technologies. In this paper, we propose a model for intrusion detection in web services based on the autoregressive integrated moving average (ARIMA). First, we apply the ARIMA model to the training data. Second, we forecast their next behavior within a specific confidence interval. Third, we examine the testing data; if any instance falls out of the range of the confidence interval, it might be an anomaly, and the system will notify the administrator. We present experiments and results obtained using real world data.	anomaly detection;autoregressive integrated moving average;autoregressive model;brute-force search;dos;data mining;data validation;embedded system;experiment;intrusion detection system;parsing;soap;sensor;service-oriented architecture;service-oriented device architecture;session hijacking;time series;web service;xml denial-of-service attack;xml namespace;xml schema	Paria Shirani;Mohammad Abdollahi Azgomi;Saed Alrabaee	2015	2015 IEEE 28th Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2015.7129383	intrusion detection system;web service;data modeling;xml;computer science;data science;time series;mathematical model;data mining;predictive modelling;world wide web;statistics	DB	6.701504214529267	-15.376760230659704	47217
75ce499dedebbccf57b09d1a5d326eaa548ff8c0	a fuzzy goal programming based procedure for machine tool selection	fuzzy data;machine tool selection;fuzzy pair wise comparison matrix;fuzzy weights;goal programming	The evaluation and selection of the appropriate machine tools is one of the most critical decisions in the design and development of an efficient production environment and the use of fuzzy set theory allows incorporating qualitative and partially known information into the decision model. This paper describes a fuzzy goal programming method for evaluation and selection of machine tool alternatives for a manufacturing company in Iran. Our approach applied triangular numbers into traditional goal programming method, and we investigated deriving fuzzy weights of criteria from the pair-wise comparison matrix with fuzzy elements. To our knowledge, no previous work investigated such problem with fuzzy goal programming. In almost fuzzy decision making problems other fuzzy methods like fuzzy AHP, fuzzy TOPSIS and etc. are used. And most of them obtained the crisp weights from fuzzy pair-wise decision matrix. Moreover, in this method we obtain the fuzzy weights by solving a simple linear programming. As a result of the study, we find that the proposed method is practical for ranking machine tool alternatives with respect to multiple conflicting criteria.	goal programming	Mohammad Izadikhah	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141311	fuzzy logic;mathematical optimization;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;goal programming;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	-4.331313477622743	-17.69907884159396	47246
4bdfd0f0a1e008e357a507a8c594ea3efe01abb1	intuitionistic fuzzy approximations and intuitionistic fuzzy sigma-algebras	fuzzy set;intuitionistic fuzzy sets;fuzzy measure;σ algebras;rough sets;rough set;intuitionistic fuzzy rough sets;measurable spaces;approximation spaces	In this paper, concepts of intuitionistic fuzzy measurable spaces and intuitionistic fuzzy σ-algebras are first introduced. Relationships between intuitionistic fuzzy rough approximations and intuitionistic fuzzy measurable spaces are then discussed. It is proved that the family of all intuitionistic fuzzy definable sets induced from an intuitionistic fuzzy serial approximation space forms an intuitionistic fuzzy σ-algebra. Conversely, for an intuitionistic fuzzy σ-algebra generated by a crisp algebra in a finite universe, there must exist an approximation space such that the family of all intuitionistic fuzzy definable sets is the class of all measurable sets in the given intuitionistic fuzzy measurable space.	approximation;intuitionistic logic	Wei-Zhi Wu;Lei Zhou	2008		10.1007/978-3-540-79721-0_50	mathematical analysis;discrete mathematics;rough set;topology;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;machine learning;mathematics;fuzzy set;fuzzy set operations	Logic	-0.8148268602158912	-22.882877623135872	47278
51f14d85e17c589b830f4d58abf4e5587b9c7141	fuzzy option value with stochastic volatility models	stochastic processes calculus uncertainty shape fuzzy sets intelligent systems fuzzy systems stochastic systems finance statistics;black scholes option valuation model;finance;fuzzy numbers;arithmetic operations;fuzzy number;arithmetic operations fuzzy option value stochastic volatility models financial models black scholes option valuation model fuzzy calculus;biological system modeling;stochastic processes arithmetic calculus finance fuzzy set theory;fuzzy calculus;stochastic volatility model;stochastic volatility fuzzy numbers parametric representation;fuzzy set theory;option valuation;parametric representation;shape;stochastic processes;calculus;stochastic volatility;fuzzy option value;mathematical model;financial models;arithmetic;stochastic volatility models;option value;profitability;differential equations;context modeling	Uncertainty and vagueness play a central role in financial models and fuzzy numbers can be a profitable way to manage them. In this paper we generalize the Black and Scholes option valuation model (with constant volatility) to the framework of a volatility supposed to vary in a stochastic way. The models we take under consideration belongs to the main classes of stochastic volatility models: the endogenous and the exogenous source of risk. Fuzzy calculus for financial applications requires massive computations and when a good parametric representation for fuzzy numbers is adopted, then the arithmetic operations and fuzzy calculus can be efficiently managed. Good in this context means that the shape of the resulting fuzzy numbers can be observed and studied in order to state fundamental properties of the model.	black–scholes model;computation;fuzzy number;numerical partial differential equations;vagueness;value (ethics);volatility	Gianna Figà-Talamanca;Maria Letizia Guerra	2009	2009 Ninth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2009.243	stochastic process;mathematical optimization;implied volatility;defuzzification;type-2 fuzzy sets and systems;artificial intelligence;fuzzy number;mathematics;mathematical economics;stochastic volatility	Robotics	1.1745105179001523	-12.11134759819411	47339
0bd014e526556d769057472a9e71108a41280d2d	a product quality forecasting using autoregressive moving average	moving average		autoregressive model	Abdul Talib Bon;Nor Aziati Hamid	2004			statistics;economics;moving average;autoregressive–moving-average model;autoregressive integrated moving average	Robotics	5.155091204319757	-14.858684224431057	47436
0f099d26d6e4e1e140e8ed31a1d9b7c6b084391d	anmerkungen zur besetzung von zellen unter verschiedenen modellen		Abstract It is to be examined, which counting strategies are optimal for determining, if a given cell-occupation-pattern is one to one. In this the counting of cells is regarded as the time-consuming element. The question is, whether the counting of full cells or the counting of empty cells is more favorable. The essential parameter, which enters the problem besides the number of cells, M, and of elements, N, is the chosen probability-model controlling the occupancy process. The models examined here (polynomial-model, Bose-Einstein-model) show a significant superiority of counting the full cells. Independently of any model this superiority also holds for the case M u003e 2N. Under the given premises, the result can be applied to checking-processes in documentation, where it is to be determined, if a given mapping is one to one.		Eckart Leiser	1972	Information Storage and Retrieval	10.1016/0020-0271(72)90026-5	information retrieval;discrete mathematics;documentation;computer science;premises;one-to-one	NLP	6.034536788923547	-12.453689542476807	47537
77e047de96441e711e84e9c3c94e9b31443915d3	stochastic data envelopment analysis in measuring the efficiency of taiwan commercial banks	distributed estimation;evaluation performance;analisis envolvimiento datos;replication;banking;entrada salida;analisis estadistico;performance evaluation;systeme aide decision;efficiency;evaluacion prestacion;estimation non parametrique;data envelopment analysis efficiency stochastic data interval data;analyse stochastique;prise de decision;secteur bancaire;sistema ayuda decision;replicacion;stochastic data;commercial banks;interval data;input output;non parametric estimation;decision support system;statistical analysis;data envelopment analysis;simulation technique;analyse statistique;stochastic analysis;simulation analysis;decision making unit;estimacion no parametrica;toma decision;data envelope analysis;analisis estocastico;entree sortie;analyse enveloppement donnee	Conventional data envelopment analysis (DEA) for measuring the efficiency of a set of decision making units (DMUs) requires the input/output data to be constant. In reality, however, many observations are stochastic in nature; consequently, the resulting efficiencies are stochastic as well. This paper discusses how to obtain the efficiency distribution of each DMU via a simulation technique. The case of Taiwan commercial banks shows that, firstly, the number of replications in simulation analysis has little effect on the estimation of efficiency means, yet 1000 replications are recommended to produce reliable efficiency means and 2000 replications for a good estimation of the efficiency distributions. Secondly, the conventional way of using average data to represent stochastic variables results in efficiency scores which are different from the mean efficiencies of the presumably true efficiency distributions estimated from simulation. Thirdly, the interval-data approach produces true efficiency intervals yet the intervals are too wide to provide valuable information. In conclusion, when multiple observations are available for each DMU, the stochastic-data approach produces more reliable and informative results than the average-data and interval-data approaches do.	data envelopment analysis	Chiang Kao;Shiang-Tai Liu	2009	European Journal of Operational Research	10.1016/j.ejor.2008.02.023	econometrics;decision support system;computer science;operations management;data envelopment analysis;mathematics;statistics	Theory	-0.5489966348581726	-15.185495218824453	47565
ffa20653a77e8b166268d4e05538cea45ec7d3fd	improved symmetry measures of simplified neutrosophic sets and their decision-making method based on a sine entropy weight model		This work indicates the insufficiency of existing symmetry measures (SMs) between asymmetry measures of simplified neutrosophic sets (SNSs) and proposes the improved normalized SMs of SNSs, including the improved SMs and weighted SMs in single-valued and interval neutrosophic settings. Then, the sine entropy measures of SNSs are presented to establish a sine entropy weight model for solving the criteria weights in decision-making. Based on the improved weighted SMs of SNSs and the sine entropy weight model, a multi-criteria decision-making (MCDM) method with unknown criteria weights (an improved MCDM method) is established in the SNS setting. In the MCDM process, corresponding to the criteria weights obtained by the sine entropy model, the ranking order of all alternatives and the best one are given by means of the improved weighted SMs between the ideal solution and each alternative. Lastly, the improved MCDM method is applied to an actual decision example in single-valued and interval neutrosophic settings to indicate the feasibility of the improved MCDM method. By comparative analysis with existing MCDM methods, the improved SMs and the sine entropy weight model not only provide a simpler and more effective method for MCDM problems with unknown criteria weights in the SNS setting, but can also overcome the insufficiency of the existing SMs and MCDM method.	algorithm;common criteria;convergence insufficiency;effective method;image processing;pattern recognition;qualitative comparative analysis	Wenhua Cui;Jun Ye	2018	Symmetry	10.3390/sym10060225	combinatorics;multiple-criteria decision analysis;normalization (statistics);mathematical optimization;mathematics;effective method;sine;ranking	NLP	-2.579870547718971	-20.69422903018265	47599
89c1d06d1b01f1c1e72aa802d3a700d1ce5309d8	a network-dea model with new efficiency measures to incorporate the dynamic effect in production networks	modelizacion;estimacion sesgada;evaluation performance;analisis envolvimiento datos;networks;rendement echelle;performance evaluation;systeme aide decision;retorno de escala;social decision;evaluacion prestacion;estimation non parametrique;prise de decision;sistema ayuda decision;dynamic production;decision maker;return to scale;production process;modelisation;non parametric estimation;decision support system;data envelopment analysis;processus fabrication;decision making unit;estimacion no parametrica;data envelopment analysis dynamic production networks returns to scale;decision colectiva;efficiency measurement;decision collective;toma decision;data envelope analysis;modeling;biased estimation;estimation biaisee;production network;proceso fabricacion;analyse enveloppement donnee;returns to scale	A production network can be described as a collection of production processes performed by several interdependent groups of sub decision-making units (SDMUs) within a DMU. Dynamic effects pertain to the situation where intermediate outputs consumed by one SDMU may also dynamically influence its output level in the future. Without considering these effects in efficiency measurement, we would obtain biased efficiency measurement, because the measure could not faithfully reflect the underlying performance. Hence the result would provide misleading information to decision-makers. This paper proposes a network-DEA model with new efficiency measures to systematically cope with the dynamic effect within a production network. Various interconnections between the new measure and the DEA-efficiency have also been established. Additionally, we also formalize the relationship between returns-to-scale properties of DMUs and those of its constituting SDMUs. This paper presents a unified framework to analyze performances in a dynamic production network. 2007 Elsevier B.V. All rights reserved.	aggregate data;algorithmic efficiency;digital mockup;information;interaction;interdependence;norm (social);performance;spectral efficiency;stochastic modelling (insurance);unified framework	Chien-Ming Chen	2009	European Journal of Operational Research	10.1016/j.ejor.2007.12.025	returns to scale;econometrics;decision support system;economics;computer science;operations management;data envelopment analysis;operations research	Web+IR	-1.1447682633358174	-14.49994241914122	47655
e6f54d4b87f0d84e81d8aef801f94c262f40eaa5	a supervised fuzzy network analysis for risk assessment in stock markets: an anfis approach	pattern clustering;fuzzy neural nets;fuzzy reasoning;stock markets fuzzy neural nets fuzzy reasoning fuzzy set theory investment learning artificial intelligence pattern clustering risk management;risk management;investment;fuzzy set theory;prediction error supervised fuzzy network analysis risk assessment anfis approach adaptive neuro fuzzy inference system approach stock market risk prediction portfolio return prediction membership functions fuzzy c mean clustering algorithm tehran stock exchange;stock markets;portfolios predictive models data models training uncertainty fuzzy logic stock markets;learning artificial intelligence	In this paper we have used an adaptive neuro-fuzzy inference system (ANFIS) approach to predict the risk of stocks. Previous works just predict the return of stocks and make their portfolio based on the predicted return. But for developing a portfolio both risk and return should be predicted. Our model predicts the risk without needing to experts and just with using available data in the market. To generate the membership functions, we use Fuzzy C-mean clustering algorithm. To test our neuro-fuzzy model we've used data on portfolios constituted from the Tehran Stock Exchange. The results show that the error of prediction is so small.	adaptive neuro fuzzy inference system;algorithm;cluster analysis;inference engine;neuro-fuzzy;risk assessment	Mohammad Hossein Fazel Zarandi;S. Farivar;I. Burhan Türksen	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608619	financial economics;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;neuro-fuzzy;machine learning;data mining;business;fuzzy set operations	NLP	5.741323806652471	-19.85689303929032	48005
15429e74a3f154db2410197ae8ee6a1b99d81c8f	estimation of state and parameters of traffic system	katedra kybernetiky;kybernetika;publikace estimation of state and parameters of traffic system;publications estimation of state and parameters of traffic system;informacni a řidici systemy;automaticke řizeni;uměla inteligence	This paper deals with the problem of traffic flow modelling and state estimation for historical urban areas. The most important properties of the traffic system are described. Then the model of the traffic system is presented. The weakness of the model is pointed out and subsequently rectified. Various estimation and identification techniques, used in the traffic problem, are introduced. The performance of various filters is validated, using the derived model and synthetic and real data coming from the center of Prague, with respect to filter accuracy	kalman filter;nonlinear system;rectifier (neural networks);synthetic intelligence	Pavla Pecherková;Jitka Homolová;Jindrich Duník	2007			control theory;control engineering;engineering	Networks	9.509871854301206	-12.413025028622988	48097
f18f8b950fd0919bbd30edab862924321963bc66	machine learning applications to power systems	learning algorithm;reseau electrique;power quality;electrical network;red electrica;evolutionary programming;intelligence artificielle;algorithme apprentissage;artificial intelligent;machine learning;power system;artificial intelligence method;pattern recognition;power system planning;artificial intelligence;inteligencia artificial;algoritmo aprendizaje;energy markets;fuzzy system;artificial neural network;expert system	The recent developments in the power system area, i.e. the on-going liberalization of the energy markets, the pressing demands for power system efficiency and power quality, the increase of dispersed, renewable generation and the growing number of interconnections and power exchanges among utilities, dictate the need for improvements in the power system planning, operation and control. At the same time, the power equipment industry faces new challenges in nowadays ever-increasing competition. Artificial Intelligence techniques together with traditional analytical techniques can significantly contribute in the solution of the related problems. Indeed, during the last 15 years, pattern recognition, expert systems, artificial neural networks, fuzzy systems, evolutionary programming, and other artificial intelligence methods have been proposed in an impressive number of publications in the power system community. Among the various power system functions, security remains a source of major concern. Power system deregulation and the increasing need to operate systems closer to their operating limits imply the use of more systematic approaches to security in order to maintain reliability at an acceptable level. Security assessment has proved therefore one of the most versatile ML applications leading to a large number of publications and an advanced application stage. Research started with Pattern recognition in the late sixties T.E. Dy Liacco [5], and seventies , C.K. Pang et al [13], etc. New methods have been developed next, able to handle the complexity and nonlinearity of power system security problems, like ANNs and machine learning methods [6], [3]. Since the mid-eighties significant interest is expressed by various electric Utilities that has contributed significantly to formalize the application methodology and to develop software tools. L. Wehenkel [21] provides an excellent overview of these developments. Forecasting is another very popular application of ML techniques, developed since the early seventies. A number of ANNs have been proposed, mainly in the areas of short-term load forecasting, e.g. Papalexopoulos et al. [14], and various real-life applications have been presented. The increasing wind power penetration mainly in isolated power systems poses the need for short-term wind power forecasting for efficient operation scheduling. Fuzzy models, genetic approaches and neural networks have proven to outperform traditional approaches [11]. Power system operation optimization is another versatile field for ML applications. The problems of Unit Commitment and Economic Dispatch, traditionally tackled as non-linear, constrained optimization problems, have lent themselves recently to genetic algorithm approaches, e.g. Sheble [17]. Other applications of ML techniques	ibm power systems;machine learning	Nikos D. Hatziargyriou	2001		10.1007/3-540-44673-7_20	evolutionary programming;electrical network;simulation;computer science;artificial intelligence;machine learning;mathematics;distributed computing;electric power system;operations research;computer security;expert system;artificial neural network;algorithm	EDA	6.978513351634292	-23.898818892282737	48298
40c68ac6def7b6091473f47cf31390eee8adc472	introduction of adaptive ts model using recursive gustafson-kessel algorithm in short term load forecasting		This paper introduces adaptive TS model developed with upgraded recursive Gustafson-Kessel (rGK) clustering in the field of short-term load forecasting (STLF), which is one of the most essential parts for electrical distributors. The problem of STLF is to forecast load consumption for a day ahead based on the weather forecast and the type of the day. Until now, most of the forecasting methods based on fuzzy logic needed a lot of expert knowledge to build and adapt the model, where rGK clustering lowers the need of this expert knowledge because of the automatic partitioning of the domain. In addition to rGK clustering, proposed solution also moves from directly forecasting the average load to forecasting the change of load from current to the next day, which is the fastest way to adapt the model to the change in electrical load system. To improve domain separation of clustering, improved membership function based both on input and output distance is also proposed.	adaptive algorithm;cluster analysis;electrical load;fastest;fuzzy logic;gustafson's law;input/output;long short-term memory;online and offline;recursion	Gregor Cerne	2017	2017 Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2017.7954822	fuzzy logic;cluster analysis;recursion;machine learning;input/output;membership function;computer science;artificial intelligence;electrical load	Vision	9.224789634044363	-17.825681394048228	48633
bdc2b39f7b954164ede34d9da9c9716a6b066906	likelihood-based assignment methods for multiple criteria decision analysis based on interval-valued intuitionistic fuzzy sets	comparative analysis;interval valued intuitionistic fuzzy set;mean likelihood determination method;likelihood based assignment method;multiple criteria decision analysis	The aim of this paper is to develop useful likelihood-based assignment methods for addressing multiple criteria decision-making problems within the environment of interval-valued intuitionistic fuzzy sets. Based on the likelihoods of interval-valued intuitionistic fuzzy preference relations, this paper determines the mean likelihoods of outranking relations and presents a mean likelihood determination method for generating a set of criterion-wise rankings of alternatives. By employing the concepts of rank frequency matrices and (ordinary) rank contribution matrices, this paper establishes a likelihood-based linear assignment model for multiple criteria decision analysis in the interval-valued intuitionistic fuzzy context. Additionally, this paper propounds two likelihood-based assignment models for handling incomplete and conflicting certain information of importance weights. These models can transform the criterion-wise ranks into the overall ranks for determining the optimal priority ranking of the alternatives. The feasibility and applicability of the proposed methods are illustrated with a practical problem of selecting a bridge construction method which involves various preference types. Finally, this paper conducts a comparative analysis with previous assignment-based methods in an interval-valued intuitionistic fuzzy setting to validate the effectiveness and advantages of the proposed methods.	decision analysis;fuzzy set	Jih-Chang Wang;Ting-Yu Chen	2015	FO & DM	10.1007/s10700-015-9208-6	qualitative comparative analysis;mathematical optimization;discrete mathematics;data mining;mathematics;multiple-criteria decision analysis	SE	-3.5296620812678574	-19.92725030222199	48669
5d018f74c9f44c4fdd63a9a5a73adb9cb484140e	combined artificial neural network and adaptive neuro-fuzzy inference system for improving a short-term electric load forecasting	hybrid intelligent system;load forecasting;mean absolute percentage error;neuro fuzzy system;hybrid system;adaptive neuro fuzzy inference system;artificial neural network	The main topic in this work was the development of a hybrid intelligent system for the hourly load forecasting in a time period of 7 days ahead, using a combination of Artificial Neural Network and Adaptive Neuro-Fuzzy Inference System. The hourly load forecasting was accomplished in two steps: in the first one, two ANNs are used to forecast the total load of the day, where one of the networks forecasts the working days (Monday through Friday), and the other forecasts the Saturdays, Sundays and public holidays; in the second step, the ANFIS was used to give the hourly consumption rate of the load. The proposed system presented a better performance as against the system currently used by Energy Company of Pernambuco, named PREVER. The simulation results showed an hourly mean absolute percentage error of 2.81% for the year 2005.	adaptive neuro fuzzy inference system;artificial neural network;neuro-fuzzy	Ronaldo R. B. de Aquino;Geane B. Silva;Milde M. S. Lira;Aida A. Ferreira;Manoel A. Carvalho;Otoni Nóbrega Neto;Josinaldo B. de Oliveira	2007		10.1007/978-3-540-74695-9_80	mean absolute percentage error;adaptive neuro fuzzy inference system;computer science;artificial intelligence;hybrid intelligent system;machine learning;data mining;artificial neural network;hybrid system	ML	9.278370818028062	-18.07025823934101	48683
a3717273d334ea41da9e06661658f4b184c2401d	dependent-chance programming with fuzzy decisions	multiobjective programming;fuzzy programming;genetic algorithms stochastic programming fuzzy set theory;spectrum;indexing terms;fuzzy simulation based genetic algorithm;fuzzy set theory;fuzzy sets;dcgp;uncertain environment;computational modeling;fuzzy simulation based genetic algorithm fuzzy decisions dcp stochastic programming fuzzy programming dependent chance multiobjective programming dcmop dependent chance goal programming dcgp uncertain environment chance function induced constraints;induced constraints;stochastic processes;mathematical programming;stochastic processes mathematical programming genetic algorithms computational modeling fuzzy sets mathematical model stochastic systems algorithm design and analysis fuzzy systems linear programming;dcmop;linear programming;mathematical model;chance function;goal programming;genetic algorithm;dependent chance multiobjective programming;genetic algorithms;stochastic systems;stochastic programming;fuzzy decisions;algorithm design and analysis;fuzzy systems;dcp;dependent chance goal programming	Dependent-chance programming (DCP) is a new type of stochastic programming and has been extended to the area of fuzzy programming. This paper provides a spectrum of DCP and dependent-chance multiobjective programming (DCMOP) as well as dependent-chance goal programming (DCGP) models with fuzzy rather than crisp decisions. The terms of uncertain environment, event, chance function, and induced constraints are discussed in the case of fuzzy decisions. A technique of fuzzy simulation is also designed for computing chance functions. Finally, we present a fuzzy simulation-based genetic algorithm for solving these models and illustrate its effectiveness by some numerical examples.	fuzzy concept	Baoding Liu	1999	IEEE Trans. Fuzzy Systems	10.1109/91.771090	mathematical optimization;genetic algorithm;defuzzification;reactive programming;type-2 fuzzy sets and systems;fuzzy classification;computer science;linear programming;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;inductive programming;fuzzy set operations;algorithm;fuzzy control system	Embedded	-0.3473082815252801	-17.41894342578357	48884
b0ad1739f76f6a8fe5adc94a7561540004f2f340	process efficiency of multistage production systems		Relational Network DEA allows the assessment of the overall efficiency of a multistage production system. However, when estimating the process efficiencies, there is uncertainty in the sense that different efficiency decompositions are feasible thus leading to interval process efficiency estimations. Recently, Nash bargaining game theory has been proposed to compute point estimates of process efficiencies for two-stage systems. In this paper that approach is extended to handle the multiplestage case.	connectionism;game theory;multistage amplifier;nash equilibrium;production system (computer science)	Sebastián Lozano	2013		10.3182/20130619-3-RU-3018.00048	econometrics;mathematical optimization;economics;operations management	AI	-2.974245313998573	-15.593717847529936	49222
f140cde58554e8775f3d0833805e5a4006571ca9	down and up operators associated to fuzzy relations and t-norms: a definition of fuzzy semi-ideals	fuzzy set;composition floue;norme t;teoria conjunto;fuzzy relation;conjunto difuso;theorie ensemble;ensemble flou;punto fijo;set theory;fixed point;point fixe;fuzzy operator;t norm;operateur flou;semi ideal;relation floue;complete lattice;fix point;relacion difusa;fuzzy composition;ideal	By means of the use of a t-norm T on a complete lattice L and of a L-fuzzy relation R on a set X , there are stated de nitions of LF-semi-ideal and of its dual LF-semilter of the set X . It is shown that both the poset of the LF-semi-ideals and one of the LF-semilters determined by the respective restrictions of the usual order in the class of L-fuzzy sets, are complete lattices. Another characterization of these concepts, as down and up L-fuzzy sets, is made via properties of the L-fuzzy relation R. At the end, we give some constructive methods with which the set of LF-semi-ideals and the set of down L-fuzzy sets can be determined. Examples and particular cases of the theory are presented. c © 2001 Elsevier Science B.V. All rights reserved.	fuzzy set;map;semiconductor industry;t-norm	Ramón Fuentes-González	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00369-8	ideal;discrete mathematics;topology;complete lattice;type-2 fuzzy sets and systems;artificial intelligence;mathematics;fixed point;t-norm;fuzzy set;equinumerosity;set theory	AI	0.6648905276464089	-22.566107440119172	49301
388cea4bb455ec48ce9ad29a132e8d37d16b7c45	some properties of (s) fuzzy integral	fuzzy set;fuzzy measure;non negative real number fuzzy integral fuzzy set;fuzzy measurable function;fuzzy integral;fuzzy set theory;fuzzy measurable function fuzzy set fuzzy measure fuzzy integral;atmospheric measurements particle measurements frequency modulation extraterrestrial measurements fuzzy sets educational institutions genetic algorithms	In this paper, we investigate some properties of fuzzy integral for fuzzy set on non-negative real number set and obtain some interesting conclusions.	fuzzy set	Wenyi Zeng;Baozhen Cui	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6234067	fuzzy logic;mathematical optimization;mathematical analysis;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	-0.6645295949509405	-22.877414191732445	49343
79d4977278feddfd5a39291bbfb9e2deabebdcb4	predicting the capital intensity of the new energy industry in china using a new hybrid grey model		Abstract Capital intensity is an important indicator for reflecting the relative changes of production factors of an industry. The facilitating effect of capital deepening, i.e., the positive impact of the augment of capital intensity, towards the structural transformation of the industry is definite. Therefore, the accurate prediction of capital intensity of the new energy industry is of great significance in facilitating the structural transformation and upgrading of the industry. Based on the Cobb-Douglas production function, an industry-level capital-labour ratio (KLR) model is established to describe the dynamic characteristics of the capital intensity. Then, by combining the estimation and prediction method of the nonlinear grey Bernoulli model (NGBM(1, 1)) with the KLR model, a new hybrid grey model, i.e., NGBM(1, 1)-KLR model is proposed. In this way, the economic meaning of the KLR and the advantage of the NGBM(1, 1) model in solving small-sample and nonlinear problems are complemented with each other, which enables one to more favourably predict the capital intensity of the industry. To verify the effectiveness and superiority of the proposed model, the NGBM (1, 1)-KLR model is used to predict the capital intensity of the new energy industry in China, and the model is compared with the GM(1, 1) and the NGBM(1, 1) models in the prediction performance. The empirical results show that the NGBM(1, 1)-KLR model can more accurately predict the capital intensity of the industry in China than the GM(1, 1) and the NGBM(1, 1) models. Moreover, the new hybrid grey model is used to carry out the out-of-sample prediction for the capital intensity of the new energy industry in China in the period of 2017–2020. The predicted results demonstrate that the structure of the industry in China will further transform and upgrade towards the capital deepening.		Hong-Hao Zheng;Q. Li;Zheng-Xin Wang	2018	Computers & Industrial Engineering	10.1016/j.cie.2018.10.012	financial economics;mathematical optimization;capital intensity;capital deepening;engineering;china;factors of production	Robotics	3.056512008646795	-14.664672320964954	49436
c7e6d93cb074c5c187d7ad99026d8f201018a56d	the impact of marginal utility and time on distributed information retrieval	marginal utility;information retrieval	This paper argues that marginal utility can be extended from the domain of Micro-economics to explain some of the problems that frustrate interaction with distributed systems. In particular, it is argued that concave utility curves can be used to analyse the electronic gridlock that occurs when remote systems cannot satisfy the number of demands which users make upon their services. Convex utility curves represent the information saturation that occurs when users cannot extract important documents from amass of irrelevant information. The paper goes on to argue that marginal utility can also be used to identify a range of interface techniques that reduce the problems associated with electronic gridlock and information saturation.	information retrieval;marginal model	Chris W. Johnson	1997			computer science;operations management;data mining;information retrieval	Web+IR	-1.4826876620686407	-11.650845957148213	49730
57ecba27e165bb46a9b35a762056182e401e6750	(n′, t, n)-implications		The use of fuzzy implications is widespread as easily found in the literature. The study of fuzzy implications attracts the attention of many authors probably because of their theoretical features and also the diversity of applications where they can be applied. In the present work we continue the study of a type of fuzzy implication, called (T, N)-implication, constructed by joining a fuzzy negation and a t-norm. However, we generalize those (T, N)-implications by presenting a new operator, named (N′, T, N)-implication, which can be derived from two negations. We also study the properties of (N′, T, N)-implications and compare the new results with the previous ones.	t-norm	Jocivania Pinheiro;Benjamín R. C. Bedregal;Regivan H. N. Santiago;Hélida Salles Santos	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491515		Visualization	-3.072579967815849	-23.691819354166118	49783
1655bd991db9fd1dc1fa6f50ef9d225099de1db5	statistics in practical decision making		This issue is devoted to statistics applied to decision maki ng problems. The issue begins from consideration of methodological possibilities to construct regressions wi th meaningful and interpretable coefficients for individua l predictors and shares of their importance. Ewa Nowakowska e xplores properties of the so-called Shapley value regression developed for adjusting regression coefficient s with multicollinearity among the predictors and estimati ng their importance. D. Liakhovitski, Y. Bryukhov, and M. Conk lin compare the approaches of random forests and relative weights of the predictors in regressions under var ious scenarios. Together with M. Conklin we investigate further features of Shapley value regression and its predic ting abilities. The next several works present statistical methods for solv ing different practical decision making problems. Dr. B. Vilge considers possibilities of regression prediction s for chemically active materials by physical and chemical characteristics measured by non-destructive testing. Pro f. J. Subramani describes how proper and meaningful decisions should be taken in modern manufacturing process c ontrol. Profs. S. Saxena, H.P. Singh, O.K. Gupta, and K.S. Rao share their experience on Bayesian assessment of fa ilure nd repair times for systems and components helping to reliability engineers to take correct decisions . And Prof. E. Demidenko shows how to choose the optimal portfolio to reach the investors’ financial goals. And continuing already established tradition of giving top ic quotations, here are some more of them on decision making.	coefficient;local interconnect network;maki kaji;random forest;singular value decomposition;software regression;stable marriage problem	Stan Lipovetsky	2010	MASA	10.3233/MAS-2010-0181	decision analysis;decision engineering;decision rule;business decision mapping	AI	-3.82451863125478	-15.483489504419268	49789
fea5ba5e47c061f2a60a2e112ddfe15a456b4e77	an intuitionistic fuzzy multi-criteria decision-making method based on an exponential-related function				Daniel O. Aikhuele;Faiz Turan	2017	IJFSA	10.4018/IJFSA.2017100103	fuzzy mathematics;fuzzy set operations;fuzzy logic;machine learning;artificial intelligence;mathematics;exponential function;fuzzy classification	EDA	-1.4173385231137055	-21.67487136706917	49801
86d5b63c9059b9dcc8eb0f1ef06ad7239a984c9c	the overall assurance interval for the non-archimedean epsilon in dea models; a partition base algorithm	62 07;analisis envolvimiento datos;analisis numerico;matematicas aplicadas;mathematiques appliquees;analisis datos;non archimedean infinitesimal epsilon;prise decision;analyse numerique;algorithme;algorithm;power plant;data analysis;numerical analysis;programacion lineal;62h30;data envelopment analysis;linear programming;programmation lineaire;linear program;analyse donnee;decision making unit;applied mathematics;toma decision;data envelope analysis;analyse enveloppement donnee;algoritmo	This study presents an algorithm for determining overall assurance interval of Epsilon in DEA models. In the previous studies in this regard, solving n linear programs are needed for this propose, where n is the number of decision making units (DMU) involved in the evaluation. This paper, based on a classification of DMU's, proposes a partition base algorithm that can determine the overall assurance interval of Epsilon by solving a few number of linear programs. The proposed algorithm is illustrated by the 44 unit data set of Power Plant Systems of Iran.	algorithm	Mohammad Reza Alirezaee	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.04.111	mathematical optimization;linear programming;calculus;data envelopment analysis;mathematics;algorithm	Logic	-0.06865070154959843	-15.68120501289467	49847
38b63367330135afa192347e8d99589989da1590	fuzzy structured element algorithm of system reliability with fuzzy parameter	fuzzy parameter;fuzzy reliability;fuzzy structured element;membership function	This paper mainly research on the system reliability with fuzzy parameters, and use two methods to solve the expression of the membership function about the fuzzy reliability of the system. When the fuzzy failure rate of the system is the fuzzy numbers that generated by fuzzy structured element, this paper put forward the formula of the fuzzy reliability and its membership function about the serial system, the parallel system, the parallel- serial system and the serial-parallel system.	algorithm	Yang Yang;Sizong Guo;Qian Zhang;Xu Cao	2010		10.1007/978-3-642-14880-4_41	fuzzy logic;control engineering;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;control theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	NLP	-1.509095433608717	-21.137425019052607	49925
f5e12f9194f13ca4323fe25ea25a256d7adc32b1	the reconciliation of multiple conflicting estimates: entropy-based and axiomatic approaches				João F. D. Rodrigues;Michael L. Lahr	2018	Entropy	10.3390/e20110815	mathematical optimization;econometrics;mathematics;axiom	Vision	0.35168138674676724	-19.818128790138157	50075
60dd55eaa7a1bf194708d4e96132a18629dfddb7	an integrated hierarchical temporal memory network for continuous multi-interval prediction of stock price trends		We propose an integrated hierarchical temporal memory (IHTM) network for continuous multi-interval prediction (CMIP) based on the hierarchical temporal memory (HTM) theory. The IHTM network is constructed by introducing three kinds of new modules to the original HTM network. One is Zeta1FirstNode which is used to cooperate with the original HTM node types for predicting stock price with multi-interval at any given time. The second is Shift-VectorFileSensor module used for inputting stock price data to the network continuously. The third is a MultipleOutputEffector module which produces multiple prediction results with different intervals simultaneously. With these three new modules, the IHTM network make sure newly arriving data is processed and continuous multi-interval prediction is provided. Performance evaluation shows that the IHTM is efficient in the memory and time consumption compared with the original HTM network in CMIP.	hierarchical temporal memory	Hyun-Syug Kang;Jianhua Diao	2012		10.1007/978-3-642-28670-4_2	financial economics;economics;data science;data mining	ML	6.422091534814446	-16.281235770671096	50318
a0b5ca688bb164f56d4655a1219eaba231baa65b	are there some manipulations of shibor: a hypothesis testing based on financial products linked to shibor	hypothesis testing	Manipulation of LIBOR events has sounded the alarm to our how to regulate the development the SHIBOR.The financial products linked to SHIBOR have emerged in recent years, bank of SHIBOR quoting may based on selfinterest, there may be an intrinsic motivation for higher or lower quoting SHIBOR .Based on analysis of financial products linked to seven-days-SHIBOR and three-months-SHIBOR between 2010 and 2011, and do a hypothesis testing that are there some manipulations of SHIBOR.The testing result showed that during the date of sales financial products, the higher quoting SHIBOR behavior may be existing, in order to raise people's expectations of yield of financial products, but lower quoting SHIBOR behavior may not be existing at the date of calculat ing interest of financial products. Open access under CC BY-NC-ND license. Open access under CC BY-NC-ND license. 940 Lin Chen and Zongfang Zhou / Procedia Computer Science 17 ( 2013 ) 939 – 944 and empirical research [9-10]. (5) Empirical research between SHIBOR and repo rate o f national bond [11-12]. Before exposing of LIBOR manipulation event, there is no scholar pay attention to the probability of SHIBOR manipulation. But LIBOR manipu lation event tell us there may be also exist the probability of SHIBOR manipulation, the intrinsic reason may come from the bug of operation mechanisms. Now, the price quotation group of SHIBOR consists of 16 commercial banks .These quoting banks are primary dealers of open market operation or market makers in the interbank market, with sound information disclosure and active RMB transactions in China's money market. In recent years, more and more commercial banks included SHIBOR price quotation member issued a lots of financial products linked to SHIBOR.So the change of SHIBOR will affect the values of more and more financial market in china. Similar to the LIBOR quotes and manipulation, some commercial banks also may be quote lower or h igher SHIBOR for certain purposes, for example, while sale date of the financial products lined to SHIBOR, commercial banks may quote more higher SHIBOR, and improve the attractiveness for investors, on the other hand, at the coupon date, commercial banks may quote more lower SHIBOR for purpose of paying more lower coupon. In short, according to SHIBOR price quotation mechanisms, commercial banks of SHIBOR price quotation member are both athletes and judges. This mechanis m lead to commercial banks may have motive to manipulate SHIBOR price quotation with self-interest.Of course, whether there are real manipulat ing events may need to be ruled from a legal perspective, this paper only from the academic side, based on limited data and logical assumptions, do a hypothesis testing on SHIBOR manipulation event. The paper is organized as follows. In section two, we analyzes the data of financial product lined to SHIBOR.In section three, we put forward some models to do hypothesis testing on SHIBOR manipulat ion event, Finally, section four offers some concluding remarks . 2. Data of financial products linked to seven-days-SHIBOR and three-months-SHIBOR Since 2010, many Chinese commercial banks have issued a lot of financial products linked SHIBOR. For example, in the past two years, a commercial bank of SHIBOR price quotation have issued more than 1100 variety financial products linked SHIBOR,and the most active financial products linked SHIBOR were linked to seven days-SHIBOR or three-months-SHIBOR.So the following research mainly focus on the financial products linked to seven-days-SHIBOR or three-months-SHIBOR. In the European financial market, Eurodollar deposits and LIBOR usually is highly relevant and consistent. So Eurodollar deposits also have a role to judge whether LIBOR is rat ional. But in Chinese, there is no an authoritative third-party rate reference to whether SHIBOR is rational. Many scholars think that the Repo rate of National bond may be a good reference to whether SHIBOR is rat ional, and do some empirical researches between SHIBOR and repo rate of national bond. So we collect the data of seven-days-SHIBOR three-months-SHIBOR seven-daysrepo-rate three-months-repo-rate between 2010 and 2011. The result of statistical analysis of the data are shown in table 1 Fig1 and Fig2. Table1. Statistical analysis on repo rate and SHIBOR Rate type Number of valid data Mean Standard deviation Unit root stationery test seven-days-SHIBOR 490 3.08% 1.517% stationery seven-days-repo-rate 490 3.27% 1.54% stationery three-months-SHIBOR 500 3.83% 1.520% none stationery three-months-repo-rate 500 3.95% 1.59% none stationery 941 Lin Chen and Zongfang Zhou / Procedia Computer Science 17 ( 2013 ) 939 – 944 Fig1. Seven-days-SHIBOR and Seven-days-Repo-Rate Fig2. Three-months-SHIBOR and Three-months -Repo-Rate According to the statistical characteristics and the trend of Figure 1~2, and referring to the repurchase market interest rate find there is obviously anomaly o f SHIBOR; this is also consistent with some empirical studies of the interaction between SHIBOR and repo rate. Further we have co llected the data o f financial products linked to SHIBOR issued by a commercial bank of SHIBOR price quotation. Those financial coupon relies on the seven-days-SHIBOR or Three-monthsSHIBOR. Corresponding to each rate of seven-days-SHIBOR and Three-months-SHIBOR between 2010 and 2011, we note those dates that it is the coupon date of financial products linked to seven-days-SHIBOR and Three-monthsSHIBOR,and denote it by variable d t S 7 and M t S 3 : other S d t 0 SHIBOR days seven to linked products financial of date coupon the is it , 1 7	anomaly detection;computer science;entity–relationship model;norsk data	Lin Chen;Zongfang Zhou	2013		10.1016/j.procs.2013.05.119	actuarial science	ECom	4.043666092934512	-16.62041232719643	50454
c481410b799a9d96f2ec2c27f50fc7990792ce7b	autoencoder networks for water demand predictive modelling		Following a number of studies that have interrogated the usability of an autoencoder neural network in various classification and regression approximation problems, this manuscript focuses on its usability in water demand predictive modelling, with the Gauteng Province of the Republic of South Africa being chosen as a case study. Water demand predictive modelling is a regression approximation problem. This autoencoder network is constructed from a simple multi-layer network, with a total of 6 parameters in both the input and output units, and 5 nodes in the hidden unit. These 6 parameters include a figure that represents population size and water demand values of 5 consecutive days. The water demand value of the fifth day is the variable of interest, that is, the variable that is being predicted. The optimum number of nodes in the hidden unit is determined through the use of a simple, less computationally expensive technique. The performance of this network is measured against prediction accuracy, average prediction error, and the time it takes the network to generate a single output. The dimensionality of the network is also taken into consideration. In order to benchmark the performance of this autoencoder network, a conventional neural network is also implemented and evaluated using the same measures of performance. The conventional network is slightly outperformed by the autoencoder network.	activation function;algorithm;analysis of algorithms;approximation;artificial neural network;autoencoder;benchmark (computing);input/output;layer (electronics);model selection;predictive modelling;rand tablet;rand index;time series;usability	Ishmael S. Msiza;Tshilidzi Marwala	2016	2016 6th International Conference on Simulation and Modeling Methodologies, Technologies and Applications (SIMULTECH)	10.5220/0005977202310238	simulation;predictive modelling;autoencoder;input/output;mean squared prediction error;artificial neural network;curse of dimensionality;usability;machine learning;computer science;artificial intelligence;multilayer perceptron	ML	9.136999123241852	-20.4014375390997	50693
08d1c985632be7cd3897133ba74a678a059b107d	consensus building with a group of decision makers under the hesitant probabilistic fuzzy environment	hesitant probabilistic fuzzy number (hpfe);maximizing score deviation (msd) method;ordered weighted operator;consensus building;group decision making	As a generalized fuzzy number, the hesitant fuzzy element (HFE) has been receiving increased attention and has recently become a popular topic. However, we find that the occurring probabilities of the possible values in the HFE are equal, which is obviously impractical. Consequently, in this paper, we propose a hesitant fuzzy number with probabilities, called the hesitant probabilistic fuzzy number, and construct its score function, deviation function, comparison laws, and its basic operations. It is well known that in the context of a group of decision makers (DMs), one of the basic approaches to built consensus is to aggregate individual evaluations or individual priorities. Thus, to use the hesitant fuzzy numbers for consensus building with a group of DMs, we further propose a method called maximizing score deviation method to obtain the DMs’ weights under the HPFE environment, based on which two extended and four new ordered weighted operators are provided to fuse the HPFE information and build the consensus of the DMs. We also analyze the differences among these ordered weighted operators and provide their application scopes. Finally, a practical case is provided to demonstrate consensus building with a group of DMs under the HPFE environment using the proposed approaches.	fuzzy concept	Zeshui Xu;Wei Zhou	2017	FO & DM	10.1007/s10700-016-9257-5	artificial intelligence;fuzzy number;data mining;mathematics	Security	-3.7160434923334744	-20.442178449887507	50781
57eb8d014628f9ad4a4ef8cb309e4e9692f1b892	a novel dnn-hmm-based approach for extracting single loads from aggregate power signals	redd dataset dnn hmm based approach power trace extraction single channel aggregate power signals nonintrusive load monitoring nilm systems source separation problem factorial hidden markov models fhmm gaussian distribution deep neural network reference energy disaggregation dataset;aggregates hidden markov models load modeling training monitoring source separation probability density function;hidden markov model;hmm;non intrusive load monitoring nilm;power system measurement gaussian distribution hidden markov models neural nets power engineering computing;deep neural networks dnn non intrusive load monitoring nilm supervised power disaggregation hidden markov model hmm;deep neural networks dnn;supervised power disaggregation	This paper presents a new supervised approach to extract the power trace of individual loads from single channel aggregate power signals in non-intrusive load monitoring (NILM) systems. Recent approaches to this source separation problem are based on factorial hidden Markov models (FHMM). Drawbacks are the needed knowledge of HMM models for all loads, what is infeasible for large buildings, and the large combinatorial complexity. Our approach trains HMM with two emission probabilities, one for the single load to be extracted and the other for the aggregate power signal. A Gaussian distribution is used to model observations of the single load whereas observations of the aggregate signal are modeled with a Deep Neural Network (DNN). By doing so, a single load can be extracted from the aggregate power signal without knowledge of the remaining loads. The performance of the algorithm is evaluated on the Reference Energy Disaggregation (REDD) dataset.	aggregate data;algorithm;deep learning;hidden markov model;markov chain;source separation;supervised learning	Lukas Mauch;Bin Yang	2016	2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2016.7472104	speech recognition;computer science;machine learning;pattern recognition;hidden markov model	HPC	8.946446613811347	-16.175487715224694	50784
0306b57878e45528bb2ec5a8034098c00077eff9	one-sided elasticities and technical efficiency in multi-output production: a theoretical framework	elasticite;economie;economia;elasticity;analisis envolvimiento datos;science gestion;entrada salida;rendement echelle;theoretical framework;retorno de escala;elasticidad;one sided elasticities;return to scale;left handed;input output;right handed;ciencias economicas;article multiple;technical efficiency;data envelopment analysis;articulo multiple;multiple item;sciences economiques;economy;economics;data envelope analysis;management science;entree sortie;analyse enveloppement donnee;returns to scale;multi output production	One of the concepts that have sparked considerable interest in the theory of production and efficiency is that of returns to scale (RTS). Economics researchers typically define RTS using the notion of elasticity. Considerable research activity on RTS has also been observed by management science researchers, who utilize the methodology of Data Envelopment Analysis (DEA) to gain insights on RTS. In this paper, we present a theoretical framework that integrates existing economics and management science literature on RTS, and provides a solid foundation for research work in this area. Our framework defines, discusses, and proposes an approach to measure input- and output-oriented elasticities, and one-sided RTS. We demonstrate how the work done in DEA is a special case of our framework, and discuss the conditions under which the resulting two left-hand, and the two right-hand elasticities can be equal. Future research directions are also discussed.		Petros Hadjicostas;Andreas C. Soteriou	2006	European Journal of Operational Research	10.1016/j.ejor.2004.05.008	returns to scale;economics;operations management;data envelopment analysis;operations research;welfare economics	Vision	-0.19467773332378946	-12.535462498150146	51024
4fb7b949a2e0ad4108b1e1b2f0f1083d68d5f46f	computing lower and upper expectations under epistemic independence	graphic method;probabilidad imprecisa;computacion informatica;random event;concepts of independence;epistemologie;intelligence artificielle;systeme incertain;imprecise probability;epistemic independence;methode graphique;mesure probabilite;probabilite imprecise;ciencias basicas y experimentales;estimacion probabilista;acontecimiento aleatorio;imprecise probabilities;random variable;epistemology;graphical model;metodo grafico;artificial intelligence;estimation probabiliste;inteligencia artificial;grupo a;epistemologia;sistema incierto;sets of probability measures;probability measure;uncertain system;multilinear programming;medida probabilidad;evenement aleatoire;probabilistic assessment	This papers investigates the computation of lower/upper expectations that must cohere with a collection of probabilistic assessments and a collection of judgements of epistemic independence. New algorithms, based on multilinear programming, are presented, both for independence among events and among random variables. Separation properties of graphical models are also investigated.	algorithm;computation;graphical model	Cassio Polpo de Campos;Fábio Gagliardi Cozman	2005	Int. J. Approx. Reasoning	10.1016/j.ijar.2006.07.013	random variable;imprecise probability;probability measure;artificial intelligence;mathematics;graphical model;algorithm;statistics	AI	2.1149769784228694	-19.626592837818453	51140
5f65380e6aaf8438be73ab6a2e9fffff87aad68b	comparing lazy and eager learning models for water level forecasting in river-reservoir basins of inundation regions	water level;lazy learning;prediction;basin;eager learning	This study developed a methodology for formulating water level models to forecast river stages during typhoons, comparing various models by using lazy and eager learning approaches. Two lazy learning models were introduced: the locally weighted regression (LWR) and the k-nearest neighbor (kNN) models. Their efficacy was compared with that of three eager learning models, namely, the artificial neural network (ANN), support vector regression (SVR), and linear regression (REG). These models were employed to analyze the Tanshui River Basin in Taiwan. The data collected comprised 50 historical typhoon events and relevant hourly hydrological data from the river basin during 1996-2007. The forecasting horizon ranged from 1?h to 4?h. Various statistical measures were calculated, including the correlation coefficient, mean absolute error, and root mean square error. Moreover, significance, computation efficiency, and Akaike information criterion were evaluated. The results indicated that (a) among the eager learning models, ANN and SVR yielded more favorable results than REG (based on statistical analyses and significance tests). Although ANN, SVR, and REG were categorized as eager learning models, their predictive abilities varied according to various global learning optimizers. (b) Regarding the lazy learning models, LWR performed more favorably than kNN. Although LWR and kNN were categorized as lazy learning models, their predictive abilities were based on diverse local learning optimizers. (c) A comparison of eager and lazy learning models indicated that neither were effective or yielded favorable results, because the distinct approximators of models that can be categorized as either eager or lazy learning models caused the performance to be dependent on individual models. Lazy and eager learning models are modeled for water level forecasting in rivers.Lazy learning models, LWR and kNN are presented.Eager learning models, ANN, SVR, and regression are compared with LWR and kNN.The 50 historical typhoons that affected the studied watershed are collected.The distinct approximators of models cause the performance to be dependent on individual models.	eager learning;lazy evaluation	Chih-Chiang Wei	2015	Environmental Modelling and Software	10.1016/j.envsoft.2014.09.026	water level;prediction;computer science;artificial intelligence;machine learning;data mining;mathematics;statistics	ML	9.629327627052389	-19.548289694077784	51151
d642d7f5ae43e3ae7d027d3e6009de3da9a87026	contradiction resolution for foreign exchange rates estimation		In this paper, we propose a new type of information-theoretic method called ”contradiction resolution.” In this method, we suppose that a neuron should be evaluated for itself (self-evaluation) and by all the other neurons (outer-evaluation). If some difference or contradiction between two types of evaluation can be found, the contradiction should be decreased as much as possible. We applied the method to the self-organizing maps with an output layer, which is a kind of combination of the self-organizing maps with the RBF networks. When the method was applied to the dollar-yen exchange rates, prediction and visualization performance could be improved simultaneously.	foreign exchange service (telecommunications);information theory;neuron;organizing (structure);quantization (signal processing);quantum fluctuation;radial basis function network;self-organization;self-organizing map;topography	Ryotaro Kamimura	2012			macroeconomics;economics;contradiction	ML	8.25278941793442	-19.236078866331184	51252
1cde1047f61e322d941de12bb784a0b855e3ede4	generalized orthopair fuzzy sets	dual of aggregation knowledge representation non standard fuzzy sets intuitionistic sets pythagorean fuzzy sets;fuzzy sets standards fuzzy set theory indexes knowledge representation pragmatics machine intelligence	"""We note that orthopair fuzzy subsets are such that that their membership grades are pairs of values, from the unit interval, one indicating the degree of support for membership in the fuzzy set and the other support against membership. We discuss two examples, Atanassov's classic intuitionistic sets and a second kind of intuitionistic set called Pythagorean. We note that for classic intuitionistic sets the sum of the support for and against is bounded by one, while for the second kind, Pythagorean, the sum of the squares of the support for and against is bounded by one. Here we introduce a general class of these sets called q-rung orthopair fuzzy sets in which the sum of the <inline-formula><tex-math notation=""""LaTeX"""">${\rm{q}}$</tex-math></inline-formula>th power of the support for and the <inline-formula><tex-math notation=""""LaTeX"""">${\rm{q}}$</tex-math></inline-formula>th power of the support against is bonded by one. We note that as q increases the space of acceptable orthopairs increases and thus gives the user more freedom in expressing their belief about membership grade. We investigate various set operations as well as aggregation operations involving these types of sets."""	fuzzy set	Ronald R. Yager	2017	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2016.2604005	fuzzy logic;discrete mathematics;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	DB	-1.3092906556341746	-22.787002086678246	51565
f82bf3ba6531da0c1cf5bbd3ed46b3eebb0049ba	application of possibilistic fuzzy regression for technology watch	technology watch;fuzzy regression;fuzzy linear regression;trend extraction	Identification and assessment of technological advances have been vital for companies to keep their competitive position or to gain new capabilities for the competition. In this content, Technology Watch Systems (TWS) have been tools of systematic analysis of technology developments that outputs regarding the technological opportunities and threats could be easily interpreted by an analyst. Among several TWS’s, Patent Alert System (PAS) has been a recently developed one which enables users to set or configure alert(s) for the trend changes in a certain technology area of requested sector. Data of associated patent counts is retrieved by extended markup language (XML) mechanism located in PAS, and then an internal alert triggering mechanism is used to search for trend changes on the associated data. This internal alert triggering mechanism is a kind of modified linear regression which sets a newer trend line once a certain amount of deviation (threshold) has risen. Although alerts, indicating the direction of technological changes, provide supportive information to the analysts, extracted trend lines have been narrowed by a strict line where possible deviations have not been reflected. However, deviations in techno-systems are known to occur as the consequence of the vagueness coming from the nature of the system. Therefore, in this work, alert triggering mechanism of PAS is reconsidered using “possibilistic linear fuzzy regression”. Results yielded better and promising outcomes for the reconsidered algorithm.		Türkay Dereli;Alptekin Durmusoglu	2010	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-2010-0467	artificial intelligence;machine learning;data mining;operations research;statistics	Robotics	-4.5321189388215215	-15.15392622062705	51576
eecbd93340cda5da9a741dee2af2e5bf4d577be5	a risk factor analysis of west nile virus: extraction of relationships from a neural-network model	built environment;west nile virus;risk factors;linear model;interpretation;neural network model;non linear;infectious disease;neural network	The West Nile Virus (WNV) is an infectious disease spreading rapidly throughout the United States, causing illness among thousands of birds, animals, and humans. The broad categories of risk factors underlying WNV incidences are: environmental, socioeconomic, built-environment, and existing mosquito abatement policies. Computational neural network (CNN) model was developed to understand the occurrence of WNV infected dead birds because of their ability to capture complex relationships with higher accuracy than linear models. In this paper, we describe a method to interpret a CNN model by considering the final optimized weights. The research was conducted in the Metropolitan area of Minnesota, which had experienced significant outbreaks from 2002 till present.	factor analysis;network model	Debarchana Ghosh;Rajarashi Guha	2010		10.1007/978-3-642-12079-4_27	interpretation;computer science;machine learning;linear model;risk factor;built environment;artificial neural network	ML	7.68915531165641	-18.137829314208858	51758
8f760f030f12185888e4d00779d1cf48ca40346a	a freeway travel time predicting method based on iov	internet of vehicles;travel time prediction;mape real time travel time traffic management traffic control internet of vehicles dynamic traffic information remaining travel time rtt iov environment markov chains mean absolute percent error;vehicles traffic control intelligent vehicles markov processes computational modeling cloud computing;internet of vehicles markov chains remaining travel time travel time prediction;remaining travel time;markov chains;traffic information systems markov processes real time systems traffic control	Real-time travel time is one of the important value for traffic management and traffic control. With the help of Internet of Vehicles (IOV), the dynamic traffic information can be collected and distributed more correctly. In this paper, we propose a method to predict the remaining travel time (RTT) for a vehicle on the freeway in the IOV environment. The Markov chains are adopted to predict a vehicle's remaining travel time in real-time. The experimental results prove that the mean absolute percent error (MAPE) is less than 10%.	approximation algorithm;approximation error;freeway;giove;markov chain;mean squared error;real-time clock;real-time transcription	Daxin Tian;Chao Liu;Yunpeng Wang;Guohui Zhang;Haiying Xia	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389017	markov chain;simulation;statistics	Robotics	9.302429670973986	-12.936076715052089	51852
ce830d10941b7faeaeabf79781777114fbd36e58	patent valuation using difference in alen		In this paper we present an approach for patent claim comparison based on the difference operator in description logics. The claims are represented using ALEN . An algorithm computing the difference between two ALEN concept descriptions is proposed and its usefulness for patent application valuation is described by means of some simple examples.	algorithm;decision support system;description logic;finite difference;longest common subsequence problem;prototype;semantic reasoner;subsumption architecture;value (ethics)	Naouel Karam;Adrian Paschke	2012			description logic;operator (computer programming);patent application;financial economics;patent valuation;valuation (finance);mathematics	AI	-1.731455237136812	-17.440758899899016	51854
e57d507c071950ff7d5358de4f05903e9c450a0e	strict fuzzy orderings in a similarity-based setting	fuzzy equivalence relation	This paper introduces and justifies a similaritybased concept of strict fuzzy orderings and provides constructions how fuzzy orderings can be transformed into strict fuzzy orderings and vice versa. We demonstrate that there is a meaningful correspondence between fuzzy orderings and strict fuzzy orderings. Unlike the classical case, however, we do not obtain a general one-to-one correspondence. We observe that the strongest results are achieved if the underlying t-norm induces a strong negation, which, in particular, includes nilpotent t-norms and the nilpotent minimum.	one-to-one (data model);stable model semantics;t-norm	Ulrich Bodenhofer;Mustafa Demirci	2005			combinatorics;discrete mathematics;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set operations	AI	-1.2123446906169568	-23.413408687312348	51873
9e4a6abd3f84d2d1b64d41a8c63c5e5ffe3b5eec	taiex forecasting based on fuzzy time series and fuzzy variation groups	forecasting;taiex forecasting;fuzzy time series;fuzzy variations;fuzzy set;fuzzy variation groups;testing;fuzzy logical relationship groups taiex forecasting fuzzy time series fuzzy variation groups taiwan stock exchange capitalization weighted stock index fuzzy sets theory;time series fuzzy set theory group theory stock markets;time series;group theory;fuzzy set theory;fuzzy sets;stock markets;fuzzy logical relationship groups;fuzzy logic;training data;accuracy;time series analysis;fuzzy logical relationships;fuzzy sets theory;predictive models;prediction model;taiwan stock exchange capitalization weighted stock index;fuzzy variations fuzzy logical relationships fuzzy sets fuzzy time series fuzzy variation groups;forecasting time series analysis fuzzy sets predictive models training data accuracy testing	In this paper, we present a new method to forecast the daily Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX) based on fuzzy time series and fuzzy variation groups, where the main input factor is the previous day's TAIEX, and the secondary factor is either the Dow Jones, the NASDAQ, the M 1b, or their combination. First, the proposed method fuzzifies the historical training data of the TAIEX into fuzzy sets to form fuzzy logical relationships. Second, it groups the fuzzy logical relationships into fuzzy logical relationship groups (FLRGs) based on the fuzzy variations of the secondary factor. Third, it evaluates the leverage of the fuzzy variations between the main factor and the secondary factor to construct fuzzy variation groups. Fourth, it gets the statistics of the fuzzy variations appearing in each fuzzy variation group. Fifth, it calculates the weights of the statistics of the fuzzy variations appearing in each fuzzy variation group, respectively. Finally, based on the weights of the statistics of the fuzzy variations appearing in the fuzzy variation groups and the FLRGs, it performs the forecasting of the daily TAIEX. Because the proposed method uses both fuzzy variation groups and FLRGs to analyze in detail the historical training data, it gets higher forecasting accuracy rates to forecast the TAIEX than the existing methods.	fuzzy set;jones calculus;time series;typhoon;xiii	Shyi-Ming Chen;Chao-Dian Chen	2011	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2010.2073712	defuzzification;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set;group theory;statistics	DB	4.259930739604919	-22.019722309508275	52021
9801b69ac1faaffafecfea250ec65a5ac33cd8af	predicting purchase decisions in mobile free-to-play games	behavioral analytics;behavioral profiling;forecasting player behavior;statistical data mining	Mobile digital games are dominantly released under the freemium business model, but only a small fraction of the players makes any purchases. The ability to predict who will make a purchase enables optimization of marketing efforts, and tailoring customer relationship management to the specific user’s profile. Here this challenge is addressed via two models for predicting purchasing players, using a 100,000 player dataset: 1) A classification model focused on predicting whether a purchase will occur or not. 2) a regression model focused on predicting the number of purchases a user will make. Both models are presented within a decision and regression tree framework for building rules that are actionable by companies. To the best of our knowledge, this is the first study investigating purchase decisions in freemium mobile products from a user behavior perspective and adopting behavior-driven learning approaches to this problem.	customer relationship management;decision tree learning;mathematical optimization;purchasing	Rafet Sifa;Fabian Hadiji;Julian Runge;Anders Drachen;Kristian Kersting;Christian Bauckhage	2015			simulation	AI	-1.6803041891264867	-10.835395196103795	52387
201aa10b8ef572c04c865b3c8a365e339ffdd8b0	on the jaccard index with degree of optimism in ranking fuzzy numbers	fuzzy number;decision maker;indexation;similarity measure	Ranking of fuzzy numbers plays an important role in prac- tical use and has become a prerequisite procedure for decision-making problems in fuzzy environment. Jaccard index similarity measure has been introduced in ranking the fuzzy numbers where fuzzy maximum, fuzzy minimum, fuzzy evidence and fuzzy total evidence are used in de- termining the ranking. However, the fuzzy total evidence is obtained by using the mean aggregation which can only represent the neutral deci- sion maker's perspective. In this paper, the degree of optimism concept which represents all types of decision maker's perspectives is applied in calculating the fuzzy total evidence. Thus, the proposed method is capa- ble to rank fuzzy numbers based on optimistic, pessimistic and neutral decision maker's perspective. Some properties which can simplify the ranking procedure are also presented.	jaccard index	Nazirah Ramli;Daud Mohamad	2010		10.1007/978-3-642-14058-7_40	fuzzy logic;decision-making;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;fuzzy measure theory;data mining;mathematics;fuzzy set operations	NLP	-2.21475591050133	-21.070993059587735	52458
0779b0f6ac69f17b53c9b99d477e70ad7e9c9bf0	half a billion simulations: evolutionary algorithms and distributed computing for calibrating the simpoplocal geographical model	multiagent system;construccion arquitectura tecnologia ambiental;high performance computing;model validation;geographical modelling;evolutionary algorithm;tecnologias;simulation model;calibration	Multi-agent geographical models integrate very large numbers of spatial interactions. In order to validate those models large amount of computing is necessary for their simulation and calibration. Here a new data processing chain including an automated calibration procedure is experimented on a computational grid using evolutionary algorithms. This is applied for the first time to a geographical model designed to simulate the evolution of an early urban settlement system. The method enables us to reduce the computing time and provides robust results. Using this method, we identify several parameter settings that minimise three objective functions that quantify how closely the model results match a reference pattern. As the values of each parameter in different settings are very close, this estimation considerably reduces the initial possible domain of variation of the parameters. The model is thus a useful tool for further multiple applications on empirical historical situations.	distributed computing;evolutionary algorithm;grid computing;interaction;multi-agent system;simulation	Clara Schmitt;Sebastien Rey-Coyrehourcq;Romain Reuillon;Denise Pumain	2014	CoRR	10.1068/b130064p	calibration;simulation;computer science;artificial intelligence;theoretical computer science;evolutionary algorithm;simulation modeling;regression model validation;statistics	HPC	8.23986351711947	-12.037544025070085	52676
1bf6b95b765859c0d51fd013cc59f2ab4774e775	short-term traffic prediction under both typical and atypical traffic conditions using a pattern transition model		One of the most challenging goals of the modern Intelligent Transportation Systems comprises the accurate and real-time short-term traffic prediction. The achievement of this goal becomes even more critical when the presence of atypical traffic conditions is concerned. In this paper, we propose a novel hybrid technique for short-term traffic prediction under both typical and atypical conditions. An Automatic Incident Detection (AID) algorithm, based on Support Vector Machines (SVM), is utilized to check for the presence of an atypical event (e.g. traffic accident). If such an event occurs, the k-Nearest Neighbors (k-NN) non-parametric regression model is used for traffic prediction. Otherwise, the Autoregressive Integrated Moving Average (ARIMA) parametric model is activated for the same purpose. In order to evaluate the performance of the proposed model, we use open real world traffic data from Caltrans Performance Measurement System (PeMS). We compare the proposed model with the unitary k-NN and ARIMA models, which represent the most commonly used non-parametric and parametric traffic prediction models. Preliminary results show that the proposed model achieves larger accuracy under both typical and atypical traffic conditions.	autoregressive integrated moving average;autoregressive model;experiment;feature extraction;k-nearest neighbors algorithm;parametric model;real-time transcription;support vector machine;vehicle routing problem	Traianos-Ioannis Theodorou;Athanasios Salamanis;Dionisis D. Kehagias;Dimitrios Tzovaras;Christos Tjortjis	2017		10.5220/0006293400790089	computer science	AI	8.717858156855616	-14.44274010945256	52730
71079a31c9668ea907cb54f7c49221ac11505395	cumulative paired φ-entropy	generalized maximum entropy principle;differential entropy;l estimator;tukey s λ distribution;dependence;cumulative residual entropy;power logistic distribution;regression;measure of polarization;cumulative entropy;measure of dispersion;absolute mean deviation;linear rank test;φ entropy;φ regression;entropy	Abstract: A new kind of entropy will be introduced which generalizes both the differential entropy and the cumulative (residual) entropy. The generalization is twofold. First, we simultaneously define the entropy for cumulative distribution functions (cdfs) and survivor functions (sfs), instead of defining it separately for densities, cdfs, or sfs. Secondly, we consider a general “entropy generating function” φ, the same way Burbea et al. (IEEE Trans. Inf. Theory 1982, 28, 489–495) and Liese et al. (Convex Statistical Distances; Teubner-Verlag, 1987) did in the context of φ-divergences. Combining the ideas of φ-entropy and cumulative entropy leads to the new “cumulative paired φ-entropy” (CPEφ). This new entropy has already been discussed in at least four scientific disciplines, be it with certain modifications or simplifications. In the fuzzy set theory, for example, cumulative paired φ-entropies were defined for membership functions, whereas in uncertainty and reliability theories some variations of CPEφ were recently considered as measures of information. With a single exception, the discussions in the scientific disciplines appear to be held independently of each other. We consider CPEφ for continuous cdfs and show that CPEφ is rather a measure of dispersion than a measure of information. In the first place, this will be demonstrated by deriving an upper bound which is determined by the standard deviation and by solving the maximum entropy problem under the restriction of a fixed variance. Next, this paper specifically shows that CPEφ satisfies the axioms of a dispersion measure. The corresponding dispersion functional can easily be estimated by an L-estimator, containing all its known asymptotic properties. CPEφ is the basis for several related concepts like mutual φ-information, φ-correlation, and φ-regression, which generalize Gini correlation and Gini regression. In addition, linear rank tests for scale that are based on the new entropy have been developed. We show that almost all known linear rank tests are special cases, and we introduce certain new tests. Moreover, formulas for different distributions and entropy calculations are presented for CPEφ if the cdf is available in a closed form.	differential entropy;entropy (information theory);erkki oja;fuzzy set;jensen's inequality;langrisser schwarz;linear logic;maximum entropy probability distribution;principle of maximum entropy;reliability engineering;set theory;shannon (unit);social inequality;uncertainty theory	Ingo Klein;Benedikt Mangold;Monika Doll	2016	Entropy	10.3390/e18070248	econometrics;entropy;joint entropy;conditional quantum entropy;regression;binary entropy function;transfer entropy;maximum entropy probability distribution;calculus;mathematics;joint quantum entropy;differential entropy;absolute deviation;statistical dispersion;min entropy;statistics;l-estimator	ML	0.5845227352496121	-20.623936780651498	52748
d22c55941cfe8f43c72865e90f459811fd9be5a2	ranking patent assignee performance by h-index and shape descriptors	rank citation curve;patent assignee ranking;shape descriptor;patentometrics;h index;geometric interpretation;indexation	We propose a geometric interpretation to the ranking of patent assignees by their h-indices as indicating the relative positions of their rank-citation curves. We then propose two shape descriptors characterizing the rank-citation curves over the h-cores and h-tails, respectively. Together with the h-indices, the shape descriptors help verifying the geometric relationship among rank-citation curves and the relative performance among the assignees’ h-cores and h-tails. The geometric interpretation and shape descriptors are proven by empirical data to be reliable, accurate, robust, flexible, and insightful, and their application could be extended to research performance evaluation as well. Crown Copyright © 2011 Published by Elsevier Ltd. All rights reserved.	angular defect;crown group;performance evaluation;robustness (computer science);shape analysis (digital geometry);tails;verification and validation	Chung-Huei Kuan;Mu-Hsuan Huang;Dar-Zen Chen	2011	J. Informetrics	10.1016/j.joi.2011.01.002	econometrics;pattern recognition;data mining;mathematics	AI	1.8368839432311523	-15.525447521962972	52856
40c5e616eed9e96f2131c3a139816fb86ca8f9e7	fuzzy id3 algorithm based on generating hartley measure	generating hartley measure;the level importance function;generating fuzzy id3 decision tree algorithm	Fuzzy decision tree induction algorithm is an important way with uncertain information. However, the current fuzzy decision tree algorithms do not systematically consider the impact of different fuzzy levels and simply make uncertainty treatment awareness into the selection of extended properties. To avoiding this problem, this paper establishes a generating Hartley measure model based on cut-standard, subsequently, proposes fuzzy ID3 algorithm based on generating Hartley measure model, finally, the results of the experiments indicates that the model is feasible and effective.	hartley (unit);id3 algorithm	Fachao Li;Dandan Jiang	2011		10.1007/978-3-642-23982-3_24	defuzzification;type-2 fuzzy sets and systems;fuzzy classification;artificial intelligence;fuzzy number;machine learning;fuzzy measure theory;fuzzy set operations;algorithm	NLP	-3.5965800500129577	-20.918893385165976	52969
96ea395f9da923366b8bcc376532032886602f23	improving the discrimination power and weights dispersion in the data envelopment analysis	benchmarking;modelizacion;multicriteria analysis;evaluation performance;analisis envolvimiento datos;entrada salida;performance evaluation;systeme aide decision;evaluacion prestacion;estimation non parametrique;weight distribution;evaluacion comparativa;invarianza;prise de decision;sistema ayuda decision;intelligence economique;input output;invariance;modelisation;non parametric estimation;discrimination power;decision support system;unit invariance;data envelopment analysis;relative efficiency;programmation objectif;eficacia relativa;competitive intelligence;goal programming;analisis multicriterio;decision making unit;estimacion no parametrica;analyse multicritere;inteligencia economica;programacion objetivo;toma decision;data envelope analysis;discriminacion;modeling;efficacite relative;discrimination;entree sortie;analyse enveloppement donnee	Data envelopment analysis (DEA) has been a very popular method for measuring and benchmarking relative efficiency of peer decision making units (DMUs) with multiple input and outputs. Beside of its popularity, DEA has some drawbacks such as unrealistic input-output weights and lack of discrimination among efficient DMUs. In this study, two new models based on a multi-criteria data envelopment analysis (MCDEA) are developed to moderate the homogeneity of weights distribution by using goal programming (GP). These goal programming data envelopment analysis models, GPDEA-CCR and GPDEA-BCC, also improve the discrimination power of DEA.	data envelopment analysis;linear discriminant analysis	Hasan Bal;H. Hasan Örkcü;Salih Çelebioglu	2010	Computers & OR	10.1016/j.cor.2009.03.028	econometrics;decision support system;competitive intelligence;computer science;data envelopment analysis;mathematics;operations research	NLP	-0.42794683762991176	-15.090083125198865	53280
083f88fe154a8bb956cd3e5177c0beb99abbdf41	forecasting the acquisition of university spin-outs: an rbf neural network approach		University spin-outs (USOs), creating businesses from university intellectual property, are a relatively common phenomena. As a knowledge transfer channel, the spin-out business model is attracting extensive attention. In this paper, the impacts of six equities on the acquisition of USOs, including founders, university, banks, business angels, venture capitals, and other equity, are comprehensively analyzed based on theoretical and empirical studies. Firstly, the average distribution of spin-out equity at formation is calculated based on the sample data of 350 UK USOs. According to this distribution, a radial basis function (RBF) neural network (NN) model is employed to forecast the effects of each equity on the acquisition. To improve the classification accuracy, the novel set-membership method is adopted in the training process of the RBF NN. Furthermore, a simulation test is carried out to measure the effects of six equities on the acquisition of USOs. The simulation results show that the increase of university’s equity has a negative effect on the acquisition of USOs, whereas the increase of remaining five equities has positive effects. Finally, three suggestions are provided to promote the development and growth of USOs.	artificial neural network;radial basis function	Weiwei Liu;Zhile Yang;Kexin Bi	2017	Complexity	10.1155/2017/6920904	artificial intelligence;machine learning;empirical research;equity (finance);intellectual property;artificial neural network;mathematics;knowledge transfer;venture capital;business model;spin-½	ML	6.938797851340441	-17.632862871104543	53376
594d2d0bc02147b6130c86544158e7dd4ece221d	svm based multi-index evaluation for bus arrival time prediction	support vector machines;support vector machine multi index evaluation machine learning bus arrival time prediction;transportation support vector machines;indexes global positioning system accuracy support vector machines data models measurement prediction algorithms;transportation;support vector machine bus arrival time prediction multiindex evaluation method svm public transport service bus arrival time real time prediction	The real time prediction of bus arrival time is important in public transport service. Many researches use a traditional error metrics to measure the predicting accuracy, which cannot evaluate the prediction service comprehensively. This paper proposes a novel multi-index evaluation method based on Support Vector Machine (SVM) for bus arrival time prediction. This method uses three new indexes including GPS coverage, release rate, and accuracy rate to evaluate the prediction service, and then uses SVM to train the model for multi-index evaluation. Experiment results show that our method is intuitive and comprehensive by using SVM based multi-index evaluation, and can position issue accurately according to the three new indexes.	experiment;global positioning system;refinement (computing);residual (numerical analysis);support vector machine;time of arrival	Zhiying He;Haitao Yu;Yong Du;Jingjing Wang	2013	2013 International Conference on ICT Convergence (ICTC)	10.1109/ICTC.2013.6675313	real-time computing;engineering;machine learning;data mining	Mobile	9.12287349569831	-13.448684110973515	53466
0667440f001de6191dc5ce45b6300bf8fbd39cee	robustness analysis of artificial neural networks and support vector machine in making prediction	banking;robustness analysis;support vector machines;neural nets;ann model;economic forecasting;accuracy rate measurement;training;islamic bank;training process;rate of return;svm model;graphs;macroeconomic variable;one month time deposit;indonesian islamic bank;accuracy;artificial neural networks;statistical analysis;data pattern;macroeconomics;rate of return robustness analysis artificial neural network support vector machine prediction model ann model svm model macroeconomic variable one month time deposit indonesian islamic bank graph analysis statistical parameter accuracy rate measurement training process data prediction data pattern;graph analysis;predictive models;prediction model;islamic bank artificial neural networks support vector machine rate of return;neurons;support vector machine;predictive models training neurons artificial neural networks support vector machines accuracy data models;statistical parameter;support vector machines banking economic forecasting economic indicators graphs macroeconomics neural nets statistical analysis;artificial neural network;data models;data prediction;economic indicators	This This study aims to investigate the robustness of prediction model by comparing artificial neural networks (ANNs), and support vector machine (SVMs) model. The study employs ten years monthly data of six types of macroeconomic variables as independent variables and the average rate of return of one-month time deposit of Indonesian Islamic banks (RR) as dependent variable. Finally, the performance is evaluated through graph analysis, statistical parameters and accuracy rate measurement. This research found that ANNs outperforms SVMs empirically resulted from the training process and overall data prediction. This is indicating that ANNs model is better in the context of capturing all data pattern and explaining the volatility of RR.	artificial neural network;rapid refresh;support vector machine;volatility	S. Anwar;R. Ismal	2011	2011 IEEE Ninth International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2011.64	support vector machine;computer science;artificial intelligence;machine learning;data mining;predictive modelling;artificial neural network	ML	6.434963931424978	-18.900890032243087	53542
38b8916746615cc951edf06d5e88ed631afdc350	using association rules for product assortment decisions: a case study	market basket analysis;association rules;domain knowledge;frequent itemset;transaction data;association rule;sensitivity analysis;profitability;product assortment decisions	It has been claimed that the discovery of association rules is well-suited for applications of market basket analysis to reveal regularities in the purchase behaviour of customers. Moreover, recent work indicates that the discovery of interesting rules can in fact only be addressed within a microeconomic framework. This study integrates the discovery of frequent itemsets with a (microeconomic) model for product selection (PROFSET). The model enables the integration of both quantitative and qualitative (domain knowledge) criteria. Sales transaction data from a fullyautomated convenience store is used to demonstrate the effectiveness of the model against a heuristic for product selection based on product-specific profitability. We show that with the use of frequent itemsets we are able to identify the cross-sales potential of product items and use this information for better product selection. Furthermore, we demonstrate that the impact of product assortment decisions on overall assortment profitability can easily be evaluated by means of sensitivity analysis.	affinity analysis;association rule learning;heuristic;transaction data	Tom Brijs;Gilbert Swinnen;Koen Vanhoof;Geert Wets	1999		10.1145/312129.312241	association rule learning;computer science;data mining	ML	-2.586532180125123	-11.390096895162818	53553
3aea6fd88e6769461d927328bed429581fda2df5	defining the borda count in a linguistic decision making context	condorcet loser;condorcet winner;fuzzy number;linguistic preferences;borda count;linguistic labels;decision rule;trapezoidal fuzzy numbers	Different kinds of decision rules have been successfully implemented under a linguistic approach. This paper aims the same goal for the Borda count, a well-known procedure with some interesting features. In order to this, two ways of extension from the Borda rule to a linguistic framework are proposed taking into account all the agents’ opinions or only the favorable ones for each alternative when compared with each other. In the two cases, both individual and collective Borda counts are analyzed, asking for properties as good as those of the original patterns.	collective intelligence;rule 184	José Luis García-Lapresta;Miguel Martínez-Panero;Luis Carlos Meneses	2009	Inf. Sci.	10.1016/j.ins.2008.12.021	computer science;artificial intelligence;fuzzy number;data mining;decision rule;mathematics;condorcet method	AI	-3.8265967455338066	-21.61960121239254	53646
c0472fb97a52af2bdc6dfa63445995b980faf197	conjoint axiomatization of min, discrimin and leximin	engineering;fuzzy set;procesamiento informacion;multi criteria decision making;axiomatic;conjunto difuso;ensemble flou;prise decision;ingenierie;axiomatico;information processing;ordinal data;multi criteria;ingenieria;sistema difuso;systeme flou;maxmin;traitement information;axiomatique;toma decision;fuzzy system	In many multi-criteria decision-making applications, the preferential information is of an ordinal nature and appropriate aggregation procedures should be used. In this paper, we build a common axiomatic framework to characterize the “Minimum” procedure as well as two of its re6nements, the “DiscriMin” and the “LexiMin”. In practical situations, this axiomatic framework could help to select the best-suited procedure. c © 2004 Elsevier B.V. All rights reserved.	axiomatic system;constraint satisfaction;game theory;ordinal data	Philippe Fortemps;Marc Pirlot	2004	Fuzzy Sets and Systems	10.1016/j.fss.2004.04.004	information processing;computer science;artificial intelligence;data mining;mathematics;fuzzy set;axiom;ordinal data;algorithm;fuzzy control system	AI	-1.5400895310653526	-20.137637509219275	53726
bf15349fed79965bf825a579910f95365d85eaa0	research of fuzzy comprehensive evaluation of intuitionistic preference relations		On the basis of establishing a evaluating system of competitive capability of industrial estate, the paper establishes function of obtaining fraction of repayment debt capability, operating capability, gaining capability and growth capability. By determining a model of competitive capability of industrial estate it realizes total valuation and rationing analysis of competitive capability of industrial estate. So it will provide scientific foundation for governmental management department to enact macroscopic policy, further to establish finance strategy of sustainable development of industrial estate.		Hui Sun;Jin Yin;Zi-min Yin	2009		10.1007/978-3-642-03664-4_175	artificial intelligence;management science;fuzzy logic;rationing;machine learning;valuation (finance);debt;sustainable development;computer science;estate	NLP	-2.530022882031596	-13.978497280202923	53749
84006606fe93c9dc5176761036a1da0fb809ad82	innovative default prediction approach	default;square;scoring;magic;prediction	A company default prediction approach based on the magic square area.Comparison of commonly used classifiers with the new proposed method.The diagram shape used for a company assessment.Performance of the proposed method is comparable to other widely used approaches. This paper introduces a new scoring method for company default prediction. The method is based on a modified magic square (a spider diagram with four perpendicular axes) which is used to evaluate economic performance of a country. The evaluation is quantified by the area of a polygon, whose vertices are points lying on the axes. The axes represent economic indicators having significant importance for an economic performance evaluation. The proposed method deals with magic square limitations; e.g. an axis zero point not placed in the axes origins, and extends its usage for an arbitrary (higher than 3) number of variables. This approach is applied on corporations to evaluate their economic performance and identify the companies suspected to default. In general, a company score reflects their economic performance; it is calculated as a polygon area. The proposed method is based on the identification of the parameters (axes order, parameters weights and angles between axes) needed to achieve maximum possible model performance. The developed method uses company financial ratios from its financial statements (debt ratio, return on costs etc.) and the information about a company default or bankruptcy as primary input data. The method is based on obtaining a maximum value of the Gini (or Kolmogorov-Smirnov) index that reflects the quality of the ordering of companies according to their score values. Defaulted companies should have a lower score than non-defaulted companies. The number of parameter groups (axes order, parameters weights and angles between axes) can be reduced without a negative impact on the model performance. Historical data is used to set up model parameters for the prediction of possible future companies default. In addition, the methodology allows calculating the threshold value of the score to separate the companies that are suspicious to the default from other companies. A threshold value is also necessary for a model true positive rate and true negative rate calculations. Training and validation processes for the developed model were performed on two independent and disjunct datasets. The performance of the proposed method is comparable to other methods such as logistic regression and neural networks. One of the major advantages of the proposed method is a graphical interpretation of a company score in the form of a diagram enabling a simple illustration of individual factor contribution to the total score value.		Július Bems;Oldrich Starý;Martin Macas;Jan Zegklitz;Petr Posík	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.04.053	econometrics;prediction;artificial intelligence;machine learning;square;data mining;magic;default;statistics	NLP	3.142147504875863	-16.753136086823794	53792
b33bc523b3c94a644817e649bd302dcdd456c2c4	linguistic decision analysis: steps for solving decision problems under linguistic information	modelisation linguistique;agregacion;multicriteria analysis;multi criteria decision making;linguistic modeling;prise decision;linguistic choice functions;aggregation;decision analysis;decision problem;linguistic information;aggregation operator;linguistic aggregation;agregation;information linguistique;analisis multicriterio;analyse multicritere;toma decision;choice function;fonction choix;linguistic modelling;agregation linguistique	A study on the steps to follow in linguistic decision analysis is presented in a context of multi-criteria=multi-person decision making. Three steps are established for solving a multi-criteria decision making problem under linguistic information: (i) the choice of the linguistic term set with its semantic in order to express the linguistic performance values according to all the criteria, (ii) the choice of the aggregation operator of linguistic information in order to aggregate the linguistic performance values, and (iii) the choice of the best alternatives, which is made up by two phases: (a) the aggregation of linguistic information for obtaining a collective linguistic performance value on the alternatives, and (b) the exploitation of the collective linguistic performance value in order to establish a rank ordering among the alternatives for choosing the best alternatives. Finally, an example is shown. c © 2000 Elsevier Science B.V. All rights reserved.	aggregate data;decision analysis;decision problem;emoticon	Francisco Herrera;Enrique Herrera-Viedma	2000	Fuzzy Sets and Systems	10.1016/S0165-0114(99)00024-X	natural language processing;choice function;decision analysis;computer science;artificial intelligence;decision problem;mathematics	AI	-1.3462266618360352	-16.935896166439083	53965
2315d51647796f7767bf18694b79507cba7b6390	algebraic structures of fuzzy t-locality spaces: part 1		The aim of this paper is to introduce the concepts of fuzzy T -locality groups. These constructs mainly will deal and relate with both fuzzy T -locality spaces (1995) and fuzzy TL-uniform spaces (2006). We establish some basic results and characterization theorems of fuzzy T -locality groups. Also, we give the necessary and the sufficient conditions for a group structure to be compatible with a fuzzy T -locality system.	locality of reference	Nehad N. Morsi;Khaled A. Hashem	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-152025	combinatorics;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;algebra	Robotics	-0.6779524526012046	-23.476829187440373	53983
cc060753c00e555af62ed3dd4ba459bafcad7c6f	forecasting model of holiday tourism requirement on the basis of rbf neural network	random cluster;mixed rbf net holiday tourism rbf neural network tourism requirement forecasting model function approximation random cluster linear program;forecasting model;holiday tourism;mixed rbf net;tourism requirement forecasting model;radial basis function networks;forecasting theory;function approximation;rbf neural network;linear program;predictive models neural networks artificial intelligence technology forecasting computer network management technology management economic forecasting computer science engineering management research and development management;travel industry forecasting theory function approximation radial basis function networks;travel industry	"""For improving intellectualized management level of holiday tourism and making a contribution to the healthy, orderly and safely development of holiday tourism, we built the tourism requirement forecasting model on the basis of RBF neural network, realized the classification, analysed and forecasted the tourism status and realized the intellectualized management of holiday tourism. It was used to generate RBF net to perform function approximation. Random cluster and linear program are brought in to design and train this """"mixed"""" RBF net"""	approximation;artificial neural network;linear programming;radial basis function network	Wensheng Guo;Junping Du;Ruijie Wang	2006	Sixth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2006.153	function approximation;computer science;linear programming;artificial intelligence;machine learning;tourism;operations research	Robotics	7.539105231506956	-19.0125935034367	53997
5b7a4870e68aab680ab844961492218a1a37a456	an approach for safety assessment in uas operations applying stochastic fast-time simulation with parameter variation		This paper presents an approach for safety assessment in unmanned aerial system (UAS) operations that uses stochastic fast-time simulation and selected published ground impact fatality/casualty models to calculate fatality risk. The application of simulation allows a sensitivity analysis measuring how different aspects and phases of a UAS operation impact the risk calculations for each of the ground impact models. Specifically, this approach consists of modelling and simulating UAS operations over a defined populated region applying stochastic parameters, such as flight track dispersion, altitude, failure rate, performance variation, and latency due to situational awareness (e.g. BVLOS). Then, published ground impact models are applied to determine the risk in terms of fatalities. This process provides risk metrics in a range, where it is then left to the decision makers as to what constitutes acceptable risk in a given situation.	aerial photography;failure rate;population;risk assessment;riskmetrics;simulation;stochastic modelling (insurance);unmanned aerial vehicle	Joao Luiz de Castro Fortes;Rafael Fraga;Kenny Martin	2016	2016 Winter Simulation Conference (WSC)		stochastic process;simulation;engineering;reliability;mathematics;operations research;measurement;statistics	Embedded	9.987885186524883	-11.177940113819254	54017
5394efcdd115acf4185e7ce62b6de4c6843bd435	an empirical comparison of bayesian and credal networks for dependable high-level information fusion	bayes methods;belief networks;decision making;decision theory;sensor fusion;statistical distributions;bayesian network;credal network;decision making;dependable high-level information fusion;graphical probabilistic method;probability distribution sets;uncertainty reduction;bayesian networks;high-level information fusion;credal networks;dependability;epistemic uncertainty;imprecise probability	Bayesian networks are often proposed as a method for high-level information fusion. However, a Bayesian network relies on strong assumptions about the underlying probabilities. In many cases it is not realistic to require such precise probability assessments. We show that there exists a significant set of problems where credal networks outperform Bayesian networks, thus enabling more dependable decision making for this type of problems. A credal network is a graphical probabilistic method that utilizes sets of probability distributions, e.g., interval probabilities, for representation of belief. Such a representation allows one to properly express epistemic uncertainty, i.e., uncertainty that can be reduced if more information becomes available. Since reducing uncertainty has been proposed as one of the main goals of information fusion, the ability to represent epistemic uncertainty becomes an important aspect in all fusion applications.	bayesian network;convex function;critical section;design of experiments;existential quantification;experiment;graphical user interface;high- and low-level;optimization problem;uncertainty quantification;weight function	Alexander Karlsson;Ronnie Johansson;Sten F. Andler	2008	2008 11th International Conference on Information Fusion		machine learning;pattern recognition;data mining;mathematics;bayesian statistics	AI	2.502917431754306	-18.846307238273475	54178
468c98ea3c389fd3df3b17a582f0d4ed03d8ccbb	turning artificial neural networks into a marketing science tool - modelling and forecasting the impact of sales promotions	h social sciences	In this study we model the effect of promotions in time-series data and we consequently forecast that extraordinary effect via Artificial Neural Networks (ANN) as implemented from the Expert Method of a popular Artificial Intelligence software. We simulate data considering five factors as to determine the actual impact of each individual promotion. We consider additive and multiplicative models, with the later presenting both linear and non-linear relationships between those five factors; in addition, we superimpose either low or high levels of noise. Our empirical findings suggest that, for nonlinear models with high level of noise, ANN outperform all benchmarks. Standard ANN topologies work well for models with up to two factors while the Expert method provided by the software works well for higher number of factors.	artificial intelligence;artificial neural network;benchmark (computing);expert system;high-level programming language;linear model;marketing science;neural networks;nonlinear system;simulation;time series;utility functions on indivisible goods	Ibrahim Zafar Qureshi;Marwan Khammash;Konstantinos Nikolopoulos	2011			computer science;artificial intelligence;machine learning	AI	7.007506474450223	-18.354192184729882	54414
31067f20237aa97cb2cecd3a1c1a1ece21d59fa3	urban traffic congestion prediction using floating car trajectory data	fuzzy comprehensive evaluation;floating car;particle swarm optimization;traffic congestion prediction;traffic flow prediction	Traffic congestion prediction is an important precondition to promote urban sustainable development. Nevertheless, there is a lack of a unified prediction method to address the performance metrics, such as accuracy, instantaneity and stability, systematically. In the paper, we propose a novel approach to predict the urban traffic congestion efficiently with floating car trajectory data. Specially, an innovative traffic flow prediction method utilizing particle swarm optimization algorithm is responsible for calculating the traffic flow parameters. Then, a congestion state fuzzy division module is applied to convert the predicted flow parameters to citizens’ cognitive congestion states. We conduct extensive experiments with real floating car data and the experimental results show that our proposed method has advantage in terms of accuracy, instantaneity and stability.	algorithm;coefficient;experiment;mathematical optimization;network congestion;particle swarm optimization;precondition;protein structure prediction	Qiuyuan Yang;Jinzhong Wang;Ximeng Song;Xiangjie Kong;Zhenzhen Xu;Benshi Zhang	2015		10.1007/978-3-319-27122-4_2	simulation;floating car data;computer science;traffic congestion reconstruction with kerner's three-phase theory;traffic flow;particle swarm optimization;computer security	AI	9.098839372190072	-13.519250595595311	54533
7959d9a659f8536bea5d8211e2874ae08a1ee707	multiple attribute group decision making based on interval-valued intuitionistic fuzzy aggregation operators and transformation techniques of interval-valued intuitionistic fuzzy values	fuzzy numbers;magdm;ivifvs;ivifa operators;transformation techniques	In this paper, we propose a new method for multiple attribute group decision making (MAGDM) based on the proposed interval-valued intuitionistic fuzzy aggregation (IVIFA) operators (including the interval-valued intuitionistic fuzzy weighted averaging (IVIFWA) operator, the interval-valued intuitionistic fuzzy ordered weighted averaging (IVIFOWA) operator and the interval-valued intuitionistic fuzzy hybrid weighted averaging (IVIFHWA) operator) of interval-valued intuitionistic fuzzy values (IVIFVs). First, we propose transformation techniques between IVIFVs and right-angled triangular fuzzy numbers based on the proposed addition operator of IVIFVs. We also prove some properties of the proposed addition operator of IVIFVs. Then, we propose the IVIFWA operator, the IVIFOWA operator and the IVIFHWA operator for aggregating IVIFVs. Finally, we propose a new MAGDM method based on the proposed IVIFA operators. The experimental results show that the proposed MAGDM method can overcome the drawbacks of the existing methods. It provides us with a useful way for MAGDM in interval-valued intuitionistic fuzzy environments.	fuzzy logic;fuzzy set;intuitionistic logic	Shyi-Ming Chen;Shou-Hsiung Cheng;Wei-Hsiang Tsai	2016	Inf. Sci.	10.1016/j.ins.2016.05.041	mathematical analysis;discrete mathematics;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set operations	AI	-2.4323619760117596	-21.153990022184978	54648
e2b2e919101a1d25e9430337acb89f6130b352a7	hesitant interval-valued intuitionistic fuzzy linguistic sets and their applications			fuzzy set	Wei Yang;Jiarong Shi;Xiuyun Zheng;Yongfeng Pang	2016	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169159	discrete mathematics;machine learning;artificial intelligence;fuzzy logic;mathematics	Robotics	-1.4215972871635927	-22.000961664538604	54907
869615e01d26bbebd68f4f8dc19bab20a40f2b3b	a distance-based decision-making method to improve multiple criteria abc inventory classification	common weights;multiple criteria decision making;inventory;distance	The present paper extends the Ng-model for multiple criteria ABC inventory classification based on a distance-based decision-making method. The proposed method determines the common weights associated with all rankings of the criteria importance, and then provides a comprehensive scoring scheme by aggregating all rankings. A numerical illustration is presented to compare our computational results with previous studies.	computation;numerical analysis	Yelin Fu;Kin Keung Lai;Yajun Miao;John W. K. Leung	2016	ITOR	10.1111/itor.12193	inventory;computer science;operations management;data mining;mathematics;management science;distance	AI	-3.1757889852413594	-17.620092644339316	55001
f32d80d732a4937125db75c5108cdd0c967d88db	an improvement of multiplicative consistency of reciprocal preference relations: a framework of granular computing		The commonly used preference elicitation method in decision making is the one using pairwise comparison between alternatives. In this kind of decision making scenario, an essential issue requiring attention is that of consistency, particularly in decision problems with numerous alternatives. Consistency is usually linked to the transitivity concept, which is modeled in several diverse ways. Given the importance of avoiding conflicting opinions in decision making, in this study, we propose an approach to improve the consistency when reciprocal preference relations are used. On one hand, consistency is modeled in terms of the multiplicative transitivity property. On the other hand, information granularity is used to introduce and develop the concept of interval reciprocal preference relations in which the entries are constructed as intervals in place of single numeric values. This provides the necessary flexibility to improve the consistency. To illustrate and test the performance of the approach that is proposed here, an example is given.	consensus (computer science);consistency model;decision problem;eisenstein's criterion;granular computing;mathematical optimization;optimization problem;particle swarm optimization;preference elicitation;preference learning;vertex-transitive graph	Francisco Javier Cabrerizo;Ignacio J. Pérez;Witold Pedrycz;Enrique Herrera-Viedma	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122786	machine learning;artificial intelligence;computer science;granularity;granular computing;decision problem;multiplicative function;preference elicitation;pairwise comparison;reciprocal;transitive relation	Robotics	-3.5276901277488486	-19.9369686074051	55365
4c0735152028e93288fdce0d1d28960f38ab1c6b	solutions with fuzzy-rational generalized lotteries of iii type with a one-dimensional parameter	one dimensional parameter;probability;uncertainty;iii type generalized lottery;expected utility generalized lotteries fuzzy rationality criteria under strict uncertainty;criteria under strict uncertainty;expected utility;price selection problem;biological system modeling;one dimensional prizes;iii type generalized lottery fuzzy rational generalized lotteries one dimensional parameter fuzzy rational decision makers ribbon distribution functions one dimensional prizes three step q expected utility criterion price selection problem;delta modulation;fuzzy set theory;fuzzy sets;fuzzy rational decision makers;upper bound;probability distribution;generalized lotteries;ribbon distribution functions;informatics;fuzzy rational generalized lotteries;fuzzy rationality;economics;distribution functions;three step q expected utility criterion;utility theory uncertainty distribution functions delta modulation fuzzy sets probability distribution;utility theory decision making fuzzy set theory probability;utility theory	Generalized lotteries of III type apply to cases, where both the number of alternatives and the number of consequences are countless. Since real decision makers elicit probabilities in an interval form, they disobey axioms of rationality and are called fuzzy rational decision makers. Generalized lotteries of III type, where the probability is partially described by ribbon distribution functions (constructed using interval probability measures of fuzzy rational decision makers), are called fuzzy rational generalized lotteries of III type. This paper focuses on the model of fuzzy-rational generalized lotteries of III type, defined by a one-dimensional parameter, and each containing one-dimensional prizes. The way to rank such lotteries is formalized in a three-step Q-expected utility criterion. It is implemented in a price selection problem with two Q criteria.	expected utility hypothesis;rationality;selection algorithm	Natalia D. Nikolova;Kiril Tenekedjiev	2010	2010 5th IEEE International Conference Intelligent Systems	10.1109/IS.2010.5548364	mathematical optimization;mathematics;mathematical economics;welfare economics	Robotics	-0.9905378438484039	-18.403590117479787	55387
e0613d40b1ab4dc3342f4d5027e75ee30dce3a75	comment on an algorithm that generates fuzzy prime implicants by lee and chang		This correspondence points out the insufficiency of the algorithm to generate the set of all fuzzy prime implicants of a fuzzy formula as presented in  Lee and Chang (1971) . Two theorems, which are the basis of a new technique that generates the complete set of fuzzy implicants, and used for the minimization of fuzzy functions, as presented in  Kandel (1972) , are discussed.	algorithm	Abraham Kandel	1973	Information and Control	10.1016/S0019-9958(73)90319-7	discrete mathematics;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set operations;algorithm	DB	-0.4636665622583026	-21.154473610264542	55414
d5c0acd1bc8dd9ef7162d264e838dbe91511e50e	aggregation of utility-based individual preferences for group decision-making	multi attribute utility theory;group decision;preference aggregation	Multi-attribu te utility theory (MAUT) elicits an individual decision maker’s preferences for single attrib utes and develops a utility function by mathemat ics formulation to add up the preferences of the entire set of attributes when asses sing alternatives. A common aggregation method of MAUT for group decisions is the simple additive weighting (SAW) method, which does not consider the different preferential levels and preferential ranks for individual decision makers’ assessments of alternatives in a decision group , and thus seems too intuitive in achieving the consensus and commitmen t for group decision aggrega tion. In this paper, the preferential differences denoting the preference degrees among different alternatives and preferential priorities denoting the favorite ran king of the alternatives for each decision maker are both considered and aggregated to construct the utili ty discriminative values for assessing alternatives in a decision group. A comparative analysis is performed to comp are the proposed app roach to the SAW model, and a satisfaction index is used to investigate the satisfaction levels of the final two resultin g group decisio ns. In addition, a feedback interview is conducted to understand the subjective perceptions of decision makers while examining the results obtained from these two approaches for the second practical case. Both investigation results show that the proposed approach is able to achieve a more satisfying and agreeable group decision than that of the SAW method. 2013 Elsevier B.V. All rights reserved.	qualitative comparative analysis;social network aggregation;utility functions on indivisible goods	Yeu-Shiang Huang;Wei-Chen Chang;Wei-Hao Li;Zu-Liang Lin	2013	European Journal of Operational Research	10.1016/j.ejor.2013.02.043	aggregation problem;optimal decision;economics;decision analysis;decision field theory;decision tree;management science;welfare economics;weighted sum model	Web+IR	-3.390967819346068	-18.211141558676932	55603
c74ed1d54d214898ce80715948dfab2b80a017b1	determining owa operator weights from ordinal relation on criteria	attitudinal character c owa operator weight centered owa convex hull decision making aggregate multiple object ordinal relation;ordinal relation;uncertainty;computability;decision maker;artificial neural networks;multiple objectives;decision making computability decidability;open wireless architecture;ordered weighted average;attitudinal character ordered weighted averaging owa operator weights ordinal relation;extreme point;attitudinal character;approximation methods;ordered weighted averaging owa operator weights;convex hull;owa operator;maximum entropy;decidability;artificial neural networks open wireless architecture uncertainty approximation methods	The centered OWA (C-OWA) operator weights, defined as the center point of a convex hull constructed by the extreme points of ordinal relation, are computed by coordinate-wise averaging of the extreme points. The resulting C-OWA operator weights display some interesting properties. First there exists one to one correspondence between the multiplier used in the ordinal relation and the attitudinal character, thus we can confer a semantic to the attitudinal character in a meaningful way. Second, the C-OWA operator weights show a high level of coincidence with the maximum entropy OWA operator weights, which is widely used in OWA aggregation- another way of generating the OWA operator weights. This approach may help the decision-maker to articulate his or her preferences in lieu of direct assessment of attitudinal character. Indirectly assessed preference information will be used to generate the C-OWA operator weights, which are then used to aggregate multiple objects and finally prioritize multitude of alternatives.	aggregate data;convex hull;high-level programming language;ordinal data	Byeong Seok Ahn	2010	2010 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2010.5642320	decidability;extreme point;decision-making;mathematical optimization;combinatorics;discrete mathematics;uncertainty;computer science;principle of maximum entropy;convex hull;mathematics;computability;artificial neural network;statistics	DB	-2.293560314454367	-21.91894575889239	56308
e55b22b9cbb6bed5e72a2f7c3df1c709d14ba43d	a personalized load forecasting enhanced by activity information	random variables;load forecasting;function approximation;mutual information;entropy;correlation	In this paper, we propose an activity-enhanced load forecasting model at house-level. We focus on the impact of residents' daily activities on entire household's power consumption. The contribution of this paper is 3-fold: 1) a web-based system for collecting daily activity information in diary-style; 2) a correlation analysis between activities and power consumption and their information-theoretic relationship; 3) a personalized load forecasting study using different prediction algorithms and an activity recognition procedure as an enhancement. Both correlation and forecasting results show consistently that our collected activity information can contribute to estimate and predict the power consumption of individual households to varying degrees, in particular for 15 minutes ahead load forecasting. An extended forecasting model with an online activity recognition component can further reduce the forecasting error.	activity recognition;algorithm;information theory;mutual information;personalization;preprocessor;web application	Yong Ding;Martin Alexander Neumann;Erwin Stamm;Michael Beigl;Sozo Inoue;Xincheng Pan	2015	2015 IEEE First International Smart Cities Conference (ISC2)	10.1109/ISC2.2015.7366172	probabilistic forecasting;econometrics;engineering;machine learning;data mining	Robotics	8.718371661018711	-17.3575663268626	56480
86695dc05cd42d95ff6c1ea9073061bf7b528f3a	subjective evaluation of discomfort in sitting positions	fuzzy measure;multicriteria decision making;choquet integral;subjective evaluation	We study the modelling of the subjective sensation of discomfort for subjects seated during a long time, in terms of local discomforts. The methodology uses fuzzy measures and integrals in a multicriteria decision making process, which enables the modelling of complex interaction between variables. Results of the experiment are detailed, giving models with respect to different kinds of discomfort, and to different macro-zones of the body.	fuzzy measure theory	Michel Grabisch;Jacques Duchêne;Frédéric Lino;Patrice Perny	2002	FO & DM	10.1023/A:1019640913523	artificial intelligence;mathematics;choquet integral	HCI	-4.438806899556202	-22.370257530005684	56531
38d4eba52eca5937d2f3dcff9dac3dd4bb7f84ec	which garch model for option valuation?	forecasting;garch;model specification;garch model;volatility clustering;out of sample;option pricing;time series;risk neutral pricing;option valuation;objective function;parsimony;leverage effect;probability measure	Characterizing asset return dynamics using volatility models is an important part of empirical finance. The existing literature on GARCH models favors some rather complex volatility specifications whose relative performance is usually assessed through their likelihood based on a time-series of asset returns. This paper compares a range of GARCH models along a different dimension, using option prices and returns under the risk-neutral as well as the physical probability measure. We judge the relative performance of various models by evaluating an objective function based on option prices. In contrast with returns-based inference, we find that our option-based objective function favors a relatively parsimonious model. Specifically, when evaluated out-of-sample, our analysis favors a model that besides volatility clustering only allows for a standard leverage effect. JEL Classification: G12	cluster analysis;loss function;occam's razor;optimization problem;stochastic process;time series;value (ethics);volatility	Peter F. Christoffersen;Kris Jacobs	2004	Management Science	10.1287/mnsc.1040.0276	financial economics;autoregressive conditional heteroskedasticity;implied volatility;actuarial science;economics;volatility smile;valuation of options;finance;mathematics;financial models with long-tailed distributions and volatility clustering;statistics	ML	3.1526831565939046	-11.079029715039864	56562
afb2dd37637d40c952529af91d035c4d4a1b2bb9	neural network forecasting for seasonal and trend time series	forecasting;time series forecasting;empirical study;neural networks;time series;moving average;seasonality;box jenkins method;neural network model;data preprocessing;neural network	Neural networks have been widely used as a promising method for time series forecasting. However, limited empirical studies on seasonal time series forecasting with neural networks yield mixed results. While some find that neural networks are able to model seasonality directly and prior deseasonalization is not necessary, others conclude just the opposite. In this paper, we investigate the issue of how to effectively model time series with both seasonal and trend patterns. In particular, we study the effectiveness of data preprocessing, including deseasonalization and detrending, on neural network modeling and forecasting performance. Both simulation and real data are examined and results are compared to those obtained from the Box–Jenkins seasonal autoregressive integrated moving average models. We find that neural networks are not able to capture seasonal or trend variations effectively with the unpreprocessed raw data and either detrending or deseasonalization can dramatically reduce forecasting errors. Moreover, a combined detrending and deseasonalization is found to be the most effective data preprocessing approach.	artificial neural network;time series	Guoqiang Peter Zhang;Min Qi	2005	European Journal of Operational Research	10.1016/j.ejor.2003.08.037	econometrics;computer science;machine learning;time series;data mining;mathematics;artificial neural network;statistics	ML	7.407840574819655	-19.961311827053635	56624
13e5c465211f0a1ffa2d8c1623218ddecb6e37eb	roughness in non-associative po-semihypergroups based on pseudohyperorder relations				Jianming Zhan;Naveed Yaqoob;Madad Khan	2017	Multiple-Valued Logic and Soft Computing		artificial intelligence;machine learning;associative property;computer science;surface finish	AI	1.985216180968386	-23.66348996714309	56662
862b78ce5b1b1f41572f195bb62fecd41e594d1b	a new model for intuitionistic fuzzy multi-attributes decision making	intuitionistic fuzzy sets;decision analysis;weighted arithmetic average;admissible order;weighted geometric average	In this study, we discuss linear orders of intuitionistic fuzzy values (IFVs). Then we introduce an intuitionistic fuzzy weighted arithmetic average operator. Some fundamental properties of this operator are investigated. Based on the introduced operator, we propose a new model for intuitionistic fuzzy multi-attributes decision making. The proposed model deals with the degree of membership and degree of nonmembership separately. It is resistant to extreme data.		Yao Ouyang;Witold Pedrycz	2016	European Journal of Operational Research	10.1016/j.ejor.2015.08.043	combinatorics;discrete mathematics;economics;decision analysis;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;mathematics;fuzzy set operations	Theory	-1.6492341903426748	-21.069454525680985	56735
b4aead86814d46a71d03710022c946958a12e29f	predicting gross movie revenue		'There is no terror in the bang, only is the anticipation of it' - Alfred Hitchcock. Yet there is everything in correctly anticipating the bang a movie would make in the box-office. Movies make a high profile, billion dollar industry and prediction of movie revenue can be very lucrative. Predicted revenues can be used for planning both the production and distribution stages. For example, projected gross revenue can be used to plan the remuneration of the actors and crew members as well as other parts of the budget [1]. Success or failure of a movie can depend on many factors: star-power, release date, budget, MPAA (Motion Picture Association of America) rating, plot and the highly unpredictable human reactions. The enormity of the number of exogenous variables makes manual revenue prediction process extremely difficult. However, in the era of computer and data sciences, volumes of data can be efficiently processed and modelled. Hence the tough job of predicting gross revenue of a movie can be simplified with the help of modern computing power and the historical data available as movie databases [2].	alfred spector;bang file;database;software release life cycle;thinking outside the box	Sharmistha Dey	2018	CoRR		data mining;crew;anticipation;liberian dollar;computer science;revenue;operations management;remuneration	ML	1.7721867387437857	-13.521914466985903	56785
59cfc425b2a7d44a993c70ebd2e5127aa818711d	research on the impact of outward foreign direct investment on trade competitiveness	analytical models;empirical study;investments;outward foreign direct investment;outward foreign direct investment ofdi;structure optimization;vector autoregression model outward foreign direct investment trade competitiveness international trade var model false regression ols econometric model means structural optimization;foreign direct investment;trade competitiveness;false regression;econometric model;biological system modeling investments mathematical model companies economics analytical models optimization;foreign exchange trading;biological system modeling;ols econometric model;companies;investment;regression analysis autoregressive processes foreign exchange trading international trade investment;autoregressive processes;var model outward foreign direct investment ofdi trade competitiveness composition of export and imports;mathematical model;composition of export and imports;regression analysis;optimization;economics;var model;means structural optimization;vector autoregression model;international trade	The relationship between foreign direct investment (FDI) and international trade has received considerable attention in the past few decades. The article analyzes the impact of outward foreign direct investment on trade competitiveness based on foreign direct investment motives by Dunning, and makes an empirical study using VAR model, to avoid false regression of OLS econometric model since the value of OFDI, import and export data increases gradually. The result shows that China's foreign direct investment has a positive role on trade competitiveness, which means structural optimization of import and export.	econometric model;fault detection and isolation;foreign function interface;foreign key;mathematical optimization;ordinary least squares;shape optimization;vector autoregression	Yan Zhang;Guo-Zhen Wang	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580980	investment;foreign direct investment	Robotics	2.7399335020010347	-12.966662938913489	56845
a45b5f130084aa29cdfd4eb20383cb53f18432c1	applying a combined fuzzy systems and garch model to adaptively forecast stock market volatility	recursive least square;time varying;garch model;stock market;estimation method;recursive least squares;convergence rate;iterative algorithm;volatility forecasting;stock market forecast;adaptive method;membership function;genetic algorithm;global optimization;stock market volatility;autoregressive conditional heteroskedasticity;fuzzy systems;fuzzy system	This paper studies volatility forecasting in the financial stock market. In general, stock market volatility is time-varying and exhibits clustering properties. Thus, this paper presents the results of using a fuzzy system method to analyze clustering in generalized autoregressive conditional heteroskedasticity (GARCH) models. It also uses the adaptive method of recursive least-squares (RLS) to forecast stock market volatility. The fuzzy GARCH model represents a joint estimation method; the membership function parameters together with the GARCH model parameters make this problem of stock market is highly nonlinear and complicated. This study presents an iterative algorithm based on a genetic algorithm (GA) to estimate the parameters of the membership functions and the GARCH models. In this paper, the GA method is employed to identify a global optimal solution with a fast convergence rate in the context of the fuzzy GARCH model estimation problem studied here. Based on simulation results, we determined that both the estimation of in-sample and the forecasting of out-of-sample volatility performance are significantly improved when the GARCH model considers both the clustering effect and the adaptive forecast. © 2011 Elsevier B.V. All rights reserved.	autoregressive model;cluster analysis;computational phylogenetics;conditional entropy;fuzzy control system;genetic algorithm;iterative method;membership function (mathematics);nonlinear system;numerical weather prediction;rate of convergence;recursion;recursive least squares filter;simulation;software release life cycle;volatility	Jui-Chung Hung	2011	Appl. Soft Comput.	10.1016/j.asoc.2011.02.020	autoregressive conditional heteroskedasticity;econometrics;actuarial science;computer science;artificial intelligence;financial models with long-tailed distributions and volatility clustering;fuzzy control system	AI	7.7762534851035685	-19.98103597567847	56971
c346586904ddb2357bb2f80aa47f7d015c08065d	improving decision making approaches based on fuzzy soft sets and rough soft sets		Abstract Hybrid soft sets, such as fuzzy soft sets and rough soft sets, have been extensively applied to decision making. However in both cases, there is still a necessity of providing improvements on approaches to obtain better decision results in different situations. In this paper several proposals for decision making are provided based on both hybrid soft sets. For fuzzy soft sets, a computational tool called D-score table is introduced to improve the decision process of a classical approach and its convenience has been proved when attributes change across the decision process. In addition, a novel adjustable approach based on decision rules is introduced. Regarding rough soft sets, several new decision algorithms to meet different decision makers’ requirements are introduced together a multi-criteria group decision making approach. Several practical examples are developed to show the validity of such proposals.		Yaya Liu;Keyun Qin;Luis Martínez-López	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.01.012	fuzzy logic;machine learning;artificial intelligence;decision rule;mathematics;group decision-making	Robotics	-3.4143940165652333	-21.07478073525704	57043
5dd45cefb94fde16cf0312cd1004c4af633bf950	study on interval intuitionistic fuzzy multi-attribute group decision making method based on choquet integral	会议论文	In this paper, a method based on Choquet integral is proposed to solve the interval intuitionistic fuzzy multiple attribute group decision making problems. Firstly, some concepts about interval intuitionistic fuzzy measure are defined, through the strict mathematical reasoning to prove the measure we proposed satisfying the axiomatic system of fuzzy measure. Then, on the basis of fuzzy measure and game theory, we propose two models to determine fuzzy measure based on interval intuitionistic fuzzy entropy and weight information matrix. By calculating the Shapely value to determine expert weights, we establish a linear programming model based on relative entropy to determine the fuzzy measure of attribute weights to reflect the interactive characteristics among the criteria, using the Choquet integral to aggregate the decision-making information. Finally, we give the process of decision making in details.	aggregate data;axiomatic system;fuzzy measure theory;game theory;intuitionistic logic;kullback–leibler divergence;linear programming;programming model	Jindong Qin;Xinwang Liu	2013		10.1016/j.procs.2013.05.060	mathematical optimization;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy associative matrix;fuzzy set operations	AI	-2.2821533075578	-20.986828861877623	57224
2430c6c576569fa48d7658f31e00c17c8d53400f	operations and aggregation methods of single-valued linguistic neutrosophic interval linguistic numbers and their decision making method		To comprehensively describe uncertain/interval linguistic arguments and confident linguistic arguments in the decision making process by a linguistic form, this study first presents the concept of a single-valued linguistic neutrosophic interval linguistic number (SVLN-ILN), which is comprehensively composed of its uncertain/interval linguistic number (determinate linguistic argument part) and its single-valued linguistic neutrosophic number (confident linguistic argument part), and its basic operations. Then, the score function of SVLN-ILN based on the attitude index and confident degree/level is presented for ranking SVLN-ILNs. After that, SVLN-ILN weighted arithmetic averaging (SVLN-ILNWAA) and SVLN-ILN weighted geometric averaging (SVLN-ILNWGA) operators are proposed to aggregate SVLN-ILN information and their properties are investigated. Further, a multi-attribute decision-making (MADM) method based on the proposed SVLN-ILNWAA or SVLN-ILNWGA operator and the score function is established under consideration of decision makers’ preference attitudes (pessimist, moderate, and optimist). Lastly, an actual example is given to show the applicability of the established MADM approach with decision makers’ attitudes.		Jun Ye;Wenhua Cui	2018	Information	10.3390/info9080196	machine learning;operator (computer programming);artificial intelligence;computer science;linguistics;decision-making;ranking;score	NLP	-3.0495448837132098	-20.95524573276561	57235
c9162dcb038654e0cfd8956e7404f9bacc6536fd	nash equilibrium strategy for non-cooperative games with interval type-2 fuzzy payoffs		This paper is the first to attempt the characterization of Nash equilibrium strategy in interval type-2 fuzzy environment. The payoffs of the matrix games are considered to be interval type-2 triangular fuzzy numbers. An ordering for interval type-2 fuzzy numbers is introduced and a model to investigate equilibrium strategies in this environment is proposed. Due to the uncertainty in payoffs, three types of equilibriums are given those are natural extensions of the Nash equilibrium from crisp games. The existence of these equilibrium strategies is studied and a method to obtain the equilibrium strategy by means of parametric bimatrix crisp game is also given. Finally, solutions to a few numerical examples of matrix games are explored using this new interval-valued fuzzy game framework.	fuzzy logic;fuzzy set;game engine;nash equilibrium;numerical analysis;parametric polymorphism;the matrix	Ernesto Cuartas Morales;Debdas Ghosh	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491573	fuzzy logic;non-cooperative game;machine learning;mathematical optimization;artificial intelligence;fuzzy set;fuzzy game;computer science;linear programming;matrix (mathematics);nash equilibrium;fuzzy number	Robotics	-1.269256719010728	-18.67203189711611	57281
8bf657fc38ee0e42f2fee6def843af2a5c9107ff	stock market forecasting based on wavelet and least squares support vector machine	least squares support vector machine;machine learning;noisy signal;stock price prediction;wavelet transforms	In this paper, we propose a novel method using wavelet transform to denoise the input of least squares support vector machine for classification of closing price of stocks. The proposed method classifies closing price as either down or up. We have tested the proposed approach using passed three-year data of 10 stocks randomly selected from sample stock of hs300 index and compared the proposed method with other machine learning methods. Good classification percentage of almost 99% was achieved by WT-SVM model. We observed that the performance of stock price prediction can be significantly enhanced by using hybrized WT in comparison with a single model.	artificial neural network;attribute–value pair;closing (morphology);correctness (computer science);data mining;feature vector;knowledge management;least squares support vector machine;machine learning;network model;noise reduction;radial basis function;randomness;sentiment analysis;wavelet transform	Xia Liang;Haoran Zhu;Xun Liang	2011			least squares support vector machine;machine learning;pattern recognition	ML	6.747487544538092	-20.115471239026515	57291
119ecb1144a312638d60bd08d89c1d185831ebaa	metric structures on some mtl-algebras and its applications	mv algebra;measurement;lattices;pseudo metric;mtl algebra;standardly separable;semantics;inference mechanisms;approximate reasoning models;sizes;fuzzy logic;approximate reasoning;cost accounting;algebra;propositional logic;ωm;cognition;metric structures;formal logic;valuation;r 0 algebra;similarity degrees;g algebra;probability measure;mtl algebras theories;product algebra	This paper focus on establishing in a unified way metric structures on some MTL-algebras. Let M be one of the R<inf>0</inf>-algebra, MV -algebra, product algebra and G-algebra, ΩM be the set of all homomorphisms from M into the real unit interval [0; 1], and µ be a probability measure on ΩM. It is proved that these MTL-algebras (R<inf>0</inf>-algebra, MV -algebra, product algebra and G-algebra) are standardly separable, it means that a ≤ b iff v(a) ≤ v(b) for any v ∈ Ω<inf>M</inf>. The concepts of sizes of elements of M and similarity degrees of pairs of elements of M w.r.t. µ are introduced, and then a pseudo-metric on M is defined therefrom. As an applications, using metric MTL-algebras theories some more flexible approximate reasoning models of propositional logic could be established.	approximation algorithm;linear separability;propositional calculus;theory	Jialu Zhang	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569690	composition algebra;fuzzy logic;symmetric algebra;filtered algebra;combinatorics;discrete mathematics;cognition;probability measure;valuation;term algebra;tensor algebra;lattice;quaternion algebra;mathematics;semantics;algebra representation;propositional calculus;logic;two-element boolean algebra;measurement;statistics;cost accounting;algebra	DB	-1.273444130573934	-23.24953649545633	57396
dc7120c0a0e69ec8c0fb679644e7215c8264ebe2	best concept selection in design process: an application of generalized intuitionistic fuzzy soft sets				Khizar Hayat;Muhammad Irfan Ali;José Carlos Rodriguez Alcantud;Bing-yuan Cao;Kalim U. Tariq	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-172121	discrete mathematics;design process;mathematics;artificial intelligence;machine learning;fuzzy logic	Robotics	-1.442273762055699	-21.42260939014968	57461
90ffb7f9d3ad523e993fd2a24c12f1a4ed837798	fuzzy gmdh-type neural network model and its application to forecasting of mobile communication	computer program;tecnologia industrial tecnologia mecanica;adaptive network;computacion informatica;gmdh;mobile communication system;ciencias basicas y experimentales;number of factors;neuro fuzzy;adaptive learning;mobile communication;mathematical model;empirical model;fuzzy gmdh;data handling;neural network model;tecnologias;nonlinear system;grupo a;decision rule;neural network;demand forecasting	In this paper, the fuzzy group method data handling-type (GMDH) neural networks and their application to the forecasting of mobile communication systems are described. At present, the GMDH family of modeling algorithms discovers the structure of empirical models and it gives only the way to get the most accurate identification and demand forecasts in case of noised and short input sampling. In distinction to neural networks, the results are explicit mathematical models, obtained in a relative short time. In this paper, an adaptive learning network is proposed as a kind of neural-fuzzy GMDH. The proposed method can be reinterpreted as a multi-stage fuzzy decision rule which is called the neural-fuzzy GMDH. The GMDH-type neural networks have several advantages compared with conventional multi-layered GMDH models. Therefore, many types of nonlinear systems can be automatically modeled by using the neuro-fuzzy GMDH. A computer program is developed and successful applications are shown in the field of estimating problems of mobile communication with a number of factors considered. 2006 Elsevier Ltd. All rights reserved.	algorithm;artificial neural network;computer program;group method of data handling;mathematical model;mobile phone;network model;neuro-fuzzy;nonlinear system;sampling (signal processing)	Heung Suk Hwang	2006	Computers & Industrial Engineering	10.1016/j.cie.2005.08.005	simulation;mobile telephony;computer science;artificial intelligence;neuro-fuzzy;machine learning;group method of data handling;mathematical model;decision rule;adaptive learning;artificial neural network	AI	8.73466968645881	-20.88425440852572	57524
4940b2708f27555e53dd1d92487427ed49657ae8	interval articulation of superiority and precise elicitation of priorities	elicitation;multicriteria analysis;multiobjective programming;programmation multiobjectif;prestation service;maximum degree;analytic hierarchy process;point estimation;analytic hierarchy process ahp;criteria importance;processus hierarchie analytique;lower and upper bound;systeme aide decision;multiple criteria decision making;prestacion servicio;priorite;sistema ayuda decision;prise decision;contrato;contracting service;fonction objectif;objective function;multiple objectives;decision support system;programacion lineal;contract;multiple decision;borne inferieure;proceso jerarquia analitico;linear programming;preferencia;programmation lineaire;linear program;decision multiple;funcion objetivo;contraccion;elicitacion;preference;analisis multicriterio;contrat;analyse multicritere;toma decision;priority;prioridad;minimum degree;lower bound;cota inferior;programacion multiobjetivo;contraction	This paper develops an approach to obtaining precise (point) estimates of priorities of objects, in particular, coefficients of importance of criteria. It is assumed that the judgements regarding the preference superiority of one object over another are given in the forms of intervals, that is by indicating the lower and upper bounds. The suggested approach is suitable both in the case of consistent and inconsistent preferences. In the former case, the available intervals are processed by contracting them to the maximum degree, in the latter, by extending them to the minimum degree. The degrees of contraction or extension are regarded as the indexes of indeterminacy or inconsistency of preferences, and treated as multiple objective functions. A method of transformation of such a multicriterial problem to a sequence of several linear programming problems is suggested.	biconnected component	Vladislav V. Podinovski	2007	European Journal of Operational Research	10.1016/j.ejor.2006.01.046	contract;mathematical optimization;analytic hierarchy process;linear programming;artificial intelligence;point estimation;contraction;mathematics;algorithm	Robotics	-0.9259166128184039	-16.185038560134643	57581
fc22f4198a53e86925ca22fd459dbb12cda4a8af	measuring the value of information: the case of data on the cost of producing cotton	quality of life;benefit cost ratio;value of information;living standards;similarity measure;economic growth	Operational measurement methods may be developed to measure the value of information in the data reported by the U.S. Government. Illustrative measures for cost of cotton production statistics indicate that the benefits from these data in certain important uses may far exceed their costs. If similar measures could be provided for major Government data programmes, it would facilitate the development of a national data policy that is oriented toward decision making and improvements in economic growth, national well-being, and quality of life. Making these estimates would begin to provide the dollar values in important uses of information in the nation's data bases. These values are needed for allocating resources to maintain, refine, and develop fundamental data series. Cutting data expenditure in the absence of these value measures may be false economy indeed; because, reducing data series with very high benefit/cost ratios might well limit, if not reduce, living standards for many generations to come.		Russell G. Thompson;L. R. Lamotte;H. Oxspring;B. Davis;O. P. Blaich	1984	Annals OR	10.1007/BF01874746	standard of living;quality of life;actuarial science;data quality;economics;public economics;value of information;welfare economics	DB	-0.19529816490398635	-11.491902884448109	57647
f63b88109e89e46cd9303ff77248ee4200226f4c	expected value of intuitionistic fuzzy number and its application to solve multi-objective multi-item solid transportation problem for damageable items in intuitionistic fuzzy environment			fuzzy number;intuitionistic logic;transportation theory (mathematics)	Dipankar Chakraborty;Dipak Kumar Jana;Tapan Kumar Roy	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-151833	mathematical optimization;discrete mathematics;fuzzy transportation;fuzzy number;mathematics	Robotics	-0.6270175983998697	-17.944921337202533	57751
944209811792916d26ab2e0e0521c35f1e8ac344	multi-objective optimization in radiotherapy: applications to stereotactic radiosurgery and prostate brachytherapy	treatment planning;operating room;distance measure;human development;multi objective optimization;clinical decision making;decision analysis;stereotactic radiosurgery;time series analysis;error correction;machine intelligence;decision theory;goal setting;thermodynamics;prostate brachytherapy;genetic algorithm;optimization;radiation therapy;expert knowledge;real time application	Treatment planning for radiation therapy is a multi-objective optimization process. Here we present a machine intelligent scheme for treatment planning based on multi-objective decision analysis (MODA) and genetic algorithm (GA) optimization. Multi-objective ranking strategies are represented in the L(p) metric under the displaced ideal model. Goal setting, protocol satisficing and fuzzy ranking of objective importance can be incorporated into the decision scheme to assimilate clinical decision making. For distance measures in the L(p) metric, a dynamic gauge function is defined based on the state energy of the decision system, which is assumed to undergo thermodynamic cooling with iteration time. The MODA scheme interacts with a robust GA engine, which adaptively evolves in the multi-modal landscape that defines the treatment plan quality. A conventionally challenging case of stereotactic radiosurgery of a brain lesion was selected for GA optimization. The resulting dose distributions are compared to human-developed plans, which are commonly regarded as clinically relevant and empirically optimal. The GA-optimized plans achieve substantially better sparing of critical normal neuroanatomy surrounding the brain lesion while respecting the preset constraints on tumor dose uniformity. In addition, machine optimization tends to produce novel treatment strategies which complements expert knowledge. The run time for producing an optimal plan is considerably shorter than the typical planning time for human experts, thus GA can also be used to aid the human treatment planning process. In prostate brachytherapy, MODA-GA was specifically applied to non-ideal conditions in which typical surgical uncertainties in seed implant positioning occur, where noisy objectives were introduced into the optimization scheme. The noisy system is found to be manageable by MODA-GA at uncertainty levels corresponding to reasonably proficient surgery teams. In contrast, noisy objectives would be very difficult to explore by human expert planners. Potential use of noisy optimization with time series analysis is being explored for error-corrective computer guidance in the operating room for prostate seed implantation. In conclusion, the combination of MODA and GA optimization offers both a solution to practical treatment planning tasks and the potential for real time applications in radiotherapy.		Yan Yu;J. B. Zhang;Gang Cheng;M. C. Schell;Paul Okunieff	2000	Artificial intelligence in medicine	10.1016/S0933-3657(99)00049-4	human development;radiation therapy;simulation;error detection and correction;genetic algorithm;decision theory;decision analysis;computer science;artificial intelligence;multi-objective optimization;time series;statistics	AI	6.434730485535677	-10.781649043465142	57776
d91f52dbc37de08bf5de725e4231d40d81c9d00c	choix multicritères dans le risque et variables multidimensionnelles: proposition de méthode et application aux réseaux de transport d'énergie	multicriteria analysis;medio ambiente;energy transport;multicriteria decision;risque;energy transportation;multi attributed utility theory;riesgo;risk;theorie utilite;environment;teoria utilidad;decision;environnement;transporte energia;analisis multicriterio;analyse multicritere;transport energie;theorie utilite multiattribut;utility theory	This paper présents an application of Multiple Attribute Utility Theory on strategie choices concerning energy transportation. The environmental assessment ofa network reinforcement strategy is emphasized. Our assessment brings about to consider multidimensional variables in MCDM. However, Multi-Attributed Utility Theory (MAUT) cannot, as a practical matter, manage such variables. We therefore work out a methodology to transform multidimensional variables into unidimensional ones. We apply it then to a pratical case. From the application, we draw some conclusions on Multi-Attributed Utility Theory and on its interest for strategie choices dealing with environmental conséquences.		Bertrand Munier;Nathalie Taverdet-Popiolek	1999	RAIRO - Operations Research	10.1051/ro:1999123	risk;natural environment;utility	AI	-1.2540730812183185	-15.130252085435359	57859
f1c1b64c133f561806eb580b02264712f7f32c37	a multiplicative masking method for preserving the skewness of the original micro-records	ciencias basicas y experimentales;matematicas;grupo a;grupo b	Masking methods for the safe dissemination of microdata consist of distorting the original data while preserving a pre-defined set of statistical properties in the microdata. For continuous variables, available methodologies rely essentially on matrix masking and in particular on adding noise to the original values, using more or less refined procedures depending on the extent of information that one seeks to preserve. Almost all of these methods make use of the critical assumption that the original datasets follow a normal distribution and/or that the noise has such a distribution. This assumption is, however, restrictive in the sense that few variables follow empirically a Gaussian pattern: the distribution of household income, for example, is positively skewed, and this skewness is essential information that has to be considered and preserved. This paper addresses these issues by presenting a simple multiplicative masking method that preserves skewness of the original data while offering a sufficient level of disclosure risk control. Numerical examples are provided, leading to the suggestion that this method could be well-suited for the dissemination of a broad range of microdata, including those based on administrative and business records.	business record;distortion;microdata (html);unsharp masking	Nicolás Ruiz	2017	CoRR		econometrics;speech recognition;statistics	ML	-1.7192800598373315	-13.401493875267743	58174
0ddcfa1b983e26cf0ca30c3e3f6131c63eb25545	a euclidean approach for ranking intuitionistic fuzzy values	cognitive bias intuitionistic fuzzy value atanassov intuitionistic fuzzy set euclidean distance minkowski distance weak order;fuzzy sets euclidean distance uncertainty measurement uncertainty hamming distance indexes decision making	In the literature on Atanassov intuitionistic fuzzy sets, several methods have been proposed in order to obtain a ranking on intuitionistic fuzzy values. However, some problems may arise when working with these methods, such as the inadmissibility problem, the nonrobustness problem, the indifference problem, etc. Based on the concept of the Euclidean distance, we propose a novel approach for ranking intuitionistic fuzzy values, which addresses these problems. With the aid of its geometrical representation, we rank the intuitionistic fuzzy values in accordance with the following basic principle: The closer the intuitionistic fuzzy value is to the most favorable intuitionistic fuzzy value, the higher the ranking of the intuitionistic fuzzy value is. Moreover, we extend this approach by taking into account human cognitive bias, which reflects a decision maker's attitude toward positive or negative consequences in decision problems involving uncertainty. Finally, we generalize our approach by introducing the Minkowski distance, and show that the generalized approach also addresses the problems encountered by the existing methods.	decision problem;euclidean distance;fuzzy set;intuitionistic logic	Zhaojun Xing;Wei Xiong;Hailin Liu	2018	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2017.2666219	mathematical analysis;discrete mathematics;topology;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy number;euclidean distance;mathematics;fuzzy set operations	SE	-2.606293426197204	-21.19955858566984	58470
b0b77ddedd2f176add2b7016dd0230c076721d8f	navigating interpretability issues in evolving fuzzy systems	evolving fuzzy systems;on line assurance;interpretability criteria	In this position paper, we are investigating interpretability issues in the context of evolving fuzzy systems (EFS). Current EFS approaches, developed during the last years, are basically providing methodologies for precise modeling tasks, i.e. relations and system dependencies implicitly contained in on-line data streams are modeled as accurately as possible. This is achieved by permanent dynamic updates and evolution of structural components. Little attention has been paid to the interpretable power of these evolved systems, which, however, originally was one fundamental strength of fuzzy models over other (data-driven) model architectures. This paper will present the (little) achievements already made in this direction, discuss new concepts and point out open issues for future research. Various well-known and important interpretability criteria will serve as basis for our investigations.	fuzzy control system	Edwin Lughofer	2012		10.1007/978-3-642-33362-0_11	computer science;artificial intelligence;data mining;algorithm	Logic	6.351557377011696	-23.920663399918734	58557
fff8b772d1ca22bdcec106a0d8859afa5305c7ae	laws for conjunctions and disjunctions in interval type 2 fuzzy sets	absorption;lattices;distributiveness;investigacion operativa;construction industry;fuzzy set theory;fuzzy sets;fuzzy sets absorption construction industry conferences lattices fuzzy systems fuzzy set theory;idempotency;distributiveness interval type 2 fuzzy sets idempotency absorption;type 2 fuzzy set;fuzzy systems;conferences;interval type 2 fuzzy sets	In this paper we study in depth certain properties of interval type 2 fuzzy sets. In particular we recall a method to construct different interval type 2 fuzzy connectives starting from an operator. We further study the law of contradiction and the law of excluded middle for these sets. Furthermore we analyze the properties: idempotency, absorption, and distributiveness.	emoticon;fuzzy set;idempotence;logical connective;nsa product types;t-norm	Humberto Bustince;Javier Montero;Edurne Barrenechea Tartas;Miguel Pagola	2008	2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)	10.1109/FUZZY.2008.4630587	fuzzy logic;combinatorics;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	-0.6601991194470322	-23.75350853158862	58562
8c5dc19d5b0d033795cc09fd1bfa5f2f9f1e8d9e	next day load forecasting using svm	forecasting;prevision;intelligence artificielle;load forecasting;power system;artificial intelligence;inteligencia artificial;carga electroenergetica;charge electroenergetique;power system load	Based on similar day method and SVM, this paper proposes a new method for next day load forecasting. The new method uses the parameters of several similar days, instead of only selecting one similar day as in similar day method. The parameters of selected similar days are used as inputs to SVM for forecasting the loads of 24 points (one hour per point) of the next day. The method behaves the advantages of both similar day method and SVM method, Corresponding software was developed and used to forecast the next day load in a practical power system and the final forecasting error is low.		Xunming Li;Dengcai Gong;Linfeng Li;Changyin Sun	2005		10.1007/11427469_101	simulation;forecasting;computer science;artificial intelligence;machine learning;electric power system;statistics	Robotics	9.716025304212154	-17.942829218413454	58596
2a9f5639a1dea282d4d1b3a84d6de9035f94a8f0	forecasting time series with a logarithmic model for the polynomial artificial neural networks	mathematical model modeling polynomials genetic algorithms time series analysis training artificial neural networks;neural nets;time series;chaotic time series time series forecasting logarithmic model polynomial artificial neural networks integer exponentials fractional exponentials nonlinear time series;time series forecasting theory neural nets;forecasting theory;artificial neural network	The adaptation made for the Polynomial Artificial Neural Networks (PANN) using not only integer exponentials but also fractional exponentials, have shown evidence of its better performance, especially, when it works with non-linear and chaotic time series. In this paper we show the comparison of the PANN improved model of fractional exponentials with a new logarithmic model. We show that this new model have even better performance than the last PANN improved model.	algorithmic efficiency;artificial neural network;computation;ising model;neural networks;nonlinear system;polynomial;time series	J. Carlos Luna-Sanchez;Eduardo Gómez-Ramírez;Kaddour Najim;Enso Ikonen	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033576	econometrics;computer science;artificial intelligence;machine learning;time series;mathematics;artificial neural network	ML	8.688023251377656	-20.87972398586862	58763
6717354c52cb9126edaba75c8b60a307c2269eae	modelling and prediction of the cny exchange rate using rbf neural network	usd cny;time varying;neural nets;transfer functions;time series exchange rates forecasting theory neural nets radial basis function networks;training;biological system modeling;exchange rates;forecasting cny spot exchange rates;time series;data mining;gbp cny;forecaster cny exchange rate rbf neural network financial time series;radial basis function networks;artificial neural networks;forecasting theory;time series analysis;transfer function;predictive models exchange rates neural networks economic forecasting power system modeling feedforward neural networks environmental economics transfer functions conference management financial management;rbf neural network;financial time series;exchange rate;cny exchange rate;forecaster;usd cny forecasting cny spot exchange rates rbf neural network financial time series gbp cny	The CNY exchange rates can be viewed as financial time series which are charactered by high uncertainty, nonlinearity and time-varying behavior. Predictions for exchange rates of GBP-CNY and USD-CNY were carried respectively by means of RBF neural network forecasters. The detailed designs for architectures of RBF neural network models, transfer functions of the hidden layer nodes, input vectors and output vectors were made with many tests. Experimental results show that the performance of RBF neural networks for forecasting CNY spot exchange rates is acceptable and effective.	artificial neural network;closing (morphology);foreign exchange service (telecommunications);nonlinear system;radial basis function;time series;transfer function	Zhaocheng Liu;Ziran Zheng;Xiyu Liu;Gongxi Wang	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.18	economics;artificial intelligence;machine learning;data mining	DB	7.725651414427169	-19.468796840071292	58820
7f6d756295efe263f4d0dd05ef974d5213a6f559	a generalized algorithm for modelling & forecasting the share prices of the banking sector	generic algorithm;banking sector	that superior models were obtained by applying the proposed algorithm compared to the financial forecasting models published in the literature. I. INTRODUCTION Modelling and forecasting shares in the banking sector has received much attention in the finance literature because of the high volatilities in its prices which represents a systematic risk faced by investors who hold these shares in their market. This is because the prices in the market are affected by many factors such as the economic and political climate, company's performance, supply and demand and investors' behaviour in general. Many investors observe the market closely when they buy or sell shares because of the future uncertainty in the prices of the shares. Hence, reliably forecasting the future values of shares is essential to minimize the risk for the investors. While some approaches have been successful in modelling certain	algorithm;feedback;model selection;sensitivity and specificity;simulation;time series;trinomial	Amar A. Majeed Rahou;Hasan Al-Madfai;Hugh Coombs;Dave Gilleland;J. Andrew Ware	2007			probabilistic forecasting;genetic algorithm;demand forecasting;computer science;machine learning	AI	4.939246439961988	-16.83995028243304	58821
577e40404368906d33d70d83748d6de6a34fd327	an automatic approach to reaching consensus in multiple attribute group decision making	consensus;multiple attribute group decision making magdm;iterative algorithm;decision matrix;group decision making;additive weighted aggregation awa operator;group decision	In this work, we consider the problem of consensus of multiple attribute group decision making, and develop an automatic approach to reaching consensus among group opinions. In the process of group decision making, each expert provides his/her preferences over the alternatives with respect to each attribute, and constructs an individual decision matrix. The developed approach first aggregates these individual decision matrices into a group decision matrix by using the additive weighted aggregation (AWA) operator, and then establishes a convergent iterative algorithm to gain a consentaneous group decision matrix. Then based on the consentaneous group decision matrix, the approach utilizes the AWA operator to derive the overall attribute values of alternatives, by which the most desirable alternative can be found out. Finally, we detailedly expound the implementation process of the approach with a practical example.		Zeshui Xu	2009	Computers & Industrial Engineering	10.1016/j.cie.2008.08.013	group decision-making;consensus;optimal decision;influence diagram;weighted product model;artificial intelligence;machine learning;decision tree;data mining;decision rule;mathematics;iterative method;weighted sum model;statistics;decision matrix	AI	-3.1586767211206532	-19.565200978187118	58855
bf7fa014ce5834e74737f08327c6c012612b0f1b	intelligent road congestion prediction employing queueing based model	会议论文	Road traffic has long been recognized to represent a country’s prosperity. Unfortunately, traffic congestion coming with it is considered as one of the most significant challenge in modern cities. Thus, a promising paradigm, namely, Intelligent Transportation System (ITS) has become a research focus which lies on its feasibility and efficiency of solving transportation issues. In the open literature, researchers have proposed theories and developed corresponding models to alleviate traffic congestion. However, many solutions are static, which means they serve to analyze and evaluate the traffic systems rather than schedule the road traffic dynamically. To this end, this paper develops a road traffic congestion prediction model to forecast the congestion level. The congestion performance metrics are obtained by developing a queueing system subject to self-similar traffic. This is because Hurst parameter evaluation, subject to road traffic flow at rush hour, presents strong self-similar characteristic. The developed model is validated by comparing results derived from model with that of simulations on real road traffic data provided by government authorities.	network congestion	Lianghao Gao;Chengfang Ma;Lei Liu;Xinjing Wei	2015		10.1007/978-3-319-27140-8_37	simulation;computer science	Metrics	9.246567215932446	-10.144690496945978	59215
41ceaae4fdc428b8673a13325b7c012401b16621	the advantages of fuzzy optimization models in practical use	right hand side;interactive solution algorithm;data processing;decision maker;multicriteria programming;satisfiability;information gathering;programming model;mathematical programming;flexible extended addition;information processing;mixed integer programming;stochastic model;fuzzy optimization;fuzzy system;fuzzy model;problem solving	"""Classical mathematical programming models require well-defined coefficients and right hand sides. In order to avoid a non satisfying modeling usually a broad information gathering and processing is necessary. In case of real problems some model parameters can be only roughly estimated. While in case of classical models the vague data is replaced by """"average data"""", fuzzy models offer the opportunity to model subjective imaginations of the decision maker as precisely as a decision maker will be able to describe it. Thus the risk of applying a wrong model of the reality and selecting solutions which do not reflect the real problem can be clearly reduced. The modeling of real problems by means of deterministic and stochastic models requires extensive information processing. On the other hand we know that an optimum solution is finally defined only by few restrictions. Especially in case of larger systems we notice afterwards that most of the information is useless. The dilemma of data processing is due to the fact that first we have to calculate the solution in order to define, whether the information must be well-defined or whether vague data may be sufficient. Based on multicriteria programming problems it should be demonstrated that the dilemma of data processing in case of real programming problems can be handled adequately by modeling them as fuzzy system combined with an interactive problem-solving. Describing the real problem by means of a fuzzy system first of all only the available information or such information which can be achieved easily will be considered. Then we try to develop an optimum solution. With reference to the cost-benefit relation further information can be gathered in order to describe the solution more precisely. Furthermore it should be pointed out that some interactive fuzzy solution algorithms, e.g. FULPAL provide the opportunity to solve mixed integer multicriteria programming models as well."""	program optimization	Heinrich J. Rommelfanger	2004	FO & DM	10.1007/s10700-004-4200-6	stochastic programming;decision-making;mathematical optimization;data processing;information processing;computer science;artificial intelligence;stochastic modelling;machine learning;mathematics;programming paradigm;algorithm;satisfiability	NLP	0.2981896560747642	-17.17383075437469	59255
694054c9fa95aadf8853d87b491fb3c3c0e0a0f7	reference point approach for multiple decision makers	commande multimodele;multiple criteria analysis;decision models;logro;multicriteria analysis;control estadistico proceso;discrete optimisation;achievement;systeme aide decision;social decision;carte controle;multiple criteria decision making;computer and information science;statistical process control;control multimodelo;multimodel control;maitrise statistique processus;sistema ayuda decision;prise decision;decision maker;systeme aide decision groupe;reference point;control chart;decision support system;group decision support systems;multiple decision;carta control;decision multiple;group decision making;analisis multicriterio;reussite;analyse multicritere;decision colectiva;data och informationsvetenskap;decision collective;toma decision;achievement function;optimisation discrete	We consider multiple criteria decision-making problems where a group of decision-makers wants to find the most preferred solution from a discrete set of alternatives. We develop a method that uses achievement functions for charting subsets of reference points that would support a certain alternative to be the most preferred one. The resulting descriptive information is provided to the decision-makers in the form of reference acceptability indices and central reference points for each decision alternative. Then, the decision-makers can compare this information with their own preferences. We demonstrate the use of the method using a strategic multiple criteria decision model for an electricity retailer.		Risto Lahdelma;Kaisa Miettinen;Pekka Salminen	2005	European Journal of Operational Research	10.1016/j.ejor.2004.01.030	decision-making;control chart;decision model;group decision-making;decision support system;economics;artificial intelligence;operations management;mathematics;operations research;statistical process control	Vision	-0.9785314166825353	-15.4159447089022	59325
6590ad396d5e0fe1c83e07f09a01c571abb3073f	empirical research on financial expenditure policy to the effect of inflation	consumer price index;global financial crisis;it adoption;empirical research	  By the global financial crisis, early 2009 China’s economic continued last year’s decline trends, in response to this situation,  the government adopted a series of financial expenditure policy. In this paper, it adopts monthly data of financial expenditure  and consumer price index which is from 1996 1 month - December 2008 in China, and analyzes impact which is from the government’s  expenditure to inflation. The results show that the explanatory power which is about financial expenditure to inflation is  very low.    		Wen-Jun Chen;Lin Hu	2010		10.1007/978-3-642-19853-3_30	consumer price index;financial analysis;economics;finance;international economics;economic policy;financial ratio;consumer price index	HCI	3.4012741333768064	-14.509080246302723	59474
c6bd5d5da29211f5baaa55756811597e2bf09eff	a novel approach to designing an adaptive committee applied to predicting company's future performance	committee;financial attribute;data proximity;engineering and technology;teknik och teknologier;random forest;som	Highlights? We present a novel technique to create adaptive committees of models. ? SOM enables building committees, specific for each data point analyzed. ? Adaptive committees were significantly more accurate than committees of other types. This article presents an approach to designing an adaptive, data dependent, committee of models applied to prediction of several financial attributes for assessing company's future performance. Current liabilities/Current assets, Total liabilities/Total assets, Net income/Total assets, and Operating Income/Total liabilities are the attributes used in this paper. A self-organizing map (SOM) used for data mapping and analysis enables building committees, which are specific (committee size and aggregation weights) for each SOM node. The number of basic models aggregated into a committee and the aggregation weights depend on accuracy of basic models and their ability to generalize in the vicinity of the SOM node. A random forest is used a basic model in this study. The developed technique was tested on data concerning companies from ten sectors of the healthcare industry of the United States and compared with results obtained from averaging and weighted averaging committees. The proposed adaptivity of a committee size and aggregation weights led to a statistically significant increase in prediction accuracy if compared to other types of committees.		Zivile Kalsyte;Antanas Verikas;Marija Bacauskiene;Adas Gelzinis	2013	Expert Syst. Appl.	10.1016/j.eswa.2012.10.018	random forest;computer science;artificial intelligence;machine learning;data mining;operations research	Arch	5.711673061936503	-20.686559340466147	59606
53ac1d027f787cdc96c78584b9ef9cd97d40279b	reasonable properties for the ordering of fuzzy quantities (i)	fuzzy quantities;ordering of fuzzy quantities;fuzzy numbers;reasonable property;fuzzy quantity;transitivity;satisfiability;fuzzy number	This work aims at the discussion of reasonable properties for the ordering of fuzzy quantities. In the fuzzy literature more than 35 indices exist for the comparison of fuzzy quantities. To grasp this amalgam of indices we split them up into three classes (with linguistic approaches excluded). In this paper we brie y introduce the ordering indices in the rst and second class. Based on these indices some ways to formulate the ranking orders among fuzzy quantities are suggested. Then we propose some axioms which serve as the reasonable properties to gure out the rationality of an ordering procedure. Finally, we check all the ordering indices in the rst and second class to see whether the proposed axioms are ful lled or not. c © 2001 Elsevier Science B.V. All rights reserved.	apple a7;british undergraduate degree classification;fuzzy number;limewire;rationality	Xuzhu Wang;Etienne E. Kerre	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(99)00062-7		AI	-1.401526479463921	-19.642955757794514	59631
b8efdd05daac0fe3776bc4deb48bb93b09c6cede	an extension of x13-arima-seats to forecast islamic holidays effect on logistic activities	moroccan case studies x13 arima seats islamic holidays effect logistic activities calendar effect classical time series forecasting methods exponential smoothing method;time series logistics smoothing methods;calendars time series analysis predictive models adaptation models logistics data models forecasting	To better manage and optimize logistic activities, factors that affect it must be determined: The calendar effect is one of these factors which must be analyzed. Analyzing such kind of data by using classical time series forecasting methods, such as exponential smoothing method and ARIMA model, will fail to capture such variation. This paper is released to present a review of the models which are used to forecast the calendar effect, especially moving holidays effect. We adopt the recent approach of X13-ARIMA-SEATS and extend it for being able to forecast the effect of Islamic holidays. Our extension is applied to Moroccan case studies, and aims to give recommendations concerning this effect on logistic activities.	autoregressive integrated moving average;centralized computing;smoothing;time complexity;time series	Malek Sarhani;Abdellatif El Afia	2014	2014 International Conference on Logistics Operations Management	10.1109/GOL.2014.6887423	financial economics;econometrics;geography;operations management	HCI	6.022902809175218	-14.934804502719855	59645
5c5055b3a23dc2c9c2f3b6554779e48bcc4d71db	another view of aggregation operators on group-based generalized intuitionistic fuzzy soft sets: multi-attribute decision making methods				Khizar Hayat;Muhammad Irfan Ali;Bing-yuan Cao;Faruk Karaaslan;Xiao-Peng Yang	2018	Symmetry	10.3390/sym10120753		AI	-1.9116092367793518	-21.439673924201898	59705
659abb00dffb4744520b6b25db0ff300cdfe5306	statistical disclosure control methods through a risk-utility framework	medida informacion;controle statistique;r u confidentiality map;statistical data;information loss;analisis estadistico;fournisseur;mesure information;statistical databases;gestion risque;risk management;supplier;attaque informatique;probabilistic approach;office for national statistics;base donnee statistique;vida privada;statistical analysis;private life;data privacy;information measure;perdida informacion;enfoque probabilista;approche probabiliste;statistical disclosure control;analyse statistique;donnee statistique;computer attack;ataque informatica;control estadistico;disclosure risk;vie privee;statistical control;gestion riesgo;h social sciences general;dato estadistico;confidentialite donnee;perte information;proveedor	This paper discusses a disclosure risk – data utility framework for assessing statistical disclosure control (SDC) methods on statistical data. Disclosure risk is defined in terms of identifying individuals in small cells in the data which then leads to attribute disclosure of other sensitive variables. Information Loss measures are defined for assessing the impact of the SDC method on the utility of the data and its effects when carrying out standard statistical analysis tools. The quantitative disclosure risk and information loss measures can be plotted onto an R-U confidentiality map for determining optimal SDC methods. A user-friendly software application has been developed and implemented at the UK Office for National Statistics (ONS) to enable data suppliers to compare original and disclosure controlled statistical data and to make informed decisions on best methods for protecting their statistical data.	statistical disclosure control	Natalie Shlomo;Caroline Young	2006		10.1007/11930242_7	information privacy;risk management;computer science;data mining;operations research;computer security;statistical process control;statistics	Robotics	-0.5814965252230384	-13.933606787888884	59967
46d633085fa6a4828c70fb1fe9c4c2f05c9287a8	building a set of additive value functions representing a reference preorder and intensities of preference: grip method	business and management;modelizacion;article accepte pour publication ou publie;utilisation information;multicriteria analysis;analytic hierarchy process;uso informacion;judgment;robustness analysis;processus hierarchie analytique;multiple criteria decision analysis preference model value function ordinal regression intensity of preference;information use;priorite;intensity of preference;prise de decision;multiple criteria;decision maker;computing;modelisation;jugement;hierarchical classification;weighted sums;robustesse;proceso jerarquia analitico;preferencia;classification hierarchique;robustness;value function;preference;juicio;analisis multicriterio;analyse multicritere;business information systems;grip method;toma decision;priority;prioridad;modeling;multiple criteria decision analysis;clasificacion jerarquizada;robustez;preference model;ordinal regression	We present a method called Generalized Regression with Intensities of Preference (GRIP) for ranking a finite set of actions evaluated on multiple criteria. GRIP builds a set of additive value functions compatible with preference information composed of a partial preorder and required intensities of preference on a subset of actions, called reference actions. It constructs not only the preference relation in the considered set of actions, but it also gives information about intensities of preference for pairs of actions from this set for a given decision maker (DM). Distinguishing necessary and possible consequences of preference information on the considered set of actions, GRIP answers questions of robustness analysis. The proposed methodology can be seen as an extension of the UTA method based on ordinal regression. GRIP can also be compared to the AHP method, which requires pairwise comparison of all actions and criteria, and yields a priority ranking of actions. As for the preference information being used, GRIP can be compared, moreover, to the MACBETH method which also takes into account a preference order of actions and intensity of preference for pairs of actions. The preference information used in GRIP does not need, however, to be complete: the DM is asked to provide comparisons of only those pairs of reference actions on particular criteria for which his/her judgment is sufficiently certain. This is an important advantage comparing to methods which, instead, require comparison of all possible pairs of actions on all the considered criteria. Moreover, GRIP works with a set of general additive value functions compatible with the preference information, while other methods use a single and less general value function, such as the weighted-sum.	utility functions on indivisible goods	José Rui Figueira;Salvatore Greco;Roman Slowinski	2009	European Journal of Operational Research	10.1016/j.ejor.2008.02.006	ordinal regression;judgment;mathematical optimization;computing;analytic hierarchy process;computer science;artificial intelligence;management information systems;data mining;mathematics;welfare economics;statistics;robustness	Vision	-1.040114943034235	-16.034088724980236	60015
8dd113d2b6443939ebcd7d3add8bfd15fd1d7571	robust global sensitivity analysis under deep uncertainty via scenario analysis	global sensitivity analysis;land use change;efast;decision theory;deep uncertainty;robust sensitivity analysis	Complex social-ecological systems models typically need to consider deeply uncertain long run future conditions. The influence of this deep (i.e. incalculable, uncontrollable) uncertainty on model parameter sensitivities needs to be understood and robustly quantified to reliably inform investment in data collection and model refinement. Using a variance-based global sensitivity analysis method (eFAST), we produced comprehensive model diagnostics of a complex social-ecological systems model under deep uncertainty characterised by four global change scenarios. The uncertainty of the outputs, and the influence of input parameters differed substantially between scenarios. We then developed sensitivity indicators that were robust to this deep uncertainty using four criteria from decision theory. The proposed methods can increase our understanding of the effects of deep uncertainty on output uncertainty and parameter sensitivity, and incorporate the decision maker's risk preference into modelling-related activities to obtain greater resilience of decisions to surprise. © 2015 Elsevier Ltd. All rights reserved.	decision theory;ecosystem;extrapolation;first-order predicate;global storage architecture;global change;microsoft outlook for mac;refinement (computing);scenario analysis;the australian	Lei Gao;Brett A. Bryan;Martin Nolan;Jeffery D. Connor;Xiaodong Song;Gang Zhao	2016	Environmental Modelling and Software	10.1016/j.envsoft.2015.11.001	econometrics;land use, land-use change and forestry;uncertainty analysis;decision theory;engineering;data mining;mathematics;management science;ecology;sensitivity analysis;statistics	SE	6.105734803232094	-10.730693704876273	60079
32bd4c16c059449bec077c44458cd229bcd33f44	understanding and forecasting air pollution with the aid of artificial intelligence methods in athens, greece	quality of life;time series;environmental policy;fast fourier transform;artificial intelligent;statistical analysis;decision making process;principal component analysis;air pollution;artificial intelligence method;air quality;arsenic;neural network;knowledge discovery	The understanding and management of air quality problems is a suitable problem concerning the application of artificial intelligence (AI) methods towards knowledge discovery for the purposes of modelling and forecasting. As air quality has a direct impact on the quality of life and on the general environment, the ability to extract knowledge concerning relationships between parameters that influence environmental policy making and pollution abatement measures becoming more and more important. In the present paper an arsenal of AI methods consisting of Neural Networks and Principal Component Analysis is being supported by a set of mathematical tools including statistical analysis, fast Fourier transformations and periodograms, for the investigation and forecasting of photochemical pollutants time series in Athens, Greece. Results verify the ability of the methods to analyze and model this knowledge domain and to forecast the levels of key parameters that provide direct input to the environmental decision making process.	artificial intelligence	Kostas D. Karatzas;George Papadourakis;Ioannis Kyriakidis	2009		10.1007/978-3-540-88069-1_4	marketing and artificial intelligence;engineering;artificial intelligence;operations management;operations research	AI	9.531441955145173	-19.11715551782739	60092
c87374c624a6cbc8bbe3a7df987e86a123431d6b	european option pricing with a fast fourier transform algorithm for big data analysis	analytical models;pricing;stochastic processes;big data;european option pricing fft algorithm based numerical solution method fast fourier transform algorithm based numerical solution method closed form pricing formula characteristic function method mixed exponential jumps nonaffine stochastic volatility coupled stochastic differential equation model fat tail leptokurtosis currency option market statistical analysis transaction data big data analysis fast fourier transform algorithm;stochastic processes big data data analysis differential equations fast fourier transforms pattern clustering pricing statistical analysis;numerical models pricing stochastic processes europe algorithm design and analysis analytical models big data;europe;numerical models;algorithm design and analysis;big data analysis multiple risks european option pricing stochastic modeling fast fourier transform algorithm	Several empirical studies show that, under multiple risks, markets exhibit many new properties, such as volatility smile and cluster fueled by the explosion of transaction data. This paper attempts to capture these newly developed features using the valuation of European options as a vehicle. Statistical analysis performed on the data collected from the currency option market clearly shows the coexistence of mean reversion, jumps, volatility smile, and leptokurtosis and fat tail. We characterize the dynamics of the underlying asset in this kind of environment by establishing a coupled stochastic differential equation model with triple characteristics of mean reversion, nonaffine stochastic volatility, and mixed-exponential jumps. Moreover, we propose a characteristic function method to derive the closed-form pricing formula. We also present a fast Fourier transform (FFT) algorithm-based numerical solution method. Finally, extensive numerical experiments are conducted to validate both the modeling methodology and the numerical algorithm. Results demonstrate that the model behaves well in capturing the properties observed in the market, and the FFT numerical algorithm is both accurate and efficient in addressing large amount of data.	algorithm;big data;characteristic function (convex analysis);coexist (image);experiment;fast fourier transform;numerical analysis;numerical partial differential equations;reversion (software development);time complexity;transaction data;value (ethics);volatility	Shuang Xiao;Shihua Ma;Guo Li;Samar K. Mukhopadhyay	2016	IEEE Transactions on Industrial Informatics	10.1109/TII.2015.2500885	pricing;algorithm design;econometrics;mathematical optimization;big data;computer science;statistics	ML	3.6294224454604294	-17.402382314699253	60456
f9bde23755179363d87e3177741bd90908044019	group decision-making and the analytic hierarchy process: exploring the consensus-relevant information content	analytic hierarchy process;consensus;development strategy;processus hierarchie analytique;social decision;prise decision;information content;teoria decision;similitude;information embedding;theorie decision;decision theory;similarity;proceso jerarquia analitico;group decision making;similitud;decision colectiva;decision collective;toma decision;similarity measure	Abstract   The Analytic Hierarchy Process (AHP) is being increasingly applied to managerial and technical group decision-making problems in which it is necessary to determine the preferences of the group as a whole. In such contexts unless there is an acceptable level of group consensus, it is premature to use mathematical techniques to generate “consensus” preference vectors. Hence it is necessary to assess the current level of group consensus before applying mathematical techniques to generate a “consensus” preference vector. In this paper, we explore possibilities that could result from the use of consensus relevant information embedded in the preference data that arise in group decision-making contexts. We offer a set of similarity measures and consensus indicators for assessing the level of group consensus that could also be used by the group facilitator to develop strategies for increasing the level of group consensus.	analytical hierarchy;self-information	Kweku-Muata Osei-Bryson	1996	Computers & OR	10.1016/0305-0548(96)00002-H	analytic hierarchy process;group decision-making;consensus;similarity;self-information;decision theory;consensus-based assessment;knowledge management;artificial intelligence;similitude;mathematics;statistics	Web+IR	-4.411802691491206	-18.661455223054745	60545
dbcae568c71ad3c203a2a5c1770d7b02359a9e0e	short-term prediction method of resources occupancy for lbs guidance system	guidance system;lbs;arma;multi model fusion;adaptive filter	Guidance system can improve the resources utilization of LBS (Location Based Service) system effectively, and the validity of guidance system depends on accurate predicting of the near-future system resources occupancy and its trend. Considering the strong randomicity of short-term characteristic of the trend, a multi-model fusion method integrating adaptive filter and ARMA (Auto Regressive Moving Average) model is proposed to predict the resources occupancy and trend accurately. With the method, observation series of recent resources occupancy is decomposed to different scales and reconstructed by wavelet transform firstly. According to the different features of series at different scale, for approximate signal, the adaptive filter algorithm is used to predict the trend of resources occupancy at coarse scale, and for detail signals in multiple fine scales, ARMA algorithms are adopted. Finally, integrating the prediction results of multi-model from different scales, the resources occupancy and trend with high prediction accuracy can be obtained. Experiment results show that the resources occupancy prediction accuracy of proposed method is higher than that of typical algorithms such as exponential smoothing and weighted Markov algorithms.	adaptive filter;approximation algorithm;guidance system;location-based service;loss function;markov algorithm;markov chain;moving-average model;multi-model database;peterson's algorithm;randomness;simulation;smoothing;stationary process;stochastic process;time complexity;wavelet transform	Ming Cen;Yuan Wei;Chunyang Wang;Yongfu Li	2014	JCP	10.4304/jcp.9.11.2545-2551	adaptive filter;econometrics;guidance system;simulation;computer science;machine learning;data mining;pound	AI	8.911867297536853	-14.604544500887842	60703
c15934388a4bbab4ede2f7643dfa3f22904686d8	on sugeno integral based mean value for fuzzy quantities	fuzzy set;distribution function;sugeno integral	Evaluation of fuzzy quantities is a contemporary issue that often appears in the process of modeling uncertainty. Some of the well-known approaches are based on the integration done with respect to the probability measure, while the approach presented here considers a fuzzy measure as a base of integration. The integral in question is the Sugeno integral and it is used for construction of the Sugeno based mean value for some specific fuzzy quantities.	fuzzy concept;fuzzy measure theory;sugeno integral;whole earth 'lectronic link;monotone	Ivana Stajner-Papuga;Zagorka Lozanov-Crvenkovic;Gabrijela Grujic	2016	2016 IEEE 14th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2016.7601488	mathematical analysis;discrete mathematics;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;fuzzy measure theory;control theory;mathematics;fuzzy set operations	Embedded	-0.07303472802150633	-22.107721307278144	60837
ca307e3511160970d0318a490f0f5ea055c85a76	information pattern, learning structure, and optimal decision rule	decision rule	In Section I various types of information structures in individual decision-making under uncertainty are discussed. A quantity which measures the value of information structure is proposed. In Section II the team decision problem is considered. Finally, in Section III we consider the multistage decision process with learning and we shall try to evaluate the effect of the learning structure and compute it by the technique of dynamic programming.		Minoru Sakaguchi	1963	Information and Control	10.1016/S0019-9958(63)90274-2	optimal decision;influence diagram;decision analysis;computer science;decision tree;pattern recognition;decision rule;mathematics;admissible decision rule;weighted sum model	AI	-3.0690210466376353	-15.816307842940585	61033
e6b5b4caada27e92e9178c5b3de11141ba0f5cf2	image threshold computation by modelizing knowledge/unknowledge by means of atanassov's intuitionistic fuzzy sets	intuitionistic fuzzy set	In this chapter, a new thresholding technique using Atanassov’s intuitionistic fuzzy sets (A-IFSs) and restricted dissimilarity  functions is introduced. We interpret the intuitionistic fuzzy index of Atanassov as the degree of unknowledge/ignorance of  an expert for determining whether a pixel of an image belongs to the background or the object of the image. Under these conditions  we construct an algorithm on the basis of A-IFSs for detecting the threshold of an image. Then we present a method for selecting  from a set of thresholds of an image the best one. This method is based on the concept of fuzzy similarity. Lastly, we prove  that in most cases our algorithm for selecting the best threshold takes the threshold calculated with the algorithm constructed  on the basis of A-IFSs.  	computation;fuzzy set	Humberto Bustince;Miguel Pagola;Pedro Melo-Pinto;Edurne Barrenechea Tartas;Pedro A. Mogadouro do Couto	2008		10.1007/978-3-540-73723-0_32	discrete mathematics;type-2 fuzzy sets and systems;fuzzy set operations	Logic	-2.119656440229392	-22.831306314876073	61177
fa5521c6fbbc30ed85b6e0a7fff54dcde57cc94c	an integrated neural network and data envelopment analysis for supplier evaluation under incomplete information	neural networks;evaluation method;statistical method;incomplete information;data envelopment analysis;mathematical programming;decision making process;evaluation criteria;supplier evaluation;decision making unit;data envelope analysis;neural network	Supplier evaluation and selection are critical decision making processes that require consideration of a variety of attributes. Several studies have been performed for effective evaluation and selection of suppliers by utilizing several techniques such as linear weighting methods, mathematical programming models, statistical methods and AI based techniques. One of the successful evaluation methods proposed for this purpose is data envelopment analysis (DEA), that utilizes techniques of mathematical programming to evaluate the performance of a set of homogeneous decision making units, when multiple inputs and outputs need to be considered. It is often complicated, costly and sometimes impossible to acquire all necessary information from all potential suppliers to attain a reasonable set of similar input and output values which is an essential for DEA. The purpose of this study is to explore a novel integration of neural networks (NN) and data envelopment analysis for evaluation of suppliers under incomplete information of evaluation criteria. 2007 Elsevier Ltd. All rights reserved.	artificial intelligence;artificial neural network;data envelopment analysis;input/output;mathematical optimization;statistical model	Dilay Çelebi;Demet Bayraktar	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.08.107	computer science;machine learning;data mining;data envelopment analysis;operations research;artificial neural network	AI	-2.8431572036311104	-16.30420368374118	61337
0422f4d17bfb0879578579a0976b115d4bae208a	a new approach to asset pricing with rational agents behaving strategically	eigenvalues and eigenfunctions;pricing;biological system modeling;vectors biological system modeling mathematical model computational modeling extraterrestrial measurements eigenvalues and eigenfunctions;stock markets;approximation theory;multi agent systems;computational modeling;vectors;stock markets approximation theory multi agent systems pricing;mathematical model;extraterrestrial measurements;financial crisis financial asset pricing rational agent behavior stock price volatility rational pricing models time varying prices equilibrium market fundamental value approximation	The volatility of stock prices is difficult to explain within the confines of rational pricing models. Changes in prices have become permanent; Therefore, as we keep the hypothesis of a rational behavior of agents, we must give a new explanation to the pricing of financial assets at any moment of time. In a model based on an original mathematical framework, we introduce persistent time-varying prices resulting from strategic interactions between rational agents. We demonstrate that in a close to equilibrium market, actual prices give the best approximation of the fundamental value; We also explain why, in some circumstances, rational behavior may lead to the development of a bubble or the surge of a financial crisis.	approximation;curve fitting;interaction;numerical analysis;rational agent;simulation;volatility	Alain Bretto;Joel Priolon	2012	2012 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr)	10.1109/CIFEr.2012.6327773	financial economics;variable pricing;investment theory;economics;rational expectations;finance;microeconomics;consumption-based capital asset pricing model;rational pricing;mathematical finance;mark to model	Theory	1.638533987819234	-11.890526981612776	61339
63d8c05cc7195365ebf0d48b85b435467dd533cc	a credit risk evaluation index system establishment of petty loans for farmers based on correlation analysis and significant discriminant	credit risk evaluation;index screening;petty loans for farmers;significant discriminant	By the end of 2011, agricultural population in accounted for 48.73% in China. Also, farmers in China are dispersed and their financial information is incomplete, which leads to credit risk evaluation system of farmers in China is not sound at all. Most Chinese banks even have not established the rating system, so farmer credit risk rating system pressed for solution. This study proposes an index screening method based on correlation analysis and significant discriminant. The proposed method is demonstrated using the 2044 petty loans for farmers of a Chinese state-owned commercial bank’s. The results demonstrate that the proposed method can accurately screen the indicators, which can effectively distinguish default customers from non-default ones. Moreover, the results of an empirical study showed that “Loan purpose”, “Expenses/ incomes”, “Increasing rate of regional GDP” are the key indicators to distinguish default customers from non-default ones. Index Terms — petty loans for farmers, credit risk evaluation, index screening; significant discriminant	chinese wall;discriminant;sports rating system	Linpeng Hai;Baofeng Shi;Guorong Peng	2013	JSW		loan purpose;empirical research;econometrics;machine learning;china;artificial intelligence;credit risk;agriculture;computer science;correlation;population;discriminant	AI	2.5672481397223987	-14.155874497895487	61397
c34c3c9d5828d9c706096d05fbe0135a6bfe94e2	membership modification and level sets	fuzzy set;level set;membership modification;jaccard similarity index measure	We introduce the concept of a membership modification function and describe its role in transforming one fuzzy set into another. We discuss the related idea of a membership modification program consisting of a collection of related membership modification functions and show how the concept of level sets is an example of a membership modification program. Using this idea of membership modification allows us to consider other transformations of fuzzy sets that soften the idea of level sets. Using these ideas we provide an extension of the Jaccard similarity index.		Ronald R. Yager	2013	Soft Comput.	10.1007/s00500-012-0914-5	mathematical optimization;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;level set;data mining;mathematics;fuzzy set	NLP	-1.938027946576623	-23.1175424713241	61481
3c9d7994cb6855e129866fd29ba52ff3bdd2e824	pricing r&d option with combining randomness and fuzziness	option pricing;cash flow;distribution function	Random fuzzy theory is introduced to evaluate the option value of R&D project in this paper. Based on a new theoretical insight, we develop a random fuzzy jump amplitude model to price R&D option. In classic stochastic jump amplitude model, the value of R&D option depends on the expected number of jumps and the expected size of the jumps. However, in practice, it is so hard to get enough data that the distribution functions of them are difficult to be confirmed accurately. So random fuzzy theory can deal with these problems better. In this paper, the number of jumps and the size of jumps are taken as random fuzzy variables, and investment costs and future cash flows are depicted as fuzzy variables. New research tools for option pricing of R&D project are developed and the capability of dealing with practical problems is enhanced.	randomness	Jinliang Zhang;Huibin Du;Wansheng Tang	2006		10.1007/11816171_100	inversion;fuzzy logic;random variable;stock exchange;investment;distribution function;cash flow;mathematics;mathematical economics	Theory	3.1259192801038616	-12.212580402258707	61554
3475a85a533937143233c98f4f9d48b88171676c	kendall distribution functions and associative copulas	engineering;ordinal sum;archimedean copula;associative copula;fuzzy set;procesamiento informacion;fonction repartition;kendall distribution function;conjunto difuso;copula;ensemble flou;ingenierie;funcion distribucion;distribution function;primary 60e05;information processing;62e10;ingenieria;sistema difuso;systeme flou;traitement information;fuzzy system;secondary 62h05	In this paper, we prove that any Kendall distribution function is the Kendall distribution function of some associative copula. We use this result to show that each equivalence class of the relation ''to have the same Kendall distribution function as'' defined on the set of copulas contains a unique associative representative.		Roger B. Nelsen;José Juan Quesada-Molina;José Antonio Rodríguez-Lallena;Manuel Úbeda-Flores	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.05.001	econometrics;information processing;computer science;artificial intelligence;copula;distribution function;calculus;mathematics;fuzzy set;fuzzy control system;statistics	ECom	1.5719480953710598	-21.072095519283245	61663
0145227ee54dd7972da0b488beeb55688c66957b	determination of float time for individual construction tasks using constraint-based discrete-event simulation	civil engineering computing;construction;discrete event simulation;project engineering;virtual reality;backward simulation;constraint-based discrete-event simulation;construction project;execution order;float time;forward simulation;individual construction tasks;task execution sequence;virtual completion date	In this paper we propose a methodology which combines Forward and Backward Simulation in order to extend discrete-event simulation by the ability to calculate float times. In the case of Backward Simulation, the simulation starts at the virtual completion date of the construction project and runs backwards in time until the start date. To determine float times it is important that the task execution sequence is in the same order for forward and backward simulation. Consequently we present an extension to the simulation concept that controls the execution order of the tasks within the backward simulation. By combining the results of the forward and backward simulation, it is possible to determine float times for each task while taking into account resources. A comprehensive case study illustrates the application of this new approach. One result of this is the determination of detailed float times for each task using discrete-event simulation.	contingency plan;simulation	Gergo Dori;André Borrmann	2012	Proceedings Title: Proceedings of the 2012 Winter Simulation Conference (WSC)		real-time computing;simulation;construction;computer science;engineering;discrete event simulation;virtual reality	Embedded	8.646847363462541	-10.845965877760866	61680
ff9f65c085f7b5992ba96ca68b951754f5ca0ae4	a time-dependent decision support system for multi-attribute decision-making	decision support system;time-dependent decision support system;multi-attribute decision-making;different sensitivity analysis;imprecise multiattribute additive utility;imprecise scaling factor;imprecise individual utility function;vectorial additive utility function;objective hierarchy;different time period;different discounting method;imprecise consequence	We describe a decision support system including multiple conflicting objectives and with consequences spanning several time periods. The system is supported by an imprecise multiattribute additive utility model, which includes the possibility of different discounting methods, aimed at identifying the optimal strategy by means of interactive multiobjective simulated annealing. The system can be useful for solving problems, including, if necessary, decisions with imprecise consequences over different time periods, which are modelled by means of an objective hierarchy. This objective hierarchy considers appropriate preferences based on judgemental inputs for assessing imprecise individual utility functions and imprecise scaling factors on objectives and attributes, which are aggregated into a scalar or vectorial additive utility function to evaluate the available alternative decisions. The system also includes different sensitivity analyses to check the robustness of the conclusions. Finally, an application to the evaluation of remedial strategies for restoring contaminated aquatic ecosystems illustrates the usefulness and flexibility of this decision support tool.	decision support system	Sixto Ríos-Insua;Antonio Jiménez;Alfonso Mateos	2004	Integrated Computer-Aided Engineering		management science	DB	-3.2247336736228718	-16.9387472865991	61777
2f794c5b7650bd716fe58dbf5637bf4a979562f2	real stock trading using soft computing models	artificial intelligence technique;fuzzy reasoning;learning;neural nets;soft computing;backpropagation;intel stock price real stock trading soft computing stock prediction artificial intelligence technique artificial neural network training backpropagation conjugate gradient algorithm mamdani takagi sugeno fuzzy inference system genetic algorithm microsoft stock price;conjugate gradient algorithm;stock markets;artificial intelligent;artificial neural networks;conjugate gradient;stock prediction;stock price;fuzzy neural networks genetic algorithms fuzzy systems artificial intelligence artificial neural networks inference algorithms backpropagation algorithms economic indicators computer science learning;backpropagation algorithms;fuzzy inference system;genetic algorithms stock markets share prices backpropagation neural nets conjugate gradient methods fuzzy reasoning;artificial neural network training;artificial intelligence;genetic algorithm;genetic algorithms;inference algorithms;profitability;computer science;share prices;real stock trading;fuzzy neural networks;conjugate gradient methods;intel stock price;fuzzy systems;microsoft stock price;artificial neural network;economic indicators;mamdani takagi sugeno fuzzy inference system	The main focus of this study is to compare different performances of soft computing paradigms for predicting the direction of individuals stocks. Three different artificial intelligence techniques were used to predict the direction of both Microsoft and Intel stock prices over a period of thirteen years. We explore the performance of artificial neural networks trained using backpropagation and conjugate gradient algorithm and a Mamdani and Takagi Sugeno fuzzy inference system learned using neural learning and genetic algorithm. Once all the different models were built the last part of the experiment was to determine how much profit can be made using these methods versus a simple buy and hold technique.	artificial intelligence;artificial neural network;backpropagation;conjugate gradient method;database transaction;genetic algorithm;hurst exponent;inference engine;performance;soft computing;whole earth 'lectronic link	Brent Doeksen;Ajith Abraham;Johnson P. Thomas;Marcin Paprzycki	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.238	computer science;artificial intelligence;machine learning;data mining	AI	6.824105722971414	-19.580242667947694	62005
9fb3534d22a8de1535567a72add09f5a1b12872e	groundwater level dynamic prediction based on chaos optimization and support vector machine	groundwater level;optimisation;least squares approximations;irrigation;support vector machines;chaos optimization;chaos;chaos support vector machines least squares methods predictive models irrigation arithmetic water resources machine learning equations lagrangian functions;training;water resources;optimization groundwater level svm chaos prediction;testing;forecasting model;small samples;inner mongolia;machine learning;least square support vector machine;predictive models;svm;optimization;prediction model;groundwater level prediciton;groundwater appraisal;support vector machine;least square support vector machine groundwater level prediciton chaos optimization groundwater appraisal;optimal prediction;process analysis;water resources least squares approximations optimisation support vector machines;prediction;least squares support vector machine	Groundwater level has random characters because of influences factors of natural and anthropogenic. Study random prediction model of groundwater level on the basis of groundwater physical process analysis is important to groundwater appraisal. The theory of supporting vector machine based on small-sample machine learning theory is introduced into dynamic prediction of groundwater level. A least square support vector machine groundwater level dynamic forecasting model based on chaos optimization peak value identification was proposed and applied in Hetao irrigation district in Inner Mongolia. The results show that the fitted values, the tested values and the predicted values of this model have little different from their real values. And they indicate that the model is feasible and effective. So the model proposed in this paper can provide a new tool for groundwater level dynamic forecasting.	machine learning;mathematical optimization;support vector machine	Jin Liu;Jian-xia Chang;Wen-ge Zhang	2009	2009 Third International Conference on Genetic and Evolutionary Computing	10.1109/WGEC.2009.25	support vector machine;computer science;machine learning;data mining	SE	10.0311128778928	-20.04538300120184	62034
d7cc52a45869c09c480841097a0ea27ac1333f40	adaptive and self-adaptive techniques for evolutionary forecasting applications set in dynamic and uncertain environments	time series forecasting;theoretical analysis;evolutionary computing	1 Department of Mathematics and Computer Science, Augusta State University, Augusta, GA 30904, USA nwagner@aug.edu 2 School of Computer Science, University of Adelaide, Adelaide, SA 5005, Australia, Institute of Computer Science, Polish Academy of Sciences, ul. Ordona 21, 01-237 Warsaw, Poland, and Polish-Japanese Institute of Information Technology, ul. Koszykowa 86, 02-008 Warsaw, Poland zbyszek@cs.adelaide.edu.au	academy;adaptive algorithm;computer science;experiment;software release life cycle	Neal Wagner;Zbigniew Michalewicz	2009		10.1007/978-3-642-01088-0_1	econometrics;computer science;artificial intelligence;machine learning;time series;statistics;evolutionary computation	Theory	7.301605052119002	-16.308253315332575	62076
2e27869002b69064f649bb6f1cf82dea2a7ddbbe	a fuzzy model of customer satisfaction index in e-commerce	e commerce;linguistic value;customer satisfaction index;customer satisfaction;fuzzy techniques;indexation;evaluation;point of view;quality of service;fuzzy model	Customer satisfaction index (CSI) is an important concept for evaluating the quality of service in e-commerce. It permits to evaluate the validity of an e-commerce operation from the point of view of consumers. In this paper, we present a model of CSI in e-commerce using fuzzy techniques and provide a method for calculating CSI, expressed in a five levels quantity table. © 2007 IMACS. Published by Elsevier B.V. All rights reserved.	e-commerce;fuzzy logic;quality of service;system analysis;theory	Xiaohong Liu;Xianyi Zeng;Ludovic Koehl	2008	Mathematics and Computers in Simulation	10.1016/j.matcom.2007.11.017	e-commerce;voice of the customer;knowledge management;customer satisfaction;service quality	AI	-4.092273715146172	-18.39798435642673	62231
b5da93f5010301a3d677e42aebf166033630bcaa	jensen type inequality for the pseudo-integral based on the smallest universal integral	chebyshev approximation convex functions extraterrestrial measurements educational institutions intelligent systems informatics;integral equations fuzzy set theory;monotone measure;operation;convex functions;pseudo;jensen s inequality;intelligent systems;pseudo integral;informatics;sufficient condition jensen type inequality pseudo integral sugeno integral choquet integral seminormed fuzzy integral shilkret integral smallest universal integral;chebyshev approximation;jensen s inequality pseudo operation monotone measure universal inte gral pseudo integral;extraterrestrial measurements;universal inte gral	Choquet and Sugeno integrals have wide applications in several practical areas, especially as aggregation functions in decision theory. Universal integral is a generalization of Choquet and Sugeno integrals. The Jensen type inequality related to the smallest universal integral which special cases are Sugeno, Shilkret, seminormed fuzzy integrals and pseudo-integral has been recently proposed. Based on this result, we have obtained the Jensen's inequality for the pseudo-integral and sufficient conditions under which that inequality holds.	aggregate function;decision theory;jensen's inequality;social inequality;sugeno integral	Endre Pap;Mirjana Strboja	2014	2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2014.6923574	mathematical optimization;mathematical analysis;daniell integral;jensen's inequality;calculus;fuzzy measure theory;mathematics;log sum inequality	Arch	-0.27496332375188637	-22.821274169719924	62381
d0a79c2d57dadeff55d3a8c0fb5482dbd3e2048a	application of bivariate and multivariate statistical techniques in landslide susceptibility modeling in chittagong city corporation, bangladesh	regression statistics;gis;remote sensing;landslides;cartography;weights of evidence	The communities living on the dangerous hillslopes in Chittagong City Corporation (CCC) in Bangladesh recurrently experience landslide hazards during the monsoon season. The frequency and intensity of landslides are increasing over time because of heavy rainfall occurring over a few days. Furthermore, rapid urbanization through hill-cutting is another factor, which is believed to have a significant impact on the occurrence of landslides. This study aims to develop landslide susceptibility maps (LSMs) through the use of Dempster-Shafer weights of evidence (WoE) and the multiple regression (MR) method. Three different combinations with principal component analysis (PCA) and fuzzy membership techniques were used and tested. Twelve factor maps (i.e., slope, hill-cutting, geology, geomorphology, NDVI, soil moisture, precipitation and distance from existing buildings, stream, road and drainage network, and faults-lineaments) were prepared based on their association with historical landslide events. A landslide inventory map was prepared through field surveys for model simulation and validation purposes. The performance of the predicted LSMs was validated using the area under the relative operating characteristic (ROC) curve method. The overall success rates were 87.3%, 90.9%, 91.3%, and 93.9%, respectively for the WoE, MR with all the layers, MR with PCA layers, and MR with fuzzy probability layers.	bivariate data;entity;map;population;principal component analysis;receiver operating characteristic;simulation;storm botnet;transformers: devastation	Bayes Ahmed;Ashraf M. Dewan	2017	Remote Sensing	10.3390/rs9040304	geomatics;hydrology;landslide;regression analysis;remote sensing	ML	4.061431148072236	-21.233850051558047	62482
58acf6b96ca8532795d899ff3459fa240810800c	parametric, semi-parametric and non-parametric models of telecommunications demand: an investigation of residential calling patterns	parametric model;semi parametric;jel classification codes d12;long distance;call duration;jel classification codes c4;cox proportional hazards model;cross section;hazard function;jel classification codes c14;telecommunications;non parametric	We investigate long distance telephone calling patterns using billing information and demographic data for a large cross-section of residential households. The joint distribution of the number and duration of toll calls is estimated using a non-parametric kernel model, a semi-parametric Cox proportional hazard model, and a fully parametric Poisson–Weibull model. Particular attention is paid to the estimation and analysis of call duration hazard functions. We find that call duration is quite inelastic with respect to price, and that while evening and night /weekend rate calls share vary similar duration characteristics, they differ substantially from peak rate calls. Published by Elsevier Science B.V.	baseline (configuration management);electronic billing;failure rate;higher-order function;library classification;parametric model;parametric polymorphism;passenger name record;peak calling;semiconductor industry;specification (regression);statistical model;structural analysis	Erik Heitfield;Armando Levy	2001	Information Economics and Policy	10.1016/S0167-6245(01)00033-6	nonparametric statistics;econometrics;simulation;parametric model;call duration;failure rate;cross section;proportional hazards model;semiparametric model;statistics	ML	6.2961232997765215	-14.253510693256047	62560
733c9a2d238e5e2ba7080553ed29d721e9c95cf1	preference elicitation in fully probabilistic design of decision strategies	probability;uncertainty;closed loop systems;bayes methods;bismuth;kullback leibler divergence;probability bayes methods closed loop systems decision theory;bayesian methods;systematic data based preference elicitation fully probabilistic design systematic decision making design decision strategy closed loop behaviour kullback leibler divergence bayesian decision making bayesian problem formulations;delta modulation;preference elicitation;decision theory;performance analysis;bayesian decision making;delta modulation performance analysis bismuth bayesian methods probabilistic logic uncertainty decision making;probabilistic logic	Any systematic decision-making design selects a decision strategy that makes the resulting closed-loop behaviour close to the desired one. Fully Probabilistic Design (FPD) describes modelled and desired closed-loop behaviours via their distributions. The designed strategy is a minimiser of Kullback-Leibler divergence of these distributions. FPD: i) unifies modelling and aim-expressing languages; ii) directly describes multiple aims and constraints; iii) simplifies an (inevitable) approximate design as it has an explicit minimiser. The paper enriches the theory of FPD, in particular, it: i) improves its axiomatic basis; ii) quantitatively relates FPD to standard Bayesian decision making showing that the set of FPD tasks is a dense extension of Bayesian problem formulations; iii) opens a way to a systematic data-based preference elicitation, i.e., quantitative expression of decision-making aims.	approximation algorithm;bayesian experimental design;decision theory;flat panel detector;kullback–leibler divergence;preference elicitation	Miroslav Kárný;Tatiana V. Guy	2010	49th IEEE Conference on Decision and Control (CDC)	10.1109/CDC.2010.5717087	delta modulation;uncertainty;decision theory;bayesian probability;machine learning;bismuth;probability;data mining;mathematics;probabilistic logic;kullback–leibler divergence;statistics	Robotics	-4.009605580070624	-12.72661096462955	62774
cecec452a4b96d91e97c730bef2d613f43fc9eb2	shopbot 2.0: integrating recommendations and promotions with comparison shopping	consumidor;modelizacion;rentabilidad;commerce electronique;consumer search;promocion venta;online retailing;comercio electronico;programacion entera;compra;sistema gestion dato tecnico;consommateur;comercializacion;promotion vente;venta menudeo;vente au detail;recommandation;by product;programmation en nombres entiers;retail marketing;detaillant;commercialisation;modelisation;promocion producto;sale promotion;recommender system;integer programming;marketing;sous produit;consumer;subproducto;recomendacion;achat;recommendation;product data management pdm;rentabilite;profitability;sales promotions;integer program;systeme gestion donnee technique sgdt;shopbots;modeling;retailers;recommender systems;electronic trade;purchases;profit maximization;product promotion;promotion produit	Recommender systems have been used by online retailers along with various promotions to attract customers. They are often in the form of a single item (best bet) along with a choice set. The majority of choice set recommendations are made based on collaborative filtering algorithms that recommend highly related items. However, we observe that very often best bets suggested by retailers are not based strictly on relatedness, since they are not members of the choice set. We found that the probability of this occurring is positively related to the popularity of the original requested item (base item). We also show that, even when best bets are closely related to base items, there are alternate options for the best bet that are still highly related, and at the same time can integrate with existing promotions to be more appealing to price sensitive customers. We argue that shopbots are in the best position to provide such integrated service and we therefore develop an integer programming model to optimize recommendations for shopbots. This model is validated using data from two online book retailers to show that significant extra savings can be achieved by suggesting alternate best bets.	comparison shopping website	Robert S. Garfinkel;Ram D. Gopal;Bhavik Pathak;Fang Yin	2008	Decision Support Systems	10.1016/j.dss.2008.05.006	systems modeling;consumer;computer science;marketing;operations management;advertising;management;profitability index;recommender system	HCI	-2.3485227273458835	-9.938023048776271	62976
d1aa79a505a70e68a9e19171acf47514df1470ab	a note on similarity relations between fuzzy attribute-oriented concept lattices		Abstract Studying fuzzy attribute-oriented concept lattices has been a challenging issue of Formal Concept Analysis. In [19], the author introduced a fuzzy similarity relation between two collections of L-sets and proved some properties of this similarity type. In this paper, based on a duality technique which allowed us to transfer some properties from antitone to isotone fuzzy concept lattices, we provide alternative proofs for the inequality relations between the similarity of fuzzy attribute-oriented concept lattices and the similarity of the fuzzy contexts.		Gabriel Ciobanu;Cristian Vaideanu	2018	Inf. Sci.	10.1016/j.ins.2018.05.034	inequality;discrete mathematics;mathematics;fuzzy concept;fuzzy logic;lattice (order);mathematical proof;duality (optimization);formal concept analysis	DB	-1.1904603006690433	-23.615069131110992	63056
9f65fed8ec7a63afc2100f647e92858b188f1ddd	an intuitionistic fuzzy programming method for group decision making with interval-valued fuzzy preference relations	article	The paper develops a new intuitionistic fuzzy (IF) programming method to solve group decision making (GDM) problems with interval-valued fuzzy preference relations (IVFPRs). An IF programming problem is formulated to derive the priority weights of alternatives in the context of additive consistent IVFPR. In this problem, the additive consistent conditions are viewed as the IF constraints. Considering decision makers’ (DMs’) risk attitudes, three approaches, including the optimistic, pessimistic and neutral approaches, are proposed to solve the constructed IF programming problem. Subsequently, a new consensus index is defined to measure the similarity between DMs according to their individual IVFPRs. Thereby, DMs’ weights are objectively determined using the consensus index. Combining DMs’ weights with the IF program, a corresponding IF programming method is proposed for GDM with IVFPRs. An example of E-Commerce platform selection is analyzed to illustrate the feasibility and effectiveness of the proposed method. Finally, the IF programming method is further extended to the multiplicative consistent IVFPR.	e-commerce payment system;fuzzy set;intuitionistic logic;preference learning;priority inheritance;programming model;utility functions on indivisible goods	Shu-Ping Wan;Feng Wang;Gai-li Xu;Jiuying Dong;Jing Tang	2017	FO & DM	10.1007/s10700-016-9250-z	mathematical optimization;artificial intelligence;data mining;mathematics;algorithm	AI	-2.9026030327975785	-19.856368080067753	63079
3e8646b81bc69cd5e43e58ea93fd210428dc69d7	fussffra, a fuzzy semi-supervised forecasting framework: the case of the air pollution in athens		Mining hidden knowledge from available datasets is an extremely time-consuming and demanding process, especially in our era with the vast volume of high-complexity data. Additionally, validation of results requires the adoption of appropriate multifactor criteria, exhaustive testing and advanced error measurement techniques. This paper proposes a novel Hybrid Fuzzy Semi-Supervised Forecasting Framework. It combines fuzzy logic, semi-supervised clustering and semi-supervised classification in order to model Big Data sets in a faster, simpler and more essential manner. Its advantages are clearly shown and discussed in the paper. It uses as few pre-classified data as possible while providing a simple method of safe process validation. This innovative approach is applied herein to effectively model the air quality of Athens city. More specifically, it manages to forecast extreme air pollutants’ values and to explore the parameters that affect their concentration. Also it builds a correlation between pollution and general climatic conditions. Overall, it correlates the built model with the malfunctions caused to the city life by this serious environmental problem.	big data;cluster analysis;fuzzy logic;machine learning;process validation;semi-supervised learning;semiconductor industry;supervised learning	Ilias Bougoudis;Konstantinos Demertzis;Lazaros S. Iliadis;Vardis-Dimitris Anezakis;Andonis Papaleonidas	2017	Neural Computing and Applications	10.1007/s00521-017-3125-2	machine learning;semi-supervised learning;fuzzy logic;pollutant;mathematics;artificial intelligence;cluster analysis;big data;data mining;process validation;air quality index;pollution	ML	9.557064342793286	-20.242737009930945	63204
2d766959ab1a8d7dd8767857ff2b559ad76aa8f2	predicting golf ball trajectories from swing plane: an artificial neural networks approach		Quantifying and validating descriptive heuristic rules that govern someone‟s skills and expertise have been known philosophical quest since the early Greek philosophers. Inherent to sport coaching is the qualitative assessment of complex human motion patterns, relying on subjective and „hard-to-quantify‟ criteria that can be subject to experts/coaches disagreement. This paper presents an application of Artificial Neural Networks (ANN) for the discovery of predictive power of swing plane heuristic rules influencing golf ball trajectories. A golf data set (531 samples from 14 golfers) utilised in the experiments, was captured via ubiquitous computing device embedded in the handle of a driver club. Out of multiple swing performance factors influencing the ball trajectory, the selected subset of features for subspace modelling was linked only to the swing plane concept. Quantitative evidence supporting empirical coaching rules for swing plane assessment were obtained by supervised learning of ANN models. Optimised ANN models Radial Basis Function (RBF) and Support Vector Machine (SVM), were able to draw inference from captured swing data linking ball trajectories with variations of swing plane (with overall classification of 87%). The obtained swing plane computer model inference, data analysis and implemented concept of generic data export utility support kinesiology, golf coaching, inform club fitting, golf manufacturing technology and demonstrate new crossand multi-disciplinary integration of sport science, augmented coaching, ubiquitous computing, computational intelligence and the applications of expert systems for growing availability of sport, injury prevention/rehabilitation and golf related data sets.	addressing mode;approximation;artificial neural network;big data;command-line interface;computational intelligence;computer simulation;computer vision;console application;data acquisition;embedded system;experiment;expert system;ffmpeg;heuristic;heuristic (computer science);human factors and ergonomics;hume (programming language);image processing;kinesiology;linux;modal logic;negative base;neural networks;numerical weather prediction;open-source software;php;pattern recognition;personalization;precision and recall;radial (radio);radial basis function;real life;software development kit;supervised learning;support vector machine;swing (java);systems design;twisted nematic field effect;ubiquitous computing;video overlay;virtual machine;xfig	Boris Bacic	2016	Expert Syst. Appl.	10.1016/j.eswa.2016.07.014	simulation;artificial intelligence;machine learning	AI	7.879896625406259	-23.768861241648167	63286
a8751fe02c9d94a95090ebd7eff3a7faf4537a0c	behind-the-meter solar generation disaggregation using consumer mixture models		To facilitate deep penetration of solar energy in smart grids, we need high observability of solar generation at the edges of the grid. Current advanced metering infrastructures (AMI) only monitor the aggregated measurements from net-metered households, but disaggregated consumption and solar generation components are required for grid optimizations. We propose an unsupervised disaggregation model for disaggregating solar generation from AMI measurements without the need of training data. The model requires only AMI measurements from consumers in a region and the solar irradiance as input, and models the consumption of consumers by neighboring households without rooftop photovoltaics (PV) to perform the disaggregation. We evaluate our results on a real life dataset from Austin, Texas. We show that our model is able to disaggregate consumption and solar generation measurements with 42.24% and 31.67% less mean squared error, respectively, in comparison to a baseline technique that uses supervised learning. This shows that our model is capable of disaggregating historical data even if the dataset has no training data and only contains minimal exogenous data.		Chung Ming Cheung;Wen Zhong;Chuanxiu Xiong;Ajitesh Srivastava;Rajgopal Kannan;Viktor K. Prasanna	2018	2018 IEEE International Conference on Communications, Control, and Computing Technologies for Smart Grids (SmartGridComm)	10.1109/SmartGridComm.2018.8587539	supervised learning;real-time computing;grid;mixture model;solar irradiance;smart grid;solar energy;photovoltaics;computer science;mean squared error	Robotics	8.971656479292353	-16.321990665579765	63439
20cf640d524084f3cddac8e1bc1470fc8547887d	fuzzy t-filters and their properties	algebra	Abstract   Fuzzification of special types of filters on several different algebras of many-valued logics has been very popular in recent years. The main aim of this paper is to point out some general principles concerning particular results about fuzzification of special types of filters. We introduce the notion of a  fuzzy t-filter  which generalizes most types of special fuzzy filters (e.g. fuzzy implicative, fuzzy boolean, fuzzy fantastic, etc.) and prove some basic properties of fuzzy  t -filters.	t-norm fuzzy logics	Martin Vita	2014	Fuzzy Sets and Systems	10.1016/j.fss.2013.11.002	fuzzy logic;t-norm fuzzy logics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;pure mathematics;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	Logic	-0.14510299443691232	-23.439018537015507	63483
d7bbf4b3be00be05aa3fdb4e9b618bb06bea432c	inducing dynamic rules of nations' competitiveness from 2001-2005 mci-wcy	rule induction;regression model;nations competitiveness;major competitiveness indicators of world competitiveness yearbook mci wcy;optimization;neural network model;article;optimization model	"""A nation's competitiveness has become more and more important in forming government strategy and business decision making. This study proposes an optimization model, instead of regression model or neural network model, to induce rules for dynamic nations' competitiveness based on the Major Competitiveness Indicators of the World Competitiveness Yearbook. Fourteen attributes are used to form the dynamic rules expressed in """"IF…THEN…"""" forms. According to the induced rules, the strategic implications are suggested for various groups of nations to improve or to sustain their competitiveness."""		Han-Lin Li;Yu-Chien Ko	2009	International Journal of Information Technology and Decision Making	10.1142/S0219622009003557	economics;computer science;marketing;operations management;machine learning;international trade;economy;management;operations research;artificial neural network;regression analysis	DB	3.2510988921255617	-14.804885024863497	63705
b39c8821babbfe9210772ea7b9e6bce3267cb021	location model for cca-treated wood waste remediation units using gis and clustering methods	location models;geographic information system;cca treated wood waste;k means;land use;self organizing maps som;clustering method;self organized map;location model	In the next decades, a significant increase is expected in the amounts of CCA-treated wood waste that annually need to be properly disposed. This waste should be recycled only after its remediation, so planning and optimisation of the remediation units location is of major importance. A location model for CCA-treated wood waste was implemented using Geographic Information Systems (ArcGIS 8.2), with geographic information, namely land use information and the results of a questionnaire sent to Portuguese wood preservation industries. Two different clustering methods (Self-Organizing Maps and K-means) were tested in different conditions to solve the multisource Weber problem using SOMToolbox for MATLAB. The solutions obtained with the data and with both clustering methods could be used to decide on the location of these plants. SOM provided more robust and reproducible results than K-means, with the disadvantage of longer computing times. The main advantage of K-means, compared to SOM, is the reduced computing time (considering an average of all the runs, the K-means computing time is half the SOM computing time) together with the fact that it allows to obtain the best solutions in the majority of the cases, in spite of bigger variances and more geographical dispersion. 2007 Elsevier Ltd. All rights reserved.	arcgis;cluster analysis;geographic information system;k-means clustering;matlab;map;mathematical optimization;weber problem	Helena Gomes;Alexandra B. Ribeiro;Victor Sousa Lobo	2007	Environmental Modelling and Software	10.1016/j.envsoft.2007.03.004	land use;computer science;operations management;machine learning;geographic information system;operations research;k-means clustering	AI	7.342952475062742	-10.79132722289619	63833
ffb37e08371c4427923eacd28dd67eecc5793f6a	ranking with partial orders and pairwise comparisons	weak order;partial order	A new approach to Pairwise Comparisons based Ranking is presented. An abstract model based on partial orders instead of numerical scales is introduced and analysed. The importance of the concept of indifference and the power of weak order extensions are discussed.	numerical analysis;pairwise summation	Ryszard Janicki	2008		10.1007/978-3-540-79721-0_61	partially ordered set;combinatorics;discrete mathematics;computer science;mathematics;statistics	AI	0.04912509176397104	-20.183121258875712	63867
73347c2045ef0670646048cd2cec663a86c10765	estimation of arx parametric model in regional economic systems	nonlinear model;arx117 model;system identification method;arx model;regional economic system;present model;arx parametric model;regional economic system model;nonlinear system;economic system model;regional authority;system identification;regional economics;parametric model	The purpose of this paper is to show how an economic system model can be selected that would allow a regional authority to control the economy of a region. This paper presents a new idea of system identification method based on fuzzy evaluation logic. It applies parametric estimation in analyzing the data of the inputs and the outputs in a nonlinear system, and then selects a regional economic system model from several nonlinear models by means of the system identification method. Due to the lower explanatory power of the present models, this paper focuses on the shortcomings of the present model based on a simple mechanism methodology which has a complicated structure and lots of variables. An alternative is to construct an ARX model with systematic consideration. The advantage of ARX model is simple, with few variables as well as being actually data-based. The result is that ARX117 model is the best one, and the IV Estimation Method is the one we suggest for use. Since this ARX model can reflect the complex characteristics of a regional economic system by integration of expertsu0027 knowledge, we can design the most reasonable fuzzy controller for a regional economic system. © 2007 Wiley Periodicals, Inc. Syst Eng 10: 290–296, 2007rnrnSupported by Liaoning Education Department funds (Project No. 05w028)		Shi Zheng;Wen Zheng;Xia Jin	2007	Systems Engineering	10.1002/sys.20077	fuzzy logic;input/output;parametric model;systems modeling;system identification;nonlinear system;computer science;engineering;artificial intelligence;economic model;operations management;knowledge engineering;operations research;expert system	DB	5.796302701404411	-21.042015982127108	64116
f8a2fdff7bdd8ce326f35fee6c64d63388bb67e1	strategic assessment of business	stock exchange activity;minimisation;financial management;investment actions;risk analysis;discounted cash flow decision making linguistic information strategic valuation;uncertainty;risk and uncertainty;data mining;financial analysts;investment;stock markets;stock exchange;cost accounting;open wireless architecture;linguistic information;calculus;strategic valuation;qualitative information;stock markets financial management investment minimisation risk analysis;cost accounting stock markets uncertainty instruments layout computer science risk analysis intelligent systems application software investments;stock market volatility;risk minimization;qualitative information business strategic assessment stock market volatility stock exchange activity financial analysts investment actions risk minimization quantitative information;discounted cash flow;business strategic assessment;quantitative information	The stock market volatility and the actual stock exchange activity have increased the need of counting with effective methods on the part of financial analysts to achieve a division in relation to the investment actions, being also growing the demand of methodological instruments that reduce and minimize the risks and uncertainty when valuating financial actives and companies. These systems not only must use quantitative information but the inclusion of qualitative information must also bear heavily on them, as an improvement element in the adjustment of these valuating methods, with the aim of throwing a more well-conceived or less mistaken decision. In this work, we present an alternative strategic assessment of business based in quantitative information.	effective method;value (ethics);volatility	Jesús M. Doña;José Ignacio Peláez;Luis G. Vargas	2009	2009 Ninth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2009.95	financial management;minimisation;stock exchange;risk analysis;uncertainty;investment;cost accounting	Robotics	0.6090082807498032	-12.130477832199478	64122
623bcedc0acfc93a805384148e6e7aee47ac65d7	gaussian process regression based traffic modeling and prediction in high-speed networks	gaussian processes;prediction algorithms;ground penetrating radar;mathematical model;predictive models;adaptation models;biological neural networks	Evolving nature of network traffic challenges existing models to fit and predict its behavior. In particular, real traffic modeling requires more flexible design that can adapt to long-range and short-range dependent traffic with dynamic patterns. Unfortunately, existing models cannot handle such requirements because various traffic behaviors such as periodic and self-similar are not taken into account. In this paper, Gaussian process regression (GPR) is adapted for traffic modeling and prediction. The connection between self-similarity as a traffic characteristic and GPR parameters has been driven and exerted to build of a new Hurst estimation method based on machine learning techniques. This led to propose self-similar covariance functions for enhancing prediction accuracy of GPR. The proposed GPR model has been applied for Hurst estimation as well as for traffic prediction on real traffic traces at different time-scales. The experimental results show the employment of self-similar covariance functions increases generalization ability of GPR for traffic modeling and prediction.	gaussian process;hurst exponent;kernel (operating system);kriging;machine learning;model selection;network traffic control;quasiperiodicity;requirement;spss;self-similarity;tier 1 network;time-scale calculus;tracing (software);traffic analysis	Abdolkhalegh Bayati;Vahid Asghari;Kim Khoa Nguyen;Mohamed Cheriet	2016	2016 IEEE Global Communications Conference (GLOBECOM)	10.1109/GLOCOM.2016.7841857	traffic generation model;simulation;ground-penetrating radar;prediction;artificial intelligence;machine learning;mathematical model;gaussian process;predictive modelling;statistics	ML	9.826225240785181	-21.371164771640775	64215
a02ecba3e3dce5866d82e7f324072c531dba019c	power geometric operators of trapezoidal intuitionistic fuzzy numbers and application to multi-attribute group decision making	power geometric operator;trapezoidal intuitionistic fuzzy number;multi attribute group decision making;weighted possibility mean	The weighted possibility means of TrIFNs are firstly introduced.A new ranking method for TrIFNs is thereby presented.Minkowski distance for TrIFNs is defined.Four kinds of power geometric operators of TrIFNs are developed.The collective overall values of alternatives are obtained by goal programming model. As a special intuitionistic fuzzy set on a real number set, trapezoidal intuitionistic fuzzy numbers (TrIFNs) have the better capability to model ill-known quantities. The purpose of this paper is to develop some power geometric operators of TrIFNs and apply to multi-attribute group decision making (MAGDM) with TrIFNs. First, the lower and upper weighted possibility means of TrIFNs are introduced as well as weighted possibility means. Hereby, a new lexicographic method is developed to rank TrIFNs. The Minkowski distance between TrIFNs is defined. Then, four kinds of power geometric operators of TrIFNs are investigated including the power geometric operator of TrIFNs, power weighted geometric operator of TrIFNs, power ordered weighted geometric operator of TrIFNs and power hybrid geometric operator of TrIFNs. Their desirable properties are discussed. Four methods for MAGDM with TrIFNs are respectively proposed for the four cases whether the weight vectors of attributes and DMs are known or unknown. In these methods, the individual overall attribute values of alternatives are generated by using the power geometric or power weighted geometric operator of TrIFNs. The collective overall attribute values of alternatives are determined through constructing the multi-objective optimization model, which is transformed into the goal programming model to solve. Thus, the ranking order of alternatives is obtained according to the collective overall attribute values of alternatives. Finally, the green supplier selection problem is illustrated to demonstrate the application and validation of the proposed method.		Shu-Ping Wan;Jiu-Ying Dong	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.12.031	mathematical optimization;combinatorics;discrete mathematics;mathematics	ECom	-2.474621685787533	-20.618495169698317	64302
1679e914c598b85be0600c3b48e0f036802dee6d	pca based bridge health model identification	health monitoring;bridge;pca;health model	Abstract Structure safety assessment is one of the fundamental objectives for bridge health monitoring. In order to identify whether a bridge is safe, a bridge health model identification method based on Principle Component Analysis (PCA) is presented in this paper. A matrix made up of vazious measurement data of a bridge is constituted, the method of determine the similazity of bridges using the eigenvalue distance between data matrixes of bridges is addressed, and some issues related to the collection of the measurement data and the establishments of data model are also discussed.	system identification	Zhangli Lan;Lina Xiang;Jianqiu Cao	2010	Intelligent Automation & Soft Computing	10.1080/10798587.2010.10643115	computer science;machine learning;data mining;bridge;principal component analysis	AI	7.097246045203521	-14.340514613428725	64400
76c7dafcfb53df88141e4e3cb5a78b7c956347f4	towards quantification of incompleteness in the pairwise comparisons method		Alongside consistency, completeness of information is one of the key factors influencing data quality. The objective of this paper is to define ways of treating missing entries in pairwise comparisons (PC) method with respect to inconsistency and sensitivity. Two important factors related to the incompleteness of PC matrices have been identified, namely the number of missing pairwise comparisons and their arrangements. Accordingly, four incompleteness indices have been developed, simple to calculate, each of them take into account both: the total number of missing data and their distribution in the PC matrix. A numerical study of the properties of these indices has been also conducted using a series of Montecarlo experiments. It demonstrated that both incompleteness and inconsistency of data equally contribute to the sensitivity of the PC matrix. Although incompleteness is only just one of the factors influencing sensitivity, a relative simplicity of the proposed indices may help decision makers to quickly estimate the impact of missing comparisons on the quality of final result.		Konrad Kulakowski;Anna Prusak;Jacek Szybowski	2018	CoRR			ML	-4.479482848489405	-19.563938039809003	64457
14dfd653435d001b0f6b16ba67bac38b3dcfdf79	approximation reduction in inconsistent incomplete decision tables	discernibility function;rough set theory;journal;attribute reduction;function approximation;maximal consistent block;approximation reduction;decision table;inconsistent incomplete decision table	0950-7051/$ see front matter 2010 Elsevier B.V. A doi:10.1016/j.knosys.2010.02.004 * Corresponding author. Address: Key Laboratory o and Chinese, Information Processing of Ministry of China. E-mail addresses: jinchengqyh@126.com (Y. Qian ljdy@sxu.edu.cn (D. Li), sxuwangfeng@126.com (F. W (N. Ma). This article deals with approaches to attribute reductions in inconsistent incomplete decision table. The main objective of this study is to extend a kind of attribute reductions called a lower approximation reduct and an upper approximation reduct, which preserve the lower/upper approximation distribution of a target decision. Several judgement theorems of a lower/upper approximation consistent set in inconsistent incomplete decision table are educed. Then, the discernibility matrices associated with the two approximation reductions are examined as well, from which we can obtain approaches to attribute reduction of an incomplete decision table in rough set theory. 2010 Elsevier B.V. All rights reserved.	approximation;decision table;information processing;rendering (computer graphics);rough set;set theory	Yuhua Qian;Jiye Liang;Deyu Li;Feng Wang;Nannan Ma	2010	Knowl.-Based Syst.	10.1016/j.knosys.2010.02.004	decision table;mathematical optimization;rough set;function approximation;computer science;machine learning;algorithm	DB	-0.5561543138755133	-20.687991834928845	64477
11890fe1331c792c0ba2cd238cf8e55fd2cba1e2	smart meter - artificial neural network for disaggregation of electrical appliances		Goal of that paper is to show a possibility for the disaggregation of electrical appliances in the load curve of residential buildings. The advantage is that the measurement system is at a central point in the household. So the installation effort decrease. For the disaggregation of the appliances out of the load curve, an approach for the development of classification algorithms is presented. One method for the classification of appliances is to use Artificial Neural Network. This idea is the main part of that paper. It is shown a method, to classify one kind of appliances. At the end, the first relsults and the next steps are presented. The disaggregation of the appliances is part of a research project at the University of Furtwangen.	algorithm;artificial neural network;load profile;simulation;smart meter;system of measurement	Dirk Benyoucef;Thomas Bier;Philipp Klein	2012			embedded system;computer science;artificial neural network;smart meter	AI	10.01290501361352	-16.145090926904874	64513
943cdb26702d7ff64a011884a5c76c01a5841381	cuts of if-sets respecting fuzzy connectives	truth value;fuzzy logic;if-set a;chosen type;intuitionistic fuzzy set;fuzzy connective;crisp set;suitable tool	"""Intuitionistic fuzzy sets (IF-sets) are a suitable tool to describe cases where it is useful to account not only the grade of membership to a collection, but also the grade of its non-membership. We consider the a-cuts of an IF-set A as crisp sets consisting of those elements x for which the truth value (in fuzzy logic) of the statement """"x belongs to A and it is not true that x does not belong to A"""" is at least α. We describe properties of such cuts depending on the chosen type of conjunction and negation."""	fuzzy logic;logical connective	Davide Martinetti;Vladimír Janis;Susana Montes	2011		10.1007/978-3-642-23713-3_5	fuzzy logic;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;fuzzy set;fuzzy set operations;algorithm	NLP	-1.1098823628956782	-23.052221112689157	64662
c52757535c732765ed27794463b98b1e2432aac5	fuzzy structures of hyper-mv-deductive systems in hyper-mv-algebras	hyper mv algebra;weak strong fuzzy hyper mv deductive system;previously weak fuzzy hyper mv deductive system;strong fuzzy hyper mv deductive system;strong hyper mv deductive system;weak strong hyper mv deductive system;weak	The notions of fuzzy hyper-MV-subalgebras, (weak, strong) fuzzy hyper-MV-deductive systems and previously weak fuzzy hyper-MV-deductive systems are introduced, and their relations/properties are investigated.	mv-algebra	Young Bae Jun;Min Su Kang;Hee Sik Kim	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2010.02.016	fuzzy logic;discrete mathematics;mathematical analysis;mathematics	Arch	0.5873947220355271	-23.333128737475594	65018
581fc96f110b73fa77055e4bb9c9afa5ef6b918a	modifiers based on some t-norms in fuzzy logic	t conorm;fuzzy set;modifier;fuzzy logic;modifier logic;demorgan class of operators;membership function;t norm	Modifiers generated by  n -placed functions are considered. The subject matter of modifiers are fuzzy sets, i.e. membership functions defined on the unit interval  I  = [0, 1]. Two sets of modifiers are considered as example cases. One of them is a set of modifiers generated by t-norms and t-conorms. Here different dual pairs of norms create modifiers of different grade of strength. These norms are examples of two-placed functions. Another case is to generate a series of modifiers using only one DeMorgan class of norms. Norms are generalized to be  n -placed functions. The place number  n  takes effect to the strength of a modifier. Two different DeMorgan classes are taken into the consideration. The first steps to the direction of many-valued modifier logics are taken.	fuzzy logic;t-norm	Jorma K. Mattila	2004	Soft Comput.	10.1007/s00500-003-0323-x	fuzzy logic;discrete mathematics;membership function;computer science;artificial intelligence;mathematics;t-norm;fuzzy set;algorithm	Logic	0.24792523367365038	-23.632388319782645	65144
74d96cf16deb4d02f5c0e8258c2ed8e407a0dbd0	interpolation of fuzzy data: analytical approach and overview	fuzzy space;fuzzy data;articulo;similarity;interpolation of fuzzy data;fuzzy function;analytic solution;conference proceeding;interpolating fuzzy function	We propose a general framework for the interpolation problem. Our framework stems from the classical elaboration of the problem. We introduce the notion of an interpolating fuzzy function and show how this function can be characterized. We examine and analyze previously published fuzzy interpolation algorithms to choose those algorithms that can be represented analytically. We also propose an analytic solution of the interpolation problem that unifies various algorithmic approaches. © 2010 Elsevier B.V. All rights reserved.	algorithm;analytic signal;centrality;computation;emoticon;fuzzy logic;fuzzy set;interpolation;maxima and minima;social inequality	Irina Perfilieva;Didier Dubois;Henri Prade;Francesc Esteva;Lluis Godo;Petra Hodáková	2012	Fuzzy Sets and Systems	10.1016/j.fss.2010.08.005	fuzzy logic;closed-form expression;mathematical optimization;discrete mathematics;bilinear interpolation;similarity;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;interpolation;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy associative matrix;fuzzy set operations	Logic	-0.5233636211081073	-21.80372228509416	65220
02f7edb5708b68a02f3c21e14249a2318880b60e	a fuzzy set-based accuracy assessment of soft classification	soft classifiers;fuzzy set;error matrix;grade of membership;accuracy measures;accuracy assessment;fuzzy set theory;fuzzy sets theory;reference data	Despite the sizable achievements obtained, the use of soft classi®ers is still limited by the lack of well-assessed and adequate methods for evaluating the accuracy of their outputs. This paper proposes a new method that uses the fuzzy set theory to extend the applicability of the traditional error matrix method to the evaluation of soft classi®ers. It is designed to cope with those situations in which classi®cation and/or reference data are expressed in multimembership form and the grades of membership represent dierent levels of approximation to intrinsically vague classes. Ó 1999 Elsevier Science B.V. All rights reserved.	approximation;category theory;confusion matrix;fuzzy set;matrix method;qualitative comparative analysis;set theory;the matrix;vagueness	Elisabetta Binaghi;Pietro Alessandro Brivio;Paolo Ghezzi;Anna Rampini	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00061-6	membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations	AI	-2.8503976689665538	-23.65339626512657	65416
c2971f007ec17c0008f98602adc041bcd9dc93e7	an extension of the analytic hierarchy process method under the belief function framework		In this paper, an extension of the belief Analytic Hierarchy Process (AHP) method is proposed, based on the belief function framework. It takes into account the fact that the pair-wise comparison between criteria and alternatives may be uncertain and imprecise. Therefore, it introduces a new way to cope with expert judgments. Thus to express his preferences, the decision maker is allowed to use a belief assessment instead of exact ratios. The proposed extension also models the relationship between the alternative and criterion levels through conditional beliefs. Numerical examples explain in detail and illustrate the proposed approach.	numerical method	Amel Ennaceur;Zied Elouedi;Eric Lefevre	2014		10.1007/978-3-319-08852-5_24	belief structure;artificial intelligence;mathematics;management science	AI	-3.619843975459642	-19.47165875202201	65614
1cf31c043fb7d6c73ddff499dc00dd3310ae1fbb	decision support systems in multicriteria groups: an approach based on fuzzy rules	group decision support systems decision making fuzzy set theory;fuzzy rules;rule based;influence degrees;decision maker;companies;fuzzy set theory;automatic generation;fuzzy sets;decision support system;mathematical model equations fuzzy sets decision making aggregates proposals companies;fuzzy rule base;group decision support systems;aggregates;automatic generation of fuzzy rules fuzzy multicriteria group decision making fmcgdm;decision support systems;automatic generation of fuzzy rules;mathematical model;importance degrees decision support systems multicriteria groups fuzzy rules group decision making influence degrees;group decision making;proposals;multicriteria groups;fuzzy multicriteria group decision making fmcgdm;importance degrees	Problems with multicriteria characteristics that involve group decision making usually require the treatment of conflicts within each group. This paper presents a method capable of treating this class of problems by using a fuzzy rules base with multicriteria characteristics. In this method, different influence degrees for each decision maker and various importance degrees assigned by them to each variable analyzed are considered. The inference is made through an automatically generated rules base.	decision support system;fuzzy rule;knowledge base	Fabio Jose Justo dos Santos;Heloisa A. Camargo	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584346	decision support system;computer science;artificial intelligence;machine learning;data mining;mathematics;management science;fuzzy set	SE	-4.172364704831954	-20.671425480182133	65845
10087dfa48bf1728407372b20f4c12bce33b1f8c	some continuous aggregation operators with interval-valued intuitionistic fuzzy information and their application to decision making	intuitionistic fuzzy sets;multicriteria;information theory	In this paper, some new operators for aggregating interval-valued intuitionistic fuzzy information are proposed to deal with multiple attribute decision making problems. Firstly, the C-IFOWA operator and C-IFOWG operator are developed to aggregate all the values in the interval-valued intuitionistic fuzzy numbers. Some of their desirable properties are also studied. Secondly, in order to aggregate a set of interval-valued intuitionistic fuzzy numbers, some new aggregation operators are proposed based on the C-IFOWA operator and C-IFOWG operator. Thirdly, two methods for multiple attribute decision making, in which the attribute values are given in the forms of interval-valued intuitionistic fuzzy numbers are presented. Finally, two numerical examples are provided to illustrate the practicality and validity of the proposed methods.		Jian Lin;Qiang Zhang	2012	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488512500092	discrete mathematics;information theory;computer science;fuzzy number;data mining;mathematics;fuzzy set operations;statistics	Robotics	-2.5915838449425803	-20.946407739971942	66146
f79c7927a1190851830532073f4e2808f173c10b	two-dimensional discrete fuzzy numbers and applications	fuzzy numbers;discrete fuzzy numbers;journal article;weak orders;ranking;college of science and engineering;0802 computation theory and mathematics;期刊论文;probability functions	In this paper, the concept of two-dimensional discrete fuzzy numbers is presented based on the idea of representation theorem of one-dimensional discrete fuzzy numbers. Then a sufficient condition is proposed, which makes a fuzzy set on R2 become a two-dimensional discrete fuzzy number, and the representations of joint membership function and the two edge membership functions of two-dimensional discrete fuzzy number are given. Some special two-dimensional discrete fuzzy numbers are also defined. And then some weak orders in the two-dimensional discrete fuzzy number space are set up based on the mass centers and the degree of ambiguities of two-dimensional discrete fuzzy numbers, and their properties are also investigated. At last, some practical examples are included to demonstrate the effectiveness and potential of the theoretic results obtained.		Guixiang Wang;Peng Shi;Yunyan Xie;Yan Shi	2016	Inf. Sci.	10.1016/j.ins.2015.07.045	fuzzy logic;combinatorics;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;ranking;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;discrete system;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;statistics	Logic	-0.972518516170297	-22.595925592705626	66537
544b774d01210fbfe33094fcd86e56d0339edb73	probabilistic neural network model based on wavelet and partical swarm optimization	partical swarm optimization exchange rate forecast probabilistic neural network wavelet;vector dimensionality probabilistic neural network model wavelet analysis partical swarm optimization foreign exchange market exchange rate formation mechanism exchange rate volatility nonlinear system forecast;foreign exchange market;neural nets;exchange rates probabilistic logic wavelet transforms noise reduction accuracy particle swarm optimization noise;exchange rates;particle swarm optimisation exchange rates forecasting theory neural nets;wavelet transform;particle swarm optimizer;forecasting theory;noise reduction;exchange rate;exchange rate volatility;probabilistic logic;nonlinear system;probabilistic neural network;particle swarm optimisation	Foreign exchange market is a complex market, with a high degree of volatility characteristics. Exchange rate formation mechanism and the factors affecting exchange rate volatility are also very complex, which is a nonlinear system. It is difficult to accurately forecast. Probabilistic neural network is applied to the frontiers of forecast, and aimed at the characteristics of probabilistic neural network to pretreat the exchange of data and forecast the tendency. And by changing the vector dimensionality experiment we obtain the best entry to embed dimensionality, tested and improved the precise prediction and valuable.	artificial neural network;mathematical optimization;network model;probabilistic neural network;swarm;wavelet	Hua Wang;Bingxiang Liu;Xiang Cheng	2011		10.1109/BMEI.2011.6098683	financial economics;probabilistic neural network;nonlinear system;computer science;artificial intelligence;machine learning;noise reduction;foreign exchange market;probabilistic logic;artificial neural network;wavelet transform	Robotics	7.32860477294779	-19.133121855788293	66639
15ab981d35f8f5e811a2d35b9ecc11a6ad31aa6f	interval-valued intuitionistic fuzzy implications	aggregation function;fuzzy set;expert systems;uncertainty;niobium;mutual dual functions interval valued intuitionistic fuzzy implications interval valued aggregation function;fuzzy set theory;fuzzy sets;fuzzy set theory fuzzy logic;fuzzy logic;intervalvalued intuitionistic implications;interval aggregation functions;cognition;fuzzy logic fuzzy sets niobium cognition context uncertainty expert systems;interval aggregation functions interval valued intuitionistic fuzzy logic intervalvalued intuitionistic implications;interval valued intuitionistic fuzzy logic;context;expert system	The paper presents a general expression of an interval-valued intuitionistic fuzzy implication generated by interval-valued aggregation function acting on a pair of mutual dual functions, named interval-valued fuzzy implications and coimplications, extending the work introduced in [1].	intuitionistic logic	Lidiane Visintin;Renata Hax Sander Reiser;Benjamín R. C. Bedregal	2011	2011 Workshop-School on Theoretical Computer Science	10.1109/WEIT.2011.22	mathematical analysis;discrete mathematics;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;fuzzy set operations;algorithm	Logic	-1.7099654090032699	-22.384260054503393	67068
59a2a3a1744b31be7634060147008ca57531914d	fuzzy location problems on networks	engineering;location problem;probleme localisation;fuzzy set;procesamiento informacion;conjunto difuso;ensemble flou;ingenierie;information processing;ingenieria;sistema difuso;problema localizacion;systeme flou;traitement information;fuzzy system	Location problems concern a wide set of /elds where it is usually assumed that exact data are known. However, in real applications, the location of the facility considered can be full of linguistic vagueness, that can be appropriately modelled using networks with fuzzy values. Thus fuzzy location problems on networks arise; this paper deals with their general formulation and the description of the ways to solve them. Namely, we show the variety of problems that can be considered in this context and, for some of them, we propose the most operative approaches for their solution. c © 2003 Elsevier B.V. All rights reserved.	fuzzy logic;norm (social);shortest path problem;vagueness	José A. Moreno-Pérez;J. Marcos Moreno-Vega;José L. Verdegay	2004	Fuzzy Sets and Systems	10.1016/S0165-0114(03)00091-5	information processing;computer science;artificial intelligence;mathematics;fuzzy set;operations research;algorithm;fuzzy control system	AI	0.6928937205627496	-17.73783596422579	67121
f3b5aca87e60b521f0e24dec36d39522e13df7e8	many valued algebraic structures as the measures for comparison	vaitoskirja;operators;expert systems;medical data;comparison;classification;aggregation;fuzzy logic;doctoral dissertation;adaptive weighting;comparison measures	In this chapter we will study properties and usability of basic many valued structures called t-norms, t-conorms, implications and equivalences in comparison tasks. We will show how these measures can be aggregated with generalized mean and what kind of measures for comparison can be achieved from this procedure. New classes for comparison measures are suggested, which are combination measure based on the use of t-norms and t-conorms and pseudo equivalence measures based on S-type implications.#R##N##R##N#In experimental part of this chapter we will show how some of the comparison measures presented here work in comparison task. For comparison task we use classification. We show by comparison to results that can be achieved through some known public domain classifier results that our classification results are highly competitive.		Kalle Saastamoinen	2006			discrete mathematics;machine learning;mathematics;algorithm	NLP	-2.679778116417435	-22.50263944283295	67185
86c117abdd8771a6eaefec51a7be53d5c0bc4140	multiplicative aggregation of division efficiencies in network data envelopment analysis		Abstract Network systems have two basic structures, series and parallel, and a network system can be transformed into a series system of subsystems, where each has a parallel structure composed of a number of divisions. The efficiency of the system can thus be expressed as the product of those of the subsystems, and the efficiency of each subsystem is a weighted average of those of its component divisions, under a relational data envelopment analysis (DEA) model. A previous study showed that the efficiency of a network system can be expressed as an additive aggregation of those of the divisions adjusted by a factor, and the former is bounded from above by the latter. This paper shows that the efficiency of the system can be expressed as a multiplicative aggregation of those of the divisions multiplied by a factor of greater than one. The system efficiency is thus bounded from below by the multiplicative aggregation of the division efficiencies. An example is used to show how to calculate the efficiency bounds, and how to identify the divisions that have stronger effects on the performance of the system.	data envelopment analysis	Chiang Kao	2018	European Journal of Operational Research	10.1016/j.ejor.2017.09.047	mathematical optimization;mathematics;relational database;multiplicative function;envelopment;bounded function;data envelopment analysis;weighted arithmetic mean	Vision	-1.1916060420325465	-20.751985848657913	67382
ff4544c904e08abc18cd1ab27ac80f778c8d1ee0	competitive analysis for the most reliable path problem with online and fuzzy uncertainties	journal;online fuzzy algorithm;most reliable path;competitive analysis;competitive ratio	Based on some results of the fuzzy network computation and competitive analysis, the Online Fuzzy Most Reliable Path Problem (OFRP), which is one of the most important problems in network optimization with uncertainty, has been originally proposed by our team. In this paper, the preliminaries about fuzzy and the most reliable path and competitive analysis are given first. Following that, the mathematical model of OFRP, in which two kinds of uncertainties, namely online and fuzzy, are combined to be considered at the same time, is established. Then some online fuzzy algorithms are developed to address the OFRP and the rigorous proofs of the competitive analysis are given in detail. Finally, some possible research directions of the OFRP are discussed and the conclusions are drawn.	competitive analysis (online algorithm);computation;mathematical model;mathematical optimization;online algorithm	Weimin Ma;Shao-Hua Tang;Ke Wang	2008	IJPRAI	10.1142/S0218001408006156	competitive analysis;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;machine learning;data mining;fuzzy set operations	Theory	-0.755161543515258	-19.741083706798328	67434
8f271747716972c539ebffdba0eaa77c14ad67e1	consensus models based on distance for interval fuzzy and multiplicative preference relations		This paper presents consensus models based on distance for group decision making problems under interval fuzzy and multiplicative preference relations. First, some quadratic programming models based upon the idea of minimizing the sum of squared distances between all pairs of weighted interval fuzzy or multiplicative preference judgments are developed to obtain the weights of experts. Then, two indices, an individual to group consensus index (ICI) and a group consensus index (GCI) are defined. Furthermore, iterative consensus algorithms are proposed and the processes stop until both the ICI and GCI are less than predefined thresholds or reaching the maximum number of iteration. Finally, two illustrative examples are given to demonstrate the feasibility and effectiveness of the proposed methods.		Yejun Xu;Jing Zhang;Huimin Wang	2016	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-162163	mathematical optimization;discrete mathematics;machine learning;mathematics	NLP	-2.7840796916154678	-20.282131903112017	67498
231ed341c32f7fb866cfcf12ead2aa338a929496	modeling attitudes towards uncertainty and risk through the use of choquet integral	choquet expected utility;stochastic dominance;risk aversion;uncertainty aversion;first order;choquet integral;risk	The aim of this paper is to present in a unified framework a survey of some results related to Choquet Expected Utility (CEU) models, a promising class of models introduced separately by Quiggin [35], Yaari [48] and Schmeidler [40, 41] which allow to separate attitudes towards uncertainty (or risk) from attitudes towards wealth, while respecting the first order stochastic dominance axiom.		Alain Chateauneuf	1994	Annals OR	10.1007/BF02032158	financial economics;mathematical optimization;actuarial science;risk aversion;economics;ambiguity aversion;stochastic dominance;first-order logic;risk;choquet integral;welfare economics	Robotics	-1.68960522876942	-12.625158699533733	67499
b90e519b1e0d8d87d06d296249d7dff187c6ed2c	line integrals of intuitionistic fuzzy calculus and their properties		Atanassov's intuitionistic fuzzy set (A-IFS) is a powerful tool to handle uncertainty and vagueness in real life. The basic elements of an A-IFS are intuitionistic fuzzy values, based on which the intuitionistic fuzzy calculus (IFC) has been proposed recently. However, to date, there is no investigation for intuitionistic fuzzy line integrals (IFLIs), which are very important for further developing IFC. In this paper, we propose the IFLIs and give their concrete values. After that, we investigate a series of basic properties of the IFLIs in detail, moreover, in order to show the utility of the proposed IFLIs, we offer an example, and finally, we discuss the relationships among the additive IFLI, the multiplicative IFLI, and the intuitionistic fuzzy aggregation operators.	fuzzy set;intuitionistic logic;real life;utility functions on indivisible goods;vagueness	Zhenghai Ai;Zeshui Xu	2018	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2017.2724502	fuzzy logic;calculus;discrete mathematics;operator (computer programming);mathematics;fuzzy set;multiplicative function;line integral;time-scale calculus;vagueness	SE	-1.5977277269596084	-21.86976090092348	67503
b2ad828c159d793ed39d85593fcd64cefd9b2cf3	dimension of valued relations	graph theory;fuzzy set;fuzzy relation;dimension;weak order;social science;binary relation;transitive closure;combinatorial analysis;utility theory;partial order	The classical notion of dimension of a partial order can be extended to the valued setting, as was indicated in a particular case by Ovchinnikov (1984) (Ovchinnikov, S.V., 1984. Representations of transitive fuzzy relations. In: Skala, H.J., Termini, S., Trillas, E. (Eds.), Aspects of vagueness. Reidel, Boston, pp. 105–118). Relying on Valverde's result (1985) (Valverde, L., 1985. On the structure of F-indistinguishability operators. Fuzzy Sets and Systems 17, 313–328) on the transitive closure of a valued relation, we define the dimension of a valued quasi order. Building then on Fodor and Roubens (1995) (Fodor, J., Roubens M., 1995. Structure of transitive valued binary relations. Mathematical Social Sciences, 30, 71–94), we also show that the definition can be generalized to all valued relations by using valued biorders instead of valued weak orders as one-dimensional relations. Interesting, combinatorial questions about the new dimension concept arise and are investigated here. In particular, we aim at a characterization of valued quasi orders of dimension two.		Jean-Paul Doignon;Jutta Mitas	2000	European Journal of Operational Research	10.1016/S0377-2217(99)00268-4	partially ordered set;mathematical optimization;combinatorics;discrete mathematics;graph theory;binary relation;mathematics;fuzzy set;transitive relation;dimension;transitive closure;utility	Theory	1.09442866257453	-19.948519601638164	67565
e8db4c00f0576dd0929ac1a56d8d471d38822525	data imputation for gas flow data in steel industry based on non-equal-length granules correlation coefficient	data imputation;non equal length granules correlation coefficient;estimation of distribution algorithm;byproduct gas of steel industry	In the field of data-driven based modeling and optimization, the completeness and the accuracy of data samples are the foundations for further research tasks. Since the byproduct gas system of steel industry is rather complicated and its data-acquisition process might be frequently affected by the unexpected operational factors, the data-missing phenomenon usually occurs, which might lead to the failure of model establishment or inaccurate information discovery. In this study, a data imputation method based on the manufacturing characteristics is proposed for resolving the data-missing problem in steel industry. A novel correlation analysis, named by non-equal-length granules correlation coefficient (NGCC), is reported, and the corresponding model based on Estimation of Distribution Algorithm (EDA) is established to study the correlation of the similar procedures. To verify the performance of the proposed method, this study considers three typical features of the gas flow data with different missing ratios. The experiment results indicate that it is greatly effective for the missing data imputation of byproduct gas, and exhibits better performance on the accuracy compared to the other methods. © 2016 Elsevier Inc. All rights reserved.	coefficient;estimation of distribution algorithm;futures studies;geo-imputation;information discovery;local optimum;mathematical optimization;missing data;steel;systems modeling;time series	Zheng Lv;Ying Liu;Wei David Wang	2016	Inf. Sci.	10.1016/j.ins.2016.05.046	econometrics;estimation of distribution algorithm;computer science;artificial intelligence;machine learning;data mining;statistics	ML	6.380986628553205	-20.83475671930901	67868
345740e98045dbac2940c8e92280691744f6c665	green degree of the smart object in iot and its measure method	pollution fuzzy set theory green computing internet of things;mathematics;green products;iot;pollution measurement;power efficiency;fuzzy set theory;medium mathematics;internet of things;electromagnetic pollution;green products pollution measurement pollution electromagnetics power demand mathematics power measurement;electromagnetics;smart object;green it;power demand;distance ratio mean function measure method iot green it environment impact evaluation green degree of smart object fuzzy problem measure of medium truth degree mmtd medium mathematics mm two dimensional quantification mapping green features power efficiency electromagnetic pollution index;green computing;power measurement;green degree;electromagnetic pollution green it iot smart object green degree medium mathematics power efficiency;pollution	The attention over Green IT keeps rising. In this paper, an attempt to build a measure method for evaluating environment impact of IoT is made. First, the green degree of the smart object in IoT is defined and studied as a fuzzy problem. Then the measure of medium truth degree (MMTD) based on the medium mathematics (MM) is used to build the two dimensional quantification mapping of the smart object on its green features, including the power efficiency and the electromagnetic pollution index. Next, the distance ratio mean function is given as a measure method to the green degree of the smart object. Finally, an example is given to show how to measure the green degree of a smart object in a tiny IoT.	performance per watt;smart objects	Yulong Deng	2012	4th IEEE International Conference on Cloud Computing Technology and Science Proceedings	10.1109/CloudCom.2012.6427559	green computing;simulation;computer science;internet of things	Robotics	-0.3760407549725467	-22.58902191863671	67871
7faecc9a9dd88b9d7f2f170105483c9c90d9c6d0	rough sets based on complete completely distributive lattice	complete completely distributive lattice;approximation operator;期刊论文;binary relation;rough set	In this paper, a pair of rough approximation operators on a complete completely distributive (CCD) lattice based on an ordinary binary relation is defined. This kind of rough sets can be seen as a unified framework for the study of rough sets based on ordinary binary relations, rough fuzzy sets and interval-valued rough fuzzy set. Moreover, depending on classes of binary relations, this paper defines several classes of rough sets on CCD lattices and investigates properties of these classes. Finally, two generalized rough set models on two CCD lattices are given at the end of this paper.	rough set	Ning Lin Zhou;Bao Qing Hu	2014	Inf. Sci.	10.1016/j.ins.2013.12.035	distributive lattice;mathematical analysis;discrete mathematics;rough set;topology;computer science;machine learning;binary relation;mathematics	NLP	-0.800891960316014	-23.098674876941875	67887
0614903451cb23cd0bbe7d889627eaae12bd1ae2	applying systematic diagnosis and product classification approaches to solve multiple products operational issues in shop-floor integration systems	shop floor integration system;support vector machines;case base reasoning;integrable system;support vector;system diagnostic;multivariate statistics;support vector machine;information system;case based reasoning;production efficiency;neural network;diagnostic method	0957-4174/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.eswa.2010.02.082 * Corresponding author. Te1.: +886 6 2757575/531 E-mail addresses: wenlit@ms21.hinet.net (W.-L. (D.-C. Li). Enterprises usually install a computerized information system to improve production efficiency. However, operational problems still occur from time to time, with different products usually requiring different solutions. This study discusses operational problems and proposes a diagnostic method for integrated shop-floor systems that manufacture multiple products. This study uses the multivariable statistics method to conduct a relevance analysis to determine the important attributes that influence production operations. Then, a neural network is used as the diagnostic system to detect operational problems. Support vector learning machines (SVM) are used to confirm the correct product classification. Finally, the diagnostic results are stored in a case-based reasoning system database for future use. 2010 Elsevier Ltd. All rights reserved.	artificial neural network;case-based reasoning;information system;reasoning system;relevance	Wen-Li Dai;Der-Chiang Li	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.02.082	support vector machine;computer science;artificial intelligence;machine learning;data mining;artificial neural network	AI	4.593894691869295	-20.357020770253513	67928
25af2967931feab0a43b442a39b995060b06c762	the effect of coercive power on supply chain inventory replenishment decisions		Supply chains often consist of stakeholders with different power levels collaborating with each other in order to meet customer demand. This imbalance of power along the supply chain is a critical factor that affects its short and long-term behavior, as well as its overall stability and efficiency. The role and the impacts of power in distribution channels have been explored quite extensively in Marketing, but far less so with regard to power in the context of supply chains. This paper explores the effect of power on supply chain functioning by focusing on a specific power type i.e. coercive power. More specifically, the impact of power and power awareness on inventory replenishment human decision-making is investigated. An experimental approach with unknown market demand and local information availability is implemented so as to provide a controlled environment for decision-making. Three different treatments are implemented in order to create situations of balanced power, imbalanced power without awareness and imbalanced power with awareness. Results show that power awareness does play a significant role in the way coercive power is exercised. In particular, a significant increase of the size and variability of order quantity and order time interval is observed in the case of imbalanced power with awareness.	heart rate variability;markov chain;power supply	Ramesh Roshan Das Guru;Amin Kaboli;Rémy Glardon	2014		10.1007/978-3-662-44736-9_28	power density;supply chain;operations management;economics;supply and demand	HCI	-3.3974891288364337	-10.584067965252649	67956
5fabead61d9be589b6594477ae5eab31296aac25	evt-based risk measurement of ownership-thematic investment in china	pot;investments;stock markets investment parameter estimation risk management;risk management;会议论文;investment;stock markets;indexes;lr test;evt;time series analysis;mathematical model;reactive power indexes mathematical model time series analysis equations investments economics;sse central soe 50 index evt based risk measurement chinese ownership thematic investment extreme value theory stock index index distribution characterization pot model peaks over threshold model var cvar parameter estimation return rates lr test sse local state owned 50 index sse private owned 50 index;economics;parameter estimation;lr test evt pot var cvar;var;cvar;reactive power	To model Chinese ownership-thematic investment, EVT (Extreme Value Theory) is applied in this paper with examples of three typical stock indexes. To characterize the distribution of the selected indexes, POT (Peaks Over Threshold) model is utilized to fit the tails of the distributions. Then VaR and CVaR are computed through the estimated parameters. Empirical results shows that POT model can fit tails of return rates of the indexes quite well; VaR are valid at high confidence levels such as 99% and 95% based on LR test but not at lower levels like 90%; with respect to CVaR, SSE Local State-owned 50 index is characterized with the greatest risk, SSE Private-owned 50 index comes the second while SSE Central SOEs 50 index shows the least risk relatively. POT model of EVT is an applicable method to measure risks of Chinese ownership-thematic investment.	cvar;extreme value theory;iso/iec 11404;lr parser;social system;tails;test-driven development;time series	Xuchao Li;Chengli Zheng	2012	2012 Fifth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2012.47	financial economics;actuarial science;economics;operations management	DB	3.4227168233583947	-11.672449519735933	68022
eb55899ab77837222a6d104fd1317b27d3bd2595	a hybrid svm-pso model for forecasting monthly streamflow		The long-term streamflow forecasts are very significant in planing and reservoir operations. The streamflow forecasts have to deal with a complex and highly nonlinear data patterns. This study employs support vector machines (SVMs) in predicting monthly streamflows. SVMs are proved to be a good tool for forecasting the nonlinear time series. But the performance of the SVM depends solely upon the appropriate choice of parameters. Hence, particle swarm optimization technique is employed in tuning SVM parameters. The proposed SVM-PSO model is used in forecasting the streamflow values of Swan River near Bigfork and St. Regis River near Clark Fork of Montana, United States. Further SVM model with various input structures is constructed, and the best structure is determined using various statistical performances. Later, the performance of the SVM model is compared with the autoregressive moving average model (ARMA) and artificial neural networks (ANN's). The results indicate that SVM could be a better alternative for predicting monthly streamflows as it provides a high degree of accuracy and reliability.	artificial neural network;autoregressive model;ensemble forecasting;mathematical optimization;moving-average model;nonlinear system;particle swarm optimization;performance;phase-shift oscillator;planning;regis;support vector machine;typset and runoff;time series;usability	Ch. Sudheer;R. Maheswaran;Bijaya Ketan Panigrahi;Shashi Mathur	2013	Neural Computing and Applications	10.1007/s00521-013-1341-y	machine learning	ML	9.058012142791084	-19.423208329104085	68213
1c9e658a36c3ab242c702edc9056a92feaed5468	the possible and the necessary for multiple criteria group decision	business and management;multiple criteria group decision;outranking methods;multiple criteria;multiple criteria choice;computing;sorting and ranking;robust ordinal regression;value function;business information systems;group decision;additive value functions;ordinal regression	We introduce the principle of robust ordinal regression to group decision. We consider the main multiple criteria decision methods to which robust ordinal regression has been applied, i.e., UTA and GRIP methods, dealing with choice and ranking problems, UTADIS , dealing with sorting (ordinal classification) problems, and ELECTRE , being an outranking method applying robust ordinal regression to well known ELECTRE methods. In this way, we obtain corresponding methods for group decision: UTA-GROUP, UTADIS-GROUP and ELECTRE-GROUP.	level of measurement;ordinal data;ordinal regression;sorting	Salvatore Greco;Vincent Mousseau;Roman Slowinski	2009		10.1007/978-3-642-04428-1_18	ordinal regression;econometrics;computing;computer science;management information systems;data mining;mathematics;ordinal optimization;ordinal data;statistics	ML	-2.4093051180117273	-19.000337913206888	68225
22c44ff0e1ed5784d5fbbcdb86c889d9d72f0d90	type 〈1, 1〉 fuzzy quantifiers determined by fuzzy measures on residuated lattices. part iii. extension, conservativity and extensionality	fuzzy integral;extensionality;conservativity;extension;fuzzy quantifier	We study the properties of extension, conservativity and extensionality of fuzzy quantifiers of type { 1 , 1 } defined using fuzzy measures and integrals. The property of extension states that truth values of quantifier applications are invariant with respect to possible extensions of the universe. Conservativity expresses the property that quantifiers are sensitive in their second argument only to objects that lie in the intersection of their arguments. Extensionality represents a form of the smoothness of quantifiers. We characterize these properties by the corresponding properties of functionals used in the definition of fuzzy quantifiers.	fuzzy measure theory;residuated lattice	Antonín Dvorák;Michal Holčapek	2015	Fuzzy Sets and Systems	10.1016/j.fss.2014.10.024	mathematical analysis;discrete mathematics;extensionality;computer science;fuzzy number;mathematics	NLP	0.028568660866230416	-22.79907092351234	68296
302dc64984f94b11f10dae217f41d84543bd5de6	rough set approximations in formal concept analysis and knowledge spaces	rough set;formal concept analysis	This paper proposes a generalized definition of rough set approximations, based on a subsystem of subsets of a universe. The subsystem is not assumed to be closed under set complement, union and intersection. The lower or upper approximation is no longer one set but composed of several sets. As special cases, approximations in formal concept analysis and knowledge spaces are examined. The results provide a better understanding of rough set approximations.	approximation;formal concept analysis;knowledge space;rough set;spaces;theory	Feifei Xu;Yiyu Yao;Duoqian Miao	2008		10.1007/978-3-540-68123-6_35	complement;closed set;rough set;computer science;formal concept analysis;machine learning;infinite set;dominance-based rough set approach	AI	-1.5484648884484693	-23.905041757298566	68544
f3c92cd52ff9aea60f1c0acc4867342716d099eb	a new method for fuzzy group decision making based on alpha-level cut and similarity	analytic hierarchy process;consensus;left hand side;processus hierarchie analytique;systeme aide decision;right hand side;fuzzy number;social decision;decision borrosa;logique floue;logica difusa;metric;decision floue;sistema ayuda decision;prise decision;similitude;fuzzy logic;decision support system;consenso;similarity;proceso jerarquia analitico;expert opinion;metrico;group decision making;similitud;decision colectiva;decision collective;toma decision;similarity function;similarity measure;metrique;fuzzy decision	Let opinions of experts among group decision making be represented as L-R fuzzy numbers. The difference of two experts' opinions is reflected by two distances, which are called the left-hand side distance and the right-hand side one. A method to calculate two types of distances based on the same α-level is presented. Then the distances are employed to construct a new similarity function to measure the similarity degrees of both sides which represent the pessimistic and optimistic similarity degrees between the experts, respectively. The degree of importance of each expert among group decision making is obtained by employing Saaty's analytic hierarchy process (AHP). The method of aggregating individual fuzzy opinions into a group consensus opinion by combining similarity degrees and the degree of importance of each expert is proposed. Finally some properties of the proposed similarity measure are proved and some numeric examples are shown to illustrate our method.		Jibin Lan;Liping He;Zhongxing Wang	2005		10.1007/11540007_61	fuzzy logic;analytic hierarchy process;group decision-making;consensus;similarity;decision support system;metric;computer science;artificial intelligence;fuzzy number;similitude;machine learning;data mining;mathematics	Vision	-2.437047393204957	-21.37863979641064	68550
61860a357cd58c06b5febef37093b3de13eeb3b7	the joint diffusion of a digital platform and its complementary goods: the effects of product ratings and observational learning		The authors study the interdependent diffusion of an open source software (OSS) platform and its software complements. They quantify the role of OSS governance, quality signals such as product ratings, observational learning, and user actions upon adoption. To do so they extend the Bass Diffusion Model and apply it to a unique data set of 6 years of daily downloads of the Firefox browser and 52 of its add-ons. The study then re-casts the resulting differential equations into non-linear, discrete-time, state space forms; and estimate them using an MCMC approach to the Extended Kalman Filtern (EKF-MCMC). Unlike continuous-time filters, the EKF-MCMC approach avoids numerical integration, and so is more computational efficient, given the length of our time-series, high dimension of our state space and need to model heterogeneity. Results show, for example, that observational learning and add-on ratings increase the demand for Firefox add-ons; add-ons can increase the market potential of the Firefox platform; a slow addon review process can diminish platform success; and OSS platforms (i.e. Chrome and Firefox) compete rather than complement each other.	add-ons for firefox;beneath a steel sky;extended kalman filter;interdependence;markov chain monte carlo;nonlinear system;numerical analysis;numerical integration;open sound system;state space;time series	Meisam Hejazi Nia;Norris Bruce	2016	CoRR		simulation;human–computer interaction;telecommunications;computer science;artificial intelligence;marketing;operations management;operating system;management;world wide web;computer security	ML	4.6388736893209765	-14.223734897233097	68661
1f9357d279cb80b7a42ac7f6a73b285ad0889023	application of neuro-fuzzy based expert system in water quality assessment	water quality;neural networks;fuzzy logic;neuro-fuzzy	In this research a framework is developed to predict the drinking water quality through the neural network models. A fuzzy rule-based system and similarity measure algorithm yield a water quality index for different sampling locations in a water distribution network (WDN), and a neural network is trained using the quality indices. Different sources of uncertainty exist in this model, including deficient, missing, and noisy data, conflicting water quality parameters and subjective information. Hourly and monthly data from Quebec City WDN are used to illustrate the performance of the proposed neuro-fuzzy model. Also, historical data from 52 sampling locations of Quebec City network is utilized to develop the rule-based model in order to train the neural network. Water quality is evaluated by categorizing quality parameters in two groups including microbial and physicochemical. Two sets of rules are defined using expert knowledge to assign water quality grades to each sampling location in the WDN. The fuzzy inference system outputs are deffuzzified using a similarity measure algorithm in this approach. The fuzzy inference system acts as a decision making agent. A utility function provides microbial and physicochemical water quality indices. Final results are used to train the neural network. In the proposed framework, microbial and physicochemical quality of water are predicted individually.	expert system;neuro-fuzzy	Elaheh Aghaarabi;Farzad Aminravan;Rehan Sadiq;Mina Hoorfar;Manuel J. Rodríguez;Homayoun Najjaran	2017	Int. J. Systems Assurance Engineering and Management	10.1007/s13198-014-0315-5	engineering;artificial intelligence;machine learning;data mining	SE	3.517158095688645	-19.15761205168666	68763
1fc82737daac9143276d9b897e54ac4cb0a5e4bf	intuitionistic fuzzy geometric heronian mean aggregation operators	multi criteria decision making;intuitionistic fuzzy set ifs;intuitionistic fuzzy geometric weighted heronian mean ifgwhm;intuitionistic fuzzy geometric heronian mean ifghm;heronian mean hm	In this paper, the multi-criteria decision making problem with the assumption that the criteria are correlative is studied under intuitionistic fuzzy environment. Some new aggregation operators for intuitionistic fuzzy information are proposed, including the intuitionistic fuzzy geometric Heronian mean (IFGHM) operator and the intuitionistic fuzzy geometric weighed Heronian mean (IFGWHM) operator. We investigate the properties of the proposed operators, such as idempotency, monotonicity, permutation and boundary. Moreover, an approach is proposed for multi-criteria decision making based on IFGWHM operator. An example about talent introduction is given to illustrate the proposed method.		Dejian Yu	2013	Appl. Soft Comput.	10.1016/j.asoc.2012.09.021	combinatorics;mathematical analysis;discrete mathematics;mathematics	Robotics	-1.793988992185519	-21.528228436937678	68910
cc9356b40ba13adba1c590057e9e7a29f702e6a2	multistage fuzzy decision making in bilateral negotiation with finite termination times	multistage;swinburne;transition matrix;decision problem;agents;fuzzy;decision;experimental evaluation;markov decision process;fuzzy constraints;constraints;negotiation	In this paper we model the negotiation process as a multistage fuzzy decision problem where the agents preferences are represented by a fuzzy goal and fuzzy constraints. The opponent is represented by a fuzzy Markov decision process in the form of offer-response patterns which enables utilization of limited and uncertain information, e.g. the characteristics of the concession behaviour. We show that we can obtain adaptive negotiation strategies by only using the negotiation threads of two past cases to create and update the fuzzy transition matrix. The experimental evaluation demonstrates that our approach is adaptive towards different negotiation behaviours and that the fuzzy representation of the preferences and the transition matrix allows for application in many scenarios where the available information, preferences and constraints are soft or imprecise.	multistage amplifier	Jan Richter;Ryszard Kowalczyk;Matthias Klusch	2009		10.1007/978-3-642-10439-8_3	fuzzy logic;markov decision process;defuzzification;fuzzy transportation;fuzzy classification;computer science;knowledge management;artificial intelligence;fuzzy number;software agent;neuro-fuzzy;machine learning;decision problem;stochastic matrix;mathematics;fuzzy associative matrix;fuzzy set operations;negotiation	AI	-2.39253076012334	-18.12651044411313	68946
d8a66a64599519edee7bcd2ef8fdd0128ed50603	rough set approximations vs. measurable spaces	oceans;helium;set theory;indexing terms;fuzzy sets;algebra;intelligent systems;rough sets;rough set;extraterrestrial measurements;extraterrestrial measurements algebra fuzzy sets sea measurements set theory rough sets helium intelligent systems educational institutions oceans;sea measurements	In this paper relationships between rough set approximations and measurable spaces are examined. It is proved that the family of all definable sets in a serial crisp (fuzzy, respectively) rough set algebra forms a crisp (fuzzy respectively) algebra. For any crisp measurable space there must exist a crisp rough set algebra such that the family of all definable sets is the given crisp algebra. Also, for a fuzzy algebra generated by a crisp algebra there must exist a fuzzy rough set algebra such that the family of all definable sets is just the given fuzzy algebra.	approximation;domain of discourse;existential quantification;rough set	Wei-Zhi Wu;Wen-Xiu Zhang	2006	2006 IEEE International Conference on Granular Computing	10.1109/GRC.2006.1635807	algebra of sets;combinatorics;discrete mathematics;rough set;intelligent decision support system;type-2 fuzzy sets and systems;term algebra;computer science;machine learning;mathematics;fuzzy set;field of sets;fuzzy set operations;dominance-based rough set approach	DB	-0.8169174310577288	-23.34564967059915	69209
5976c11be1b1e2f94805671183fd94797a308c2a	hierarchical classifier-regression ensemble for multi-phase non-linear dynamic system response prediction: application to climate analysis	kernel;spatiotemporal phenomena climate mitigation nonlinear dynamical systems pattern classification phase transformations regression analysis;nonlinear dynamical systems;anomaly detection;spatio temporal data mining;classification;tropical cyclone prediction;predictive models wind tropical cyclones mathematical model hurricanes kernel;phase transformations;tropical cyclones;regression;classification anomaly detection rainfall prediction tropical cyclone prediction spatio temporal data mining regression;pattern classification;spatiotemporal phenomena;mathematical model;hurricanes;rainfall prediction;multivariate spatiotemporal data hierarchical classifier regression ensemble multiphase nonlinear dynamic system response prediction climate analysis dynamic physical system phase transitions system parameters hurricane activity liquid vapor phase transition nonlinearly coupled fluctuations linear regression techniques least absolute deviation lad nonlinear system response stepwise regression multiphase system regression model;climate mitigation;predictive models;regression analysis;wind	A dynamic physical system often undergoes phase transitions in response to fluctuations induced on system parameters. For example, hurricane activity is the climate system's response initiated by a liquid-vapor phase transition associated with non-linearly coupled fluctuations in the ocean and the atmosphere. Because our quantitative knowledge about highly non-linear dynamic systems is very meager, scientists often resort to linear regression techniques such as Least Absolute Deviation (LAD) to learn the non-linear system's response (e.g., hurricane activity) from observed or simulated system's parameters (e.g., temperature, precipitable water, pressure). While insightful, such models still offer limited predictability, and alternatives intended to capture non-linear behaviors such as Stepwise Regression are often controversial in nature. In this paper, we hypothesize that one of the primary reasons for lack of predictability is the treatment of an inherently multi-phase system as being phase less. To bridge this gap, we propose a hybrid approach that first predicts the phase the system is in, and then estimates the magnitude of the system's response using the regression model optimized for this phase. Our approach is designed for systems that could be characterized by multi-variate spatio-temporal data from observations, simulations, or both.	coefficient;dynamical system;dynamical systems theory;hierarchical classifier;information;least absolute deviations;linear system;multi-agent system;nonlinear system;simulation;stepwise regression	Doel L. Gonzalez;Zhengzhang Chen;Isaac K. Tetteh;Tatdow Pansombut;Fredrick H. M. Semazzi;Vipin Kumar;Anatoli V. Melechko;Nagiza F. Samatova	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.133	anomaly detection;tropical cyclone;computer science;machine learning;mathematics	EDA	9.650507799490885	-18.98972440933879	69369
6fb77d21863c10babccabaed858e03473cc2e52a	consumer profile identification and allocation	continuous variable;profiles;kohonen maps;decision maker;logistic regression;non ordered polychotomous logit model;kohonen map;logit model;decision rule	We propose an easy-to-use methodology to allocate one of the groups which have been previously built from a complete learning data base, to new individuals. The learning data base contains continuous and categorical variables for each individual. The groups (clusters) are built by using only the continuous variables and described with the help of the categorical ones. For the new individuals, only the categorical variables are available, and it is necessary to define a model which computes the probabilities to belong to each of the clusters, by using only the categorical variables. Then this model provides a decision rule to assign the new individuals and gives an efficient tool to decision-makers. This tool is shown to be very efficient for customers allocation in consumer clusters for marketing purposes, for example.	computer cluster;database	Patrick Letrémy;Marie Cottrell;Eric Esposito;Valérie Laffite;Sally Showk	2007		10.1007/978-3-540-73007-1_65	categorical variable;computer science;machine learning;data mining;mathematics;logistic regression;statistics	ML	2.68569170423467	-17.00825631046468	69373
0d2ec84903fd084fbaa284a0fe843aa39fe29fd8	new similarity measures on intuitionistic fuzzy sets	intuitionistic fuzzy set;pattern recognition;similarity measure	Intuitionistic fuzzy sets, proposed by Atanassov, have gained attention from researchers for their applications in various fields. Then similarity measures be- tween intuitionistic fuzzy sets were developed. In this paper, some examples are applied to show that some existing similarity measures are not effective in some cases. Then, based on the kind of geometrical background, we propose several new similarity mea- sures of IFSs in which we consider three parameters describing a IFS. Finally, we apply these measures to pattern recognition.	fuzzy set	Jin-Han Park;Jong-Seo Park;Young Chel Kwun;Ki Moon Lim	2007		10.1007/978-3-540-71441-5_3	discrete mathematics;computer science;pattern recognition;data mining;mathematics	NLP	-2.653570046506166	-23.255319121835907	69472
1adee2490cf1ee5b1a17818a441799ed9cb70e56	an efficiency-driven approach for setting revenue target	efficiency;strong ordinal data;data envelopment analysis dea;ordinal data;efficiency analysis;efficiency measurement;data envelope analysis	This paper addresses the efficiency measurement and revenue setting problems drawn from a home improvement company with 22 chain stores in Taiwan. The top management attaches great importance to efficiency analysis of their stores. Furthermore, when the proposal to establish a new store is under development, the regional manager must determine what efficiency level the new store should achieve and what amount of business revenue it should earn. An approach by using the imprecise DEA (IDEA) and inverse IDEA models as core techniques is proposed to deal with such problems. A simulated application illustrates the implementation of the proposed approach.		Hung-Tso Lin	2010	Decision Support Systems	10.1016/j.dss.2010.03.006	economics;operations management;data mining;data envelopment analysis;efficiency;ordinal data;management;welfare economics	ECom	-3.5617563469720994	-15.21712356873744	69664
be55f55c7991088825fc529c5735da46f4c2e1ce	a curvelet based approach to time series forecasting		In this paper we propose a new Curvelet based methodology for modeling the financial time series data, addressing and incorporating the diverse range of data characteristics. With the proposed methodology, we analyze and model the geometric multi scale and chaotic data characteristics. Empirical studies with some typical financial time series data show that more diverse heterogeneous data characteristics can be revealed and modeled in the projected time delayed domain. The proposed algorithm demonstrates the superior performance compared with the benchmark models.	curvelet;time series	Yingchao Zou;Kaijian He;Meiying Jiang	2015		10.1016/j.procs.2015.07.116	econometrics;machine learning;data mining	ML	7.83683067443361	-21.390689157157	69693
551eb674ec9bb9ca232fb7f43691de63ebd273d2	delay prediction system for large-scale railway networks based on big data analytics		State-of-the-art train delay prediction systems do not exploit historical train movements data collected by the railway information systems, but they rely on static rules built by expert of the railway infrastructure based on classical univariate statistic. The purpose of this paper is to build a data-driven train delay prediction system for large-scale railway networks which exploits the most recent Big Data technologies and learning algorithms. In particular, we propose a fast learning algorithm for predicting train delays based on the Extreme Learning Machine that fully exploits the recent in-memory large-scale data processing technologies. Our system is able to rapidly extract nontrivial information from the large amount of data available in order to make accurate predictions about different future states of the railway network. Results on real world data coming from the Italian railway network show that our proposal is able to improve the current state-of-the-art train delay prediction systems.		Luca Oneto;Emanuele Fumeo;Giorgio Clerico;Renzo Canepa;Federico Papa;Carlo Dambra;Nadia Mazzino;Davide Anguita	2016		10.1007/978-3-319-47898-2_15	engineering;data science;data mining;world wide web	ML	7.867630632497488	-22.707050579509936	70036
130f18a9d4df30853b233a84f904443c34d0ad2b	a hybrid grey based kohonen model and biogeography-based optimization for project portfolio selection		The problemof selection and the best option are themain subject of operation research science in decision-making theory. Selection is a process that scrutinizes and investigates several quantitative and qualitative, and most often incompatible, factors. One of the most fundamental management issues in multicriteria selection literature is the multicriteria adoption of the projects portfolio. In such decision-making condition, manager is seeking for the best combination to build up a portfolio among the existing projects. In the present paper, KOHONEN algorithm was first employed to build up a portfolio of the projects. Next, each portfolio was evaluated using grey relational analysis (GRA) and then scheduled risk of the project was predicted usingMamdani fuzzy inference method. Finally, the multiobjective biogeography-based optimization algorithm was utilized for drawing risk and rank Pareto analysis. A case study is used concurrently to show the efficiency of the proposed model.	artificial neural network;cluster analysis;data mining;decision theory;firefly (cache coherence protocol);firefly algorithm;grey relational analysis;it risk management;inferential programming;inferential theory of learning;mathematical model;mathematical optimization;metaheuristic;operations research;pareto efficiency;particle swarm optimization;selection (genetic algorithm);self-organizing map;teuvo kohonen	Farshad Faezy Razi;Abbas Toloie Eshlaghy;Jamshid Nazemi;Mahmood Alborzi;Alireza Pourebrahimi	2014	J. Applied Mathematics	10.1155/2014/159675	mathematical optimization;management science	AI	-3.1729464873423336	-16.920609013514408	70066
55794e6b47bf4fcc5603983b3d5f3b399f7c1229	a new solution of intuitionistic fuzzy multiple attribute decision-making based on attributes preference	intuitionistic fuzzy set;intuitionistic fuzzy sets;uncertainty;ideal alternatives;data mining;multiple attribute decision making;fuzzy set theory;fuzzy sets;the best attribute weight;negative ideal alternative;the best attribute weight multiple attribute decision making ideal alternatives;vectors;topsis method;decision making fuzzy sets entropy uncertainty fuzzy systems mathematics quality management investments;attribute weight vector;entropy theory;attributes preference;mathematical model;vectors decision making entropy fuzzy set theory;attribute weight vector intuitionistic fuzzy sets multiple attribute decision making attributes preference positive ideal alternative negative ideal alternative topsis method entropy theory;entropy;positive ideal alternative	For the situations that the preference information to attributes and estimating information to alternatives are expressed by intuitionistic fuzzy values, a new solution of the multiple attribute decision-making is given. Firstly, the solution of positive-ideal alternatives and negative-ideal alternatives are studied based on the traditional TOPSIS method. We give a method to choose the possible positive-ideal solution and the possible negative-ideal solution, and deal with the uncertainty information by entropy theory to get the only ideal alternatives. Secondly, the new solution of the best attribute weight vector is discussed. A new math model for solving the best attribute weight vector is given based on the isomorphism of intuitionistic fuzzy sets, and the process for solving multiple attribute decision-making is introduced. Finally, a practical example is illustrated to verify the effectiveness of this method.	fuzzy set;intuitionistic logic;mathematical model	Juan Li;Chengyi Zhang	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.832	mathematical optimization;discrete mathematics;attribute domain;computer science;data mining;mathematics;fuzzy set;statistics	DB	-2.483410506487879	-20.158033569629918	70082
06640c769643c70cd2f9ffcbd28eacefdea0f1a6	a gis-based decision support system for hotel room rate estimation and temporal price prediction: the hotel brokers' context	journal_article;data mining;geographic information systems;hotels;regression analysis;hedonic methods;price prediction	The vastly increasing number of online hotel roombookings is not only intensifying the competition in the travel industry as a whole, but also prompts travel intermediates (i.e. e-companies that aggregate information about different travel products from different travel suppliers) into a fierce competition for the best prices of travel products, i.e. hotel rooms. An important factor that affects revenues is the ability to conclude profitable deals with different travel suppliers. However, the profitability of a contract not only depends on the communication skills of a contractmanager. It significantly depends on the objective information obtained about a specific travel supplier and his/her products. While the contract manager usually has a broad knowledge of the travel business in general, collecting and processing specific information about travel suppliers is usually a time and cost expensive task. Our goal is to develop a tool that assists the travel intermediate to acquire the missing strategic information about individual hotels in order to leverage profitable deals. We present a GIS-based decision support system that can both, estimate objective hotel room rates using essential hotel and locational characteristics and predict temporal room rate prices. Information about objective hotel room rates allows for an objective comparison and provides the basis for a realistic computation of the contract's profitability. The temporal prediction of room rates can be used for monitoring past hotel room rates and for adjusting the price of the future contract. This paper makes three major contributions. First, we present a GIS-based decision support system, the first of its kind, for hotel brokers. Second, the DSS can be applied to virtually any part of the world, whichmakes it a very attractive business tool in real-life situations. Third, it integrates awidely used datamining framework that provides access to dozens of ready to run algorithms to be used by a domain expert and it offers the possibility of adding new algorithms once they are developed. The system has been designed and evaluated in close cooperation with a company that develops travel technology solutions, in particular inventory management and pricing solutions for many well-known websites and travel agencies around the world. This company has also provided us with real, large datasets to evaluate the system. We demonstrate the functionality of the DSS using the hotel data in the area of Barcelona, Spain. The results indicate the potential usefulness of the proposed system.	aggregate data;algorithm;autodesk maya;computation;data acquisition;data mining;decision support system;geographic information system;hedonic regression;inventory;josm;java;list of statistical packages;mathematical model;open-source software;openstreetmap;prototype;r language;real life;requirement;self-propelled particles;spaces;subject-matter expert;the great giana sisters	Slava Kisilevich;Daniel A. Keim;Lior Rokach	2013	Decision Support Systems	10.1016/j.dss.2012.10.038	economics;computer science;artificial intelligence;marketing;operations management;data mining;geographic information system;management;world wide web;regression analysis;commerce	Web+IR	1.6660762822299153	-13.381850052385461	70105
0fc8207ddca59159bc0ffedf53e6d41b15b34a46	multi-granulation fuzzy preference relation rough set for ordinal decision system	classification;fuzzy rough set;fuzzy preference relation;multi granulation;ordinal decision system	Preference analysis is a class of important issues in multi-criteria ordinal decision making. Rough set is an effective approach to handle preference analysis. In order to solve the multi-criteria preference analysis problem, this work improves the fuzzy preference relation rough set model with additive consistent fuzzy preference relation, and expands it to multi-granulation case. Cost is also an important issue in decision analysis. Taking the cost into consideration, we also expand the model to cost sensitive multi-granulation fuzzy preference relation rough set. Some theorems are represented, and the classification and sample condensation algorithms based on our model are investigated. Some experiments are complete and the experimental results show that our model and algorithms are effective for preference decision making of ordinal decision system.	decision support system;ordinal data;rough set	Wei Pan;Kun She;Pengyuan Wei	2017	Fuzzy Sets and Systems	10.1016/j.fss.2016.08.002	ordinal regression;biological classification;fuzzy classification;machine learning;pattern recognition;data mining;mathematics;fuzzy set operations;dominance-based rough set approach	AI	-2.09158502313741	-20.712813436095374	70197
f13c18ac6cfc114d5d53b8a04a0925f4193c4a07	urbanization level forecast based on bp neural network	forecast;economic development factors;training artificial neural networks neurons economics predictive models biological system modeling mathematical model;town and country planning;urbanization level forecast;backpropagation network;world development;neural nets;urbanization level;economic forecasting;training;biological system modeling;town and country planning backpropagation economic forecasting neural nets;forecasting model;backpropagation;adverse effect;artificial neural networks;relative error;forecast bp neural network urbanization level;back propagation network;bp neural network;mathematical model;predictive models;economic development;neurons;economics;backpropagation network urbanization level forecast bp neural network world development scientific forecast economic development factors;scientific forecast;neural network	Urbanization is the general trend and tide of the current world development and also one of the most remarkable social and economical phenomena in the world. Urbanization level becomes an important symbol of the economic strength and modernization level in a region and thus how to improve the local urbanization level has become a priority for economic development. With the increasing urbanization level, a series of adverse effects have been brought about. It is a very important task to make a scientific forecast for the urbanization of a region. Take Zhejiang Province as an example; only considering the present economic development factors, the currently most popular BP network (Back-Propagation Network) in the neural network is adopted to establish the forecast model; and the urbanization level of Zhejiang Province in 2000-2004 is forecasted. For the forecast result, the maximum relative error is 2.51%, the minimum relative error is 0.23% and the mean absolute percent error is 1.05%. Thus, the result indicates that the model has high forecast precision and therefore can be used to forecast the urbanization level of Zhejiang Province in the future.	approximation error;artificial neural network;software propagation	Mingqi Chang;Junping Liu	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5583170	geography;operations research;economic growth	DB	9.646802204576872	-19.957962036329832	70216
d9d1da0ddfe52f0102246b25e1ead603b44a7e0e	covering based rough sets and relation based rough sets		Relation based rough sets and covering based rough sets are two important extensions of the classical rough sets. This paper investigates relationships between relation based rough sets and the covering based rough sets in a particular framework of approximation operators, presents a new group of approximation operators obtained by combining coverings and neighborhood operators and establishes some relationships between covering based rough sets and relation based rough sets.	rough set	Mauricio Restrepo;Jonatan Gómez	2014		10.1007/978-3-319-08729-0_13	rough set	Robotics	-1.4039221041729524	-23.790832436723992	70227
2eb62cdeb103881e2d886cbb2ac86a00efe20657	towards robust flood forecasts using neural networks	weather forecasting backpropagation floods geophysics computing neural nets;noise measurement;mathematical model;mathematical model noise measurement matlab;australia robust flood forecasts neural network design domain specific problem flood events forecasting noise noisy data loss function backpropagation matlab flood data talebudgera;matlab	In this paper, design of a neural network for a domain-specific problem is described. The problem of concern is forecasting flood events where data is contaminated heavily by noise, training examples have different importance levels and noisy data coincides with the most important ones. To this end, two ideas are explored namely, changing the loss function and integrating a coefficient that reflects on the relative importance of training examples. To this end, backpropagation is re-derived considering implication of having a more general objective function. Independently, inclusion of scores associated with each training examples and its implication of overall loss function and the way weights are optimized is explored. The derived model is implemented in MATLAB and flood data from Talebudgera, Australia is considered for investigations. Compared to the base case being backpropagation, the results suggest that inclusion of scored for training examples corresponds to visible improvement when predicting peaks.	artificial neural network;backpropagation;coefficient;loss function;matlab;recursion;signal-to-noise ratio	Seyyed Adel Alavi Fazel;Hamid Mirfenderesk;Rodger Tomlinson;Michael Blumenstein	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280600	computer science;noise measurement;artificial intelligence;machine learning;mathematical model;data mining;statistics	ML	8.203530796721033	-17.791771196729012	70355
320729296b843915d02d4cb4811bc821a2ac1c79	time series forecasting methodology for multiple-step-ahead prediction	computational intelligence;forecasting;neu- ral networks;clustering;time series forecasting;state space;neural network;artificial neural network	This paper presents a time series forecasting methodology and applies it to generate multiple–step–ahead predictions for the direction of change of the daily exchange rate of the Japanese Yen against the US Dollar. The proposed methodology draws from the disciplines of chaotic time series analysis, clustering, and artificial neural networks. In brief, clustering is applied to identify neighborhoods in the reconstructed state space of the system; and subsequently neural networks are trained to model the dynamics of each neighborhood separately. The results obtained through this approach are promising.	artificial neural network;cluster analysis;false nearest neighbor algorithm;feedforward neural network;iteration;recurrent neural network;state space;test set;time series	Nicos G. Pavlidis;Dimitris K. Tasoulis;Michael N. Vrahatis	2005			pattern recognition;machine learning;computer science;time series;artificial intelligence;artificial neural network;state space;computational intelligence	ML	9.399553558877889	-22.445882070702165	70376
b19a909e1670c936819bf88faa5be9e6ceb83150	interactive construction of criterion relations for multi-criteria decision making		Multi-criteria decision making (MCDM) is a category of techniques for solving decision making problems based on the performance of multiple criteria. One shortcoming of the existing MCDM techniques is that they rarely consider the relations among decision criteria. Nevertheless, different types of criterion relations significantly impact the results of the decision making problem. In this paper, we solve this problem by establishing and measuring different types of relations among decision criteria. We propose a MCDM framework, named InterDM, to rank a set of alternatives based on the utilities of both singleton criteria and criterion coalitions, in which we design an Interactive Interpretive Structural Modeling technique to construct consistent criterion relations. We use a case study of ranking cloud services to demonstrate the efficiency of InterDM.		Le Sun;Jinyuan He	2018		10.1007/978-3-030-00006-6_54	operations research;computer science;multiple-criteria decision analysis;cloud computing;singleton;distributed computing;ranking	Theory	-4.1579206786196306	-18.932486671315473	70667
2f5f9fc892fa7994ac5c1ece0f16931e16e050c4	time series data analysis using probabilistic and neural network	bayesian network;time series data;portfolio;neural network	Artificial intelligence decision support system is always a popular topic in providing the human user with an optimized decision recommendation when operating under uncertainty in complex environments. The particular focus of our discussion is to compare different methods of artificial intelligence decision support systems in the investment domain – the goal of investment decision-making is to select an optimal portfolio that satisfies the investor’s objective, or, in other words, to maximize the investment returns under the constraints given by investors. In this study we applied several artificial intelligence systems like Influence Diagram (a special type of Bayesian network) and Neural Network to get experimental comparison analysis to facilitate users to intelligently select the best portfolio.	artificial intelligence;artificial neural network;bayesian network;decision support system;influence diagram;time series	Chiu-Che Tseng	2006		10.2991/jcis.2006.134	stochastic neural network;probabilistic neural network;variable-order bayesian network;machine learning;time delay neural network	AI	4.15610470982808	-18.733615022551355	70736
31ed52f6569716d438fff343fa9ed0fba4086b68	forecasting enrollment model based on first-order fuzzy time series	conference;meeting	This paper proposes a novel improvement of forecasting approach based on using time-invariant fuzzy time series. In contrast to traditional forecasting methods, fuzzy time series can be also applied to problems, in which historical data are linguistic values. It is shown that proposed time-invariant method improves the performance of forecasting process. Further, the effect of using different number of fuzzy sets is tested as well. As with the most of cited papers, historical enrollment of the University of Alabama is used in this study to illustrate the forecasting process. Subsequently, the performance of the proposed method is compared with existing fuzzy time series time-invariant models based on forecasting accuracy. It reveals a certain performance superiority of the proposed method over methods described in the literature.	experiment;first-order predicate;fuzzy set;time series;time-invariant system;vii	Melike Sah;Konstantin Y. Degtiarev	2004			probabilistic forecasting;forecasting;engineering;artificial intelligence;data mining;operations research	AI	6.80415856632283	-20.69587645152508	70783
9874f33f903f3d79df2309e9d2a2678080a4a2ba	a clustering method based on fuzzy equivalence relation for customer relationship management	fuzzy equivalence relation;customer relationship management;fuzzy data;clustering;clustering method;binary relation;transitive closure;fuzzy compatible relation	In real world, customers commonly take relevant attributes into consideration for the selection of products and services. Further, the attribute assessment of a product or service is often presented by a linguistic data sequence. To partition these linguistic data sequences of customers’ assessment on a product or service, a proper clustering method is essential and proposed in this paper. In the clustering method, the linguistic data sequences are presented by fuzzy data sequences and a fuzzy compatible relation is first constructed to present the binary relation between two data sequences. Then a fuzzy equivalence relation is derived by max–min transitive closure from the fuzzy compatible relation. Based on the fuzzy equivalence relation, the linguistic data sequences are easily classified into clusters. The clusters representing the selection preferences of different customers on the product or service will be the foundation of developing customer relationship management (CRM). 2010 Elsevier Ltd. All rights reserved.	cluster analysis;customer relationship management;fuzzy logic;maxima and minima;transitive closure;turing completeness	Yu-Jie Wang	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.02.076	customer relationship management;discrete mathematics;defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;binary relation;data mining;database;mathematics;cluster analysis;transitive closure;fuzzy set operations	AI	-3.7375063252687277	-22.534685805585884	70849
ed6464a4590b953daac247e91e2291ba3ca4baca	context sensitive indoor temperature forecast for energy efficient operation of smart buildings	bayesian regularized neural network technique context sensitive indoor temperature forecasting energy efficient operation smart buildings knowledge discovery real time system monitoring data driven techniques predictive short term indoor temperature models daily operation;ubiquitous computing bayes methods building management systems data mining neural nets;smart buildings;data modeling;buildings heating temperature sensors temperature measurement temperature distribution monitoring;thermal comfort;data modeling smart buildings thermal comfort	This paper analyzes the potential of knowledge discovery from sensed data, which enables real-time systems monitoring, management, prediction and optimization in smart buildings. State of the art data driven techniques generate predictive short-term indoor temperature models based on real building data collected during daily operation. The most accurate results are achieved by the Bayesian Regularized Neural Network technique. Our results show that we are able to achieve a low relative predictive error for each room temperature in the range of 1.35% - 2.31% with low standard deviation of the residuals.	approximation error;artificial neural network;bayesian network;experiment;granada;mathematical optimization;mobile backend as a service;real-time clock;real-time computing	María Victoria Moreno Cano;Antonio F. Gómez-Skarmeta;Alberto Venturi;Mischa Schmidt;Anett Schülke	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389140	thermal comfort;data modeling;simulation;building automation;computer science;data mining	AI	9.598353047129503	-16.990086264739123	70871
585013a80f871ed782a542eee46892a04ec1468d	"""a note on """"the revised method of ranking lr fuzzy number based on deviation degree"""""""	ranking of fuzzy numbers;fuzzy numbers;fuzzy number;lr fuzzy numbers;defuzzification;expert system	Recently, Asady [(2010). The revised method of ranking LR fuzzy number based on deviation degree. Expert Systems with Applications, 37, 5056-5060] pointed out that Wang et al.'s method has some drawback by a numerical example and then Wang's method is modified to present an easy way to rank fuzzy numbers. In this note, we will indicate that Asady's revision has a shortcoming exactly as the same as Wang's method.	fuzzy number;lr parser	T. Hajjari;Saeid Abbasbandy	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.04.081	discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy associative matrix;expert system;fuzzy set operations;algorithm	DB	-0.6888466929279401	-20.96119872551684	70886
026dff5070d2e8c45a6338c11799310be3c61a9a	identifying important variables for predicting travel time of freeway with non-recurrent congestion with neural networks	travel time prediction;neural networks;electronic toll collection;non recurrent congestion;article;freeway	The provision of long-distance travel time information has been a major factor facilitating the intelligent transportation system to become more successful. Previous studies have pointed out that non-recurrent congestion is the major cause of freeway delay. The long travel distance complicates the characteristics of traffic flow. Hence, how to improve the prediction capability of long-distance travel time in the case of non-recurrent congestion is an important issue that must be overcome in the field of travel time prediction. This study constructs the travel time prediction model for a segment of 36.1 kms (including eight interchanges) in the National Freeway No. 1, Taiwan, by using the multilayer perceptron. To improve the prediction capability of the model in the case of non-recurrent congestion, this study collects data of average spot speed and heavy vehicle volume gathered by dual-loop vehicle detectors, in addition to rainfall and temporal feature. Furthermore, the historical travel time inferred from the original data of electronic toll collection (ETC) system is also used as the input variable, and the actual travel time inferred from ETC is used as the training target to establish a robust prediction model. As suggested by the results of 168 experimental combinations, the most appropriate prediction model established in this study is a highly accurate forecasting model with MAPE of 6.47 %.	am broadcasting;artificial neural network;categorization;dual loop;electronic toll collection;freeway;geo-imputation;line code;missing data;multilayer perceptron;network congestion;sensor	Chi-Sen Li;Mu-Chen Chen	2012	Neural Computing and Applications	10.1007/s00521-012-1114-z	simulation;electronic toll collection;computer science;machine learning;operations research;artificial neural network	ML	8.623774213973912	-14.557853969120428	71000
ab21aabb751624eb3e997d99ebb5e46e544d7e6e	a deep belief network based fault diagnosis model for complex chemical processes	deep belief network;early warning;feature extraction;alarm management;fault diagnosis	Data-driven methods have been regarded as desirable methods for fault detection and diagnosis (FDD) of practical chemical processes. However, with the big data era coming, how to effectively extract and present fault features is one of the keys to successful industrial applications of FDD technologies. In this paper, an extensible deep belief network (DBN) based fault diagnosis model is proposed. Individual fault features in both spatial and temporal domains are extracted by DBN sub-networks, aided by the mutual information technology. A global two-layer backpropagation network is trained and used for fault classification. In the final part of this paper, the bench marked Tennessee Eastman process is utilized to illustrate the performance of the DBN based fault diagnosis model.	backpropagation;bayesian network;big data;deep belief network;fault detection and isolation;mutual information	Zhanpeng Zhang;Jinsong Zhao	2017	Computers & Chemical Engineering	10.1016/j.compchemeng.2017.02.041	feature extraction;computer science;engineering;artificial intelligence;machine learning;warning system;data mining;deep belief network	AI	8.90329911591216	-23.657388766925333	71083
0b6403c70c71594e4e20bb45ce696cdefab08a7f	automated generation of new knowledge to support managerial decision-making: case study in forecasting a stock market	forecasting;stock market;data mining;case based reasoning;neural network;knowledge discovery	Abstract: The deluge of data available to managers underscores the need to develop intelligent systems to generate new knowledge. Such tools are available in the form of learning systems from artificial intelligence. This paper explores how the novel tools can support decision-making in the ubiquitous managerial task of forecasting.#R##N##R##N##R##N##R##N#For concreteness, the methodology is examined in the context of predicting a financial index whose chaotic properties render the time series difficult to predict. The study investigates the circumstances under which enough new knowledge is extracted from temporal data to overturn the efficient markets hypothesis.#R##N##R##N##R##N##R##N#The efficient markets hypothesis precludes the possibility of anticipating in financial markets. More precisely, the markets are deemed to be so efficient that the best forecast of a price level for the subsequent period is precisely the current price. Certain anomalies to the efficient market premise have been observed, such as calendar effects. Even so, forecasting techniques have been largely unable to outperform the random walk model which corresponds to the behavior of prices under the efficient markets hypothesis.#R##N##R##N##R##N##R##N#This paper tests the validity of the efficient markets hypothesis by developing knowledge-based tools to forecast a market index. The predictions are examined across several horizons: single-period forecasts as well as multiple periods. For multiperiod forecasts, the predictive methodology takes two forms: a single jump from the current period to the end of the forecast horizon, and a multistage web of forecasts which progresses systematically from one period to the next.#R##N##R##N##R##N##R##N#These models are first evaluated using neural networks and case-based reasoning, and are then compared against a random walk model. The computational models are examined in the context of forecasting a composite for the Korean stock market.		Se-Hak Chun;Steven H. Kim	2004	Expert Systems	10.1111/j.1468-0394.2004.00277.x	case-based reasoning;forecasting;computer science;artificial intelligence;machine learning;data mining;knowledge extraction;artificial neural network	NLP	4.9190695673093146	-17.8815159008068	71261
6e66589d3477f4369215866c24833056df095214	wireless airtime traffic estimation using a state space model	minimisation;radio networks;kalman filtering;recursive estimation;minimization;sequences;mape;structural model;state space methods;forecasting technique;service provider;autoregressive moving average processes;kalman filters;traffic control;kalman filter;communication system traffic;testing;spectrum;minimization methods;state estimation;telecommunication traffic autoregressive moving average processes kalman filters minimisation radio networks sequences state space methods;mean absolute percentage error basic structural model autoregressive integrated moving average kalman filter;telecommunication traffic;mean absolute percentage error;moving average;smoothing methods;autoregressive integrated;seasonality;traffic control state estimation state space methods predictive models costs smoothing methods parameter estimation recursive estimation testing demand forecasting;wireless airtime traffic estimation;training sequence;autoregressive integrated moving average;basic structural model;predictive models;parameter estimation;state space model;wireless service provider wireless airtime traffic estimation state space model forecasting technique extended structural model esm basic structural model bsm training sequence standard kalman filter minimization mean absolute percentage error mape autoregressive integrated moving average;extended structural model;standard kalman filter;esm;bsm;demand forecasting;wireless service provider	A new forecasting technique called the extended structural model (ESM) is presented. This technique is derived from the basic structural model (BSM) by the introduction of extra parameters that were assumed to be 1 in the BSM. The ESM model is constructed from the training sequence using the standard Kalman filter recursions, and then the extra parameters are estimated to minimize the mean absolute percentage error (MAPE) of the validation sequence. The model is evaluated by prediction of the total number of minutes of wireless airtime per month on the Bell Canada network. The ESM model shows an improvement in MAPE of the test sequence over both the BSM and seasonal autoregressive integrated moving average. The improved prediction can significantly reduce the cost for wireless service providers, who need to accurately predict future wireless spectrum requirements	airtime;approximation error;autoregressive integrated moving average;autoregressive model;black–scholes model;kalman filter;openbsm;recursion;requirement;state space;state-space representation	Farzaneh Kohandani;Derek W. McAvoy;Amir K. Khandani	2006	4th Annual Communication Networks and Services Research Conference (CNSR'06)	10.1109/CNSR.2006.58	kalman filter;simulation;demand forecasting;computer science;statistics	Robotics	9.140975926270775	-13.473837009886795	71284
8d598337f6508fc52c05fede0536417458d1dd30	generalized metric based test selection and coverage measure for communication protocols	test selection;communication protocols;generalized metric;coverage measure;communication protocol	This paper presents an important generalization of the metric based test selection and coverage measure, originally proposed in [14, 5]. Although the original method introduces a significant analytical solution to the problem of coverage and test selection for protocols, its applicability is limited to only the control part of protocols. We extend this method to handle a protocol behavior space where both the control sequences and the data valuations for event parameters are included. We prove that the important properties of total boundedness and completeness are preserved in the generalized metric space, thus ensuring the possible approximation of the specification (infinite number of execution sequences) with a finite test suite within arbitrary degree of precision. A generalized test selection algorithm and coverage measure are also discussed.		Jinsong Zhu;Son T. Vuong	1997			communications protocol;computer science;theoretical computer science	EDA	0.04253160435721938	-20.8068640381257	71394
dd8643bc28c1c75ae940305e2303ecab0067803e	statistical decisions in presence of imprecisely reported attribute data		The paper presents a new methodology for making statistical decisions when data is reported in an imprecise way. Such situations happen very frequently when quality features are evaluated by humans. We have demonstrated that traditional models based either on the multinomial distribution or on predefined linguistic variables may be insufficient for making correct decisions. Our model, which uses the concept of the possibility distribution, allows to separate stochastic randomness from fuzzy imprecision, and provides a decision – maker with more information about the phenomenon of interest.	common criteria;fuzzy logic;multinomial logistic regression;randomness	Olgierd Hryniewicz	2009			data mining;geometry;computer science;regular polygon;bounded function	Web+IR	-0.2876903250276091	-18.715987818672144	71416
402af7f8d16ec49b25d222d2c1cd4afffe9aade6	using owdg to support a multicriteria group decision in a logistics problem	owd;logistics;decision support systems;vectors logistics companies decision making open wireless architecture decision support systems weight measurement;group decision making;decision process owdg multicriteria group decision logistics problem ordered weighted disagreement weight vectors compensatory effect;owd logistics decision support systems group decision making;logistics decision making	In this paper, a model based on the Ordered Weighted Disagreement (OWDg) to aid workgroups to make decisions is presented. This model proposes a framework including some weight vectors and a measure to evaluate the compensatory effect among the opinions of decision makers. Therefore, it is possible to obtain more information about the decision process. Finally, a case study based on a logistics problem is used to illustrate the applicability of the model.	flowchart;logistics;numerical analysis;ordered weighted averaging aggregation operator;ordinal data;weight function	José Leao e Silva Filho;Suzana de França Dantas Daher;Danielle Costa Morais	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.17	logistics;r-cast;group decision-making;optimal decision;influence diagram;decision support system;intelligent decision support system;decision analysis;decision engineering;computer science;knowledge management;decision analysis cycle;decision tree;decision rule;management science;evidential reasoning approach;business decision mapping	Robotics	-4.2696981265049505	-17.606916129018828	71595
fa0f86d478848de9b8a74db11c1dca4558150c4f	enriched travel demand estimation by including zonal and traveler characteristics and their relationships		Current procedure in travel demand estimation models is to separately deal with attraction, production and trip distribution, where the latter typically assumes inverse distance proportionality. We show that this procedure leads to errors in the demand estimation, particularly when dealing with very specific zones and heterogeneous travel behavior. We argue that this traditional procedure is rooted in traditional ways of data collection, while new (big) data sources allow direct observation of travel demand patterns. Using such data, we propose an enriched travel demand estimation method in which zonal and traveler characteristics and their relationships are consistently carried over from the empirical data into the demand model. This can improve both the validity and richness of demand estimations.	big data	J. P. van der Vliet;Adam J. Pel;J. W. C. van Lint	2017	2017 5th IEEE International Conference on Models and Technologies for Intelligent Transportation Systems (MT-ITS)	10.1109/MTITS.2017.8005696	data collection;econometrics;data modeling;economics;demand patterns;proportionality (mathematics);species richness;attraction;trip distribution;travel behavior	Robotics	6.793051151457599	-14.72141581863186	71601
dca65ef6826a28a9d2c4a4cddb5350ddb55487af	relationship between vehicle stream in the circular roadway of a one-lane roundabout and traffic volume on the roundabout at peak hour		One-lane roundabouts are popular mainly due to high traffic capacity and the opportunities for ensuring a high level of road traffic safety to the users. Furthermore, they have been very often used as an effective means of reducing the traffic. This paper presents the analysis of relationships of a one-lane roundabout on the main factors that determine its value, such as total traffic volume that unloads the intersection at a particular external diameter of the roundabout. The effect of the study is development of a nomogram of traffic capacity for one-lane roundabouts.	roundabout	Elzbieta Macioszek	2014		10.1007/978-3-662-45317-9_12	transport engineering	Mobile	9.521289991554335	-9.905528177083168	71632
f8cb754fd01a3d9bf6eefef84897f359f95b3753	combination of multiple classifiers for the customer's purchase behavior prediction	combination;purchase behavior prediction;data mining;multiple classifiers;combination of multiple classifiers;genetic algorithm	In these days, EC companies are eager to learn about their customers using data mining technologies. But the diverse situations of such companies make it difficult to know which is the most effective algorithm for the given problems. Recently, a movement towards combining multiple classifiers has emerged to improve classification results. In this paper, we propose a method for the prediction of the EC customer’s purchase behavior by combining multiple classifiers based on genetic algorithm. The method was tested and evaluated using Web data from a leading EC company. We also tested the validity of our approach in general classification problems using handwritten numerals. In both cases, our method shows better performance than individual classifiers and other known combining methods we tried. D 2002 Elsevier Science B.V. All rights reserved.	computer performance;data mining;genetic algorithm	Eunju Kim;Wooju Kim;Yillbyung Lee	2003	Decision Support Systems	10.1016/S0167-9236(02)00079-9	random subspace method;genetic algorithm;combination;computer science;machine learning;pattern recognition;data mining	ML	5.706558554188823	-19.493183493575373	71679
4a4fc4ef8a6fcf02540e7567893a49f5dc7e7e9d	simplification of decision making matrix in fuzzy multiple attribute decision making	fuzzy multiple attribute decision making;topsis;rough set theory;rough set theory decision making fuzzy set theory matrix algebra;matrix algebra;multiple attribute decision making;fuzzy set theory;attribute reduction;attribute reduction fuzzy multiple attribute decision making decision making matrix topsis;decision making matrix;decision making workstations delta modulation engines fuzzy sets;rough set theory decision making matrix fuzzy multiple attribute decision making simplification theory attribute reduction algorithm order preserving attribute subsets knowledge reduction	In the multiple attribute decision making, the computational intensity of problems quickly increases with the number of attributes increases. In this paper, we will introduce the simplification theory and technology of decision matrix in fuzzy multiple attribute decision making, and propose an attribute reduction algorithm. The purpose of this algorithm is to find all of the simplest order-preserving attribute subsets and the order-preserving attribute core of the corresponding multiple attribute decision making problem. One of the simplest order-preserving attribute subsets will be exploited as a substitute of the original attribute set. Thus the original multiple attribute decision making problem could be simplified and unnecessary cost related to redundant attributes could be saved. Finally, an illustrative numerical example is given to demonstrate our approach. And through this example, a dissimilarity between the attribute reduction in decision making and the knowledge reduction in rough set theory is discovered.	algorithm;computation;computer program;fuzzy set;level of detail;numerical analysis;rough set;set theory;text simplification;type-2 fuzzy sets and systems;workstation	Zhi Pei;Li Zheng	2011	2011 IEEE International Conference on Industrial Engineering and Engineering Management	10.1109/IEEM.2011.6117874	topsis;rough set;optimal decision;attribute domain;computer science;machine learning;pattern recognition;data mining;decision rule;mathematics;fuzzy set;weighted sum model	DB	-2.572178804197683	-21.20031912762561	71747
34f567ee647ee1df77636a135e9c948a53f278bc	prebiotic competition between information variants, with low error catastrophe risks	energy;prebiotic evolution;dynamic system;error catastrophe;biada;autocatalysis	During competition for resources in primitive networks increased fitness of an information variant does not necessarily equate with successful elimination of its competitors. If variability is added fast to a system, speedy replacement of pre-existing and less-efficient forms of order is required as novel information variants arrive. Otherwise, the information capacity of the system fills up with information variants (an effect referred as “error catastrophe”). As the cost for managing the system’s exceeding complexity increases, the correlation between performance capabilities of information variants and their competitive success decreases, and evolution of such systems toward increased efficiency slows down. This impasse impedes the understanding of evolution in prebiotic networks. We used the simulation platform Biotic Abstract Dual Automata (BiADA) to analyze how information variants compete in a resource-limited space. We analyzed the effect of energy-related features (differences in autocatalytic efficiency, energy cost of order, energy availability, transformation rates and stability of order) on this competition. We discuss circumstances and controllers allowing primitive networks acquire novel information with minimal “error catastrophe” risks. We present a primitive mechanism for maximization of energy flux in dynamic networks. This work helps evaluate controllers of evolution in prebiotic networks and other systems where information variants compete. OPEN ACCESS Entropy 2015, 17 5275	automaton;catastrophe theory;channel capacity;entropy maximization;simulation;spatial variability	Radu Popa;Vily Marius Cimpoiasu	2015	Entropy	10.3390/e17085274	energy;autocatalysis;dynamical system;physics;quantum mechanics		0.22011531943579832	-10.705451470055646	71984
ed5dfac2e72df382f8dd3885f3659a8850322aeb	a dynamic multi-expert multi-criteria decision making model for risk analysis		Risks behavior may vary over time. New risks may appear, secondary risks may arises from the treatment of initial risks and the project managers may decide to ignore some insignificant risks. These facts demand to perform Risk Analysis in a dynamic way to support de- cisions by an effective and continuous process instead of single one. Risk Analysis is usually solved using Multi-Criteria Decision Making methods that are not efficient in handling the changes of risks exposure values dur- ing different periods. Therefore, our aim in this contribution is to propose a Dynamic Multi-Expert Multi-Criteria Decision Making Model for Risk Analysis, which allows not only to integrate the traditional dimensions of risks (probability and impact), but also to consider the current and past performances of risks exposure values in the project life cycle.		Yeleny Zulueta;Vladimir Martell;Juan Carlos Martínez;Luis Martínez-López	2013		10.1007/978-3-642-45114-0_11	risk management tools;risk analysis;management science;operations research	ML	-3.369772951595436	-12.347434439829925	72097
55e58ba46b40409d902bbd117d37ff3d245924da	can religion explain cross-country differences in inequality? a global perspective		This paper investigates the relationship between different religious groups and income inequality. In particular, we examine whether different religious groups (Christianity, Islam, Judaism and Buddhism) have an impact on income inequality in a global perspective. Furthermore, this paper also sheds light on the effects of major religious sub-groups (Christianity and Islam) on income inequality. Using data for 130 countries from 1970 to 2013, we estimate a panel data model, controlling for religious beliefs, savings rate, arable land rate and age-dependency ratio. Our results indicate that religion plays an important role in explaining income inequality. In particular, we found that Islam and Judaism reduce income inequality while in general, Christianity and Buddhism increases inequality. However, the effects from sub-groups of Christianity on inequality are mixed; in particular, Anglican and Orthodox significantly reduce inequality while the effect from Catholic and Protestant is opposite. These findings are robust to different measures of inequality and alternative estimation techniques that take care of endogeneity.	social inequality	Amjad Naveed;Cong Wang	2018	Social Choice and Welfare	10.1007/s00355-017-1093-1		ECom	-2.392707412423178	-10.550198864183203	72128
594f9e581513f2420be3b4766e0d17581d565875	applications of the moving average of n  th  -order difference algorithm for time series prediction	pseudo periodical time series prediction;data collection;nth order difference;time series;data mining;moving average;machine learning;time series analysis;time series prediction;artificial neural network	Currently, as a typical problem in data mining, Times Series Analysis and Prediction are facing continuously more applications on a wide variety of domains. Huge data collections are generated or updated from science, military, financial and environmental applications. Prediction of the future trends based on previous and existing values is of a high importance and various machine learning algorithms have been proposed. In this paper we discuss results of a new approach based on the moving average of the n-order difference of limited range margin series terms. Based on our original approach, a new algorithm has been developed: performances on measurement records of sunspots for more than 200 years are reported and discussed. Finally, Artificial Neural Networks (ANN) are added for improving the precision of prediction by addressing the error of prediction in the initial approach.	approximation algorithm;artificial neural network;complexity;connectionism;data mining;longest common subsequence problem;machine learning;network model;neural networks;performance;time series	Yang Lan;Daniel Neagu	2007		10.1007/978-3-540-73871-8_25	computer science;data science;machine learning;time series;data mining;order of integration;artificial neural network;statistics	ML	8.355952848571048	-21.62029262671822	72360
f6eaf83a1d4d0037dbbc359957aabc44101cd9f5	the combination forecasting model for financial distress prediction of listed corporations based on support vector machines	forecasting;bp;combination forecast;combining forecast;support vector machines;financial management;logistic model;companies;support vector machines combination forecasting model financial distress prediction;financial distress;financial distress prediction;logistics;forecasting theory;business;logistic model combination forecast svm bp;predictive models;svm;support vector machine;predictive models support vector machines logistics economic forecasting financial management neural networks support vector machine classification computer science software engineering investments;combination forecasting model;data models;support vector machines financial management forecasting theory	This paper establish a new model of financial distress prediction---the combination forecasting model based on support vector machines (CFSVM), with Chinese listed corporations as samples. After rectifying the model, it gets higher predicting precision. Comparing to other warning models, the CFSVM model for listed corporations in financial distress prediction really has bigger application foreground because of its better characters such as higher accuracy rate.	distress (novel);inventory;rectifier;support vector machine	Wei Sun;Xingxing Yan;Yanhong Li	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1002	financial management;support vector machine;actuarial science;computer science;machine learning;data mining	SE	6.402422986035038	-19.020208883433025	72518
9ac74677226bc59b520ba0a4914bae7c0d3acce3	weighted value assessment of linear fractional programming for possibilistic multi-objective problem	weighting method;weight assessment;frr;multi objective decision making;possibilistic programming;mathematical modelling;linear fractional programming;fuzzy random regression;weighted value assessment;weight values	Determining the weight values is crucial in developing problem's mathematical model. The value of the model's weight must be determined before the model is solved. Nevertheless, the developed mathematical model is troublesome when the weight values are not exactly known, as relevant data are sometimes not given or difficult to obtain or estimate. Since numerous researches focusing in finding the solution of the model, this paper focuses on determining the weight value and a weighting method specifically for linear fractional programming to solve possibilistic programming of the multi-objective decision-making problem. Fuzzy random regression approach is applied to estimate the multi-objective model's weight value. Meanwhile, the minimal and maximal values of the objective function are utilised in determination for objective function weight value. Since most of the weight values in the developed model discusses in this paper are estimated from real data, assessment to these weights value in the objective function is executed. The weight value assessment uses weight absolute percentage error of fuzzy demand WAPE_FD. This analysis concludes that it is worthwhile to pursue proposed solution approach to the multi-objective evaluation scheme, which addresses some limitation to determine and assess the weight values within fuzzy circumstances.	fractional programming;linear-fractional programming	Nureize Arbaiy;Pei-Chun Lin	2016	IJAIP	10.1504/IJAIP.2016.074777	mathematical optimization;linear-fractional programming;machine learning;mathematical model	Logic	-0.47496678402563414	-18.01218889679775	72729
4603f897dae728668395c315401dde53fb4f5404	evolved bayesian network models of rig operations in the gulf of mexico	databases;belief networks;commercial market intelligence databases;bayesian network;availability;drilling bayesian methods petroleum data models databases geology availability;drilling rigs;oil drilling;evolved bayesian network models;bayesian methods;gulf of mexico;operations research;mexico gulf;ods petrodata ltd;petroleum;drilling;geology;ods petrodata ltd evolved bayesian network models mexico gulf drilling rigs genetic algorithms k2ga chainga rig operations management commercial market intelligence databases;chainga;competitive intelligence;genetic algorithm;genetic algorithms;operations management;rig operations management;operations research belief networks competitive intelligence genetic algorithms oil drilling;data models;k2ga	The operation of drilling rigs is highly expensive. It is therefore important to be able to identify and analyse factors affecting rig operations. We investigate the use of two Genetic Algorithms, K2GA and ChainGA, to induce a Bayesian Network model for the real world problem of Rig Operations Management. We sample from a unique dataset derived from the commercial market intelligence databases assembled by ODS-Petrodata Ltd. We observe a trade-off between K2GA, which finds significantly better scoring networks on our dataset, and ChainGA, which uses only one quarter of the computation time. We analyse the best structures produced from an industry standpoint and conclude by outlining a few potential applications of the models to support rig operations.	bayesian network;computation;data drilling;database;genetic algorithm;gulf of evaluation;network model;operational data store;time complexity	François A. Fournier;John A. W. McCall;Andrei Petrovski;Peter J. Barclay	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586021	simulation;genetic algorithm;competitive intelligence;computer science;machine learning;operations research	AI	4.473158808745225	-17.734995531858	72732
9d158c575a190d65a87018ad917e6d9f54138a75	dependence between volatility of stock price index returns and volatility of exchange rate returns under qe programs: case studies of thailand and singapore		Abstract This study found the evidences of the dependence between the volatility of stock price index returns and the volatility of exchange rate returns measured against US Dollar and Japanese Yen, and the independence between the volatility of stock price index returns and the volatility of exchange rate returns measured against Euro, in both Thailand and Singapore, under the operation of QE programs. It also found that all bivariate copula of the volatility of stock price index returns—the volatility of Thai Baht/US Dollar exchange rate returns, and the volatility of stock price index returns—the volatility of Thai Baht/Japanese Yen of Thailand, had a degree of dependence greater than that of Singapore. This can be explained that the QE programs can affect capital flows to Thailand and Singapore, and also may have different effects on the volatility of each exchange rate returns and the volatility of stock price index returns, of the individual country. This information can be useful for policy makers and investors so that they can directly focus on avoiding adverse implications from the operation of QE programs, in terms of the risks incurred from the volatility of exchange rate returns and the volatility of stock price index returns.	bivariate data;query expansion;volatility	Ornanong Puarattanaarunkorn;Teera Kiatmanaroch;Songsak Sriboonchitta	2016		10.1007/978-3-319-27284-9_27	financial economics;forward volatility;volatility swap;implied volatility;volatility smile;volatility risk premium;finance;macroeconomics;business	Theory	3.058817795788929	-13.634339029249663	72744
fac229ec67f47ee3b646f644bf8e6a2e95d30415	fuzzy acceptance sampling and characteristic curves	sample size;linguistic variable;fuzzy set;fuzzy set theory;fuzzy sets;sampling plan;acceptance sampling;characteristic curves	Acceptance sampling is primarily used for the inspection of incoming or outgoing lots. Acceptance sampling refers to the application of specific sampling plans to a designated lot or sequence of lots. The parameters of acceptance sampling plans are sample sizes and acceptance numbers. In some cases, it may not be possible to define acceptance sampling parameters as crisp values. These parameters can be expressed by linguistic variables. The fuzzy set theory can be successfully used to cope with the vagueness in these linguistic expressions for acceptance sampling. In this paper, the main distributions of acceptance sampling plans are handled with fuzzy parameters and their acceptance probability functions are derived. Then the characteristic curves of acceptance sampling are examined under fuzziness. Illustrative examples are given.	acceptance testing;emoticon;fuzzy set;nl (complexity);sampling (signal processing);set theory;vagueness	Ebru Turanoglu;Ihsan Kaya;Cengiz Kahraman	2012	Int. J. Comput. Intell. Syst.	10.1080/18756891.2012.670518	discrete mathematics;computer science;artificial intelligence;data mining;mathematics;fuzzy set;statistics	AI	0.3035806031816348	-18.875585726619708	73002
a4310e3881f669be80f0f2944345615e8db70a64	traffic equilibrium assignment based on interval-valued fuzzy trip costs	interval estimation;fuzzy number;road traffic;variational techniques fuzzy set theory road traffic statistical analysis transportation;variational techniques;variational inequality interval valued fuzzy number traffic equilibrium problem;fuzzy set theory;equilibrium problem;statistic interval estimation traffic equilibrium assignment interval valued fuzzy trip costs traffic network interval valued fuzzy numbers variational inequality;interval valued fuzzy number;statistical analysis;transportation;estimation educational institutions mathematical model vehicles roads fuzzy systems;membership function;traffic equilibrium problem;variational inequality	In this paper, we present a new equilibrium assignment model for traffic network whose trip costs are described as interval-valued fuzzy numbers. The new equilibrium principle is defined based on ranking fuzzy numbers and equilibrium condition is formulated as a variational inequality. The membership function of the interval-valued fuzzy number is constructed by the statistic interval estimation. A numerical example can be interpreted the proposed model obtains more realistic results than the UE.	calculus of variations;fuzzy number;numerical analysis;real life;social inequality;variational inequality;variational principle	Xiaoceng Wu;Jun Wan;Feng Huang	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233815	transport;mathematical optimization;discrete mathematics;interval estimation;variational inequality;membership function;defuzzification;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set;mathematical economics;fuzzy set operations	Robotics	-0.08112185546255102	-17.720152583487767	73018
f86bd8017b3eaff7cea1bb29c83d4d0dc78d3b82	rough set approximations on granular structures and feature characterizations	granular structures;knowledge space;rough sets;topology spaces	This paper focuses on generalization of rough set model. First a generalized definition of rough set approximations is proposed on general granular structure, so as to rough set models on some special granular structures are its special cases, these special granular structures include covering, knowledge space, topology space and Pawlak approximation space. Some basic notions are understood as counterparts in topology, and basic properties of new rough set approximations are investigated. Furthermore, with the defined lower and upper approximation operators, we can characterize the features of granular structures, and obtain positive and negative certainty decision rules. © 2010 Springer-Verlag.	approximation;rough set	Tong-Jun Li;Yan-Ling Jing	2010		10.1007/978-3-642-17622-7_9	decision rule;operator (computer programming);discrete mathematics;rough set;certainty;knowledge space;mathematics	Theory	-1.3616874128912164	-23.892763971371675	73508
aa09fb863a580c7c9b9967ae6cffa06969dcad66	computing generalized belief functions for continuous fuzzy sets	belief function;fuzzy set	Abstract   Intelligent systems often need to deal with various kinds of uncertain information. It is thus essential to develop evidential reasoning models that (1) can cope with different kinds of uncertain information in a theoretically sound manner and (2) can be implemented efficiently in a computer system. Generalizing the Dempster-Shafer theory to fuzzy sets has been suggested as a promising approach for dealing with probabilistic data, vague concepts, and incomplete information in a uniform framework. However, previous efforts in this area do not preserve an important principle of D-S theory—that belief and plausibility measures are the lower and upper bounds on belief measures. Recently, Yen proposed an alternative approach in which the degree of belief and the degree of plausibility of a fuzzy set are interpreted as its lower and upper belief measure, respectively. This paper briefly describes his generalized D-S reasoning model and discusses the computational aspects of the model. In particular, efficient algorithms are presented for computing the generalized belief function and plausibility functions for strong convex fuzzy sets, which are a wide class of fuzzy sets used most frequently in existing fuzzy intelligent systems. The algorithm not only facilitates the application of the generalized D-S model but also provides the basis for developing efficient algorithms for more general cases.	fuzzy set	John Yen	1992	Int. J. Approx. Reasoning	10.1016/0888-613X(92)90037-Z	membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;belief structure;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy set operations	AI	-2.4004074956819355	-23.599553981200252	73604
4fa68a25872bfbd847cfb8d21e9ee04b6eec7c2a	"""summary of the special issue """"neutrosophic information theory and applications"""" at """"information"""" journal"""		Over a period of seven months (August 2017–February 2018), the Special Issue dedicated to “Neutrosophic Information Theory and Applications” by the “Information” journal (ISSN 2078-2489), located in Basel, Switzerland, was a success. The Guest Editors, Prof. Dr. Florentin Smarandache from the University of New Mexico (USA) and Prof. Dr. Jun Ye from the Shaoxing University (China), were happy to select—helped by a team of neutrosophic reviewers from around the world, and by the “Information” journal editors themselves—and publish twelve important neutrosophic papers, authored by 27 authors and coauthors. There were a variety of neutrosophic topics studied and used by the authors and coauthors in Multi-Criteria (or Multi-Attribute and/or Group) Decision-Making, including Cross Entropy-Based MAGDM, Neutrosophic Hesitant Fuzzy Prioritized Aggregation Operators, Biparametric Distance Measures, Pattern Recognition and Medical Diagnosis, Intuitionistic Neutrosophic Graph, NC-TODIM-Based MAGDM, Neutrosophic Cubic Set, VIKOR Method, Neutrosophic Multiple Attribute Group Decision-Making, Competition Graphs, Intuitionistic Neutrosophic Environment, Neutrosophic Commutative N-Ideals, Neutrosophic N-Structures Applied to BCK/BCI-Algebras, Neutrosophic Similarity Score, Weighted Histogram, Robust Mean-Shift Tracking, and Linguistic Neutrosophic Cubic Numbers. Neutrosophic logic, symbolic logic, set, probability, statistics, etc., are, respectively, generalizations of fuzzy and intuitionistic fuzzy logic and set, classical and imprecise probability, classical statistics, and so on. Neutrosophic logic, symbol logic, and set are gaining significant attention in solving many real-life problems that involve uncertainty, impreciseness, vagueness, incompleteness, inconsistency, and indeterminacy. A number of new neutrosophic theories have been proposed and have been applied in computational intelligence, multiple-attribute decision making, image processing, medical diagnosis, fault diagnosis, optimization design, etc. This Special Issue gathers original research papers that report on the state of the art, as well as on recent advancements in neutrosophic information theory in soft computing, artificial intelligence, big and small data mining, decision-making problems, pattern recognition, information processing, image processing, and many other practical achievements. In the first chapter (NS-Cross Entropy-Based MAGDM under Single-Valued Neutrosophic Set Environment), the authors Surapati Pramanik, Shyamal Dalapati, Shariful Alam, Florentin Smarandache, Tapan Kumar Roy propose a new cross entropy measure under a single-valued neutrosophic set (SVNS) environment, namely NS-cross entropy, and prove its basic properties. Additionally, they define the weighted NS-cross entropy measure, investigate its basic properties, and develop a novel multi-attribute group decision-making (MAGDM) strategy that is free from the drawbacks of asymmetrical behavior and undefined phenomena. It is capable of dealing with an unknown weight of attributes and an unknown weight of decision-makers. Finally, a numerical example of multi-attribute Information 2018, 9, 49; doi:10.3390/info9030049 www.mdpi.com/journal/information Information 2018, 9, 49 2 of 4 group decision-making problem of investment potential is solved to show the feasibility, validity and efficiency of the proposed decision-making strategy. Single-valued neutrosophic hesitant fuzzy set (SVNHFS) is a combination of a single-valued neutrosophic set and a hesitant fuzzy set, and its aggregation tools play an important role in the multiple criteria decision-making (MCDM) process. The second paper (Generalized Single-Valued Neutrosophic Hesitant Fuzzy Prioritized Aggregation Operators and Their Applications to Multiple Criteria Decision-Making) investigates MCDM problems in which the criteria under SVNHF environment are in different priority levels. First, the generalized single-valued neutrosophic hesitant fuzzy prioritized weighted average operator and generalized single-valued neutrosophic hesitant fuzzy prioritized weighted geometric operator are developed based on the prioritized average operator. Second, some desirable properties and special cases of the proposed operators are discussed in detail. Third, an approach combining the proposed operators and the score function of single-valued neutrosophic hesitant fuzzy element is constructed to solve MCDM problems. Finally, the authors Rui Wang, Yanlai Li provide the example of investment selection to illustrate the validity and rationality of the proposed method. Single-valued neutrosophic sets (SVNSs) handling the uncertainties characterized by truth, indeterminacy, and falsity membership degrees are a more flexible way of capturing uncertainty. In the third paper (Some New Biparametric Distance Measures on Single-Valued Neutrosophic Sets with Applications to Pattern Recognition and Medical Diagnosis), the authors Harish, Garg and Nancy propose some new types of distance measures, overcoming the shortcomings of the existing measures, for SVNSs with two parameters along with their proofs. The various desirable relations between the proposed measures are also derived. A comparison between the proposed and existing measures is performed in terms of counter-intuitive cases for showing its validity. The proposed measures are illustrated with case studies of pattern recognition, as well as medical diagnoses, along with the effect of the different parameters on the ordering of the objects. A graph structure is a generalization of simple graphs. Graph structures are very useful tools for the study of different domains of computational intelligence and computer science. In the fourth research paper, Certain Concepts in Intuitionistic Neutrosophic Graph Structures, the authors Muhammad Akram and Muzzamal Sitara introduce certain notions of intuitionistic neutrosophic graph structures, illustrating these notions with several examples. They investigate some related properties of intuitionistic neutrosophic graph structures, and also present an application of intuitionistic neutrosophic graph structures. A neutrosophic cubic set is the hybridization of the concept of a neutrosophic set and an interval neutrosophic set. A neutrosophic cubic set has the capacity to express the hybrid information of both the interval neutrosophic set and the single valued neutrosophic set simultaneously. Since the neutroaophic cubic sets have only recently been defined, not much research on the operations and applications of neutrosophic cubic sets is currently available in the literature. In the fifth paper, NC-TODIM-Based MAGDM under a Neutrosophic Cubic Set Environment, the authors Surapati Pramanik, Shyamal Dalapati, Shariful Alam and Tapan Kumar Roy propose score and accuracy functions for neutrosophic cubic sets and prove their basic properties. They also develop a strategy for ranking of neutrosophic cubic numbers based on the score and accuracy functions. The authors firstly develop a TODIM (Tomada de decisao interativa e multicritévio) in the neutrosophic cubic set (NC) environment, which is called the NC-TODIM. They establish a new NC-TODIM strategy for solving multi-attribute group decision-making (MAGDM) problems in neutrosophic cubic set environments. They illustrate the proposed NC-TODIM strategy for solving a multi-attribute group decision-making problem to show the applicability and effectiveness of the developed strategy. They also conduct sensitivity analysis to show the impact of the ranking order of the alternatives on the different values of the attenuation factor of losses for multi-attribute group decision-making strategies. In the sixth paper, VIKOR Method for Interval Neutrosophic Multiple Attribute Group Decision-Making, the authors Yu-Han Huang, Gui-Wu Wei and Cun Wei extend the VIKOR method to multiple-attribute Information 2018, 9, 49 3 of 4 group decision-making (MAGDM) with interval neutrosophic numbers (INNs). Firstly, the basic concepts of INNs are briefly presented. The method first aggregates all individual decision-makers’ assessment information based on an interval neutrosophic weighted averaging (INWA) operator, and then employs the extended classical VIKOR method to solve MAGDM problems with INNs. The validity and stability of this method are verified by example analysis and sensitivity analysis, and its superiority is illustrated by a comparison with the existing methods. The concept of intuitionistic neutrosophic sets provides an additional possibility for representing imprecise, uncertain, inconsistent and incomplete information that exists in real situations. The seventh research article (Certain Competition Graphs Based on Intuitionistic Neutrosophic Environment) presents the notion of intuitionistic neutrosophic competition graphs. Then, the authors Muhammad Akram and Maryam Nasir discuss p-competition intuitionistic neutrosophic graphs and m-step intuitionistic neutrosophic competition graphs. Further, applications of intuitionistic neutrosophic competition graphs in ecosystem and career competition are described. The notion of a neutrosophic commutative N-ideal in BCK-algebras is introduced in the eighth paper (Neutrosophic Commutative N-Ideals in BCK-Algebras), and several properties are investigated. Relations between a neutrosophic N-ideal and a neutrosophic commutative N-ideal are discussed by the authors Seok-Zun Song, Florentin Smarandache, and Young Bae Jun. Characterizations of a neutrosophic commutative N-ideal are considered. Neutrosophic N-Structures Applied to BCK/BCI-Algebras is the title of the ninth paper. The notions of a neutrosophic N-subalgebra and a (closed) neutrosophic N-ideal in a BCK/BCI-algebra are introduced by authors Young Bae Jun, Florentin Smarandache and Hashem Bordbar, and several related properties are investigated. Characterizations of a neutrosophic N-subalgebra and a neutrosophic N-ideal are	artificial intelligence;brain–computer interface;common criteria;computational intelligence;computer science;cross entropy;cubic ide;cubic function;data mining;fuzzy logic;fuzzy set;graph (discrete mathematics);graph theory;graphical user interface;han unification;image processing;indeterminacy in concurrent computation;information processing;information theory;international standard serial number;jun wang (scientist);mathematical optimization;nc (complexity);numerical analysis;pattern recognition;rationality;real life;serial ata;sitara arm processor;soft computing;switzerland;undefined behavior;vagueness	Florentin Smarandache;Jun Ye	2018	Information	10.3390/info9030049	fuzzy logic;information theory;data mining;vikor method;computer science;commutative property;graph	AI	-2.2070689761497024	-21.520788073799583	73708
76ed982aebcfacd00013426a66513dbcfa2f8653	improved sea level anomaly prediction through combination of data relationship analysis and genetic programming in singapore regional waters	data model integration;error forecasting;average mutual information;tide surge interaction	With recent advances in measurement and information technology, there is an abundance of data available for analysis and modelling of hydrodynamic systems. Spatial and temporal data coverage, better quality and reliability of data modelling and data driven techniques have resulted in more favourable acceptance by the hydrodynamic community. The data mining tools and techniques are being applied in variety of hydro-informatics applications ranging from data mining for pattern discovery to data driven models and numerical model error correction. The present study explores the feasibility of applying mutual information theory by evaluating the amount of information contained in observed and prediction errors of non-tidal barotropic numerical modelling (i.e. assuming that the hydrodynamic model, available at this point, is best representation of the physics in the domain of interest) by relating them to variables that reflect the state at which the predictions are made such as input data, state variables and model output. In addition, the present study explores the possibility of employing ‘genetic programming’ (GP) as an offline data driven modelling tool to capture the sea level anomaly (SLA) dynamics and then using them for updating the numerical model prediction in real time applications. These results suggest that combination of data relationship analysis and GP models helps to improve the forecasting ability by providing information of significant predicative parameters. It is found that GP based SLA prediction error forecast model can provide significant improvement when applied as data assimilation schemes for updating the SLA prediction obtained from primary hydrodynamic models. & 2014 Elsevier Ltd. All rights reserved.	anomaly detection;coefficient;data assimilation;data mining;data modeling;error detection and correction;genetic programming;impredicativity;informatics;information theory;mathematical model;mutual information;numerical analysis;online and offline;service-level agreement	Alamsyah Kurniawan;Seng Keat Ooi;Vladan Babovic	2014	Computers & Geosciences	10.1016/j.cageo.2014.07.007	econometrics;computer science;machine learning;data mining;operations research;statistics	AI	9.970159750388529	-18.643657765924527	73786
41119bcbff7a62bd4c69d14fbd3658fa86af0fdd	a common set of weights for ranking decision-making units with undesirable outputs: a double frontiers data envelopment analysis approach				Lei Chen;Fei-Mei Wu;Feng Feng;Fujun Lai;Ying-Ming Wang	2018	APJOR	10.1142/S0217595918500392		ML	-3.489614499250455	-15.559248123442636	73924
9439cf16d0b5b61c2534ac4f9acb852c7d219c8f	an empirical analysis of electricity consumption intensity based on structure factor and efficiency factor	efficiency factors;empirical analysis;consumption intensity;structure factor;electricity consumption;industrial structure;energy consumption;structural factors;structure share;economic development;china;electricity consumption intensity;efficiency share	Energy, especially the electricity, is the basis of a country's economic development. It is of great significance for a country to develop the economy. Meanwhile, this also can illustrate the mechanism and change trend of electricity consumption. This paper regards the intensity of electricity consumption as the research object and uses it to study the quantitative relationship between electricity and economy. Mainly considering the structural factor and efficiency factor, this paper divided the change of electricity consumption intensity into structure share and efficiency share and put forward their calculating method, then used the statistical data from 1995 to 2007 of Beijing to calculate the structure share and efficiency share in the change of electricity consumption intensity and analysed the impact of economic structure adjustment and efficiency improvement on it. The result shows that the development of industrial structure and improvement of electricity utilisation efficiency are two major factors which could promote the decline of the electricity consumption intensity, based on it, some suggestions for making the industry's development plan and further study of the relationship between electricity and economy were given.		Dongxiao Niu;Zhihong Gu	2011	IJITM	10.1504/IJITM.2011.037765	economics;structure factor;microeconomics;economy;commerce	Web+IR	2.8818935791272415	-14.371470519941834	73993
de2695761e59be9dc7899fe87b42d9a9e6558c23	some generalized uncertain linguistic aggregating operators	generalized uncertain linguistic weighted aggregating operator;generalized uncertain linguistic ordered aggregating operator generalized uncertain linguistic weighted aggregating operator multiple attribute group decision making problem attribute weight expert weight attribute preference value linguistic hybrid aggregating operator;linguistic variable;probability density function;linguistic hybrid aggregating operator;data mining;mathematical operators;attribute preference value;open wireless architecture;aggregation operator;decision theory;attribute weight;group decision making;computational linguistics;economics;mathematical operators computational linguistics decision making decision theory;fuzzy systems;expert weight;generalized uncertain linguistic ordered aggregating operator;multiple attribute group decision making problem;decision making conference management engineering management environmental economics art;harmonic analysis	With respect to multiple attribute group decision making problem with uncertain linguistic information, in which the attribute weights and expert weights take the form of real numbers, and the attribute preference values take the form of uncertain linguistic variables, some new generalized uncertain linguistic aggregating operators have been proposed: generalized uncertain linguistic weighted aggregating (GULWA) operator, generalized uncertain linguistic ordered weighted aggregating (GULOWA) operator and generalized uncertain linguistic hybrid aggregating (GULHA) operator. It has been shown that both GULWA and GULOWA operators are the special case of the GULHA operator. The GULHA operator generalizes both the GULWA and GULOWA operators, and reflects the importance degrees of both the given arguments and their ordered positions. Based on the GULWA and GULHA operators, an approach has been proposed to solve the MAGDM problems under uncertain linguistic environment. Finally, an illustrative example is given to verify the developed approach and to demonstrate its practicality and effectiveness.		Guiwu Wei	2009	2009 IITA International Conference on Services Science, Management and Engineering	10.1109/SSME.2009.92	probability density function;group decision-making;decision theory;computer science;artificial intelligence;computational linguistics;machine learning;harmonic analysis;data mining;mathematics;statistics	DB	-2.4560973685438388	-21.1384013259677	73998
a5a827d38683af866b375eaf593cd48c34b14f38	freight vehicle travel time prediction using sparse gaussian processes regression with trajectory data		Travel time prediction is important for freight transportation companies. Accurate travel time prediction can help these companies make better planning and task scheduling. For several reasons, most companies are not able to obtain traffic flow data from traffic management authorities, but a large amount of trajectory data were collected everyday which has not been fully utilised. In this study, we aim to fill this gap and performed travel time prediction for freight vehicles at individual level using sparse Gaussian processes regression (SGPR) models with trajectory data. The results show that the prediction performance can be gradually improved by adding more mean speed estimates of traveled distance from the first 5 min as the real-time information. The overall performances of SGPR models are very similar to full GP, supported vector regression (SVR) and artificial neural network (ANN) models. The computational complexity of SGPR models is (O(mn^2)), and it does not require lengthy model fitting process as SVR and ANN. This makes GP models more practicable for real-world practice in large-scale transportation data analyses.	gaussian process;sparse	Xia Li;Ruibin Bai	2016		10.1007/978-3-319-46257-8_16	econometrics;simulation;machine learning	ML	8.639010314865153	-14.777247904487755	74016
dcd488c944ac45472851323d5917f1579084dec7	support vector machines approach to credit assessment	credit assessment;banking industry;support vector machines;statistical method;classification;artificial intelligent;machine learning;prediction accuracy;support vector machine;credit cards	Credit assessment has attracted lots of researchers in financial and banking industry. Recent studies have shown that Artificial Intelligence (AI) methods are competitive to statistical methods for credit assessment. This article applies support vector machines (SVM), a relatively new machine learning technique, to the credit assessment problem for better explanatory power. The structure of SVM has many computation advantages, such as special direction at a finite sample and irrelevance between the complexity of algorithm and the sample dimension. A real credit card data experiment shows that SVM method has outstanding assessment ability. Compared with the method that is currently used by a major Chinese bank, the SVM method has a great potential superiority in predicting accuracy.	algorithm;artificial intelligence;computation;machine learning;relevance;support vector machine	Jianping Li;Jingli Liu;Weixuan Xu;Yong Shi	2004		10.1007/978-3-540-25944-2_115	support vector machine;computer science;data science;machine learning;data mining	AI	6.463822012898803	-19.956423700077963	74289
eabe5a35903feef849836e9749615ce4321f4643	application of the fuzzy-stochastic methodology to appraising the firm value as a european call option	fuzzy set;stochastic process;finance;fuzzy number;pricing;financial decision making;fuzzy sets;decision support system;stochastic processes;stochastic model;contingent claim;firm value	The valuing of a firm equity as a call option is a crucial problem in financial decision-making. There are two basic aspects that are studied; contingent claim features (payoff functions) and risk (stochastic process of underlying assets). However, non-preciseness (vagueness, uncertainty) of input data is often neglected. Thus, a combination of risk (stochastic) and uncertainty (fuzzy instruments) could be a useful approach in calculating a firm value as a call option. The Black–Scholes methodology of appraising equity as a European call option is applied. Fuzzy–stochastic methodology under fuzzy numbers ( T -numbers) is proposed and described. Fuzzy–stochastic model of appraising a firm equity is proposed. Input data are in a form of fuzzy numbers and result, firm possibility-expected equity value is also determined vaguely as a fuzzy set. Illustrative example is introduced.		Zdenek Zmeskal	2001	European Journal of Operational Research	10.1016/S0377-2217(01)00042-X	financial economics;stochastic process;actuarial science;economics;type-2 fuzzy sets and systems;computer science;artificial intelligence;marketing;finance;mathematics;fuzzy set	Theory	0.747510413001747	-12.438417762444187	74302
d5df02f020466858b2838b6c40b22b1ce202e620	fuzzy irrigation decision support system	decision support system	Water is a limiting factor in agriculture and when improperly managed reduces the yield potential of crops. The objective of this study is to develop a Fuzzy Irrigation Decision Support System (FIDSS) to optimize water management for soybean production. Management of irrigation systems for greatest benefit requires an understanding of many physical, biological and chemical processes, and economical factors. Such processes are very complicated and involves many uncertainties. During the last 20 years, considerable progress has been made in developing computer crop growth simulation models. These models are developed with the desire to incorporate quantitatively the fundamental mechanisms controlling the above processes. However, this objective has been compromised by simplifying the mechanisms’ representations by general process descriptions which use empirical relationships determined from experimental data. In most cases, it is not possible to collect and correlate data for all conditions due to the nature of these processes. For example, variables such as soil water uptake by roots, soil water content at various depths and water lost by evapotranspiration interact with each other in a complex way making it hard to determine fundamental quantitative relationships among these variables. This study is therefore undertaken to begin the development of a mechanistic model using qualitative rather than quantitative relationships of the most important variables for making irrigation decisions. The Fuzzy set theory introduce by Lofti Zadeh in 1965 provides the framework to cope with ambiguity and unnaturalness of the traditional crisp method in this domain. Fuzzy expert systems utilize human experience and decision methods, and they have proven to be valuable when dealing with non-linear and complex relationships. In FIDSS we have developed membership functions for 21 important variables of three primary component s: 1) variables for plant, such as, leaf area index (LAI) and evapotranspiration (EP); 2) variables for water status and flow in soil, such as, soil-water content in a layer (0~) and infilteration (IN); and 3) variables for weather, such as, maximum temperature (TM) and solar radiation (RAD). FIDSS has 163 rules in its knowledge base specified as follows:	decision support system;expectation propagation;expert system;fuzzy set;knowledge base;nonlinear system;process (computing);rapid application development;set theory;simulation	Hong Xiang;Brahm P. Verma;Gerrit Hoogenboom	1994			decision support system;intelligent decision support system;computer science;artificial intelligence	AI	3.459417520810717	-19.417439234875406	74317
0992a4502661bb216f5dd4bf2d43a3d9bd743254	oil price trackers inspired by immune memory	artificial intelligent;evolutionary computing;long term memory	We outline initial concepts for an immune inspired algorithm to evaluate and predict oil price time series data. The proposed solution evolves a short term pool of trackers dynamically, with each member attempting to map trends and anticipate future price movements. Successful trackers feed into a long term memory pool that can generalise across repeating trend patterns. The resulting sequence of trackers, ordered in time, can be used as a forecasting tool. Examination of the pool of evolving trackers also provides valuable insight into the properties of the crude oil market.	algorithm;memory pool;time series	William O. Wilson;Phil Birkin;Uwe Aickelin	2006	CoRR			Vision	7.266852656324316	-15.190887568911895	74442
c2238139f91968429e6ca9cfe1b2ef50b14f79be	continuous m-dimensional distorted probabilities		Abstract Fuzzy measures, also known as non-additive measures, monotonic games, and capacities, have been used in many contexts. For example, in economics, risk analysis, in computer science, computer vision and machine learning and, in general, in mathematics. However, when looking at applications, one of the problems that still needs to be solved is how the measure should be defined in an easy and intuitive way. When the reference set is finite, a few families of measures have been established, e.g. distorted probabilities, k-additive and decomposable measures. But, when the reference set is infinite, the only family is distorted probabilities. In this paper we give a definition for m -dimensional distorted probabilities in the case that the reference set is not finite, and we study some properties of this family. We also give a definition for hierarchically decomposable m -dimensional distorted probabilities that relates to another family of measures defined for the finite case.		Vicenç Torra;Montserrat Guillen;Miguel Santolino	2018	Information Fusion	10.1016/j.inffus.2017.12.004	artificial intelligence;discrete mathematics;machine learning;fuzzy logic;mathematics;monotonic function	NLP	0.511434205490394	-20.76700248816958	74576
7ed940aa4c506d228f95ce2e5a97bbc679213c9b	bonferroni distances with owa operators		The aim of the paper is to develop new aggregation operators using Bonferroni means, ordered weighted averaging (OWA) operators and some distance measures. We introduce the Bonferroni-Hamming weighted distance, Bonferroni OWA distance, and Bonferroni distances with OWA operators and weighted averages. The main advantages of using these operators are that they allow considering different aggregations contexts, multiple-comparison between each argument and distance measures in the same formulation.	aggregate function;algorithm;batman: arkham city;hamming distance;iw engine;ordered weighted averaging aggregation operator	Fabio Blanco-Mesa;José Maria Merigó Lindahl	2016	2016 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS)	10.1109/NAFIPS.2016.7851586	mathematical optimization;mathematical analysis;discrete mathematics;mathematics	DB	-1.2246778675719039	-21.404205280667316	74578
50ba499b15f9b61efe0c6f54a74ccce501fa98da	on complete sublattices of the algebra of truth values of type-2 fuzzy sets	absorption;fuzzy set;lattices;algebra fuzzy sets lattices mercury metals upper bound absorption;fuzzy set theory;fuzzy sets;upper bound;algebra;mercury metals;convex normal elements interval valued fuzzy sets subalgebras;interval valued fuzzy set;type 2 fuzzy set;convex normal elements;interval valued fuzzy sets;complete lattice;subalgebras	The algebra of truth values of type-2 fuzzy sets contains isomorphic copies of the algebra of truth values of type-1 fuzzy sets and the algebra of truth values of interval-valued fuzzy sets. The algebra of truth values of type-2 fuzzy sets is not a lattice, but these subalgebras are lattices, and in fact, are complete lattices. There are many other subalgebras that are lattices, for example, the subalgebra of convex normal elements. This paper examines several subalgebras which are lattices, with the goal of determining whether or not they are complete.	fuzzy set;type-2 fuzzy sets and systems	John Harding;Carol L. Walker;Elbert A. Walker	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295439	fuzzy logic;filtered algebra;boolean algebra;mathematical analysis;discrete mathematics;topology;membership function;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy subalgebra;truth function;mathematics;fuzzy set;field of sets	DB	-0.5144564413584047	-22.717530694652467	74653
8800bbd5a853f473b667a40145c467e5275d2605	intelligent system for fish stock prediction and allowable catch evaluation	prediction of marine fish stocks;environmental factors;total allowable catch;marine fish;knowledge based system;semantic network;semantic networks;artificial intelligent;natural resource management;control system;intelligent system;knowledge based systems;environmental factor;value prediction;expert system	The paper describes an intelligent system for evaluation and prediction of marine fish stocks and determination of allowable quotas. The system can forecast population abundance and biomass, possible total allowable catch, determine the quota allocation to subjects of fishery, based on various criteria. As knowledge sources, the system uses the results of long-term observations and research, regular dependencies established by experts, and human experts themselves. As initial data, monitoring and sampling results are used. Practical use of the system in 1995–1997 demonstrated satisfactory agreement between the values predicted by the system and the actual population characteristics. The system has been developed using SIMER 1 MIR intelligent techniques (Osipov, G.S., Gukova, S.M., Komarov, S.I., Kurshev, E.P., 1987. Expert system tools for badly structured fields. In: Plander, I. (Ed.), Artificial Intelligence and Information-Control Systems of Robots-87. North-Holland, Amsterdam).  1999 Published by Elsevier Science Ltd. All rights reserved.	algorithm;artificial intelligence;caspian;control system;database;ecosystem;expert system;knowledge acquisition;knowledge base;personalization;regular expression;sampling (signal processing);solver;workstation	Ludmila Sazonova;Gennady Osipov;Maxim Godovnikov	1999	Environmental Modelling and Software	10.1016/S1364-8152(98)00100-5	computer science;engineering;artificial intelligence;operations management;knowledge-based systems;semantic network;ecology;operations research	AI	3.9229774067246015	-18.6440793466579	74910
b7f5c4d585412eb3b7e64321794d769e87f7e2d7	modelling fish habitat preference with a genetic algorithm-optimized takagi-sugeno model based on pairwise comparisons	science general;species distribution models	Species-environment relationships are used for evaluating the current status of target species and the potential impact of natural or anthropogenic changes of their habitat. Recent researches reported that the results are strongly affected by the quality of a data set used. The present study attempted to apply pairwise comparison to modelling fish habitat preference with Takagi-Sugeno-type fuzzy habitat preference models (FHPMs) optimized by a genetic algorithm (GA). The model was compared with the result obtained from the FHPM optimized based on mean squared error (MSE). Three independent data sets were used for training and testing of these models. The FHPMs based on pairwise comparison produced variable habitat preference curves from 20 different initial conditions in the GA. This could be partially ascribed to the optimization process and the regulations assigned. This case study demonstrates applicability and limitations of pairwise comparison-based optimization in an FHPM. Future research should focus on more flexible learning process to make a good use of the advantages of pairwise comparisons. Shinji Fukuda Kyushu University, 6-10-1 Hakozaki, Fukuoka 812-8581, Japan, e-mail: shinji-fkd@agr.kyushuu.ac.jp Willem Waegeman Ghent University, Couple links 653, 9000 Ghent, Belgium e-mail: Willem.Waegeman@ugent.be Ans Mouton Research Institute for Nature and Forest (INBO), Kliniekstraat 25, 1070 Brussels, Belgium e-mail: Ans.Mouton@inbo.be Bernard De Baets Ghent University, Couple links 653, 9000 Ghent, Belgium e-mail: Bernard.DeBaets@ugent.be	digital monster (virtual pet);email;evert willem beth;genetic algorithm;habitat;initial condition;maniac mansion;mathematical optimization;mean squared error	Shinji Fukuda;Willem Waegeman;Ans M. Mouton;Bernard De Baets	2011		10.1007/978-3-642-24001-0_34	econometrics;computer science;data mining	AI	7.391009133911917	-16.270325168059017	75129
84ec5ef4c675a997a12567448f4b7104956523d9	economie cycles and the shot model	investments economics low pass filters fluctuations filtering theory industries;filtering and prediction theory economic cybernetics;filtering and prediction theory;random effects shot model economic cycles discrete events filtering economic statistics economic decisions business cycle models cyclical fluctuations;economic cybernetics	It is shown how economic cycles can be generated by discrete events occurring at random. The economic impact of such events is described by the filtered output of a shot model. The necessary filtering is shown to arise naturally in economic systems because of the way economic statistics are compiled and because many economic processes have long time constants. Since a variety of economic decisions take the form of discrete events, it is argued that the model discussed represents a plausible addition to the stock of business cycle models. This leads to the question of how the model is affected by cyclical fluctuations in the rate at which random effects occur engendered by other sources of economic cycles. It is shown that there is a considerable tendency for the cycles generated by the two sources to interlock. Finally a number of applications of the model are discussed.	compiler;interlock (engineering);random effects model	J. A. Sharp	1984	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1984.6313343	mathematical economics;statistics	Theory	0.2164366158365903	-10.158952083232032	75165
e5547c50b4f01f9b38689bdfdb45c716d03c223e	the impact of training data tailoring on demand forecasting models in retail	predictive models training data models forecasting biological system modeling analytical models buildings;final forecasting model training data tailoring demand forecasting models retail business retail information systems data mining tools building forecasting models data mining algorithm training set preparation;retail data processing data mining demand forecasting	Demand forecasting plays a very important role in retail business. Retail information systems commonly store large amounts of data which are subsequently used by sophisticated data mining tools for building forecasting models. Quality of these models is usually measured through their predictive accuracy as their most important property, followed by other measures which consider average underestimate and overestimate costs etc. Even though the choice of data mining algorithm is usually paramount, training set cleansing and preparation has a significant influence on final model performance. This article discusses and analyses the impact of training set preparation and tailoring on a final forecasting model performance used in a real world example from the retail industry.	algorithm;artificial neural network;cluster analysis;data mining;demand optimization;information system;support vector machine;test set;venue (sound system)	Mladen Karan;Damir Pintar;Zoran Skocir;Mihaela Vranic;Adrian Alajkovic;Jelena Milojevic;Marina Plesa	2014	2014 37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.1109/MIPRO.2014.6859799	demand forecasting;data science;data mining	ML	5.000948481971671	-19.196895461780443	75185
e5b73c0313b2036e85dc327ec5e554245165f613	a new approach for choosing the most appropriate fuzzy ranking algorithm for solving madm problems	conference proceeding	There are many fuzzy ranking algorithms available to solve multi-attribute decision making (MADM) problems. Some are more suitable than others for particular decision problems. This paper proposes a new method for choosing the most appropriate fuzzy ranking algorithm for solving MADM problems based on the type and number of attributes and the number of alternatives, considering the least time consumption and the least computation for ranking alternatives. In addition, we develop a software to simulate three main fuzzy ranking algorithms: SAW, Negi, and Chen and Hwang (Chen and Hwang 1992). This software can be used in any MADM decision support system.	algorithm	Fahimeh Ramezani;Jie Lu	2012		10.1007/978-3-642-24806-1_2	mathematical optimization;machine learning;data mining;mathematics	AI	-2.88812006353167	-20.288321151601735	75237
1770df1c08baa005fb3c4723d3d14e2d1a1c495c	dynamic decision making without expected utility: an operational approach	arbre graphe;loterie;non expected utility;decision tree;funcion utilidad;dynamic consistency;uncertainty modeling;laminado;stochastic dominance;fonction poids;tree graph;lottery;systeme aide decision;fonction utilite;dominancia estocastica;expected utility;analisis decision;utility function;theorie modeles;sistema ayuda decision;prise decision;arbol decision;separability;decision maker;teoria decision;decision analysis;rolling;decision problem;utilidad no espera;systeme incertain;decision support system;utilidad dependiente orden;rank dependent utility;programacion lineal;separabilidad;utilite dependant du rang;theorie decision;theorie utilite;decision theory;teoria utilidad;funcion peso;linear programming;preferencia;expected utility theory;programmation lineaire;linear program;dominance stochastique;separabilite;preference;utilite attendue;loteria;weight function;arbol grafo;toma decision;sistema incierto;utilidad espera;laminage;teoria modelos;uncertain system;arbre decision;analyse decision;dynamic decision making;utilite non attendue;utility theory;model theory	Non-expected utility theories, such as rank dependent utility (RDU) theory, have been proposed as alternative models to EU theory in decision making under risk. These models do not share the separability property of expected utility theory. This implies that, in a decision tree, if the reduction of compound lotteries assumption is made (so that preferences at each decision node reduce to RDU preferences among lotteries) and that preferences at different decision nodes are identical (same utility function and same weighting function), then the preferences are not dynamically consistent; in particular, the sophisticated strategy, i.e., the strategy generated by a standard rolling back of the decision tree, is likely to be dominated w.r.t. stochastic dominance. Dynamic consistency of choices remains feasible, and the decision maker can avoid dominated choices, by adopting a non-consequentialist behavior, with his choices in a subtree possibly depending on what happens in the rest of the tree. We propose a procedure which: i) although adopting a non-consequentialist behavior, involves a form of rolling back of the decision tree; and ii) selects a non-dominated strategy that realizes a compromise between the decision maker’s discordant goals at the different decision nodes. Relative to the computations involved in the standard expected utility evaluation of a decision problem, the main computational increase is due to the identification of non-dominated strategies by linear programming. A simulation, using the rank dependent utility criterion, confirms the computational tractability of the model.	algorithm;backward induction;book;cit program tumor identity cards;computation;computer science;customer support;decision problem;decision theory;expected utility hypothesis;fits;influence diagram;linear programming;linear separability;requirement;simulation;tree (data structure);weight function	Thomas D. Nielsen;Jean-Yves Jaffray	2006	European Journal of Operational Research	10.1016/j.ejor.2004.05.029	mathematical optimization;optimal decision;influence diagram;decision theory;two-moment decision model;expected utility hypothesis;decision field theory;linear programming;artificial intelligence;stochastic dominance;decision tree;incremental decision tree;decision rule;mathematics;subjective expected utility;mathematical economics;evidential decision theory;welfare economics;von neumann–morgenstern utility theorem;weighted sum model	AI	0.8615867557030551	-15.890834692626845	75239
217bb03a35627993877bf927dc6ccc67c4f103b5	foreign exchange trading with support vector machines	support vector machine;foreign exchange	This paper analyzes and examines the general ability of Support Vector Machine (SVM) models to correctly predict and trade daily EUR exchange rate directions. Seven models with varying kernel functions are considered. Each SVM model is benchmarked against traditional forecasting techniques in order to ascertain its potential value as out-of-sample forecasting and quantitative trading tool. It is found that hyperbolic SVMs perform well in terms of forecasting accuracy and trading results via a simulated strategy. This supports the idea that SVMs are promising learning systems for coping with nonlinear classification tasks in the field of financial time series applications.	foreign exchange service (telecommunications);support vector machine	Christian Ullrich;Detlef Seese;Stephan K. Chalup	2006		10.1007/978-3-540-70981-7_62	support vector machine;kernel (statistics);time series;foreign exchange market;data mining;exchange rate;quantitative investing;nonlinear system;engineering	ML	6.677700014414881	-19.87264668768566	75270
5001957d30a19755300ebad366e0de77db5fb7c7	application of artificial neural networks in estimating participation in elections		It is approved that artificial neural networks can be considerable effective in anticipating and analyzing flows in which traditional methods and statics aren’t able to solve. In this article, by using two-layer feedforward network with tan-sigmoid transmission function in input and output layers, we can anticipate participation rate of public in kohgiloye and Boyerahmad Province in future presidential election of Islamic Republic of Iran with 91% accuracy. The assessment standards of participation such as confusion matrix and ROC diagrams have been approved our claims.	artificial neural network;confusion matrix;diagram;feedforward neural network;input/output;neural networks;receiver operating characteristic;sigmoid function;transaction authentication number;transfer function	Seyyed Reza Khaze;Mohammad Masdari;Sohrab Hojjatkhah	2013	CoRR	10.5121/ijitmc.2013.1303	computer science;artificial intelligence;machine learning;data mining;operations research	ML	6.457489523181252	-22.377855120481765	75531
18f69770d1277ab184be813a8d2d4de2f4df8ed0	undesirable factors in efficiency measurement	62 07;analisis envolvimiento datos;undesirable factor;matematicas aplicadas;mathematiques appliquees;undesirable factors;efficiency;production process;input output;eficacia;facteur non desirable;data envelopment analysis;data envelopment analysis dea;processus fabrication;efficacite;efficiency measurement;58a25;applied mathematics;data envelope analysis;proceso fabricacion;analyse enveloppement donnee	Many production processes yield both desirable factors (inputs/outputs) and undesirable ones. There are some models that evaluate efficiency level in the presence of undesirable factors. The current models consider only undesirable outputs (inputs). In this paper we propose a model for treating such factors in the framework of Data Envelopment Analysis (DEA). The proposed model considers both of the undesirable factors and we discuss efficiency measurement in the context of the model. A numerical example is given.		Abdollah Hadi-Vencheh;Reza Kazemi Matin;M. Tavassoli Kajani	2005	Applied Mathematics and Computation	10.1016/j.amc.2004.02.022	econometrics;data envelopment analysis;mathematics;operations research	ECom	-0.19646903079733172	-15.063760185545833	75661
b0fe251ce369f656e8ca2c5d5c6293778c5c3810	multicriteria inventory classification using a genetic algorithm	computadora;computers;multicriteria analysis;analytic hierarchy process;processus hierarchie analytique;ordinateur;multi criteria analysis;algoritmo genetico;computer;classification;inventory;administracion deposito;research paper;gestion stock;proceso jerarquia analitico;algorithme genetique;genetic algorithm;genetic algorithms;analisis multicriterio;analyse multicritere;analytical hierarchy process;inventory control;clasificacion;parameter optimization	One of the application areas of genetic algorithms is parameter optimization. This paper addresses the problem of optimizing a set of parameters that represent the weights of criteria, where the sum of all weights is 1. A chromosome represents the values of the weights, possibly along with some cut-off points. A new crossover operation, called continuous uniform crossover, is proposed, such that it produces valid chromosomes given that the parent chromosomes are valid. The new crossover technique is applied to the problem of multicriteria inventory classification. The results are compared with the classical inventory classification technique using the Analytical Hierarchy Process. @ 1998 Elsevier Science B.V.	analytical hierarchy;crossover (genetic algorithm);genetic algorithm;mathematical optimization	H. Altay Güvenir;Erdal Erel	1998	European Journal of Operational Research	10.1016/S0377-2217(97)00039-8	analytic hierarchy process;genetic algorithm;computer science;artificial intelligence;operations management;mathematics;operations research	AI	0.23909543274221182	-16.097660813662984	75867
7bab6ec567b05df8e594e0c0751835e4862323f0	fuzzy xnor connectives in fuzzy logic	t conorm;fuzzy xnor connective;t norm;fuzzy implication;fuzzy xor connective	In this paper, a generalized XNOR connective called fuzzy XNOR connective is introduced. First, the definition of fuzzy XNOR connective is proposed and its properties are analyzed. Then, two forms of fuzzy XNOR connectives are obtained by the composition of t-norms, t-conorms and fuzzy negations. Moreover, the relationships between fuzzy XNOR connectives and fuzzy Xor connectives introduced in Bedregal et al. (Electron Notes Theor Comput Sci 247:5–18, 2009) are discussed. At last, two new kinds of fuzzy implications are constructed by fuzzy XNOR connectives and other connectives, their main properties are also studied.	electron;fuzzy logic;logical connective;t-norm;xnor gate;xor	Yingfang Li;Keyun Qin;Xingxing He	2011	Soft Comput.	10.1007/s00500-011-0708-1	arithmetic;discrete mathematics;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;mathematics;t-norm;algorithm	AI	0.41242394429908946	-23.352858143044084	76137
094253a73e70c2830b74e735c3bfd379940071c4	variable selection for five-minute ahead electricity load forecasting	neural nets;regression analysis backpropagation load forecasting neural nets power engineering computing power markets;prediction algorithms;support vector regression;linear regression;industries;backpropagation;correlation prediction algorithms feature extraction load forecasting predictive models electricity industries;load forecasting;very short term electricity load forecasting;variable selection;autocorrelation analysis;power markets;power engineering computing;support vector regression algorithms;australian national electricity market;feature extraction;autocorrelation analysis very short term electricity load forecasting prediction variable selection;five minute ahead electricity load forecasting;electricity;predictive models;australian electricity data;regression analysis;prediction model;backpropagation neural networks;correlation;australian national electricity market variable selection five minute ahead electricity load forecasting autocorrelation analysis australian electricity data linear regression support vector regression algorithms backpropagation neural networks energy markets;prediction;energy markets	We use autocorrelation analysis to extract 6 nested feature sets of previous electricity loads for 5-minite ahead electricity load forecasting. We evaluate their predictive power using Australian electricity data. Our results show that the most important variables for accurate prediction are previous loads from the forecast day, 1, 2 and 7 days ago. By using also load variables from 3 and 6 days ago, we achieved small further improvements. The 3 bigger feature sets (37-51 features) when used with linear regression and support vector regression algorithms, were more accurate than the benchmarks. The overall best prediction model in terms of accuracy and training time was linear regression using the set of 51 features.	algorithm;autocorrelation;feature selection;support vector machine	Irena Koprinska;Rohen Sood;Vassilios G. Agelidis	2010	2010 20th International Conference on Pattern Recognition	10.1109/ICPR.2010.711	econometrics;prediction;computer science;machine learning;data mining;predictive modelling;feature selection;artificial neural network	Vision	8.517968617428274	-18.199134873820793	76515
89c475e5a7bd54dec040207f9ec909cb83dd6249	complex network construction method of disaster regional association based on optimized compressive sensing		iming at the disaster regional association issues, a complex network con- struction method of disaster regional association based on compressive sensing is proposed in this paper. The disaster system dynamic equations of network node are obtained through the use of power series expansion and the correlation coefficients between nodes are obtained through the use of compressed sensing theory, then the solving process is optimized by hyperbolic tangent function and revised Newton method, so as to realize the effective construction of the network topology. Expe- rimental results show that, complete network construction requires less amount of time series information and the construction result has a certain rationality.	complex network;compressed sensing	Si Liu;Cong Feng;Zhi-Juan Jia;Ming-Sheng Hu	2014		10.1007/978-3-319-09339-0_77	mathematical optimization;simulation;artificial intelligence	Robotics	6.524807687202596	-13.647648038076944	76796
10b9d4a6a277d872e8cdc2fecc1d61a3f85d62e4	fuzzy views on black-litterman portfolio selection model		In this paper, views of investor are described in fuzzy sets, and two fuzzy Black-Litterman models are constructed with fuzzy views and fuzzy random views respectively. In the models, expected returns and uncertainty matrix of views are redefined and the views are formulated by fuzzy approaches suitably. Then the models are tested with data from Chinese financial markets. Empirical results show that the fuzzy random views model performs the best, and both the fuzzy models are better than the traditional ones, demonstrating that the fuzzy approaches can contain more information in the views and measure the uncertainty more correctly.	black–litterman model	Yong Fang;Lin Bo;Daping Zhao;Shouyang Wang	2018	J. Systems Science & Complexity	10.1007/s11424-017-6330-2	fuzzy logic;econometrics;mathematical optimization;financial market;mathematics;fuzzy set;matrix (mathematics);black–litterman model;portfolio;fuzzy number	Logic	-1.9945992132538526	-19.79048901289052	77383
43f811c9a1c26704e1d76ba21cceea3f1b374d2d	possibilistic linear programming problems involving normal random variables	possibilistic linear programming problem;normal distribution;multi objective linear programming problem;fuzzy programming method;triangular possibility distribution	A new solution procedure of possibilistic linear programming problem is developed involving the right hand side parameters of the constraints as normal random variables with known means and variances and the objective function coefficients are considered as triangular possibility distribution. In order to solve the proposed problem, convert the problem into a crisp equivalent deterministic multi-objective mathematical programming problem and then solved by using fuzzy programming method. A numerical example is presented to illustrate the solution procedure and developed methodology.	linear programming	Suresh K. Barik;M. P. Biswal	2016	IJFSA	10.4018/IJFSA.2016070101	mathematical optimization;constraint programming;mathematical analysis;discrete mathematics;basic solution;second-order cone programming;linear-fractional programming;nonlinear programming;cutting stock problem;mathematics;active set method;quadratic programming	Theory	-0.39983868658677757	-18.01961311961375	77673
80afba6c70239a4976b6c9bdaa9139e24296be77	hesitant probabilistic fuzzy information aggregation using einstein operations		In this paper, a hesitant probabilistic fuzzy multiple attribute group decision making is studied. First, some Einstein operations on hesitant probability fuzzy elements such as the Einstein sum, Einstein product, and Einstein scalar multiplication are presented and their properties are discussed. Then, several hesitant probabilistic fuzzy Einstein aggregation operators, including the hesitant probabilistic fuzzy Einstein weighted averaging operator and the hesitant probabilistic fuzzy Einstein weighted geometric operator and so on, are introduced. Moreover, some desirable properties and special cases are investigated. It is shown that some existing hesitant fuzzy aggregation operators and hesitant probabilistic fuzzy aggregation operators are special cases of the proposed operators. Further, based on the proposed operators, a new approach of hesitant probabilistic fuzzy multiple attribute decision making is developed. Finally, a practical example is provided to illustrate the developed approach.		Jin-Han Park;Yu Kyoung Park;Mi-Jung Son	2018	Information	10.3390/info9090226	artificial intelligence;fuzzy logic;discrete mathematics;machine learning;group decision-making;computer science;operator (computer programming);probabilistic logic;einstein;scalar multiplication	NLP	-2.268748593008603	-21.228375479133543	77797
6a74ce35a37db6cb054c121b52e9b8390ff46260	weighted fuzzy soft multiset and decision-making		Fuzzy soft multiset (FSMS) is a substantial and important fuzzy generalization of soft set and multiset (also called bag). In this paper, we have presented the idea of weighted fuzzy soft multiset (WFSMS) as a generalization of FSMS and its basic properties are to be studied. Also, we introduce a new adjustable approach to WFSMS based decision-making, for solving decision-making in an uncertain situation. The feasibility of our proposed WFSMS based decision-making procedure in practical application is read by a mathematical example.	approximation;on intelligence;rough set;soft computing;vagueness	Ajoy Kanti Das	2018	Int. J. Machine Learning & Cybernetics	10.1007/s13042-016-0607-y	combinatorics;discrete mathematics;mathematics;fuzzy set operations;algorithm	AI	-1.625180158365409	-23.660671629647503	77801
391761f8e8267ef58283095c5dbf9ad9f91d64f4	fuzzy robustness analysis		"""This paper proposes a confluence between soft OR and soft computing methods, by means of an application of fuzzy logic ideas to robustness analysis. Both methods try to add flexibility to proposed solutions of real world problems, although insofar as their application contexts are concerned, they have been quite apart. So it is only natural to bring them together in what could be called a """"fuzzification"""" of robustness analysis."""	confluence;fuzzy concept;fuzzy logic;fuzzy set;soft computing	Luiz Fernando Loureiro Legey;Heloisa Firmo Kazay	2001			robustness (computer science);fuzzy logic;robustness testing;machine learning;fuzzy set;soft computing;artificial intelligence;confluence;computer science	AI	-2.2979709145233804	-23.895898102235442	77876
136fe940f6060feb7913b45d08231cc7b59b5a86	a fuzzy membership filtering aided neural network based transmission loss allocation scheme using game theory	game theory;shapley value;bilateral transaction;fuzzy membership;artificial neural network;transmission loss allocation	The present paper proposes development of a transmission loss allocation scheme in a deregulated environment using fuzzy memberships and supervised neural networks. This method can be effectively utilized in online applications where game theory based solutions, which otherwise produce acceptable results, cannot be utilized for prohibitive computation load. We propose a fuzzy membership based approach to filter data from a global database and create a local relevant database, for each transaction detail online, each time. A neural network is trained for each such local database formed and utilized for estimating loss allocations among players, for the transaction detail under consideration. The proposed method has been employed for an IEEE 14 bus system and the results of our proposed method have been shown to be sufficiently accurate, when compared to results obtained by using game theoretic approach. 2011 Elsevier Ltd. All rights reserved.	artificial neural network;computation;game theory;global serializability	Nalin B. Dev Choudhury;Amitava Chatterjee;S. K. Goswami	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.09.002	game theory;computer science;artificial intelligence;machine learning;data mining;shapley value;artificial neural network	AI	3.3414595334473853	-19.665629127143294	77890
5d53029adcf040ba6408f0050a2531b20f1921d8	energy consumption, structural breaks and economic growth: evidence from china	energy conservation;energy;demand side model;granger causality;production side model;structural breaks;biological system modeling;granger causality relationship;time series economic indicators energy consumption;time series;multivariate time series;multivariate time series model;yttrium;energy price;energy consumption;real gdp;mathematical model;production;energy consumption economic indicators biological system modeling production mathematical model yttrium;causality energy gdp structural breaks;structural break;bi directional causalities energy consumption structural break economic growth china granger causality relationship multivariate time series model demand side model real gdp energy price production side model;china;gdp;var model;bi directional causalities;economic growth;economic indicators;causality	This paper attempts to shed light into the Granger causality relationship between energy consumption and economic growth allowing structural breaks for China, based on two multivariate time series models: a demand side model of energy consumption, real GDP and energy price and a production side model of GDP, energy, capital, and labor. To test for Granger causality in the presence of cointegration between the variables, we employ a VECM rather than a VAR model estimation. Empirical results from the two models for China over the period 1952-2007 show that there is a bi-directional causalities between energy consumption and GDP, both in the short-run and long-run. This indicates unanimously that energy acts as an engine of economic growth, and that energy conservation policies may harm economic growth in China. It also implies that energy consumption keeps on growing as long as the economy grows in China.			2010		10.1109/ICEE.2010.196	granger causality;energy;causality;economics;energy conservation;structural break;yttrium;time series;macroeconomics;mathematical model;economy;china;statistics	NLP	3.27589009896059	-14.201029464986798	78001
4361515094d5e91a12a4b13cef4e934ee7d4fd0f	fuzzy logic controlled model car	fuzzy logic	On this paper, an application of a high performance fuzzy logic hardware controller is explained, a model car is given autonomous character. Development of a fuzzy rule base, aided by a simulator program, is followed by trials made to come to conclusions regarding ideal and real functioning of the hardware developed.	autonomous robot;fuzzy logic;fuzzy rule;rule-based system;simulation	Gerardo Aranguren Aramendía;Luis A. L. Nozal;Miguel Rodríguez Gómez;Enrique García	1999			fuzzy set operations;machine learning;fuzzy electronics;fuzzy rule;fuzzy associative matrix;defuzzification;combs method;computer science;fuzzy number;fuzzy classification;artificial intelligence	AI	7.332501013260612	-13.21103422219311	78020
becd93a7854a43bce96e3e5b52adaaff66572bbb	evaluating performance of flow line systems with blocking under fuzzy environments	performance measure;fuzzy theory;capacity planning;lower and upper bound;queuing model;fuzzy number;indexing method;single channel;membership function;extension principle;mixed integer nonlinear programming	Flow line systems with blocking are often seen in the fuzzy real world. This paper proposes a mixed integer nonlinear programming (MINLP) approach for evaluating the performances of the flow line system with blocking in fuzzy environments. The main idea is to model this system as a single-channel, multiple-phase queuing model with finite capacity, wherein the arrival rate and service rates are fuzzy numbers. This model is then transformed to a family of conventional crisp queues by applying the α-cut approach in fuzzy theory. On the basis of α-cut representation and the extension principle, two pairs of mixed integer nonlinear programs are formulated to calculate the lower and upper bounds of the fuzzy performance measures at possibility level α, from which the membership functions of the performance measures are derived. This paper also provides a representative value of the performance measure via the Yager ranking index method. An example is investigated successfully to illustrate the validity and the informative benefits of using the proposed approach. Since the performance measures are expressed by membership functions rather than by crisp values, the fuzziness of input information is conserved completely and more information is provided for capacity planning in flow line systems. Managerial implications of the analyses are also examined.	blocking (computing);defuzzification;embedded system;mathematical optimization;membership function (mathematics);nl (complexity);nonlinear system;queueing theory;simulation	Shih-Pin Chen	2009	Cybernetics and Systems	10.1080/01969720802506741	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations;statistics	EDA	-2.7195688380493195	-19.529194227416646	78402
35db0cf8dfb3df3118c6ce352243102684fa937f	runway incursion event forecast model based on ls-svr with multi-kernel	multi kernel;forecast model;least square support vector machine;airport runways	Forecasting of runway incursion event is very significant to guide the job of civil aviation safety management. It is an important part of the runway incursion early warning management. However, prediction of runway incursion event is a complicated problem due to its non-linearity and the small quantity of training data. As a novel type of learning machine, support vector machine (SVM) has been gaining popularity due to their promising performance, such as dealing with the data of small sample, the high dimension and the excellent generalization ability. However, the generalization ability of SVM often relies on whether the selected kernel function is suitable for real data. To lessen the sensitivity of different kernels and improve generalization ability, least square support vector regression (LS-SVR) with multi-kernel is proposed to forecast the runway incursion event in this paper. The two experimental results indicate that LS-SVR with multi-kernel model is better than LS-SVR with individual kernel model and generalized regression neural network (GRNN) model. Consequently, multi-kernel LS-SVR model is a proper alternative for forecasting of the runway incursion event.	kernel (operating system);least squares;runway bus	Guimei Xu;Shengguo Huang	2011	JCP	10.4304/jcp.6.7.1346-1352	simulation;operations research	Vision	8.07740818183327	-18.6526209797283	78504
d4855ee475c6a9609d942f2c99685804562c1372	nowcasting: accurate and precise short-term wind power prediction using hyperlocal wind forecasts		To increase wind power integration, it is essential for electric utilities to accurately predict how much power wind turbines will generate. While purely autoregressive (and nonlinear autoregressive) approaches to prediction using historical data perform well for immediate future (10 to 30 minutes ahead) horizons, their accuracy dramatically deteriorates for farther time horizons. Predicting generation up to 5-6 hours ahead is essential for scheduling multi-tier generation systems that have varying dynamic response. We propose a method that augments autoregressive approaches with exogenous inputs from hyperlocal wind speed forecasts to improve the prediction accuracy and precision beyond 30 min ahead. Our approach reduces the mean absolute error to 2.11%--14.25% for predictions made 10 min to 6 hours ahead. Importantly, it also reduces the uncertainty associated with the predictions by over 15% in comparison with approaches presented in related work.	approximation error;arx;artificial neural network;autoregressive model;cross-validation (statistics);curve fitting;feedforward neural network;ground truth;image scaling;linear model;maxima and minima;microwave;multitier architecture;nonlinear autoregressive exogenous model;nonlinear system;quantum number;scheduling (computing);test set;weather research and forecasting model;xfig	Varun Badrinath Krishna;Wander S. Wadman;Younghun Kim	2018		10.1145/3208903.3208919	wind power;accuracy and precision;artificial neural network;nowcasting;wind speed;autoregressive model;hyperlocal;power law;control theory;computer science	HCI	9.495084680647418	-17.85723148254906	78567
98591578cf3517ad2a35d3a2105101c67cb6751b	statistical analysis of nomao customer votes for spots of france		We investigate the statistical properties of votes of customers for spots of France collected by the startup company Nomao. The frequencies of votes per spot and per customer are characterized by a power law distribution which remains stable on a time scale of a decade when the number of votes is varied by almost two orders of magnitude. Using the computer science methods we explore the spectrum and the eigenvalues of a matrix containing user ratings to geolocalized items. Eigenvalues nicely map to large towns and regions but show certain level of instability as we modify the interpretation of the underlying matrix. We evaluate imputation strategies that provide improved prediction performance by reaching geographically smooth eigenvectors. We point on possible links between distribution of votes and the phenomenon of self-organized criticality.	complex systems;computer science;geo-imputation;geolocation;instability;moore's law;picture archiving and communication system;self-organized criticality;towns;world wide web	Róbert Pálovics;Bálint Daróczy;András A. Benczúr;Júlia Pap;Leonardo Ermann;Samuel Phan;Alexei Chepelianskii;Dima L. Shepelyansky	2015	CoRR		order of magnitude;imputation (statistics);statistics;pareto distribution;condensed matter physics;self-organized criticality;criticality;matrix (mathematics);instability;physics;phenomenon	ML	3.9251152182671416	-13.184377745651473	78582
0355e520b4e99f4240af45e5d85a30d2afc0ca66	simulating cointegrated time series	asset prices;johnson system;varta algorithm;long term dependence;dependence measures;pricing;resource management;biological system modeling;portfolio allocation models cointegrated time series dependence measures long term dependence cointegrated sample paths vector auto regressive to anything varta algorithm financial asset price marginal distribution johnson system;cointegrated sample paths;portfolio allocation;portfolios;time series;investment;portfolio allocation models;financial asset price;statistical distributions;portfolios asset management finance stochastic processes security operations research industrial engineering financial management testing frequency estimation;autoregressive processes;marginal distribution;time series analysis;yttrium;vector auto regressive to anything;correlation;time series autoregressive processes investment pricing statistical distributions;security;cointegrated time series;vector auto regressive	When one models dependence solely via correlations, portfolio allocation models can perform poorly. This motivates considering dependence measures other than correlation. Cointegration is one such measure that captures long-term dependence. In this paper we present a new method to simulate cointegrated sample paths using the vector auto-regressive-to-anything (VARTA) algorithm. Our approach relies on new properties of cointegrated time series of financial asset prices and allows for marginal distributions from the Johnson system. The method is illustrated on two data sets, one real and one artificial.	algorithm;marginal model;simulation;time series	Alexander Galenko;David P. Morton;Elmira Popova;Ivilina Popova	2009	Proceedings of the 2009 Winter Simulation Conference (WSC)	10.1109/WSC.2009.5429356	econometrics;actuarial science;resource management;time series;mathematics;statistics	Robotics	3.985369447071876	-11.221780824185343	78609
ca6356c6841e0eec5f475a8f25b0fdd8cdaa81e4	new robust forecasting models for exchange rates prediction	robust prediction models;wilcoxon functional link artificial neural network wflann;exchange rate prediction;wilcoxon norm;wilcoxon artificial neural network wann	This paper introduces two robust forecasting models for efficient prediction of different exchange rates for future months ahead. These models employ Wilcoxon artificial neural network (WANN) and Wilcoxon functional link artificial neural network (WFLANN). The learning algorithms required to train the weights of these models are derived by minimizing a robust norm called Wilcoxon norm. These models offer robust exchange rate predictions in the sense that the training of weight parameters of these models are not influenced by outliers present in the training samples. The Wilcoxon norm considers the rank or position of an error value rather than its amplitude. Simulation based experiments have been conducted using real life data and the results indicate that both models, unlike conventional models, demonstrate consistently superior prediction performance under different densities of outliers present in the training samples. Further, comparison of performance between the two proposed models reveals that both provide almost identical performance but the later involved low computational complexity and hence is preferable over the WANN model.		Babita Majhi;Minakhi Rout;Ritanjali Majhi;Ganapati Panda;Peter J. Fleming	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.05.017	econometrics;machine learning;mathematics;statistics	ML	8.572837482069563	-21.352112063173404	78815
8e65b558179b8d9505561ffa3869d5b2f3fd6685	a parameterized l2 metric between fuzzy numbers and its parameter interpretation	bootstrap techniques;fuzzy number;imprecise data;mathematics and statistics;linear regression model;valued random variables;wabl ldev rdev representation of fuzzy numbers;l 2 metric;info eu repo semantics article;approximation;interval;expected value;time trajectories;l 2;l 2 wabl ldev rdev metric;sets;weighting parameter;strong laws	When handling fuzzy number data, it is a common practice to make use of a metric to quantify distances between fuzzy numbers. Several metrics have been suggested in the literature for this purpose. When statistically analyzing fuzzy number-valued data, L metrics become especially useful. This paper introduces a new family of generalized L metrics which take into account key features of the involved fuzzy numbers, namely, a measure of central location and two measures associated with the shape of the fuzzy numbers are used. A crucial property related to these three measures is that necessary and sufficient conditions can be established for them to characterize fuzzy numbers. Furthermore, the family of generalized L metrics depends on one parameter. A discussion is provided regarding the interpretation of this parameter which can guide selection of its value in practice.	fuzzy number	Beatriz Sinova;María Angeles Gil;María Teresa López;Stefan Van Aelst	2014	Fuzzy Sets and Systems	10.1016/j.fss.2014.01.006	interval;combinatorics;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;linear regression;artificial intelligence;fuzzy number;machine learning;approximation;fuzzy measure theory;mathematics;fuzzy set operations;expected value;statistics	Logic	0.5003838908227899	-20.676275513315215	79409
24345dd4e3b2ca6365b2fa2e6c9c9a73ee7d7d4e	entropy, similarity measure and distance measure of vague soft sets and their relations	distance measure;vague soft set;期刊论文;entropy;soft set;similarity measure	Abstract   Soft set theory offers a general mathematical tool for dealing with uncertain, fuzzy, not clearly defined objects. A vague soft set is a combination of a vague set and a soft set. In this paper, we introduce axiomatic definitions of entropy, similarity measure and distance measure for vague soft sets, and some formulas have also been put forward to calculate them. Furthermore, 13 theorems are proposed showing how the entropy, the similarity measure and the distance measure for vague soft sets can be deduced from each other. Some formulas have also been put forward to calculate the entropy, the similarity measure and the distance measure of vague soft sets by these relations.	similarity measure;vagueness	Chang Wang;Anjing Qu	2013	Inf. Sci.	10.1016/j.ins.2013.05.013	information theory and measure theory;entropy;mathematical analysis;discrete mathematics;topology;mathematics;discrete measure	NLP	-2.315778951929232	-22.654562609661028	79433
2d7b6c1288f1057d48bf419156c4fd695575af8f	alternative form of dempster's rule for binary variables	plausibility functions;belief functions;securite donnee;risque securite information;regle emballage;teoria dempster shafer;dempster s rule;dempster shafer theory;packaging rules;binary variables;donnee binaire;combination of evidence;dato binario;audicion;binary data;regle dempster;article;audit;security of data;theorie dempster shafer	This article develops an alternative form of Dempster’s rule of combination for binary variables. This alternative form does not only provide a closed form formulae for efficient computation but also enables researchers to develop closed form analytical formulae for assessing risks such as information security risk, fraud risk, audit risk, independence risk, etc., involved in assurance services. We demonstrate the usefulness of the alternative form in calculating the overall information security risk and also in developing an analytical model for assessing fraud risk.	computation;information security	Rajendra P. Srivastava	2005	Int. J. Intell. Syst.	10.1002/int.20083	dempster–shafer theory;artificial intelligence;pattern recognition;data mining;mathematics;audit;statistics	ML	-0.5444181707988272	-13.910928362115696	79472
894c3528a173b5edff007720c26b6a189e48c7c9	a new method for forecasting the taiex based on high-order fuzzy logical relationships	forecasting;fuzzy time series;fuzzy set;fuzzy forecasting;time series;data mining;fuzzy set theory;fuzzy sets;stock markets;fuzzy logic;training data;indexes;fuzzy logic fuzzy sets fuzzy set theory technology forecasting fuzzy systems computer science stock markets cybernetics usa councils chaos;high order fuzzy logical relationships;time series analysis;adjacent fuzzy sets;high order fuzzy logical relationships fuzzy sets fuzzy time series fuzzy forecasting high order fuzzy time series;indexation;taiex;taiwan stock exchange capitalization weighted stock index;high order fuzzy time series;taiwan stock exchange;adjacent fuzzy sets taiex high order fuzzy logical relationships taiwan stock exchange capitalization weighted stock index;forecast accuracy;historical data;stock markets fuzzy set theory	In this paper, we present a new method to forecast the Taiwan Stock Exchange Capitalization Weighted Stock Index (TAIEX) based on high-order fuzzy logical relationships. First, the proposed method fuzzifies the historical data into fuzzy sets to form high-order fuzzy logical relationships. Then, it calculates the value of the variable between the subscripts of adjacent fuzzy sets appearing in the antecedents of high-order fuzzy logical relationships. Then, it lets the high-order fuzzy logical relationships having the same antecedent to form a high-order fuzzy logical relationship group. Finally, it chooses a high-order fuzzy logical relationship group to forecast the TAIEX. The proposed method gets a higher average forecasting accuracy rate than the existing methods to forecast the TAIEX.	entity–relationship model;fuzzy set;horner's method	Shyi-Ming Chen;Chao-Dian Chen	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346231	defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations;statistics	DB	4.220943535344931	-22.158337143472654	79931
62fd9401579ce8ab2a5a84e791e4e54360ff7e0b	prediction and identification of urban traffic flow based on features	feature of traffic flow;hybrid elman neural network;traveler information broadcast;traffic information systems fuzzy neural nets traffic control;fuzzy neural nets;elman neural network;urban traffic flow;fuzzy identify;and identification;real time traffic control;real time traffic;traffic control;traffic flow;fuzzy techniques;short term urban expressway feature of traffic flow elman neural network fuzzy identify;traffic information systems;urban expressway;nonlinear problem;traffic control vehicle driving telecommunication traffic communication system traffic control neural networks fuzzy neural networks intelligent transportation systems vehicle dynamics broadcast technology broadcasting;urban expressway prediction and identification urban traffic flow traveler information broadcast real time traffic control hybrid elman neural network fuzzy techniques;prediction;short term	Identifying and predicting the situation of traffic flow play an important role in traveler information broadcast and real-time traffic control. In this paper, to pick up the effective characteristic parameters of traffic, the features and the transition between different situations in traffic are studied and analyzed, A hybrid Elman neural network and fuzzy techniques are good at working out the nonlinear problem and identifying the state of system, so they can apply to predict and distinguish the traffic situation in short term. As a result, it proves that there are some advantages, e.g. simple configuration, good prediction and exact identification. So it is fit to online predict and identify the traffic flow in urban expressway	artificial neural network;fuzzy control system;fuzzy logic;nonlinear system;real-time clock;recurrent neural network;time series	Xiao-Xiong Weng;Yu-An Tan;Gao-Li Du;Qin-Ming Hong	2006	2006 9th International Conference on Control, Automation, Robotics and Vision	10.1109/ICARCV.2006.345268	traffic generation model;simulation;prediction;engineering;traffic flow;mathematics;short-term memory;transport engineering;computer security;statistics	Robotics	9.21019134530967	-12.988345139243233	79951
d50632f7b6a6fba1761eba8eab0230ea0ffbf133	gaussian-chain filters for heavy-tailed noise with application to detecting big buyers and big sellers in stock market		"""—In this paper we propose a new heavy-tailed distribution-Gaussian-Chain (GC) distribution, which is inspirited by the hierarchical structures prevailing in social organizations. We determine the mean, variance and kurtosis of the Gaussian-Chain distribution to show its heavy-tailed property, and compute the tail distribution table to give specific numbers showing how heavy is the heavy-tails. To filter out the heavy-tailed noise, we construct two filters-2 nd and 3 rd-order GC filters-based on the maximum likelihood principle. Simulation results show that the GC filters perform much better than the benchmark least-squares algorithm when the noise is heavy-tail distributed. Using the GC filters, we propose a trading strategy, named Ride-the-Mood, to follow the """" mood """" of the market by detecting the actions of the big buyers and the big sellers in the market based on the noisy, heavy-tailed price data. Application of the Ride-the-Mood strategy to five blue-chip Hong Kong stocks over the recent two-year period from April 2, 2012 to March 31, 2014 shows that their returns are higher than the returns of the benchmark Buy-and-Hold strategy and the Hang Seng Index Fund."""	algorithm;benchmark (computing);least squares;sensor;simulation;tails	Li-Xin Wang	2014	CoRR		financial economics;economics;marketing;commerce	ML	4.231527053898001	-14.130338454702846	80038
d2132f3bb784411732a6474526ea1a9e95b484ba	generalized point operators for aggregating intuitionistic fuzzy information	generic point	We first develop a series of intuitionistic fuzzy point operators, and then based on the idea of generalized aggregation (Yager RR. Generalized OWA aggregation operators. Fuzzy Optim Decis Making 2004;3:93–107 and Zhao H, Xu ZS, Ni MF, Liu SS. Generalized aggregation operators for intuitionistic fuzzy sets. Int J Intell Syst 2010;25:1–30), we develop various generalized intuitionistic fuzzy point aggregation operators, such as the generalized intuitionistic fuzzy point weighted averaging (GIFPWA) operators, generalized intuitionistic fuzzy point ordered weighted averaging (GIFPOWA) operators, and generalized intuitionistic fuzzy point hybrid averaging (GIFPHA) operators, which can control the certainty degrees of the aggregated arguments with some parameters. Furthermore, we study the properties and special cases of our operators. © 2010 Wiley Periodicals, Inc.		Meimei Xia;Zeshui Xu	2010	Int. J. Intell. Syst.	10.1002/int.20439	ordered weighted averaging aggregation operator;mathematical analysis;discrete mathematics;computer science;generic point;mathematics	AI	-1.1739943009831797	-21.4771196721567	80093
4fdbcaf65cb6618b47901edf3ff04ba0d5056c85	general types of $$({\in,}\,{\in}\,{\vee}\,{\rm q})$$ -fuzzy filters in bl-algebras	fuzzy filter;hbox q fuzzy filter;vee;in;fuzzifying filter;t implication based fuzzy filter;hbox q _k fuzzy filter	As a generalization of an $$({\in,}\,{\in}\,{\vee}\, \hbox{q})$$ -fuzzy filter in a BL-algebra, the notion of an $$({\in,}\,{\in}\,{\vee}\,\hbox{q}_k)$$ -fuzzy filter in a BL-algebra is introduced, and related properties are investigated. Characterizations of an $$({\in,}\,{\in\,\vee}\,\hbox{q}_k)$$ -fuzzy filter are considered. The implication-based fuzzy filters of a BL-algebra are discussed.	bl (logic)	Young Bae Jun;Yong Uk Cho;Eun Hwan Roh;Jianming Zhan	2010	Neural Computing and Applications	10.1007/s00521-010-0379-3	control theory	AI	0.2516931148683317	-23.382554543579985	80121
8dce20c2d2403a95ec0cc7fc265ca107dc911c82	a novel approach for precipitation forecast via improved k-nearest neighbor algorithm	precipitation;knn algorithm;improved knn algorithm;precipitation forecast	The prediction method plays crucial roles in accurate precipitation forecasts. Recently, machine learning has been widely used for forecasting precipitation, and the  K -nearest neighbor (KNN) algorithm, one of machine learning techniques, showed good performance. In this paper, we propose an improved KNN algorithm, which offers robustness against different choices of the neighborhood size  k , particularly in the case of the irregular class distribution of the precipitation dataset. Then, based our improved KNN algorithm, a new precipitation forecast approach is put forward. Extensive experimental results demonstrate that the effectiveness of our proposed precipitation forecast approach based on improved KNN algorithm.	k-nearest neighbors algorithm	Mingming Huang;Runsheng Lin;Shuai Huang;Tengfei Xing	2017	Advanced Engineering Informatics	10.1016/j.aei.2017.05.003	precipitation;computer science;engineering;machine learning;pattern recognition;data mining;quantitative precipitation forecast;k-nearest neighbors algorithm	SE	9.35152453049363	-20.535238825898574	80449
93311e73013bd319bed8f1e4bf8e81888ed3619b	using one axiom to characterize l-fuzzy rough approximation operators based on residuated lattices		Axiomatic characterization of approximation operators plays an important role in the study of rough set theory. Different axiom sets of abstract operators can illustrate different classes of rough set systems. In this paper, we are devoted to searching for a single axiom to characterize L-fuzzy rough approximation operators based on residuated lattices. Axioms of L-fuzzy set theoretic operators make sure of the existence of certain types of L-fuzzy relations which produce the same operators. We demonstrate that the lower (upper) L-fuzzy rough approximation operators generated by a generalized L-fuzzy relation can be characterized by only one axiom. Furthermore, we also use one axiom to characterize L-fuzzy rough approximation operators produced by the L-fuzzy serial, reflexive, symmetric and T -transitive relations as well as any of their compositions. Keywords—Rough sets; residuated lattices; L-fuzzy rough sets; approximation operators; axioms	approximation;fuzzy set;residuated lattice;rough set;set theory	Yan-Ling Bao;Hai-Long Yang;Yanhong She	2018	Fuzzy Sets and Systems	10.1016/j.fss.2017.07.016	combinatorics;operator theory;axiom of choice;discrete mathematics;mathematics;urelement;constructive set theory;fuzzy set;zermelo–fraenkel set theory;axiom schema;axiom of extensionality	AI	-0.7986907908289945	-23.033096474320782	80492
74096c65e802b8a8dd272f7295e45992ddd3a88f	linear programming method for multiattribute group decision making using if sets	intuitionistic fuzzy set;fuzzy set;uncertainty;decision maker;journal;decision making process;multiattribute group decision making;indexation;linear programming;linear program;preference;group decision making	The purpose of this paper is to develop a linear programming methodology for solving multiattribute group decision making problems using intuitionistic fuzzy (IF) sets. In this methodology, IF sets are constructed to capture fuzziness in decision information and decision making process. The group consistency and inconsistency indices are defined on the basis of pairwise comparison preference relations on alternatives given by the decision makers. An IF positive ideal solution (IFPIS) and weights which are unknown a priori are estimated using a new auxiliary linear programming model, which minimizes the group inconsistency index under some constraints. The distances of alternatives from the IFPIS are calculated to determine their ranking order. Moreover, some properties of the auxiliary linear programming model and other generalizations or specializations are discussed in detail. Validity and applicability of the proposed methodology are illustrated with the extended air-fighter selection problem and the doctoral student selection problem. 2010 Elsevier Inc. All rights reserved.	linear programming;programming model;selection algorithm;software development process	Deng-Feng Li;Guo-Hong Chen;Zhi-Gang Huang	2010	Inf. Sci.	10.1016/j.ins.2010.01.017	decision-making;mathematical optimization;optimal decision;decision analysis;computer science;linear programming;artificial intelligence;linear partial information;data mining;mathematics;weighted sum model	AI	-2.7931787238365793	-19.04945628474147	80727
720b50b417a65782a5a16a9d33214d4c7f37cffc	macroeconomic determinants of the behavior of dhaka stock exchange (dse)	emerging market;dhaka stock exchange;stock returns;financial markets;bangladesh;globalization;macroeconomic variables;vector autoregression model	Many past studies documented a strong evidence of a linkage between stock prices and macroeconomic activities across different stock markets and time horizons. However, most of these studies have focused on developed economies and highlighted the impact of either domestic variables or a few global factors. In recent times, the impact of global macroeconomic factors upon stock returns has garnered a lot of interest due to globalization. The aim of this paper is therefore to examine the combined influence of global and domestic macroeconomic factors upon stock returns and extend this relationship to an emerging market of Bangladesh. Using Vector Autoregression (VAR) model, findings indicate a considerable impact of money supply for the stock returns of Dhaka Stock Exchange (DSE). Additionally, an insignificant influence of the world price index is observed, which implies a complete segmentation of DSE from the global financial markets. Finally, the study highlights regulatory changes and policy-making decisions from the perspective of Bangladesh. KEywoRDS Bangladesh, Dhaka Stock Exchange, Emerging Market, Financial Markets, Globalization, Macroeconomic Variables, Stock Returns, Vector Autoregression Model	autoregressive model;linkage (software);vector autoregression	Nabila Nisha	2016	IJABIM	10.4018/IJABIM.2016010101	stock exchange;stock market bubble;economics;globalization;macroeconomics;economy;market economy;economic growth;economic policy	AI	3.3965589188059337	-14.421350738316995	80805
95112b0aa6a3112534bec6176ca9047e33cb3fd6	the (n)ever-changing world: stability and change in organizational routines	sociomateriality;workflow system;networks of action;stability and change;organizational routines	This paper uses data on invoice processing in four organizations to distinguish empirically between two competing theories of organizational routines. One theory predicts that routines should generate patterns of action that are few in number and stable over time, and that atypical patterns of action are driven primarily by exceptional inputs. The competing theory predicts the opposite. By modeling the routines as networks of action and using a first-order Markov model to test for stationarity, we find support for the competing theory. The routines generated hundreds of unique patterns that changed significantly during a five-month period without any apparent external intervention. Changes did not appear to reflect improved performance or learning. Furthermore, we found that exogenous factors (such as large invoices from unusual vendors) are not associated with atypical patterns of action, but endogenous factors (such as the experience of the participants) are. We also found that increased automation can increase variation under some circumstances. These findings offer empirical support for endogenous change in organizational routines and underscore the importance of the sociomaterial context in understanding stability and change.	first-order predicate;generative systems;markov chain;markov model;stationary process;theory;vendor lock-in	Brian T. Pentland;Thorvald Haerem;Derek Hillison	2011	Organization Science	10.1287/orsc.1110.0624	computer science;knowledge management;communication;social psychology	HCI	-0.06227076031721044	-10.099729992089957	80934
eb38fe8b7ce9173aa1d56916a1037fc3d276dec3	option price sensitivities through fuzzy numbers	fuzzy numbers;fuzzy number;parameterization;option pricing;probabilistic approach;black and scholes model;interest rate;stock price;sensitivity analysis	The main motivation in using fuzzy numbers in finance lies in the need for modelling the uncertainty and vagueness that are implicit in many situations. However, the fuzzy approach should not be considered as a substitute for the probabilistic approach but rather as a complementary way to describe the model peculiarities. Here, we consider, in particular, the Black and Scholes model for option pricing, and we show that the fuzzification of some key parameters enables a sensitivity analysis of the option price with respect to the risk-free interest rate, the final value of the underlying stock price, the volatility, and also better forecasts (see Thavaneswaran et al. (2009) [12] for details). The sensitivities with respect to the variables of the model are represented by different letters of the Greek alphabet and they play an important role in the definition of the shape of the fuzzy option price.		Maria Letizia Guerra;Laerte Sorini;Luciano Stefanini	2011	Computers & Mathematics with Applications	10.1016/j.camwa.2010.11.024	actuarial science;fuzzy number;mathematics;mathematical economics	Theory	0.8172226184209554	-12.454574955582721	81176
b06326fc971d3dab7c8e5e2a591962a2aea59290	a grid scheduling optimization strategy based on fuzzy multi-attribute group decision-making	grid scheduling;selection model;prediction method;fuzzy set;decision making process;high performance computer;group decision making;high performance	In grid environments, the grid scheduling technique is more complex than the conventional ones in high performance computing  system, and grid scheduling is one of the major factors that would affect the grid performance. In order to optimize grid  scheduling, we have to consider the various factors. By combining the analysis and prediction methods that are of different  principles and approaches, we would be able to make comprehensive decisions on different scenarios and provide reference for  scheduling optimization. In this paper, a method of fuzzy multi-attribute group decision-making is proposed, which introduces  fuzzy set and its operations into decision-making process, and reflects a group or collective ranking of alternatives based  on the individual preferences of those alternatives. The flexible selection models heighten the expressive force and adaptability  greatly. The experiments show that the grid scheduling with this method has high performance.    It should be pointed out that the decision-making approach in this paper is built on the compensability between the decision  attributes. But in some cases, the compensability between the decision attributes is conditional, and even non-compensable.  Therefore, the other comprehensive decision-making approaches are needed for these features. These approaches will be our  further research focus.      	scheduling (computing)	Jin Huang;Hai Jin;Xia Xie;Jun Zhao	2006		10.1007/3-540-33880-2_10	fair-share scheduling;computer science;machine learning;data mining;management science;fuzzy set operations	EDA	-4.226790619391556	-18.479442745698037	81240
0148ec83d892e8af1382159e0f08a32262593f97	ensemble neurocomputing based oil price prediction		In this paper, we investigated an ensemble neural network for the prediction of oil prices. Daily data from 1999 to 2012 were used to predict the West Taxes, Intermediate. Data were separated into four phases of training and testing using different percentages and obtained seven sub-datasets after implementing different attribute selection algorithms. We used three types of neural networks: Feed forward, Recurrent and Radial Basis Function networks. Finally a good ensemble neural network model is formulated by the weighted average method. Empirical results illustrated that the ensemble neural network outperformed other models.	algorithm;artificial neural network;experiment;network model;neurocomputing;radial (radio);radial basis function;recurrent neural network	Lubna A. Gabralla;Hela Mahersia;Ajith Abraham	2014		10.1007/978-3-319-13572-4_24	computer science;artificial intelligence;machine learning;data mining;ensemble learning	ML	8.979066507036181	-19.87758076839962	81275
919d12974f9cd9a200c10a2f4be59ac270a7f2e5	fuzzy topsis method with ordered fuzzy numbers for flow control in a manufacturing system	ordered fuzzy numbers ofns;fuzzy topsis method ftopsis;flexible manufacturing system fms;discrete simulation;multi criteria decision making method mcdm	The aim of the paper is to present a practical solution to a particular real-life problem of discrete flow control in a manufacturing system. For this purpose a fuzzy Multi-Criteria Decision Making (MCDM) method, which is an extension of the Fuzzy Technique for Order Preference by Similarity to Ideal Solution (FTOPSIS) approach using Ordered Fuzzy Numbers (OFNs), is used. The paper includes a case study of the application of this method as a flow controller for the transport trolley in a flexible manufacturing system. The immediate aim of control is to choose the transport trolley’s destination (production line). A set of criteria (parameters) for the evaluation of the flow’s destination is suggested, related to time efficiency and equal workload of machines. The advantage of the proposed method is its ability to distinguish the type of criteria (benefit, cost) by using the orientation of ordered fuzzy numbers. An example of calculations using the presented method is shown and the results of the method, based on the simulation of a manufacturing system, are presented. The results thus obtained are compatible with flexible automation. In order to test the FTOPSIS method with OFNs the proposed flow control method is compared with the TOPSIS method and other simple control methods (dispatching rules). The proposed method is more efficient than the methods used for comparison and can be effective and efficient for diversified production and fast, automatic changeovers on production lines.	flow control (data);fuzzy number;real life;simulation	Katarzyna Rudnik;Dariusz Kacprzak	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.09.027	mathematical optimization;computer science;fuzzy number;discrete event simulation;fuzzy set operations	Robotics	-2.506187294409021	-17.578971058411277	81367
4cd05cd801ff4f527e1e89f1a934b8ea36c435ed	optimization in an intuitionistic fuzzy environment	intuitionistic fuzzy set;intuitionistic fuzzy sets;mathematical programming fuzzy optimization;satisfiability;optimization problem;qa75 electronic computers computer science;mathematical programming;fuzzy optimization	A new concept of the optimization problem under uncertainty is proposed and treated in the paper. It is an extension of fuzzy optimization in which the degrees of rejection of objective(s) and of constraints are considered together with the degrees of satisfaction. This approach is an application of the intuitionistic fuzzy (IF) set concept to optimization problems. An approach to solving such problems is proposed and illustrated with a simple numerical example. It converts the introduced intuitionistic fuzzy optimization (IFO) problem into the crisp (non-fuzzy) one. The advantage of the IFO problems is twofold: they give the richest apparatus for formulation of optimization problems and, on the other hand, the solution of IFO problems can satisfy the objective(s) with bigger degree than the analogous fuzzy optimization problem and the crisp one. (c) Elsevier		Plamen P. Angelov	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00009-7	probabilistic-based design optimization;optimization problem;mathematical optimization;discrete mathematics;defuzzification;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;mathematics;continuous optimization;vector optimization;fuzzy set operations;algorithm;satisfiability	Logic	-0.8192541535422544	-18.321911387653635	81798
5ee6a675318cf0bf8229c72e8ddfc0f2707c1b4f	ids operation management method using holt-winters method		IDS(Intrusion Detection System) is widely used. However, IDS has a problem that it is difficult to manage its operation because of occurring a large number of alerts. As a general usage of IDS, it is known that a certain threshold value is set and only the alerts exceeding it are reported. Generally, the threshold values are determined based on the subjectivity of administrator. Therefore, we have proposed a novel IDS operation management method which does not rely on the subjectivity of the administrator by using predicted number of IDS alerts by Holt-Winters method. However, there has been a problem that the prediction accuracy deteriorated when large detection of an unexpected alert existed at the learning time of Holt-Winters method. In this paper, we propose a new method to calculate forecast values sequentially, excluding large detection of alerts and evaluate its prediction accuracy. Lastly we discuss the operation management method of IDS.	basic;intrusion detection system;system administrator	Satoshi Kimura;Hiroyuki Inaba	2017	2017 IEEE 6th Global Conference on Consumer Electronics (GCCE)	10.1109/GCCE.2017.8229201	exponential smoothing;time series;operations management;intrusion detection system;computer science	SE	5.8083970032480785	-21.9346737748932	81872
7857d99c462fed4d78bfe7b2c69ebeb1ab9f78f6	quantitative composite decision-theoretic rough set		In practical decision-making, we prefer to characterize the uncertain problems with the hybrid data, which consists of various types of data, e.g., categorical, numerical, set-valued and interval-valued. The extended rough sets can deal with single types of data based on specific binary relation, including the equivalence relation, neighborhood relation, partial order relation, tolerance relation, etc. However, the fusion of these relations is a significant challenge task in such composite information table. To tackle this issue, this paper proposes the intersection and union composite relation, and further introduces a quantitative composite decision-theoretic rough set model. Moreover, we present a novel matrix-based approach to compute the upper and lower approximations in proposed model. Finally, an numerical example is conducted to illustrate the efficiency of proposed method.	approximation;attribute-value system;numerical analysis;polynomial matrix;problem solving;rough set;theory;turing completeness	Linna Wang;Ling Liu;Xin Yang;Pan Zhuo	2017	2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)	10.1109/ISKE.2017.8258759	data type;discrete mathematics;equivalence relation;probabilistic logic;data modeling;categorical variable;binary relation;matrix (mathematics);mathematics;rough set	Robotics	-2.0917852163353263	-22.991854960167107	82026
bd638d9b896d0d32d304408465de455ef82b143b	linkage effects mining in stock market based on multi-resolution time series network		Previous research on financial time-series data mainly focused on the analysis of market evolution and trends, ignoring its characteristics in different resolutions and stages. This paper discusses the evolution characteristics of the financial market in different resolutions, and presents a method of complex network analysis based on wavelet transform. The analysis method has proven the linkage effects of the plate sector in China’s stock market and has that found plate drift phenomenon occurred before and after the stock market crash. In addition, we also find two different evolutionary trends, namely the W-type and M-type trends. The discovery of linkage plate and drift phenomena are important and referential for enterprise investors to build portfolio investment strategy, and play an important role for policy makers in analyzing evolution characteristics of the stock market.		Lingyu Xu;Huan Xu;Jie Yu;Lei Wang	2018	Information	10.3390/info9110276	wavelet;data mining;wavelet transform;stock market;complex network;financial market;computer science;portfolio investment;stock market crash;phenomenon;finance	ML	4.178080963531862	-14.882751993509665	82191
0f9248fa8f3fac3cf74a3e8ba7c4e6ab98254fb3	a neural network model to predict long-run operating performance of new ventures	neural networks;sample design;long run performance;asymmetric information;decision problem;initial public offering;logit models;neural network model;classification accuracy;logit model;neural network;operating performance;initial public offerings	The prediction of long-run operating performance of new ventures, known as Initial Public Offerings (IPOs), represents a challenging decision problem. Factors adding to the complexity of the problem include asymmetrically informed agents, incentive problems, and inability to specify functional relationships between variables. Research literature identifying determinants of long-run performance of new issues is limited. This study uses a data driven, nonparametric, neural network based approach to predict the long-run operating performance of new ventures. The classification accuracy of the neural network model is compared with that of a logit model. Methodological issues such as sample design and estimation of optimal cutoff probabilities for classification are addressed. The results suggest that the neural networks generally outperform logit models. Copyright Kluwer Academic Publishers 1998	artificial neural network;network model	Bharat A. Jain;Barin N. Nag	1998	Annals OR	10.1023/A:1018910402737	econometrics;initial public offering;mixed logit;economics;computer science;marketing;data mining;artificial neural network;statistics	NLP	2.913736750969705	-11.157553288279837	82551
09e7286016ab49077f617c6e8439317bde2b16f3	on classifying inputs and outputs in dea: a revised model	modelizacion;evaluation performance;analisis envolvimiento datos;entrada salida;performance evaluation;efficiency;evaluacion prestacion;estimation non parametrique;variable status;operations research;flexible inputs;input output;modelisation;non parametric estimation;computational error;data envelopment analysis;flexible outputs;estimacion no parametrica;data envelope analysis;modeling;data envelopment analysis variable status flexible inputs flexible outputs efficiency computational error;entree sortie;analyse enveloppement donnee	Cook and Zhu [Cook, W.D., Zhu, J., 2007. Classifying inputs and outputs in data envelopment analysis. European Journal of Operational Research 180, 692-699] introduced a new method to determine whether a measure is an input or an output. In practice, however, their method may produce incorrect efficiency scores due to a computational problem as result of introducing a large positive number to the model. This note introduces a revised model that does not need such a large positive number.		Mehdi Toloo	2009	European Journal of Operational Research	10.1016/j.ejor.2008.08.017	econometrics;computer science;operations management;data envelopment analysis;operations research	Theory	0.09971013269541863	-15.378412284045147	82609
05e65c49e5609f1c953c1ab9c3676752d7a1b5b3	fuzzy optimization of units products in mix-product selection problem using fuzzy linear programming approach	fuzzy constraint;uncertainty;data collection;decision maker;vagueness;qa75 electronic computers computer science;membership function;industrial application;production planning;fuzzy linear programming;degree of satisfaction and decision maker;fuzzy constraints;fuzzy optimization;fuzzy system	In this paper, the modified S-curve membership function methodology is used in a real life industrial problem of mix product selection. This problem occurs in the production planning management where by a decision maker plays important role in making decision in an uncertain environment. As analysts, we try to find a good enough solution for the decision maker to make a final decision. An industrial application of fuzzy linear programming (FLP) through the S-curve membership function has been investigated using a set of real life data collected from a Chocolate Manufacturing Company. The problem of fuzzy product mix selection has been defined. The objective of this paper is to find an optimal units of products with higher level of satisfaction with vagueness as a key factor. Since there are several decisions that were to be taken, a table for optimal units of products respect to vagueness and degree of satisfaction has been defined to identify the solution with higher level of units of products and with a higher degree of satisfaction. The fuzzy outcome shows that higher units of products need not lead to higher degree of satisfaction. The findings of this work indicates that the optimal decision is depend on vagueness factor in the fuzzy system of mix product selection problem. Further more the high level of units of products obtained when the vagueness is low.	fuzzy control system;high-level programming language;linear programming;mathematical optimization;principle of good enough;real life;selection algorithm;vagueness;windows fundamentals for legacy pcs	Pandian Vasant;Nader N. Barsoum	2006	Soft Comput.	10.1007/s00500-004-0437-9	decision-making;mathematical optimization;uncertainty;membership function;defuzzification;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;linear partial information;mathematics;fuzzy set operations;fuzzy control system;statistics;data collection	Web+IR	-4.069483950060586	-16.948458730313337	82993
6fec8ec3c6ed6e2a57efde7d54ba32fad073bd8f	adaptive regression and classification models with applications in insurance	zinātniskās publikācijas;rīgas tehniskā universitāte;classification;izdevums rtu zinātniskie raksti;regression;rtu;prediction;insurance	Abstract Nowadays, in the insurance industry the use of predictive modeling by means of regression and classification techniques is becoming increasingly important and popular. The success of an insurance company largely depends on the ability to perform such tasks as credibility estimation, determination of insurance premiums, estimation of probability of claim, detecting insurance fraud, managing insurance risk. This paper discusses regression and classification modeling for such types of prediction problems using the method of Adaptive Basis Function Construction		Gints Jekabsons;Marina Uhanova	2014	Appl. Comput. Syst.	10.2478/acss-2014-0004	actuarial science;marketing;data mining;business	Metrics	5.39978009607365	-17.6020312962655	83002
6df385fb61c18895d4f1b653c31dd408678cf5fe	notes on “reducing algorithm complexity for computing an aggregate uncertainty measure”	aggregate uncertainty au;uncertainty measures;aggregate uncertainty measure;uncertainty measures aggregate uncertainty au computational complexity dempster shafer d s theory;dempster shafer theory of evidence;target identification;uncertainty handling;dempster shafer theory aggregate uncertainty measure computational complexity reduction meyerowitz richman walker algorithm;measurement uncertainty;gold;computational complexity;dempster shafer d s theory;aggregates;dempster shafer theory;meyerowitz richman walker algorithm;dempster shafer;humans;entropy;aggregates measurement uncertainty gold computational complexity information theory entropy power measurement humans;computational complexity reduction;information theory;power measurement;uncertainty handling computational complexity	In a recent paper, Liu have proposed the so-called F -algorithm which conditionally reduces the computational complexity of the Meyerowitz-Richman-Walker algorithm for the computation of the aggregate-uncertainty measure in the Dempster-Shafer theory of evidence, along with an illustration of its application in a practical scenario of target identification. In this correspondence, we will point out several technical mistakes, which some of them lead to some inexact or incomplete statements in the paper of Liu The corrections of these mistakes will be made, and some further improvement and results will be derived.	aggregate data;aggregate function;algorithm;amiga walker;computation;computational complexity theory	Van-Nam Huynh;Yoshiteru Nakamori	2010	IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans	10.1109/TSMCA.2009.2030962	dempster–shafer theory;information theory;artificial intelligence;machine learning;mathematics;statistics	Robotics	1.5170321016453903	-18.70151266436093	83295
0cc87658447e6a385b4178afe271ac140a98bebd	an experiment on the consistency of aggregated comparison matrices in ahp	agregacion;analytic hierarchy process;media geometrica;processus hierarchie analytique;systeme aide decision;social decision;dimension groupe;simulation;sistema ayuda decision;prise decision;aggregation;group decisions;decision support system;arithmetic mean;moyenne geometrique;proceso jerarquia analitico;agregation;group size;group decision making;geometric mean;decision colectiva;moyenne arithmetique;decision collective;toma decision;monte carlo simulation;group decision;dimension grupo;media aritmetica	The analytic hierarchy process can be used for group decision making by aggregating individual judgments or individual priorities. The most commonly used aggregation methods are the geometric mean method and the weighted arithmetic mean method. While it is known that the weighted geometric mean comparison matrix is of acceptable consistency if all individual comparison matrices are of acceptable consistency, this paper addresses the following question: Under what conditions would an aggregated geometric mean comparison matrix be of acceptable consistency if some (or all) of the individual comparison matrices are not of acceptable consistency? Using Monte Carlo simulation, results indicate that given a sufficiently large group size, consistency of the aggregate comparison matrix is guaranteed, regardless of the consistency measures of the individual comparison matrices, if the geometric mean is used to aggregate. This result implies that consistency at the aggregate level is a non-issue in group decision making when group size exceeds a threshold value and the geometric mean is used to aggregate individual judgments. This paper determines threshold values for various dimensions of the aggregated comparison matrix.		Rhonda Aull-Hyde;Sevgi Erdogan;Joshua M. Duke	2006	European Journal of Operational Research	10.1016/j.ejor.2004.06.037	combinatorics;geometric mean;analytic hierarchy process;group decision-making;decision support system;arithmetic mean;artificial intelligence;size of groups, organizations, and communities;mathematics;statistics;monte carlo method	Vision	-1.2184864819863983	-16.26526099264985	83493
c83b0f87b34a17c61493a2ef198808dba87c1974	time series forecasting using empirical mode decomposition and hybrid method		Recently, various applications produce large amount of time series data. In these domains, accurately forecasting time series has been getting important for decision makers. autoregressive integrated moving average (ARIMA) as a linear method and Artificial Neural Networks (ANNs) as a nonlinear method have been widely used to forecast time series. However, many theoretical and empirical studies showed that assembling of those two approaches in hybrid methods can be efficient to improve forecasting performance by alleviating volatility problem in time series. Rather than two components, the developed method decomposes data into relatively stationary multiple components by using Empirical Mode Decomposition (EMD). Then each of them are separately modelled by the hybrid method proposed by Zhang which shows great success in time series forecasting as compared to well known methods. The evaluation of the developed method is performed on three different publicly available benchmark dataset and achieved highly successful results as compared to existing well-accepted hybrid methods.	artificial neural network;autoregressive integrated moving average;autoregressive model;benchmark (computing);hilbert–huang transform;neural networks;nonlinear system;stationary process;time series;volatility	Ümit Cavus Büyüksahin;Seyda Ertekin	2018	2018 26th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2018.8404560	pattern recognition;artificial neural network;computer science;hilbert–huang transform;time series;data modeling;empirical research;nonlinear system;machine learning;autoregressive integrated moving average;volatility (finance);artificial intelligence	AI	7.67378562475071	-20.025739863503997	83712
c98936aa90f6de3fc5cb949f70c3c0830e43dc7e	semigroup structure of singleton dempster–shafer evidence accumulation	semigroup homomorphism;procesamiento informacion;evidence accumulation;uncertainty;uncertainty fusion power generation pattern recognition statistical learning biosensors sensor fusion medical diagnosis biometrics biomedical engineering neural networks;bayes methods;inference mechanisms;data fusion;set theory;carta de datos;homomorphism;dempster shafer mass;teoria dempster shafer;mappage;fusion donnee;dempster shafer theory;inverse mapping semigroup structure singleton dempster shafer evidence accumulation mass assignments bayes set of probabilities;information processing;homomorphisme;bayes inference;dempster shafer;mapping;semigroup;homomorfismo;traitement information;fusion datos;uncertainty bayes inference data fusion dempster shafer mass dempster shafer theory evidence accumulation semigroup semigroup homomorphism;theorie dempster shafer;set theory bayes methods inference mechanisms	Dempster-Shafer theory is one of the main tools for reasoning about data obtained from multiple sources, subject to uncertain information. In this work abstract algebraic properties of the Dempster-Shafer set of mass assignments are investigated and compared with the properties of the Bayes set of probabilities. The Bayes set is a special case of the Dempster-Shafer set, where all non-singleton masses are fixed at zero. The language of semigroups is used, as appropriate subsets of the Dempster-Shafer set, including the Bayes set and the singleton Dempster-Shafer set, under either a mild restriction or a slight extension, are semigroups with respect to the Dempster-Shafer evidence combination operation. These two semigroups are shown to be related by a semigroup homomorphism, with elements of the Bayes set acting as images of disjoint subsets of the Dempster-Shafer set. Subsequently, an inverse mapping from the Bayes set onto the set of these subsets is identified and a procedure for computing certain elements of these subsets, acting as subset generators, is obtained. The algebraic relationship between the Dempster-Shafer and Bayes evidence accumulation schemes revealed in the investigation elucidates the role of uncertainty in the Dempster-Shafer theory and enables direct comparison of results of the two analyses.	linear algebra;tree accumulation	Andrzej K. Brodzik;Robert H. Enders	2009	IEEE Transactions on Information Theory	10.1109/TIT.2009.2030447	complement;discrete mathematics;result set;solution set;dempster–shafer theory;information processing;pattern recognition;power set;mathematics;index set;k-approximation of k-hitting set;special classes of semigroups;empty set;singleton;set function;algorithm;infinite set;statistics	Logic	1.813181466746651	-21.549979413988464	83988
4ce006bb87c23a7f7d9c1624ff59aa3208a409fa	financial early warning of non-life insurance company based on rbf neural network optimized by genetic algorithm		SummaryrnDue to the characteristic of risk diversification in non–life insurance industry, the companyu0027s financial risk early warning is very important. In order to reasonably predict the financial status of non–life insurance company, the evaluation system of financial risk indicator is constructed from the aspects of solvency, profitability, and growth ability. Taking data of non–life insurance companies in past years as sample, the evaluation indicators are weighted objectively using the entropy method. The RBF neural network model is improved with the genetic algorithm, and the early warning model is established. The empirical results show that the prediction accurate rate of RBF neural network model based on genetic algorithm is increased.		Chun Yan;Lin Wang;Wei Liu;Man Qi	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4343	computer science;data mining;artificial neural network;genetic algorithm;financial risk;profitability index;solvency;finance;diversification (marketing strategy);warning system	SE	5.6119042792164	-18.084600465746096	84045
591e895007472818eb11515ec11aa8438c2a6773	analysis and validation of 24 hours ahead neural network forecasting of photovoltaic output power	energy forecasting;photovoltaic system;artificial neural network	7 In this paper an artificial neural network for photovoltaic plant energy fore8 casting is proposed and analyzed in term of its sensitivity with respect to the 9 input data sets. 10 Furthermore, the accuracy of the method has been studied as a function 11 of the training data sets and error definitions. The analysis is based on exper12 imental activities carried out on a real photovoltaic power plant accompanied 13 by clear sky model. 14 In particular, this paper deals with the hourly energy prediction for all 15 the daylight hours of the following day, based on 48 hours ahead weather 16 forecast. This is very important due to the predictive features requested 17 by smart grid application: renewable energy sources planning, in particular 18 storage system sizing, and market of energy. 19	artificial neural network;computer data storage;daylight;ray casting	Sonia Leva;Alberto Dolara;Francesco Grimaccia;Marco Mussetta;Emanuele Ogliari	2017	Mathematics and Computers in Simulation	10.1016/j.matcom.2015.05.010	artificial intelligence;photovoltaic system;machine learning;artificial neural network	Robotics	9.61724095212504	-17.820939715177317	84262
5c648b8bced609e69ee24a24c1a29b8320e78163	fuzzy risk analysis based on a geometric ranking method for generalized trapezoidal fuzzy numbers	ranking fuzzy numbers;generalized trapezoidal fuzzy numbers;fuzzy risk analysis	In this study, we present a new method for ranking generalized trapezoidal fuzzy numbers based on the incenter and inradius of a triangle. The proposed method is simple and easy to apply to real life problems. The method can also rank crisp numbers and fuzzy numbers with the same centroid point. Some comparative examples are also given to illustrate the advantages of the proposed method. Based on the proposed ranking method, we also give an application to the fuzzy risk analysis problem.	fuzzy number;real life	Emrah Akyar;Handan Akyar;Serkan Ali Düzce	2013	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-2012-0628	fuzzy logic;mathematical optimization;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	Robotics	-2.14905267362985	-20.956058622276423	84535
dcbc288262053c277c20d3a58ad5e735a9d96ae6	a new gep algorithm and its applications in vegetable price forecasting modeling problems		In this paper, a new Gene Expression Programming (GEP) algorithm is proposed, which increase “inverted series” and “extract” operator. The new algorithm can effectively increase the rate of utilization of genes, with convergence speed and solution precision is higher. Taking the Chinese vegetables price change trend of mooli, scallion as example, and discuss the way to solve the forecasting modeling problem by adopting GEP. The experimental results show that the new GEP Algorithm can not only increase the diversity of population but overcome the shortage of primitive GEP. In addition, it can improve convergence accuracy compared to original GEP.	algorithm;gene expression programming	Lei Yang;Kangshun Li;Wensheng Zhang;Yaolang Kong	2015		10.1007/978-981-10-0356-1_14	financial economics;economics;marketing;advertising	Theory	5.830666108116054	-17.678137427874127	84661
ccc610cc9a59d726adb4d5d4961ed04a10f1b4a3	the multiplicative consistency index of hesitant fuzzy preference relation	euclidean distance;indexes hafnium decision making density functional theory fuzzy sets hamming distance euclidean distance;fuzzy sets;density functional theory;indexes;hamming distance;hafnium	Hesitant fuzzy preference relation (HFPR) shows to be a unique and suitable technique to integrate all the values of decision makers when comparing pairwise alternatives (or criteria), while the consistency index of a HFPR determines the accuracy and reliability. In order to improve the accuracy in checking the multiplicative consistency of a HFPR, this paper aims to provide a new method to determine the value of the consistency index for a HFPR. First of all, we point out the weaknesses of the existing method in checking the multiplicative consistency of a HFPR. As there is no any theoretical evidence to support the given consistency threshold, to determine the value of such consistency index, we investigate the density function of the consistency index of a HFPR. After making some theoretical discussions on three methods, an algorithm is then proposed to determine the value of the multiplicative consistency index of a HFPR. Based on some simulations, a value table of critical values of the multiplicative consistency index of a HFPR is determined, whose elements vary with respect to the order of the HFPR and the measure used on distance calculations. Finally, some illustrative examples are given to show the applicability and efficiency of the determined critical value table of multiplicative consistency index in the process of consistency checking over a HFPR.	algorithm;cladogram;simulation	Haifeng Liu;Zeshui Xu;Huchang Liao	2016	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2015.2426315	database index;econometrics;discrete mathematics;hamming distance;computer science;artificial intelligence;euclidean distance;mathematics;fuzzy set;hafnium;density functional theory;statistics	DB	-3.5019688140497918	-20.1197843965346	84885
88d88fdfba089b3d0a818e3d7af16cdd7ae92dcd	investments unwrapped: demystifying and automating technical analysis and hedge-fund strategies		"""In this thesis we use nonlinear and linear estimation techniques to model two common investment strategies: hedge funds and technical analysis. Our models provide transparent and low-cost alternatives to these two nontransparent, and in some cases prohibitively costly, financial approaches. In the case of hedge funds, we estimate linear factor models to create passive replicating portfolios of common exchange-traded instruments, that provide similar risk exposures as hedge funds, but at lower cost and with greater transparency. While the performance of linear clones is generally inferior to their hedge-fund counterparts, in some cases the clones perform well enough to warrant serious consideration as low-cost passive alternatives to hedge funds. In the case of technical analysis also known as """"charting"""" we develop an algorithm based on neural networks that formalizes and automates the highly subjective technical practice of detecting, with the naked eye, certain geometric patterns that appear on price charts and that are believed to have predictive value. We then evaluate the predictive ability of these technical patterns by applying our algorithm to stocks and exchange rates data for a number of stocks and currencies over many time periods, and comparing the unconditional distribution of returns to the return distribution conditional on the occurrence of technical patterns. We find that several technical patterns do provide incremental information, suggesting that technical analysis may add value to the investment process. To further demystify the highly controversial practice of technical analysis, we complement our implementation and validation study with a historical overview of the field and interviews with its leading practitioners. Thesis Supervisor: Andrew W. Lo Title: Harris and Harris Group Professor, Sloan School of Management"""		Jasmina Hasanhodzic	2007				ML	1.601839977899423	-14.274888409535553	85061
254c062786101805fa0591418d9d608bd3979913	power system parameters forecasting using hilbert-huang transform and machine learning	forecasting;прогнозирование;neural networks;ann;временной ряд;singular integral;article peer reviewed;time series;сингулярный интеграл;мов;integral transforms;boosting;machine learning;интегральное преобразование;artificial intelligence;svm;wind forecast;feature analysis;машинное обучение;бустинг;time series prediction;инс;анализ признаков	A novel hybrid data-driven approach is developed for forecasting power system parameters with the goal of increasing the efficiency of short-term forecasting studies for non-stationary time-series. The proposed approach is based on mode decomposition and a feature analysis of initial retrospective data using the Hilbert-Huang transform and machine learning algorithms. The random forests and gradient boosting trees learning techniques were examined. The decision tree techniques were used to rank the importance of variables employed in the forecasting models. The Mean Decrease Gini index is employed as an impurity function. The resulting hybrid forecasting models employ the radial basis function neural network and support vector regression. Apart from introduction and references the paper is organized as follows. The second section presents the background and the review of several approaches for short-term forecasting of power system parameters. In the third section a hybrid machine learningbased algorithm using Hilbert-Huang transform is developed for short-term forecasting of power system parameters. Fourth section describes the decision tree learning algorithms used for the issue of variables importance. Finally in section six the experimental results in the following electric power problems are presented: active power flow forecasting, electricity price forecasting and for the wind speed and direction forecasting.	algorithm;artificial neural network;data pre-processing;decision tree learning;electricity price forecasting;gradient boosting;hilbert space;hilbert–huang transform;iterative method;long short-term memory;machine learning;mathematical optimization;preprocessor;radial (radio);radial basis function;radio frequency;random forest;stationary process;support vector machine;time series;white noise	Victor G. Kurbatsky;Nikita V. Tomin;Vadim A. Spiryaev;Paul Leahy;Denis N. Sidorov;Alexei V. Zhukov	2014	CoRR		computer science;artificial intelligence;machine learning;time series;mathematics;operations research;artificial neural network	ML	8.6815455248907	-19.587015863527967	85091
df68fce00affdbf2a087dd5ed25604f789cf0ec8	fuzzy theory based multi-objectives evaluation models and their applications on water resources allocation	resource utilization;water resource;optimisation;decision factors;fuzzy theory;multiobjectives fuzzy optimization model;multiobjectives evaluation model;resource allocation;water resources;resource management;biological system modeling;fuzzy variable;multiobjectives evaluation index system;satisfiability;fuzzy set theory;indexes;fuzzy variable assessment;decision making process;fuzzy variable assessment fuzzy theory multiobjectives evaluation model water resources allocation decision factors resource utilization uncertain decision making multiobjectives evaluation index system multiobjectives fuzzy optimization model grey correlation analysis;assessment methods;indexation;grey systems;cities and towns;water resources decision making fuzzy set theory grey systems optimisation resource allocation;water resources resource management decision making cities and towns fuzzy systems optimization methods blindness biological system modeling helium mathematics;grey correlation analysis;correlation;fuzzy optimization;water resources allocation;evaluation model;correlation analysis;uncertain decision making	Water resources allocation relates to many decision factors, such as society, economy, environment and resource utilization, which is a complicated and uncertain decision-making process. Usually, the problem of conflict and competition among the different targets and water users should be solved. Taking Dalian’s water resources allocation as an example, this paper develops a multi-objectives evaluation index system. On the basis of it, water resource allocation schemes are evaluated and inter- compared with multi-objectives fuzzy optimization model, grey correlation analysis method and fuzzy variable assessment method to avoid the subjectivity and blindness of the decision-making. And a relative satisfied scheme is obtained, which will improve the decision-making for water resources allocation in Dalian.	fuzzy logic;mathematical optimization	He Bin;Wang Guoli;Huang Xu	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.671	database index;decision-making;water resources;in situ resource utilization;resource allocation;computer science;resource management;management science;fuzzy set;correlation;satisfiability	DB	-2.648175061537893	-14.977276643865618	85178
677a89cd9d9b4d8a317ca27e76da3ad9e171dd02	using electronic toll collection data to understand traffic demand	etc data;traffic simulation;travel demand;intelligent transportation systems;od estimation;origin and destination;automated toll collection;traffic demand;traffic volume	In this study, we explored the potential of using electronic toll collection (ETC)-derived data that are a part of intelligent transport systems (ITS). Dynamic origin–destination (OD) traffic volumes were estimated using ETC data on the Hanshin Expressway. A dynamic OD estimation model that was suggested in a previous study was used, and abundant ETC data were input to improve the estimation accuracy. The results of OD estimation were analyzed to understand traffic demand and its variation. External factors were clarified that have an influence on variances in the OD flows, and statistical analysis methods for the variations were proposed depending on the factors. Moreover, the improvements in traffic simulation accuracy and performance as a result of using ETC data as input variables in the simulation models were discussed. According to the results of this study, ETC data have potential to assist in understaningd traffic demand and its variation, and the results can be applied to network management.	electronic toll collection	Jinyoung Kim;Fumitaka Kurauchi;Nobuhiro Uno;Takeshi Hagihara;Takehiko Daito	2014	J. Intellig. Transport. Systems	10.1080/15472450.2013.806858	intelligent transportation system;simulation;floating car data;computer science;transport engineering;traffic volume;computer network	Metrics	8.62322751153407	-13.502512758297653	85430
9463c03fe4fc2cc75ebdd68522af2a87f813f313	fuzzy optimal solution of fully fuzzy linear programming problems using ranking function	ranking function;triangular fuzzy numbers;fully fuzzy linear programming problems	In this paper, a new method is proposed to find the fuzzy optimal solution of fully fuzzy linear programming (FFLP) problems with mixed constraints. By using the proposed method the fuzzy optimal solution of FFLP problems with mixed constraints occurring in real life situations can be easily obtained. To illustrate the proposed method a numerical example is solved. The proposed method can be easily applied to find the fuzzy optimal solution of FFLP problems occurring in real life situations. 6	linear programming;numerical analysis;ranking (information retrieval);real life	Amit Kumar;Jagdeep Kaur	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-120742	mathematical optimization;discrete mathematics;defuzzification;fuzzy transportation;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set operations	AI	-0.7290850261787165	-18.244876206207568	85751
5db71b6cee77d134a793820ce2cbf751df2e3b57	selecting the most preferable alternatives in a group decision making problem using dea	optimal solution;decision maker;objective function;preferential voting system;data envelopment analysis;linear program;group decision making;data envelope analysis;most preferable alternative;qa76 computer software	0957-4174/$ see front matter 2008 Elsevier Ltd. A doi:10.1016/j.eswa.2008.07.011 * Corresponding author. E-mail address: a.emrouznejad@aston.ac.uk (A. Em Group decision making is the study of identifying and selecting alternatives based on the values and preferences of the decision maker. Making a decision implies that there are several alternative choices to be considered. This paper uses the concept of Data Envelopment Analysis to introduce a new mathematical method for selecting the best alternative in a group decision making environment. The introduced model is a multi-objective function which is converted into a multi-objective linear programming model from which the optimal solution is obtained. A numerical example shows how the new model can be applied to rank the alternatives or to choose a subset of the most promising alternatives. 2008 Elsevier Ltd. All rights reserved.	constraint (mathematics);data envelopment analysis;linear programming;loss function;mathematical model;numerical analysis;optimization problem;programming model	L. MajidZerafatAngiz;Ali Emrouznejad;Adli Mustafa;A. Rashidi Komijan	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.07.011	optimal decision;influence diagram;decision analysis;weighted product model;computer science;linear programming;data mining;decision rule;rank reversals in decision-making;data envelopment analysis;weighted sum model;business decision mapping	AI	-2.535204401920582	-18.55475397175623	86089
c1976ec9256836ef3800d1ecf835a8e8f5e4345c	a new method for power system contingency ranking using combination of neural network and data envelopment analysis		In this paper, an integrated algorithm has been proposed for ranking contingencies in the deregulated network. The network security and economical indices should be considered when dealing with market environment. Locational marginal price and congestion cost indices are the best signals to completely illustrate the market operation. In this paper, voltage violation, line flow violation, locational marginal price and congestion cost indices have been simultaneously considered to rank the contingencies. This algorithm uses neural networks method to estimate the power system parameters (locational marginal price, bus voltage magnitudes and angles). The efficiency of each of contingencies was calculated using data envelopment analysis and this index was employed for ranking. The efficiency of each contingency shows its severity and indicates that it affects network security and economic indices concurrently. Considering the proposed formulation for data envelopment analysis, the efficiency of a contingency will be higher if the calculated indices for that contingency are higher. More efficiency leads to increased severity of the contingency and shows that the contingency has concurrently more affected network security and economic indices. The proposed algorithm has been tested on IEEE 30-bus test power system. Simulation results show the high efficiency of the algorithm. 15	algorithm;artificial neural network;contingency plan;data envelopment analysis;marginal model;network congestion;network security;simulation;test case	Mohsen Simab;Seyavash Chatrsimab;Sepide Yazdi;Ali Simab	2017	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-162169	machine learning;pattern recognition;data mining	Mobile	2.5099618524472462	-15.982649849554615	86115
17cafdf9bf513deb43e3f19892a7e4799b361650	a hybrid approach using maximum entropy and bayesian learning for detecting delinquency in financial industry	data mining;bayesian learning;credit card;financial;maximum entropy;fraud detection	The use of credit card has increased tremendously in the past few years because of the boom in the economy which has also resulted in the increase in the credit card fraud cases. Various leading banks and software development companies worldwide are taking serious measures to deal with the gravity of this situation. This paper proposes a framework for credit card fraud detection that will detect frauds using maximum entropy according to the irregular behavior of the customers in various transactions of credit card. The comparative study of above approach with existing approaches is also addressed. Results show the feasibility and validity of each approach.		Dharminder Kumar;Suman Arora	2016	IJKBO	10.4018/IJKBO.2016010105	actuarial science;data mining;business;commerce	ML	3.781854807699785	-17.650893577605636	86529
7b6ed37480f7e44bb3a86ead63d5e53eb725ca60	fuzzy analytic hierarchy process: a logarithmic fuzzy preference programming methodology	non linear effect;navio;multicriteria analysis;multiobjective programming;programmation multiobjectif;analytic hierarchy process;non linear programming;point estimation;processus hierarchie analytique;nonlinear programming;systeme aide decision;programacion no lineal;multiple criteria decision making;decision borrosa;logique floue;priorite;efecto no lineal;logica difusa;prise de decision;programmation non lineaire;decision floue;sistema ayuda decision;fuzzy logic;logarithmic fuzzy preference programming;funcion logaritmica;decision support system;hierarchical classification;logarithmic function;analyse floue;fonction logarithmique;proceso jerarquia analitico;classification hierarchique;fuzzy preference programming;analisis multicriterio;ship;fuzzy analytic hierarchy process;analyse multicritere;effet non lineaire;fuzzy analysis;toma decision;priority;prioridad;clasificacion jerarquizada;ship registry selection;fuzzy decision;programacion multiobjetivo;navire	Fuzzy analytic hierarchy process (AHP) proves to be a very useful methodology for multiple criteria decision-making in fuzzy environments, which has found substantial applications in recent years. The vast majority of the applications use a crisp point estimate method such as the extent analysis or the fuzzy preference programming (FPP) based nonlinear method for fuzzy AHP priority derivation. The extent analysis has been revealed to be invalid and the weights derived by this method do not represent the relative importance of decision criteria or alternatives. The FPP-based nonlinear priority method also turns out to be subject to significant drawbacks, one of which is that it may produce multiple, even conflict priority vectors for a fuzzy pairwise comparison matrix, leading to entirely different conclusions. To address these drawbacks and provide a valid yet practical priority method for fuzzy AHP, this paper proposes a logarithmic fuzzy preference programming (LFPP) based methodology for fuzzy AHP priority derivation, which formulates the priorities of a fuzzy pairwise comparison matrix as a logarithmic nonlinear programming and derives crisp priorities from fuzzy pairwise comparison matrices. Numerical examples are tested to show the advantages of the proposed methodology and its potential applications in fuzzy AHP decision-making.	analytical hierarchy;software development process	Ying-Ming Wang;Kwai-Sang Chin	2011	Int. J. Approx. Reasoning	10.1016/j.ijar.2010.12.004	fuzzy logic;logarithm;analytic hierarchy process;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;nonlinear programming;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;point estimation;fuzzy measure theory;mathematics;fuzzy associative matrix;fuzzy set operations;algorithm	AI	-0.5271195551549922	-15.866369921742033	86587
aa5dde35aba8fde5e168c4e79a2fa292e47ef169	influence of the fuzzy implication operator on the method-of-cases inference rule	inference rule	This paper is a natural continuation of Mizumoto's work on the influence of the choice of a fuzzy implication operator on the validity of the fuzzy modus ponens, the fuzzy modus tollens, and the fuzzy syllogism. We present a detailed investigation of the validity of a fuzzified version of the well-known method-ofcases under 17 dO~ferent forms of the fuzzy implication operator. The inference is based on a generalized version of Zadeh's compositional rule of inference, using cylindrical extension and projection of fuzzy relations. It is pointed out that only nine of the forms of the operator satisfy the fuzzy method-of-cases.	continuation;whole earth 'lectronic link	Da Ruan;Etienne E. Kerre;Gert de Cooman;Bart Cappelle;F. Vanmassenhove	1990	Int. J. Approx. Reasoning	10.1016/0888-613X(90)90004-L	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system;rule of inference	AI	-0.47409748976450133	-23.233808794864697	86649
b40c926138b2bbf3793029337d10fb7a04201386	restricting virtual weights in data envelopment analysis	assurance regions;weights restrictions;input output;data envelopment analysis;virtuals;data envelope analysis	The consequences of the use of absolute weights restrictions (i.e. restricting the multipliers) on the efficiency score and targets of a DEA model have been explored elsewhere, the same is not true for the use of restrictions on the virtuals (i.e. the product of the input/output factor by its multiplier). In this paper, a reflection on the uses of virtual weights restrictions is presented. The reasons for using virtual weights restrictions instead of absolute weights restrictions, in particular cases, are explained. Following a critique of Wong and Beasley's [J. Oper. Res. Soc. 41 (1990) 829] first proposed method for constraining the virtuals in DEA, a new classification scheme for virtual weights restrictions is presented, which brings the concept of assurance regions into virtual weights restrictions. It is shown that the use of simple virtual restrictions and virtual assurance regions are preferable to the use of the more generally advocated WB's proportional virtual weights restrictions. In recognition of levels of decision making at the unit, and external to the unit, the use of the terms unit of assessment (UOA) and controller is proposed. It is concluded that the use of virtual assurance regions applying to the target UOA can be a natural representation of preference structures and translate established patterns between the input–output divide. Also, the meaning of the efficiency score and targets in this approach most approximate traditional DEA. Alternatives to using virtual weights restrictions are considered, namely using absolute weights restrictions with a virtual meaning. Finally, an empirical example is offered.	data envelopment analysis	Cláudia S. Sarrico;R. G. Dyson	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00402-8	computer science;operations management;data envelopment analysis;mathematics;operations research;welfare economics	HPC	-3.2878297696287175	-14.419893393696816	86734
94395f42c5011875f6876f02b2d011a35c202abc	notes on covering-based rough sets from topological point of view: relationships with general framework of dual approximation operators			approximation;rough set	Lynn D'eer;Chris Cornelis	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2017.06.006		Theory	-0.4566028551324089	-23.71687957730058	86808
b054d0a6376aa24e245632f1eb62b19a028d6104	financial crisis, omori's law, and negative entropy flow	profound economic difficulty;pareto-like distribution;american international group;pareto distribution;economic crisis;positive pareto income;financial crisis;omori-law-like distribution;negative entropy flow;economic downturn;negative income;fannie mae	The 2008 global financial crisis has revived great interest in early warning system (EWS) models for reducing the risks of future crises. Existing EWS models employ aggregated variables that cannot examine the nonlinear dynamics of participating players on scales smaller than a country in unstable, non-equilibrium economies. To help understand the mechanism of financial crises and identify new robust indicators for financial crises and economic recessions, in this work, we take an “anatomical” approach, i.e., to examine the income structures of different sectors of an economy separately, as well as to analyze the exposure networks associated with Fannie Mae/Freddie Mac, Lehman Brothers, and American International Group. We show that the losses in exposure networks can be modeled by a two-parameter Omori-law-like distribution for earthquake aftershocks. Such a distribution suggests that losses will be widespread around crises or recessions. Indeed, around crises or recessions, the heavy-tailed distributions for the negative income cluster are even heavier than those for the positive income cluster. Consequently, the entropies associated with the distribution of the negative income cluster exceed that of the positive income cluster. Moreover, instability propagates from the crisis initiating sector to other sectors. Therefore, the anatomical approach developed here can indeed shed some light on the detailed dynamics of financial crises and economic recessions, and the distribution and entropy approaches can help predict economic downturns.	negentropy	Jianbo Gao;Jing Hu	2013		10.1007/978-3-642-37210-0_49	actuarial science	Vision	-4.374610808839343	-10.3390272751322	86906
850dd86dbdf81abfe2007de6cf322b78f7066c27	an extension of universal generating function in multi-state systems considering epistemic uncertainties	probability;reliability universal generating function multistate system mss ugf universal generating function method availability evaluation model ill known probability aleatory uncertainty epistemic uncertainty interval arithmetic operation;uncertainty probability distribution reliability theory upper bound fuzzy sets differential equations;probability power system reliability;power system reliability;random sets belief functions theory epistemic uncertainties multi state systems	Many practical methods and different approaches have been proposed to assess Multi-State Systems (MSS) reliability measures. The universal generating function (UGF) method, introduced in 1986, is known to be a very efficient way of evaluating the availability of different types of MSSs. In this paper, we propose an extension of the UGF method considering epistemic uncertainties. This extended method allows one to model ill-known probabilities and transition rates, or to model both aleatory and epistemic uncertainty in a single model. It is based on the use of belief functions which are general models of uncertainty. We also compare this extension with UGF methods based on interval arithmetic operations performed on probabilistic bounds.	interval arithmetic	Sébastien Destercke;Mohamed Sallak	2013	IEEE Transactions on Reliability	10.1109/TR.2013.2259206	reliability engineering;discrete mathematics;probability;mathematics;statistics	Robotics	0.08441731556420569	-20.432797944430853	86934
0b64fe4e2938bb299afdf38165052c095a222dce	a decision analysis approach to the computer lease-purchase decision	decision analysis	Abstract   The lease-purchase decision is analyzed with the use of decision analysis. The formulation proposed captures both the stochastic and sequential nature of this decision, and should provide a substantial improvement over a simple present value approach.	decision analysis	Steven F. Maier;James H. Vander Weide	1977	Computers & OR	10.1016/0305-0548(77)90012-0	optimal decision;influence diagram;partially observable markov decision process;decision analysis;decision field theory;decision engineering;computer science;knowledge management;decision analysis cycle;decision tree;data mining;decision rule;mathematics;management science;evidential reasoning approach;evidential decision theory;weighted sum model;business decision mapping	Vision	-3.0378113665046493	-15.609538825895513	87029
6c723e13336896b71d23823dac760fa0a0e4f5e4	a new hybrid quadratic regression and cascade forward backpropagation neural network	forecasting;root mean square error;bayesian model averaging;mean absolute error;regression;neural network	In this study, a quadratic regression model (QRM) and a cascade forward backpropagation neural network (CFBN) are jointly integrated together to form a hybrid model called the new hybrid quadratic regression method and cascade forward backpropagation neural (QRM-CFBN) network method. The new hybrid method was tested on a daily time series data obtained from the UCI repository data link and the the Bayesian model averaging technique, which was used to obtain a combined forecast from the two separate methods. The model resulting from the joint integration was applied on the log difference series of the original time series data. The results obtained from the new hybrid QRM-CFBN were compared with the results obtained from the hybrid ARIMA-RNN, standalone cascade forward backpropagation neural (CFBN) network and layered recurrent neural network (LRNN) after being tested on the same sample time series data respectively. The comparison indicates that the results emerging from the new hybrid QRM-CFBN method on the average, generally results in better performance when compared with the hybrid ARIMA-RNN, the standalone CFBN and the standalone LRNN for 1 day, 3 days as well as 5 days prediction mean absolute error (MAE) and root mean square error (RMSE) for varying data samples of 50, 100, 200, 400 and 800 days respectively. The RMSEs and the MAEs were applied to ascertain the assertion that the new jointly integrated forecast has better forecasting performance greater than the standalone CFBN and LRNN forecasts as well as ARIMA-RNN forecast. The analysis for this study was simulated using MATLAB software, version 8.03. & 2015 Elsevier B.V. All rights reserved.	approximation error;artificial neural network;autoregressive integrated moving average;autoregressive model;backpropagation;bivariate data;difference polynomials;ensemble learning;futures studies;hybrid neural network;input/output;matlab;mean squared error;network model;nonlinear system;random neural network;recurrent neural network;sampling (signal processing);stationary process;time series;video synopsis	Augustine Pwasong;Saratha Sathasivam	2016	Neurocomputing	10.1016/j.neucom.2015.12.034	econometrics;regression;forecasting;computer science;machine learning;mean squared error;artificial neural network;mean absolute error;statistics	AI	9.09146638017878	-21.700994638863754	87068
1d8f6e754daec1402ab0a326796fe22486897325	intuitionistic fuzzy similarity measures and their role in classification		We present some similarity and distance measures between intuitionistic fuzzy sets (IFSs). Thus, we propose two semi-metric distance measures between IFSs. The measures are applied to classification of shapes and handwritten Arabic sentences described with intuitionistic fuzzy information. The experimental results permitted to do a comparative analysis between intuitionistic fuzzy similarity and distance measures, which can facilitate the selection of such measure in similar applications.	fuzzy set;intuitionistic logic;qualitative comparative analysis;semiconductor industry	Leila Baccour;Adel M. Alimi;Robert Ivor John	2016	J. Intelligent Systems		machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics	NLP	-4.536471976009903	-23.849851722940596	87170
b0e8c9afbc991ec7ce62e10c8e0937e669a7fd11	impact of 1d vs 3d earth conductivity based electric fields on geomagnetically induced currents		Electric fields that drive geomagnetically induced currents (GICs) depend on the deep earth conductivity. In a power system GIC study, the model used to represent this conductivity thus influences the GIC results. In this paper, we evaluate the differences between the commonly used lD conductivity model and the more complex 3D transfer functions, in terms of their impact on simulated GICs, by validating them against actual GIC measurements. The goal is to provide illustrative results on the performance of these models, in order to show the present capabilities and areas for improvement of conductivity models, and power system GIC modeling. The paper highlights the importance of data availability for model validation research, in order to mitigate the GMD risk.		Komal S Shetve;Adam B. Birchfield;Raymund H. Lee;Thomas J. Overbye;Jennifer L. Gannon	2018	2018 IEEE PES Innovative Smart Grid Technologies Conference Europe (ISGT-Europe)	10.1109/ISGTEurope.2018.8571514		Visualization	6.985473049032097	-11.23994976329233	87415
c34e160ff36bb3c8ad4424464ff463459305d482	on lattice structure and implications on ordered fuzzy numbers	implication;ordered fuzzy numbers;convex fuzzy numbers;defuzzification functionals;lattice;partial order	Ordered fuzzy numbers (OFN) invented by the second author and his two coworkers in 2002 make possible to utilize the fuzzy arithmetic and to construct the Abelian group of fuzzy numbers and then an ordered ring. The definition of OFN uses the extension of the parametric representation of convex fuzzy numbers. Fuzzy implication is proposed with the help of algebraic operations and a lattice structure defined on OFN.	approximation algorithm;crystal structure;degree (graph theory);formal system;fuzzy logic;fuzzy number;linear algebra;mathematical optimization;multi-agent system;ordered pair	Magdalena Kacprzak;Witold Kosinski	2011		10.2991/eusflat.2011.156	combinatorics;mathematical analysis;discrete mathematics;membership function;defuzzification;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;surreal number;fuzzy set operations	NLP	-0.28116885295501076	-22.629456969523165	87431
6c1224cd0daa09e36fe5c3b65582c445d84f3764	a generalized interactive goal programming procedure	multicriteria analysis;decision aid;mode conversationnel;interactive mode;ayuda decision;prise decision;estudio caso;mathematical programming;modo conversacional;programmation objectif;etude cas;aide decision;goal programming;analisis multicriterio;analyse multicritere;programacion objetivo;toma decision;programmation mathematique;programacion matematica	This paper generalizes the interactive sequential goal programming (ISGP) procedure of Masud and Hwang [1]. A fundamental ISGP assumption concerning the preemptive relative importance of achieving adjusted goal target levels is shown to be unnecessarily restrictive and is relaxed. The modified procedure is illustrated with examples.	goal programming	Gary R. Reeves;Scott R. Hedin	1993	Computers & OR	10.1016/0305-0548(93)90061-M	mathematical optimization;computer science;artificial intelligence;goal programming;mathematics;operations research;algorithm	HCI	-0.4062758750198445	-15.757886850121439	87509
344d217013c53f029214a16282162a1ab55d871a	learning betting tips from users' bet selections	forecasting;data mining;machine learning;nearest neighbors;sports betting	In this paper we address the problem of using bet selections of a large number of mostly non-expert users to improve sports betting tips. A similarity based approach is used to describe individual users' strategies and we propose two different scoring functions to evaluate them. The information contained in users' bet selections improves on using only bookmaker odds. Even when only bookmaker odds are used, the approach gives results comparable to those of a regression-based forecasting model.		Erik Štrumbelj;Marko Robnik-Sikonja;Igor Kononenko	2009		10.1007/978-3-642-03070-3_51	forecasting;computer science;machine learning;data mining;statistics	HCI	4.463977443549522	-19.266266859964773	87538
08e85f6588786b99c7a012e2d63433a4dab9e268	combined rough sets with flow graph and formal concept analysis for business aviation decision-making	empirical study;south america;rough set theory;data mining;east asia;information system;rough set;article;decision rule;business aviation;formal concept analysis;knowledge discovery	Although business aviation has been popular in the USA, Europe, and South America, however, top economies in East Asia, including Japan, Korea, and Taiwan, have been more conservative and lag behind in the development of business aviation. In this paper, we hope to discover possible trends and needs of business aviation for supporting the government to make decision in anticipation of eventual deregulation in the near future. We adopt knowledge-discovery tools based on rough set to analyze the potential for business aviation through an empirical study. Although our empirical study uses data from Taiwan, we are optimistic that our proposed method can be similarly applied in other countries to help governments there make decisions about a deregulated market in the future.	eventual consistency;formal concept analysis;rough set	Yu-Ping Ou Yang;How-Ming Shieh;Gwo-Hshiung Tzeng;Leon Yen;Chien-Chung Chan	2009	Journal of Intelligent Information Systems	10.1007/s10844-009-0110-y	rough set;computer science;machine learning;data mining;knowledge extraction;business rule;operations research	HCI	3.1713187853027565	-15.937596461250665	87539
371d9ddf54a821a646ac1869623ffc12402ec203	generalized fuzzy quantitative association rules mining with fuzzy generalization hierarchies	generalized fuzzy quantitative association rules mining with fuzzy generalization hierarchies;generalized association rule;international journal of fuzzy logic and intelligent systems vol 2 no 3;keon myung lee;한국지능시스템학회;importance weight;quantitative association rule;association rule;korean institute of intelligent systems;vol 2 no 3;fuzzy association rule	Association rule mining is an exploratory learning task to discover some hidden dependency relationships among items in transaction data. Quantitative association rules denote association rules with both categorical and quantitative attributes. There have been several works on quantitative association rule mining such as the application of fuzzy techniques to quantitative association rule mining, the generalized association rule mining for quantitative association rules, and importance weight incorporation into association rule mining fer taking into account the users interest. This paper introduces a new method for generalized fuzzy quantitative association rule mining with importance weights. The method uses fuzzy concept hierarchies fer categorical attributes and generalization hierarchies of fuzzy linguistic terms fur quantitative attributes. It enables the users to flexibly perform the association rule mining by controlling the generalization levels for attributes and the importance weights f3r attributes.		Keon-Myung Lee	2002	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2002.2.3.210	association rule learning;artificial intelligence;machine learning;data mining;mathematics;fuzzy set operations	AI	-4.359450253671989	-22.458581898661063	87659
9baae1be9ee53621c159326794c58a89dda62b00	on the properties of equidifferent owa operator	agregacion;minimum variance;metodo entropia maxima;variance minimale;equidifferent owa operator;parameterized owa operator;aggregation;ordered weighted averaging owa operator;general methods;aggregation operator;agregation;methode entropie maximum;computational efficiency;method of maximum entropy;owa operator;maximum entropy	Getting OWA weights under given orness level is an active topic in the OWA operator research. The paper proposes a series of weights generating methods in equidifferent forms. Similar to the geometric (maximum entropy) OWA operator, we propose a parameterized OWA operator called equidifferent OWA operator, which consist the adjacent weighes with a common difference. The maximum spread equidifferent OWA (MSEOWA) operator is equivalent to the minimum variance OWA operator, but is more computational efficient. Some properties associated with the orness level are discussed. One of them is that the aggregation value for any elements set is always increasing with the orness level, which can used as a parameterized aggregation method with orness as its control parameter. These properties similar to that of the geometric (maximum entropy) OWA operator, which can also be seen as the discrete case of equidifferent RIM (regular increasing monotone) quantifiers. The general forms of equidifferent OWA operator are proposed, and the weights generating methods are also extended in a similar way.		Xinwang Liu	2006	Int. J. Approx. Reasoning	10.1016/j.ijar.2005.11.003	mathematical optimization;minimum-variance unbiased estimator;combinatorics;discrete mathematics;principle of maximum entropy;mathematics;statistics	AI	-0.9484978197437085	-22.826790416135008	87820
5ac70cd780612f2e715c1a047a96a63e1bfbd2b5	decision-making with sugeno integrals - bridging the gap between multicriteria evaluation and decision under uncertainty	sugeno integral;decision under uncertainty;multiple criteria decision making;necessity measure;possibility measure;representation;axiomatization	To cite this version : Couceiro, Miguel and Dubois, Didier and Prade, Henri and Waldhauser, Tamas Décision-cadre with Sugeno Integrals Bridging the Gap Between Multicriteria Evaluation and Decision Under Uncertainty. (2016) Order, vol. 33 (n° 3). pp. 517535. ISSN 0167-8094  Open Archive TOULOUSE Archive Ouverte (OATAO) OATAO is an open access repository that collects th e work of Toulouse researchers and makes it freely available over the web where possib le.		Miguel Couceiro;Didier Dubois;Henri Prade;Tamás Waldhauser	2016	Order	10.1007/s11083-015-9382-8	mathematical optimization;combinatorics;discrete mathematics;mathematics	NLP	1.416093273142497	-20.44480120829006	87934
c6c4ee1e2e23fe0d6f76134c0e9c10ed156d752b	ata: the artificial technical analyst. building intra-day market strategies	electronic trading;futures market;high frequency data;stock market ata artificial technical analyst intra day market strategies trading system artificial neural networks self organising maps pattern recognition project guidelines known pattern searching useful knowledge extrapolation future price movement forecasting s p500 intra day futures market contract prices topology representing networks data mining high frequency data;commodity trading;extrapolation;contracts;trading system;data mining;marketing strategy;forecasting theory;self organising feature maps;contracts commodity trading electronic trading self organising feature maps pattern recognition extrapolation forecasting theory data mining;pattern recognition;network topology economic forecasting linearity space technology engines pattern analysis pattern recognition gaussian processes guidelines chaos;artificial neural network;self organising map	A trading system based on artijkial neural networks is therein presented: the ArtiJicial Technical Analyst (ATA) is part of a project focusing on the capabilities of variants of Self-Organising Maps (SOMs) as pattern recognition tools, as well as on their perspective use as forecasters. The basic idea is to take advantage by the analogies shared by SOMs and human technical traders, that is their common search for known patterns, through which extrapolating useful knowledge to forecast future moves of prices. The project is here depicted in its guidelines, discussing experimental results over S&PSOO intra-day futures market contracts prices. ATA: THE ARTIFICIAL TECHNICAL ANALYST BUILDING INTRA-DAY MARKET STRATEGIES Marina Resta, D1.E.M. sez. Matematica Finanziaria, via Vivaldi 2, 16 126,University of Genova, Italy resta@economia.unige.it	algorithmic trading;artificial neural network;extrapolation;futures and promises;pattern recognition;serial ata;traders	Marina Resta	2000		10.1109/KES.2000.884150	artificial intelligence;data mining;business;commerce	AI	7.233943322743192	-19.27354119551685	87985
ffeefaa2c024c8d87d26357701b26859186213e3	a gis-based multi-criteria seismic vulnerability assessment using the integration of granular computing rule extraction and artificial neural networks		This study proposes multi-criteria group decision-making to address seismic physical vulnerability assessment. Granular computing rule extraction is combined with a feed forward artificial neural network to form a classifier capable of training a neural network on the basis of the rules provided by granular computing. It provides a transparent structure despite the traditional multi-layer neural networks. It also allows the classifier to be applied on a set of rules for each incoming pattern. Drawbacks of original granular computing (GrC) are covered, where some input patterns remained unclassified. The study was applied to classify seismic vulnerability of the statistical units of the city of Tehran, Iran. Slope, seismic intensity, height and age of the buildings were effective parameters. Experts ranked 150 randomly selected sample statistical units with respect to their degree of seismic physical vulnerability. Inconsistency of the expertsu0027 judgments was investigated using the induced ordered weighted averaging (IOWA) operator. Fifty-five classification rules were extracted on which a neural network was based. An overall accuracy of 88%, κ = 0.85 and R2 = 0.89 was achieved. A comparison with previously implemented methodologies proved the proposed method to be the most accurate solution to the seismic physical vulnerability of Tehran.	artificial neural network;geographic information system;granular computing;rule induction	Hossein Sheikhian;Mahmoud Reza Delavar;Alfred Stein	2017	Trans. GIS	10.1111/tgis.12274	data mining;artificial neural network;granular computing;computer science;operator (computer programming);feed forward;machine learning;vulnerability;vulnerability assessment;ranking;artificial intelligence	HCI	6.357787430736121	-22.515350084058312	88011
8208bce5c639715e6924427ac89587a614e7ef9b	possibilistic information measures and selection approach	distribution;buoyancy;especificidad;medida informacion;flotabilidad;flottabilite;mesure information;selection;flottabilite maximale stable;stability;information measure;possibility theory;specificity;specificite;seleccion;stabilite;distribucion;specificite minimale stable;teoria posibilidad;estabilidad;theorie possibilite	"""The aim of this paper is to introduce a use of both the principle of minimal specificity (mS) and maximal buoyancy (MB) for selecting a distribution from a given set of quantitative possibility distributions. In this paper some conditions of a set of possibility distributions and weights which guarantee """"always selected situation"""" of a possibility distribution selected by the use of these principles from the given distribution set are proposed."""		Do Van Thanh	1999	Computers and Artificial Intelligence		distribution;possibility theory;selection;stability;artificial intelligence;calculus;mathematics;buoyancy;statistics	AI	1.5399638146713688	-20.666376993189996	88082
aed26e604580c3ec8aa07b274f00efb142f90bf5	a neural network model for road traffic flow estimation		Real-time road traffic state information can be used for traffic flow monitoring, incident detection and other related traffic management activities. Road traffic state estimation can be done using either data driven or model based or hybrid approaches. The data driven approach is preferable for real-time flow prediction but to get traffic data for performance evaluation, hybrid approach is recommended. In this paper, a neural network model is employed to estimate real-time traffic flow on urban road network. To model the traffic flow, the microscopic model Simulation of Urban Mobility (SUMO) is used. The evaluation of the model using both simulation data and real-world data indicated that the developed estimation model could help to generate reliable traffic state information on urban roads.	network model	Ayalew Belay Habtie;Ajith Abraham;Dida Midekso	2015		10.1007/978-3-319-27400-3_27	traffic generation model;network traffic simulation	Networks	8.942445722887452	-13.589535498753794	88204
d4993df1a14c9e743995c140e86c1a4c3c8a02f3	a belief function distance metric for orderable sets	sensor fault detection;orderable sets;dempster shafer theory;distance metric	This paper describes a new metric for characterizing conflict between belief assignments. The new metric, specifically designed to quantify conflict on orderable sets, uses a Hausdorff-based measure to account for the distance between focal elements. This results in a distance metric that can accurately measure conflict between belief assignments without saturating simply because two assignments do not have common focal elements. The proposed metric is particularly attractive in sensor fusion applications in which belief is distributed on a continuous measurement space. Several example cases demonstrate the proposed metricu0027s performance, and comparisons with other common measures of conflict show the significant benefit of using the proposed metric in cases where a sensoru0027s error and noise characteristics are not known precisely a priori.		Zachary Sunberg;Jonathan Rogers	2013	Information Fusion	10.1016/j.inffus.2013.03.003	convex metric space;hausdorff distance;combinatorics;discrete mathematics;topology;dempster–shafer theory;metric;metric k-center;word metric;intrinsic metric;mathematics;equivalence of metrics;string metric;statistics	AI	1.6887986237350507	-19.049810318064964	88256
5e02e9bddfc84ea58594957cd82fdd05665783fe	a model updating strategy for predicting time series with seasonal patterns	model selection;dynamic model;support vector regression;forecasting model;time series;seasonal pattern;exponential smoothing;model updating;dynamic models;time series prediction;neural network	Traditional methodologies for Q1 time series prediction take the series to be predicted and split it into training, validation, and test sets. The first one serves to construct forecasting models, the second set for model selection, and the third one is used to evaluate the final model. Different time series approaches such as ARIMA and exponential smoothing, as well as regression techniques such as neural networks and support vector regression, have been successfully used to develop forecasting models. A problem that has not yet received proper attention, however, is how to update such forecasting models when new data arrives, i.e. when a new event of the considered time series occurs. This paper presents a strategy to update support vector regression based forecasting models for time series with seasonal patterns. The basic idea of this updating strategy is to add the most recent data to the training set every time a predefined number of observations takes place. This way, information in new data is taken into account in model construction. The proposed strategy outperforms the respective static version in almost all time series studied in this work, considering three different error measures. 2009 Elsevier B.V. All rights reserved. * Corresponding author. Fax: +56 2 978 4011. E-mail addresses: jguajard@gmail.com (J.A. Guajardo), rweber@dii.uchile.cl (R. Weber), jmirandap@fen.uchile.cl (J. Miranda).	4000 series;artificial neural network;autoregressive integrated moving average;fax;miranda;model selection;smoothing;support vector machine;test set;time complexity;time series	Jose Guajardo;Richard Weber;Jaime Miranda	2010	Appl. Soft Comput.	10.1016/j.asoc.2009.07.005	econometrics;computer science;machine learning;time series;data mining;dynamic factor;order of integration;artificial neural network;statistics	AI	8.100602451398872	-20.215206133758446	88381
9e8ad2b669bc21ae799c988f5a6c9dfeb63a204e	an interval-valued 2-tuple linguistic group decision-making model based on the choquet integral operator		The Choquet integral (IL) operator is an effective approach for handling interdependence among decision attributes in complex decision-making problems. However, the fuzzy measures of attributes and attribute sets required by IL are difficult to achieve directly, which limits the application of IL. This paper proposes a new method for determining fuzzy measures of attributes by extending Marichalu0027s concept of entropy for fuzzy measure. To well represent the assessment information, interval-valued 2-tuple linguistic context is utilised to represent information. Then, we propose a Choquet integral operator in an interval-valued 2-tuple linguistic environment, which can effectively handle the correlation between attributes. In addition, we apply these methods to solve multi-attribute group decision-making problems. The feasibility and validity of the proposed operator is demonstrated by comparisons with other models in illustrative example part.		Bingsheng Liu;Meiqing Fu;Shuibo Zhang;Bin Xue;Qi Zhou;Shiruo Zhang	2018	Int. J. Systems Science	10.1080/00207721.2017.1407007	operator (computer programming);fuzzy logic;tuple;mathematics;linguistics;group decision-making;choquet integral	NLP	-3.308881169661829	-20.768333156685124	88514
dab6fadf0b47cb86efbaf4180c2a0cd3120ad746	a copula method for correlation of credit rating migration	finance;correlation technique;generic model;financial management;time measurement;correlation theory;credit rating migration;risk management;distributed computing;conference management;random variables;matrix algebra;joints;data mining;margin distribution function;copula method;distribution function;statistical distributions;migration rating matrix;rating migration;statistical distributions correlation theory finance matrix algebra risk management;engineering management;copula function;subprime mortgage loan crisis;loans and mortgages;distribution functions risk management distributed computing crisis management loans and mortgages random variables time measurement conference management financial management engineering management;credit risk management;migration rating matrix credit rating migration copula method correlation technique credit risk management subprime mortgage loan crisis margin distribution function joint distribution function;credit rating;distribution functions;correlation;crisis management;credit risk;copula function credit risk rating migration correlation;joint distribution function	How to monitor the changes of obligators’ credit rating is very important to manage credit risk, especially the obligators have strong correlated with each other, such as subprime mortgage loan crisis. In order to study the credit rating migration of obligators who have correlations with each other, we introduced a new tool to study the correlation of credit risk, the copula function, and built the general model to connect the margin distribution function to a joint distribution function of obligators, then We computed the migration rating matrix with S&P’s data by t-copula, the results show that it is very important of correlation for studying credit rating migration in credit risk management.	risk management	Xiaosi Xu;Ying Chen;Jun Zheng	2009	2009 International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2009.176	financial economics;credit default swap index;actuarial science;finance;business;credit valuation adjustment	DB	3.9446745305678292	-11.100840206421411	88530
a5c22fa24f1a8f0f24ba7a789d1e3966d2297062	intelligent transportation system in macao based on deep self-coding learning		As the basic content of the construction of the intelligent city, the application of intelligent transportation system plays an important role in improving the quality and safety of modern urban traffic operation. Intelligent transportation system combines information technology, data communication technology, electronic sensing technology, global positioning technology, geographic information system technology, computer processing technology, and system engineering technology together reasonably. The intelligent transportation system is a kind of real-time, accurate, efficient, and intelligent transportation management system. In this paper, we introduce the deep code learning technique and apply it to the Macao intelligent system. At the same time, we combine the deep belief network model and support vector regression classifier as the prediction model, and use the deep belief network model to learn traffic flow characteristics.	artificial intelligence;bayesian network;deep belief network;geographic information system;global positioning system;network model;real-time transcription;smart city;support vector machine;systems engineering	Daming Li;Lianbing Deng;Zhiming Cai;Bill Franks;Xiang Yao	2018	IEEE Transactions on Industrial Informatics	10.1109/TII.2018.2810291	simulation;real-time computing;smart city;deep belief network;management system;computer science;traffic flow;geographic information system;intelligent transportation system;information technology;information and communications technology	Robotics	8.96724233014374	-15.236339689154237	88548
7ef09f61c2df53e22105f36a3ba9dd7ae2e3db64	representing and solving asymmetric decision problems using valuation networks		This paper deals with asymmetric decision problems. We describe a generalization of the valuation network representation and solution technique to enable efficient representation and solution of asymmetric decision problems. The generalization includes the concepts of indicator valuations and effective frames. We illustrate our technique by solving Raiffa’s oil wildcatter’s problem in complete detail.	value (ethics)	Prakash P. Shenoy	1995		10.1007/978-1-4612-2404-4_10	mathematical analysis;discrete mathematics;topology	ML	-0.41517275722363367	-20.18414874483499	88796
9bf1fe90e7343610652d707cc783ad1b88ac8e4b	group decision making methodology based on the atanassov's intuitionistic fuzzy set generalized owa operator	intuitionistic fuzzy set;fuzzy set;uncertainty;atanassov s intuitionistic fuzzy set ifs;atanassov s intuitionistic fuzzy relation;journal;group decision making;the ordered weighted averaging owa operator;owa operator	The aim of this paper is to develop a new methodology for solving group decision making problems in which preference comparisons between alternatives are expressed with Atanassov's intuitionistic fuzzy (IF) preference relations. In this methodology, the generalized ordered weighted averaging (GOWA) operator is extended to develop the Atanassov's IF set (IFS) generalized ordered weighted averaging (IFSGOWA) operator, which can aggregate vague or imprecise information expressed with the Atanassov's IFSs. The Atanassov's IFSGOWA operator based methodology is further developed to solve group decision making problems with Atanassov's IF preference relations. A real example of the agroecological region assessment problem is used to illustrate the implementation and applicability of the proposed methodology. It is also shown that the obtained decision results could be affected by the choice of the parameter lambda and the nature of the weights.	fuzzy set	Deng-Feng Li;Li-Ling Wang;Guo-Hong Chen	2010	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488510006805	discrete mathematics;group decision-making;uncertainty;computer science;artificial intelligence;data mining;mathematics;fuzzy set;statistics	Robotics	-2.79028510597411	-20.67846428538632	88837
cdbab39fb69229757c56e916e982066da52884b7	an intuitionistic 2-tuple linguistic information model and aggregation operators		Dealing with uncertainty is always a challenging problem, and different tools have been proposed to deal with it. Fuzzy sets was presented to manage situations in which experts have some membership value to assess an alternative. The fuzzy linguistic approach has been applied successfully to many problems. The linguistic information expressed by means of 2-tuples, which were composed by a linguistic term and a numeric value assessed in [ - 0.5, 0.5. Linguistic values was used to assess an alternative and variable in qualitative settings. Intuitionistic fuzzy sets were presented to manage situations in which experts have some membership and nonmembership value to assess an alternative. In this paper, the concept of an I2LI model is developed to provide a linguistic and computational basis to manage the situations in which experts assess an alternative in possible and impossible linguistic variable and their translation parameter. A method to solve the group decision making problem based on intuitionistic 2-tuple linguistic information I2LI by the group of experts is formulated. Some operational laws on I2LI are introduced. Based on these laws, new aggregation operators are introduced to aggregate the collective opinion of decision makers. An illustrative example is given to show the practicality and feasibility of our proposed aggregation operators and group decision making method.	information model	Ismat Beg;Tabasam Rashid	2016	Int. J. Intell. Syst.	10.1002/int.21795	artificial intelligence;data mining;mathematics;management science	DB	-3.8738941454154547	-21.01632305063063	88884
5320c701726cd296719e48f586a8a7b9d6bc4e2a	operators and comparisons of hesitant fuzzy linguistic term sets	operations research computational linguistics decision making fuzzy set theory mathematical operators;possibility degree formula;hesitant fuzzy linguistic term set theory mcdm multicriteria decision making problems hesitant fuzzy lowa operator hesitant fuzzy lwa operator hflts aggregation operators linguistic assessments;期刊论文;possibility degree formula hesitant fuzzy linguistic term sets hfltss multicriteria decision making mcdm;hesitant fuzzy linguistic term sets hfltss;multicriteria decision making mcdm	The theory of hesitant fuzzy linguistic term sets (HFLTSs) is very useful in objectively dealing with situations in which people are hesitant in providing linguistic assessments. The purpose of this paper is to develop comparison methods and study the aggregation theory for HFLTSs. We first define operations on HFLTSs and give possibility degree formulas for comparing HFLTSs. We then define two aggregation operators for HFLTSs: a hesitant fuzzy LWA operator and a hesitant fuzzy LOWA operator. In actual application, we use these operators and the comparison methods to deal with multicriteria decision-making problems with different situations in which importance weights of criteria or experts are known or unknown.		Cuiping Wei;Na Zhao;Xijin Tang	2014	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2013.2269144	discrete mathematics;artificial intelligence;mathematics;algorithm	DB	-3.145892140421468	-21.320243433015076	88960
6ac3bcb8a5f24277dd112c134b929e4abd7b19d6	a simulation based multi-attribute group decision making technique with decision constraints	decision constraints;topsis;triangular distribution;absolute judgments;multi attribute group decision making;monte carlo simulation	The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) is a useful technique for solving Multi Attribute Group Decision Making (MAGDM) problems. In MAGDM, the performance scores of the alternatives and the weights of assessment attributes are mostly vague. Therefore, using of deterministic data throughout decision making process may lead to inaccurate results. In order to overcome inherent vagueness and uncertainty, various fuzzy MAGDM techniques were presented in the literature. However, these fuzzy MAGDM techniques are focused on expected and extreme values, which are sometimes insufficient for the precise determination of alternatives’ preference structure. In this paper, in order to eliminate the limitations of deterministic and fuzzy MAGDM methods, we present a probabilistic methodology, which is based on TOPSIS and Monte-Carlo simulation of triangular data. In addition to its straightforward application and thanks to its versatility, simulation enables decision makers to incorporate some decision constraints into decision-making process. Two illustrative examples are also given to show the effectiveness of the proposed methodology. The method is also compared with a fuzzy TOPSIS technique from the literature.	simulation	Hüsamettin Bayram;Ramazan Sahin	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.08.049	topsis;artificial intelligence;triangular distribution;data mining;mathematics;management science;statistics;monte carlo method	Logic	-3.767999362124946	-19.166693025897562	88993
2311cb59fab6de4fdb095207d63ea7b444fc2a6f	divergence measure between fuzzy sets using cardinality			fuzzy set	Vladimír Kobza	2017	Kybernetika	10.14736/kyb-2017-3-0418	divergence;discrete mathematics;cardinality;mathematics;mathematical optimization;fuzzy set	Vision	0.7502377066030036	-21.840089109291544	89398
b61ad8be3cb53c07c56bcb474ab788e9759844d6	inverse dea with frontier changes for new product target setting	frontier change;data envelopment analysis;new product development;article	Inverse data envelopment analysis (DEA) is a reversed optimization problem that can serve as a useful planning tool for managerial decisions by providing information such as how much resources (or outcomes) should be invested (or produced) to achieve a desired level of competitiveness whereas the conventional DEA focuses mainly on a post-hoc assessment of the organizational performance. Inverse DEA studies however are based on an assumption that the efficiency level of observed decision making units (DMUs) will not change within the period of interest, which in fact confines the use of inverse DEA to a sensitivity analysis by simply addressing what alternative levels of input and/or output would have been possible to result in the same efficiency score obtained. In this paper, we discuss an inverse DEA problem considering expected changes of the production frontier in the future by integrating the inverse optimization problem with a time series application of DEA so that it can be an ex-ante decision support tool for the new product target setting practices. We use an example of the vehicle engine development case to demonstrate the proposed method.		Dong Joon Lim	2016	European Journal of Operational Research	10.1016/j.ejor.2016.03.059	econometrics;economics;computer science;operations management;data envelopment analysis;mathematics;operations research;new product development	Theory	-0.03221506040265674	-13.109493014138666	89477
522ef548c054fd03109ff1886d243a11ce6d1e2a	mapreduce-based deep learning with handwritten digit recognition case study	handwriting recognition;training;distributed computing;distortion;computational modeling;character recognition;biological neural networks	Faced with the continuously increasing scale of data and expectation on response time, complex deep learning technologies, though highly accurate, present two non-rival challenges: a large amount of training data makes a model impossible to be built in short time and intolerable time-cost prohibits acceptable real-time responses. In this research we focus on improving the accuracy and efficiency of the handwritten digit recognition problem. We chose this problem because it is regarded as the prototype of a lot of complex recognition and classification problems. The success of classification of the handwritten digit dataset can be extended further to other advanced areas. The Convolutional Neural Network (CNN) is implemented to do the recognition. We further improvd the accuracy by adding elastic distortion to the input data, which helps the model better select the features. In addition we implement distributed computing to reduce the time cost. The training process is divided and a final model is formulated by the combination of each trained model. The results shows two facts: the elastic distortion helped the CNN model to improve the accuracy by 7–10%; and the distributed computing method reduced the training time consumption by about 50%.	algorithm;artificial neural network;convolutional neural network;deep learning;distortion;distributed computing;mapreduce;prototype;real-time clock;response time (technology)	Nada Basit;Yutong Zhang;Hao Wu;Haoran Liu;Jieming Bin;Yijun He;Abdeltawab M. Hendawi	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840783	speech recognition;intelligent character recognition;computer science;artificial intelligence;machine learning	Robotics	7.720458507731233	-22.500995631714414	89567
a8efbbe09cf722b0376e0d74133628d95aea2c73	an analytic formula for the weighted average of fuzzy numbers under tw(the weakest t-norm)-based fuzzy arithmetic operations	t_w norm;t w norm;extended operations;fuzzy weighted average	Many authors considered the computational aspect of sup-min convolution when applied to weighted average operations. They used a computational algorithm based on a-cut representation of fuzzy sets, nonlinear programming implementation of the extension principle, and interval analysis. It is well known that TW (the weakest t-norm)-based addition and multiplication preserve the shape of L-R type fuzzy numbers. Recently, TW-based division was considered by Hong (2006). In this paper, we consider the computational aspect of the extension principle by the use of TW when this principle is applied to fuzzy weighted average operations. We give the exact solution for the case where the variables and coefficients are L-L fuzzy numbers without programming or the aid of computer resources	algorithm;coefficient;computation;convolution;fuzzy number;fuzzy set;interval arithmetic;mahdiyar;maxima and minima;nonlinear programming;nonlinear system;r-type	Dug Hun Hong;Kyung Tae Kim	2006	2006 International Conference on Computational Intelligence and Security	10.5391/IJFIS.2007.7.1.085	mathematical optimization;combinatorics;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy number;mathematics;fuzzy set operations	AI	-0.8232756625314497	-21.61288870365878	89582
86e8db4fe382640e6f3d0e6d930107e8c422514c	retail commodity sale forecast model based on data mining	analytical models;data mining;solid modeling;predictive models;markov processes;decision trees;data models	In terms of the retail commodity sale forecast, people did more in particular aspect with commodity's single sale attribute such as the sale volume, the sale money, the season factor, but all has not considered the most important factor-profit, the profit is the key factor of retail enterprises winning the survival and development. However, such a one-sided analysis is not conducive to assist the managers understand the overall situation of retail sales, and make the decision the sale and the inventory. So this paper firstly selected the profit ratio which was on behalf of commodity profit element and several other key sale attributes including the season ratio and the sale volume to establish the SPV Model, secondly done commodity sale state segmentation based on the SPV Model with ID3 decision tree algorithm, And on this basis we predicted the sale state of the commodity at some future time, Finally, we compared and analysis the results of the SPV Model, the Season Model and the Markov Model through experiments, and get the conclusion that the SPV Model can reach higher correctness than the other two.	algorithm;correctness (computer science);data mining;decision tree;experiment;list of algorithms;markov chain;markov model;money	Jing Zhang;Juan Li	2016	2016 International Conference on Intelligent Networking and Collaborative Systems (INCoS)	10.1109/INCoS.2016.42	data modeling;computer science;machine learning;decision tree;data mining;predictive modelling;markov process;solid modeling	Web+IR	-0.9858443287606631	-11.129556453317356	89800
0ec9641c1c214c1e01f5163fb6400f9d1d0db950	numerical weather prediction or stochastic modeling: an objective criterion of choice for the global radiation forecasting		Numerous methods exist and were developed for global radiation forecasting. The two most popular types are the numerical weather predictions (NWP) and the predictions usingstochastic approaches. We propose to compute a parameter noted  constructed in part from the mutual information which is a quantity that measures the mutual dependence of two variables. Both of these are calculated with the objective to establish the more relevant method between NWP and stochastic models concerning the current problem.	mutual information;numerical analysis;numerical method;numerical weather prediction;stochastic modelling (insurance);stochastic process	Cyril Voyant;Gilles Notton;Christophe Paoli;Marie-Laure Nivet;Marc Muselli;Kahina Dahmani	2014	CoRR		computer simulation;meteorology;probabilistic forecasting;econometrics;atmospheric sciences;prediction;weather forecasting;stochastic modelling;global forecast system;model output statistics;numerical weather prediction;solar energy;mutual information;stochastic;statistics	ML	7.8947859189406016	-15.528986718656748	90008
4cd26349f27659320b2a6bc17259ce62031613cb	the relationship between the minimum-variance and minimax disparity rim quantifier problems	minimum variance;fuzzy set;absolute continuity;fuzzy sets;rim quantifier;generating function;minimax disparity;correctness proof;owa operator	Recently, Liu and Lou [On the equivalence of some approaches to the OWA operator and RIM quantifier determination, Fuzzy Sets and Systems 159 (2007) 1673–1688] investigated the equivalence of solutions to the minimum-variance and minimax disparity RIM quantifier problems. However, their proofs are very sensitive to the assumption, and some are mathematically incomplete. In this regard, this paper provides a counterexample of the minimax disparity RIM quantifier problem for the case in which generating functions are continuous. The paper also provides a correct proof of the minimax disparity RIM quantifier problem for the case in which generating functions are absolutely continuous and a generalized result for the minimum-variance RIM quantifier problem for the case in which generating functions are Lebesgue integrable. Based on the results, the paper provides a correct relationship between the minimum-variance and minimax disparity RIM quantifier problems. © 2011 Elsevier B.V. All rights reserved.	binocular disparity;fuzzy sets and systems;fuzzy set;minimax;quantifier (logic);turing completeness	Dug Hun Hong	2011	Fuzzy Sets and Systems	10.1016/j.fss.2011.05.014	mathematical optimization;discrete mathematics;quantifier elimination;computer science;artificial intelligence;mathematics;fuzzy set;statistics	AI	0.27776894334624735	-21.61133380583722	90049
df607913f39d982d8246b936096ee5d35a965a71	multiple attribute group decision making based on generalized trapezoid fuzzy linguistic prioritized weighted average operator		In this paper, we investigate the trapezoid fuzzy linguistic multiple attribute group decision making problem with priority considerations among attributes as well as decision makers. By combining the idea of generalized mean and prioritized weighted average operator, we propose a new prioritized weighted aggregation operator called generalized trapezoid fuzzy linguistic prioritized weighted average (GTFLPWA) operator for aggregating trapezoid fuzzy linguistic information. Properties and special cases of the new aggregation operator are studied in detail. Furthermore, using GTFLPWA operator, an approach to deal with multiple attribute group decisionmaking problems under trapezoid fuzzy linguistic environments is developed. Finally, a practical example is illustrated to show the feasibility and superiority of the proposed approach.	ordered weighted averaging aggregation operator	Rajkumar Verma	2017	Int. J. Machine Learning & Cybernetics	10.1007/s13042-016-0579-y	mathematical optimization;discrete mathematics;machine learning;mathematics	AI	-2.9125900951906822	-20.682093179015403	90175
a5dcb30406e8ff210d24c81472daf0ff52b3859e	estimation models generation using linear genetic programming	time series forecasting;genetic program;programming language;linear genetic programming;model generation;consumer price index;economic indicator;soybean;evolutionary algorithm;decision rule	The use of decision rules and estimation techniques is increasingly common for decision making. In recent years studies were conducted which applies Genetic Programming (GP) to obtain rules to make predictions. A new branch in the area of Evolutionary Algorithms (EA) is Linear Genetic Programming (LGP). LGP evolves instructions sequences of an imperative programming language. This paper proposes estimation models generation for time series forecasting using LGP. The forecasting result for the Consumer Price Index (CPI) and the price of soybeans per ton shows the potential of this new proposal.	artificial neural network;evolutionary algorithm;experiment;imperative programming;linear function;linear genetic programming;nonlinear system;predictive modelling;programming language;test case;time series	Javier Martinez Canillas;Roberto Sanchez;Benjamín Barán	2009	CLEI Electron. J.		genetic programming;consumer price index;computer science;engineering;artificial intelligence;machine learning;evolutionary algorithm;economic indicator;time series;decision rule	AI	7.412313647600212	-19.311417968884136	90234
6a4d9fdc98f7eb8b7093b4238683869e735392f5	on mean type aggregation	fuzzy logic mean type aggregation self identity centering property weighted mean aggregation;randomized experiment;fuzzy logic;aggregation operator;commutation machine intelligence	We introduce and define the concept of mean aggregation of a collection of n numbers. We point out that the lack of associativity of this operation compounds the problem of the extending mean of n numbers to n+1 numbers. The closely related concepts of self identity and the centering property are introduced as one imperative for extending mean aggregation operators. The problem of weighted mean aggregation is studied. A new concept of prioritized mean aggregation is then introduced. We next show that the technique of selecting an element based upon the performance of a random experiment can be considered as a mean aggregation operation.	copy (object);idempotence;imperative programming;least mean squares filter;mean squared error;numbers;sexual abstinence	Ronald R. Yager	1996	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.485833	fuzzy logic;ordered weighted averaging aggregation operator;combinatorics;discrete mathematics;computer science;artificial intelligence;randomized experiment;mathematics;algorithm;statistics	Embedded	-0.8130648512374838	-22.249869010688002	90398
2c1ca2ebc12d9601430535aba7e2af356e36c8db	group consistency and group decision making under uncertain probabilistic hesitant fuzzy preference environment		With regard to element uncertainty in a hesitant fuzzy element (HFE), the probabilistic hesitant fuzzy element (PHFE) was developed. It has been noted that the occurrence probabilities of elements in PHFE are difficult to obtain accurately and sufficiently through subjective evaluation. Therefore, in this study, we propose the uncertain probabilistic hesitant fuzzy element (UPHFE), a generalized fuzzy number, which includes four types of HFEs. Compared with other fuzzy numbers, subjective preference information can be described more properly by the UPHFE. Furthermore, the UPHFE is extended to the uncertain probabilistic hesitant fuzzy preference relations (UPHFPRs). Besides, investigations about the expected consistency, acceptable expected consistency, probability-obtaining approach, and consistency-improving iterative algorithm for reasonable application of UPHFPRs are illustrated respectively. Then, we introduce the UPHFPRs and these methods into a group decision-making process, for which two operators are proposed to aggregate the UPHFPRs and ensure that the aggregated preference relations can remain UPHFPRs. Because the aggregated UPHFPRs may be inconsistent, we further design an acceptable group consistency test. Subsequently, we summarize the group decision-making process under the UPHFPR environment. Finally, an example that selects the optimal objective from four newly listed stocks on the Growth Enterprises Market board in China is provided to demonstrate the proposed approaches.	aggregate data;algorithm;control theory;fuzzy number;fuzzy set;global optimization;granular computing;human factors and ergonomics;iterative method;mathematical optimization	Wei Zhou;Zeshui Xu	2017	Inf. Sci.	10.1016/j.ins.2017.06.004	artificial intelligence;iterative method;operator (computer programming);mathematics;fuzzy logic;machine learning;group decision-making;probabilistic logic;fuzzy number	AI	-3.450441162197407	-19.934154891004038	90526
5f0f4ec48b22e3d298d5d0b36e76bc27502b0bb7	a method to evaluate and compare evolutionarily stable strategies in fuzzy payoff games		Evolutionarily stable strategy (ESS) is a key concept in evolutionary game theory. ESS provides an evolutionary stability criterion for biological, social and economic behaviors. In this paper, a method is developed to evaluate ESS in symmetric two-person games with fuzzy payoffs. Every strategy is assigned a membership that describes to what extent it is an ESS. The fuzzy set of ESS can characterize the nature of ESS, and also gives a ranking of the stablest strategies. This method uses the satisfaction function to compare fuzzy payoffs, and adopts the fuzzy decision rule to obtain the membership function of the fuzzy ESS set. The relation between fuzzy ESS and fuzzy Nash equilibrium is also explored. In a symmetric two-person game, the fuzzy ESS set is a subset of the fuzzy symmetric Nash equilibrium set. The numeric results are congruous as expected, therefore this method to evaluate and compare ESSs is appropriate for fuzzy payoff games.	fuzzy set;game theory;nash equilibrium	Haozhen Situ	2015	CoRR		mathematical optimization;membership function;mathematics;mathematical economics;welfare economics	AI	-1.7087240165120496	-18.514624330180492	90573
3d0530e35c93d9d96de97f3ee414539030ba9481	key course selection for academic early warning based on gaussian processes		Academic early warning (AEW) is very popular in many colleges and universities, which is to warn students who have very poor grades. The warning strategies are often made according to some simple statistical methods. The existing AEW system can only warn students, and it does not make any other analysis for academic data, such as the importance of courses. It is significant to discover useful information implicit in data by some machine learning methods, since the hidden information is probably ignored by the simple statistical methods. In this paper, we use the Gaussian process regression (GPR) model to select key courses which should be paid more attention to. Specifically, an automatic relevance determination (ARD) kernel is employed in the GPR model. The length-scales in the ARD kernel as hyperparameters can be learned through the model selection procedure. The importance of different courses can be measured by these corresponding length-scales. We conduct experiments on real-world data. The experimental results show that our approaches can make reasonable analysis for academic data.	experiment;gaussian process;kernel (operating system);kriging;machine learning;model selection;point of view (computer hardware company);relevance	Min Yin;Jing Zhao;Shiliang Sun	2016		10.1007/978-3-319-46257-8_26	simulation;artificial intelligence;machine learning	ML	5.145533096875835	-21.53719261109995	90700
bed28c767b58e9dedf63358ab4e0d57f4a41e5a4	investing in emerging markets using neural networks and particle swarm optimisation	stock markets investment learning artificial intelligence neural nets particle swarm optimisation profitability;training;industry standard stock selection method emerging market investing neural networks particle swarm optimisation automated trading strategies stock trading model learning matthews correlation coefficient trading profit transaction costs;industries;artificial neural networks;training industries economics artificial neural networks;emerging markets particle swarm optimisation trading stock selection;economics	Emerging markets represent a particular challenge to both investors and those interested in developing automated trading strategies. However as well as exposing investors to potential risk, these markets can also offer high returns. Here, a stock trading model is developed for these markets, using both particle swarm optimisation and neural networks. Learning is in part driven by the Matthews correlation coefficient, a task-unspecific but effective fitness measure for unbalanced data sets, used by the authors in previous work, and in addition by a realistic measure of trading profit that incorporates transaction costs. The recommendations from the hybrid model are compared to those obtained from an industry standard stock selection method, with favourable results.	algorithmic trading;artificial neural network;effective fitness;mathematical optimization;matthews correlation coefficient;particle swarm optimization;technical standard;unbalanced circuit	Pascal Khoury;Denise Gorse	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280777	computer science;trading strategy;machine learning;artificial neural network;algorithmic trading	ML	5.861500805069889	-18.752141558914218	90710
5588082c0dee61fab573838c4ffbbd25982b3b6f	combinatorial auction under fuzzy environment	optimal solution;multiple prices;price uncertainty;fuzzy logic;possibilistic distribution;fuzzy logics;information value;risk assessment;winner determination problem;forward auction;combinatorial auction	Combinatorial auction (CA) mechanism allows bundling of multiple items in packages, which can be solved through a clearing method termed as the winner determination problem (WDP). However, to date, there has yet to be a CA model that accounts for the fuzziness of bidders' submitted prices. The imprecision in submitted prices is the result of the time gap between bid placement and winning bid announcement, which reflects the bidders' expected values of the goods at the point of contract sale. Despite this common understanding, conventional CA modeling still treats the prices as deterministic. This causes a major shortcoming when an uncertain environment is assumed to be deterministic and solved through conventional WDP. This study shows that a fuzzy environment modeled via a deterministic WDP approach provides overly optimistic revenue for the auctioneer. A method of using possibilistic distributions of submitted prices to account for price uncertainty is proposed and formalized as Fuzzy Combinatorial Auction Winner Determination Problem (Fuzzy CA WDP). The difference in optimal solutions in deterministic WDP and fuzzy WDP reflects the amount of over estimation when a fuzzy situation is treated as though it is precise. It also reflects the information value when the uncertainty inherent in the fuzzy environment is resolved. Given that the information value is quantified in unit dollars, the fuzzy WDP approach allows the auctioneer to estimate its ''true'' revenue despite price uncertainties.		Joshua Ignatius;Young-Jou Lai;Seyyed Mahdi Hosseini Motlagh;Mohammad Mehdi Sepehri;Adli Mustafa	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.03.022	fuzzy logic;risk assessment;combinatorial auction;computer science;artificial intelligence;fuzzy number;machine learning;value of information;forward auction	ECom	1.2612198483442036	-13.046442851174868	91074
31a24111b9020b4c2b3909460d0b3b47c2990554	an anfis algorithm for forecasting overall equipment effectiveness parameter in total productive maintenance.				Ebru Turanoglu;Mehmet Çakmakci;Cengiz Kahraman	2015	Multiple-Valued Logic and Soft Computing		simulation	AI	7.3453288077820424	-10.855799587154094	91503
7301311f9ad372894745e7da6d08b40e147ec0a5	surprisal-driven feedback in recurrent networks		Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC.	artificial neural network;discrepancy function;feedforward neural network;recurrent neural network;self-information;top-down and bottom-up design	Kamil Rocki	2016	CoRR		computer science;artificial intelligence;machine learning;data mining	ML	9.001646391672422	-23.12916610726884	91901
b7179b03d1d0a9539af606aba6bd540bdffa0617	dynamic control mechanisms for revenue management with flexible products	revenue management;capacity control;flexible products;dynamic control;mathematical model;capacity utilization	Revenue management with flexible products has experienced a growing interest in the academic literature within the last few years. Flexible products allow supply-side substitution between resources and can therefore help to maximize overall revenue as well as capacity utilization in markets with highly uncertain demand. This paper addresses the question of how the mathematical models which have been developed for capacity control with flexible products should be used over time to exploit the substitution opportunities, while keeping practical applicability in mind. Several dynamic control mechanisms are proposed, each of which makes use of the flexibility to a different extent. A comprehensive computational study shows the potential of the different approaches by revealing their strengths and weaknesses.	control system	Anita Petrick;Jochen Gönsch;Claudius Steinhardt;Robert Klein	2010	Computers & OR	10.1016/j.cor.2010.02.003	capacity utilization;mathematical model;mathematics	ECom	-3.7966909733915806	-13.312428583579008	91985
af966a0b59764dbb9d801d62598331a606d006dc	management of ignorance by interval probability	probability humans uncertainty;finite set;probability;interval probability;uncertainty;subjective probability;evidence theory;finite set ignorance management interval probability evidence theory dempster shafer theory pairwise comparison matrix;inference mechanisms;ignorance management;posterior probability;matrix algebra;set theory;dempster shafer theory;pairwise comparison matrix;humans;conditional probability;set theory inference mechanisms matrix algebra probability	Interval probabilities have been proposed as one of non-additive measures. The frame of interval probabilities is similar to evidence theory proposed by Dempster and Shafer and they can be regarded as evidences on a finite set. The interval probability is suitable to represent ignorance on the given phenomenon so that it can be used as a kind of subjective probability. We show how to obtain the evidence by a pairwise comparison matrix on a finite set. The pariwise comparisons are usually inconsistent each other since they are given based on human judgements. The interval probabilities from them are determined so as to include such inconsistency. In case of two evidences whose prior and conditional probabilities are obtained as intervals, the marginal and posterior probabilities are also calculated as interval probabilities from the view of possibility. The illustrative numerical example is given in this paper.	marginal model;numerical analysis;utility functions on indivisible goods	Tomoe Entani;Hideo Tanaka	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295475	calibrated probability assessment;discrete mathematics;uncertainty;conditional probability;dempster–shafer theory;finite set;pattern recognition;probability;chain rule;mathematics;law of total probability;posterior probability;statistics;set theory	Vision	-0.05997506592345547	-19.432474030372205	92054
a3779837352d38083c00c47aa42700fa095ebb5f	fuzzy number-based hierarchical fuzzy system	hierarchical system;numero difuso;hierarchized structure;fuzzy number;systeme hierarchise;logique floue;nombre flou;priorite;logica difusa;structure hierarchisee;fuzzy logic;sistema jerarquizado;priority;prioridad;estructura jerarquizada;fuzzy system	Hierarchical fuzzy systems allow for reducing number of rules and for prioritization of rules. To retain fuzziness, intermediate signals should be fuzzy. Transferring fuzzy signal is computationally demanding. Special form of hierarchical fuzzy system is proposed to reduce computational burden.	fuzzy concept;fuzzy control system;fuzzy number	Adam E. Gaweda;Rafal Scherer	2004		10.1007/978-3-540-24844-6_42	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;control theory;mathematics;hierarchical control system;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	Robotics	2.523490345082533	-22.954245521328765	92190
57fd13e1852961fe39a2499f4df62bbb44fc688f	development of intuitionistic fuzzy super-efficiency slack based measure with an application to health sector		Abstract Data envelopment analysis (DEA) is a linear programming based technique, which determines the performance efficiencies of homogeneous decision making units (DMUs). Slack based measure (SBM) model finds the performance efficiency, and it deals with the input excesses and output shortfalls of DMUs. In conventional SBM, the data is crisp. But it fluctuates in the real world applications. Such data can take the form of fuzzy/intuitionistic fuzzy (IF) number. In this paper, we propose an IF slack based measure (IFSBM) model to determine the efficiency of DMUs and IF super efficiency SBM (IFSESBM) model to determine the efficiency of efficient DMUs for α in ( 0 , 1 ] and β in [ 0 , 1 ) . Also, we propose a ranking method for intuitionistic fuzzy interval numbers (IFINs) based on α and β -cuts. Finally, a health sector application of the proposed model is presented with two IF inputs: (i) number of beds (ii) number of doctors and two IF outputs: (i) number of pathology operations (ii) number of minor surgeries.	slack variable	Alka Arya;Shiv Prasad Yadav	2018	Computers & Industrial Engineering	10.1016/j.cie.2017.11.028	fuzzy logic;engineering;mathematical optimization;homogeneous;linear programming;ranking;data envelopment analysis	SE	-2.3775304244850557	-16.12037957446375	92226
4819e207a15a4a20508d1f66b8d2ed3aab2f110a	a multiple criteria decision making model with entropy weight in an interval-transformed hesitant fuzzy environment	hesitant fuzzy set;interval-transformed hesitant fuzzy set;multiple attribute decision making;entropy measure	This article first aims to critically review the existing literature on entropy measures for hesitant fuzzy elements (HFEs), and then introduces the concept of interval-transformed HFE (ITHFE) which bridges HFEs and interval-valued fuzzy sets (IVFSs). As discussed later, this bridge will also benefit researchers in terms of opening up more directions for future work, concentrating on HFE entropy measures. By taking the concept of ITHFE into account, we here exploit three features of an interval value including its lower and upper bounds, and the range of possible values to define a new class of entropy measures for HFEs. Then, we introduce the axiomatic framework of the new measures of entropy for HFEs, and two families of HFE entropy measures are also constructed. A comparison results shows that the proposed entropy measures for HFEs are more confident in distinguishing different HFEs rather than the most existing entropy measures. Finally, a multiple attribute decision making problem based on TOPSIS is applied to a case study of the health-care waste management.	axiomatic system;entropy (information theory);fuzzy set;human factors and ergonomics;interval arithmetic	B. Farhadinia	2017	Cognitive Computation	10.1007/s12559-017-9480-6	computer science;fuzzy logic;artificial intelligence;machine learning;axiom;fuzzy set;data mining;decision-making models;topsis	SE	-3.4954539144845884	-22.04681815704064	92370
5b12c3b2db52ebfff63e0e34421f9c43988e49d3	fuzzy linear regression analysis from the point of view risk	possibility;fuzzy regression;fuzzy numbers;necessity;linear regression;mathematical programming;point of view	In this paper, fuzzy linear regression models with fuzzy/crisp output, fuzzy/crisp input are considered. In this regard, we define risk-neutral, risk-averse and risk-seeking fuzzy linear regression models. In order to do that , two equality indices are applied to express the degree of equality between a pair of fuzzy numbers. We also develop three mathematical models to obtain the parameters of fuzzy linear regression models. Minimizing the difference between the total spread of the observed and estimated values is the objective of these models. The advantage of our proposed models is the simplicity in programming and computation.	computation;fuzzy number;mathematical model;point of view (computer hardware company);risk aversion	Mohammad Modarres;Ebrahim Nasrabadi;Mohammad Mehdi Nasrabadi	2004	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488504003120	fuzzy logic;econometrics;mathematical optimization;proper linear model;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;linear regression;artificial intelligence;fuzzy number;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	-1.701787123479826	-18.40352935600524	92591
2b7605e4e2c1d9929068e602bc5be931e481998a	a real-time integrated hierarchical temporal memory network for the real-time continuous multi-interval prediction of data streams	hierarchical temporal memory;multiple interval prediction;data streams;real time prediction	Continuous multi-interval prediction (CMIP) is used to continuously predict the trend of a data stream based on various intervals simultaneously. The continuous integrated hierarchical temporal memory (CIHTM) network performs well in CMIP. However, it is not suitable for CMIP in real-time mode, especially when the number of prediction intervals is increased. In this paper, we propose a real-time integrated hierarchical temporal memory (RIHTM) network by introducing a new type of node, which is called a Zeta1FirstSpecializedQueueNode (ZFSQNode), for the real-time continuous multi-interval prediction (RCMIP) of data streams. The ZFSQNode is constructed by using a specialized circular queue (sQUEUE) together with the modules of original hierarchical temporal memory (HTM) nodes. By using a simple structure and the easy operation characteristics of the sQUEUE, entire prediction operations are integrated in the ZFSQNode. In particular, we employed only one ZFSQNode in each level of the RIHTM network during the prediction stage to generate different intervals of prediction results. The RIHTM network efficiently reduces the response time. Our performance evaluation showed that the RIHTM was satisfied to continuously predict the trend of data streams with multi-intervals in the real-time mode.	circular buffer;coupled model intercomparison project;html;hierarchical temporal memory;performance evaluation;real-time clock;real-time transcription;response time (technology)	Hyun-Syug Kang	2015	JIPS	10.3745/JIPS.02.0011	real-time computing;computer science;machine learning;data mining;data stream mining;hierarchical temporal memory	ML	6.435255637619674	-16.20834479883426	92771
e778a95bc607dba6283ff55d2ecc46a4d68ec605	on-line economic dispatch of distributed generation using artificial neural networks		In recent years, distributed generators (DG) are most widely installed in distribution system to meet the increasing demand and especially to reduce the losses. According to demand, dispatch of generator should be modified for economic operation. The Economic Dispatch (ED) of DGs are usually solved by conventional methods such as Lambda iteration method, Dynamic Programming etc., or any optimization technique such as Genetic algorithm (GA), Evolutionary Programming (EP) etc., This off-line methods of solving ED problem require comparatively large computation time and are not suitable for on-line applications. Therefore, it is important to estimate Real Power dispatch values within a short period. This paper presents an On-line ED of various non-renewable DGs for various demands using Artificial Neural Networks namely Back Propagation Neural Network (BPNN) and Radial Basis Function Neural Network (RBFNN). The input pattern for Neural Networks (NN) is demand and output is corresponding optimal real power dispatch. The input and output patterns for NN is obtained using evolutionary programming method. In this work two diesel engines and two fuel cells are used as DG. This case study has been illustrated in a distribution system having two types of four numbers of DGs. The test result shows that the proposed method is better for real time ED.	artificial neural network;neural networks	M. Arumuga Babu;R. Mahalakshmi;S. Kannan;M. Karuppasamypandiyan;A. Bhuvanesh	2014		10.1007/978-3-319-20294-5_24	artificial neural network;genetic algorithm;evolutionary programming;mathematical optimization;economic dispatch;computer science;dynamic programming;distributed generation;input/output;backpropagation	Robotics	9.327375768403424	-17.603380103294164	92839
68f6a931101bcad9aae40865eb5133090130421a	a note on ahp group consistency for the row geometric mean priorization procedure	analytic hierarchy process;row geometric mean;geometric consistency index;multicriteria decision making;ahp;indexation;group decision making;geometric mean;group decision;consistency;eigenvectors;group decisions and negotiations	This work analyses the consistency in group decision making for the analytic hierarchy process (AHP). When using the weighted geometric mean method (WGMM) as the aggregation procedure, the row geometric mean method (RGMM) as the priorization procedure, and the geometric consistency index as the inconsistency measure, the paper proves that the inconsistency of the group is smaller than the largest individual inconsistency. This result complements that obtained by Xu [Eur. J. Oper. Res. 126 (2000) 683] for the eigenvector priorization method (EM) and its associated consistency index [Saaty, Multicriteria Decision Making: The Analytic Hierarchy Process, McGraw-Hill, 1980]. Moreover, our result guarantees that by using the RGMM priorization procedure, the group priorities obtained through the aggregation of the individual priorities verify the requirement of consistency proposed in AHP methodology if the individual priorities also verify this requirement.		María Teresa Escobar;Juan Aguarón;José María Moreno-Jiménez	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00154-1	mathematical optimization;discrete mathematics;analytic hierarchy process;mathematics;welfare economics	Vision	-3.0819882255606967	-19.813915139333414	92945
40b7733028419f970bd6d7a6d3fd13bf22799a9a	a c-owa operator-based approach to decision making with interval fuzzy preference relation	agregacion;analisis datos;systeme aide decision;fuzzy relation;decision borrosa;decision floue;sistema ayuda decision;prise decision;aggregation;fuzzy preference relation;resolucion problema;data analysis;decision support system;preferencia;agregation;analyse donnee;preference;relation floue;toma decision;relacion difusa;owa operator;fuzzy decision;problem solving;resolution probleme	Yager @IEEE Trans Syst Man Cybern B 2004;34:1952–1963# introduced a continuous interval argument OWA ~C-OWA! operator, which extends the ordered weighted averaging ~OWA! operator, introduced by Yager @IEEE Trans Syst Man Cybern B 1988;18:183–190#, to the case in which the given argument is a continuous valued interval rather than a finite set of values. In this article, we utilize the C-OWA operator to derive the priority vector of an interval fuzzy preference relation and then develop a practical approach to solving the decision-making problem with interval fuzzy preference relation. Finally, a numerical example is provided to demonstrate the practicability and efficiency of the developed approach. © 2006 Wiley Periodicals, Inc.	john d. wiley;numerical analysis;ordered weighted averaging aggregation operator	Zeshui Xu	2006	Int. J. Intell. Syst.	10.1002/int.20184	decision support system;computer science;artificial intelligence;mathematics;data analysis;algorithm	DB	-1.618277589591966	-20.303437402905352	93026
ce761b9eabed866a4f09be0cd16325f943c78b76	a dynamic prizing scheme for a congestion charging zone based on a network fundamental diagram		The continuously rising amount of traffic is becoming more and more of a problem that cities have to deal with it, as it causes congestion, noise and air pollution, among other side effects. As a solution approach for these diverse problems, many cities have already decided to install a toll, which is often designed as a congestion-pricing area. With that, they aim to reduce, if not completely prohibit, traffic in the inner cities, and to provide a source of additional income for the city budget. In order to get the maximum traffic benefits out of such a tolling area and to avoid traffic jams within the tolled area, it is necessary to control the system with an efficient algorithm. However, at the moment there are no dynamic, traffic responsive city tolling systems. The major reason is that such controlling mechanisms, especially if they have to represent a whole network, would require a lot of data. This data is not only difficult to collect and manage, particularly if most of the roads inside the network have to be measured, it also makes it hard to design and calibrate the control algorithm. With the help of a network fundamental diagram (NFD), representing the traffic state of the whole network, this data collection and management problem can be solved. This motivates the idea of using an NFD as a basis for controlling the pricing scheme of a city tolling system. To examine the feasibility of this approach, a microscopically simulated test network was created and a NFD for this network was developed. Using this NFD, a desired occupancy value for the whole network is set, and the toll height is calculated from different control functions of the occupancy inside the tolled network compared to the desired occupancy value. This toll height is subsequently included into the route choice algorithm of the microsimulation, shifting the traffic onto the non-tolled roads. In order to assess the different control approaches, evaluation criteria including a comparison of the resulting network occupancy with the desired one, the stability of the price, and the reaction speed are set. Several approaches to determine the prizing based on the fundamental diagram are developed and tested in the simulated network. The development of the traffic flow and network occupation over time inside the tolled network is compared between the various tolling functions in order to identify the best performing one.	algorithm;best practice;common criteria;control function (econometrics);diagram;netware file system;network congestion;offset binary;performance tuning;quantum fluctuation;simulation;subnetwork	Benedikt Bracher;Klaus Bogenberger	2017	2017 5th IEEE International Conference on Models and Technologies for Intelligent Transportation Systems (MT-ITS)	10.1109/MTITS.2017.8005597	toll;traffic generation model;microsimulation;real-time computing;data collection;occupancy;floating car data;traffic flow;network traffic control;transport engineering;engineering	Robotics	8.22344929797731	-10.773945667046224	93126
1589dea429f2a8340d905816e10001388114cdd4	structural analysis of audit evidence using belief functions	belief;belief function;9b50;analisis estructural;belief functions;analisis decision;risque;prise decision;raisonnement;raisonnement a base evidence;decision analysis;evidential reasoning;riesgo;modelo;croyance;fonction croyance;risk;dempster shafer theory;razonamiento;completitud;modele;completeness;reasoning;analyse structurale;creencia;toma decision;structural analysis;completude;models;analyse decision;audit;theorie dempster shafer;structure analysis	"""This article performs two types of analysis using Dempster-Shafer theory of belief functions for evidential reasoning. The first analysis deals with the impact of the structure of audit evidence on the overall belief at each variable in the network, variables being the account balance to be audited, the related transaction streams, and the associated audit objectives. The second analysis deals with the impact of the relationship (logical """"and"""" and """"algebraic relationship"""") among various variables in the network on the overall belief. For our first analysis, we change the evidential structure from a network to a tree and determine its impact."""	analysis of algorithms;computation;computer program;linear algebra;structural analysis;transaction processing;tree (data structure);tree accumulation;tree structure	Rajendra P. Srivastava;Hai Lu	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00259-7	decision analysis;artificial intelligence;data mining;mathematics;structural analysis;evidential reasoning approach;algorithm;statistics	AI	-0.09964543434871759	-14.874019646107364	93233
a2a95b6ad0ef59756bd21111471efe8d17a8ab8d	nonlinear neural network forecasting model for stock index option price: hybrid gjr-garch approach	garch;option pricing;forecasting model;artificial neural networks;grey forecasting model;indexation;option pricing model;artificial neural network;neural network	This study integrated new hybrid asymmetric volatility approach into artificial neural networks option-pricing model to improve forecasting ability of derivative securities price. Owing to combines the new hybrid asymmetric volatility method can be reduced the stochastic and nonlinearity of the error term sequence and captured the asymmetric volatility simultaneously. Hence, in the ANNS option-pricing model, the results demonstrate that Grey-GJR–GARCH volatility provides higher predictability than other volatility approaches. 2007 Elsevier Ltd. All rights reserved.	artificial neural network;closing (morphology);econometric model;nonlinear system;stochastic process;volatility	Yi-Hsien Wang	2009	Expert Syst. Appl.	10.1016/j.eswa.2007.09.056	autoregressive conditional heteroskedasticity;actuarial science;computer science;machine learning;valuation of options;black–scholes model;artificial neural network	AI	6.4124158754927	-17.80665619350104	93306
525dbad3d115f2e657b6ffd3a76437073e080934	a new linguistic scale for interval type-2 trapezoidal fuzzy number based multiple criteria decision making method	pragmatics;uncertainty;topsis multiple criteria decision making interval type 2 fuzzy set ambiguity type reduction method;fuzzy sets;informatics;pragmatics decision making context fuzzy sets informatics uncertainty;context	Decision making is a process for managing the decision problem for human beings that use linguistic information. However, it is sometimes limited by the fact that the linguistic models use only positive linguistic terms, which may not reflect exactly what the experts mean. The previous studies neglected the equilibrium concept (i.e., two sides of a matter) that takes its roots from the Yin Yang theory. The Yin Yang theory philosophically deals with two sides of things in the universe, and focuses on the balance of the two sides. Thus, the purpose of this paper is to introduce the new linguistic scales of positive and negative Interval Type-2 Trapezoidal Fuzzy Number (IT2TrFN) to the decision environment of interval type-2 fuzzy context for solving Interval Type-2 Fuzzy Technique for Order Preference by Similarity to Ideal Solution (IT2FTOPSIS) problems. This new linguistic scales reacts to the subjective judgments from the experts where the lowest of the scale and the highest of the scale are equally strong. In decision making, it is rare to find the negative scale, where it actually does not mean wrong or corrupt. Here, the negative data represents a hypothesis that can make it well-separated. The positive and negative are relatives. Along with considering the context of the new linguistic scale, this paper employs a hybrid averaging approach with ambiguity method and type-reduction method to formulate a collective decision environment. This hybrid averaging approach helps to reduce values of Interval Type-2 Fuzzy Sets (IT2FS) to a crisp number. The feasibility and applicability of the proposed methods are illustrated with an example.	decision problem;fuzzy number;fuzzy set;type-2 fuzzy sets and systems;yang	Nurnadiah Zamri;Syibrah Naim;Lazim Abdullah	2015	2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2015.7337870	uncertainty;defuzzification;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set;informatics;fuzzy set operations;weighted sum model;statistics;pragmatics	Robotics	-3.6671331597635812	-21.534294277230188	93392
118d636731bc100eeeb7f81dca59919895cd01aa	an interactive procedure for multiple attribute group decision making with incomplete information: range-based approach	theoretical model;informacion incompleta;analisis decision;prise decision;decision maker;utility range;decision analysis;incomplete information;multiple attribute group decision making;information incomplete;linear program;incomplete preferences;group decision making;toma decision;analyse decision	This paper presents an interactive procedure for solving a multiple attribute group decision making (MAGDM) problem with incomplete information. The main properties of the procedure are: (1) Each decision maker is asked to express his/her preference in relation to an additive value model with incomplete preference statements. (2) A rangetyped representation method for utility is used. The range-typed utility representation makes it easy to compare each group member's utility information with a group's one and to aggregate each group member's utility information into a group's one. Utility range is calculated from each group member's incomplete information. (3) An interactive procedure is provided to help the group reach a consensus. It helps each group member to modify or complete his/her utility with ease compared to group's utility range. (4) We formally describe theoretic models for establishing group's pairwise dominance relations with group's utility range by using a separable linear programming technique. Ó 1999 Elsevier Science B.V. All rights reserved.	aggregate data;consensus (computer science);decision support system;interactivity;linear programming;preference elicitation;theory;utility functions on indivisible goods	Soung Hie Kim;Sang Hyun Choi;Jae Kyeong Kim	1999	European Journal of Operational Research	10.1016/S0377-2217(98)00309-9	decision-making;mathematical optimization;group decision-making;optimal decision;economics;decision analysis;linear programming;artificial intelligence;decision tree;data mining;mathematics;evidential reasoning approach;welfare economics;complete information;weighted sum model	AI	-3.0824365535545684	-19.119904428838137	93629
2b0038b16acc5d2764f7104b2f77e19dcc0115dd	induced simplified neutrosophic correlated aggregation operators for multi-criteria group decision-making		AbstractInduced Choquet integral is a powerful tool to deal with imprecise or uncertain nature. This study proposes a combination process of the induced Choquet integral and neutrosophic information. We first give the operational properties of simplified neutrosophic numbers (SNNs). Then, we develop some new information aggregation operators, including an induced simplified neutrosophic correlated averaging (I-SNCA) operator and an induced simplified neutrosophic correlated geometric (I-SNCG) operator. These operators not only consider the importance of elements or their ordered positions, but also take into account the interactions phenomena among decision criteria or their ordered positions under multiple decision-makers. Moreover, we present a detailed analysis of I-SNCA and I-SNCG operators, including the properties of idempotency, commutativity and monotonicity, and study the relationships among the proposed operators and existing simplified neutrosophic aggregation operators. In order to handle the ...		Ridvan Sahin;Hong-yu Zhang	2018	J. Exp. Theor. Artif. Intell.	10.1080/0952813X.2018.1430857	computer science;artificial intelligence;operator (computer programming);idempotence;discrete mathematics;machine learning;multiple-criteria decision analysis;monotonic function;group decision-making;commutative property;choquet integral	AI	-2.772757589123678	-21.060180397255742	93716
7771626d389d1dd0c2dca442b1267a2857fb5424	an application of multiobjective programming to the study of workers' satisfaction in the spanish labour market	modelizacion;multiobjective programming;programmation multiobjectif;analisis estadistico;analisis datos;multiobjective programming workers satisfaction reference point goal programming econometric analysis;econometric analysis;mercado trabajo;econometria;comunidad europea;workers satisfaction;satisfiability;reference point;modelisation;european community;data analysis;statistical analysis;marche travail;analyse statistique;labour market;programmation objectif;household;menage;goal programming;analyse donnee;econometrics;communaute europeenne;programacion objetivo;familia;modeling;european community household panel;econometrie;econometric analyses;programacion multiobjetivo	In this paper, a multiobjective scheme is used to study the satisfaction levels of the Spanish workers. Data obtained from a panel survey conducted in several European countries are used to build up a multiobjective model, on the basis of a previous statistical and econometric analysis of these data. Then, a Reference Point based method is implemented to determine the profile of the most satisfied worker in Spain nowadays. Finally, a combined Goal Programming – Reference Point approach is used to determine policies than can be carried out in order to increase the workers’ satisfaction levels.		Óscar D. Marcenaro-Gutierrez;Mariano Luque;Francisco Ruiz	2010	European Journal of Operational Research	10.1016/j.ejor.2009.07.017	simulation;systems modeling;operations management;goal programming;mathematics;data analysis;operations research;statistics;satisfiability	HCI	-0.03140452996594667	-15.096960315602173	93942
6d1a93a3cf0c6c1ed2bb51631d5221f3d48c29d4	topological characterizations of generalized fuzzy rough sets	fuzzy relation;fuzzy topology;fuzzy implicator;generalized fuzzy rough set	In this paper, topological characterizations of generalized fuzzy rough sets are investigated in the context of basic rough equalities. Various fuzzy topologies induced by different fuzzy relations are studied with respect to the lower fuzzy rough approximation operator determined by a fuzzy implicator, which is left-continuous in the first argument and right-continuous in the second argument. The relationships among fuzzy topologies induced by fuzzy relations are discussed. The algebraic structures of T -similarity set of fuzzy relations are investigated with respect to different fuzzy implicators, where T is a left-continuous t-norm. Moreover, the T -transitive subset of T -similarity set is proven to be a complete distributive lattice. © 2016 Elsevier B.V. All rights reserved.	approximation;fuzzy logic;fuzzy set;linear algebra;rough set;t-norm	Chun Yong Wang	2017	Fuzzy Sets and Systems	10.1016/j.fss.2016.02.005	fuzzy logic;t-norm fuzzy logics;combinatorics;discrete mathematics;topology;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	AI	-0.9428739535016336	-23.069527477469606	93967
02b7de7824df7c8b1bbe6482d1e92737cb5dead7	a method for predicting the network security situation based on hidden brb model and revised cma-es algorithm	belief rule base brb;covariance matrix adaption evolution strategy cma es;network security situation prediction;hidden behavior;modified operator	It is important to establish the forecasting model of the network security situation. But the network security situation cannot be observed directly and can only be measured by other observable data. In this paper the network security situation is considered as a hidden behavior. In order to predict the hidden behavior, some methods have been proposed. However, these methods cannot use the hybrid information that includes qualitative knowledge and quantitative data. As such, a forecasting model of network security situation is proposed on the basis of the hidden belief rule base (BRB) model when the inputs are multidimensional. The initial parameters of the hidden BRB model given by experts may be subjective and inaccurate. In order to train the parameters, a revised covariance matrix adaption evolution elief rule base (BRB) ovariance matrix adaption evolution trategy (CMA-ES) odified operator strategy (CMA-ES) algorithm is further developed by adding a modified operator. The revised CMA-ES algorithm can optimize the parameters of the hidden BRB model effectively. The case study shows that compared with other methods, the proposed hidden BRB model and the revised CMA-ES algorithm can predict the network security situation effectively to improve the forecasting precision by making full use of qualitative knowledge. © 2016 Elsevier B.V. All rights reserved. 41 42 43 44 45 46 47 48 49 50 51 52 53 . Introduction Network security situation can reflect the network status [1], nd it is the important information of the active defense in the etwork. In order to determine the network status and make the ccurate decision, it is necessary to predict the network security ituation. There are two characteristics when the network security ituation is predicted. Firstly, the network security situation canot be observed directly and only be measured by other observable ata such as attack type and attack strength. In other words, the etwork security situation can be considered a hidden behavior. econdly, the available information includes qualitative knowledge nd quantitative data when the network security situation is prePlease cite this article in press as: G.-Y. Hu, et al., A method for predic and revised CMA-ES algorithm, Appl. Soft Comput. J. (2016), http://dx icted. In order to establish the forecasting model of the network ecurity situation, three types of methods that include analytical ∗ Corresponding authors. E-mail addresses: zhouzhijie1978@163.com, zhouzj04@mails.tsinghua.edu.cn Z.-J. Zhou), zhangbangcheng@mail.ccut.edu.cn (B.-C. Zhang). ttp://dx.doi.org/10.1016/j.asoc.2016.05.046 568-4946/© 2016 Elsevier B.V. All rights reserved. 54 55 56 57 model-based method, data-driven based method, and qualitative knowledge based method have been proposed. In the analytical model-based method, some predictors are firstly developed from the corresponding filters that include Kalman filter [2], strong tracking filter [3], particle filter [4] and so on. Then these predictors and the observable data are used to predict the hidden behavior. In the data-driven based method, the observable data and the corresponding methods are used directly to establish the forecasting model the characteristic value that can reflect the hidden behavior. The data-driven method includes hidden Markov model (HMM) based method [5–8], grey theory based method [9], dynamic Bayesian network (DBN) based method [10,11], and Wavelet neural network (WNN) based method [12]. The qualitative knowledge based method that includes expert system based model [13], Petri net based model [14] and so on can be adopted to establish the forecasting model of the hidden behavior. However, the limitations are existed in the above methods. The ting the network security situation based on hidden BRB model .doi.org/10.1016/j.asoc.2016.05.046 analytical model-based method is not suitable when the analytical model of a complex system cannot be established. The qualitative knowledge based method mainly uses the qualitative knowledge and the forecasting results may be inaccurate. In the HMM based 58 59 60 61 ARTICLE IN G Model ASOC 3626 1–15 2 G.-Y. Hu et al. / Applied Soft Com Nomenclature Acronym BRB Belief rule base CMA-ES Covariance matrix adaption evolution strategy HMM Hidden markov model DBN Dynamic Bayesian network WNN Wavelet neural network MSE Mean square errors SQP Sequential quadratic programming PSO Particle swarm optimization EvHMM Evidential hidden markov model Notation x (t) Hidden behavior at time instant t t Time instant Rk kth rule in BRB Dk Consequent of the kth rule G Good security situation level O Common security situation level W Warning security situation level T Terrible security situation level ˇj,k Belief degree of Dj ˇD,k Remaining belief degree k Rule weight of the kth rule ı Weight of the attribute wk Activation weight of the kth rule ak Matching degree of the input in the kth rule y (t) Observable data in time instant t f Observation equation Parameters of the observation equation (t) Noise vector of the observation equation A Parameter of the observation equation B Parameter of the observation equation Parameter of the observation equation H Likelihood function Parameter vector of the BRB model l Number of parameters in  ̋ T Maximum time instant U (D) Utility of the evaluated degrade q qth solution of CMA-ES Population size of CMA-ES g Generation of CMA-ES m Mean value of population of CMA-ES S Step size of CMA-ES N Normal distribution C Covariance matrix of CMA-ES Ex Excess value in modified operator of CMA-ES Offspring population size of CMA-ES h Weight coefficient of CMA-ES i: ith individual in the individuals of CMA-ES E Orthogonal matrix I Identity matrix F2 Diagonal matrix c1, c2 Learning rate for updating the covariance matrix eff Variance effective selection mass of CMA-ES pc Evolution path of CMA-ES cc Backward time horizon of the evolution path d damping parameter of the evolution path c backward time horizon of the conjugate evolution path 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118	alsa;algorithm;artificial neural network;attack model;cma-es;coefficient;complex system;dynamic bayesian network;evolution strategy;expert system;hidden markov model;holographic principle;internet slang;kalman filter;knowledge-based systems;markov chain;mathematical optimization;network security;observable;offset binary;particle filter;particle swarm optimization;petri net;rule-based system;sequential quadratic programming;serial ata;soft computing;wavelet	Guan-Yu Hu;Zhi-Jie Zhou;Bangcheng Zhang;Xiaojing Yin;Zhi Gao;Zhiguo Zhou	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.05.046	computer science;artificial intelligence;machine learning;data mining	AI	7.017427837206403	-16.38988259776065	94147
0f13ea8fccd4234a485a4cd25bde7a1e5fc66598	incorporating feature selection method into neural network techniques in sales forecasting of computer products	sales forecasting;data collection;support vector regression;forecasting model;data mining;artificial neural networks;computer product;feature selection;product variety;prediction model;multivariate adaptive regression splines;neural network model;cerebellar model articulation controller;artificial neural network;neural network	Sales forecasting of computer products is regarded as an important but difficult task since computer products are characterized by product variety, rapid specification changes and rapid price declines. Artificial neural networks (ANNs) have been found to be useful techniques for sales forecasting. However the inability to identify important forecasting variables is one of the main shortcomings of ANNs. For selecting an appropriate number of forecasting variables which can best improve the performance of the neural network prediction model, a commonly discussed data mining technique, multivariate adaptive regression and splines (MARS), is adapted in this study. The proposed model, firstly, uses the MARS to select important forecasting variables. The obtained significant variables are then served as the inputs for two neural network models-support vector regression (SVR) and cerebellar model articulation controller neural network (CMACNN). A real sales data collected from a Taiwanese computer dealer is used as an illustrative example. Experimental results showed that the obtained important variables from MARS can improve the forecasting performance of the SVR and CMACNN models. The proposed two-stage forecasting models provide good alternatives for sales forecasting of computer products.	artificial neural network;feature selection	Chi-Jie Lu;Jui-Yu Wu;Tian-Shyug Lee;Chia-Mei Lian	2011		10.1007/978-3-642-21111-9_27	support vector machine;multivariate adaptive regression splines;computer science;artificial intelligence;machine learning;data mining;predictive modelling;artificial neural network;data collection	ML	6.511364446743908	-19.86945880847432	94149
9dcccc01c8a05cc51b9d3db6595882dedd390824	optimal partial questioning in large dimensional ahp	ahp;pairwise comparisons;consistency	In this paper we propose a flexible method for optimally choosing and sequencing in time a subset of pairwise comparisons between the alternatives in large--dimensional AHP problems. Two criteria are taken into account in defining the choice rule: the fair involvement of all the alternatives in the pairwise comparisons and the consistency of the elicited judgements. The combination of the two criteria guarantees the best reliability of the already collected information. The method indicates at each step the two alternatives to be next compared and stops the process taking into account both the reliability of the already elicited judgements and the necessity of bounding the potentially large number of judgements to be submitted to the decision maker.		Michele Fedrizzi;Silvio Giove	2008		10.3233/978-1-58603-984-4-185	pairwise comparison;analytic hierarchy process;computer science;potentially all pairwise rankings of all possible alternatives;data mining;mathematics;consistency;statistics	Vision	-4.225516705688375	-19.332346258700007	94235
63ae65a408cdbc99d37b2af629b1df24ce5010a4	fuzzy random events in incomplete probability models	fuzzy set;rough set theory;fuzzy set theory;random variable;probability model;rough set;probability space	Fuzzy set theory has been well developed and applied in a wide variety of real problems. In some probabilistic problems, there does not exist complete information about the probability model. In this paper, using fuzzy rough set theory, we obtain a lower and upper probability for an arbitrary fuzzy random event and then we introduce a measure for inclusivity of fuzzy events.		H. Torabi;Bijan Davvaz;Javad Behboodian	2006	Journal of Intelligent and Fuzzy Systems		fuzzy logic;random variable;combinatorics;probability mass function;discrete mathematics;rough set;membership function;random element;probability measure;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;regular conditional probability;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;algebra of random variables;fuzzy set operations;statistics	Robotics	0.30404760659797286	-20.44105560168544	94292
dc41357030dc9038c7f9be1e761540fe92057180	most productive scale size decomposition for multi-stage systems in data envelopment analysis		Abstract The most productive scale size (MPSS) of decision systems has been measured for the whole system by using conventional data envelopment analysis (DEA) methodology. This paper investigates the MPSS measurements for systems consisting of multiple stages connected in series by taking into account the interrelationship of the stages within the system. New models are proposed for determining the MPSS of the system and of the individual stages. Mathematical analysis proves that the MPSS of the system can be decomposed as the sum of the MPSS values of the individual stages. As a result, the system is overall MPSS if and only if it is MPSS in each stage. With MPSS decomposition, the decision maker can identify the non-MPSS stages and make subsequent improvements. For these improvements, an approach to project the non-MPSS system onto the MPSS region is proposed. Numerical examples are provided to show the applicability of the proposed methods in both estimating MPSS and deriving MPSS projections.	data envelopment analysis	Saeed Assani;Jianlin Jiang;Rong-Mei Cao;Feng Yang	2018	Computers & Industrial Engineering	10.1016/j.cie.2018.04.043	engineering;mathematical optimization;data envelopment analysis	SE	-2.211887797778214	-14.497614702393967	94449
9bf11ced392d0aa695770c5e080a596de0247e0c	convex combinations of fuzzy logical operations	r implication;q implication;s implication;triangular norm;triangular conorm;fuzzy implication;convex combination;fuzzy negation	One may ask when (non-trivial) convex combinations of fuzzy logical operations result in operations of the same type. This question has been intensively studied for triangular norms and it still remains open for continuous ones. An equivalent problem is obtained for triangular conorms. We show equivalence with the analogous problem for S-implications. Negative answers are obtained for strong fuzzy negations and for R-implications corresponding to continuous triangular norms. The situation for Q-implications and related problems are discussed.	logical connective	Mirko Navara	2015	Fuzzy Sets and Systems	10.1016/j.fss.2014.10.013	mathematical optimization;combinatorics;discrete mathematics;convex combination;artificial intelligence;mathematics;fuzzy set operations	NLP	-0.5598526620068139	-22.636723591738	94565
7757cf520811d723a90f6ce1b0b7aa129f63dc2d	ordinal measurement in decision aid	learning process;fuzzy classification;majority rule;three dimensions;decision aid;qualitative data;rule based;decision problem;machine learning;empirical validation;point of view;subject areas;evaluation model	"""In the rst part we de ne the problem of ordinal measurement aggregation (for instance: a three dimension object which is \high"""", \large"""" and \short"""" is a \big"""" object?). We also introduce the problem of hierarchical aggregation when the dimensions under which a set of objects has to be analyzed form a hierarchy. From a formal point of view the problem consists in de ning a comprehensive ordinal measurement of each object taking in account a subset of evaluation dimensions. We demonstrate that a meaningful ordinal measure can be obtained using aggregation procedures based on majority rules, voting schemes and the concordance/discordance principle. Such procedures are then compared to methods based on valued similarity, fuzzy classi cation and rule based classi cation."""	concordance (publishing);evaluation function;level of measurement;object composition;ordinal data	Alexis Tsoukiàs;Vincent Mousseau	1999	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(04)00045-9	rule-based system;three-dimensional space;majority rule;mathematical optimization;qualitative property;fuzzy classification;artificial intelligence;machine learning;decision problem;data mining;mathematics;algorithm	ML	-3.0696031731919695	-23.322730588550122	94609
acd7e0ee709360a51ef2ecb9c001d81ee0acffdb	dual bipolar measures of atanassov's intuitionistic fuzzy sets	dual bipolar;measure atanassov s intuitionistic fuzzy set aifs determinacy dual bipolar hesitancy;correlation frequency selective surfaces accuracy fuzzy sets area measurement uncertainty information management;measure;hesitancy;fuzzy set theory decision making;determinacy;atanassov s intuitionistic fuzzy set aifs;dual bipolar scales aifs dual bipolar measures atanassov intuitionistic fuzzy sets superiority noninferiority determinacy nonhesitancy	Measures of Atanassov's intuitionistic fuzzy sets (AIFSs), such as subsethood, cardinality, distance, similarity, correlation, and evaluation functions, are often used in application problems. This paper investigates such measures from various perspectives. First, based on the relative relations of an AIFS to other AIFSs, four functions, namely superiority, noninferiority, determinacy, and nonhesitancy, are constructed, which consist of dual bipolar scales, namely (superiority, noninferiority) and (determinacy, nonhesitancy). Then, the proposed measures of AIFSs are axiomatically defined using the dual bipolar scales. Geometrical demonstrations, in general, show consistency between the proposed measures and existing ones. However, unlike existing measures, the proposed measures can highlight the significant features of AIFSs. In addition, the attitude of decision makers is also implanted in measures to allocate the importance in the dual bipolar scales. Two numerical examples are used to demonstrate the applicability and distinctiveness of the proposed measures.	evaluation function;fuzzy set;indeterminacy in concurrent computation;intuitionistic logic;numerical analysis	Liang-Hsuan Chen;Chien-Cheng Tu	2014	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2013.2278991	mathematical analysis;discrete mathematics;measure;mathematics;determinacy	Vision	-2.6115940086698317	-21.256921392553323	94614
bab30d1d97f53e9297cdb8e082d2abcf82b5482b	developing a bioaerosol detector using hybrid genetic fuzzy systems	decision tree;data interpretation;multiobjective evolutionary algorithms moeas;rule based;fuzzy classifiers fcs;multiobjective evolutionary algorithm;model building;true positive;transparency;bioaerosol detector;false positive;genetic fuzzy system;genetic fuzzy systems gfs;fuzzy classifier	The aim of this work is to develop a model, which works as a reasoning mechanism in a bioaerosol detector. Ability to distinguish between safe and harmful aerosols is one of its main requirements. Instead of commonly used misclassification rate as a metric of accuracy, true positive (TP) and false positive (FP) rates are used because of the uneven misclassification costs and class distributions of the collected data. Interpretability of the model builds up the confidence for the developed model and enables its adjustment in cases when bioaerosol detector is further developed. Thus, it is another crucial requirement for the model. Clearly, the objectives are contradicting and therefore multiobjective evolutionary algorithms (MOEAs) are applied to find tradeoff models. Fuzzy classifiers (FCs) are selected as a model type because their linguistic rules are intuitive to human beings. FCs are identified by hybrid genetic fuzzy system (GFS) which initializes the population adequately using decision trees (DTs) and simplification operations. During MOEA optimization transparency of fuzzy partition is used as a metric of interpretability and TP and FP rates as metrics of accuracy. Heuristic rule and rule condition removal is applied to offspring population in order to keep the rule base consistent. The identified FCs are highly comprehensible yet accurate and their linguistic rules provide valuable insights for further development of bioaerosol detector. r 2008 Elsevier Ltd. All rights reserved.	applications of artificial intelligence;decision tree;evolutionary algorithm;fuzzy control system;fuzzy logic;fuzzy set;genetic fuzzy systems;heuristic;level of detail;moea framework;mathematical optimization;phase detector;population;requirement;rule 184;rule-based system;run time (program lifecycle phase);semiconductor detector	Pietari Pulkkinen;Jarmo Hytönen;Hannu Koivisto	2008	Eng. Appl. of AI	10.1016/j.engappai.2008.01.006	rule-based system;mathematical optimization;model building;type i and type ii errors;computer science;artificial intelligence;machine learning;decision tree;data mining;transparency;data analysis;statistics	AI	3.5085989015925083	-20.351709681293823	94859
d28ce38f7ee457ce5b846c002a83e0d5b2f671ca	customer's relationship segmentation driving the predictive modeling for bad debt events	customer behavior;predictive modeling;data mining;bad debt events;link analysis;artificial neural networks;model building;clustering;nearest neighbor;pattern recognition;prediction model;neural network model;cluster model;customers behavior;artificial neural network;telecommunications;neural network;knowledge discovery	This paper covers a comparison between two distinct approaches to neural network modeling. The first one is based on a developing of a single neural network model to predict bad debt events. The second one is based on combined models, building firstly a clustering model to recognize the pattern assigned to the customers, with a particular focus on the insolvency, and then developing several distinct neural networks to predict bad debt. In the second approach, for each group identified by the clustering model one neural network had been constructed. In that way, we turned the quite heterogeneous customer base more homogeneous, increasing the average accuracy for the predictive modeling once several straightforward models were built.		Carlos André Reis Pinheiro;Markus Helfert	2009		10.1007/978-3-642-02247-0_30	engineering;artificial intelligence;machine learning;data mining	Robotics	5.4073001052350325	-19.996472015892813	94878
69132da270a9691df06e4c2f4f40d67f2ff9f6a3	red: a new method for performance ranking of large decision making units	large decision making units;prediction methods;input oriented model;data envelopment analysis;dmus ranking;ratio efficiency dominance	Data envelopment analysis (DEA) method has been widely used in many economic and industrial applications to measure efficiency and rank performances of decision making units (DMUs). Improving the accuracy and computation time in measuring the efficiency of DMUs have been two main challenges for the DEA. Specifically, with large DMUs, the DEA-based methods are argued to require large amount of memory space and CPU time to measure DMUs efficiencies, and suffer from inability to obtain complete performance ranking. To address these issues, in this paper, a new alternative method that is based on input oriented model (IOM) and efficiency ratio (ER), called ratio efficiency dominance (RED), is proposed. The proposed method seeks to minimize the inputs while maximizing the outputs to obtain efficiency or performance scores, which is independent of DEA method and the use of linear programming (LP). It is also to overcome the drawbacks of uncontrolled convergence, non-generalization and instability induced from integrating prediction techniques such as neural networks (NNs) with DEA. To evaluate the proposed method, experiments were performed on small, large and very large DMUs data sets to show the effectiveness of proposed method. The experimental results demonstrated that, in all cases, the proposed method is able to produce a complete and more accurate ranking compared to the conventional DEA methods or its hybrids.		Mohammadreza Farahmand;Mohammad Ishak Desa	2017	Soft Comput.	10.1007/s00500-015-1860-9	econometrics;computer science;data mining;data envelopment analysis	AI	-2.368182755936538	-16.102000661307105	94984
1de424ce7c8ff4a55160921997d15b597cb7b5e1	fault detection and other time series opportunities in the petroleum industry	integrated operations;oil and gas industry;soft computing;oil;time series;information flow;machine learning;fault detection;integral operator;intelligent systems;intelligent system;petroleum industry;time series prediction	Data-centric methods like soft computing and machine learning have gained greater interest and acceptance in the oil and gas industry in recent years. We give an overview of the opportunities and challenges facing applied time series prediction in this domain, with a focus on fault prediction. In particular, we argue that the physical processes and hierarchies of information flow in the industry strongly determine the choice of soft computing or machine learning methods.	time series	Roar Nybø	2010	Neurocomputing	10.1016/j.neucom.2009.10.020	integrated operations;simulation;computer science;petroleum industry;machine learning;time series;soft computing;statistics	ML	6.438369248835762	-23.926115692859714	95165
308e0bfb42dbeced7aa7a8c49b61541f43c99b67	on compatibility of uncertain multiplicative linguistic preference relations based on the linguistic cowga	uncertain multiplicative linguistic preference relation;lcowga operator;compatibility;group decision making	The aim of this work is to develop a new compatibility for the uncertain multiplicative linguistic preference relations and utilize it to determine the optimal weights of experts in the group decision making (GDM). First, the compatibility degree and compatibility index for the two multiplicative linguistic preference relations are proposed. Then, based on the linguistic continuous ordered weighted geometric averaging (LCOWGA) operator, some concepts of the compatibility degree and compatibility index for the two uncertain multiplicative linguistic preference relations are presented. We prove the property that the synthetic uncertain linguistic preference relation is of acceptable compatibility under the condition that the uncertain multiplicative linguistic preference relations given by experts are all of acceptable compatibility with the ideal uncertain multiplicative linguistic preference relation, which provides a theoretic basis for the application of the uncertain multiplicative linguistic preference relations in GDM. Next, an optimal model is constructed to determine the weights of experts based on the criterion of minimizing the compatibility index in GDM. Moreover, an approach to GDM with uncertain multiplicative linguistic preference relations is developed, and finally, an application of the approach to supplier selection problem with uncertain multiplicative linguistic preference relations is pointed out.	preference learning;selection algorithm;synthetic intelligence;theory	Ligang Zhou;Yingdong He;Huayou Chen;Jinpei Liu	2013	Applied Intelligence	10.1007/s10489-013-0454-4	group decision-making;compatibility	AI	-2.8653978347885265	-20.64379753608063	95327
07c8b4a5119def1107b9e25fd2a8d2682e32b68d	application of multivariable time series based on rbf neural network in prediction of landslide displacement	rbf neural network prediction of landslide displacement chaos phase space reconstruction multivariate time series;stress;forecasting;comparative analysis;terrain factors;chaos;groundwater factor;deformation characteristic multivariable chaotic time series model rbf neural network landslide displacement prediction groundwater factor remedial measure dynamic system physical mechanics;chaotic time series;dynamic system;prediction of landslide displacement;multivariable chaotic time series model;time series;genetics;physical mechanics;radial basis function networks;phase space reconstruction;multivariate time series;artificial neural networks;erosion;geophysics computing;geology;monitoring;time series analysis;time series erosion geology geophysics computing groundwater radial basis function networks;deformation characteristic;rbf neural network;predictive models;groundwater;landslide displacement prediction;remedial measure;neural networks terrain factors deformable models predictive models genetics displacement control displacement measurement time measurement chaos history;forecast accuracy;time series model	Landslide is a kind of genetic type of slope and has the same characteristics with slope. The major external motivation factor of landslide displacement is groundwater and it is under the control of remedial measures at the same time after its remediation. Chaotic time series of landslide displacement and its influential factors could reflect the history of landslide displacement of dynamic system, the displacement could be predicted by reconstructing landslide displacement of dynamic system according to the observation of multivariate time series and adopting RBF neural network to reflect relationship between variables. Comparative analysis of the results from the forecast show that: multivariable time series model can predict landslide displacement effectively, and the forecast accuracy is higher than the accuracy of a single variable time series model; multivariable time series model is of clearer sense of the physical mechanics and reflects the real evolution of deformation characteristics more effective.	artificial neural network;displacement mapping;dynamical system;radial basis function;time series	Yao Zeng;Echuan Yan;Chunfeng Li;Ying Li	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.163	time series;artificial neural network;statistics	Visualization	9.900804220011635	-19.97500600062502	95405
aa180c3aef166cd8818ebaeb66813893ea090e8c	a procedure for the generation of interval type-2 membership functions from data	extended π interval type 2 membership function;hybrid metaheuristic;interval type 2 fuzzy set;extended π membership function;parametric optimization;metaheuristic	This paper proposes a new interval type-2 fuzzy set taking extended π interval type-2 membership function (IT2 MF) as its values, and presents a new procedure for generating a set of extended π IT2 MFs from data for an interval type-2 linguistic variable. An extended π IT2 MF is defined as the min and max of two extended π (type-1 or ordinary) membership functions. The procedure has the following steps: (i) for each interval type-2 linguistic variable, specifying the number of membership functions to be generated, i.e. the granularity level, (ii) choosing two fuzzy exponents to be used, (iii) for each fuzzy exponent, applying the fuzzy c-means variant (FCMV) proposed by Liao et al. [1] to obtain the corresponding centers and membership values, and (iv) carrying out parametric optimization by applying a metaheuristic or a hybrid metaheuristic algorithm to determine the optimal parameters associated with the extended π IT2 MFs so that the mean squared error (MSE) or sum of squared errors (SSE) between the membership values obtained by FCMV and those predicted by the extended π IT2 MFs is minimized. The proposed procedure was illustrated with an example and further tested with iris data and weld data. The effects of using two different interval distance measures and the cluster means obtained by the FCMV as part of the initial solutions in the differential evolution metaheuristic were also investigated and discussed.	membership function (mathematics)	T. Warren Liao	2017	Appl. Soft Comput.	10.1016/j.asoc.2016.09.034	mathematical optimization;discrete mathematics;membership function;computer science;mathematics;algorithm;metaheuristic	Logic	-1.2804527478580812	-20.969940609899332	95564
b2b492a6a7ab8c7b2da24c13cb0eb420a26b94cf	a generalized model between the owa operator, the weighted average and the probability	weighted averaging;generalized probabilistic weighted average;generalized probabilistic;owa operator;main advantage;generalized model;generalized probabilistic owa operator;quasi-powawa operator;generalized owawa;weighted average;generalized mean;probabilities;decision support system;arithmetic mean;decision support systems	We introduce a new model that unifies the probability, the weighted average and the OWA operator in a general framework based on the use of generalized means. We present the generalized probabilistic ordered weighted averaging weighted averaging (GPOWAWA) operator. The main advantage of this model is that it unifies these three concepts considering the degree of importance that each one has in the aggregation. We study some of its main properties and particular cases such as the POWAWA, the quadratic POWAWA, the generalized probabilistic weighted average, the generalized OWAWA and generalized probabilistic OWA operator. We end the paper presenting a further generalization by using quasi-arithmetic means obtaining the Quasi-POWAWA operator.		José M. Merigó	2010			ordered weighted averaging aggregation operator;mathematical optimization;decision support system;arithmetic mean;weighted arithmetic mean;weighted product model;computer science;probability;statistics	Vision	-1.3583536363949982	-21.17450536322172	95648
ff327130c69225196d5bd4668f7742e7c7b48add	scalable forecasting techniques applied to big electricity time series		This paper presents different scalable methods to predict time series of very long length such as time series with a high sampling frequency. The Apache Spark framework for distributed computing is proposed in order to achieve the scalability of the methods. Namely, the existing MLlib machine learning library from Spark has been used. Since MLlib does not support multivariate regression, the forecasting problem has been split into h forecasting subproblems, where h is the number of future values to predict. Then, representative forecasting methods of different nature have been chosen such as models based on trees, two ensembles techniques (gradient-boosted trees and random forests), and a linear regression as a reference method. Finally, the methodology has been tested on a real-world dataset from the Spanish electricity load data with a ten-minute frequency.	apache spark;big data;distributed computing;general linear model;gradient boosting;machine learning;newton's method;random forest;sampling (signal processing);scalability;time series	Antonio Galicia;José F. Torres;Francisco Martínez-Álvarez;Alicia Troncoso Lora	2017		10.1007/978-3-319-59147-6_15	machine learning;artificial intelligence;random forest;data mining;computer science;scalability;spark (mathematics);big data;linear regression;multivariate statistics	ML	7.916001197683263	-21.166010337902428	95879
968a8a5bea7b46aa198281cd9fc5d5ddf210fea5	a method for big data analysis of the impact of economic and social events on japanese stock prices		In this paper, we propose an approach for isolating the effects of economic and social events on stock prices. Using a newly-proposed Bayesian modeling technique, we decompose the daily time series of stock price data into three components: a trend component, a cyclical component, and an irregular component. We can then analyze the behavior of each estimated component in relation to economic and social events. As an empirical example, we analyze the daily time series for closing values of the Nikkei Stock Average (NSA) from January 4, 2000 to November 28, 2017, and examine relationships between the estimated components of NSA and significant events together with variations in the economic and social.	bayesian network;big data;closing (morphology);time series	Koki Kyo	2018		10.1145/3206157.3206164	data mining;econometrics;computer science;big data;bayesian inference	ML	4.653957135121274	-14.352296593692474	95914
2800970e9769a0dbfa6f68b496aefea7556ce498	on possibilistic mean value, variance, covariance and correlation of fuzzy numbers	fuzzy number	In 2001 we introduced the notions of possibilistic mean value and variance of fuzzy numbers. In this paper we give a short survey of these notations and show some examples of their application from the literature.		Christer Carlsson;Robert Fullér	2009		10.1007/978-3-642-03737-5_2	econometrics;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;machine learning;mathematics;statistics	AI	0.5248542410022503	-20.53269057087356	96029
1e491c01442631e8a4dcbf2cc26beab87948717c	roughness in hemirings based on fuzzy strong h -ideals.				Jianming Zhan;Kuanyun Zhu;Violeta Leoreanu Fotea	2017	Multiple-Valued Logic and Soft Computing		fuzzy logic;mathematical optimization;computer science;surface finish	Logic	1.8688569872584515	-23.63317332802029	96068
2eca874dfb35a3d4ffedde2829d6885b4711c181	cell traffic prediction using joint spatio-temporal information		In future cellular networks, the ability to predict network parameters such as cell load will be a key enabler of several proposed adaptation and resource allocation techniques. In this study, we consider a joint exploitation of spatio-temporal data to improve the prediction accuracy of standard regression methods. We test several such methods from the literature on a publicly available dataset and document the advantages of the proposed approach.	mathematical optimization	Enrico Lovisotto;Enrico Vianello;Davide Cazzaro;Michele Polese;Federico Chiariotti;Daniel Zucchetto;Andrea Zanella;Michele Zorzi	2017	2017 6th International Conference on Modern Circuits and Systems Technologies (MOCAST)	10.1109/MOCAST.2017.7937674	simulation;engineering;data science;data mining	Robotics	8.13563074504384	-23.006460389832796	96252
e6b4a4cbcfb8a0c6923dc425e1110b306dd72fe1	an interval-valued fuzzy-stochastic programming approach and its application to municipal solid waste management	fuzzy set;uncertainty;infinite α cuts;interval;probability distribution;waste management	In this study, an interval-valued fuzzy-stochastic programming (IVFSP) approach is developed for municipal solid waste (MSW) management under uncertainty. IVFSP can tackle multiple uncertainties presented as intervals as well as possibilistic and probabilistic distributions. The adoption of interval-valued fuzzy sets is capable of reflecting waste managers' confidence levels over subjective judgments, and can thus enhance the system robustness. An infinite @a-cuts method is employed for discretizing the interval-valued fuzzy sets in IVFSP. Such a method can communicate all fuzzy information into the optimization process without ignoring valuable uncertain information. Moreover, IVFSP can permit in-depth analyses of various policy scenarios that are associated with different levels of economic penalties when the promised waste-allocation targets are violated. The developed approach is applied to a MSW management problem to demonstrate its applicability. The results indicate that interval solutions associated with different risk levels of constraint violation have been generated. They can help waste managers to identify desired waste-flow-allocation schemes and capacity-expansion plans according to their preference and practical conditions, as well as facilitate in-depth analyses of tradeoffs between economic efficiency and constraint-violation risk.	stochastic programming	Guo H. Huang;Boting Yang	2012	Environmental Modelling and Software	10.1016/j.envsoft.2011.10.007	interval;probability distribution;uncertainty;engineering;operations management;mathematics;management science;fuzzy set;welfare economics;statistics	SE	-4.085965955497515	-16.545263333156885	96320
c04de5918628050730834c26c7bc25b68f424710	peer-estimation for multiple criteria abc inventory classification	multicriteria analysis;performance index;peer estimation;multiple criteria;administracion deposito;abc inventory classification;hierarchical classification;gestion stock;classification hierarchique;multi criteria;criteria weights;analisis multicriterio;analyse multicritere;inventory control;clasificacion jerarquizada	Inventory classification is an effective way to manage a large number of items. As a basic methodology, ABC analysis is widely used for classification. The traditional ABC classification is based on only a single criterion. However, it is generally recognized that multiple criteria should be considered in practice. A peer-estimation approach is proposed in this paper for multi-criteria inventory classification (MCIC). The proposed approach determines two common sets of criteria weights and aggregates the resulting two performance scores in the most favorable and least favorable senses for each item without any subjectivity. Comparisons of the proposed approach with some previous methods are illustrated based on a classical MCIC problem. It is shown that our proposed approach can provide a more reasonable and comprehensive performance index for MCIC.	peer-to-peer	Jin-Xiao Chen	2011	Computers & OR	10.1016/j.cor.2011.02.015	inventory control;process performance index;data mining;operations research	NLP	-0.34754620316196694	-15.718888270891439	96385
f6aa5eb53b38c7a0b59239534f926aa774c7732e	"""reply to """"comments on the paper: on the properties of equidifferent owa operator"""""""	minimum variance;owa operator	In reply to Péter Majlender, the connection between the (maximum spread) equidifferent OWA operator weights and the analytical method for the minimum variance OWA operator problem [R. Fullér, P. Majlender, On obtaining minimal variability OWA operator weights, Fuzzy Sets and Systems 136 (2003) 203–215] is pointed out and the differences between them are clarified. 2006 Elsevier Inc. All rights reserved.	fuzzy sets and systems;fuzzy set;spatial variability	Xinwang Liu	2006	Int. J. Approx. Reasoning	10.1016/j.ijar.2006.02.004	minimum-variance unbiased estimator;computer science;artificial intelligence;machine learning;mathematics;algorithm;statistics	AI	-0.09664168193956948	-21.58153608239186	96427
1e6f022bb32f49cb9c3f49ad7bd387a5cfaf38aa	towards a pervasive intelligent system on football scouting - a data mining study case		Football, which is a popular world-wide sport, has become one of the most practiced sports but also, with more study cases. Scouting and game analysis that is currently made has offered the possibility to improve the competition and increase the performance levels within a team. Taking this into account it emerged the term Scouting. The objective of this study is to streamline the Scouting process in Football, through Data Mining (DM) techniques and following the Cross Industry Standard Process for Data Mining (CRIPS-DM) methodology. The goal of DM was to develop and evaluate predictive models capable of forecasting a score of a football player’s performance. Based on this target, 2808 classification models and 936 regression models were developed and evaluated. For the classification, the maximum accuracy percentage was centered at 94% for the Forward player position, while for the regression the minimum error value was 0.07 for the Forward position. The results obtained allow to streamline the Scouting process in Football thus enhancing the sporting advantage.	artificial intelligence;data mining	Tiago Vilela;Filipe Portela;Manuel Filipe Santos	2018		10.1007/978-3-319-77700-9_34	regression analysis;cross industry standard process for data mining;data mining;football;business	AI	5.0342250485691045	-19.111548781285077	96508
43ecdd05103b99124b3b9d87fea14ac4b223e8be	chaotic time series prediction with residual analysis method using hybrid elman-narx neural networks	prediction method;elman neural network;embedding theorem;nonlinear time series prediction;chaotic time series;narx neural network;time series;phase space;nonlinear time series;residual analysis;neural network;chaos theory	Residual analysis using hybrid Elman-NARX neural network along with embedding theorem is used to analyze and predict chaotic time series. Using embedding theorem, the embedding parameters are determined and the time series is reconstructed into proper phase space points. The embedded phase space points are fed into an Elman neural network and trained. The residual of predicted time series is analyzed, and it was observed that residuals demonstrate chaotic behaviour. The residuals are considered as a new chaotic time series and reconstructed according to embedding theorem. A new Elman neural network is trained to predict the future value of the residual time series. The residual analysis is repeated several times. Finally, a NARX network is used to capture the relationship among the predicted value of original time series and residuals and original time series. The method is applied to Mackey-Glass and Lorenz equations which produce chaotic time series, and to a real life chaotic time series, Sunspot time series, to evaluate the validity of the proposed technique. Numerical experimental results confirm that the proposed method can predict the chaotic time series more effectively and accurately when compared with the existing prediction methods.	artificial neural network;nonlinear autoregressive exogenous model;time series	Muhammad Ardalani-Farsa;Saeed Zolfaghari	2010	Neurocomputing	10.1016/j.neucom.2010.06.004	artificial intelligence;machine learning;phase space;time series;residual;control theory;mathematics;chaos theory;artificial neural network	ML	9.401869588333089	-21.666759034677764	96860
705f9f7c8cdbe7b58e5e974b2899ebe95fac0827	a mathematical programming-based method for heterogeneous multicriteria group decision analysis with aspirations and incomplete preference information		Aspirations, which serve as a performance target and simplify cognitive processes associated with decision making, are an important decision factor for individuals and organizations. However, this factor is usually ignored in traditional multicriteria decision making. This paper considers a multicriteria group decision making problem with aspirations and incomplete preference information, in which criteria values and aspirations accept multiple formats. To solve this problem, new consistency and inconsistency indices considering importance and interaction as well as aspirations of criteria are defined. Then, we propose a bi-objective intuitionistic fuzzy programming model to identify importance and interaction parameters, based on which, an individual ranking of alternatives can be elicited. Next, to elicit a group ranking of individuals, a flexible mix 0-1 nonlinear programming model of minimizing the inconsistencies between the group final ranking and the individual ranking is established by comprehensively considering both the majority and the minority principles. Finally, an example of selecting the best strategic freight forwarder is used to illustrate the feasibility of the proposed method, followed by a sensitivity analysis and a comparison analysis. The prominent advantages of the developed method are its ability to handle multiple preference information characterizing bounded rationality and nonadditive behaviors of decision makers as well as improve a cardinal inputs-based group decision making model.	decision analysis;mathematical optimization;nonlinear programming;nonlinear system;programming model;rationality;strategic computing initiative	Wenkai Zhang;Yanbing Ju;Xiaoyue Liu;Mihalis Giannakis	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.09.030	mathematical optimization;weighted sum model;management science;decision engineering;decision analysis;business decision mapping;group decision-making;programming paradigm;ranking;bounded rationality;mathematics	AI	-3.0001222708260427	-18.13691228665176	96938
8692b30d22e1850cb538e135c9f9b7d4b63018e5	ordinal decomposability and fuzzy connectives	fuzzy set;fuzzy reasoning;decomposition;fuzzy rules;fuzzy relation;definicion;fuzzy set theory;fuzzy connectives;weak order;decomposabilite ordinale;definition;membership function;representation connaissance;ordinal decomposability;sistema difuso;systeme flou;descomposicion;knowledge representation;relation floue;raisonnement flou;ordinal conjoint;connective floue;measure theory;relacion difusa;fuzzy system;fuzzy connective	The benefit of computing with linguistic terms is now generally accepted. Fuzzy set theory provides us the conceptual tool for the interpretation and evaluation of linguistic concepts and expressions. It constitutes a quantification of the compatibility degree of objects with the associated linguistic concept through a membership function. When we make computation using fuzzy membership values such as in the evaluation of fuzzy rules confidence, the implicit assumptions are that the membership values have quantitative semantics (the extensive scale assumption) and that the numeric values are commensurate among the different fuzzy sets generated by the different concepts involved (the common scale assumption). In most situations these assumptions are difficult to justify and may lead to various anomalies. The membership values are more suitably interpreted only as ordinal scales where the numeric representations reflect compatibility orderings. In this paper, we examine the concept of fuzzy intersection and union from the perspective of decomposability and ordinal conjoint structure in measurement theory. We determine conditions under which a weak order, induced by a fuzzy set or otherwise, can be decomposed into other weak orders. We show particular cases of ordinal decomposability which correspond naturally to our concept of fuzzy intersection and union. This perspective of fuzzy connectives help us resolve some of the difficulties related to the above assumptions.	logical connective;ordinal data	John W. T. Lee	2003	Fuzzy Sets and Systems	10.1016/S0165-0114(02)00270-1	fuzzy logic;knowledge representation and reasoning;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;mathematics;fuzzy set;fuzzy set operations;algorithm;fuzzy control system	NLP	-3.35762742987697	-23.183103016089063	97067
77571d6da14b2917ba699b2c40e5745fcbb95f31	a generalized l1-type metric between fuzzy numbers for an approach to central tendency of fuzzy data	mean distance minimization;fuzzy numbers;l 1 distance;characterization of a fuzzy number;random fuzzy numbers	In dealing with data generated from a random experiment, L metrics are suitable for many statistical approaches and developments. To analyze fuzzy-valued experimental data a generalized L metric based on the mid/spread representation of fuzzy values has been stated, and a related methodology to conduct statistics with fuzzy data has been carried out. Most of the developed methods concern either explicitly or implicitly the mean values of the involved random mechanisms producing fuzzy data. Other statistical approaches and studies with experimental data consider L metrics, especially in dealing with errors or in looking for a more robust solution and intuitive interpretation. This paper aims to introduce a generalized L metric between fuzzy numbers based on a new characterization for them that will be referred to as the midb=b leftdev=b-rightdev characterization. More precisely, the metric will take into account both absolute differences in ‘location’ and absolute differences in ‘shape/imprecision’ of fuzzy numbers; moreover one can choose the weight of the influence of the second differences in contrast to the first one. After introducing the new characterization for fuzzy numbers, as well as the associated L metric, we will examine some properties. Finally, as an immediate application, the problem of minimizing the mean distance between a fuzzy number and the distribution of a random mechanism producing fuzzy number-valued data will be given, discussed and illustrated. 2013 Elsevier Inc. All rights reserved.	fuzzy logic;fuzzy number	Beatriz Sinova;Sara de la Rosa de Sáa;María Angeles Gil	2013	Inf. Sci.	10.1016/j.ins.2013.03.063	fuzzy logic;mathematical optimization;combinatorics;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;fuzzy measure theory;mathematics;t-norm;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system;statistics	AI	-0.11508249330788563	-21.015905041438867	97074
39c76846e833757fa0c2d089f9671e7fcb2dd156	prediction of ocean wave energy from meteorological variables by fuzzy logic modeling	renewable energy;takagi sugeno;pacific ocean;ocean wave energy;fuzzy logic;wind speed;fuzzy inference system;ocean wave;prediction;air temperature;wave energy	Research highlights? A new approach based on an expert system of fuzzy logic modeling was intoduced in ocean wave energy prediction. ? It is found that the fuzzy model for wave energy prediction performs better than classical approaches. ? By this proposed approach, it is possible to determine potential wave power in any area from meteorological measurements in the absence of spectral wave measurements. Ocean wave energy which is one of the promising renewable energy types has a direct relationship with the wave climate. The purpose of this study is to investigate the relationship between ocean wave energy and meteorological variables such as wind speed, air temperature, and sea temperature. It was shown that fuzzy logic modeling of these variables provides the possible non-linear relationship between them and consequently the wave energy can be predicted including the possible uncertainties in the system behavior. Compared to traditional approaches, fuzzy logic is more efficient in linking the multiple inputs to a single output in a non-linear manner. Here Takagi-Sugeno (TS) type fuzzy inference system was employed to predict wave energy amount from meteorological variables in the absence of wave records. For the application of the proposed approach the offshore stations located in the Pacific Ocean were used. The results were compared with the classical wave energy equation.	fuzzy logic	Mehmet Özger	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.11.090	fuzzy logic;wind speed;renewable energy;wind wave;prediction;temperature;computer science;wind wave model;statistics	AI	9.899172431557016	-18.611843722077985	97183
02b12cf99850d6d4c2730c59d6a9523e838341f4	towards a generic framework for short term firm-specific stock forecasting	forecasting;market research;neural networks;clustering algorithms technical analysis sentiment analysis stock market analysis robust learning engine stock trends prediction market hypothesis technical parameters end predictive power supervised nonshallow learning architectures optimization;companies stock markets forecasting sentiment analysis market research neural networks;companies;machine learning stock forecasting sentiment analysis technical analysis;stock markets;information and communication technology;sentiment analysis;stock markets forecasting theory learning artificial intelligence optimisation pattern clustering	This paper investigates the predictive power of technical analysis, sentiment analysis and stock market analysis coupled with a robust learning engine in predicting stock trends in the short term for specific companies. Using large and varied datasets stretching over a duration of ten years, we set out to train, test and validate our system in order to either contradict or confirm efficient market hypothesis. Our results reveal a significant improvement over the efficient market hypothesis for majority companies and thus strongly challenge it. Technical parameters and algorithms operating upon them are shown to have a significant impact upon the end-predictive power of the system, thus bolstering claims of their efficacy. Moreover, sentiment analysis results also show a strong correlation with future market trends. Lastly, the superiority of supervised non-shallow learning architectures is illustrated via a comparison of results obtained through a myriad of optimization and clustering algorithms.	algorithm;artificial neural network;bayesian network;cluster analysis;deep belief network;encoder;machine learning;mathematical optimization;noise reduction;sentiment analysis;web crawler	Mansoor Ahmed;Anirudh Sriram;Sanjay Singh	2014	2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2014.6968411	market research;information and communications technology;forecasting;computer science;mathematics;sentiment analysis;statistics	ML	5.516538967211658	-19.152074551543972	97536
af7321f7e10850d0bc42a8e312362203c99b6f5b	efficient pricing and hedging under the double heston stochastic volatility jump-diffusion model	cos method;91b70;option pricing;jump risks;91g20;60e10;heston model;91g60;dynamic hedging	The performances of jump-risk mitigation under a double Heston stochastic volatility jump-diffusion dbHJ model are examined. The quadratic exponential QE scheme is used to simulate the    -measure price paths supposed to follow the dbHJ model, whereas a Fourier-COS-expansion-based scheme i.e. the COS formula, see [F. Fang and C.W. Oosterlee, A novel pricing method for European based on Fourier-cosine series expansions, SIAM J. Sci. Comput. 31 2008, pp. 826–848] is employed to price options and to calculate Greeks. Numerical results from extensive dynamic hedging experiments suggest that, when facing a    -measure market with stiff volatility skews and non-trivial jumps, the dbHJ model is better than the plain double Heston and Black–Scholes models, and slightly outperforms the Heston stochastic volatility jump-diffusion HJ model and the Merton model in mitigating the jump risk of an option. This conclusion holds independent of the model specification of    -measure market.	volatility	Youfa Sun	2015	Int. J. Comput. Math.	10.1080/00207160.2015.1079311	implied volatility;actuarial science;replicating portfolio;volatility smile;valuation of options;heston model;mathematical economics;stochastic volatility	ECom	3.494958896476155	-11.490745271044599	97792
349e20bebfaaac7f7e06597e957f7489e0691d01	study of the gm(1,1) model and improved model of the trend term of micromechanical gyroscope drift	gyroscopes;polynomial regression;trend term;1 model micromechanical gyroscope trend term gm 1;nonstationary random drift signal;polynomial regression methods;normal random series;signal processing gyroscopes micromechanical devices polynomials random processes regression analysis;linear regression;micromechanical gyroscope;polynomials;fitting;micromechanical devices;time series analysis;signal processing;random processes;exponential smoothing;mathematical model;micromechanical gyroscope drift;gm 1;predictive models;regression analysis;navigation system;normal random series micromechanical gyroscope drift microinertial integrated navigation system nonstationary random drift signal linear regression polynomial regression methods stationary random series;correlation;stationary random series;1 model;microinertial integrated navigation system;micromechanical devices gyroscopes predictive models equations signal processing signal analysis mathematical model sun automation educational institutions;data models	To improve measure precise of micro inertial integrated navigation system, modified GM(1,1) using exponential smooth is proposed to depict nonstationary random drift signal of micromechanical gyroscope under zero input condition. The model is compared with ones build using linear regression and polynomial regression methods. The experiment and respected simulation results show that the modified GM(1,1) model is better for eliminating the trend term of nonstationary signal and the extracted signal is proved to be zero-mean, normal and stationary random series.	fibre optic gyroscope;polynomial;simulation;stationary process;time complexity	Xuelian Li;Yao Sun;Hongwei Mo;Kejun Wang	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.137	exponential smoothing;data modeling;econometrics;gyroscope;computer science;linear regression;signal processing;time series;polynomial regression;mathematical model;mathematics;predictive modelling;correlation;regression analysis;statistics;polynomial	EDA	10.029664345336483	-14.577821117696727	98127
12de2bc1263e65f8fbec2d200f4b78075c862b2b	comparison of arithmetic operations of generalized fuzzy numbers: case study in risk assessment	fuzzy numbers;average width;generalized fuzzy numbers;risk assessment	ABSTRACTThis study attempts to perform arithmetic operations of generalized triangular fuzzy numbers apart from the existing approaches. Also, average width of generalized fuzzy numbers is defined. Numerical examples are illustrated and uncertainty is measured by using defined average width, and finally, the results are compared with the existing approaches. A case study for health risk assessment is carried out in this comparison.	risk assessment	Palash Dutta	2016	Cybernetics and Systems	10.1080/01969722.2016.1182354	risk assessment;combinatorics;discrete mathematics;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set operations	Logic	-2.2329040736150776	-20.527462573121753	98398
95e80084d820750814a0e745598f65c53384b1fa	a hybrid approach to forecast stock market index	anfis;neural networks;adaptive network based fuzzy inference system;subtractive clustering;stock markets;fuzzy logic;stock market predictability;stock market forecasting;adaptive neuro fuzzy inference system	The forecasting of stock market problem from the available data is quite often of uncertain nature, hence the stock market prediction is a very challenging and difficult task. In this paper, we have investigated the predictability of stock market of Bombay Stock Exchange (BSE30), Hang Sang China Stock Index (HS), Japan Stock Index (NIKKEI) and Taiwan Weighted Index (TWI) with adaptive network-based fuzzy inference system (ANFIS) combined with subtractive clustering technique. In this process, we compared stock markets with variable numbers of data clusters. Optimised subtractive clustering is used to cluster the data and create fuzzy membership functions. Finally, a hybrid learning algorithm has been used to combine least square method and back propagation gradient-decent method for training the fuzzy inference system. This paper represents a state of the art for ANFIS application to forecast stock market index.	adaptive neuro fuzzy inference system;algorithm;backpropagation;cluster analysis;fuzzy logic;gradient;inference engine;serial ata;software propagation;time series	Gurbinder Kaur;Joydip Dhar;Rangan K. Guha	2015	IJAISC	10.1504/IJAISC.2015.070638	adaptive neuro fuzzy inference system;computer science;artificial intelligence;data mining;artificial neural network	ML	8.548999168172411	-20.031153106658195	98436
d6a1adcf67661e40eb1327dc65af5dd0b90f5eb5	monte carlo simulation of traffic noise dynamics at a bus stop based on real sound signals		Noise from vehicular traffic is the main source of noise pollution in cities. Sound power variations from this kind of noise are caused by instabilities in traffic due to crossings, speed bumps, bus stops, and idiosyncratic driver behavior. This study aims to estimate the impact from vehicular traffic noise dynamics in the vicinity of a bus stop. Simulations were done using a probabilistic model based on the Monte Carlo method. The hypothesis is that the variability of noise due to bus arrivals and departures at a bus stop increases the level of local noise impact. Therefore, Traffic Noise Index (TNI) and Noise Pollution Level (LNP) were adopted as the main acoustic parameters. Methodological procedures were the following: selection of the study object, a bus stop; recording sound signals for passing vehicles; acquisition of geometric parameters, acoustic and traffic cues; simulations and analysis of traffic noise scenarios with different times between bus arrivals at the bus stop. It was found that small...	monte carlo method;simulation	Italo César Montalvão Guedes;Stelamaris Rolla Bertoli;Jugurta R. Montalvão Filho	2016	Proc. Meetings on Acoustics	10.1121/2.0000477	simulation;sound power;monte carlo method;noise pollution;traffic noise;statistical model;electronic engineering;engineering	Networks	9.779466030413227	-11.090432299819172	98490
58f4a77470b0c31045a834ea02d377ffc890a4dd	on simulating the resilience of military hub and spoke networks	random processes;stochastic processes;redundancy;topology;sampling methods	Hub and spoke networks, while highly efficient, are fragile to targeted attacks: removal of the central hub destroys connectivity of the network. This fragility has led to the assertion that these networks are not suited to military distribution systems. However, military supply chains have redundancy induced by heterogeneous transportation modes (e.g., road, marine, and air) leading to enriched connectivity over a pure hub and spoke structure. In this paper a global military (hierarchical) hub and spoke network model is developed; the topological resilience of such networks are probed by stochastically sampling an ensemble of networks and simulating both random and targeted edge knockout, and the network properties relevant to resilience measured. It is found that such networks are resilient to continual attack and loss (network erosion), performing well relative to preferential (scale free) and random network benchmarks. This regime of network erosion is descriptive of modern asymmetric warfare.	knockout;network model;random graph;sampling (signal processing);simulation;usb hub	Robert Bryce;Raman Pall;Ahmed Ghanmi	2013	2013 Winter Simulations Conference (WSC)		stochastic process;sampling;simulation;engineering;mathematics;redundancy;computer security;statistics	Metrics	7.320584572679662	-10.45298783716797	98581
e3f1e792be2a631c6c761f1db72a36303e24e6f7	save the best for last? the treatment of dominant predictors in financial forecasting	forecasting;hg finance;hd61 risk management;variable importance;financial markets	We study forecasting applications where the response variable is heavily correlated with one or a small set of covariates which we term dominant predictors. Dominant predictors commonly occur in financial forecasting where future market prices are heavily influenced by current prices, and to a much lesser degree, by many other, more subtle factors such as weather or calendar effects. We hypothesize that dominating predictors may mask the influence of the subtle factors, reducing forecasting accuracy. Consequently, we argue that it is crucial to find means of accurately accounting for the effect of the subtle factors on the response variable. To achieve this we present a two-stage modeling methodology which postpones the introduction of dominating predictors into the model building process until all predictive value from the other covariates has been extracted. To confirm our hypothesis and to test the effectiveness of the two-stage approach, we conduct an empirical study related to forecasting the outcome of sports events, which are well known to exhibit dominating predictors. Our results confirm that especially complex, nonlinear models are vulnerable to the masking effect and benefit from the two-stage paradigm. Our findings have important implications for forecasters who operate in environments where the influence of some predictors on the variable being forecast exceeds those of other covariates by a wide margin and we demonstrate appropriate ways to approach such forecasting tasks. 2012 Elsevier Ltd. All rights reserved.	additive model;benchmark (computing);decision support system;dual total correlation;edge dominating set;expect;imperative programming;kerrison predictor;linear model;mask (computing);mike lesser;nonlinear acoustics;nonlinear system;norm (social);numerical weather prediction;programming paradigm;psychoacoustics;variable (computer science);while	Ming-Chien Sung;Stefan Lessmann	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.02.091	econometrics;actuarial science;forecasting;financial market;statistics	HCI	-1.1525140997870815	-10.505031667493798	98613
e8b0c5191bdc7b8b6be9f2b2ee37326a7681215d	the construction of fuzzy-valued t-norms and t-conorms	fuzzy set	We are interested in generalizations of the concepts of t-norm and t-conorm. In a companion chapter in this volume [14] we have presented a family of hypert-norms ∧q and a family of hyper-t-conorms ∨p. The prefix hyper is used to indicate multi-valued operations, also known as hyperoperations (see [5, 14]), i.e. operations which map pairs of elements to sets of elements. See [14] for the construction of hyperoperations which generalize t-norms and t-conorms. These hyperoperations are crisp, i.e. their output is a crisp set. A natural generalization is to consider fuzzy hyperoperations, i.e. operations which map elements to fuzzy sets. Hence in this chapter we will present a procedure to construct fuzzy-valued t-norms and t-conorms. Relatively little work in this direction has appeared in the literature. A pioneering paper is [4] which introduces fuzzy hypoperations which induce fuzzy hypergroups. A fuzzy hypergroup, different from the one used by Corsini, is [13] and a version of fuzzy min and fuzzy max operations appears in [11, 12]. As explained also in [14], our motivation to study multi-valued and fuzzyvalued connectives is that, while fuzzy logic is a calculus of uncertain reasoning, not much attention has been paid to the case where uncertainty is	computational intelligence;fuzzy cognitive map;fuzzy logic;fuzzy set;logical connective;multistage interconnection networks;t-norm	Athanasios Kehagias	2007		10.1007/978-3-540-72687-6_18	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	-1.406864330679835	-23.462508257920913	98950
8b02bccbb3fa342140a77c71026ccee053ae4448	estimation of true efficient frontier of organisational performance using data envelopment analysis and support vector machine learning	support vector machines;dea;dmus;organisational performance;data envelopment analysis;performance analysis;svm;stochastic frontier functions;decision making units;support vector machine	Data envelopment analysis (DEA) and stochastic frontier functions (SFF) are two well-known tools for performance and efficiency analysis of profit and non-profit organisations, referred to as decision making units (DMUs). The challenge to traditional DEA is how to account for both managerial and observational errors if present in the analysis, so as to determine true efficient frontiers. This paper proposes a novel methodology to determine true frontiers in a non-parametric environment. Specifically, traditional DEA is integrated with SFF through support vector machine (SVM) learning to provide an adaptive way to estimate true frontiers considering managerial and observational errors. A statistical ratio is utilised to find the true frontiers, and the proposed methodology is applied to a real data set where frontiers are compared to ones obtained by other existing methods. The work in this paper can help organisations to plan a more realistic investment by providing reasonable sense of benchmarking.	data envelopment analysis;machine learning;support vector machine	Kerry Poitier;Sohyung Cho	2011	IJIDS	10.1504/IJIDS.2011.040421	support vector machine;computer science;operations management;machine learning;data mining;data envelopment analysis;operations research	ML	0.06746842811286784	-13.35160327858211	99041
63ec428359f1a6b78ec085a63c4882f1ade0c4a5	a balanced relationship analysis between chinese economic growth and the iron and steel production based on time series	balanced relationship analysis;liquefied natural gas;time series industrial economics production management steel manufacture;empirical analysis;industrial economics;liquefied natural gas iron steel economic indicators production time series analysis;iron;long run equilibrium relationship;error correction model he growth of economic the iron and steel production cointegration test;time series;the iron and steel production;production management;he growth of economic;time series analysis;iron production fluctuation;steel manufacture;steel production fluctuation;cointegration test;steel;production;chinese economic growth fluctuation;economic growth;long run equilibrium relationship balanced relationship analysis chinese economic growth fluctuation iron production fluctuation steel production fluctuation time series error correction model cointegration test;error correction model;economic indicators	In this paper, the empirical analysis between China's economic growth and iron and steel production data from 1978 to 2008 is made by the Cointegration Test and Error Correction Model. By the Cointegration Test, the results shows that the growth of economic and the iron and steel production had long-run equilibrium relationship, that is, the two time series had the movement of the same trend, but, the fluctuation of the iron and steel production has small accelerate effect on the fluctuation of the growth of economic in the short term. Though the ECM, the results is that the fluctuation of the iron and steel production has limited effect on the fluctuation of the growth of economic in the short term.	time series		2010		10.1109/ICEE.2010.877	economics;operations management;time series;economy;statistics	HCI	3.472308204786128	-14.475529965393786	99289
7688b2e38076c408f8e3bf5f8c741181aeb9b479	a hybrid method for evaluating biomass suppliers – use of intuitionistic fuzzy sets and multi-periodic optimization		Evaluation of biomass suppliers is a time-dependent problem that requires assessment of different supply schemes in different periods. This paper presents a hybrid method for evaluating biomass suppliers that combines Intuitionistic Fuzzy Sets (IFS), linear programming (LP) and multi-periodic optimization (MPO). IFS allow evaluators to express their hesitation when they assess alternative suppliers. LP is used to estimate weights of evaluation criteria and calculate suppliers’ ratings in a specific period. These ratings are utilized by a MPO model to determine what type and how much feedstock should be supplied by each supplier in each period.	common criteria;fuzzy set;intuitionistic logic;linear programming;mathematical optimization;path ordering (term rewriting);rendering (computer graphics)	Vassilis C. Gerogiannis;Vasiliki Kazantzi;Leonidas G. Anthopoulos	2012		10.1007/978-3-642-33409-2_23	periodic graph (geometry);artificial intelligence;machine learning;computer science;fuzzy set;linear programming;raw material;biomass	AI	-0.9969639918228329	-17.654737167535956	99397
44ca8beb1e170777d0eb4367ff0c3346703ffc33	distance and similarity measures between uncertain variables		Uncertainty theory is a branch of mathematics based on normality, monotonicity, self-duality, countable subadditivity, and product measure axioms. Different from randomness and fuzziness, uncertainty theory provides a new mathematical model for uncertain phenomena. Distance and similarity measure have played an important role in uncertainty theory as well as many other disciplines. This paper first proposes some new distances between uncertain variables. Then the definition of the degree of similarity between uncertain variables is introduced and several similarity measures between uncertain variables are generated from distance measures. Finally, the distance and similarity measures proposed are applied to pattern recognition.	machine learning;mathematical model;pattern recognition;randomness;similarity measure;uncertainty theory	Xiaozhong Li;Ying Liu	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141486	combinatorics;mathematical analysis;discrete mathematics;mathematics	ML	-0.7883191195084511	-21.80911631224249	99499
ef9a6853b46e590ef5400c34816e04bf8040bc9b	rating equity funds against return of random traders	timing indexes stock markets mutual funds function approximation benchmark testing;normal distribution;investment;stock markets investment normal distribution sampling methods;stock markets;taiwan equity funds rating random traders return random trader scheme equity fund benchmark return all in all out strategy market trade capital market size drrt random variable sampling log normal distribution taiex index cumulative distribution function;rating equity funds;equity fund evaluation;sampling methods;cumulative distribution function approximation	In this paper, we study the problem of benchmarking returns of equity funds. We present a novel approach, the random-trader scheme, to benchmark return of an equity fund during a specific period. A random trader uses all-in-all-out strategy to trade in the market at random timing with capital being negligible as compared with the market size.In this paper, we study the problem of benchmarking returns of equity funds. We present a novel approach, the random-trader scheme, to benchmark return of an equity fund during a specific period. A random trader uses all-in-all-out strategy to trade in the market at random timing with capital being negligible as compared with the market size. Let DRRT be the distribution of returns of random traders, and Rrt be a random variable sampling from DRRT. In this paper we model DRRT as a log-normal distribution, denoted as sDRRT, and provide an efficient algorithm to compute the mean and variance of sDRRT, denoted as μDRRT and σDRRT, respectively. Using TAIEX index as data set, our experiments showed sDRRT approximates DRRT well when the length of given period is one month. We then score each equity fund by the cumulative distribution function of sDRRT i.e., s(m; DRRT) = Pr[Rrt ≤ R(m)] , where R(m) denotes the return of the equity fund m during the given period. Using the historical data on equity funds in Taiwan, we observed interesting characteristics. When analyzing monthly returns, although there are some winners who are able to achieve scores higher than .9 sometimes, it is difficult for them to always keep up at the high scores. However, few equity funds showed the stability of scores higher than .7, when analyzing long-term returns. Furthermore, there are times when most funds obtain high scores. There are also times when no funds perform well.	algorithm;approximation;backtesting;benchmark (computing);experiment;sampling (signal processing);theory;traders	Ta-Wei Hung;Mu-En Wu;Hsueh-I Lu;Jan-Ming Ho	2013	2013 International Conference on Social Computing	10.1109/SocialCom.2013.113	normal distribution;actuarial science;investment;statistics	ML	3.1645481155655375	-12.016496176752918	99555
55377801f1c8f8c1810c3b86be84311901cc4b59	taiex forecasting based on fuzzy time series, particle swarm optimization techniques and support vector machines	fuzzy time series;support vector machines;fuzzy sets;particle swarm optimization;taiex	In this paper, a new method for forecasting the TAIEX is presented based on fuzzy time series, particle swarm optimization techniques and support vector machines. The proposed method to forecast the TAIEX is based on the slope of one-day variation of the TAIEX and the slope of two-days average variation of the TAIEX. Because the slope of two-days average variation of the TAIEX is smoother than the slope of one-day variation of the TAIEX, it is chosen to define the universe of discourse. The particle swarm optimization techniques are used to get optimal intervals in the universe of discourse. The support vector machine is used to classify the training data set. The first feature and the second feature of the support vector machine are the slope of one-day variation and the slope of two-days average variation of the TAIEX, respectively. The experimental results show that the proposed method outperforms the existing methods for forecasting the TAIEX.	mathematical optimization;particle swarm optimization;support vector machine;time series	Shyi-Ming Chen;Pei-Yuan Kao	2013	Inf. Sci.	10.1016/j.ins.2013.06.005	support vector machine;mathematical optimization;computer science;machine learning;pattern recognition;mathematics;fuzzy set;particle swarm optimization	DB	8.178613932719076	-19.88382506991	99582
2c71783e86fcce76262c532d466b3e5f952b0c33	a portfolio-based evaluation of affine term structure models	asset prices;term structure models;statistical evaluation;active asset allocation;affine term structure model;factor model;affine models;portfolio management;asset allocation;optimal portfolio;dynamic optimization	We focus on affine term structure models as tools for active bond portfolio management. We use multifactor term structure models to produce forecasts for the future values of the state variables. Starting from the conditional moments of the state vector implied by the models, we introduce binomial approximations to come up with discrete scenarios for the future state variables. From the theoretical asset pricing relations we compute bond prices for various maturities at future dates and the associated returns. We use these returns as inputs for the portfolio optimization problem faced by an investor with a six month horizon who takes into account the possibility to rebalance after one quarter. Each quarter the optimizer selects the optimal portfolio, and the sequence of optimal portfolios is then evaluated in terms of financial properties. The results show that a financial based evaluation of term structure models may yield results conflicting with those obtained from a statistical evaluation.	approximation;best, worst and average case;dynamic programming;emoticon;mathematical optimization;offset binary;optimization problem;relevance;self-balancing binary search tree;volatility	Andrea Beltratti;Paolo Colla	2007	Annals OR	10.1007/s10479-006-0134-4	financial economics;post-modern portfolio theory;mathematical optimization;capital asset pricing model;actuarial science;economics;replicating portfolio;affine term structure model;finance;asset allocation;portfolio optimization;factor analysis;project portfolio management	Web+IR	2.9630590130190533	-10.537149137368921	99704
85f82c687bb273b2d2b90d7ed82f690e517ffaee	a linear programming method for generating the most favorable weights from a pairwise comparison matrix	agregacion;multicriteria analysis;evaluation performance;analytic hierarchy process;analisis envolvimiento datos;performance evaluation;processus hierarchie analytique;most favorable weights;systeme aide decision;evaluacion prestacion;estimation non parametrique;analisis decision;priorite;rank reversal;prise de decision;sistema ayuda decision;ahp;aggregation;eigenvector;decision analysis;rank preservation;vector propio;ds ahp method;non parametric estimation;decision support system;hierarchical classification;programacion lineal;vectors;data envelopment analysis;layout design;priorities;proceso jerarquia analitico;linear programming;classification hierarchique;agregation;programmation lineaire;linear program;preservation;preference;analisis multicriterio;estimacion no parametrica;analyse multicritere;toma decision;data envelope analysis;priority;prioridad;clasificacion jerarquizada;article;preservacion;analyse decision;vecteur propre;eigenvectors;multicriteria decision analysis;analyse enveloppement donnee	This paper proposes a linear programming method for generating the most favorable weights (LP-GFW) from pairwise comparison matrices, which incorporates the variable weight concept of data envelopment analysis (DEA) into the priority scheme of the analytic hierarchy process (AHP) to generate the most favorable weights for the underlying criteria and alternatives on the basis of a crisp pairwise comparison matrix. The proposed LP-GFW method can generate precise weights for perfectly consistent pairwise comparison matrices and approximate weights for inconsistent pairwise comparison matrices, which are not too far from Saaty’s principal right eigenvector weights. The issue of aggregation of local most favorable weights and rank preservation methods is also discussed. Four numerical examples are examined using the LP-GFW method to illustrate its potential applications and significant advantages over some existing priority methods. 2007 Elsevier Ltd. All rights reserved.	analytical hierarchy;approximation algorithm;data envelopment analysis;finite element method;horner's method;lp-type problem;linear programming;newton's method;numerical analysis	Ying-Ming Wang;Celik Parkan;Ying Luo	2008	Computers & OR	10.1016/j.cor.2007.05.002	analytic hierarchy process;eigenvalues and eigenvectors;computer science;linear programming;artificial intelligence;data envelopment analysis;mathematics;algorithm;statistics	AI	-0.6360537839207212	-16.08091465181042	99731
08576a814f49a334907a6aed90c0d0f6532b11e2	on calibration of stochastic and fractional stochastic volatility models	fractional stochastic volatility model;option pricing;optimization;heston model;calibration	In this paper we study optimization techniques for calibration of stochastic volatility models to real market data. Several optimization techniques are compared and used in order to solve the nonlinear least squares problem arising in the minimization of the difference between the observed market prices and the model prices. To compare several approaches we use a popular stochastic volatility model firstly introduced by Heston (1993) and a more complex model with jumps in the underlying and approximative fractional volatility. Calibration procedures are performed on two main data sets that involve traded DAX index options. We show how well both models can be fitted to a given option price surface. The routines alongside models are also compared in terms of out-of-sample errors. For the calibration tasks without having a good knowledge of the market (e.g. a suitable initial model parameters) we suggest an approach of combining local and global optimizers. This way we are able to retrieve superior error measures for all considered tasks and models.	volatility	Milan Mrázek;Jan Pospísil;Tomás Sobotka	2016	European Journal of Operational Research	10.1016/j.ejor.2016.04.033	financial economics;econometrics;mathematical optimization;implied volatility;calibration;economics;valuation of options;heston model;mathematical economics;stochastic volatility;sabr volatility model	Vision	3.1721863457353368	-11.067606648369384	99748
12fc95ad0fab34c0abbaf971df6b75d05f843ebd	a new method for autocratic decision making using group recommendations	order weighted averaging owa operator autocratic decision making correlation coefficients group recommendations;correlation coefficients autocratic decision making group recommendation order weighted averaging owa operator similarity measures;abstracts open wireless architecture gold;group theory decision making	This paper presents a new method for autocratic decision making using group recommendations based on the order weighted averaging (OWA) operator, similarity measures and correlation coefficients. The proposed method can overcome the drawback of Chen and Lee's method [2] due to the fact that Chen and Lee's method cannot deal with the preference ordering of alternatives in some situations. It provides us with a useful way for autocratic decision making based on group recommendations.	coefficient;entity–relationship model	Shyi-Ming Chen;Bing-Han Tsai	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890780	artificial intelligence;machine learning;data mining;mathematics;weighted sum model	Robotics	-2.801937898559488	-20.652592851566755	99791
fc41ecc4a6763d5cb28fb792750b9d04fafb3d4d	a modification of the time-geographic framework to support temporal flexibility in 'fixed' activities	mobility;geocomputation;time geography	The time-geographic distinction between fixed and flexible activities is widely acknowledged to be a somewhat arbitrary dichotomy but is still the current modus operandi for time-geographic calculations. This paper proposes a modification of the classic time-geographic framework to support temporal flexibility in ‘fixed’ activities. This modification is crucial for time-geographic calculations on public transit networks with a low frequency of service, which would otherwise return unstable results.		Laure Charleux	2015	International Journal of Geographical Information Science	10.1080/13658816.2015.1009464	simulation;time geography;computer science;operations research	Vision	-3.684107802515625	-10.952930766232754	99887
5351286b96a8fd8fc0b450f1a52f714e4a9eb37b	a note on reasoning conditions of kóczy's interpolative reasoning method	fuzzy rules;fuzzy relation;triangular type membership function;raisonnement;linear interpolative reasoning;reasoning conditions;linear interpolation;fonction appartenance;fuzzy inference;razonamiento;membership function;interpolation lineaire;funcion pertenencia;reasoning;relation floue;relacion difusa;interpolacion lineal	Abstract   In this note, we analyze continually the interpolative reasoning method by Koczy and Hirota, and prove that the reasoning conditions given by the authors are also necessary conditions to guarantee the fuzzy inference consequence to be of triangular type if the fuzzy rules and an observation are defined by triangular-type membership functions.		Yan Shi;Masaharu Mizumoto	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00340-5	discrete mathematics;membership function;computer science;artificial intelligence;mathematics;reasoning system;linear interpolation;deductive reasoning;reason;algorithm	AI	1.275527029949565	-22.89494344538524	100034
b0d5331a0046e4a86ab9867246c7df78100d27fe	uncertain identification problems in the context of granular computing		The chapter is devoted to applications of selected methods of computational intelli- gence: evolutionary algorithms and artificial neural networks, in identification of physical sys- tems being under the uncertain conditions. Uncertainties can occur in boundary conditions, in material coefficients or some geometrical parameters of systems and are modeled by three kinds of granularity: interval mathematics, fuzzy sets and theory of probability. In order to evaluate fitness functions the interval, fuzzy and stochastic finite element methods are applied to solve granular boundary-value problems for considered physical systems. Several numerical tests and examples of identification of uncertain parameters are presented.	granular computing	Tadeusz Burczynski;Piotr Orantek	2009		10.1007/978-3-540-92916-1_14	mathematical optimization;artificial intelligence;machine learning;mathematics	HPC	2.6383227964412153	-22.32435880655363	100305
098867dfdd0673e53ed8478d9c7c34568f70925e	developing computer hex using global and local evaluation based on board network characteristics		The game of Hex was invented in the 1940s, and many studies have proposed ideas that led to the development of a computer Hex. One of the main approaches developing computer Hex is using an evaluation function of the electric circuit model. However, such a function evaluates the board states only from one perspective. Consequently, it is recently defeated by the Monte Carlo Tree Search approaches. In this paper, we therefore propose a novel evaluation function that uses network characteristics to capture features of the board states from two perspectives. Our proposed evaluation function separately evaluates the board network and the shortest path network using betweenness centrality, and combines the results of these evaluations. Furthermore, our proposed method involves changing the ratio between global and local evaluations through a support vector machine (SVM). So, it yields an improved strategy for Hex. Our method is called Ezo. It was tested against the world-champion Hex program MoHex. The results showed that our method was superior to the 2011 version of MoHex on an (11 times 11) board.		Kei Takada;Masaya Honjo;Hiroyuki Iizuka;Masahito Yamamoto	2015		10.1007/978-3-319-27992-3_21	simulation;operations management;computer security	Vision	7.164941140754398	-21.083503168075616	100367
acd1d24d544e899c7703be09d5808a347c393fc3	fuzzy measures on finite scales as families of possibility measures	possibility theory;fuzzy measures;qualitative moebius transform	We show that any capacity or fuzzy measure ranging on a qualitative scale can be viewed both as the lower bound of a set of possibility measures, and the upper bound of a set of necessity measures. An algorithm is provided to compute the minimal set of possibility measures dominating a given capacity. This algorithm relies on the representation of the capacity by means of its qualitative Moebius transform, and the use of selection functions of the corresponding focal sets. We also introduce the counterpart of a contour function, that turns out to be the union of all most specific possibility distributions dominating the capacity. Finally we show the connection between Sugeno integrals and lower possibility measures.	algorithm;enumerated type;focal (programming language);fuzzy measure theory;maxima and minima;maximal set;moebius: empire rising;prospective search	Didier Dubois	2011		10.2991/eusflat.2011.148	fuzzy measure theory	Vision	1.2934877304617878	-21.017949263629216	100510
944a6a1165f700f38fb9d32cfe1c9f7f412c5c0b	estimating skill fungibility and forecasting services labor demand		We present an approach for forecasting labor demand in a services business. We introduce an arrangement of machine learning techniques, each constructed by necessity to overcome issues with data veracity and high dimensionality.	machine learning;veracity	Brian Johnston;Benjamin Zweig;Michael Peran;Charlie Wang;Rachel Rosenfeld	2017	2017 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2017.8258351	data mining;semantics;fungibility;curse of dimensionality;computer science;cluster analysis;labor demand	Robotics	4.782890739144933	-18.01342494998316	100583
e3d0946257996609f67c7001142b63ab12773ac3	l-fuzzy sets and intuitionistic fuzzy sets	intuitionistic fuzzy set;fuzzy membership function;fuzzy set;computational intelligence;fuzzy inference;complete lattice	In this article we firstly summarize some notions on L−fuzzy sets, where L denotes a complete lattice. We then study a special case of L−fuzzy sets, namely the “intuitionistic fuzzy sets”. The importance of these sets comes from the fact that the negation is being defined independently from the fuzzy membership function. The latter implies both flexibility and effectiveness in fuzzy inference applications. We additionally show several practical applications on intuitionistic fuzzy sets, in the context of computational intelligence.	computational intelligence;fuzzy logic;fuzzy set;intuitionistic logic	Anestis G. Hatzimichailidis;Basil K. Papadopoulos	2007		10.1007/978-3-540-72687-6_16	fuzzy logic;t-norm fuzzy logics;mathematical analysis;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Theory	-1.3401892566156186	-23.17977620786523	100701
132e6b10dac87d1117878f6746c473f6875436ff	a novel approach to estimate freeway traffic state: parallel computing and improved kalman filter		This paper presents a novel approach for freeway traffic state estimation. Although there is currently a wide variety of estimation methods, new designs and technologies can still be created and implemented to improve the accuracy and time efficiency. In this study, a parallel computing framework is developed. The parallel computing framework uses a genetic algorithm process to calibrate the parameters of the traffic model based on the freeway traffic data once an hour. Meanwhile, the framework uses a Kalman filter algorithm process to optimize the traffic model results with the real-time freeway traffic data. Under the framework, the operations of the two processes will not interfere with each other, thus reducing the time it takes to estimate, increasing the efficiency. Furthermore, an improved Kalman filter algorithm is proposed. The algorithm optimizes the traffic model results by balancing the ratio of detector measurements to model results based on their variances, instead of using the Taylor series expansion. Therefore, time efficiency and accuracy of the Kalman filter algorithm are improved. The effectiveness of the approach is evaluated using real field data. Experiments have shown that this approach has high real-time tracking accuracy, and is faster and more accurate than the extended Kalman filter. Results also indicated that the estimation accuracy can be enhanced by accessing additional data sources.	experiment;extended kalman filter;freeway;genetic algorithm;parallel computing;peterson's algorithm;real-time clock;series expansion	Chong Wang;Bin Ran;Han -Soeb Yang;Jian Feng Zhang;Xu Qu	2018	IEEE Intelligent Transportation Systems Magazine	10.1109/MITS.2018.2806627	parallel computing;genetic algorithm;kalman filter;traffic model;engineering;taylor series;detector;extended kalman filter;data modeling	Robotics	9.866696602270467	-13.238920330902673	100862
cbbca5d102be35ca826b785228bfb33ffc5378cf	understanding the impact of brand colour on brand image: a preference disaggregation approach	preference disaggregation;brand colour;multicriteria decision aid;brand image	What is the role that colour plays in perception of a brand by customers? How can we explore the cognitive role that colour plays in determining brand perception? To answer these questions we propose a preference disaggregation method based on multi-criteria decision aid. We identify the criteria aggregation model that underlies the global preference of a brand with respect to each brand image attribute. The proposed method is inspired by the well-known UTASTAR algorithm, but unlike the original formulation, it represents preferences by means of non-monotonic value functions. The method is applied to a database of brands ranked on each brand image attribute. For each brand image attribute, non-monotonic marginal value functions from each component of the brand colour are obtained separately. These functions contain the fitness between each colour component and each brand image attribute, in an understandable manner. © 2015 Published by Elsevier B.V.	algorithm;color space;database;marginal model;whole earth 'lectronic link	Mohammad Ghaderi;Francisco Javier Ruiz;Núria Agell	2015	Pattern Recognition Letters	10.1016/j.patrec.2015.05.011	brand;multimedia	AI	-2.8767984319958564	-17.424771584147045	100870
b89524d007e8713a34a4fdd183f3fbe74ad78a78	adversarial life testing: a bayesian negotiation model	reliability;life testing;exponential families;negotiation model;bayesian analysis	Life testing is a procedure intended for facilitating the process of making decisions in the context of industrial reliability. On the other hand, negotiation is a process of making joint decisions that has one of its main foundations in decision theory. A Bayesian sequential model of negotiation in the context of adversarial life testing is proposed. This model considers a general setting for which a manufacturer offers a product batch to a consumer. It is assumed that the reliability of the product is measured in terms of its lifetime. Furthermore, both the manufacturer and the consumer have to use their own information with respect to the quality of the product. Under these assumptions, two situations can be analyzed. For both of them, the main aim is to accept or reject the product batch based on the product reliability. This topic is related to a reliability demonstration problem. The procedure is applied to a class of distributions that belong to the exponential family. Thus, a unified framework addressing the main topics in the considered Bayesian model is presented. An illustrative example shows that the proposed technique can be easily applied in practice.	bayesian network	María J Rufo;Jacinto Martín;Carlos J. Perez	2014	Rel. Eng. & Sys. Safety	10.1016/j.ress.2014.06.007	reliability engineering;exponential family;bayesian probability;engineering;data mining;reliability;mathematics;statistics	SE	5.4830133971751716	-11.445870061358185	100944
9452f33eafee4fab11e0776611bfa11978f46966	application of svr with chaotic gasa algorithm to forecast taiwanese 3g mobile phone demand	support vector regression svr;cat mapping function;third generation 3g phones demand;chaotic genetic algorithm simulated annealing cgasa	Along with the increases of 3G relevant products and the updating regulations of 3G phones, 3G phones are gradually replacing 2G phones as the mainstream product in Taiwan. Taiwan will be the country with higher 3G phone penetration rate in the world. Therefore, accurate 3G phones demand forecasting is necessary for those communication related enterprises. Due to complicate market growth tendency and multi-variate competitions, different subscribers with different demand types, 3G phones demand forecasting is with highly nonlinear characteristics. Recently, support vector regression (SVR) has been successfully applied to solve nonlinear regression and time series problems. This investigation presents a 3G phones demand forecasting model which combines chaotic sequence (mapped by cat function) with genetic algorithm–simulated annealing algorithm (namely CGASA) to improve the forecasting performance. The proposed SVRCGASA employs internal randomness of chaos iterations which is with better performance in function optimization to overcome premature local optimum that is suffered by GA–SA. Subsequently, a numerical example of 3G phones demand data from Taiwan are used to illustrate the proposed SVRCGASA model. The empirical results reveal that the proposed model outperforms the other three models, namely the autoregressive integrated moving average (ARIMA) model, the general regression neural networks (GRNN) model, SVRGA model, and SVRGASA model. & 2013 Elsevier B.V. All rights reserved.	artificial neural network;autoregressive integrated moving average;autoregressive model;genetic algorithm;iteration;local optimum;mathematical optimization;mobile phone;nonlinear system;numerical analysis;randomness;signetics 2650;simulated annealing;software release life cycle;support vector machine;time series	Li-Yueh Chen	2014	Neurocomputing	10.1016/j.neucom.2013.08.010	simulation;machine learning	AI	6.948545255398199	-18.241951354338678	101420
b956b240c58d082f5113bc6c0bdd0a856527c9ff	intelligent stock selecting via bayesian naive classifiers on the hybrid use of scientific and humane attributes	belief networks;trading quantity;stock market;investment activity;intelligent stock selection;pricing;bayes methods;training;bayesian naive classifier;stock evaluation;bayesian methods;testing;investment;stock markets;investment concept learning stock evaluation bayesian naive classifier;training data;indexes;price intelligent stock selection bayesian naive classifier humane attribute scientific attribute investment activity security name trading quantity;human factors;stock markets bayes methods belief networks human factors investment pattern classification pricing;scientific attribute;humane attribute;bayesian methods predictive models stock markets classification tree analysis niobium compounds investments mathematical model machine learning algorithms hybrid intelligent systems industrial relations;prediction accuracy;pattern classification;mathematical model;concept learning;profitability;investment concept learning;price;security name	Among all kinds of investment activities, security's transaction is an important activity among all investors' involvements in the past decade. How to find out the relationships between a security's name, price, trading quantity, and/or other scientific technical indices, humane feeling, and how these factors affect the buy or the sell timing is an important condition to be a successful investor. A Bayesian naive classifier is used to decide the future trends of a stock. 107 Data records were collected, a total of 9 attributes were used in this classification process. We exclusively take one thirds (30 examples) to test the validity of the develop inference model. The model shows a plain 57% of predicting accuracy and a high estimated possibility of 86.54% without losing his money by investing wrong targets. This result is helpful for those who have great interests to make profit in a stock market of similar situations.	naive bayes classifier;timing closure	Tien-Tsai Huang;Chir-Ho Chang	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.148	pricing;concept learning;investment;computer science;machine learning;mathematical model;data mining	ML	4.383291931257433	-17.097819481849314	101541
3993a88d77f57668520e7f7a7cb90c83171d13b2	diagnosing chaos by a fuzzy classifier	variable discreta;discrete data;analisis datos;chaos;dimension correlacion;classifier system;application henon;caos;chaotic time series;donnee discrete;dimension correlation;time series;classification;correlation dimension;linear filtering;approximate reasoning;data analysis;time series analysis;membership functions;indexation;henon mapping;nonlinear dynamics;serie temporelle;membership function;serie temporal;analyse donnee;data classification;clasificacion;fuzzy classifier	"""A simple fuzzy classi""""er system seems to successfully diagnose nonlinear chaotic time series. The system contains two sub-classi""""ers, each taking a single input and produces a single output. The """"rst detects the degree of linearity in a series by classifying R2 values from linear """"ltering. The second detects the degree of nonlinearity by classifying values of a relative complexity index h. This index is the ratio of two correlation dimension estimates, one from residuals of linear """"ltering and the other from the same residuals but randomly shu%ed. It produces a ratio approaching one if the data is random, and a ratio approaching zero if the data is deterministic. The classi""""er system converts these statistics into memberships in classes and combines them to provide a symbolic characterization of dynamical structures. This system characterizes """"ve frequently investigated chaotic series: the logistic, the Henon, and tent maps which produce discrete data, and the Lorenz system and Mackey}Glass equation which produce continuous data. Classi""""cation results are consistent with expectations. ( 1999 Elsevier Science B.V. All rights reserved."""	chaos theory;complexity index;correlation dimension;deterministic algorithm;discrete mathematics;dynamical system;executable;lorenz system;map;nonlinear system;pseudorandomness;randomness;time series;world wide web	Mak A. Kaboudan	1999	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00312-6	discrete mathematics;nonlinear system;artificial intelligence;machine learning;time series;mathematics;statistics	ML	6.327836539581392	-21.695503950384992	101563
1dd3c9b12fcb12cea3561cec2e025b1f741eee99	a plea for the use of lukasiewicz triplets in the definition of fuzzy preference structures. (ii). the identity case	fuzzy set;transformacion matematica;łukasiewicz triplets;mathematical transformation;fuzzy relation;conjunto difuso;ensemble flou;triangular norm;preference modelling;technology and engineering;fuzzy preference structures;transformation mathematique;norma triangular;t norm;norme triangulaire;fuzzy relations	Abstract   It has been shown in the first part of this paper that the concept of a fuzzy preference structure is only meaningful provided that the de Morgan triplet involved contains a continuous Archimedean triangular norm having zero divisors, or hence a φ-transform of the Łukasiewicz triangular norm. In this second part, additional arguments for this statement are supplied in what we call the ‘identity case’ ( φ   τ   is the identity mapping, and the involutive negator is the standard negator). First, it is shown that the use of a continuous non-Archimedean triangular norm having zero divisors in the definition of a fuzzy preference structure indeed is possible. Secondly, using such a triangular norm implies that at least in the square   [0,   4  5  ]     2    it actually behaves as the Łukasiewicz triangular norm. Furthermore, an important transformation theorem indicates that any fuzzy preference structure with respect to a de Morgan triplet containing such a continuous non-Archimedean triangular norm having zero divisors can be transformed into a fuzzy preference structure with respect to the standard Łukasiewicz triplet. These additional arguments conclude in a convincing way our plea for the use of Łukasiewicz triplets in the definition of fuzzy preference structures.		Bernard De Baets;Bartel Van de Walle;Etienne E. Kerre	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00396-X	discrete mathematics;computer science;artificial intelligence;calculus;mathematics;t-norm;fuzzy set	NLP	0.43588404414827264	-23.185855385657224	101655
02b0982dfbe8cbf675899cc60d7adb3b3299cc5f	an application of stochastic time strength rbf neural network to forecasting spot prices of crude oil			artificial neural network;radial basis function network	Hongli Niu	2018		10.3233/978-1-61499-927-0-613		ML	8.664451564712051	-19.217448031948127	101721
525332f8bb98c0d49e1cf3418330dc30b06c999f	modelling thresholds and volatility in us ecological patents	garch;star garch;time varying;volatility;garch model;asymmetry;regime switching;gjr;ecological patents;star gjr;conditional variance;regime switching model;trends	Ecological patents have been increasing steadily over time. This paper analyses trends and volatility in ecological patents in the USA from 1975 to 1997. Germany contributed more than 10% of the total US ecological patents, and is by far the strongest foreign performer. This paper estimates a set of novel regime-switching models to investigate the time-varying nature of the conditional mean, as well as the conditional variance of the patent ratio, namely the ratio of US ecological patents to total US patents, using monthly data from January 1975 to December 1997. The regime-switching LSTAR-GARCH model is found to be optimal for modelling the ecological patent ratio. 2004 Elsevier Ltd. All rights reserved.	ecology;star model;the australian;volatility	Felix Chan;Dora Marinova;Michael McAleer	2005	Environmental Modelling and Software	10.1016/j.envsoft.2004.09.017	financial economics;autoregressive conditional heteroskedasticity;econometrics;economics;mathematics;statistics	AI	3.7764768753636253	-13.14229035500575	101726
2de0a237024d836e2a7627896bbf701d5df3581d	large earthquake occurrence estimation based on radial basis function neural networks	engineering;seismology earthquakes geophysics computing multilayer perceptrons radial basis function networks;radial basis function rbf;seismology;electrical electronic;technology;multilayer perceptrons;quiescence;southern california;physical sciences;time;earthquakes;input space;radial basis function networks;algorithm;radial basis function rbf clustering methods earthquakes interevent times neural networks nns;multiple seismicity indicators;earthquakes artificial neural networks catalogs training data data models training predictive models;science technology;geophysics computing;remote sensing;interevent times;partition;model;imaging science photographic technology;neural networks nns;large earthquake occurrence estimation predictive tool nn architecture multilayer perceptron networks proposed approach superiority significant seismic events interevent time estimation produced rbf model california earthquake catalog resulting framework performance data processing aftershock event removal reasenberg clustering technique data driven techniques earthquake model limited training data number leave one out training procedure training algorithm powerful fuzzy nn training reliable calculation main events seismicity rates input vector rbf nn models large earthquake event occurrence estimation novel scheme radial basis function neural networks;clustering methods;prediction;geochemistry geophysics;images	This paper presents a novel scheme for the estimation of large earthquake event occurrence based on radial basis function (RBF) neural network (NN) models. The input vector to the network is composed of different seismicity rates between main events, which are easy to calculate in a reliable manner. Training of the NNs is performed using the powerful fuzzy means training algorithm, which, in this case, is modified to incorporate a leave-one-out training procedure. This helps the algorithm to account for the limited number of training data, which is a common problem when trying to model earthquakes with data-driven techniques. Additionally, the proposed training algorithm is combined with the Reasenberg clustering technique, which is used to remove aftershock events from the catalog prior to processing the data with the NN. In order to evaluate the performance of the resulting framework, the method is applied on the California earthquake catalog. The results show that the produced RBF model can successfully estimate interevent times between significant seismic events, thus resulting to a predictive tool for earthquake occurrence. A comparison with a different NN architecture, namely, multilayer perceptron networks, highlights the superiority of the proposed approach.	algorithm;artificial neural network;cluster analysis;ising model;multilayer perceptron;radial (radio);radial basis function	Alex Alexandridis;Eva Chondrodima;Evangelos Efthimiou;Giorgos Papadakis;Filippos Vallianatos;Dimos Triantis	2014	IEEE Transactions on Geoscience and Remote Sensing	10.1109/TGRS.2013.2288979	partition;prediction;artificial intelligence;machine learning;data mining;mathematics;statistics;remote sensing;technology	ML	9.97390479368978	-20.38184134589609	101770
26206840a401f030bafdf4b820ea598bc1c97129	forecasting rice production in west bengal state in india: statistical vs. computational intelligence techniques		Forecasting rice production is a challenging problem in agricultural statistics. The inherent difficulty lies in demand and supply affected by many uncertain factors viz. economic policies, agricultural factors, credit measures, foreign trade etc. which interact in a complex manner. Since last few decades, Statistical techniques are used for developing predictive models to estimate required parameters. Determination of nature of rice production time series data is difficult, expensive, time consuming and involves tedious tests. In this paper, we use Interval Type n Fuzzy Auto Regressive Integrated Moving Average (ITnARIMA), Adaptive Neuro Fuzzy Inference System (ANFIS) and Modified Regularized Least Squares Fuzzy Support Vector Regression (MRLSFSVR) for prediction of Productivity Index percent (PI %) of rice production time series data and compare it with traditional Statistical tool of Multiple Regression. The accuracies of ITnARIMA and ANFIS techniques are evaluated as relatively similar. It is found that ANFIS exhibits high performance than ITnARIMA, MRLSFSVR and Multiple Regression for predicting PI %. The performance comparison shows that Computational Intelligence paradigm is a promising tool for minimizing uncertainties in rice production data. Further Computational Intelligence techniques also minimize potential inconsistency of correlations. Forecasting Rice Production in West Bengal State in India: Statistical vs. Computational Intelligence Techniques	adaptive neuro fuzzy inference system;computation;computational intelligence;computational linguistics;predictive modelling;programming paradigm;regularized least squares;support vector machine;time series;viz: the computer game	Arindam Chaudhuri	2013	IJAEIS	10.4018/ijaeis.2013100104	econometrics;engineering;artificial intelligence;operations management;machine learning	AI	7.504065386326714	-19.55984526307041	102034
95febebe4fd12dc44152c6578ea5e25cd04ad710	analysis of bridge safety assessment with correlation between measuring points for bridge health monitoring	bridge safety assessment;bridge health monitoring;measuring points;correlation	By using linear statistical coefficients such as Pearson, Spearman and Kendall’s, and nonlinear methods such as time-delayed transfer entropy and mutual information, the correlation between the observation stations of bridge health monitoring system is obtained. A model for predicting structural safety based on the correlation coefficients is established, which provides a new research method for bridge health assessment and prediction based on bridge health monitoring system.	coefficient;mutual information;nonlinear system;transfer entropy	Jianxi Yang	2011	Intelligent Automation & Soft Computing	10.1080/10798587.2011.10644199	correlation	AI	7.080126758512676	-14.382245751749208	102087
e71b1aba9e673d089ebbfc262dd9b42b5a684500	statistical arbitrage trading with wavelets and artificial neural networks	bootstrap method;financial data processing;costing stock markets financial data processing wavelet transforms neural nets;neural nets;costing;stochastic volatility model;option pricing;feature space;equity options market statistical arbitrage trading wavelets artificial neural networks option pricing derivative markets binomial tree stochastic volatility model nonlinear feature space bootstrap method delta hedged arbitrage trades;stock markets;wavelet transforms;confidence interval;binomial tree;stochastic volatility;black scholes formula;artificial neural networks pricing stochastic processes neural networks wavelet domain log normal distribution share prices costs elasticity particle measurements;profitability;artificial neural network;neural network	The paper outlines the use of an altemative option pricing scheme to perform statistical arbitrage in derivative markets. The method links a binomial tree to an innovative stochastic volatility model that is based on wavelets and artificial neural networks. Wavelets provide a convenient signalhoise decomposition of volatility in a non-linear feature space. Neural networks are used to infer future volatility levels from the wavelets feature space in an iterative manner. The bootstrap method provides 95% confidence intervals for the option prices. When used to set up delta-hedged arbitrage trades in the US equity options market, the proposed approach generates substantial profits.	algorithmic trading;artificial neural network;binomial heap;binomial options pricing model;bootstrapping (statistics);equity crowdfunding;feature vector;iteration;iterative method;neural networks;nonlinear system;point of view (computer hardware company);software deployment;turbulence;volatility;wavelet	Christopher Zapart	2003		10.1109/CIFER.2003.1196339	implied volatility;index arbitrage;confidence interval;actuarial science;feature vector;computer science;machine learning;valuation of options;black–scholes model;stochastic volatility;binomial options pricing model;artificial neural network;activity-based costing;profitability index;wavelet transform	ML	5.746114312275345	-16.650872738743608	102224
a01a77aa7e890645c7dc96b14c7e26da063f1604	an extension of statistical decision theory with information theoretic cost functions to decision fusion: part i	cost function;bayesian probability theory;probability density function;kalman filter;bayesian decision theory;decision problem;statistical decision theory;levels of abstraction;probability distribution;decision fusion;recursive algorithm;fusion rule;information fusion;information theoretic;decision rule;information theory	A review of information theory and statistical decision theory has led to the recognition that decisions in statistical decision theory can be interpreted as being determined by the similarity between the distribution of probabilities obtained from measurements and characteristic distributions of probabilities representing the members of the set of decisions. The obscure interpretation was found during a review of statistical decision theory for the special case where the cost function of statistical decision theory is an information theoretic cost function.#R##N##R##N#Additional research has found that the resulting information theoretic decision rule has a number of interesting characteristics that may have previously been recognized in terms of mathematical interest, but until now have not been recognized for their implications for information fusion. Bayesian probability theory has been criticized for problematic changes in decisions when hypotheses and decisions are reorganized to different levels of abstraction, weak justification for the selection of prior probabilities, and the need for all probability density functions to be defined. The characteristics of the information theoretic decision rule show that the decisions are less sensitive to changes in the reorganization of hypothesis and decision sets to different levels of abstraction in comparison to Bayesian probability theory. Extension of the information theoretic rule to a fusion rule (to be provided in a companion paper) will be shown to provide increased justification for the selection of prior probabilities through the adoption of Laplace’s principle of indifference. The criticism of the need for all probability density functions can be partially mitigated by arguing that the hypothesis abstraction levels can be selected so that all the probability density functions may be obtained. Further refutation of the third criticism will require that the assumption that the probability density functions are not definitively known but may be ambiguous as well and will not be pursued as a line of inquiry within the two companion papers.	decision theory	Michael B. Hurley	2003	Information Fusion	10.1016/S1566-2535(03)00045-9	kalman filter;probability distribution;probability density function;optimal decision;bayes estimator;information theory;computer science;machine learning;decision problem;pattern recognition;decision rule;mathematics;admissible decision rule;evidential decision theory;statistics;recursion	Theory	1.6448559758081425	-18.179840763310875	102243
68e492aacd7bc27e87a655adaafaffe69121d12c	on solving linear programs with the ordered weighted averaging objective	lexicographic maximin;multiple criteria;objective function;equity;ordered weighted average;weighted sums;linear programming;linear program;computational efficiency;ordered weighted averaging	The problem of aggregating multiple criteria to form overall objective functions is of considerable importance in many disciplines. The most commonly used aggregation is based on the weighted sum. The ordered weighted averaging (OWA) aggregation, introduced by Yager, uses the weights assigned to the ordered values (i.e. to the worst value, the second worst and so on) rather than to the specific criteria. This allows to model various aggregation preferences, preserving simultaneously the impartiality (neutrality) with respect to the individual criteria. In this paper we analyze solution procedures for linear programs with the OWA objective functions. Two alternative linear programming formulations are introduced and their computational efficiency is analyzed. 2002 Elsevier Science B.V. All rights reserved.	linear programming;ordered weighted averaging aggregation operator;weight function	Wlodzimierz Ogryczak;Tomasz Sliwinski	2003	European Journal of Operational Research	10.1016/S0377-2217(02)00399-5	ordered weighted averaging aggregation operator;mathematical optimization;combinatorics;linear programming;mathematics;mathematical economics;equity	AI	-1.5652760559182788	-17.61616938546548	102460
222124939c79b8cb0fb927b03ca5ee0737beb1d4	knowledge-based estimation of stockout costs in logistic systems	genetic program;inventory management;evolutionary computation;knowledge based system;neural networks;neural nets;costing;neural networks stockouts stockout costs opportunity costs knowledge based systems evolutionary computation genetic programming;gp algorithm knowledge based estimation logistic systems stockout consequence identification stockout cost quantification inventory management opportunity cost estimation decision making genetic programming approach learning opportunity cost functions case based decisions neural networks;genetic programming;decision maker;satisfiability;production engineering computing;opportunity cost;logistics;opportunity costs;stockouts;genetic algorithms;cost function artificial neural networks logistics training context reliability benchmark testing;production engineering computing costing decision making genetic algorithms inventory management knowledge based systems logistics neural nets;stockout costs;structural similarity;knowledge based systems;neural network;evolutionary computing;knowledge base	The approach introduced in this paper depicts the topic of identification and evaluation of stockout consequences, commonly denoted as stockout cost quantification. Our work is motivated by the limited number of approaches dealing with this problem and, primarily in the field of inventory management, a subsequent need for applicable methods providing reliable stockout cost parameters. We focus on the problem of estimating opportunity costs of stockouts as the most difficult cost component to be determined. Therefore, a method to elicit information by confronting relevant decision makers with representative stockout cases (a priori) is presented. Subsequently, a Genetic Programming (GP) approach for learning opportunity cost functions from these case-based decisions is introduced. It is shown on exemplary tests instances that solutions can be generated which converge to structurally similar opportunity cost functions for representative stockout items. Based on a comparison to benchmarks generated by Neural Networks, it can be concluded that the quality of solutions from the GP algorithm is satisfying.	approximation algorithm;artificial neural network;benchmark (computing);computation;converge;data mining;experiment;genetic programming;inventory;knowledge-based systems;neural network software;prospective search;simulation;support vector machine;weka	Sebastian Langton;Martin Josef Geiger	2011	2011 11th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2011.6121750	service level;opportunity cost;computer science;artificial intelligence;machine learning;data mining;management science;artificial neural network	Robotics	3.3917047486603025	-18.556191723875827	102487
d93b4f507f612964f73ef28b660ee24c21caf4bd	a value transfer gert network model for carbon fiber industry chain based on input–output table	gert network;carbon fiber industry chain;value flow;input–output table	This paper builds an input–output table for carbon fiber industry chain in accordance with input–output theory. In order to get a clear picture of the dynamic input–output process between a variety of sectors involved in the carbon fiber industry chain and the increment of value hereinto, a Graphical Evaluation and Review Technique (GERT) network model is constructed on the basis of value flow, and an analytical algorithm for the GERT network model is proposed to get the value transfer probability, the amount of value increment, and the fluctuation variance, from which the equivalence value transfer process and results between sectors are obtained. Finally, in the case study part, this paper finds out the value transfer relationship within the carbon fiber industry chain in Jiangsu Province, China based on the empirical data gotten from field investigations, and then some constructive policy-making and investment suggestions are put forward in view of the results and analyses.	algorithm;computer graphics;input/output;network model;optical fiber;quantum fluctuation;theory;total loss;turing completeness	Xiaqing Liu;Zhigeng Fang;Na Zhang	2017	Cluster Computing	10.1007/s10586-017-0960-y	constructive;network model;equivalence (measure theory);mathematical optimization;distributed computing;graphical evaluation and review technique;input/output;operations research;carbon fibers;computer science	PL	2.9416173137243438	-14.879289525190742	102718
64ae0be87fe5cf6d988eb6245b13dea809bf39e0	on a simple and efficient approach to probability distribution function aggregation	probability distribution aggregation group judgment linear feedback iteration;aggregation group judgment linear feedback iteration probability distribution;probability distribution computers fuzzy logic computational modeling optimization cybernetics decision making	In group decision making, it is inevitable that the individual decision maker’s subjectivity is involved, which causes difficulty in reaching a group decision. One of the difficulties is to aggregate a small set of expert opinions with the individual subjectivity or uncertainty modeled with probability theory. This difficult problem is called probability distribution function aggregation (DFA). This paper presents a simple and efficient approach to the DFA problem. The main idea of the proposed approach is that the DFA problem is modeled as a nonlinear function of a set of probability distribution functions, and then a linear feedback iteration scheme is proposed to solve the nonlinear function, leading to a group judgment or decision. Illustration of this new approach is given by a well-known DFA example which was solved with the Delphi method. The DFA problem is a part of the group decision problem. Therefore, the proposed algorithm is also useful to the decision making problem in general. Another contribution of the this paper is the proposed notation of systematically representing the imprecise group decision problem with the classification of imprecise information into three classes, namely incomplete information, vague information, and uncertain information. The particular DFA problem dealt with in this paper is then characterized with this general notation.	aggregate data;algorithm;decision problem;decision theory;delphi method;iteration;iterative method;nonlinear system;vagueness	Mengya Cai;Yingzi Lin;Bin Han;Changjun Liu;Wenjun Zhang	2017	IEEE Transactions on Systems, Man, and Cybernetics: Systems	10.1109/TSMC.2016.2531647	mathematical optimization;discrete mathematics;mathematical statistics;mathematics;statistics	DB	-2.163385263070345	-19.801564736500577	102793
45c6bf683211bf390eb7f7eff3fd693a9fde0464	hydrozip: how hydrological knowledge can be used to improve compression of hydrological data	data compression;streamflow;hydrology;algorithmic information theory;mopex;inference	From algorithmic information theory, which connects the information content of a data set to the shortest computer program that can produce it, it is known that there are strong analogies between compression, knowledge, inference and prediction. The more we know about a data generating process, the better we can predict and compress the data. A model that is inferred from data should ideally be a compact description of those data. In theory, this means that hydrological knowledge could be incorporated into compression algorithms to more efficiently compress hydrological data and to outperform general purpose compression algorithms. In this study, we develop such a hydrological data compressor, named HydroZIP, and test in practice whether it can outperform general purpose compression algorithms on hydrological data from 431 river basins from the Model Parameter Estimation Experiment (MOPEX) data set. HydroZIP compresses using temporal dependencies and parametric distributions. Resulting file sizes are interpreted as measures of information content, complexity and model adequacy. These results are discussed to illustrate points related to learning from data, overfitting and model complexity.	algorithm;algorithmic information theory;computer program;data compression;estimation theory;overfitting;self-information	Steven V. Weijs;Nick van de Giesen;Marc Parlange	2013	Entropy	10.3390/e15041289	data compression;computer science;theoretical computer science;machine learning;data mining;mathematics;streamflow;algorithmic information theory;statistics	ML	6.956510586510023	-21.592347012566933	102860
130a60f96f8d5b2a0f1be5cb1d55ce5389dfad0a	utilising the cross industry standard process for data mining to reduce uncertainty in the measurement and verification of energy savings	energy efficiency;measurement and verification;baseline energy modelling;data mining;conference item	This paper investigates the application of Data Mining (DM) to predict baseline energy consumption for the improvement of energy savings estimation accuracy in Measurement and Verification (M&V). M&V is a requirement of a certified energy management system (EnMS). A critical stage of the M&V process is the normalisation of data post Energy Conservation Measure (ECM) to pre-ECM conditions. Traditional M&V approaches utilise simplistic modelling techniques, which dilute the power of the available data. DM enables the true power of the available energy data to be harnessed with complex modelling techniques. The methodology proposed incorporates DM into the M&V process to improve prediction accuracy. The application of multi-variate regression and artificial neural networks to predict compressed air energy consumption in a manufacturing facility is presented. Predictions made using DM were consistently more accurate than those found using traditional approaches when the training period was greater than two months.		Colm V. Gallagher;Ken Bruton;Dominic T. J. O'Sullivan	2016		10.1007/978-3-319-40973-3_5	computer science;data science;data mining;efficient energy use	Mobile	9.449777366174812	-16.6016111132142	102933
4156346a45a5aac9e45013e88dc04d5e1972a553	empirical evidence of some stylized facts in international crude oil markets		Research on the dynamic behavior of crude oil prices has become a hot issue in recent years. Currently the study of petroleum prices is largely based on the mainstream literature of financial markets whose fundamental assumption is that returns of stock prices follow a normal distribution and price behaviors obey a so-called random walk hypothesis. This notion was first introduced by Bachelier in 1900 [1], since then it has become the essence of many asset pricing models. However, daily financial time series also provide empirical evidence that there exist fundamentally different ubiquitous properties called “stylized facts,” such as fat-tailed distribution, volatility clustering, and scaling/multiscaling features [2–12]. Another important context in this domain is the efficient market hypothesis (EMH) proposed by Fama in [13] which states that stock prices already reflect all available information useful in evaluating their value. These hypotheses have been widely criticized in the financial literature.	cluster analysis;existential quantification;fama im;image scaling;time series;volatility	Ling-Yun He;Feng Zheng	2008	Complex Systems		artificial intelligence;random walk hypothesis;financial economics;machine learning;mathematics;stylized fact;volatility clustering;financial market;capital asset pricing model;empirical evidence;normal distribution;efficient-market hypothesis	ECom	2.2725594981394153	-12.242013299506798	102940
fdc1b110cd576c9c846be40d8b3c1508faa663e2	a fuzzy weighted additive approach for stochastic fuzzy goal programming	aspiration level;matematicas aplicadas;fuzzy programming;mathematiques appliquees;modelo determinista;68n19;funcion aleatoria;possibility programming;programmation stochastique;modele deterministe;additive model;distribution function;approche chance contrainte;fuzzy goal programming;programmation possibilite;chance constrained approach;random function;independent random variables;programmation objectif;fuzzy weighted additive model;goal programming;programmation floue;applied mathematics;stochastic programming;programacion objetivo;deterministic model;programacion estocastica;stochastic fuzzy goal programming;programacion difusa;modele additif pondere flou;fonction aleatoire	In this paper, a certain structure for the stochastic fuzzy goal programming problem is presented. The goal constraints are considered stochastic and fuzzy, with fuzziness in inequalities, while the aspiration levels are assumed independent random variables with known distribution functions. The chance-constrained approach is utilized to transform the stochastic constraints to their deterministic equivalent. On the other hand, when any preemptive priority structure can not be determined, a fuzzy weighted additive approach is proposed, through the possibility programming technique that relies on the a-cut approach, to formulate a deterministic-crisp model. The suggested approach is illustrated by a numerical example. 2003 Elsevier Inc. All rights reserved.	goal programming;numerical analysis;preemption (computing);randomness;utility functions on indivisible goods	Maged George Iskander	2004	Applied Mathematics and Computation	10.1016/S0096-3003(03)00734-3	stochastic programming;mathematical optimization;combinatorics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;distribution function;machine learning;random function;deterministic system;goal programming;mathematics;additive model;fuzzy set operations;statistics	AI	0.4520112863808629	-16.495550058844024	102995
68af8b0cec8f95bf9df477c00721854f113410aa	the bp neural network optimizing design model for agricultural information measurement based on multistage dynamic fuzzy evaluation	design model;multistage dynamic fuzzy judgment;neural nets;evaluation method;backpropagation;fuzzy set theory;agricultural information measurement;linear agricultural information degree evaluation;neural nets agriculture backpropagation fuzzy set theory;bp neural network;optimal design;agriculture;multistage dynamic fuzzy judgment bp neural network design model agricultural information measurement multistage dynamic fuzzy evaluation china linear agricultural information degree evaluation;china;evaluation model;neural network;neural networks design optimization fuzzy neural networks neurons power measurement electric variables measurement power generation economics cities and towns uncertainty fuzzy systems;multistage dynamic fuzzy evaluation	The agricultural information level is on the initial stage in China, so we should pay more attention to its construction, but how to measure the agricultural information degree is a major issue. This paper overcomes the shortcoming of traditional linear agricultural information degree evaluation method, proposes a BP neural network evaluating method based on the multistage dynamic fuzzy judgment, takes the multistage dynamic fuzzy judgment as the sampling foundation, uses the BP neural network principle to establish evaluation model. This method not only can exert the unique advantages ofBP neural network, but also overcome the difficulty of seeking the high grade training sample data. The agricultural information degree evaluation of 10 cities in Jilin province indicates that the method to evaluate the agricultural information degree is stable and reliable.	artificial neural network;multistage amplifier;optimizing compiler;sampling (signal processing)	Zhibin Liu;Li Bai	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.29	agriculture;computer science;artificial intelligence;optimal design;backpropagation;machine learning;data mining;china;artificial neural network	AI	5.719095526474597	-21.12246868583532	103115
3966958574f402c89d4faeabc8bee5fcec8723be	mobile communication service income prediction method based on grey buffer operator theory	practical applications of grey models;grey systems modelling and prediction	Purpose – The mobile communication industry in China is vulnerable to competition, industry regulation, macroeconomy and so on, which leads to service income’s volatility and non-stationarity. Traditional income prediction models fail to take account of these factors, thus resulting in a low precision. The purpose of this paper is to to set up a new mobile communication service income prediction model based on grey system theory to overcome the inconformity between traditional models and qualitative analysis. Design/methodology/approach – At first, mobile telecommunication service income is divided into number of users (NU) and average revenue per user (ARPU) prediction, respectively. Then, grey buffer operators are introduced to preprocess the time series according to their features and tendencies to eliminate the effect of shock disturbance. As a result, two grey models based on GM(1, 1) are constructed to forecast NU and ARPU, and thus the service income is obtained. At last, a case on Zhujiang mobile communication company is studied. The result proves that the proposed method is not only more accurate, but also could discover the turning point of income. Findings – The results are convincing: it is more effective and accurate to employ grey buffer operator theory to predict the mobile communication service income compared with other methods. Besides, this method is applicable to cases with less data samples and faster development. Practical implications – It’s common to come across a system with less data and poor information. At this case, the grey prediction method exposed in the paper can be used to forecast the future trend which will give the predictors advice to achieve fine outcomes. Buffer operators can reduce the effect of shock disturbance and the GM(1, 1) model has the advantages of exploiting information using only a couple of data. Originality/value – Considering the fast development of China’s mobile communication in recent years, only limited data can be acquired to predict the future, which will definitely reduce the prediction precision using traditional models. The paper succeeds in introducing GM(1, 1) model based on grey buffer operators into the income prediction and the outcome proves that it has higher prediction precision and extensive application.	exploit (computer security);futures studies;mobile phone;preprocessor;stationary process;systems theory;time series;volatility	Pinpin Qu	2014	Grey Systems: T&A	10.1108/GS-12-2013-0037	simulation;engineering;operations management;operations research	Web+IR	5.425407987287838	-15.87017694164332	103163
b5283489c0cc98ccddf7e7052915f16146ddde15	agfsm: an new fsm based on adapted gaussian membership in case retrieval model for customer-driven design	suitable coefficient;adapted gaussian membership;design reuse;case retrieval;similarity measurement;computational complexity;customer driven design;power transformer;genetic algorithm;retrieval model;similarity measure	0957-4174/$ see front matter 2010 Elsevier Ltd. A doi:10.1016/j.eswa.2010.07.067 * Corresponding author. Tel.: +86 21 34206552; fax E-mail address: hujie@sjtu.edu.cn (J. Hu). In customer-driven design, reusing the design experiences of solving previous problems is a potential methodology, and the case retrieval (CR) process is a major step process, in which similarity measurement (SM) among cases is its core. However, performing the CR model with high retrieval accuracy and low computational complexity for the fuzzy, vague and imprecision customer requirements is a huge challenge for researchers and few studies attempt to research the CR model for customer-driven design. This paper proposes a new fuzzy SM (FSM) method in CR model which based on adapted Gaussian membership for customer driven design, i.e., AGFSM. In AGFSM, the adapted Gaussian membership is established based on demand information, meanwhile, the adjustment parameter is optimized via genetic algorithm (GA). Subsequently, the corresponding local similarity (LS) and global similarity (GS) are obtained. In order to find the more proper design solution, the similar case with higher suitable coefficient (SC), instead of similarity degree, is recommended as the finally design solution. Furthermore, we take power transformer design as an example to illustrate the process of the CR model with AGFSM and compare with other FSM methods to validate its superiority. As a result, the AGFSM is more efficient than previous FSM methods on the basis of retrieval accuracy and computational complexity. 2010 Elsevier Ltd. All rights reserved.	coefficient;computational complexity theory;fax;genetic algorithm;least squares;requirement;roland gs;software release life cycle;transformer;vagueness	Jin Qi;Jie Hu;Ying-hong Peng;Wei-ming Wang;Zhenfei Zhang	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.07.067	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;computational complexity theory;transformer	AI	3.180912610105955	-20.191752660279263	103227
b5b4ac95127d11b2dff72735b2ea55d2ec2235d3	aggregation of partial ordinal rankings: an interval goal programming approach	incomplete ordinal scale;aggregation;goal programming;group decision making;interval goal programming	This paper shows how interval goal programming can be a useful tool for aggregating incomplete individual patterns of preference in a group decision-making problem. The different consensus solutions obtained have a precise preferential meaning and hold interesting properties. The way in which the methodology functions is illustrated with the help of a numerical example.#R##N#Scope and purpose#R##N#This paper deals with group decision-making problems where decision makers are not able to give a complete ranking of alternatives but partial orders. The aggregation of these partial orders is addressed within a distance-based framework. The proposed approach is made operational with the help of an Interval Goal Programming formulation.	goal programming;ordinal data	Jacinto González-Pachón;Carlos Romero	2001	Computers & OR	10.1016/S0305-0548(00)00010-1	mathematical optimization;group decision-making;computer science;goal programming;data mining;mathematics	AI	-3.402899043867596	-19.88969857245861	103402
dde8672d1ec8c921b0b74049e55df20ac8c6703c	passenger hailing safety pasw modeler and big data statistical analysis study		This paper presents a study based on passenger hailing safety big data collection and PASW statistical analysis. A regression analysis model was used on data collected to study whether two or more variables were correlated. Changes in the direction and strength of correlation, and a regression analysis of arguments given estimates for the conditional expectation of the dependent variables, fully revealed the complex dependence. In addition, the RSS, which reflects the influence of random errors on dependent variables, measured the influence of the variance of factors other than passengers’ hailing safety, data collection, and statistics analysis. In the linear regression analysis model, to improve traffic safety prediction and control, R2 represents the contribution rate of analytic variables to a forecast change.	big data	Sachula Meng;Andy C Huang;T. J. Huang;Jianjun Chen;J. S. Pan	2016		10.1007/978-3-319-48499-0_2	simulation;data science;data mining	ML	6.935076395160607	-14.739978399031624	103433
a9ee78f29bca0e287bee0d808774dde22efdfac4	continuity focused choice of maxima: yet another defuzzification method	eleccion;control theory;fuzzy controller;fuzzy set;fuzzy programming;efficiency;logique floue;conjunto difuso;logica difusa;ensemble flou;theorie commande;fuzzy logic;eficacia;mathematical programming;efficacite;programmation floue;choix;programmation mathematique;programacion matematica;choice;defuzzification;programacion difusa	In this paper, we present a new defuzzification method for a Mamdani fuzzy controller. The method selects an element of the core of the fuzzy output set as defuzzification value, in a way that the continuity of the controller can be preserved. However, in order for this continuity to be guaranteed, some constraints on the components of the controller have to be taken into account. Except for some constraints of continuity, convexity and points of intersection for the input and output linguistic terms, we will define the property of linguistic continuity that has to be fulfilled by the controller. Further we show that the new defuzzification method is computationally very efficient and we point out how the fuzzy controller can be tuned by modifying the output linguistic terms. Finally, we also give two variants of the proposed formula.	defuzzification;maxima;scott continuity;yet another	Werner Van Leekwijck;Etienne E. Kerre	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(00)00025-7	fuzzy logic;mathematical optimization;defuzzification;computer science;artificial intelligence;control theory;mathematics;efficiency;fuzzy set;algorithm	Logic	2.44302817938466	-22.85537502523952	103549
051af4a886e2a40bf2d7cecbd077d06f8403e18b	the apiobpcs deziel and eilon parameter configuration in supply chain under progressive information sharing strategies	customer service level worsens;low information sharing level;smoothing parameters increase;different smoothing parameter level;production control system smoothing;smoothing parameter variation;deeper information;smoothing parameter;supply chain;eilon parameter configuration;information sharing level increase;apiobpcs deziel;progressive information;supply chain management;simulation;mean squared error;information management;differential equation;differential equations;pipelines	The aim of this paper is to investigate how different smoothing parameter levels of the Automatic Pipeline Inventory and Order Based Production Control System smoothing replenishment rule impact on the bullwhip dampening efficacy, under progressive information sharing strategies. The main results of this work are: (1) The smoothing parameter variations significantly impact on performance of the supply chains characterised by low information sharing level. (2) As smoothing parameters increase, the supply chain process performance improves and the customer service level worsens. This opposite trend noticeably decreases as information sharing level increases. (3) Amongst the bullwhip dampening techniques, deeper information sharing weights more than the value of smoothing parameters. The analysis is performed through continuous time differential equation modelling.	control system;smoothing	Salvatore Cannella;Elena Ciancimino	2008	2008 Winter Simulation Conference		supply chain management;simulation;computer science;information management;differential equation	AI	1.6666343379276998	-10.022563564570767	103568
5424812ff72e5516fd75e86b34879292c05f5a39	owa operator-based fuzzy comprehensive assessment of transformer condition	fuzzy sets;condition assessment;entropy;owa operator	The indicators system and indicators normalization method for the condition assessment of Transformer is developed and membership function of the indicators is established. The establishment of Expert Weight is decided jointly by subjective weight and objective weight which is based on entropy weight thoughts, while the indicators weight is gained by the weight which is derived from the standard Analytic Hierarchy Process and the expert weight. A comprehensive condition assessment model of transformer based on OWA(Ordered Weighted Averaging) operator and fuzzy assessment is proposed and divided into two layers, fuzzy polymerization is adopted for the second layer sub-indicators to get the fuzzy membership of the first layer main indicators, while OWA operator polymerized with the final comprehensive condition assessment of the transformer is adopted for the main indicators of the first layer. In order to fully consider the impact of indicators weight and membership on the condition assessment result, a fuzzy polymerization conversion function based on OWA operator is introduced in the model, so as to integrate the attribute information from various important information sources. Case analysis indicates that the condition assessment of transformer can be carried out by this method; the reasonable and objective conclusion is close to the true condition of transformer.	database normalization;transformer	Liping Shi;Hongxia Xie	2014	JCP	10.4304/jcp.9.4.956-965	entropy;computer science;artificial intelligence;data mining;mathematics;fuzzy set	AI	-3.667122321448209	-21.443483962443405	103671
64b1e94455e2e792aad84488933956ddf4ddf258	a new emission model including on-ramps for two-class freeway traffic control	freeway management systems;ramp metering two class freeway traffic control two class macroscopic emission model pollutant emissions on ramp emissions;traffic queuing;exhaust gases;ramp metering;road traffic control;traffic congestion;vehicles computational modeling traffic control acceleration mathematical model simulation;emissions models;freeways;on ramps	The main objective of this paper is to propose a new two-class macroscopic emission model to describe the pollutant emissions produced by freeway traffic. The innovative aspect of the proposed model consists in considering the on-ramp emissions, which are explicitly modeled for different traffic scenarios. Next, a two-class local controller based on a ramp metering is reported with the aim of minimizing emissions and congestion in the freeway system. The relevance of the on-ramp emission model is in this way highlighted, since ramp metering may lead to creation of queues at the entering on-ramps and hence a concentration of pollutants on these on-ramps. Simulation results show the effectiveness of the proposed control strategy for a case of study.	control theory;freeway;network congestion;ramp simulation software for modelling reliability, availability and maintainability;relevance	Cecilia Pasquale;Shuai Liu;Silvia Siri;Simona Sacone;Bart De Schutter	2015	2015 IEEE 18th International Conference on Intelligent Transportation Systems	10.1109/ITSC.2015.189	simulation;engineering;automotive engineering;transport engineering	Robotics	9.4358618475908	-11.292285504273023	103726
c0856fbf13e9fb339aeb9caf6a384d8624b0047a	approaches to interval intuitionistic trapezoidal fuzzy multiple attribute decision making and their application to evaluating the cluster network competitiveness of smes	multiple attribute decision making madm;interval intuitionistic trapezoidal fuzzy numbers;cluster network competitiveness;weight information;maximizing deviation method;interval intuitionistic trapezoidal fuzzy weighted averaging iitfwa operator	With respect to multiple attribute decision making problems with interval intuitionistic trapezoidal fuzzy information, some operational laws of interval intuitionistic trapezoidal fuzzy numbers, score function and accuracy function of interval intuitionistic trapezoidal fuzzy numbers are introduced. An optimization model based on the maximizing deviation method, by which the attribute weights can be determined, is established. For the special situations where the information about attribute weights is completely unknown, we establish another optimization model. By solving this model, we get a simple and exact formula, which can be used to determine the attribute weights. We utilize the interval intuitionistic trapezoidal fuzzy weighted averaging (IITFWA) operator to aggregate the interval intuitionistic trapezoidal fuzzy information corresponding to each alternative, and then rank the alternatives and select the most desirable one(s) according to the score function and accuracy function. Finally, an illustrative example for evaluating the cluster network competitiveness of small and medium-sized enterprises is given to verify the developed approach and to demonstrate its practicality and effectiveness.	aggregate data;attribute grammar;computer cluster;intuitionistic logic;mathematical optimization	Xinhua Hu;Xumei Zhang	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141381	mathematical optimization;discrete mathematics;data mining;mathematics	ML	-2.624848045056079	-20.444395131122132	103836
187f31fee378a5416c63a9c04b8c0adc81f4f440	computational intelligence techniques for solar photovoltaic system applications	solar photovoltaic energy fuzzy rule based classifiers genetic algorithms pattern classification;solar power stations genetic algorithms photovoltaic power systems smart power grids;solar photovoltaic energy;solar power stations;fuzzy rule based classifiers;smart power grids;pattern classification;production fuzzy systems photovoltaic systems genetic algorithms electricity arrays pragmatics;genetic algorithms;photovoltaic power systems;electricity management computational intelligence techniques solar photovoltaic system fuzzy classifier energy production solar photovoltaic installations fuzzy models genetic algorithm fuzzy system electric grid smart grid decision making electricity production	In this paper we propose a fuzzy classifier of energy production in solar photovoltaic installations based on the values of some environmental parameters. The classifier is built through a hierarchical process and is obtained by merging basic fuzzy models built on input domain regions increasingly smaller, as the result of the construction of appropriate grids on the input domain. The system parameters are optimized by means of a genetic algorithm. The interpretability of the fuzzy system helps the electric grid (e.g., smart grid) operator have fast and easy understanding of the energy production, thus allowing easier and faster decision making about electricity production and management. The achieved results show an average correct classification rate of 97.38% with a maximum of 97.91%.	computational intelligence;fuzzy concept;fuzzy control system;fuzzy logic;fuzzy rule;genetic algorithm;rule-based system;software release life cycle	Eleonora D'Andrea;Beatrice Lazzerini	2012	2012 Sustainable Internet and ICT for Sustainability (SustainIT)		control engineering;electronic engineering;engineering;artificial intelligence;grid-connected photovoltaic power system	AI	9.783662102575065	-16.441582566718353	103994
7bf665c0d09af78962216e07b5cfdb01da373fe3	effects of network structure on the performance of a modeled traffic network under drivers' bounded rationality		We propose a minority route choice game to investigate the effect of the network structure on traffic network performance under the assumption of drivers’ bounded rationality. We investigate ring-and-hub topologies to capture the nature of traffic networks in cities, and employ a minority gamebased inductive learning process to model the characteristic behavior under the route choice scenario. Through numerical experiments, we find that topological changes in traffic networks induce a phase transition from an uncongested phase to a congested phase. Understanding this phase transition is helpful in planning new traffic networks.	agent-based model;experiment;freeway;nash equilibrium;network congestion;network performance;network topology;nonlinear system;numerical analysis;optimization problem;preference learning;rationality;simulation;usb hub	Toru Fujino;Yu Chen	2017	CoRR		phase transition;simulation;traffic generation model;computer science;network topology;bounded rationality;network performance	AI	8.841148034059962	-10.532143751812942	104042
8d27e7f2e5a1c3d7c42c5126fee4ab932102ac01	modeling and analysis of network security situation prediction based on covariance likelihood neural	situation prediction;network security;covariance;neural	Security situation is the premise of network security warning. For lack of self-learning on situation data processing in existing complex network, a modeling and analysis of network security situation prediction based on covariance likelihood neural is presented. With the introduction of the error covariance likelihood function, and considering the impact of sample noise, the network security situation prediction model using the situation sequences as input sequences, and in the back-propagation to achieve the parameters adjustment. Results show that the model can take advantage of the relationship characteristics between the complexity and efficiency in complex neural networks, and the method has good performance of situation prediction.	artificial neural network;backpropagation;complex network;network security;software propagation	Chenghua Tang;Xin Wang;Reixia Zhang;Yi Xie	2011		10.1007/978-3-642-24553-4_11	computer science;covariance;network security;machine learning;pattern recognition;data mining;artificial neural network;statistics	ML	9.233126011611407	-21.612059169175556	104203
01d44c9d0782d5ae783a3bf7f2f191cf9bfd2e20	a high precision global prediction approach based on local prediction approaches	forecasting;residual corrections;fourier series;time series;matrix algebra;indexing terms;matrices;markov fourier gray model;forecasting theory;model free estimators;matrix algebra forecasting theory time series markov processes fourier series;prediction model;markov processes;time series prediction;fuzzy model;neural network;markov matrices;prediction models;global prediction;predictive models training data neural networks accuracy fuzzy systems fuzzy neural networks fourier series linearity information management computer science;residual corrections prediction models time series global prediction markov fourier gray model markov matrices model free estimators	Traditional model-free prediction approaches, such as neural networks or fuzzy models use all training data without preference in building their prediction models. Alternately, one may make predictions based only on a set of the most recent data without using other data. Usually, such local prediction schemes may have better performance in predicting time series than global prediction schemes do. However, local prediction schemes only use the most recent information and ignore information bearing on far away data. As a result, the accuracy of local prediction schemes may be limited. In this paper, a novel prediction approach termed as the Markov–Fourier gray Model (MFGM) is proposed. The approach builds a gray model from a set of the most recent data and a Fourier series is used to fit the residuals produced by this gray model. Then, the Markov matrices are employed to encode possible global information generated also by the residuals. It is evident that MFGM can provide the best performance among existing prediction schemes. Besides, we also implemented a short-term MFGM approach, in which the Markov matrices only recorded information for a period of time instead of all data. The predictions using MFGM again are more accurate than those using short-term MFGM. Thus, it is concluded that the global information encoded in the Markov matrices indeed can provide useful information for predictions.	artificial neural network;encode;fuzzy concept;markov chain;stochastic matrix;time series	Shun-Feng Su;Chan-Ben Lin;Yen-Tseng Hsu	2002	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/TSMCC.2002.806745	econometrics;computer science;machine learning;time series;mathematics;predictive modelling;artificial neural network;statistics	ML	8.649333796695657	-21.049370865550706	104249
97556a948c088fbc1e10e41963eebec8953c80fe	efficient non linear time series prediction using non linear signal analysis and neural networks in chaotic diode resonator circuits	second order;prediction method;non linear signal analysis;neural networks;chaos;non linear time series;signal analysis;diode;time series;backpropagation;correlation dimension;time series analysis;prediction;back propagation;deterministic chaos;artificial neural network;neural network	A novel non linear signal prediction method is presented using non linear signal analysis and deterministic chaos techniques in combination with neural networks for a diode resonator chaotic circuit. Multisim is used to simulate the circuit and show the presence of chaos. The Time series analysis is performed by the method proposed by Grasberger and Procaccia, involving estimation of the correlation and minimum embedding dimension as well as of the corresponding Kolmogorov entropy. These parameters are used to construct the first stage of a one step / multistep predictor while a back-propagation Artificial Neural Network (ANN) is involved in the second stage to enhance prediction results. The novelty of the proposed two stage predictor lies on that the backpropagation ANN is employed as a second order predictor, that is as an error predictor of the non-linear signal analysis stage application. This novel two stage predictor is evaluated through an extensive experimental study.		M. P. Hanias;Dimitris A. Karras	2007		10.1007/978-3-540-73435-2_26	computer science;artificial intelligence;backpropagation;machine learning;time series;artificial neural network	ML	9.09985288781953	-22.274499109835407	104447
5758ef296d76885035ce187702fd2b1a32f378a1	work-in-progress: maximizing model accuracy in real-time and iterative machine learning		As iterative machine learning (ML) (e.g. neural network based supervised learning and k-means clustering) becomes more ubiquitous in our daily life, it is becoming increasingly important to complete model training quickly to support real-time decision making, while still achieving high model accuracy (e.g. low prediction errors) that is critical for profits of ML tasks. Motivated by the observation that the small proportions of accuracy-critical input data can contribute to large parts of model accuracy in many iterative ML applications, this paper introduces a system middleware to maximize model accuracy by spending the limited time budget on the most accuracy-related input data. To achieve this, our approach employs a fast method to divide the input data into multiple parts of similar points and represents each part with an aggregated data point. Using these points, it quickly estimates the correlations between different parts and model accuracy, thus allowing ML tasks to process the most accuracy-related parts first. We incorporate our approach with two popular supervised and unsupervised ML algorithms on Spark and demonstrate its benefits in providing high model accuracy under short deadlines.	algorithm;artificial neural network;cluster analysis;data point;iterative and incremental development;iterative method;k-means clustering;machine learning;middleware;real-time clock;real-time transcription;spark;supervised learning	Rui Han;Fan Zhang;Lydia Y. Chen;Jianfeng Zhan	2017	2017 IEEE Real-Time Systems Symposium (RTSS)	10.1109/RTSS.2017.00055	supervised learning;work in process;spark (mathematics);cluster analysis;artificial neural network;computer science;machine learning;artificial intelligence;middleware	Embedded	7.725792452343228	-22.388541769730352	104532
d0e9d5a77afb47f5bf58d821664930e852510b7e	comparative advantage and disadvantage in dea	comparative disadvantage;strategic alliance;strategy;comparative advantage;data envelopment analysis;frontier;data envelope analysis	In this paper, we define comparative advantage and disadvantage within the framework of Data Envelopment Analysis (DEA). We develop models that address the measurement of comparative advantage and disadvantage entirely in terms of proportional changes in levels of output and input activities. The models allow inclusion of explicit limits on admissible tradeoffs across both types of activities. The assessment of comparative advantage and disadvantage establishes a framework for competitor analysis and the feasibility and desirability of strategic alliances.	data envelopment analysis	Agha Iqbal Ali;Catherine S. Lerme	1997	Annals OR	10.1023/A:1018929228294	economics;operations management;international trade;data envelopment analysis;welfare economics	SE	-2.111041387309948	-13.873812057402262	104547
0d4d51c8ca7b0600844294ce357eb0c22d12883f	on the distributivity between t-norms and t-conorms	engineering;fuzzy set;procesamiento informacion;fuzzy measure;conjunto difuso;ensemble flou;triangular norm;ingenierie;information processing;ingenieria;sistema difuso;systeme flou;traitement information;fuzzy system	The main result presented here concerns the distributivity between triangular norms and conorms understood in an extended meaning, that is, maps defined on an interval [a,b] instead of the classical interval [0,1]. As a secondary result, the bidistributivity has been used to characterize the union and the intersection of fuzzy sets by means of algebraic conditions.	t-norm	Carlo Bertoluzza;Viviana Doldi	2004	Fuzzy Sets and Systems	10.1016/j.fss.2003.10.034	discrete mathematics;information processing;computer science;artificial intelligence;mathematics;fuzzy set;algorithm;fuzzy control system	Robotics	0.9932744770273001	-22.684254611794636	104960
1ac4795eb1cb692f69a37ddc3b19d0ea32d54d8a	the performance of alternative exchange rate regimes and their countries' condition: matching analysis and selection model building	selection exchange rate regime performance match;exchange rates economic indicators accuracy predictive models decision trees neural networks;match;neural nets;performance;selection;exchange rates;neural nets decision trees exchange rates learning artificial intelligence;exhaustive chaid decision tree exchange rate regime performance model country condition factors exchange rate regime selection model data collection currency crisis period removal matching relationship analysis exhaustive prune neural network;learning artificial intelligence;decision trees;exchange rate regime	There is much discussion on the performance and selection of exchange rate regimes, and it is still in controversy. For the failure in the works, we collect more data including recent years and removing the periods of the currency crisis, then analyze the matching relationship between the exchange rate regimes and the condition of countries in the exhaustive CHAID decision tree, and build the exchange rate regime selection model in the exhaustive prune neural network. The conclusion shows the good performance of exchange rate regimes need different condition of countries to match, especially the integrate effect of condition factors is more important, then we build the exchange rate regime selection model in the exhaustive prune neural network, and the comparison and robust testing shows it is effective.	artificial neural network;decision tree	Haizhen Yang;Yunpeng Song;Kun Guo	2012	2012 IEEE 12th International Conference on Data Mining Workshops	10.1109/ICDMW.2012.15	selection;performance;computer science;artificial intelligence;machine learning;decision tree;artificial neural network	DB	5.613541601972385	-19.131797487866713	105069
e33f8c29fbbb46572bd345b1fd9080de15990af6	stability, indistinguishability and small numbers	fuzzy set;fuzzy logic;functional equation;probabilistic metric space	We describe a model for the fuzzy sets “near zero” (small numbers) and we show how to derive some T-indistinguishabilities in the real line, which are related to the model considered in probabilistic metric spaces. We revisite the concept of stability for functional equations arising in Fuzzy Logic by using the “near zero”’ model.	fuzzy logic;fuzzy set	Claudi Alsina;Enric Trillas	2005			fuzzy logic;combinatorics;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;t-norm;fuzzy set;fuzzy associative matrix;fuzzy set operations	Theory	0.19028271901170504	-22.060412507514634	105185
9e75cf757c1d6353f8c378c14bbeb1f1954dd178	fuzzy cognitive map model for supervisory manufacture systems	supervisory manufacture system;fuzzy cognitive map model;fuzzy cognitive maps;fuzzy cognitive map;soft computing	A new approach to modeling the supervisor of manufacturing systems is proposed which it is based on Fuzzy Cognitive Map (FCM) Theory. In this paper the description and the construction of Fuzzy Cognitive Maps are examined. A two level control structure for manufacturing systems, where Supervisor is modeled as a FCM, is presented. Then, an FCM model for a process example is constructed and a supervisor consisted of two FCMs that are used for failure detection and decision analysis, is proposed. The application of soft computing methodologies such as FCM, in the field of manufacturing systems may contribute in the development of more autonomous and intelligent manufacturing systems.	fuzzy cognitive map	Chrysostomos D. Stylios;Peter P. Groumpos	1998			control engineering;fuzzy cognitive map;engineering;artificial intelligence;operations management	Robotics	-2.2149987862561695	-16.596274810900425	105190
a4c6f912905a08db319af5d265e002fdd518c613	the rationality of punishment - measuring the severity of crimes: an ahp-based orders-of-magnitude approach	decision aid;ahp;multivariate measurement;crime and punishment	We propose an innovative AHP-based model to assess the severity of the harms a criminal commits to society in a comprehensive and coherent way. Different from the traditional approach of structuring alternatives into one level, we organize the alternatives into multiple levels of that hierarchy. This arrangement and evaluation of alternatives differs from one criterion to another, which adds to the sophistication of the task when assessing numerous heterogeneous criminal activities. Structuring multifaceted decisions with the proposed model enables us to better deal with its inherent complexity. Our approach can systematically tackle problems of widespread orders of magnitude of criteria and alternatives. When the sizes are actually very small or very large, the accuracy of rating alternatives one at a time is very low. With the proposed method, we are able to obtain priorities with greater precision.	rationality	Thomas L. Saaty;Müjgan Sagir Özdemir;Jennifer S. Shang	2015	International Journal of Information Technology and Decision Making	10.1142/S0219622014500850	analytic hierarchy process;economics;computer science;artificial intelligence;marketing;operations management;data mining;management;operations research;welfare economics	Robotics	-3.8505299275858693	-11.53282902999123	105208
8ef6d0b45976939c9cd31f912fce5c608e3cfb48	a novel technical analysis-based method for stock market forecasting		Owing to the dynamic changes of the stock market and numerous influences on stock prices, assessing stock prices has become increasingly difficult. Furthermore, when dealing with information on stocks, people tend to amplify the importance of available and self-correlative information, a habit that runs contrary to objective and reasonable investment decision-making. Therefore, how to use effective stock information to assist investors in making stock investment decisions is a major topic in stock investment. This study develops a novel technical analysis method for stock market forecasting to effectively promote forecasting accuracy, which can help investors to increase their decision support quality and profitability. Specifically, this study involves the following tasks: (1) design a technical analysis-based stock market forecasting process, (2) develop techniques related to technical analysis-based stock market forecasting, and (3) demonstrate and evaluate the developed technical analysis-based method for stock market forecasting. In developing techniques associated with the technical analysis-based stock market forecasting method, the techniques involve trend-based stock classification, adaptive stock market indicator selection, and stock market trading signal forecasting.		Yuh-Jen Chen;Yuh-Min Chen;Shiang-Ting Tsao;Shu-Fan Hsieh	2018	Soft Comput.	10.1007/s00500-016-2417-2	finance;machine learning;stock obsolescence;stock market;artificial intelligence;market capitalization;market depth;build to stock;investment decisions;demand forecasting;technical analysis;computer science	NLP	4.94667586260148	-17.402115927004893	105281
cd0c307816b1dcf75b003ab4a9e238563aeed99b	a fuzzy programming approach for bilevel stochastic programming		This article presents a fuzzy programming (FP) method for modeling and solving bilevel stochastic decision-making problems involving fuzzy random variables (FRVs) associated with the parameters of the objectives at different hierarchical decision-making units as well as system constraints. In model formulation process, an expectation model is generated first on the basis of the fuzzy random variables involved with the objectives at each level. The problem is then converted into a FP model by considering the fuzzily described chance constraints with the aid of applying chance constrained methodology in a fuzzy context. After that, the model is decomposed on the basis of tolerance ranges of fuzzy numbers associated with the parameters of the problem. To construct the fuzzy goals of the decomposed objectives of both decision-making levels under the extended feasible region defined by the decomposed system constraints, the individual optimal values of each objective at each level are calculated in isolation. Then, the membership functions are formulated to measure the degree of satisfaction of each decomposed objectives in both the levels. In the solution process, the membership functions are converted into membership goals by assigning unity as the aspiration level to each of them. Finally, a fuzzy goal programming model is developed to achieving the highest membership degree to the extent possible by minimizing the under deviational variables of the membership goals of the objectives of the decision makers (DMs) in a hierarchical decision-making environment. To expound the application potentiality of the approach, a numerical example is solved.		Nilkanta Modak;Animesh Biswas	2012		10.1007/978-81-322-1602-5_14	inductive programming	Robotics	-2.455231145924983	-17.30270162939884	105395
5af8d38edabc42c10869f4a161127f0acdb05ca4	using tuneable fuzzy similarity in non-metric search	fuzzy similarity search model tuneable fuzzy similarity nonmetric search spaces answering queries triangle inequality distance inequality tuneable fuzzy conjunctor indexing techniques;metric space;range query;indexing application software software engineering computer science upper bound multimedia databases implants force measurement turning fuzzy logic;answering queries;query processing;triangle inequality;data mining;fuzzy set theory;fuzzy logic;non metric search fuzzy logic indexing;distance inequality;tuning;estimation;indexing;nonmetric search spaces;tuneable fuzzy conjunctor;tuneable fuzzy similarity;indexation;query processing fuzzy set theory indexing;indexing techniques;search problems;non metric search;extraterrestrial measurements;fuzzy similarity search model	We propose an alternate method for indexing data for answering queries in non-metric spaces. The traditional use of distance and triangle inequality is substituted with the use of fuzzy similarity fulfilling the transitivity property with a tuneable fuzzy conjunctor. In a non-metric space it is still possible that there is a fuzzy conjunctor such that transitivity holds and usual indexing techniques based on pivots for range queries can be applied.	range query (data structures);social inequality;vertex-transitive graph	Peter Vojtás;Alan Eckhardt	2009	2009 Second International Workshop on Similarity Search and Applications	10.1109/SISAP.2009.18	fuzzy logic;range query;search engine indexing;estimation;discrete mathematics;metric space;computer science;theoretical computer science;triangle inequality;data mining;mathematics;geometry;fuzzy set;fuzzy set operations;statistics	DB	-1.9367206540465092	-22.579611801457474	105438
18c8de1937c1f9ea80d22b1237bca57604b4b538	a decision-theoretic approach to data mining	information systems;information extraction;investment data mining decision making knowledge discovery systems classification decision theory cost sensitive decisions actionability large databases probability linear programming;information science;indexing terms;data mining;data mining decision making databases investments data acquisition microprocessors transducers data analysis product design marketing and sales;classification;investment;decision problem;journal article paginated;business data processing;decision theory;decision theoretic;linear programming;artificial intelligence;evaluation;computer science;learning science;point of view;economics of information;linear programming data mining decision theory classification business data processing investment;information analysis;knowledge discovery	In this paper, we develop a decision-theoretic framework for evaluating data mining systems, which employ classification methods, in terms of their utility in decision-making. The decision-theoretic model provides an economic perspective on the value of “extracted knowledge,” in terms of its payoff to the organization, and suggests a wide range of decision problems that arise from this point of view. The relation between thequality of a data mining system and the amount of investment that the decision maker is willing to make is formalized. We propose two ways by which independent data mining systems can be combined and show that the combined data mining system can be used in the decision-making process of the organization to increase payoff. Examples are provided to illustrate the various concepts, and several ways by which the proposed framework can be extended are discussed.	data mining;decision problem;decision theory	Yuval Elovici;Dan Braha	2003	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/TSMCA.2003.812596	concept mining;decision theory;information science;investment;computer science;linear programming;artificial intelligence;data science;evaluation;machine learning;decision problem;data mining;management science;information extraction	ML	-4.055013525558905	-12.925339584733434	105488
2be019591c35f5a87d49ef9f75d36fe6dabbf5e8	inferring exhaust gases levels using taxi service and meteorological data: an experiment in the city of porto, portugal	pollution measurement;public transportation;gases;urban areas;monitoring;vehicles;meteorology	In this work, we study exhaust gas concentration patterns in Porto, Portugal, and explore techniques to estimate the level of NO2 concentrations using taxi service and meteorological data as sensors. The exploratory analysis revealed daily and seasonal patterns of exhaust gases, with higher concentrations in the morning and on colder months. Based on nine months of data, we are able to estimate the concentration of NO2 using a multilayer perceptron (r = 0.7358) and linear regression (r = 0.5407). The analysis was extended to estimate NO and NOx, showing a lower performance.	activation function;computer performance;inventory theory;kerrison predictor;multilayer perceptron;sensor;sigmoid function;topography	Marco Veloso;Pedro M. d'Orey;Santi Phithakkitnukoon;Carlos Bento;Michel Ferreira	2016	2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2016.7795874	meteorology;simulation;environmental engineering;geography	Robotics	9.804618166602198	-19.124138538708838	105533
3827c4687a15d2b55c7626f3ed9406698f3781ad	optimal and naive diversification in currency markets		DeMiguel, Garlappi, and Uppal (Review of Financial Studies, 22 (2009), 1915–1953) showed that in the stock market, it is difficult for an optimized portfolio constructed using meanvariance analysis to outperform a simple equally-weighted portfolio because of estimation error. In this paper, we demonstrate that portfolio optimization can be made to work in currency markets. The key difference between the two settings is that in currency markets interest rates provide a predictor of future returns that is free of estimation error, which permits the application of mean-variance analysis. We show that over the last 26 years, a mean-variance efficient portfolio constructed in this fashion has a Sharpe ratio of 0.91, versus only 0.15 for the equally-weighted portfolio. We also consider the practical implementation of this strategy.	atari portfolio;diversification (finance);kerrison predictor;mathematical optimization;mean squared error;pc-fx;two-moment decision model	Fabian Ackermann;Walt Pohl;Karl Schmedders	2017	Management Science	10.1287/mnsc.2016.2497	financial economics;post-modern portfolio theory;economics;replicating portfolio;modern portfolio theory;finance;portfolio optimization;microeconomics;portfolio insurance;separation property;black–litterman model;currency	ML	2.9155656420610545	-10.833164841162285	105566
f792a4f7a4989c4fc8ee4c9b64616330eeca8daf	a brief survey on event prediction methods in time series		Time series mining is a new area of research in temporal data bases. Hitherto various methods have been presented for time series mining which the most of an existing works in different applied areas have been focused on event prediction. Event prediction is one of the main goals of time series mining which can play an effective role for appropriate decision making in different applied areas. Due to the variety and plenty of event prediction methods in time series and lack of a proper context for their systematic introduction, in this paper, a classification is proposed for event prediction methods in time series. Also, event prediction methods in time series are evaluated based on the proposed classification by some proposed measures. Using the proposed classification can be beneficial in selecting the appropriate method and can play an effective role in the analysis of event prediction methods in different application domains.	time series	Soheila Mehrmolaei;Mohammad Reza Keyvanpour	2015		10.1007/978-3-319-18476-0_24	data mining;temporal database;computer science	ML	6.238222367733618	-23.312160200756576	105691
9ce246818fe6e21778f629a65fc0a6c042ec90e6	some results on interval-valued fuzzy bg-algebras	conference;meeting	In this note the notion of interval-valued fuzzy BG-algebras (briefly, i-v fuzzy BG-algebras), the level and strong level BG-subalgebra is introduced. Then we state and prove some theorems which determine the relationship between these notions and BG-subalgebras. The images and inverse images of i-v fuzzy BG-subalgebras are defined, and how the homomorphic images and inverse images of i-v fuzzy BG-subalgebra becomes i-v fuzzy BG-algebras are studied. KeywordsBG-algebra, fuzzy BG-subalgebra, intervalvalued fuzzy set, interval-valued fuzzy BG-subalgebra.	fuzzy set;fuzzy subalgebra;job control (unix);mv-algebra	Arsham Borumand Saeid	2005			discrete mathematics;fuzzy logic;mathematics	AI	0.780263081281803	-23.06078836606846	105747
10a50e52c3bbcb34ebf209173f5d4acb7b90abbc	two classes of pseudo-triangular norms and fuzzy implications	triangular norm;satisfiability;fuzzy implications;pseudo t norms;residual operators;fuzzy connective;t seminorms	Two kinds of extensions of triangular norms (t-norms) are proposed, and the relations between these extensions and fuzzy implications are discussed in this paper. First, two classes of pseudo-t-norms (ps-t-norms), called type-A and type-B ps-t-norms, and their respective residual operators are defined. Then, we prove that these residual operators are fuzzy implications and satisfy the left neutral property. For these two classes of pseudo-t-norms, we give a series of equivalent conditions of left-continuity with respect to their first or second variable. By combining the axioms of the two classes of pseudo-t-norms, we simply get the definition of the triangular seminorms. Furthermore, we define two classes of induced operators from fuzzy implications and give the conditions under which they are type-A ps-t-norms, type-B ps-t-norms and t-seminorms. For a fuzzy implication, a series of equivalent conditions of right-continuity with respect to its second variable are established. Finally, another method inducing type-A ps-t-norms, type-B ps-t-norms and t-seminorms by implications is proposed.	fuzzy concept	Huawen Liu	2011	Computers & Mathematics with Applications	10.1016/j.camwa.2010.12.025	mathematical analysis;discrete mathematics;topology;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;mathematics;algebra;satisfiability	HCI	0.044644357708482504	-23.001354561978694	105811
381d9fa60d6b4e793be9b4d36a20b1b463352ba8	solving the multi-response problem in taguchi method by benevolent formulation in dea	dea;multi response problem;benevolent formulation;taguchi method	The Taguchi method is an efficient approach for optimizing a single quality response. In practice, however, most products/processes have more than one quality response of main interest. Recently, the multi-response problem in the Taguchi method has gained a considerable research attention. This research, therefore, proposes an efficient approach for solving the multi-response problem in the Taguchi method utilizing benevolent formulation in data envelopment analysis (DEA). Each experiment in Taguchi’s orthogonal array (OA) is treated as a decision making unit (DMU) with multiple responses set inputs and/or outputs. Each DMU is evaluated by benevolent formulation. The ordinal value of the DUM’s efficiency is then used to decide the optimal factor levels for multi-response problem. Three frequently-investigated case studies are adopted to illustrate the proposed approach. The computational results showed that the proposed approach provides the largest total anticipated improvement among principal component analysis (PCA), DEA based ranking approach (DEAR) and other techniques in literature. In conclusion, the proposed approach may provide a great assistant to practitioners for solving the multiresponse problem in manufacturing applications on the Taguchi method.	command & conquer:yuri's revenge;computation;data envelopment analysis;decision theory;disk storage;emoticon;entity–relationship model;hard disk drive;initial condition;input/output;linear function;linear model;nonlinear system;optimization problem;ordinal data;principal component analysis;taguchi methods;xio;z/vm	Abbas Al-Refaie;Mohammad D. Al-Tahat	2011	J. Intelligent Manufacturing	10.1007/s10845-009-0312-8	mathematical optimization;taguchi methods;engineering;operations research;statistics	AI	-2.2677548536621823	-15.925569310462327	105893
2cd979df9f534b97c278cc7537545deeddd34629	a note on multi-criteria abc inventory classification using weighted linear optimization	modelizacion;multiple criteria analysis;multicriteria analysis;valor elevado;modele lineaire;valeur elevee;modelo lineal;multiple criteria;operations research;classification;inventory;administracion deposito;modelisation;programacion lineal;recherche operationnelle;indexation;linear model;gestion stock;linear programming;programmation lineaire;high value;linear optimization;analisis multicriterio;analyse multicritere;modeling;inventory control;clasificacion;investigacion operacional	Ramanathan [R. Ramanathan, ABC inventory classification with multiple-criteria using weighted linear optimization, Computers & Operations Research 33 (2006) 695–700] recently proposed a weighted linear optimization model for multi-criteria ABC inventory classification. Despite its many advantages, Ramanathan’s model (R-model) could lead to a situation where an item with a high value in an unimportant criterion is inappropriately classified as a class A item. In this paper we present an extended version of the R-model for multi-criteria inventory classification. Our model provides a more reasonable and encompassing index since it uses two sets of weights that are most favourable and least favourable for each item. An illustrative example is presented to compare our model and the R-model.	linear programming;mathematical optimization	Peng Zhou;Liwei Fan	2007	European Journal of Operational Research	10.1016/j.ejor.2006.08.052	inventory control;mathematical optimization;systems modeling;inventory;biological classification;linear programming;artificial intelligence;operations management;linear model;mathematics;operations research	Vision	-0.04411855908307347	-15.549861528546328	106218
4bb3648ed04f137a8cb3610ac421a26bfa9796f6	"""comments on """"statistical reasoning with set-valued information: ontic vs. epistemic views"""""""	statistical methods with set fuzzy set valued data;random set fuzzy set valued variables;set fuzzy set valued models	In this comment, several paragraphs from the paper ''Statistical reasoning with set-valued information: Ontic vs. epistemic views'' have been selected and discussed. The selection has been based, on one side, on a personal view of what can be considered the most clarifying points in the paper and, on the other side, on the aspects I am more familiar with and interested in and being quite unequivocally ontic-oriented. For sure, it is a biased selection, but the aim of these comments is that of sharing what I have found to be more appealing within the discussion and I would like to point out in connection with my own expertise.	ontic	María Angeles Gil	2014	Int. J. Approx. Reasoning	10.1016/j.ijar.2014.04.007	discrete mathematics;data mining;mathematics;algorithm	AI	-3.7879701386823923	-23.83906297737447	106377
8db217030f424532914535d07a5a328ccc58aca5	application of evolutionary computation on ensemble forecast of quantitative precipitation		An evolutionary computation algorithm known as genetic programming (GP) has been explored as an alternative tool for improving the ensemble forecast of 24-h accumulated precipitation. Three GP versions and six ensembles’ languages were applied to several real-world datasets over southern, southeastern and central Brazil during the rainy period from October to February of 2008–2013. According to the results, the GP algorithms performed better than two traditional statistical techniques, with errors 27–57% lower than simple ensemble mean and the MASTER super model ensemble system. In addition, the results revealed that GP algorithms outperformed the best individual forecasts, reaching an improvement of 34–42%. On the other hand, the GP algorithms had a similar performance with respect to each other and to the Bayesian model averaging, but the former are far more versatile techniques. Although the results for the six ensembles’ languages are almost indistinguishable, our most complex linear language turned out to be the best overall proposal. Moreover, some meteorological attributes, including the weather patterns over Brazil, seem to play an important role in the prediction of daily rainfall amount.	approximation algorithm;ensemble forecasting;ensemble learning;evolutionary computation;genetic programming;linear grammar;probability of precipitation	Amanda Sabatini Dufek;Douglas Adriano Augusto;Pedro Leite da Silva Dias;Helio J. C. Barbosa	2017	Computers & Geosciences	10.1016/j.cageo.2017.06.011	evolutionary computation;computer science;ensemble average;genetic programming;artificial intelligence;machine learning;precipitation;ensemble forecasting;bayesian inference	AI	7.107775435549265	-21.232728945862487	106394
972984ffb8310ff746701c5c25587354229188d3	developing a fuzzy bicluster regression to estimate heat tolerance in plants by chlorophyll fluorescence	statistical regression analysis;chlorophyll fluorescence measurement;biology computing;fuzzy regression;fuzzy bicluster regression;linear regression model;heat tolerance;biothermics;linear regression;regression model;fluorescence photochemistry temperature sensors regression analysis fuzzy sets fuzzy set theory linear regression genetic algorithms large hadron collider temperature distribution;fuzzy set theory;heat tolerance of plants;data clustering;fuzzy clustering;heat tolerance of plants chlorophyll fluorescence fuzzy bicluster regression fbcr fuzzy c regression models fuzzy set theory genetic algorithms;fuzzy c regression models;fuzzy bi cluster regression;nonlinear inflections;plant species;linear model;genetic algorithm;genetic algorithms;regression analysis;leaf nature;biological techniques;plant heat tolerance;regression analysis biological techniques biology computing biothermics fuzzy set theory genetic algorithms photosynthesis;fuzzy bicluster regression fbcr;photosynthesis;chlorophyll fluorescence;fuzzy intersection set;nonlinear inflections fuzzy bicluster regression plant heat tolerance chlorophyll fluorescence measurement fuzzy regression approach photosynthesis linear regression lines statistical regression analysis leaf nature genetic algorithms fuzzy intersection set;linear regression lines;fuzzy regression approach	This paper presents a straightforward and useful fuzzy regression approach to estimate heat tolerance of plants by chlorophyll fluorescence measurement. The chlorophyll fluorescence measurement is an indicator of functional change of photosynthesis and is sensitive to temperature. Using the fluorescence-temperature curves, the experimenter may determine the heat tolerance (Tc) of plants by intersections of two linear regression lines. However, as traditional statistical regression analysis shows, the experiment may contain uncertain factors or phenomena such as leaf nature and growth environment, which concludes that data may vary among individual plants and different species. This research presents a fuzzy bicluster regression (FBCR) analysis with genetic algorithms, which helps derive a fuzzy intersection set and fuzzy heat tolerance of plants, in addition to the traditional statistical regression analysis. A fuzzy clustering concept and simultaneously optimal determination of data clusters is also developed. Especially, when there are nonlinear inflections in data curves, due to the imperative use of linear regression models, the traditional regression analysis may become unable to sufficiently model the uncertainties exhibited. The FBCR analysis can resolve this problem effectively due to the nonlinear tolerance of the system, even in a linear model. To demonstrate the FBCR analysis, it was applied to estimate the heat tolerance of five plant species. The results derived appeared to be more suitable than that of the conventional method. The approach may provide a useful means for the experimenters to derive more credible results from their chlorophyll fluorescence-temperature data.	biclustering;cluster analysis;fuzzy clustering;fuzzy set operations;genetic algorithm;imperative programming;linear model;nonlinear system	Ping-Teng Chang;Kuo-Ping Lin;Chih-Sheng Lin;Kuo-Chen Hung;Lung-Ting Hung;Ban-Dar Hsu	2009	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2008.924216	econometrics;genetic algorithm;computer science;linear regression;machine learning;mathematics;regression analysis;statistics	Robotics	4.246887248248856	-23.14320474479494	106489
ed0567588da98a64494e8d568b71f4c2407dcd90	a method for processing the unreliable expert judgments about parameters of probability distributions	probabilidad imprecisa;calcul probabilite;expert judgments;systeme incertain;jugement expert;imprecise probability;programacion lineal;upper and lower probabilities;probabilite superieure;probabilite imprecise;probabilite inferieure;probability distribution;imprecise probabilities;random variable;linear programming;programmation lineaire;linear program;sistema incierto;expert judgment;uncertain system;theory of probabilities;uncertainty modelling	A method for combining two types of judgments about an object analyzed, which are elicited from experts, is considered in the paper. It is assumed that the probability distribution of a random variable is known, but its parameters may be determined by experts. The method is based on the use of the imprecise probability theory and allows us to take into account the quality of expert judgments, heterogeneity and imprecision of information supplied by experts. An approach for computing ”cautious“ expert beliefs under condition that the experts are unknown is studied. Numerical examples illustrate the proposed method.	am broadcasting;angular defect;computational complexity theory;information;judgment (mathematical logic);mathematical optimization;maxima and minima;numerical method;optimization problem	Lev V. Utkin	2006	European Journal of Operational Research	10.1016/j.ejor.2005.04.041	probability distribution;random variable;imprecise probability;linear programming;artificial intelligence;data mining;mathematics;statistics	ML	0.9957300214091548	-16.786283695433028	106681
0b6b47370d57d10a6e59531c4d31c8df6a18e170	applying a fuzzy and neural approach for forecasting the foreign exchange rate	forecasting;hybrid approach;back propagation network;fuzzy linear regression;foreign exchange rate	Accurately forecasting the foreign exchange rate is important for export-oriented enterprises. For this purpose, a fuzzy and neural approach is applied in this study. In the fuzzy and neural approach, multiple experts construct fuzzy linear regression (FLR) equations from various viewpoints to forecast the foreign exchange rate. Each FLR equation can be converted into two equivalent nonlinear programming problems to be solved. To aggregate these fuzzy foreign exchange rate forecasts, a two-step aggregation mechanism is applied. At the first step, fuzzy intersection is applied to aggregate the fuzzy forecasts into a polygon-shaped fuzzy number to improve the precision. A back propagation network is then constructed to defuzzify the polygon-shaped fuzzy number and generate a representative/crisp value to enhance accuracy. To evaluate the effectiveness of the fuzzy and neural approach, a practical case of forecasting the foreign exchange rate in Taiwan is used. According to the experimental results, the fuzzy and neural approach improved both the precision and accuracy of the foreign exchange rate forecasting by 79% and 81%, respectively.	an/flr-9;aggregate data;backpropagation;foreign exchange service (telecommunications);fuzzy number;fuzzy set operations;nonlinear programming;nonlinear system;software propagation	Toly Chen	2011	IJFSA	10.4018/ijfsa.2011010103	financial economics;econometrics;economics;defuzzification;adaptive neuro fuzzy inference system;fuzzy number;neuro-fuzzy;machine learning;fuzzy set operations	AI	4.986855399326451	-20.93971930037794	106753
1d69d2be4150f873b88f8d4c2f902729a9b3c063	efficiency and equity tradeoffs in voting machine allocation problems	efficient allocation;efficiency equity tradeoff;journal of the operational research society;voting operations;public service	Efficiency and equity are the two crucial factors to be considered when allocating public resources such as voting machines. Existing allocation models are all single-objective, focusing on maximizing either efficiency or equity despite the fact that the actual decision-making process involves both issues simultaneously. We propose a bi-objective integer program to analyse the tradeoff between the two competing objectives. The new model quantifies the sacrifice in efficiency in order to achieve a certain improvement in equity and vice versa. Using data from the 2008 United States Presidential election in Franklin County, Ohio, we demonstrate that our model is capable of producing significantly more balanced allocation plans, in terms of efficiency and equity, than current practice or other competing methods. Journal of the Operational Research Society advance online publication, 12 November 2014; doi:10.1057/jors.2014.107	access control;elegant degradation;equity crowdfunding;franklin electronic publishers;integer programming;objective-c;operations research;transparency (graphic)	Xinfang Wang;Muer Yang;Michael J. Fry	2015	JORS	10.1057/jors.2014.107	actuarial science;economics;public economics;marketing;finance;management;operations research;welfare economics	AI	5.099450345422493	-10.097003834540722	106951
6e70b95ca66c5c53c248d33042d36520bddcc26f	recurrent wavelet network with new initialization and its application on short-term load forecasting	least mean squares methods;demand side management;recurrent neural nets demand side management least mean squares methods load forecasting power engineering computing;dynamic model;load forecasting;orthogonal least square;load forecasting recurrent wavelet network initialization;short term load forecasting;power engineering computing;electricity consumption;orthogonal least square technique recurrent wavelet network short term load forecasting intelligent demand side management electricity consumption;load forecasting predictive models neural networks neurons computer networks computational modeling energy consumption least squares methods recurrent neural networks weather forecasting;recurrent neural nets;recurrent wavelet network;initialization	A key issue in intelligent demand-side management is the accurate prediction of electricity consumption. This paper presents a dynamic model for short-term special days load forecasting which uses a Recurrent Wavelet Network (RWN). However, initialization of this network encounters a major problem. Thus, a new initialization method is suggested based on Orthogonal Least Square (OLS) technique. Finally, a RWN with the proposed initialization method is applied to experimental special days load data. Simulation results show that the proposed network is capable of handling the inherent complexity of load forecasting problem.	algorithm;mathematical model;ordinary least squares;simulation;time series;wavelet	Amir Baniamerian;Meysam Asadi;Ehsan Yavari	2009	2009 Third UKSim European Symposium on Computer Modeling and Simulation	10.1109/EMS.2009.41	simulation;computer science;artificial intelligence;machine learning	Embedded	9.527023210903389	-18.463349385491686	107064
2c6f1e12100a22ef0e22be131f5046ba9fa603a9	optimisation problems as decision problems: the case of fuzzy optimisation problems		Abstract The importance that decision-making problems and optimisation problems have today in all aspects of life is beyond all doubt. Despite that importance, both problems tend to be thought of as following different routes, when they have, in fact, a “symbiotic” relation. Here, we consider the different decision problems that arise when different kinds of information and framework of behaviour are considered, and we explore the corresponding optimisation problems that can be derived for searching the best possible decision. We explore the case where Fuzzy Mathematical Programming problems are obtained as well as other new ones in the fuzzy context.	decision problem;mathematical optimization	María Teresa Lamata;David A. Pelta;José L. Verdegay	2018	Inf. Sci.	10.1016/j.ins.2017.07.035	mathematics;fuzzy logic;machine learning;fuzzy set operations;fuzzy transportation;artificial intelligence;management science;decision problem	AI	-0.8687322204832206	-18.166189294676634	107153
e35120cb7c2e5638511c063de369c796ca5c176b	modeling temporal pattern and event detection using hidden markov model with application to a sludge bulking data	sludge bulking;hidden markov model	This paper discusses a method of modeling temporal pattern and event detection based on Hidden Markov Model (HMM) for a continuous time series data. We also provide methods for checking model adequacy and predicting future events. These methods are applied to a real example of sludge bulking data for detecting sludge bulking for a water plant in Chicago.		Naveen K. Bansal;Xin Feng;Wenjing Zhang;Wutao Wei;Yuanhao Zhao	2012		10.1016/j.procs.2012.09.059	data science;machine learning;data mining	ML	9.51578473808546	-18.636369415256464	107164
dfeb45ed585c797646b1001a948ac723b55d66b7	biprobabilistic values for bicooperative games	optimisation;combinatorics;optimizacion;ternary voting games;combinatoria;combinatoire;vector space;bicooperative games;voting game;shapley value;voting;informatique theorique;compatible order values;probabilistic values;optimization;voto;espace vectoriel;vote;espacio vectorial;biprobabilistic values;computer theory;informatica teorica	The present paper introduces bicooperative games and develops some general values on the vector space of these games. First, we define biprobabilistic values for bicooperative games and observe in detail the axioms that characterize such values. Following the work of Weber [R.J. Weber, Probabilistic values for games, in: A.E. Roth (Ed.), The Shapley Value: Essays in Honor of Lloyd S. Shapley Cambridge University Press, Cambridge, 1988, pp. 101–119], these axioms are sequentially introduced observing the repercussions they have on the value expression. Moreover, compatible-order values are introduced and there is shown the relationship between these values and efficient values such that their components are biprobabilistic values. c © 2007 Elsevier B.V. All rights reserved.	realms of the haunting;stable marriage problem	Jesús Mario Bilbao;Julio Rodrigo Fernández García;N. Jiménez;J. J. López	2008	Discrete Applied Mathematics	10.1016/j.dam.2007.11.007	mathematical optimization;combinatorics;voting;vector space;computer science;artificial intelligence;mathematics;shapley value;mathematical economics;algorithm;electoral-vote.com	AI	1.3137227316118327	-19.69934108098524	107239
cd91423ad9acb7b7ea6dce8b195c9b99b7ce4bfd	generalized neutrosophic soft expert set for multiple-criteria decision-making		Smarandache defined a neutrosophic set to handle problems involving incompleteness, indeterminacy, and awareness of inconsistency knowledge, and have further developed it neutrosophic soft expert sets. In this paper, this concept is further expanded to generalized neutrosophic soft expert set (GNSES). We then define its basic operations of complement, union, intersection, AND, OR, and study some related properties, with supporting proofs. Subsequently, we define a GNSES-aggregation operator to construct an algorithm for a GNSES decision-making method, which allows for a more efficient decision process. Finally, we apply the algorithm to a decision-making problem, to illustrate the effectiveness and practicality of the proposed concept. A comparative analysis with existing methods is done and the result affirms the flexibility and precision of our proposed method.	algorithm;indeterminacy in concurrent computation;qualitative comparative analysis	Vakkas Uluçay;Memet Sahin;Nasruddin Hassan	2018	Symmetry	10.3390/sym10100437		AI	-3.33068009573316	-21.386950980064203	107324
d64896f7493d46a163e0569b86d821e7e7e4b6ae	research on traffic speed prediction by temporal clustering analysis and convolutional neural network with deformable kernels (may, 2018)		Real-time traffic speed prediction is one of the most essential parts of the Intelligent Transportation System. In recent years, with the development of artificial intelligence technology, such as in-depth learning, new prediction methods have emerged in endlessly and achieved good results. However, the spatio-temporal information, traffic environment, and their interaction have been hardly depicted in these methods. Therefore, a novel traffic speed prediction method based on temporal clustering analysis and deformable convolution neural network (TCA-DCNN) is proposed in this paper. Temporal clustering analysis (TCA) based on Differential Evolution and hierarchical clustering can adaptively distinguish traffic environment by discriminating the traffic speed variation pattern. By further introducing the deformable convolutional kernels, the characteristics of spatio-temporal traffic speed variation are precisely located in deformable perception fields. After that, a set of DCNN models are trained by using the data processed by TCA, and the output of one of the basic DCNN model is selected as the final traffic speed prediction result. The simulation results based on the measured data in Hangzhou, China, show that the TCA-DCNN algorithm has better performance than other algorithms in traffic speed prediction.	advanced telecommunications computing architecture;algorithm;artificial intelligence;artificial neural network;cluster analysis;convolution;convolutional neural network;differential evolution;hierarchical clustering;real-time transcription;simulation	Guojiang Shen;Chaohuan Chen;Qihong Pan;Si Shen;Zhi Liu	2018	IEEE Access	10.1109/ACCESS.2018.2868735	convolutional neural network;differential evolution;feature extraction;hierarchical clustering;intelligent transportation system;convolution;cluster analysis;computer science;distributed computing;machine learning;data modeling;artificial intelligence	AI	8.60830854175647	-22.646903568289417	107328
01750ce2d4653eac2fe10baef5a7219ee2219fe8	time series modeling and forecasting using memetic algorithms for regime-switching models	time series autoregressive processes forecasting theory fuzzy set theory iterative methods knowledge based systems neural nets search problems;neural nets;biological system modeling;autoregression;time series;fuzzy set theory;threshold autoregressive model tar;iterative methods;neuro coefficient smooth transition autoregressive model ncstar;computational modeling;forecasting theory;autoregressive processes;time series analysis;memetic algorithms;biological system modeling time series analysis buildings computational modeling sociology optimization;optimization;search problems;local search algorithm time series modeling time series forecasting memetic algorithm regime switching model model fitting procedure neuro coefficient smooth transition autoregressive model ncstar statistically founded iterative building procedure fuzzy rule based system mathematically sound building procedure forecasting model parameter estimation grid search procedure;threshold autoregressive model tar autoregression memetic algorithms neuro coefficient smooth transition autoregressive model ncstar regime switching models;regime switching models;sociology;knowledge based systems;buildings	In this brief, we present a novel model fitting procedure for the neuro-coefficient smooth transition autoregressive model (NCSTAR), as presented by Medeiros and Veiga. The model is endowed with a statistically founded iterative building procedure and can be interpreted in terms of fuzzy rule-based systems. The interpretability of the generated models and a mathematically sound building procedure are two very important properties of forecasting models. The model fitting procedure employed by the original NCSTAR is a combination of initial parameter estimation by a grid search procedure with a traditional local search algorithm. We propose a different fitting procedure, using a memetic algorithm, in order to obtain more accurate models. An empirical evaluation of the method is performed, applying it to various real-world time series originating from three forecasting competitions. The results indicate that we can significantly enhance the accuracy of the models, making them competitive to models commonly used in the field.	autoregressive model;cma-es;curve fitting;ephedra sinica;estimation theory;fuzzy rule;iteration;iterative method;local search (optimization);machine learning;markov chain;mathematical optimization;matthews correlation coefficient;memetic algorithm;population parameter;projections and predictions;rule-based system;search algorithm;terminate (software);time series;chaperone-mediated autophagy	Christoph Bergmeir;Isaac Triguero;Daniel Molina;José Luis Aznarte;José Manuel Benítez	2012	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2012.2216898	econometrics;mathematical optimization;computer science;knowledge-based systems;machine learning;time series;mathematics;artificial neural network;statistics	Vision	8.068701115909779	-20.678734111021196	107484
101605ca1a54465e338c0004de5f5001c3712af9	decision under risk as a multicriteria problem	modelizacion;analyse risque;multiple criteria analysis;multicriteria analysis;factor riesgo;fuzzy set;uncertainty modeling;decision under risk;decision aid;risk factor;risk analysis;prospect theory;teoria decision bajo riesgo;modelo determinista;analisis decision;conjunto difuso;ensemble flou;modele deterministe;prise decision;decision maker;facteur risque;decision analysis;fuzzy sets;preparacion serie fabricacion;decision problem;modelisation;analisis riesgo;systeme incertain;risk perception;theorie decision hazardeuse;analisis multicriterio;process planning;analyse multicritere;toma decision;sistema incierto;deterministic model;preparation gamme fabrication;modeling;uncertain system;analyse decision	"""Most of the approaches to decision problems under uncertainty are based on decision paradigms, generally associated to an optimization process that leads to a final solution. For the Decision Maker, the basic decision is thus what paradigm to choose, the rest of the procedure being mainly technical. In this paper, a different approach is advocated for this kind of problems. The main idea is to leave prescriptive models in favor of a more flexible approach, where risk related criteria are explicitly considered, conducting to an """"equivalent"""" multicriteria (deterministic) model where decision-aid procedures can be used, with a greater involvement of the Decision Maker. The paper discusses first the uncertainty model and then reviews existing paradigms for the single criterion problem under uncertainty. Proposed risk and opportunity attributes come mainly from the analysis of those methodologies and from risk perception studies reports. Some hints about multicriteria aid methods and an illustrative example complete the paper."""		Manuel A. Matos	2007	European Journal of Operational Research	10.1016/j.ejor.2005.11.057	optimal decision;economics;decision analysis;computer science;artificial intelligence;operations management;mathematics;fuzzy set;evidential reasoning approach;operations research;weighted sum model;business decision mapping	Theory	-1.531961172105589	-14.33060657111268	107522
9fbcde6824b6eb84fe97053319c1e79af1a9768b	analytic hierarchy process based on fuzzy analysis	analytic hierarchy process;decision maker;regression analysis;analytical hierarchical process	Analytic Hierarchical Process(AHP) is proposed to give the priority weight with respect to many items. The priority weights  are obtained from the pairwise comparison matrix whose elements are given by a decision maker as crisp values. We extend the  crisp pairwise comparisons to fuzzy ones based on uncertainty of human judgement. To give uncertain information as a fuzzy  value is more rational than as a crisp value. We assume that the item’s weight is a fuzzy value, since the comparisons are  based on human intuition so that they must be inconsistent each other. We propose a new AHP, where the item’s weight is given  as a fuzzy value, in order to deal with inconsistency in the given matrix. The purpose is to obtain fuzzy weights so as to  include all the given fuzzy pairwise comparisons, in the similar way to the upper approximation in interval regression analysis.  		Tomoe Entani;Kazutomi Sugihara;Hideo Tanaka	2004		10.1007/3-540-31182-3_27	machine learning;management science;analytic network process;statistics	Logic	-3.1875514849580178	-20.033471244082744	107552
41b535fbfed33f6527378ba50488758cd6505494	on the bias caused by spatial aggregation of flows on the estimation of the degree of scale economies	estimacion sesgada;spatial aggregation;statistique;economic analysis;traffic distribution;effet echelle;scale effect;output;economie transport;volume;econometria;scale economies;coste marginal;cluster analysis;cout marginal;efecto escala;transportation;economia transporte;statistics;marginal cost;transportation systems;econometrics;economy of transports;bias statistics;freight transportation;biased estimation;estimation biaisee;distance;econometrie;estadistica	From an economic viewpoint, transportation supply has been historically characterized through very aggregated descriptions of the output of a transportation firm. The use of measures like ton-miles or passenger-kilometers was standard practice through the end of the 1970's. Currently, the accepted description of transportation output is in terms of a vector of flows; aggregate measures have empirically been shown to be the source of significant problems in the economic analysis of transportation systems. In particular, the scarce empirical work on multioutput cost functions shows that the scalar volume-distance measure generates biased estimates of both marginal costs and degree of scale economies. In this paper, these biases are analyzed from an econometric viewpoint, using linear cost functions as first order approximations. Biases are expressed as differences of weighted sums, which are then used to show that, under reasonable statistical assumptions, the aggregated model causes a systematic overestimation of the degree of scale economies. This phenomenon is generalized to include a fairly general dependence of marginal costs on flows and distances.		Sergio R. Jara-Díaz;Pedro P. Donoso	1989	Transportation Science	10.1287/trsc.23.3.151	marginal cost;econometrics;transport;economics;economies of scale;mathematics;cluster analysis;mathematical economics;distance;welfare economics;volume;statistics	DB	0.04957510633034755	-12.472274097162977	107629
6acce6a5d9e34779014ee3f9897dce26b7acc506	fuzzy backpropagation neural networks for nonstationary data prediction	neurofuzzy modelling;backpropagation neural network;data prediction	The backpropagation neural network is one of the most widely used connectionist model, especially in the solution of real life problems. The main reasons for the popularity of this model are its conceptual simplicity and its ability to tackle a broad range of problems. But, on the other hand, this architecture shows a well known problem for dealing with nonstationary data. In this paper, a variation of feedforward neural model which uses qualitative data both for feeding the network and for back propagating the error correction is presented. The data are coded by means of a fuzzy concept of local stability.	artificial neural network;backpropagation	Ramon Soto	2007		10.1007/978-3-540-72950-1_32	catastrophic interference;feedforward neural network;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;time delay neural network	ML	8.896106231609764	-22.747835401040074	107658
4110c54cddb2a1a6e73f6f3e12076cac335115ad	a final price prediction model for online english auctions - a neuro fuzzy approach		Markov Chain Model provides a concise mathematical model to describe the online English auction process, converting the complicated interaction between the bidders and auctioneer into a tractable mathematical problem, which is a milestone for researches involved in this area. However, the assumptions about the parameters are not consistent with the actual phenomena, for example, the distribution of the private values and the arrival rates. Furthermore it is hard to obtain the values of these parameters. In this paper, a hybrid method, neuro fuzzy, is proposed to predict the final price in addition to exploring the complicated, possibly nonlinear, relationship among the auction mechanisms and final price. The empirical results show that neuro fuzzy system can predict the final price accurately much better than the others, which is of great help for the buyers to avoid overpricing and for the sellers to facilitate the auction. Besides, the knowledge base obtained from neuro fuzzy provides the elaborative relationship among the variables, which can be further tested for theory building.	cobham's thesis;fuzzy control system;fuzzy logic;knowledge base;markov chain;mathematical model;neuro-fuzzy;nonlinear system	Chin-Shien Lin;Shihyu Chou;Chi-Hong Chen;Tai-Ru Ho;Yu-Chen Hsieh	2006		10.2991/jcis.2006.320		AI	1.3967994255080303	-13.233317172748993	107718
902ef0e515de3bf1c92e65daf0be4a981cd89a8f	enhancing forecasting performance of multivariate time series using new hybrid feature selection	gra analyzer;t technology general;cooperative feature selection;qa mathematics;multivariate time series;accuracy;ann optimizer	The aim of this study is to propose a new hybrid feature selection model to improve the performance of multivariate time series (MTS) forecasting under uncertainty situation. This new hybrid model is called cooperative feature selection (CFS) and consists of two different component; GRA Analyzer and ANN Optimizer. The performance of CFS is evaluated on KLSE close price. The statistical analysis of the results shows that CFS has high ability to recognize and remove irrelevant input for obtaining optimum input factors, shortening the learning time and improving forecasting accuracy for vague MTS.	feature selection;time series	Roselina Sallehuddin;Siti Mariyam Hj. Shamsuddin;Noorfa Haszlinna Mustafa	2012		10.1007/978-3-642-31837-5_54	speech recognition;artificial intelligence;machine learning;mathematics;accuracy and precision;statistics	HCI	6.047547352947611	-19.722354198395454	107745
aa13b712bf9546a02936a25ac37d05f20b378878	fuzzy market sentiments model: studying contrarians strategy under behavioral finance				Soumya Banerjee;Hameed Al-Qaheri	2010	Egyptian Computer Science Journal		fuzzy logic;management science;actuarial science;engineering;control engineering;behavioral economics	Theory	-4.259855541771213	-12.245912712805987	107770
6af2701fdbd4638e827a4ed640c7a91377caf327	validation of logistic regression models for landslide susceptibility maps	binary logistic regression;reservoirs;binary logistic regression analyses;terrain factors;geological map;land cover data;hazards;landslide susceptibility;terrain mapping digital elevation models geographic information systems geomorphology geophysics computing statistical analysis;digital elevation model;validation landslide susceptibility gis binary logistic regression;data mining;three gorges reservoir region;landslide susceptibility maps;geomorphology;geophysics computing;geology;logistics;statistical analysis;gis;numerical model;environmental applications;geographic information systems;decision making process;validation;digital elevation models;satellite imagery;terrain mapping;logistic regression models;physical model;numerical models;china;geographic information systems logistic regression models landslide susceptibility maps numerical models decision making process environmental applications statistically based methods three gorges reservoir region china binary logistic regression analyses digital elevation model geological map land cover data satellite imagery;statistically based methods;land cover;logistic regression model;field study;logistics terrain factors numerical models decision making reservoirs regression analysis digital elevation models geology geographic information systems satellites	A wide range of numerical models and tools have been developed over the last decades to support the decision making process in environmental applications, ranging from physical models to a variety of statistically-based methods. In this study, a landslide susceptibility map of a part of Three Gorges Reservoir region of China was produced, employing binary logistic regression analyses. The available information includes the digital elevation model of the region, geological map and different GIS layers including land cover data obtained from satellite imagery. The landslides were observed and documented during the field studies. The validation analysis is exploited to investigate the quality of mapping.	computer simulation;digital elevation model;geographic information system;logistic regression	S. B. Bai;Jun Wang;Alexei Pozdnoukhov;Mikhail F. Kanevski	2009	2009 WRI World Congress on Computer Science and Information Engineering	10.1109/CSIE.2009.1019	geomatics;digital elevation model;logistic regression;field research	SE	9.830020150879903	-19.177591600714752	107845
e6174b76c97f513a1f3a96662abc4a0d1fbf57e7	a new insight into prediction modeling systems	forecasting;intelligent database;data mining and knowledge discovery;time series;stochastic;regression;smoothing;box jenkins;prediction;arima;neural network;expert system	Data mining and forecasting has attracted a lot of research interest in time series data sequence. The paper brings a new insight in how to select different time series forecasting models to make prediction according to different situations and data patterns. The contribution of this research is to facilitate prediction modelling system design for extensive forecasting purpose.		Sang C. Suh;Sam I. Saffer;Dan Li;Jingmiao Gao	2004	Transactions of the SDPS		engineering;data science;machine learning;data mining	Visualization	6.340065853073246	-23.311967205846752	108009
5c5ffe9fedc4034449f8d730676fe7b363dffe53	setting optimal intrusion-detection thresholds	secure computation;intrusion detection;optimal intrusion detection thresholds;false positive rate;indexation;comparative study;process model;intrusion process model	In this paper a model is developed to study an intrusion detection process. From the model, a measure called the Secure Computation Index is proposed. This index is used to quantify the total aspect of an intrusion-safe (or intrusionresistant) system. Comparative studies based on the index can assist in making decisions on optimal strategic controls against any possible system intrusion. In this paper, we show how the model can be used to help in setting optimal intrusion-detection thresholds, which will provide the best intrusion coverage with the minimum false positive rate.	intrusion detection system	Ben Soh;Tharam S. Dillon	1995	Computers & Security	10.1016/0167-4048(95)00017-8	intrusion detection system;simulation;false positive rate;computer science;comparative research;process modeling;data mining;computer security	Crypto	-0.5883329601103426	-13.827619863813974	108249
44166ccaa1795f59ed133fdbbfb87495f9a4f363	neural networks for air pollution nowcasting	data quality control;statistical analysis;air pollution;urban area;air pollutants;artificial neural network;neural network	This work illustrates the use and some results of Artificial Neural Networks (ANNs) for data quality control of air pollutants. ANNs are applied to the short-term predicting of air pollutant concentrations in urban areas. Observed versus predicted data are compared to test the efficacy of ANNs in simulating environmental processes. Statistical analysis is used for choice of neural structure. The model is validated on original data.	data quality;neural networks;simulation;statistical model	Ivanka Videnova;Dimitar Nedialkov;Maya Dimitrova;Silvia Popova	2006	Applied Artificial Intelligence	10.1080/08839510600753741	computer science;machine learning;operations research;artificial neural network;air pollution	AI	9.729410817903775	-19.159960378619395	108359
491912a108c112540e6f9b232bac313586451da5	pseudo-atoms of fuzzy and non-fuzzy measures	28cxx;engineering;fuzzy set;condicion necesaria;procesamiento informacion;fuzzy measure;exhaustive;metodo descomposicion;condition necessaire suffisante;methode decomposition;conjunto difuso;ensemble flou;condition suffisante;ingenierie;pseudo atom;decomposition method;necessary condition;condicion suficiente;atom;necessary and sufficient condition;information processing;order continuous;ingenieria;sistema difuso;28bxx;condition necessaire;systeme flou;sufficient condition;traitement information;condicion necesaria suficiente;fuzzy system	In this paper, we introduce the concept of pseudo-atoms of set functions, and establish some fundamental conclusions about pseudo-atoms on the assumption of null-null-additivity. We give the necessary and sufficient conditions which insure that the set of non-null-measure contains no pseudo-atoms. Further, we obtain several decomposition theorems about pseudo-atoms and atoms of set functions.	fuzzy measure theory	Congxin Wu;Sun Bo	2007	Fuzzy Sets and Systems	10.1016/j.fss.2006.12.011	decomposition method;atom;information processing;computer science;artificial intelligence;calculus;mathematics;fuzzy set;algorithm;fuzzy control system	Robotics	1.4791068038866717	-22.475901746797724	108366
5aaee1079f2568f78708ebfcdf23a22bb4e8fc36	maximin efficiencies under treatment-dependent costs and outcome variances for parallel, aa/bb, and ab/ba designs		If there are no carryover effects, AB/BA crossover designs are more efficient than parallel (A/B) and extended parallel (AA/BB) group designs. This study extends these results in that (a) optimal instead of equal treatment allocation is examined, (b) allowance for treatment-dependent outcome variances is made, and (c) next to treatment effects, also treatment by period interaction effects are examined. Starting from a linear mixed model analysis, the optimal allocation requires knowledge on intraclass correlations in A and B, which typically is rather vague. To solve this, maximin versions of the designs are derived, which guarantee a power level across plausible ranges of the intraclass correlations at the lowest research costs. For the treatment effect, an extensive numerical evaluation shows that if the treatment costs of A and B are equal, or if the sum of the costs of one treatment and measurement per person is less than the remaining subject-specific costs (e.g., recruitment costs), the maximin crossover design is most efficient for ranges of intraclass correlations starting at 0.15 or higher. For other cost scenarios, the maximin parallel or extended parallel design can also become most efficient. For the treatment by period interaction, the maximin AA/BB design can be proven to be the most efficient. A simulation study supports these asymptotic results for small samples.	allocation;amino acids;business architecture;efficiency;emoticon;financial cost;mathematical optimization;minimax;mixed model;numerical analysis;simulation;vagueness;version	Math J. J. M. Candel	2018		10.1155/2018/8025827	interaction;statistics;machine learning;artificial intelligence;crossover;mixed model;crossover study;minimax;computer science	Metrics	2.396733505492434	-10.38088728200909	108416
508623bb8e6a2c583f3178cb54cc67487e54bde2	the application of nonlinear fuzzy parameters pde method in pricing and hedging european options	nonlinear fuzzy pde;fuzzy parameters;option pricing;optimal hedging	In recent years, fuzzy sets theory has been introduced as a means of modeling the uncertainties of the input parameters of the Black-Scholes-Merton European options pricing formula. However, some standard assumptions underlying the BlackScholes-Merton model including those of constant interest rate and volatility no longer hold in fuzzy environments. Therefore, it is inappropriate to price options with uncertain parameters based on the Black-Scholes-Merton formula. In this paper, we propose a methodology for option pricing under fuzzy environments which is essentially different from the Black-Scholes-Merton option pricing framework. We build a nonlinear fuzzy-parameter PDE model for obtaining the fuzzy option prices and we develop dominating optimal hedging strategies under fuzzy environments which provide valuable insights for risk management and trading in financial markets.	black–scholes model;fuzzy set;nonlinear system;risk management;volatility	Hua Li;Antony Ware;Lan Di;George Yuan;Anatoliy Swishchuk;Steven Yuan	2018	Fuzzy Sets and Systems	10.1016/j.fss.2016.12.005	actuarial science;valuation of options	AI	1.142047665039575	-12.011619263909097	108476
ab08a63ce114761146bb25f0f56581fc6fac6168	data-driven frequency-based airline profit maximization	ensemble prediction;regression;airline demand and market share prediction;airline profit maximization	Although numerous traditional models predict market share and demand along airline routes, the prediction of existing models is not precise enough, and to the best of our knowledge, there is no use of data mining--based forecasting techniques for improving airline profitability. We propose the maximizing airline profits (MAP) architecture designed to help airlines and make two key contributions in airline market share and route demand prediction and prediction-based airline profit optimization. Compared to past methods used to forecast market share and demand along airline routes, we introduce a novel ensemble forecasting (MAP-EF) approach considering two new classes of features: (i) features derived from clusters of similar routes and (ii) features based on equilibrium pricing. We show that MAP-EF achieves much better Pearson correlation coefficients (greater than 0.95 vs. 0.82 for market share, 0.98 vs. 0.77 for demand) and R2-values compared to three state-of-the-art works for forecasting market share and demand while showing much lower variance. Using the results of MAP-EF, we develop MAP--bilevel branch and bound (MAP-BBB) and MAP-greedy (MAP-G) algorithms to optimally allocate flight frequencies over multiple routes to maximize an airline’s profit. We also study two extensions of the profit maximization problem considering frequency constraints and long-term profits. Furthermore, we develop algorithms for computing Nash equilibrium frequencies when there are multiple strategic airlines. Experimental results show that airlines can increase profits by a significant margin. All experiments were conducted with data aggregated from four sources: the U.S. Bureau of Transportation Statistics (BTS), the U.S. Bureau of Economic Analysis (BEA), the National Transportation Safety Board (NTSB), and the U.S. Census Bureau (CB).	benchmark (computing);branch and bound;broadcast television systems inc.;cluster analysis;coefficient;column (database);credit bureau;data mining;dynamic programming;emoticon;ensemble forecasting;entity framework;entropy maximization;expectation–maximization algorithm;experiment;frequency allocation;game theory;greedy algorithm;imax;kerrison predictor;kriging;machine learning;mathematical model;mathematical optimization;nash equilibrium;open-source software;random sample consensus;self-organizing map;source data;term (logic);ticket granting ticket;veracity;xiii	Bo An;Haipeng Chen;Noseong Park;V. S. Subrahmanian	2017	ACM TIST	10.1145/3041217	data mining;architecture;profitability index;profit (economics);branch and bound;computer science;ensemble forecasting;nash equilibrium;market share;profit maximization;microeconomics	ML	5.5067294507822755	-10.58218104303941	108621
3e9e9f575842f96c4a6bc26c825415ca67c04208	phase transition of urban freeway traffic flow	induced transition;congested traffic;empirical analysis;road traffic;intelligent transportation systems;urban freeway traffic flow;propagating transition;traffic engineering computing road traffic;free flow;smooth traffic;traffic flow;computational method;traffic control road transportation telecommunication traffic intelligent transportation systems predictive models helium road vehicles vehicle driving pattern analysis systems engineering and theory;beijing china;coherent moving flow;urban areas;phase transition;congested traffic urban freeway traffic flow traffic science free flow coherent moving flow synchronized flow jam spontaneous transition propagating transition induced transition beijing urban freeway smooth traffic;traffic density;traffic engineering computing;beijing urban freeway;freeways;jam;traffic science;conferences;synchronized flow;spontaneous transition	"""At present, research on traffic flow theory has mainly focused on highway traffic, which is significantly different from urban freeway traffic. Traffic science is an """"empirical science"""", and as such is it is based on empirical urban freeway traffic flow data. In this study, first four steady phases are identified in the flow-density plane of traffic flow: free flow, coherent-moving flow, synchronized flow and jam. Then, three modes of phase transition, i.e. spontaneous transition, propagating transition (PT) and induced transition (IT) are analyzed, and the judgment conditions as well as computable methods of PT and IT are discussed in detail. Finally, based on actual data of the Beijing urban freeway, an empirical analysis of phase transition from coherent-moving flow to synchronized flow, which is the process of """"smooth traffic"""" transferring to """"congested traffic"""", is presented to validate the concepts and methods proposed in this paper."""	coherence (physics);computable function;computational economics;freeway;jam;kosterlitz–thouless transition;open road tolling;spontaneous order;state transition table	Wei Guan;Shuyan He;Jihui Ma	2008	2008 11th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2008.4732519	simulation;fundamental diagram of traffic flow;geography;civil engineering;traffic congestion reconstruction with kerner's three-phase theory;traffic flow;three-phase traffic theory;transport engineering;traffic wave	EDA	9.564862303748331	-10.847687951403136	108657
374547d307359e221a6fffec6b7929e1c5f8292e	time series modeling of vulnerabilities	forecasting;time series modeling;vulnerabilties;web browsers;vulnerabilities;exponential smoothing;arima	Vulnerability prediction models forecast future vulnerabilities and can be used to assess security risks and estimate the resources needed for handling potential security breaches. Although several vulnerability prediction models have been proposed, such models have shortcomings and do not consider trend, level, and seasonality components of vulnerabilities. Through time series analysis, this study built predictive models for five popular web browsers: Chrome, Firefox, Internet Explorer, Safari and Opera and for all reported vulnerabilities elsewhere. Results showed that time series models provide a good fit to our vulnerability datasets and can be useful for vulnerability prediction. Results also suggested that the level of the series is the best estimator of the prediction models.	time series	Yaman Roumani;Joseph Nwankpa;Yazan F. Roumani	2015	Computers & Security	10.1016/j.cose.2015.03.003	exponential smoothing;autoregressive integrated moving average;forecasting;vulnerability;data mining;world wide web;computer security	Crypto	6.5834453915135365	-15.35600571917923	108732
707961793c3099e5650f892d4915315d610741f3	utility and dynamic social networks	reseau social;structural model;agent based simulation;real time;dynamical processes;simulation;utility function;actor position;acteur social;utility maximization;dynamic social networks;utilisation;social network;social actor;structure determination;model;social choice;modele;modele structurel;simulation model;position de l acteur;utilization	A dynamic process model of utility maximizing social actors embedded within social networks is developed. Actors make social choices under four combinations of tie formation and deletion rules: unilateral and mutual tie formation, and unilateral and mutual tie deletion. Ties are dual valued, parameterized with both a benefit and a cost. A utility function is used to assess the structurally determined utility of an actor's position in the social network. The process generates eight types of networks: Null, near-Null, Star, near-Star, Shared, near-Shared, Complete, near-Complete. Agent-based simulation modeling is the basic methodology used in these analyses. Two types of simulation models were developed: a multi-thread “real time” model, and a discrete event model.	social network	Norman P. Hummon	2000	Social Networks	10.1016/S0378-8733(00)00024-1	psychology;social choice theory;social science;artificial intelligence;simulation modeling;utilization;sociology;social psychology;social network	ML	-1.3024213126288544	-14.310292854972204	108945
03ec8c91e76ed4d188880777f8fa3d0abcea1b6a	a consensus model for group decision making with incomplete fuzzy preference relations	feedback mechanism;consensus;feedback mechanism group decision making fuzzy preference relations consensus reaching process selection process consistency criteria;fuzzy preference relations;group decision making gdm;uoa 23 computer science and informatics;indexing terms;aggregation;fuzzy set theory;fuzzy preference relation;group decision making gdm aggregation consensus fuzzy preference relations;incomplete information;decision making feedback aggregates information management computer science artificial intelligence computational intelligence humans;ordered weighted average;rae 2008;consensus reaching process;consistency criteria;fuzzy set theory decision making;group decision making;missing values;article;selection process	Two processes are necessary to solve group decision making problems: A consensus process and a selection process. The consensus reaching process is necessary to obtain a final solution with a certain level of agreement between the experts; and the selection process is necessary to obtain such a final solution. In a previous paper, we present a selection process to deal with group decision making problems with incomplete fuzzy preference relations, which uses consistency measures to estimate the incomplete fuzzy preference relations. In this paper we present a consensus model. The main novelty of this consensus model is that of being guided by both consensus and consistency measures. Also, the consensus reaching process is guided automatically, without moderator, through both consensus and consistency criteria. To do that, a feedback mechanism is developed to generate advice on how experts should change or complete their preferences in order to reach a solution with high consensus and consistency degrees. In each consensus round, experts are given information on how to change their preferences, and to estimate missing values if their corresponding preference relation is incomplete. Additionally, a consensus and consistency based induced ordered weighted averaging operator to aggregate the experts' preferences is introduced, which can be used in consensus models as well as in selection processes. The main improvement of this consensus model is that it supports the management of incomplete information and it allows to achieve consistent solutions with a great level of agreement.	aggregate data;consensus (computer science);consistency model;expectation propagation;feedback;google moderator;missing data;preference learning	Enrique Herrera-Viedma;Sergio Alonso;Francisco Chiclana;Francisco Herrera	2007	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.889952	group decision-making;consensus;index term;computer science;artificial intelligence;machine learning;data mining;uniform consensus;feedback;mathematics;fuzzy set;complete information	DB	-3.1533875012815056	-19.531115573177114	108953
572afb86af3119c86eed44c4e53eb5560edc31ee	assessing bids of greek public organizations service providers using data envelopment analysis		The existing framework for the procurement of products and services for the Greek Public Organizations describes specific criteria structure and fixedweighted formulas for the assessment of the provider’s bids. This assessment procedure suffers from specific shortcomings: it overestimates the price, it is very sensitive to small changes to performance indicators and especially for the services, is not able to incorporate variable price information. In this paper we develop a Data Envelopment Analysis model that overcomes the above mentioned shortcomings. It uses variable weights that are estimated in favor of each evaluated bid and are properly restricted to comply with the existing framework and to reflect criteria priorities. It also encounters ranges for prices that correspond to minimum and maximum expected number of service calls. For illustration purposes we provide a real case application for the assessment of courier service providers.	data envelopment analysis;linear programming;procurement	Maria Panta;Yiannis Smirlis;Michael Sfakianakis	2013	Operational Research	10.1007/s12351-011-0108-4	economics;marketing;operations management;management;commerce	Web+IR	-2.658175413269196	-13.56604660402171	109306
f3f0a5def65a6f6bf0826e5b4706ec7058087f62	linear regression prediction model of prefecture level highway passenger transport volume	optimal transportation;resource allocation;linear regression;regression model;transport system;mathematical model road transportation economics equations solid modeling data models predictive models;forecasting theory;roads;transportation forecasting theory regression analysis resource allocation roads socio economic effects;transportation;growth forecasting linear regression prediction model highway passenger transportation prefectures level highway prefecture passenger planning transport resource allocation highway passenger transport volume excel socioeconomic effects;regression analysis;prediction model;linear regression prediction model prefecture level highway passenger transport volume highway transport demand forecast;socio economic effects;demand forecasting	Prefecture level highway passenger transport occupied an important position in China highway transportation system. Correct forecast growth of prefecture highway passenger transport volume was the key for prefecture passenger planning and optimizing transport resource allocation. This paper analyzed the influence factors of prefecture level highway passenger transport volume and constructed linear regression prediction model of passenger transport volume. The model was calibrated by using specific prefecture highway passenger transport volume and socioeconomic data for 11 consecutive years by EXCEL. The highway passenger transport volumes of prospective years were forecasted. Example application show that explanatory variables are different because of different industy structure and economic activity economic levels. The calibrated regression model accurately reflects the economic and social reasons of the highway passenger transport volumes growth and the highway passenger transport volumes growth forecast has good explanatory.	coefficient;prospective search;signature block;social structure	Shuangying Xu;Jian Ma;Xiaolin Gao	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023040	demand forecasting;computer science;civil engineering;machine learning;transport engineering;regression analysis	Robotics	8.35043858087066	-11.528122461772634	109313
ff71cadb0cf721fef60e9c93b2cecc6519b2a534	a gray input-output model based on the standard interval grey number	grey matrix gray input output model standard interval grey number;biological system modeling;grey matrix;presses;matrix algebra;input output;computational modeling;gray input output model;mathematical model;grey systems;production;biological system modeling mathematical model economics presses fuzzy systems computational modeling production;matrix algebra grey systems;economics;fuzzy systems;standard interval grey number	A gray input-output model based on the standard interval grey number, the condition of convergence of M steps grey matrix and the solution method were given, then produced the corresponding mathematics proof in this paper. The model and solution method obtained a good result in the solution process of the gray input-output model. Finally, the validity and feasibility of this model was confirmed when used in a input-output example that just has two-departments.		Sifeng Liu;Zhigeng Fang;Zhou Wei;Aiqing Ruan	2008	2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)	10.1109/FUZZY.2008.4630532	input/output;computer science;artificial intelligence;mathematical model;mathematics;computational model;operations research;fuzzy control system	Robotics	3.805846714431622	-22.876727288489537	109372
fe5566baca1e5e8e956d299c85ee0941c33393f6	how to pick a winner almost every time: provably-good algorithms for decision making in the face of uncertainty.				Frank Thomson Leighton	1996			artificial intelligence;machine learning;operations research	Theory	-4.229268744357298	-12.077098451502593	109381
dce5d2dbf02cf3aaaaf17e80879f40fb8e2506ac	electricity price forecast for futures contracts with artificial neural network and spearman data correlation		Futures contracts are a valuable market option for electricity negotiating players, as they enable reducing the risk associated to the day-ahead market volatility. The price defined in these contracts is, however, itself subject to a degree of uncertainty; thereby turning price forecasting models into attractive assets for the involved players. This paper proposes a model for futures contracts price forecasting, using artificial neural networks. The proposed model is based on the results of a data analysis using the spearman rank correlation coefficient. From this analysis, the most relevant variables to be considered in the training process are identified. Results show that the proposed model for monthly average electricity price forecast is able to achieve very low forecasting errors.		João Nascimento;Tiago Pinto;Zita A. Vale	2018		10.1007/978-3-319-99608-0_2	econometrics;artificial neural network;spearman's rank correlation coefficient;futures contract;volatility (finance);correlation;distributed computing;electricity;computer science	AI	5.454301334795458	-16.849171782266826	109553
584d96c9816f5b8a488c67962f71ca1668b11944	extended vikor as a new method for solving multiple objective large-scale nonlinear programming problems	multicriteria analysis;multiobjective programming;programmation multiobjectif;generation colonne;90c06;non linear programming;systeme grande taille;nonlinear programming;generacion columna;systeme aide decision;multi criteria decision making;methode echelle multiple;programacion no lineal;teoria conjunto;theorie ensemble;metric;prise de decision;90c30;metodo escala multiple;programmation non lineaire;large scale system;sistema ayuda decision;set theory;large scale;multiple objectives;decision support system;hierarchical classification;programacion lineal;multiple decision;ideal solution;compromise programming;vikor method;linear programming;classification hierarchique;programmation lineaire;decision multiple;metrico;multiscale method;analisis multicriterio;90v29;analyse multicritere;toma decision;clasificacion jerarquizada;metrique;column generation;sistema gran escala;large scale systems;programacion multiobjetivo	The VIKOR method was introduced as a Multi-Attribute Decision Making (MADM) method to solve discrete decision-making problems with incommensurable and conflicting criteria. This method focuses on ranking and selecting from a set of alternatives based on the particular measure of “closeness” to the “ideal” solution. The multi-criteria measure for compromise ranking is developed from the l−p metric used as an aggregating function in a compromise programming method. In this paper, the VIKOR method is extended to solve Multi-Objective Large-Scale Non-Linear Programming (MOLSNLP) problems with block angular structure. In the proposed approach, the Y-dimensional objective space is reduced into a one-dimensional space by applying the Dantzig-Wolfe decomposition algorithm as well as extending the concepts of VIKOR method for decision-making in continues environment. Finally, a numerical example is given to illustrate and clarify the main results developed in this paper.	algorithm;angularjs;centrality;dantzig–wolfe decomposition;linear programming;nonlinear programming;numerical analysis	Majeed Heydari;Mohammad Kazem Sayadi;Kamran Shahanaghi	2010	RAIRO - Operations Research	10.1051/ro/2010011	column generation;mathematical optimization;metric;nonlinear programming;artificial intelligence;mathematics;ideal solution;algorithm;vikor method;set theory	Robotics	-0.47762508447926144	-16.858396897528877	109561
50ba24bdbcdcc32eefcbde67c4e273373d5c387a	dea modeling for efficiency optimization of indian banks with negative data sets		Indianbankinghasexperiencedexponentialgrowthafterreformsof1990s that helped to improve the profitability, performance and efficiency. However, still there are conflicting concerns of operating efficiency and risk management across the major bank categories particularly after the global financial crisis. We have used Data Envelopment Analysis (DEA) for measuring the efficiency of a set of decision makingunits(DMUs)whichtraditionallyassumesthatalltheinputandoutputvalues arenon-negative. Quantitativemeasuresofbankperformancelikenetprofits,growth rates and default portfolios frequently show negative values for output variables. We draw motivation from some studies done in other developing countries for handling the negative data sets. We cross examine the approaches for dealing with variables thatarepositiveforsomeDMUsandnegativeforothersandtestthevalidityofRange Directional Measure Model (RDM) for examining cases when some inputs and/or outputs can take negative as well as positive values. We find some support for the RDM inhandling data negative sets without the need for any transformation (conver- sion of the negative values with small positive values) as a measure of efficiency akin to the radial measures in traditional DEA. Our preliminary investigation indicates no significant difference between the operational efficiency and profitability of public and private banks modeled for negative data and undesirable output.		Pankaj Kumar Gupta;Seema Garg	2013		10.1007/978-3-319-07001-8_23	econometrics;actuarial science	ML	-1.8828960844279947	-13.149762610158097	109719
8633ffcda31d8541da9b24fb9571e7e6a74497f7	energy demand prediction using gmdh networks	forecasting;group method of data handling gmdh networks;emerging market;energy demand;forecasting model;time series;independent system operator;artificial neural networks;self organization;energy system;electric utilities;model test;electric power;group method of data handling;seasonal effect;artificial neural network;neural network;demand forecasting;self organizing networks	The electric power industry is in transition as it moves towards a competitive and deregulated environment. In this emerging market, traditional electric utilities as well as energy traders, power pools and independent system operators (ISOs) need the capability to predict as precisely as possible how much energy their customers will use in the near future. This paper presents a medium-term the end-use consumption sector of the energy system, representing residential, industrial, commercial, non-industrial, entertainment and public lighting load. The demand forecasting system is organized and implemented in a modular fashion using high accuracy forecast models. These models are developed for each sector to account for the growth trends and seasonal effects. A comparative evaluation of various traditional and neural network-based methods for obtaining the forecast of monthly energy demand was carried out. Among the models tested, artificial neural network (ANN)-based models were determined to produce better results. In particular, group method of data handling (GMDH) neural network, composed of self-organizing active neurons, was proven very effective in producing forecasts that were significantly more accurate and less labor-intensive than traditional time-series and regression-based models. & 2008 Elsevier B.V. All rights reserved.	artificial neural network;energy systems language;group method of data handling;organizing (structure);power supply;self-organization;sysop;time series;traders	Dipti Srinivasan	2008	Neurocomputing	10.1016/j.neucom.2008.08.006	self-organization;simulation;electric power;demand forecasting;forecasting;computer science;artificial intelligence;machine learning;group method of data handling;time series;self-organizing network;emerging markets;artificial neural network	AI	8.459273349427614	-17.869161536104173	109798
566ca702b0b61cefd37914f5d627b2adb71a35cf	comprehensive predictions of tourists' next visit location based on call detail records using machine learning and deep learning methods		Recent developments in data mining and machine learning have helped to solve many issues in prediction and recommendation. In this project, we run a comprehensive study on individual behavior patterns from call detail records (CDR) data to predict tourists' future stops. Multiple classification algorithms are employed, including Decision Tree, Random Forest, Neural Network, Naïve Bayes and SVM. In addition, a Recurrent Neural Network-Long Short Term Memory (LSTM) that is ordinarily applied to language inference problems is tested. Surprisingly, we find that LSTM provides us with the best prediction (94.8%), while Random Forest/Neural Network give the second best (85%). Our investigation suggests that the memory-dependence property of LSTM architecture gives it great expressive power to model our time-series location data, making it an outstanding classifier.	algorithm;artificial neural network;data mining;decision tree;deep learning;long short-term memory;machine learning;naive bayes classifier;random forest;recurrent neural network;time series	Nai Chun Chen;Wanqin Xie;Roy E. Welsch;Kent Larson;Jenny Xie	2017	2017 IEEE International Congress on Big Data (BigData Congress)	10.1109/BigDataCongress.2017.10	online machine learning;computer science;data mining;artificial neural network;architecture;deep learning;random forest;types of artificial neural networks;machine learning;recurrent neural network;learning classifier system;artificial intelligence;pattern recognition	ML	7.938969953175683	-23.320844238632468	109957
bdcbe3953f79ae2f73e1aef14e2c6cbf7fb7967c	on a class of weak triangular norm operators	modelizacion;agregacion;systeme intelligent;aggregation function;fuzzy system models;weighted averaging;sistema inteligente;logique floue;logica difusa;triangular norm;aggregation;dynamical system;fuzzy logic;modelisation;systeme dynamique;aggregation operator;boundary condition;intelligent system;agregation;sistema dinamico;modeling;fuzzy model	Some basic properties associated with aggregation operators are introduced. We introduce two classes of aggregation operators called weak t-norm and weak t-conorm operators. These classes are obtained by relaxing the associativity and boundary conditions of the t-norms and t-conorm, respectively. Using the fuzzy modeling technology, we construct families of these operators. The families developed involve a nonlinear weighted averaging of t-norms and t-conorms. the nonlinearity results from the fact that the weights used are themselves dependent on the argument values used in the aggregation. Particular emphasis is given, as used in this work, to the possibilities of using the fuzzy systems modeling technology as a tool for constructing aggregation functions from a specification of some properties of the desired operator.	t-norm	Ronald R. Yager	1997	Inf. Sci.	10.1016/S0020-0255(96)00140-5	fuzzy logic;ordered weighted averaging aggregation operator;mathematical optimization;operator theory;mathematical analysis;discrete mathematics;systems modeling;operator norm;boundary value problem;computer science;artificial intelligence;dynamical system;mathematics	DB	1.3816099158182529	-22.60674761860652	110243
af8a84ac370e495308da46eb72c58ab7f48a292d	individual and social strategies to deal with ignorance situations in multi-person decision making	consensus;fuzzy preference relations;fuzzy preference relation;incomplete information;ignorance;is strategy;article;consistency	Multi-Person Decision Making Problems involve the preferences of some experts about a set of alternatives in order to find the best one. However, sometimes experts might not possess a precise or sufficient level of knowledge of part of the problem and as a consequence that expert might not give all the information that is required. Indeed, this may be the case when the number of alternatives is high and experts are using fuzzy preference relations to represent their preferences. In the literature, incomplete information situations have been studied, and as a result, procedures which are able to compute the missing information of a preference relation have been designed. However, these approaches usually need at least a piece of information about every alternative in the problem in order to be successful in estimating all the missing preference values. In this paper we address situations in which an expert does not provide any information about a particular alternative, which we call situations of total ignorance. We analyse several strategies to deal with these situations. We classify these strategies into: i) individual strategies that can be applied to each individual preference relation without taking into account any information from the rest of experts and ii) social strategies, that is, strategies that make use of the information available from the group of experts. Both individual and social strategies use extra assumptions or knowledge which could not by directly instantiated in the experts preference relations. We also provide an analysis of the advantages and disadvantages of each one of the strategies presented, and the situations where some of them may be more adequate to be applied than the others.	fuzzy set	Sergio Alonso;Enrique Herrera-Viedma;Francisco Chiclana;Francisco Herrera	2009	International Journal of Information Technology and Decision Making	10.1142/S0219622009003417	consensus;computer science;knowledge management;mathematics;management science;consistency;complete information	Web+IR	-3.993135915761109	-19.585901256962693	110340
4983c6b3afc431dda45d8fb6f36f426a23818a5e	warrants price forecasting using kernel machine and ekf-ann: a comparative study	garch;anfis;neural networks;derivatives;extended kalman filters;kernel machine;black scholes;comparative study;warrants;svm;kernel machines	Due to the six unreasonable assumptions companioned with the Black-Scholes options pricing model (BSM), which often make the miss-pricing result because of the difference of market convention in practical. This study try to combine the BSM and extended Kalman filters-based artificial neural networks (EKF-ANN) to deal with the limitation of consideration of the influences from many unexpected real world phenomena. If we were to soundly take these phenomena into account, the pricing error could be reduced. In this paper, we try to make a comparative study with examined the forecasting accuracy between the BSM-based kernel machines (KM-BSM) and the BSM-based EKF-ANN (EKF-ANN-BSM). From the evidence of Taiwan Warrants market, we found that the performance indicates the KM is superior to the others, and the hybrid EKF-ANN-BSM framework is also better than the pure EKF-ANN. The results show that the KM-BSM and hybrid model could significantly reduce the normalized root-mean-squareerrors (NRMSE) of forecasting, it helps to provide an alternative way to refine the options valuation.	artificial neural network;black–scholes model;extended kalman filter;kernel method;openbsm;value (ethics)	Hsing-Wen Wang;Jian-Hong Wang;Tse-Ping Dong;Sheng-Hsun Hsu	2006		10.2991/jcis.2006.99	econometrics;engineering;artificial intelligence;machine learning;polynomial kernel	AI	6.7321782257581635	-18.12428008522641	110356
022acd3c2664e592c8ea94d5a708100cc4719bb3	towards a new multicriteria decision support method using fuzzy measures and the choquet integral	decision support;criteria interaction;ranking problem;amfi aggregation method by fuzzy integrals;fuzzy integrals;multicriteria analysis method mcam;aggregation of performances	In literature, there is a large panoply of multicriteria analysis methods MCAM, each one is characterized by the nature of its input data, the way to edit its outputs and the operations used to perform calculations especially the performances aggregation. Aggregation is the operation consisting in grouping several quantities in a unique value in order to facilitate the manipulation and the interpretation of the original values. MCAM are classified according to the type of aggregation that they perform, so we can distinguish total, partial and local aggregation. Each MCAM has advantages and suffers from some limits. In this paper, the authors proposed a new multicriteria analysis method AMFI dedicated to solve ranking decision support problems. AMFI is based on the use of fuzzy measures and Choquet integral to represent interactions between criteria and improve the coherence of the results. The authors proceeded to a series of experimentations allowing highlighting theoretical elements of the proposed method and they performed sensitivity analysis to test its robustness.	fuzzy measure theory	Emdjed Alnafie;Djamila Hamdadou;Karim Bouamrane	2016	IJFSA	10.4018/IJFSA.2016010104	mathematical optimization;machine learning;data mining;mathematics	Robotics	-3.587362945168764	-19.937087274138804	110509
1d68a1a9df7d23e5429d6926c9dccbd55b17745b	fuzzy multi-criteria decision making on combining fuzzy analytic hierarchy process with representative utility functions under fuzzy environment		In 1980, Saaty proposed the analytic hierarchy process (AHP) to evaluate alternatives with multi-criteria being multi-criteria decision making. Then, numerous approaches engaged on extension of AHP under fuzzy environment named fuzzy analytic hierarchy process (FAHP) for evaluation of multi-criteria alternatives under fuzzy environment being fuzzy multi-criteria decision making (FMCDM). In the current approaches, the extent analysis method proposed by Chang in 1996 was a famous FAHP method for FMCDM. However, computing priorities in matrix by Chang’s method is difficult for comparing pairwise fuzzy numbers, and calculating possibility degrees has drawback for some special fuzzy numbers. To resolve above ties, we combine FAHP with representative utility functions under fuzzy environment. Through combination of FAHP and representative utility functions to FMCDM, our method easily and quickly solves FMCDM problems.	analytical hierarchy	Yu-Jie Wang	2018	Soft Comput.	10.1007/s00500-016-2428-z	artificial intelligence;fuzzy logic;machine learning;fuzzy set operations;analytic hierarchy process;management science;pairwise comparison;defuzzification;computer science;fuzzy classification;fuzzy number	ECom	-3.745102713758207	-19.58355929200352	110777
90dde6ba09561e1592dc6c8c348b6406dd2ed2c3	the impact of unidentified location effects on dispersion-effects identification from unreplicated factorial designs	unreplicated factorial design;dispersion-effects identification;unidentified location effect;fractional factorial design;design of experiment;cumulant;factorial design;quality improvement;design of experiments	There have been several publications on identifying dispersion effects from an unreplicated fractional or full factorial experiment. Some of these methods implicitly assume that unknown location effects are first identified correctly from the data. In practice, however, an empirical location-effects identification procedure may leave some small to moderate location effects undetected. It is shown in this study that such unidentified location effects can cumulatively impair subsequent dispersione effects identification. Numerical evidence based on a welding experiment and results from a simulation study are provided. A mathematical explanation of the impact of unidentified location effects on dispersion-effects identification is given. The dispersion-effects identification methods initially proposed for unreplicated data can be naturally extended to replicated experiments with the impact of unidentified location effects eliminated. A fluorescent-lamp experiment is analyzed to illustrate these methods.		Guohua Pan	1999	Technometrics	10.1080/00401706.1999.10485931	econometrics;quality management;mathematics;design of experiments;statistics	HCI	6.220850583519874	-13.263239792769795	110804
452043dd4595ee001a4630ac4fa78231062c7c9a	product of interval-valued fuzzy graphs and degree				Hossein Rashmanlou;Madhumangal Pal;Rajab Ali Borzooei;F. Mofidnakhaei;Biswajit Sarkar	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-181488		Robotics	0.9819305370427573	-23.28199575955087	111100
f85099478a19b3480a9199468fc5d4bffe9a4ba3	using a combination of weighting methods in multiattribute decision-making		In this paper we introduce a preliminary approach for a weighting method that is intended to overcome possible biases detected in traditional weighting methods. The procedure is based on a combined application of several methods, SWING weighting, TRADE-OFFS weighting or Direct point allocation. It benefits from the value tree and the propagation of attribute weights through the tree to perform consistency checks and admits imprecision concerning the DM responses, whcih leads to imprecise local and attribute weights. However, the possible inclusion of other weighting methods in the process and the analysis of associated biases still needs to be analyzed.		Antonio Jiménez;Sixto Ríos-Insua;Alfonso Mateos	2005		10.1007/3-540-32539-5_120	a-weighting;mathematical optimization;mathematics;decision support system;weighting	NLP	-3.4679103985975743	-19.174352114664497	111322
062db33737c40586cda6c05601f850d58edae116	an exchange rate forecasting method based on probabilistic neural network	probabilistic logic exchange rates biological neural networks accuracy training prediction algorithms forecasting;probability;neural nets;economic forecasting;complex market exchange rate forecasting method probabilistic neural network foreign exchange market volatility characteristics exchange rate formation mechanism nonlinear system vector dimensionality;exchange rates;exchange rate;probabilistic neural network;forecasting method;probabilistic neural network exchange rate forecast;probability economic forecasting exchange rates neural nets	Foreign exchange market is a complex market, with a high degree of volatility characteristics. Exchange rate formation mechanism and the factors affecting exchange rate volatility is also very complex, is a nonlinear system, it is difficult to accurately forecast, probabilistic neural network is applied to the frontiers of forecast, and aimed at the characteristics of probabilistic neural network to pretreatment the exchange of data and forecast the tendency. And by changing the vector dimensionality experiment obtain the best entry to embed dimensionality, tested and improved the precise prediction and valuable.	artificial neural network;foreign exchange service (telecommunications);nonlinear system;probabilistic neural network;volatility	Hua Wang;Bingxiang Liu;Xiang Cheng;Xuan Xiao	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023749	probabilistic forecasting;econometrics;probabilistic neural network;computer science;machine learning;economic forecasting;probability;mathematics;artificial neural network;statistics	Robotics	7.301485278562403	-19.133410494795978	111674
9e0b1d1f0b93311f27a6385197e131b17875f96e	subsequence dynamic time warping for charting: bullish and bearish class predictions for nyse stocks		Abstract Advanced pattern recognition algorithms have been historically designed in order to mitigate the problem of subjectivity that characterises technical analysis (also known as ‘charting’). However, although such methods allow to approach technical analysis scientifically, they mainly focus on automating the identification of specific technical patterns. In this paper, we approach the assessment of charting from a more generic point of view, by proposing an algorithmic approach using mainly the dynamic time warping (DTW) algorithm and two of its modifications; subsequence DTW and derivative DTW. Our method captures common characteristics of the entire family of technical patterns and is free of technical descriptions and/or guidelines for the identification of specific technical patterns. The algorithm assigns bullish and bearish classes to a set of query patterns by looking the price behaviour that follows the realisation of similar, in terms of price and volume, historical subsequences to these queries. A large number of stocks listed on NYSE from 2006 to 2015 is considered to statistically evaluate the ability of the algorithm to predict classes and resulting maximum potential profits within a test period that spans from 2010 to 2015. We find statistically significant bearish class predictions that generate on average significant maximum potential profits. However, bullish performance measures are not significant.	dynamic time warping	Prodromos Tsinaslanidis	2018	Expert Syst. Appl.	10.1016/j.eswa.2017.10.055	subsequence;machine learning;data mining;artificial intelligence;realisation;derivative (finance);stock (geology);generic point;dynamic time warping;technical analysis;computer science	Theory	4.193125894151573	-16.998628447442073	111959
57eea4ebb4d9dcda6166b230c6504b3deb0bcb11	an evolutionary approach for short-term traffic flow forecasting service in intelligent transportation system		In recent years, traffic flow prediction has become a crucial technique in ITS (intelligent transportation system), which is helpful for alleviating the congestion in many metropolises and improving the efficiency of public traffic service. On the other hand, with the development of traffic sensors, traffic data are collected with a fantastic scale. It leads ITS into a data-driven application fashion. With this observation, it is a challenge to accurately and promptly forecast the traffic flow by effectively utilizing the big traffic data. In view of this challenge, in this paper, we propose an evolutionary method for short-term traffic flow forecasting service. Concretely, in our method, traffic flow is firstly specified by a model of time series. Then, the model is decomposed into seasonal component and the residual component. The seasonal component reflects the history average condition, while we treat the residual component as the output of a linear filter. The proposed method is evaluated with real bus transaction dataset. The experimental results show the effectiveness of our method.		Fan Fei;Shu Li;Wan-Chun Dou;Shui Yu	2016		10.1007/978-3-319-52015-5_49	simulation;transport engineering;advanced traffic management system;computer network	Robotics	8.717922337021236	-14.462922877255211	112056
cd140ba8974c4ff969840239ace0277b1ec206e3	measuring discrimination using principles of stochastic dominance	stochastic dominance;discrimination	This note develops a new approach to measuring discrimination. A partial ordering of discrimination patterns is proposed that is consistent with the properties of second-degree stochastic dominance (SSD), which are related to changes in the distributions of either the reference (advantaged) or comparison (disadvantaged) group, while keeping the other group's distribution unchanged. Furthermore, a corresponding summary index is derived. This index provides a complete ordering to rank discrimination patterns and also satisfies the principles of SSD.		Michael Hoy;Rachel J. Huang	2017	J. Economic Theory	10.1016/j.jet.2016.07.005	econometrics;discrimination;economics;stochastic dominance;mathematics;welfare economics;statistics	ECom	-1.460541030966204	-19.36879080383915	112128
03acfea42938a3eb88e5c599750e24b217cab9ea	efficient forecasting for hierarchical time series	forecasting;time series;hierarchies;optimization	Forecasting is used as the basis for business planning in many application areas such as energy, sales and traffic management. Time series data used in these areas is often hierarchically organized and thus, aggregated along the hierarchy levels based on their dimensional features. Calculating forecasts in these environments is very time consuming, due to ensuring forecasting consistency between hierarchy levels. To increase the forecasting efficiency for hierarchically organized time series, we introduce a novel forecasting approach that takes advantage of the hierarchical organization. There, we reuse the forecast models maintained on the lowest level of the hierarchy to almost instantly create already estimated forecast models on higher hierarchical levels. In addition, we define a hierarchical communication framework, increasing the communication flexibility and efficiency. Our experiments show significant runtime improvements for creating a forecast model at higher hierarchical levels, while still providing a very high accuracy.	experiment;time series	Lars Dannecker;Robert Lorenz;Philipp Rösch;Wolfgang Lehner;Gregor Hackenbroich	2013		10.1145/2505515.2505622	simulation;forecasting;machine learning;time series;hierarchy;statistics	AI	6.375517023445232	-15.892757841628141	112218
5b77f0f9fea0fbf2bcb0abc1e60c9db9619b94b0	operating heavy duty vehicles under extreme heat conditions: a fuzzy approach for smart gear-shifting strategy	heat;military heavy duty vehicles extreme heat conditions fuzzy approach smart gear shifting strategy public transportation fuzzy controller terrains construction logistics;neural networks;gears vehicles heating engines roads neural networks computational modeling;fuzzy control;intelligent control systems;road traffic control;traffic platooning;overheating;gear shifting;fuzzy controllers;weather conditions;road vehicles fuzzy control road traffic control;heavy duty vehicles;road vehicles	Heavy duty vehicles are vastly used in numerous fields such as public transportation, construction, logistics and military. The increased possibility of overheating the engine of such vehicles, when operating under extreme heat conditions, is a well-known and widely accepted problem which in turn results on stopping the vehicle and waiting to cool down. In this paper a novel fuzzy controller is presented that handles with such overheating issues. Several simulated experiments were performed in order to evaluate the controller's applicability and performance in various terrains. Results show an improvement on the performance of each vehicle as well as the whole convoy.	ct scan;controller (computing);controller (control theory);experiment;external variable;fuzzy logic;fuzzy rule;fuzzy set;logistics;matlab;multistage interconnection networks	Stefanos Skalistis;Dobrila Petrovic;Siraj Ahmed Shaikh	2013	16th International IEEE Conference on Intelligent Transportation Systems (ITSC 2013)	10.1109/ITSC.2013.6728356	simulation;engineering;automotive engineering;transport engineering	Robotics	9.175999079460158	-12.193726681040351	112500
7880fd7f46ab1ede0e38e61e867e873f0e665f6c	integrated feature selection of arima with computational intelligence approaches for food crop price prediction		Because of global climate change, lack of arable land, and rapid population growth, the supplies of three major food crops (i.e., rice, wheat, and corn) have been gradually decreasing worldwide. The rapid increase in demand for food has contributed to a continuous rise in food prices, which directly threatens the lives of over 800 million people around the world who are reported to be chronically undernourished. Consequently, food crop price prediction has attracted considerable attention in recent years. Recent integrated forecasting models have developed various feature selection methods (FSMs) to capture fewer, but more important, explanatory variables. However, one major problem is that the future values of these important explanatory variables are not available. Thus, predictions based on these variables are not actually possible. Because an autoregressive integrated moving average (ARIMA) can extract important self-predictor variables with future values that can be calculated, this study incorporates an ARIMA as the FSM for computational intelligence (CI) models to predict three major food crop (i.e., rice, wheat, and corn) prices. Other than the ARIMA, the components of the proposed integrated forecasting models include artificial neural networks (ANNs), support vector regression (SVR), and multivariate adaptive regression splines (MARS). The predictive accuracies of ARIMA, ANN, SVR, MARS, and the proposed integrated model are compared and discussed. Experimental results reveal that the proposed integrated model achieves superior forecasting performance for predicting food crop prices.		Yuehjen E. Shao;Jun-Ting Dai	2018	Complexity	10.1155/2018/1910520	multivariate adaptive regression splines;support vector machine;machine learning;artificial intelligence;mathematics;artificial neural network;computational intelligence;autoregressive integrated moving average;arable land;feature selection;food prices	AI	7.292289536592051	-18.684214707648827	112566
d7100b599dc18ef089383d21f65c2958b009c4c5	acceptability analysis and priority weight elicitation for interval multiplicative comparison matrices	logarithmic least square;decision analysis;acceptability;interval multiplicative comparison matrix;consistency	Existing research on acceptability of pairwise interval comparison matrices focuses on acceptable consistency by controlling their inconsistency levels to within a certain threshold. However, a perfectly consistent but highly indeterminate interval comparison matrix can be unacceptable as it contains little (sometimes no) useful decision information. This paper first analyzes the current definition of acceptable consistency for interval multiplicative comparison matrices (IMCMs) and shows its technical deficiencies. We then introduce a new notion of acceptable IMCMs, considering both inconsistency and indeterminacy levels in IMCMs. A geometric-mean-based index is proposed to measure the indeterminacy ratio of an IMCM, and useful properties are derived for consistent IMCMs and acceptable IMCMs. An indeterminacy-ratio and geometric-mean-based transformation equation is subsequently put forward to convert normalized acceptable interval multiplicative weights into an acceptable IMCM with consistency. By introducing an auxiliary constraint, a logarithmic least square model is established to generate interval multiplicative weights from acceptable IMCMs. A geometric-mean-based possibility degree formula is designed to compare and rank normalized interval multiplicative weights. Two numerical examples are presented to illustrate how to utilize the proposed framework.		Kevin W. Li;Zhou-Jing Wang;Xiayu Tong	2016	European Journal of Operational Research	10.1016/j.ejor.2015.09.010	mathematical optimization;combinatorics;discrete mathematics;economics;decision analysis;mathematics;consistency;statistics	Theory	-3.420482999358203	-19.891555963227066	112876
3685bdd6abe37c3fd9cdd496392806c9e2728d4a	"""comments on """"system dynamics models: some obscurities"""""""	system dynamics modeling;history;magnetohydrodynamic power generation;time series analysis;aggregates;pipelines;econometrics;parameter estimation;inventory control	The deliberate simplification of theory in system dynamics (SD) methods, and the conventions of DYNAMO are the main reasons for apparent obscurities in SD. Memory, solution interval, and delay processes all have analogies outside SD which can be fruitfully studied. Overemphasis on relating dynamics to apparent loop polarity can itself be couterintuitive.	system dynamics	J. A. Sharp;C. J. Stewart	1980	IEEE Trans. Systems, Man, and Cybernetics	10.1109/TSMC.1980.4308398	inventory control;econometrics;mathematical optimization;simulation;computer science;artificial intelligence;time series;control theory;mathematics;pipeline transport;system dynamics;estimation theory;statistics	Embedded	5.188140075176024	-12.416593532966402	113353
ade5154c8683cb231fd2f4155f67c954130eee60	algebraic representations of the weighted mean	triangular conorms norms and uninorms;priority normalization;aggregation on semifield domains;pairwise comparison matrices;weighted mean;consistency and anti consistency;additive and multiplicative structures	We introduce a general framework for the algebraic representation of the weighted mean in the case in which priorities and values refer to a common aggregation domain, for instance in hierarchical multicriteria decision models of the AHP type. The general framework proposed is based on the semifield structure of open interval domains and provides a natural algebraic description of weighted mean aggregation, including the essential mechanism of normalization which transforms priorities into weights. Such description necessarily involves two operations, addition (abelian semigroup) and multiplication (abelian group), which generalize the role of addition and multiplication in   P=(0,∞)     P  =  (  0  ,  ∞  )       . In this sense, the semifield framework extends recent work by Cavallo, D'Apuzzo, and Squillante on the multiplicative group structure on the basis of the representation of pairwise comparison matrices and their associated priority vectors. We consider open interval domains   S⊆R     S  ⊆  R        whose semifield structures are generated by bijections   ϕ:S⊆R→P     ϕ  :  S  ⊆  R  →  P       . Continuous (thus strictly monotonic) bijections play a central role. In such case, continuous strict triangular conorms/norms and uninorms emerge naturally in the representation of the semifield structure and, in their weighted version, they also provide the representation of the weighted mean, in both the arithmetic and geometric forms.		Silvia Bortot;Ricardo Alberto Marques Pereira	2017	Fuzzy Sets and Systems	10.1016/j.fss.2016.07.007	combinatorics;mathematical analysis;discrete mathematics;weighted arithmetic mean;mathematics;statistics	Robotics	0.7201464695157124	-21.537077935783692	113398
4af6ff9fb87a2ca179ccd5b26ec768481c9a2158	electric load forecasting by the svr model with differential empirical mode decomposition and auto regression	electric load forecasting;support vector regression;auto regression;differential empirical mode decomposition	Electric load forecasting is an important issue for power utility, associated with the management of daily operations such as energy transfer scheduling, unit commitment, and load dispatch. Inspired by strong non-linear learning capability of support vector regression (SVR), this paper presents a SVR model hybridized with the differential empirical mode decomposition (DEMD) method and auto regression (AR) for electric load forecasting. The differential EMD method is used to decompose the electric load into several detail parts associated with high frequencies (intrinsic mode function (IMF)) and an approximate part associated with low frequencies. The electric load data from the New South Wales (NSW, Australia) market and the New York Independent System Operator (NYISO, USA) are employed for comparing the forecasting performances of different alternative models. The results illustrate the validity of the idea that the proposed model can simultaneously provide forecasting with good accuracy and interpretability.	hilbert–huang transform	Guo-Feng Fan;Li-ling Peng;Wei-Chiang Hong;Fan Sun	2016	Neurocomputing	10.1016/j.neucom.2015.08.051	support vector machine;econometrics;simulation;computer science;machine learning;autoregressive model	ML	8.516728003393377	-18.169174877447556	113675
d4111ae37c7d0fad27b721526ef149ded5ff863c	a note on some algebraic properties of discrete sugeno integrals		Abstract Based on the link between Sugeno integrals and fuzzy measures, we discuss several algebraic properties of discrete Sugeno integrals. We recall that the composition of Sugeno integrals is again a Sugeno integral, and that each Sugeno integral can be obtained as a composition of binary Sugeno integrals. In particular, we discuss the associativity, dominance, commuting and bisymmetry of Sugeno integrals.	linear algebra	Radomír Halas;Radko Mesiar;Jozef Pócs;Vicenç Torra	2019	Fuzzy Sets and Systems	10.1016/j.fss.2018.01.009	algebraic number;mathematics;fuzzy logic;discrete mathematics;binary number;associative property;sugeno integral	Logic	0.41563262497993064	-23.019030278383187	113840
4f189ac69da80f596cd0010bc7a62159b4b45347	efficient andness-directed importance weighted averaging operators	object recognition;importance weighting;power means;weighted averaging;owa;information retrieval;andness;decision problem;aggregation operator;information processing;query answering;owa operator;maximum entropy;averaging operator	Importance weighted averaging is a central information processing task in multicriteria decision problems of many kinds, such that selection, classification, object recognition, query answering, and information retrieval. These problems are characterized by a query, i.e., a set of importance weighted criteria, and a set of options queried. While each criterion determines a ranking of the options, the task of the averaging operator is essentially to aggregate these rankings into an overall ranking under the consideration of the criterion importance. We present a class of such operators, based on the power means, namely the Andness-directed Importance Weighted Averaging (AIWA) operators. The operators are equipped with an approximate andness measure allowing an easy, direct control of the andness in the unit interval. The aggregation behavior of the operators appears to be similar to that of importance weighted maximum entropy OWA operators. However, AIWA operators aggregates n arguments (criterion satisfaction values) in O(n) time as opposed to O(n log n) time for OWA operators. An interesting property provided by AIWA operators is decomposability, allowing us to consider new or improved criteria without recomputing with all arguments. Overall, the AIWA operators appear to be effective as andness controlled, importance weighted averaging operators, as well as easy to apply and computationally efficient.	aggregate data;algorithmic efficiency;approximation algorithm;decision problem;fuzzy logic;information access;information processing;information retrieval;maxima and minima;maximal set;outline of object recognition;selection (user interface)	Henrik Legind Larsen	2003	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488503002272	ordered weighted averaging aggregation operator;mathematical optimization;discrete mathematics;information processing;computer science;principle of maximum entropy;cognitive neuroscience of visual object recognition;decision problem;data mining;mathematics;statistics	AI	-1.1583076461772046	-20.79986815619708	113954
28135b7e2552f0d8abe8e8504fe07fe6f2503a7f	an improvement in dea cross-efficiency aggregation based on the shannon entropy	weights;aggregation;cross efficiency;variance coefficient method	This paper improves a recently proposed Data Envelopment Analysis (DEA) cross-efficiency aggregation method based on the Shannon entropy. The weights for determining cross-efficiency are derived from minimizing the square distance of weighted cross-efficiency and weighted CCR efficiency. Our calculation example indicates that this method may produce inappropriate weights, which is significantly inconsistent with a widely accepted viewpoint. A variance coefficient method based on the Shannon entropy is presented to overcome the drawbacks of the DEA cross-efficiency aggregation method. In this study, comparisons of weights and cross-efficiency scores are provided.	entropy (information theory);shannon (unit)	Lianlian Song;Fan Liu	2018	ITOR	10.1111/itor.12361	econometrics;delegation;computer science;mathematics;mathematical economics;statistics	Vision	-0.9761742232785756	-19.321291698624897	114098
612f6765c31ed4ff7ccec78aa27e829ce1487f02	generalized complex fuzzy set-valued integrals and their properties	loss measurement;integral equations;two dimensional displays;fuzzy sets;extraterrestrial measurements;fuzzy systems;knowledge discovery	In this paper the fuzzy measure and fuzzy measurable functions are further extended to broader complex fuzzy sets, the concepts of complex set-valued simple function and generalized complex fuzzy set-valued integrals are proposed, the basic properties of generalized complex fuzzy set-valued integrals are studied, and obtain two important convergent theorems of complex fuzzy set-valued integrals.	fuzzy measure theory;fuzzy set	Shengquan Ma;Dejun Peng;Zhiqing Zhao	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603298	mathematical analysis;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;knowledge extraction;fuzzy associative matrix;fuzzy set operations;integral equation;fuzzy control system	Robotics	-0.6201977497054357	-23.158891212518498	114252
1d00e3dd80d0a9163215741826e90856008a98f5	a hybrid adaptive rule based system for smart home energy prediction		The increase in energy prices combined with the environmental impact of energy production has made energy efficiency a key component towards the development of smart homes. An efficient energy management strategy for smart homes results in minimized electricity consumption leading to cost savings. Towards this goal, we investigate the impact of environmental factors on home energy consumption. Home energy demand is observed to be affected by environmental factors such as temperature, wind speed and humidity which are inherently uncertain. Analyzing the impact of these factors on electricity consumption is challenging due to the unpredictability of weather conditions and non-linear relationship between environmental factors and electricity demand. For demand estimation based on these time varying factors, a hybrid intelligent system is developed that integrates the adaptability of neural networks and reasoning of fuzzy systems to predict daily electricity demand. A smart home dataset is utilized to build an unsupervised artificial neural network known as the Self-Organizing Map (SOM). We further develop a fuzzy rule based system from the SOM to predict home energy demand. Evaluation of the system shows a strong correlation between home energy demand and environmental factors and that the system predicts home energy consumption with higher accuracy.	artificial intelligence;artificial neural network;fuzzy control system;fuzzy rule;home automation;hybrid intelligent system;nonlinear system;rule-based system;self-organizing map	J Jithish;Sriram Sankaran	2017			rule-based system;real-time computing;home automation;computer science	AI	9.61862988215693	-17.275336589609516	114286
9788c52211b3a9acddeb621118628ad0ee35316f	a kind of synthetic evaluation method based on the attribute computing network	theoretical model;model combination;attribute computing network;the attribute computing network;evaluation method;computational method;data mining;satisfactory degree function;computer network;simulation experiment;indexes;computational modeling;pattern recognition synthetic evaluation method attribute computing network qualitative mapping;synthetic evaluation method;pattern recognition decision theory;indexation;decision theory;pattern recognition;boundary study;qualitative mapping;synthetic evaluation the attribute computing network boundary study satisfactory degree function;computer networks pattern recognition educational institutions mathematics business mathematical model computational modeling statistics arithmetic benchmark testing;benchmark testing;recruitment;data models;synthetic evaluation	Based on input and output relationship of Qualitative Mapping(QM), the attribute computing network model has been created. It brings forward a kind of computing method using input to adjust qualitative benchmark of attribute network, which makes it possible to achieve pattern recognition. Now the new attribute computing network model combined pattern recognition with synthetic evaluation is established. Firstly qualitative benchmarks of indexes are gotten by boundary study, and then by way of marking, preference for indexes is obtained, and lastly a set of satisfactory degrees for indexes is computed and outputted in descending sequence which ameliorates the effect of old satisfactory degree. Finally the simulation experiment is carried out to validate the theoretical model.	benchmark (computing);computer simulation;input/output;item unique identification;network model;pattern recognition;synthetic intelligence	Xiaolin Xu;Guanglin Xu;Jiali Feng	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255044	database index;data modeling;benchmark;decision theory;computer science;artificial intelligence;theoretical computer science;machine learning;data mining;mathematics;computational model;statistics	Vision	4.696800243176013	-23.219190649128183	114307
d877d8232524637230badaf317e9dc866ce7b940	frontier-based vs. traditional mutual fund ratings: a first backtesting analysis	fdh;shortage function;dea;mutual fund rating;mean variance portfolio frontier	We explore the potential benefits of a series of existing and new non-parametric convex and non-convex frontier-based fund rating models to summarize the information contained in the moments of the mutual fund price series. Limiting ourselves to the traditional mean-variance portfolio setting, we test in a simple backtesting setup whether these efficiency measures fare any better than more traditional financial performance measures in selecting promising investment opportunities. The evidence points to a remarkable superior performance of these frontier models compared to most, but not all traditional financial performance measures.	backtesting;cluster analysis;coherence (physics);convex function;firefox;hierarchical clustering;omega;performance;sports rating system;star (classification)	Olivier Brandouy;Kristiaan Kerstens;Ignace Van de Woestyne	2015	European Journal of Operational Research	10.1016/j.ejor.2014.11.010	financial economics;actuarial science;economics;mutual fund separation theorem;commerce	Vision	2.209213821899027	-14.45578303350798	114339
69ac4f1250ee2e3527934de90eb9b30a6345064a	monthly maximum accumulated precipitation forecasting using local precipitation data and global climate modes	artificial neural networks;particle swarm optimization;regression analysis;seasonal autoregressive integrated moving average arima;extreme precipitation			Junaida Binti Sulaiman;Herdianti Darwis;Hideo Hirose	2014	JACIII	10.20965/jaciii.2014.p0999	computer science;machine learning;particle swarm optimization;artificial neural network;regression analysis	ML	8.839873263578596	-19.092734205566064	114442
715c908b981c2c7743567c34c127c191d76eec58	averaging operators in fuzzy classification systems	classification;aggregation operator;choquet integral;multi polarity;owa operator	We examine averaging operators used for aggregation of outputs of fuzzy classification rules. We discuss several methods used in fuzzy rule-based classification systems and show related multi-polar averaging operators. We further define new aggregation methods based on the idea used for definition of OWA operators. We show connection between multi-polar averaging operators and the multi-polar Choquet integral and using this connection we study the conditions under which are the respective averaging operators monotone. We include several examples of special multi-polar OWA operators and their relation to existing bipolar aggregation operators. © 2014 Published by Elsevier B.V.	ewald summation;fuzzy classification;fuzzy rule;logic programming;sugeno integral;utility functions on indivisible goods;monotone	Andrea Mesiarová-Zemánková;Khurshid Ahmad	2015	Fuzzy Sets and Systems	10.1016/j.fss.2014.06.010	ordered weighted averaging aggregation operator;mathematical optimization;mathematical analysis;discrete mathematics;biological classification;mathematics;choquet integral	Web+IR	-1.1253528002052093	-21.916991909128235	114453
f4b8f01eb12273ec518692aa25ab4d7c639ad3ca	optimized structure of the traffic flow forecasting model with a deep learning approach	pedestrian safety;poison control;injury prevention;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;forecasting predictive models artificial neural networks machine learning prediction algorithms data models computational modeling;poisoning prevention;falls;ergonomics;suicide prevention;stacked denoising autoencoders deep learning forecasting neural network nn applications	Forecasting accuracy is an important issue for successful intelligent traffic management, especially in the domain of traffic efficiency and congestion reduction. The dawning of the big data era brings opportunities to greatly improve prediction accuracy. In this paper, we propose a novel model, stacked autoencoder Levenberg-Marquardt model, which is a type of deep architecture of neural network approach aiming to improve forecasting accuracy. The proposed model is designed using the Taguchi method to develop an optimized structure and to learn traffic flow features through layer-by-layer feature granulation with a greedy layerwise unsupervised learning algorithm. It is applied to real-world data collected from the M6 freeway in the U.K. and is compared with three existing traffic predictors. To the best of our knowledge, this is the first time that an optimized structure of the traffic flow forecasting model with a deep learning approach is presented. The evaluation results demonstrate that the proposed model with an optimized structure has superior performance in traffic flow forecasting.	artificial neural network;autoencoder;big data;biological neural networks;computation;computational model;dawning information industry;deep learning;exptime;entity name part qualifier - adopted;freeway;geo-imputation;granulation procedure;greedy algorithm;kerrison predictor;levenberg–marquardt algorithm;machine learning;missing data;network congestion;physician:id:pt:lm glasses:nar;projections and predictions;sumo activating enzyme complex;selective area epitaxy;stationary process;statistical imputation;taguchi methods;time complexity;unsupervised learning;anatomical layer	Hao-Fan Yang;Tharam S. Dillon;Yi-Ping Phoebe Chen	2017	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2016.2574840	machine learning;autoencoder;artificial intelligence;pattern recognition;architecture;traffic flow;computer science;unsupervised learning;artificial neural network;data modeling;deep learning;taguchi methods	AI	8.744698595220198	-23.41265616726043	114491
4b631afce2f2a8b9b4c16c5e13c3765e75a38e54	load forecasting via deep neural networks		Nowadays, electricity plays a vital role in national economic and social development. Accurate load forecasting can help power companies to secure electricity supply and scheduling and reduce wastes since electricity is difficult to store. In this paper, we propose a novel Deep Neural Network architecture for short term load forecasting. We integrate multiple types of input features by using appropriate neural network components to process each of them. We use Convolutional Neural Network components to extract rich features from historical load sequence and use Recurrent Components to model the implicit dynamics. In addition, we use Dense layers to transform other types of features. Experimental results on a large data set containing hourly loads of a North China city show the superiority of our method. Moreover, the proposed method is quite flexible and can be applied to other time series prediction tasks. c © 2017 The Authors. Published by Elsevier B.V. Selectio and/or peer-review under responsibility of ITQM2017.	artificial neural network;convolutional neural network;deep learning;network architecture;neural networks;scheduling (computing);time series	Wan He	2017		10.1016/j.procs.2017.11.374	artificial intelligence;machine learning;architecture;artificial neural network;convolutional neural network;mains electricity;time series;computer science;scheduling (computing)	AI	8.79632152634623	-18.288472050354184	114551
095cb98be006008db5689adecda1a39e6f6692c7	fuzzy traffic controller in ramp metering of urban expressway	traffic simulation;fuzzy controller;inflow traffic control;ramp metering;urban expressway		ramp simulation software for modelling reliability, availability and maintainability	Masashi Okushima;Yoshiharu Takihi;Takamasa Akiyama	2003	JACIII	10.20965/jaciii.2003.p0207	computer science;machine learning;control theory;fuzzy logic;artificial intelligence;metering mode;control engineering;traffic simulation	HCI	9.24109760144874	-12.005747917484427	114702
6f75f2f3418b770a042b961cefe0762757a54e57	measuring energy intensity in japan: a new method	energy conservation;distance function;japan;energy intensity	Energy intensity and energy conservation have been important pillars of energy policies in Japan. Recently, the government has introduced new initiatives to enhance energy efficiency and reduce energy intensity. We analyze the energy intensity in Japan for the period 1973–2006 by proposing a new method which takes into account all other inputs used in production and corrects for the bias in the traditional energy intensity measure. We show that the traditional energy intensity measure has serious flaws. The traditional measure overestimates actual energy intensity before the mid-1980s and largely underestimates afterwards. It is found that aggregate energy intensity has risen remarkably from 1991 to 2001. The main cause of this rise is the rapid rise in energy intensity in manufacturing and energy sectors.		Osman Zaim;Tugçe Uygurtürk Gazel;K. Ali Akkemik	2017	European Journal of Operational Research	10.1016/j.ejor.2016.09.023	economics;energy conservation;metric;operations management;mathematics;economy;energy intensity	Robotics	2.9031357100457766	-14.258457143158186	114758
ad551bfc24e9903160828ae564753360478b5606	a nutrition evaluation system based on hierarchical fuzzy approach	uncertainty;hierarchical fuzzy system;nutrition status evaluation	In this paper, we propose a hierarchical fuzzy based nutrition evaluation system that can analyze the individuals' nutrition status through the inference results generated by each layer. Moreover, a method to minimize the uncertainty of inference in the evaluated nutrition status is discussed. To show the effect of the uncertainty in fuzzy inference, we compared the results of nutrition evaluation with/without the certainty factor of rules on 132 people over the age of 65. From the experimental results, we can see that the evaluation method with the modified certainty factor provides better reliability than that of the general evaluation method without the certainty factor.	fuzzy logic	Chang-S. Son;Gu-Beom Jeong	2008	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2008.8.2.087	econometrics;adaptive neuro fuzzy inference system;artificial intelligence;data mining;mathematics	AI	-4.051161010346442	-20.950855678737643	115048
47695c036de043ff262ff8302a961b29e58e932b	averages of ratios compared to ratios of averages: mathematical results	average of ratios;ratio of averages;journal contribution;global impact factor;average impact factor;crown indicator	The recently awakened discussion on the usability of averages of ratios (AoR) compared to ratios of averages (RoA) has led to the mathematical results in this paper. Based on the empirical results in Lariviere and Gingras (2011) we prove, under reasonable conditions, the following relations between AoR and RoA for a set of points:(i)The regression line of RoA in function of AoR is the first bisectrix.(ii)(AoR−RoA)/AoR in function of the number N of papers is a cloud of points comprised between a multiple of 1/N and −1/N.(iii)(AoR−RoA)/AoR versus RoA has a decreasing regression line.		Leo Egghe	2012	J. Informetrics	10.1016/j.joi.2011.12.007	econometrics;calculus;mathematics;statistics	HPC	1.8413597618572328	-15.339141629701311	115073
119f3e3dfb7beeb90e74463b97f384dd3c60b9c3	real time robot soccer game event detection using finite state machines with multiple fuzzy logic probability evaluators	real time;event detection;fuzzy logic;robot soccer;finite state machine	This paper presents a new algorithm for real time event detection using Finite State Machines with multiple Fuzzy Logic Probability Evaluators (FLPEs). A machine referee for a robot soccer game is developed and is used as the platform to test the proposed algorithm. A novel technique to detect collisions and other events in microrobot soccer game under inaccurate and insufficient information is presented. The robots’ collision is used to determine goalkeeper charging and goal score events which are crucial for the machine referee’s decisions. The Main State Machine (MSM) handles the schedule of event activation. The FLPE calculates the probabilities of the true occurrence of the events. Final decisions about the occurrences of events are evaluated and compared through threshold crisp probability values. The outputs of FLPEs can be combined to calculate the probability of an event composed of subevents. Using multiple fuzzy logic system, the FLPE utilizes minimal number of rules and can be tuned individually. Experimental results show the accuracy and robustness of the proposed algorithm.		Elmer P. Dadios;Soo ho Park	2009	Int. J. Computer Games Technology	10.1155/2009/375905	fuzzy logic;real-time computing;simulation;computer science;artificial intelligence;machine learning;finite-state machine	AI	6.348556009819804	-12.55754148862548	115115
11aa673936699a923da4c5ba6ce164f2760b6a81	development of an operational model-based warning system for tropospheric ozone concentrations in bordeaux, france	human health;soft computing;environmental effect;nonlinear adaptive estimation;gain scheduling;tropospheric ozone;air pollution;state space;air quality monitoring;smog duration;multi layer perceptron;development methodology;ozone;air quality;short term forecasting;adaptive estimation;neural network	The paper describes the status of an on-going research program to develop a highly reliable operational public warning system for air pollution monitoring in Bordeaux, France.#R##N##R##N#Experimental results are presented for ground-level ozone concentrations. Meteorological variables are used as input in order to obtain an estimate of the next day’s maximum ozone concentration. Moreover, the warning system provides additional information regarding the duration of a smog episode that is very important for assessments of human health hazards and negative environmental effects.#R##N##R##N#The developed methodology is based on hard and soft computing techniques. The proposed approach combines new adaptive nonlinear state space modelling techniques, a gain scheduling strategy and multi-layer perceptron neural networks. A key characteristic of such a system is that its behaviour can be adapted to the short term changes of air pollution and consequently the model can handle the time-evolving nature of the phenomena and does not need frequent adjustments.#R##N##R##N#The monitoring software is developed in a MATLAB® environment.		Ali Zolghadri;Michel Monsion;David Henry;Christian Marchionini;O. Petrique	2004	Environmental Modelling and Software	10.1016/S1364-8152(03)00136-1	meteorology;ozone;air quality index;simulation;environmental engineering;computer science;state space;machine learning;soft computing;gain scheduling;multilayer perceptron;air pollution	Robotics	9.974430396112327	-18.774379948974175	115234
7800e816deaa16a7964bc860a9d4b122fcc4774c	investment location selection based on economic intelligence and macbeth decision aid model		In this paper, the authors present a case study that aims to apply some sound MCDM techniques in the case of Economic Intelligence EI and show how the use of strategic information may help deciders to choose among geographic locations in which they could settle their investments. In this regard, the authors propose a new method that uses the multi-criteria decision support of MACBETH to tackle this issue. This method is used to rank thirteen countries likely to be chosen for location in order of preference from good to unfavourable. The integration of the MCDM in Economic Intelligence EI permits to rank countries of the Mediterranean according to their territorial competitiveness obtained through the global scores computed by the aforementioned technique of MACBETH. The results obtained allow the authors to affirm that France and Morocco have favourable strategic assets to attract foreign investment.		Mostafa Bachrane;Abdelilah Khaled;Jamila El Alami;Mostafa Hanoune	2016	JITR	10.4018/JITR.2016070103	artificial intelligence;data mining;management;operations research;law	AI	-4.274192361791999	-15.09178316605385	115925
e6ed3d26e1bd14087b81ece930fa9bde1b4079dd	an aspect of discrepancy in the implementation of modus ponens in the presence of fuzzy quantities	modus ponens	In this note we discuss some aspects of a commonly used scheme of reasoning, namely modus ponens, currently used in the implementation o f fuzzy controllers. Some drawbacks of the existing algorithms leading to implementation of the inference scheme are clearly indicated. Given a set o f rules { Ak -'* Bk, k = 1 . . . . . N} and a fuzzy antecedent A~, the expected consequent Bk is generally not obtained, but a sufficient condition is proposed to get this result.	algorithm;discrepancy function;scheme	Antonio di Nola;Witold Pedrycz;Salvatore Sessa	1989	Int. J. Approx. Reasoning	10.1016/0888-613X(89)90018-2	discrete mathematics;modus ponens;deduction theorem;computer science;artificial intelligence;modus ponendo tollens;mathematics;algorithm	AI	-2.4611889672208065	-23.475884600297466	116124
207ed64cc5cb4ce3228cd11e1b467ef0fd0b9f87	model predictive control for ramp metering combined with extended kalman filter-based traffic state estimation	predictive control;predictive models predictive control kalman filters traffic control state estimation communication system traffic control filtering computer networks telecommunication traffic control systems;road traffic;e17 motorway ghent antwerp model predictive control ramp metering extended kalman filter traffic state estimation dynamic traffic control traffic flow model;traffic flow model;kalman filters;traffic control;traffic flow;state estimation;ramp metering;model predictive control;mathematical models;weather condition;traffic control kalman filters predictive control road traffic state estimation;prediction model;extended kalman filter;ramps interchanges;traffic measurement	Ramp metering is a dynamic traffic control measure that has proven to be very effective. There are several methods to determine appropriate ramp metering signals for a given traffic situation. In this paper, a framework consisting of model predictive control (MPC) for ramp metering, combined with extended Kalman filter-based (EKF) traffic state estimation is presented. Based on traffic measurements at a limited number of locations, the EKF is able to provide the MPC ramp metering controller with estimations of the traffic states in the motorway segments of the motorway stretch under control. By using the same traffic flow model in the EKF as in the MPC prediction model, some important model parameters of the MPC prediction model can be estimated and be fed directly to the MPC controller. This functionality enables the MPC prediction model to track changes in the traffic system (e.g. due to weather conditions, incidents, etc.). The presented EKF-MPC controller for ramp metering is simulated for a case study on the E17 motorway Ghent-Antwerp in Belgium	extended kalman filter;ramp simulation software for modelling reliability, availability and maintainability;software metering	Tom Bellemans;Bart De Schutter;Geert Wets;Bart De Moor	2006	2006 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2006.1706775	control engineering;simulation;engineering;control theory	Robotics	9.450308524448918	-12.467406430017446	116129
887e56ae9c1c01bc5a5859bddba173689cb497fa	multivariate shortfall risk allocation and systemic risk		The ongoing concern about systemic risk since the outburst of the global financial crisis has highlighted the need for risk measures at the level of sets of interconnected financial components, such as portfolios, institutions or members of clearing houses. The two main issues in systemic risk measurement are the computation of an overall reserve level and its allocation to the different components according to their systemic relevance. We develop here a pragmatic approach to systemic risk measurement and allocation based on multivariate shortfall risk measures, where acceptable allocations are first computed and then aggregated so as to minimize costs. We analyze the sensitivity of the risk allocations to various factors and highlight its relevance as an indicator of systemic risk. In particular, we study the interplay between the loss function and the dependence structure of the components, that provides valuable insights into the properties of good loss functions. Moreover, we provide numerical schemes to assess the risk allocation in high dimensions.	computation;it risk;loss function;numerical analysis;relevance;risk measure;software house	Yannick Armenti;Stéphane Crépey;Samuel Drapeau;Antonis Papapantoleon	2018	SIAM J. Financial Math.	10.1137/16M1087357	financial economics;risk assessment;actuarial science;risk analysis;economics;financial risk;welfare economics;time consistency;financial risk management	Metrics	-1.684774306738863	-12.398433749228987	116140
efac1f408d5e997b98119425446a9641362816e3	comprehensive measure and influential factors of fragility of commercial bank system in china	banking;financial management;fragility reduction comprehensive measure influential factors chinese commercial bank system fragility comparative empirical analysis state owned commercial bank joint stock commercial bank city commercial bank rural commercial bank fragility index car nplr roa roe c d fce ahp multivariable linear regression model eviews 6 0 financial environment explanatory variables macroeconomic environment indicators financial indicators banking industry market concentration deposit reserve ratio credit interest rate assets market share foreign banks healthy financial system;fragility state owned commercial banks joint stock commercial banks city commercial banks rural commercial banks;rural commercial banks;indexes equations cities and towns banking economic indicators;fuzzy set theory;indexes;state owned commercial banks;macroeconomics;regression analysis banking decision making economic indicators financial management fuzzy set theory macroeconomics;cities and towns;regression analysis;fragility;joint stock commercial banks;city commercial banks;economic indicators	In order to comparing fragility index and the influential factors of State-owned Commercial Bank, Joint-stock Commercial bank, City Commercial Bank and Rural Commercial Bank, CAR, NPLR, ROA, ROE and C/D are chosen to make up fragility index of systems of the four commercial banks respectively by combination of FCE & AHP from 2001 to 2010 in this research. Influential factors of fragility analyzed by formatting multivariable linear regression model by Eviews 6.0, 17 indicators which stand for macroeconomic environment and financial environment are chosen to be as explanatory variables. Empirical results showing that fragility of State-owned Commercial Bank and City Commercial Bank are higher than other two types significantly at the beginning, but lower at the end. The systems of four commercial banks are stable since 2005. Comparing with macroeconomic environment indicators, affection from financial indicators is more significant. Specially, market concentration of banking industry, adjustment range of deposit reserve ratio, spreads between deposit and credit interest rate, assets market share of foreign banks in China indicate strong correlation with fragility of commercial bank system. It is necessary for us to provide the healthy financial system and environment to reduce fragility of commercial banks in China.	corporate governance;resource-oriented architecture;social credit system	Xiaofeng Zhang;Lan Yao	2012	2012 Fifth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2012.54	actuarial science;finance;business	Robotics	2.627243829133266	-14.096736188677578	116211
31e7006ab83baf715b098b48ef804abde7d9e20f	classifying through an algebraic fuzzy structure: the relevance of the attributes	linguistique;fuzzy set;conjunto difuso;ensemble flou;classification;methode algebrique;linguistica;algebraic method;evaluation;sistema difuso;metodo algebraico;systeme flou;evaluacion;clasificacion;fuzzy system;linguistics	Abstract#R##N##R##N#An appropriate fuzzy structure was previously defined to generate appropriate classifications in a universe of discourse in which the characteristics of the elements are defined through attributes whose values are linguistic labels. the way the structure operates depends upon suitable operations on fuzzy attributes represented by fuzzy numbers. However, this classification method suffers from a main disadvantage, i.e., the inherent rigidity deriving from the circumstance that all the attributes share the same weight and, consequently, are taken into account completely disregarding the fact that usually some attributes are more relevant than others. In this article we analyze a suitable weighting rule allowing us to treat in a different way attributes whose relevance is different. the validity of the approach was checked by means of a case study concerning diets for breast-feeding women. © 1995 John Wiley & Sons, Inc.	linear algebra;relevance	Antonio Gisolfi	1995	Int. J. Intell. Syst.	10.1002/int.4550100803	biological classification;fuzzy classification;computer science;artificial intelligence;evaluation;mathematics;fuzzy set;algorithm;fuzzy control system	DB	-3.3181881413566017	-23.28386166853632	116379
00c4c8ef7893a5b06ac6c6a2f22a3bf0ad40d836	fuzzy soft set over a fuzzy topological space		The aim of this article, we combine the fuzzy soft set and fuzzy topological space. We introduce the notion of fuzzy soft set over common universe of fuzzy topological space. We defined quasi-open fuzzy soft set over fuzzy topological space and we study the properties of extended union of two quasi-open fuzzy soft set over fuzzy topological space. We study ‘‘OR’’ and ‘‘AND’’ operations of quasi-open fuzzy soft set over fuzzy topological space. We also define open fuzzy soft set, b-open fuzzy soft set and b-closed fuzzy soft set over fuzzy topological space. We give some characterizations of these concepts.	decision support system;fuzzy set;pattern recognition;set theory	Saleem Abdullah;Wafa S. Al Shammakh;Tahir Mahmood;Muhammad Shahzad	2016	Int. J. Machine Learning & Cybernetics	10.1007/s13042-015-0420-z	fuzzy logic;mathematical analysis;discrete mathematics;topology;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Web+IR	-0.8108884292192253	-23.368861491073798	116393
059e5b94983c102b7f1e40098a25776173598ca3	traffic flow breakdown prediction using feature reduction through rough-neuro fuzzy networks	traffic behavior;forecasting;traffic flow breakdown;rule basis;fuzzy neural nets;fuzzy set;fuzzy reasoning;flow process;human expert;neural networks;metropolitan area;rough set theory;multilayer perceptrons;fuzzy relation;automated highways;neuro fuzzy network;traffic control;fuzzy inference mechanism traffic flow breakdown prediction feature reduction rough neuro fuzzy networks traffic behavior prediction metropolitan area routing artificial neural network são paulo inference morphology dynamic routing rough sets theory multilayer perceptron radial basis function;traffic flow;multilayer perceptron;dynamic routing;fuzzy set theory;traffic parameters;fuzzy inference mechanism;fuzzy sets;radial basis function networks;feature reduction;technological improvements;radial basis function;dynamics;membership functions;neuro fuzzy;rough sets theory;fuzzy inference;routing performance;membership function;surface response;rough sets;trabalho apresentado em evento;traffic engineering computing;multi layer perceptron;fuzzy relations;routing process;traffic engineering computing automated highways fuzzy neural nets fuzzy reasoning multilayer perceptrons radial basis function networks rough set theory;traffic breakdown;fuzzy networks;traffic breakdown artificial neural network feature reduction fuzzy sets rough sets;rough set;artificial neural network;rough sets fuzzy sets humans inference mechanisms fuzzy neural networks artificial neural networks;radial basis functions	The prediction of the traffic behavior could help to make decision about the routing process, as well as enables gains on effectiveness and productivity on the physical distribution. This need motivated the search for technological improvements in the Routing performance in metropolitan areas. The purpose of this paper is to present computational evidences that Artificial Neural Network ANN could be use to predict the traffic behavior in a metropolitan area such São Paulo (around 16 million inhabitants). The proposed methodology involves the application of Rough-Fuzzy Sets to define inference morphology for insertion of the behavior of Dynamic Routing into a structured rule basis, without human expert aid. The dynamics of the traffic parameters are described through membership functions. Rough Sets Theory identifies the attributes that are important, and suggest Fuzzy relations to be inserted on a Rough Neuro Fuzzy Network (RNFN) type Multilayer Perceptron (MLP) and type Radial Basis Function (RBF), in order to get an optimal surface response. To measure the performance of the proposed RNFN, the responses of the unreduced rule basis are compared with the reduced rule one. The results show that by making use of the Feature Reduction through RNFN, it is possible to reduce the need for human expert in the construction of the Fuzzy inference mechanism in such flow process like traffic breakdown.	artificial neural network;computation;mathematical morphology;multilayer perceptron;quad flat no-leads package;radial (radio);radial basis function;rough set;routing	Carlos Affonso;Renato José Sassi;Ricardo P. Ferreira	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033462	rough set;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set;artificial neural network	AI	9.695023802728333	-20.36348027098834	116544
025a112b9d0dd7af4345a5bd999df0700640d9d0	genetic algorithm based multiobjective bilevel programming for optimal real and reactive power dispatch under uncertainty		This chapter presents how multiobjective bilevel programming (MOBLP) in a hierarchical structure can be efficiently used for modeling and solving optimal power generation and dispatch problems via genetic algorithm (GA) based Fuzzy Goal Programming (FGP) method in a power system operation and planning horizon. In MOBLP formulation of the proposed problem, first the objectives of real and reactive power (P-Q) optimization are considered as two optimization problems at two individual levels (top level and bottom level) with the control of more than one objective at each level. Then the hierarchically ordered problem is fuzzily described to accommodate the impression in P-Q optimization simultaneously in the decision making context. In the model formulation, the concept of membership functions in fuzzy sets for measuring the achievement of highest membership value (unity) of the defined fuzzy goals to the extent possible by minimising their under-deviational variables on the basis of their weights of importance is considered. The aspects of FGP are used to incorporate the various uncertainties in power generation and dispatch. In the solution process, the GA is used in the framework of FGP model in an iterative manner to reach a satisfactory decision on the basis of needs and desires of the decision making environment. The GA scheme is employed at two different stages. At the first stage, individual optimal decisions of the objectives are determined for fuzzy goal description of them. At the second stage, evaluation of goal achievement function for minimization of the weighted under-deviational variables of the membership goals associated with the defined fuzzy goals is considered for achieving the highest membership value (unity) of the defined fuzzy goals on the basis of hierarchical order of optimizing them in the decision situation. The proposed approach is tested on the standard IEEE 6-Generator 30-Bus System to illustrate the potential use of the approach.	genetic algorithm	Papun Biswas	2015		10.1007/978-3-319-11017-2_8	mathematical optimization	Robotics	-2.5395872536431434	-17.236369196717703	116752
8c81e5acd4dc8552859eb460a007c39c893bc58c	stable forecasting of environmental time series via long short term memory recurrent neural network		In a recent decade, deep neural networks have been applied for many research areas after achieving dramatic improvements of accuracy in solving complex problems in vision and computational linguistics area. However, some problems, such as environmental modeling, are still limited to benefit from the deep networks because of its difficulty in collecting sufficient data of learning process. In this paper, aside from the accuracy issue, we raise another property—stability—of the deep networks useful for even such data-limited problems, especially in time-series modeling. Recurrent neural networks with memory cell structures, a deep network, can be deemed as a more robust network structure for long-term forecasting under coarse data observation and associated uncertainties, including missing values and sampling/measurement errors. The stability in forecasting is induced from balancing impact of inputs over all time steps in the networks. To analyze this property in various problem conditions, we adapt the recurrent networks with memory structure to environmental time-series problems, such as forecasting water pollution, air pollution, and ozone alarm. In the results, the recurrent networks with memory showed better performance of forecasting in non-stationary environment and long-term time lags.		Kangil Kim;Dong-Kyun Kim;Junhyug Noh;Minhyeok Kim	2018	IEEE Access	10.1109/ACCESS.2018.2884827	long short term memory;distributed computing;missing data;artificial neural network;recurrent neural network;computer science;machine learning;artificial intelligence;memory cell	ML	8.904196600509144	-22.80127301851174	116774
0154169fbf822fe8da3d06d40e35f6802c51e33c	an empirical study on risk responses for various operation risks of container terminals in hong kong and china	terminal risk risk responses operational risk container terminals risk evaluation;container terminals;risk evaluation;risk management;industries;operational risk;companies;risk response decision making process container terminals hong kong china safety management operational risks terminal operation correspondence analysis shareholding;companies containers industries educational institutions risk management cities and towns;risk responses;sea ports decision making risk management;cities and towns;terminal risk;containers	Hong Kong and mainland China, being the places with most important container terminals among the world, have not carried out any studies in relation to risk and safety management. Countries around the two places like Taiwan have already devoted some efforts on the issue. Operational risks in terminal operation can lead to many bad consequences like incurring high cost for recovering the losses if they were not handled properly. By using the correspondence analysis, this research attempts to reveal the relationship between the risk responses, risk items and terminal companies' characteristics in order to show the rationale for risk responses decisions and identify the common practices for managing the operation risks. From this study, it is found that reducing both the severity and the occurrence is the common practices in the industry. Location and shareholding are playing a vital role in the risk responses decision making process.	computer terminal;correspondence analysis;design rationale;it risk	Eric Su;Kin Keung Lai;Yan Pui Lee	2013	2013 Sixth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2013.27	it risk management;risk management;operations management;it risk;business;enterprise risk management;commerce;financial risk management	SE	-3.2123567071416312	-13.226339537582621	117064
bb5e4b155e9603249ce239f942455f63f42c166d	applying computational intelligence methods for predicting the sales of newly published books in a real editorial business management environment		When a new book is launched the publisher faces the problem of how many books should be printed for delivery to bookstores; printing too many is the main issue, since it implies a loss of investment due to inventory excess, but printing too few will also have a negative economic impact. In this paper, we are tackling the problem of predicting total sales in order to print the right amount of books and doing so even before the book has reached the stores. A real dataset including the complete sales data for books published in Spain across several years has been used. We have conducted an analysis in three stages: an initial exploratory analysis, by means of data visualisation techniques; a feature selection process, using different techniques to find out what are the variables that have more impact on sales; and a regression or prediction stage, in which a set of machine learning methods has been applied to create forecasting models for book sales. The obtained models are able to predict sales from pre-publication data with remarkable accuracy, and can be visualised as simple decision trees. Thus, these can be used as decision-aid tools for publishers, which can provide a reliable guidance on the decision process of publishing a book. This is also shown in the paper by addressing four example cases of representative publishers, regarding their number of sales and the number of different books they sell.	book;computational intelligence	Pedro A. Castillo;Antonio Mora García;Hossam Faris;Juan Julián Merelo Guervós;Pablo García-Sánchez;Antonio Fernández-Ares;Paloma de las Cuevas;Maribel García Arenas	2017	Knowl.-Based Syst.	10.1016/j.knosys.2016.10.019	sales order;computer science;artificial intelligence;data science;machine learning;operations research	AI	3.7333629482332906	-16.154932434113835	117108
b53c4a7ebeaf84e197ad2957e5fac9700d17507f	short-term traffic flow prediction based on optimised support vector regression		In order to provide accurate and reliable prediction of short-term traffic flow to realise intelligent transportation control, support vector machine (SVM) regression method is established to predict short-term traffic flow. Then, parameter selection optimisation model for SVM is studied. Support vector penalty coefficient and the parameters of the kernel function play an important role in learning precision and generalisation ability of regression model. So, a kind of improved artificial fish swarm algorithm is used to optimise the SVM regression to select the optimal parameters. The experiment results show that the proposed scheme can effectively reduce mean absolute percentage error and mean square error in the real traffic flow forecasting. The proposed scheme can improve the prediction precision of the short-term traffic flow.	support vector machine	Da-wei Hu;Bing Su	2017	IJADS	10.1504/IJADS.2017.10005300	support vector machine;kernel (statistics);regression analysis;economics;mean absolute percentage error;traffic flow;artificial intelligence;intelligent transportation system;pattern recognition;mean squared error	ML	9.016344143484172	-19.75496246210032	117148
b133d098c8a947efd78f1dcb334229c110139fde	fuzzy measures for correlation coefficient of fuzzy numbers	correlacion;non linear programming;fuzzy set;fuzzy measure;fuzzy programming;coeficiente correlacion;nonlinear programming;fuzzy number;programacion no lineal;conjunto difuso;ensemble flou;programmation non lineaire;teoria medida;fuzzy sets;real world application;statistical analysis;random variable;membership function;extension principle;programmation floue;correlation;correlation coefficient;theorie mesure;coefficient correlation;measure theory;programacion difusa	Correlation coefficient of random variables has wide applications in statistical analysis. This paper extends the applications to fuzzy environment, with a methodology for calculating the correlation coefficient of fuzzy numbers developed. Different from previous studies, the correlation coefficient calculated in this paper is a fuzzy number, rather than a crisp value. The idea is based on Zadeh's extension principle. A pair of nonlinear programs is formulated to find the α-cut of the fuzzy correlation coefficient. From different values of α, the membership function of the fuzzy correlation coefficient is constructed. To illustrate how to interpret the fuzzy correlation coefficient in real world applications, the correlation between the technology level and management achievement from a sample of 15 machinery firms in Taiwan is exemplified. All indications show that the correlation between technology and management in Taiwan machinery firms is rather low.	coefficient;fuzzy measure theory	Shiang-Tai Liu;Chiang Kao	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00199-3	membership function;defuzzification;nonlinear programming;computer science;artificial intelligence;fuzzy number;partial correlation;calculus;fuzzy measure theory;mathematics;fuzzy set;statistics	NLP	-1.8962391597630692	-19.01756057389918	117162
a9933fa759abecf1a3848233e679b257b91944bc	generalized aggregation operators for intuitionistic fuzzy sets		The generalized ordered weighted averaging (GOWA) operators are a new class of operators, which were introduced by Yager (Fuzzy Optim Decision Making 2004;3:93–107). However, it seems that there is no investigation on these aggregation operators to deal with intuitionistic fuzzy or interval-valued intuitionistic fuzzy information. In this paper, we first develop some new generalized aggregation operators, such as generalized intuitionistic fuzzy weighted averaging operator, generalized intuitionistic fuzzy ordered weighted averaging operator, generalized intuitionistic fuzzy hybrid averaging operator, generalized interval-valued intuitionistic fuzzy weighted averaging operator, generalized interval-valued intuitionistic fuzzy ordered weighted averaging operator, generalized interval-valued intuitionistic fuzzy hybrid average operator, which extend the GOWA operators to accommodate the environment in which the given arguments are both intuitionistic fuzzy sets that are characterized by a membership function and a nonmembership function, and interval-valued intuitionistic fuzzy sets, whose fundamental characteristic is that the values of its membership function and nonmembership function are intervals rather than exact numbers, and study their properties. Then, we apply them to multiple attribute decision making with intuitionistic fuzzy or interval-valued intuitionistic fuzzy information. C © 2009 Wiley Periodicals, Inc.	fuzzy logic;fuzzy set;intuitionistic logic;john d. wiley;loss function;ordered weighted averaging aggregation operator	Hua Zhao;Zeshui Xu;Mingfang Ni;Shousheng Liu	2010	Int. J. Intell. Syst.	10.1002/int.20386	ordered weighted averaging aggregation operator;combinatorics;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;mathematics;fuzzy set operations	AI	-1.277904549993331	-21.607468393473262	117283
a5eb1f9b8331d6c48d3986065951bb1025301448	a genetic search for exploiting a fuzzy preference model of portfolio problems with public projects	portfolio problem;non linear programming;fuzzy set;multicriteria decision;multiple criteria;fuzzy sets;genetics;optimization problem;model development;public project;evolutionary algorithm;public projects	When an organization solves a portfolio problem with public projects evaluated by multiple criteria, in which the economic dimension is not essential or not well characterized, the classical methods are not useful. We propose a non-linear preference model developed from normative Value Theory and using fuzzy sets to model some sources of imprecision. This model can be considered as a generalization of the classical approaches. However, the optimization problem is very complex in order to be solved with non-linear programming techniques. Therefore, the model is exploited by an evolutionary algorithm, able to achieve a strong improvement of the quality of solution. Copyright Kluwer Academic Publishers 2002		Eduardo Fernández;Jorge Navarro	2002	Annals OR	10.1023/A:1021577608097	mathematical optimization;type-2 fuzzy sets and systems;computer science;evolutionary algorithm;mathematics;management science;fuzzy set;welfare economics;fuzzy set operations	AI	-2.973976352799475	-17.15169933134773	117285
c6f39cc45f02e4351d1b5252e9942116424e26c7	volatility forecasting from multiscale and high-dimensional market data	high dimensionality;support vector machines;risk management;high frequency finance;volatility forecasting;volatility models;high dimensional data;long memory;stock market volatility;support vector machine;foreign exchange;high frequency	Advantages and limitations of the existing volatility models for forecasting foreign-exchange and stock market volatility from multiscale and high-dimensional data have been identi/ed. Support vector machines (SVM) have been proposed as a complimentary volatility model that is capable of e0ectively extracting information from multiscale and high-dimensional market data. SVM-based models can handle both long memory and multiscale e0ects of inhomogeneous markets without restrictive assumptions and approximations required by other models. Preliminary results with foreign-exchange data suggest that SVM can e0ectively work with high-dimensional inputs to account for volatility long-memory and multiscale e0ects. Advantages of the SVM-based models are expected to be of the utmost importance in the emerging /eld of high-frequency /nance and in multivariate models for portfolio risk management. c © 2003 Elsevier B.V. All rights reserved.	approximation;multiscale modeling;risk management;support vector machine;volatility	Valeriy V. Gavrishchaka;Supriya B. Ganguli	2003	Neurocomputing	10.1016/S0925-2312(03)00381-3	support vector machine;econometrics;implied volatility;actuarial science;risk management;computer science;machine learning;stochastic volatility	ML	6.139612004276386	-17.72853739663647	117320
50bbfe27763872aaf3cc561087502bd18e939d71	a multigranularity linguistic group decision-making method based on hesitant 2-tuple sets		Hesitant fuzzy linguistic term set HFLTS is a very useful technology in dealing with decision-making problems where people have hesitancy in providing their linguistic assessments. Distinct methods have been developed to aid decision making with HFLTSs, yet there is little research involving the issue that how to deal with the multigranularity hesitant fuzzy linguistic information. The aim of this paper is to develop the aggregation method for multigranularity hesitant fuzzy linguistic information and solve the linguistic group decision problem with different linguistic term sets. To do so, we first modify the translation functions and aggregation operators in the existing 2-tuple linguistic representation models so as to aggregate linguistic terms from different linguistic term sets. Then, we introduce the notion of hesitant 2-tuple sets to make computation of HFLTSs without loss of information, and develop some new operators to aggregate HFLTSs from different linguistic term sets. Using these operators, we propose a method to deal with multigranularity linguistic group decision-making problems with different situations where importance weights of either criteria or experts are known or unknown. Finally, the multigranularity linguistic group decision-making model is implemented to the healthcare waste treatment in West China Hospital to validate its effectiveness and efficiency in aiding decision-making process.		Cuiping Wei;Huchang Liao	2016	Int. J. Intell. Syst.	10.1002/int.21798	artificial intelligence;data mining;mathematics;algorithm	NLP	-3.832430796225307	-21.009866836808726	117384
71cf51aead05e96853aa33d1dc5c1e368fe1ba4d	an approach to build industry asset price index: take four industries for examples	economic indicator	Abstract Because of the importance and the absence of industry asset price index, we come up with an approach to conduct the industry asset price index. We built the industry asset price indices based on the data of the listed companies from Wind Financial Database of four industries in China. Some topics in price engineering are argued in this paper, includ ing how to select data, how to select asset types and determine weights, how to choose the best macro-and micro-industry indicators, and how to synthesize. The indexs we got are good, as they are consistent with the results in a recent research.		Fan Meng;Ye Wang;Jing Li	2013		10.1016/j.procs.2013.05.091	data mining;leverage (finance);consumer price index;wholesale price index;computer science;producer price index;mid price;finance;capital asset pricing model;price index;producer price index (india)	DB	3.407762033578873	-14.526451488395187	117503
30296709c561797336f3d85d3dd14b51c1a65ec2	application of the artifical neural network in predicting the direction of stock market index	genetic algorithm ga forecast direction indicator artificial neural network ann;input variables;prediction algorithms;indexes stock markets artificial neural networks predictive models input variables genetic algorithms prediction algorithms;stock markets;indexes;artificial neural networks;predictive models;genetic algorithms;artifical neural network ga genetic algorithm stock market index direction prediction ann model optimization;stock markets genetic algorithms neural nets prediction theory	In the business sector, it has always been a difficult task to predict the exact daily price of the stock market index, hence, there is a great deal of research being conducted regarding the prediction of the direction of stock price index movement. Many factors such as political events, general economic conditions, and traders' expectations may have an influence on the stock market index. There are numerous research studies that use indicators to forecast the direction of the stock market index. In this study, we applied two types of input variables to predict the direction of the daily stock market index. The main contribution of this study is the ability to predict the direction of the next day's price of the Japanese stock market index by using an optimized artificial neural network (ANN) model. To improve the prediction accuracy of the trend of the stock market index in the future, we optimize the ANN model using genetic algorithms (GA). We demonstrate and verify the predictability of stock price direction by using the hybrid GA-ANN model and then compare the performance with prior studies. Empirical results show that the Type 2 input variables can generate a higher forecast accuracy and that it is possible to enhance the performance of the optimized ANN model.	artificial neural network;experiment;genetic algorithm;hit (internet);mathematical model;nsa product types;software release life cycle;traders	Mingyue Qiu;Li Cheng;Song Yu	2016	2016 10th International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS)	10.1109/CISIS.2016.115	financial economics;economics;artificial intelligence;machine learning	NLP	7.219886794649612	-18.822743136877314	117652
62fe5eb586128f083b9b1fa8ea6505bad787a819	a note on fuzzy time series model selection with sample autocorrelation functions	autocorrelation function;model selection;fuzzy set;fuzzy data;time series;time series model	Sample autocorrelation functions of fuzzy time series are proposed and used in model selection. The main idea is to select a number of different data sets from each fuzzy set and calculate the sample autocorrelation function for each data set. The average autocorrelation function value calculated with different data sets is taken as the collective sample autocorrelation function, as a measure of dependency between fuzzy data.	autocorrelation;computable function;model selection;time series	Qiang Song	2003	Cybernetics and Systems	10.1080/01969720302867	econometrics;surrogate data;time series;moving-average model;pattern recognition;mathematics;autocorrelation technique;statistics	ML	6.71944287599859	-21.276814336177615	117673
1084794d744a5038f1ebe7aae303692a18e6f28b	comparison of clusters from fuzzy numbers	fuzzy cluster analysis;fuzzy numbers;numero difuso;fuzzy number;algoritmo borroso;fuzzy relation;nombre flou;ranking function;fuzzy preference relation;fuzzy clustering;cluster analysis;fuzzy algorithm;indexation;algorithme flou;classification automatique;relation floue;nondominated alternatives;relacion difusa	Fuzzy numbers are used for representation of numerical quantities in a vague environment. Their comparison or ranking is important for application purposes. A new index for comparing of fuzzy numbers based on their geometrical properties is suggested in this paper. This geometrical index is tested on a group of selected examples and compared with the other well-known indexes. A method for comparison of m-tuples of fuzzy numbers and an algorithm for comparison of subsets (clusters) of similar, closed m-tuples of trapezoidal fuzzy numbers are presented.		Vania Peneva;Ivan Popchev	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00360-0	discrete mathematics;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set operations;algorithm	NLP	-2.1713917873619	-21.74341790236038	117815
5ba0d65086cc903c5c385aabc843f7e97cad1f22	preference programming for robust portfolio modeling and project selection	elicitation;modelizacion;multicriteria analysis;bolsa valores;investment appraisal;inversion;informacion incompleta;portfolio selection;seleccion cartera;analisis decision;investment;decision analysis;bourse valeurs;selection portefeuille;stock exchange;portfolio optimization;modelisation;incomplete information;systeme incertain;selection projet;route;preference elicitation;multiple decision;evaluation criteria;robustesse;indexation;information incomplete;investissement;portfolio management;preferencia;carretera;decision multiple;robustness;elicitacion;preference;gestion cartera;highway;analisis multicriterio;analyse multicritere;gestion portefeuille;sistema incierto;modeling;multiple criteria decision analysis;seleccion proyecto;uncertain system;analyse decision;project selection;robustez	In decision analysis, difficulties of obtaining complete information about model parameters make it advisable to seek robust solutions that perform reasonably well across the full range of feasible parameter values. In this paper, we develop the Robust Portfolio Modeling (RPM) methodology which extends Preference Programming methods into portfolio problems where a subset of project proposals are funded in view of multiple evaluation criteria. We also develop an algorithm for computing all non-dominated portfolios, subject to incomplete information about criterion weights and project-specific performance levels. Based on these portfolios, we propose a project-level index to convey (i) which projects are robust choices (in the sense that they would be recommended even if further information were to be obtained) and (ii) how continued activities in preference elicitation should be focused. The RPM methodology is illustrated with an application using real data on road pavement projects. 2006 Elsevier B.V. All rights reserved.	algorithm;decision analysis;preference elicitation	Juuso Liesiö;Pekka Mild;Ahti Salo	2007	European Journal of Operational Research	10.1016/j.ejor.2005.12.041	inversion;route;stock exchange;economics;decision analysis;investment;operations management;data mining;portfolio optimization;welfare economics;robustness	AI	-0.27838376994596914	-15.440841131989613	117966
e79b21c91320983e8a13554532d7803e755fb6bb	forecasting electricity consumption in czech republic	forecast;prediction algorithms polynomials accuracy power demand predictive models neural networks linear regression;regression analysis learning artificial intelligence load forecasting mining industry power consumption power system planning prediction theory;prediction electricity consumption forecast machine learning optimalization;electricity consumption forecasting czech republic power plant planning mining planning machine learning algorithms predictive model local polynomial regression algorithm prediction error;optimalization;electricity consumption;machine learning;prediction	Correct prediction of electricity consumption is important for planning its production in the short term, but also in the long term due to the construction of new power plants and mining planning. Accurate prediction is a challenging task because the consumption changes both in the day and during the whole year. The paper describes a method based only on input data for consumption. No additional influences were included such as temperature, wind, GDP (Gross Domestic Product). Five machine learning algorithms were used to create a predictive model. The best results were achieved with a local polynomial regression algorithm. Daily prediction error was 5.77%, weekly 3.49% and monthly 2.41%.	algorithm;machine learning;polynomial;predictive modelling	Václav Uher;Radim Burget;Malay Kishore Dutta;Petr Mlynek	2015	2015 38th International Conference on Telecommunications and Signal Processing (TSP)	10.1109/TSP.2015.7296264	econometrics;prediction;machine learning;data mining;mathematics;statistics	Robotics	8.972415338066925	-17.727561020611553	118209
1fd33b6831c40803302c173760974ed751c69da9	analyzing the effects of driver behavior within an adaptive ramp control scheme: a case-study with alinea		Freeway traffic control strategies such as ramp metering and variable speed limits are gaining importance in relieving freeways congestion. Along with reducing the travel time, freeway traffic control strategies are also considered as key players in improving the traffic safety and decreasing the vehicle emissions. Traffic simulation tools overtook the real-world implementations, for testing traffic control strategies, with less time consumption, and economical and more flexible traffic analysis environment. This paper presents a micro-simulation based case-study on Istanbul freeways, where it discusses: the microscopic model calibration, the impact of ramp metering strategy Asservissement Lineaire ďEntrée Autoroutiere (ALINEA) on the traffic characteristics and traffic oscillations, and the impact of driver's behavior at merges on ALINEA's performance. The control algorithm is utilized online by integrating VISSIM with MATLAB via COM.	algorithm;downstream (software development);freeway;matlab;network congestion;ramp simulation software for modelling reliability, availability and maintainability;traffic analysis;vissim	Ismail M. Abuamer;Mohd Sadat;Mehmet Ali Silgu;Hilmi Berk Celikoglu	2017	2017 IEEE International Conference on Vehicular Electronics and Safety (ICVES)	10.1109/ICVES.2017.7991910	traffic optimization;control engineering;vissim;engineering;traffic congestion reconstruction with kerner's three-phase theory;merge (version control);traffic analysis;traffic simulation	Robotics	9.673996405763713	-10.172749566256384	118252
4c74297f3450101640040e680b2c060d18cd2f69	aggregation operators of interval-valued 2-tuple linguistic information		The group decision-making problem with linguistic information evaluation values of decision makers are used based on 2-tuple interval-valued. Operational laws on interval value 2-tuple are introduced. On the basis of these laws, new aggregation operators are introduced by using the Choquet integral. A multiple attribute decision-making method based on these aggregation operators is proposed. An example is given to illustrate the efficiency, practicality, and feasibility of our method. C © 2014 Wiley Periodicals, Inc.	interval arithmetic;john d. wiley	Ismat Beg;Tabasam Rashid	2014	Int. J. Intell. Syst.	10.1002/int.21650	mathematical optimization;discrete mathematics;mathematics;management science	AI	-2.7521463807466406	-20.65017425929786	118386
baa1325b96071d93e48dac3b382bf331b74b7e85	research on customers demand forecasting for e-business web site based on ls-svm	forecasting;web sites backpropagation electronic commerce least squares approximations radial basis function networks support vector machines;least squares approximations;ls svm customers demand forecasting e business;customers demand;electronic commerce;e business;ls svm;support vector machines;forecasting model;backpropagation;radial basis function networks;accuracy;artificial neural networks;e business web site;artificial neural networks support vector machines forecasting business predictive models accuracy demand forecasting;rbf neural network;business;web sites;least squares support vector machines;predictive models;rbf neural network predictor;regression analysis;customers demand forecasting;bp neural network predictor;ls svm customers demand forecasting e business web site least squares support vector machines regression analysis linear neural network predictor rbf neural network predictor bp neural network predictor;linear neural network predictor;water demand;neural network;least squares support vector machine;demand forecasting	This paper introduces a novel customers' demand forecasting model based on least squares support vector machines (LS-SVM) for e-business enterprises. Firstly, the paper presents actual state of e-business, and discusses some factors that block e-business advance in China. Then, some common techniques used for forecasting are briefly reviewed together with their shortcomings respectively. To solve these disadvantages, the paper reviews the fundamental theory of least squares support vector machines for regression, and analyses some merits of the theory. At last, based on the theory, the paper proposes a forecasting model to forecast pure water demand in a week for an e-business website. Compared with linear neural network predictor, RBF neural network predictor and BP neural network predictor, the LS-SVM forecasting model shows outstanding performance in simulation and practical results.	artificial neural network;electronic business;kerrison predictor;least squares;link-state routing protocol;radial basis function;simulation;support vector machine	Qisong Chen;Yun Wu;Xiaowei Chen	2008	2008 International Symposium on Electronic Commerce and Security	10.1109/ISECS.2008.204	support vector machine;least squares support vector machine;demand forecasting;forecasting;computer science;artificial intelligence;backpropagation;machine learning;data mining;accuracy and precision;predictive modelling;artificial neural network;regression analysis	Arch	8.532355336885567	-20.14943438922541	118457
3a5dfd9721a219231a7efd0957fd9d1371916383	decision-making via neutrosophic support soft topological spaces		The concept of interval neutrosophic sets has been studied and the introduction of a new kind of set in topological spaces called the interval valued neutrosophic support soft set has been suggested. We study some of its basic properties. The main purpose of this paper is to give the optimum solution to decision-making in real life problems the using interval valued neutrosophic support soft set.	algorithm;fuzzy logic;fuzzy set;indeterminacy in concurrent computation;operations research;real life	Parimala Mani;Karthika Muthusamy;Saeid Jafari;Florentin Smarandache;Udhayakumar Ramalingam	2018	Symmetry	10.3390/sym10060217	combinatorics;topological space;soft set;mathematics	ECom	-1.6880853007396788	-22.535096014283493	118553
bab8031acde2a62f8332a009af28e9fcb0b58be0	ranking interval type 2 fuzzy sets using parametric graded mean integration representation	pragmatics;cybernetics;parametric graded mean integration representation type 2 fuzzy sets fuzzy ranking;electronic mail;sorting;uncertainty;fuzzy sets;number theory fuzzy set theory;fuzzy sets cybernetics decision making pragmatics uncertainty electronic mail sorting;fuzzy ranking parametric graded mean integration representation pgmir interval type 2 fuzzy sets it2fs defuzzification type 1 fuzzy set t1fs	In this paper, we develop the parametric graded mean integration representation (PGMIR) for the interval type 2 fuzzy sets (IT2FS). The PGMIR value can be served as the type reduced or defuzzification of an IT2FS. A set of IT2FSs can then be ordered by the corresponding PGMIR values. Firstly, the parametric representation of the embedded fuzzy sets is defined for the general IT2FS. The IT2FS is characterized by the set of embedded fuzzy sets induced from the parametric function. Then, the graded mean integration representation (GMIR) of a type 1 fuzzy set (T1FS) is extended to the IT2FS through the parametric form of an IT2FS. The GMIR of the parametric embedded T1FS is defined and is called the PGMIR. In this manner, the IT2FSs are defuzzified and can be ranked by sorting the PGMIR values. The average value of the PGMIR is used to determine IT2FS ranking. A numerical example is presented to show that the proposed approach is superior to the ranking method from IT2FS centroid.	control system;defuzzification;embedded system;fuzzy set;nsa product types;numerical analysis;sorting;spectral centroid	Kuo-Ping Chiao	2016	2016 International Conference on Machine Learning and Cybernetics (ICMLC)	10.1109/ICMLC.2016.7872956	fuzzy logic;combinatorics;discrete mathematics;uncertainty;membership function;cybernetics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;sorting;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;pragmatics	Robotics	-1.8331291793975686	-21.784845165973554	118665
98b277b9e8dd40d6159fec7b0276fe01f3425e66	characterization and explanation of the sustainability of the european wood manufacturing industries: a quantitative approach	wood manufacturing industry;indicators;sustainability;econometric models;goal programming	This paper has a twofold purpose. First, to characterize the sustainability of the European wood manufacturing industry. In this way, a ranking of the European countries analyzed in terms of sustainability is established. To undertake this task the sustainability of each country is defined by using several indicators of diverse nature (economic, environmental and social). These indicators are aggregated into a composite or synthetic index with the help of a binary goal programming model. In this way, a ranking according to the sustainability of the wood manufacturing industry in the European countries studied is obtained. The second step in the research consists of explaining the causes behind the level of sustainability of each country. This task is carried out by taking the composite indexes of sustainability as endogenous variables and a tentative set of economic, environmental and social variables as explanatory variables. The link between endogenous and exogenous variables is made with the help of econometric models.		Roberto Voces;Luis Diaz-Balteiro;Carlos Romero	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.12.040	goal programming;sustainability;econometric model	Vision	-1.6825193185276341	-12.714349202014672	119042
08394badc4ce60d03d23427bebb66b933bf28c32	cosine similarity and the borda rule		Cosine similarity is a commonly used similarity measure in computer science. We apply this similarity measure to define a voting rule, namely, the cosine similarity rule. This rule selects a social ranking that maximizes cosine similarity between the social ranking and a given preference profile. Our main finding is that the cosine similarity rule in fact coincides with the Borda rule.	cosine similarity	Yoko Kawada	2018	Social Choice and Welfare	10.1007/s00355-017-1104-2	mathematical economics;economics;voting;similarity measure;cosine similarity;ranking;artificial intelligence;pattern recognition	AI	-4.102270612585786	-21.775847194812226	120190
9f5a408603887d4f6ff7f35cea6c805bb09d14df	a novel similarity/dissimilarity measure for intuitionistic fuzzy sets and its application in pattern recognition	fuzziness;intuitionistic fuzzy sets;knowledge;dissimilarity;similarity measure	Among the most interesting measures in intuitionistic fuzzy sets (IFSs) theory, the similarity measure is an essential tool to compare and determine degree of similarity between IFSs. Although there exist many similarity measures for IFSs, most of them cannot satisfy the axioms of similarity measure or provide reasonable results. In this paper, a novel knowledge-based similarity/dissimilarity measure between IFSs is proposed. Firstly, we define a new knowledge measure of information conveyed by the IFS and prove some properties of the proposed knowledge measure. Based on the proposed knowledge measure of IFSs, we construct a novel similarity/dissimilarity measure between IFSs and prove some properties of the proposed similarity measure. Then we use some illustrative examples to show that the proposed measures, though simple in concept and calculus, can overcome the drawbacks of the existing measures. Finally, we apply the proposed similarity/dissimilarity measure between IFSs in the pattern recognition problems to demonstrate that the proposed measure is the most reliable to deal with the pattern recognition problem in comparison with the existing similarity measures. © 2015 Published by Elsevier Ltd.	existential quantification;fuzzy set;intuitionistic logic;pattern recognition;similarity measure	Hoang Nguyen	2016	Expert Syst. Appl.	10.1016/j.eswa.2015.09.045	discrete mathematics;pattern recognition;data mining;mathematics;knowledge	Vision	-2.5501490681915864	-22.941552979324875	120303
64e630846e1832564183a90f68666defcb410378	the hellinger distance in multicriteria decision making: an illustration to the topsis and todim methods	topsis;stochastic dominance degree;probability distribution;hellinger distance;todim;multicriteria decision making mcdm	Due to the difficulty in some situations of expressing the ratings of alternatives as exact real numbers, many well-known methods to support Multicriteria Decision Making (MCDM) have been extended to compute with many types of information. This paper focuses on the information represented as probability distribution. Many of the methods that deal with probability distribution use the concept of stochastic dominance, which imposes very strong restrictions to differentiate two probability distributions, or uses the probability distributions to obtain a quantity that will be used to rank the alternatives. This paper brings the Hellinger distance concept to the MCDM context to assist the models to deal with probability distributions in a direct way without any transformation. Transformations in the data or summary quantities may miss represent the original information. For direct comparisons among probability distributions we use the stochastic dominance degree (SDD). We illustrate how simple it can be to adapt the existing methods to deal with probability distributions through the Hellinger distance and SDD by adapting the TOPSIS and TODIM (an acronym in Portuguese of Interactive and Multicriteria Decision Making) methods. 2014 Elsevier Ltd. All rights reserved.		Rodolfo Lourenzutti;Renato A. Krohling	2014	Expert Syst. Appl.	10.1016/j.eswa.2014.01.015	probability distribution;topsis;econometrics;mathematical optimization;mathematics;statistics;hellinger distance	AI	-3.8957741947473274	-19.87324912717592	120794
317c9056c23771b909296717d2f4b36c57767950	a nonparametric model for short-term travel time prediction using bluetooth data	kalman filtering;travel time prediction;travel time;arma auto regressive moving average model;bluetooth technology;traffic control;traffic estimation;traffic conditions;nonparametric model;k nearest neighbor models;bluetooth detection;nonparametric analysis;mathematical prediction	Reliable travel time prediction enables both users and system controllers to be well informed of the future conditions on roadways. This in turn helps with informed pre-trip plans and effective traffic control strategies. This article studies short-time travel time prediction for stochastic freeway applications using real time Bluetooth travel time data. A set of four prediction models including historical average, auto-regressive integrated moving average (ARIMA), Kalman filter, and K-nearest neighbors (KNN), are implemented. A modified nonparametric model KNN-T is proposed that considers the traffic pattern features to enhance the basic KNN model with trend adjustment. The results of the prediction performances of each model from the case studies are investigated and reported. Results indicated that both nonparametric models, the basic KNN and KNN-T, outperformed the historical average, ARIMA, and Kalman filter models by decreasing the MAPE more than 10% for the all-day period and 20% for peak-hour peri...	bluetooth	Wenxin Qiao;Ali Haghani;Masoud Hamedi	2013	J. Intellig. Transport. Systems	10.1080/15472450.2012.748555	kalman filter;nonparametric statistics;econometrics;simulation;computer science;engineering;machine learning;mathematics;statistics	Robotics	8.785830636082558	-13.823958783319929	120828
7135345411f0567316078bd7a3a58ba5a9bc147e	the fuzzy properties of the ship control in collision situations		This article focuses on fuzzy set theory as intelligent tools for navigator's decision-making to improve the safety of marine vessels in collision situations. A lot of progress has been made, especially in the field of artificial intelligence. The paper goal is to develop a decision support system based on artificial intelligence to determine a ship's trajectory in a collision situation. In this present work ship trajectory optimization in collision situations is presented as multistage decision-making in a fuzzy environment. The navigator's subjective assessment in making a decision are taken under consideration in the process model and it shows the modified membership function of constraints.	artificial intelligence;decision support system;fuzzy set;mathematical optimization;membership function (mathematics);multistage amplifier;process modeling;set theory;trajectory optimization	Mostefa Mohamed-Seghir	2017	2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)	10.1109/INISTA.2017.8001141	fuzzy logic;collision;simulation;decision support system;trajectory optimization;fuzzy set;trajectory;membership function;engineering	Robotics	7.2956431342129475	-12.957137793038406	120873
6c678ba31434e82c67d33d8f43328f257d8f4ad3	application of neuro-fuzzy pattern recognition for non-intrusive appliance load monitoring in electricity energy conservation	energy conservation;pattern clustering;load recognition;fluorescent lamps;load deenergizing transient features neuro fuzzy pattern recognition nonintrusive appliance load monitoring electricity energy conservation global warming climate change electricity energy consumption power consumption residences buildings nialm nfpr lh linguistic hedges fuzzy c means clustering fcm scaled conjugate gradient training algorithm scg;training;neuro fuzzy pattern recognition;home appliances;power system measurement;fuzzy set theory;transient analysis;home appliances transient analysis feature extraction training monitoring pattern recognition fluorescent lamps;non intrusive appliance load monitoring;power engineering computing;monitoring;feature extraction;power signatures fuzzy c means load recognition neuro fuzzy pattern recognition non intrusive appliance load monitoring;fuzzy c means;pattern recognition;power system measurement climate mitigation conjugate gradient methods energy conservation fuzzy set theory pattern clustering power engineering computing;climate mitigation;power signatures;conjugate gradient methods	Due to the global warming and climate change, it is very important to effectively improve the efficiency of the electricity energy consumption. Monitoring the power consumption of residences and buildings is one of the approaches that can improve the efficiency of the electricity energy consumption. In this paper, a Non-Intrusive Appliance Load Monitoring (NIALM) system, which applies a neuro-fuzzy pattern recognizer (NFPR) with Linguistic Hedges (LHs) to recognize the operation status of individual appliances, is proposed. A two-stage fuzzy pattern recognition process is presented in this paper. First, Fuzzy C-Means (FCM) clustering is employed to coarsely estimate the parameters used in NFPR. Following this stage, the Scaled Conjugate Gradient (SCG) training algorithm is applied to adaptively fine tune the parameters. In the proposed NIALM system, either load energizing or load de-energizing transient features are extracted from an acquired transient current waveform. NFPR performs load recognition based on these transient features. The recognition results obtained from different real experimental environments confirm that the proposed approach is able to identify the operational status of individual appliances.	algorithm;cluster analysis;conjugate gradient method;feature extraction;finite-state machine;fuzzy cognitive map;gradient descent;neuro-fuzzy;pattern recognition;waveform	Yu-Hsiu Lin;Men-Shen Tsai	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251160	simulation;energy conservation;feature extraction;computer science;artificial intelligence;machine learning;fuzzy set	Robotics	9.749185705681494	-16.32757633712446	120921
7dd26cc488cbf2f977d1582f02c28329936d36a1	heterogeneity and endogenous nonlinearity in an artificial stock model	noise trader;structural model;stock market;fat tail;heterogeneous agents;volatility clustering;generation time;satisfiability;statistical properties;indexation;dependence structure;nonlinearity;endogenous fluctuations;computational finance	We present a nonlinear structural stock market model which is a nonlinear deterministic process buffeted by dynamic noise. The market is composed of two typical trader types, the rational fundamentalists believing that the price of an asset is determined solely by its fundamental value and the boundedly rational noise traders governed by greed and fear. The interaction among heterogeneous investors determines the dynamics and the statistical properties of the system. We find the model is able to generate time series that exhibit dynamical and statistical properties closely resembling those of the S&P500 index, such as volatility clustering, fat tails (leptokurtosis), autocorrelation in square and absolute return, larger amplitude, crashes and bubbles. We also investigate the nonlinear dependence structure in our data. The results indicate that the GARCH-type model cannot completely account for all nonlinearity in our simulated market, which is thus consistent with the results from real markets. It seems that the nonlinear structural model is more powerful to give a satisfied explanation to market behavior than the traditional stochastic approach.	nonlinear system	Hongquan Li;Wei Shang;Shouyang Wang	2008		10.1007/978-3-540-69387-1_47	fat-tailed distribution;computational finance;generation time;statistics;satisfiability	NLP	1.8862901211800018	-11.815374027199862	120942
71fc737ff7047c2d4d8b39d6f1dc63d1bc892949	research on selection method of product configuration program	optimal solution;program product configuration;optimisation;mathematics computing;technique for order preference by similarity to ideal solution;linear optimization model;enterprise modules;probability density function;mass production;commerce;satisfiability;data mining;knowledge engineering mass production decision making optimization methods vectors constraint optimization mass customization cost function mathematical model;similarity order preference program product configuration linear optimization model matlab binary vector normalization ideal solution method customer personalized requirements enterprise modules customer requirements;effectiveness linear optimization model normalization;normalization;binary vector normalization;mathematical model;similarity order preference;effectiveness;linear optimization;optimization;customer personalized requirements;configuration management;optimisation commerce configuration management mass production mathematics computing;matlab;ideal solution method;product configuration;customer requirements	To satisfy the customer’s requirements, the linear optimization model is established. Using the Mat lab as the optimized tool, the optimized solutions are obtained. A universal normalization method is chosen to normalize the binary vectors. The Technique for Order Preference by Similarity to Ideal Solution method is applied to configuration field to get “ideal product” of the ideal solution. The compositor of configuration programs selection is obtained based on the customer’s personalized requirements after calculating the effectiveness of programs. With the analysis of the existed modules of enterprises, a method to find the most similar combination product to satisfy the customer requirements is presented. An example is given to validate the feasibility of the method.	knowledge-based configuration;linear programming;mathematical optimization;personalization;requirement	Sujing Song;Fuyun Liu	2009	2009 Fifth International Conference on Semantics, Knowledge and Grid	10.1109/SKG.2009.13	mathematical optimization;probability density function;mass production;computer science;linear programming;machine learning;normalization;mathematical model;data mining;database;configuration management;statistics;satisfiability	SE	-2.127537284422471	-19.139021026621894	120980
b8fed465733ea436395e9a97af0bab18a26bc312	pattern selection for support vector regression based response modeling	response modeling;support vector regression;pattern selection;training complexity	Highlights? A pattern selection method called Expected Margin based Pattern Selection (EMPS) is proposed. ? EMPS reduces the training complexities of SVR for use as a response modeling dataset. ? The experimental results involving one real-world marketing dataset showed that EMPS improved SVR efficiency for response modeling. Two-stage response modeling, identifying respondents and then ranking them according to their expected profit, was proposed in order to increase the profit of direct marketing. For the second stage of two-stage response modeling, support vector regression (SVR) has been successfully employed due to its great generalization performances. However, the training complexities of SVR have made it difficult to apply to response modeling based on the large amount of data. In this paper, we propose a pattern selection method called Expected Margin based Pattern Selection (EMPS) to reduce the training complexities of SVR for use as a response modeling dataset with high dimensionality and high nonlinearity. EMPS estimates the expected margin for all training patterns and selects patterns which are likely to become support vectors. The experimental results involving 20 benchmark datasets and one real-world marketing dataset showed that EMPS improved SVR efficiency for response modeling.	support vector machine	Dongil Kim;Sungzoon Cho	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.02.026	support vector machine;computer science;machine learning;pattern recognition;data mining	ML	5.515805581054654	-19.238004986573415	121016
8905d4fa89c7c6d6c3a241ad3629f51248ae4e8a	approximate reasoning with generalized orthopair fuzzy sets		We introduce the idea of generalized orthopair fuzzy sets, which provide an extension of intuitionistic fuzzy sets. The basic properties of these generalized orthopair fuzzy sets are discussed. We discuss the use of these sets in knowledge representation. We consider the use of these types of orthopair fuzzy sets as a basis for the system of approximate reasoning introduced by Zadeh. This is referred to as OPAR. The basic operations of OPAR are introduced. A reasoning mechanism in OPAR, based on the idea of entailment, is provided. We look at the formulation of the ideas of possibility and certainty using these orthopair fuzzy sets.	fuzzy set	Ronald R. Yager;Naif Alajlan	2017	Information Fusion	10.1016/j.inffus.2017.02.005	discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;mathematics;fuzzy set;fuzzy set operations;algorithm	AI	-1.8790370323953876	-23.43885977221674	121045
ee75b0c795b7d33603d78cc261b486b833380fc9	the ordered weighted geometric averaging operators	operator;conmutatividad;operador;monotonie;moyenne ponderee ordonnee;moyenne geometrique ponderee;monotonicity;operateur;monotonia;commutativity;reseau neuronal;red neuronal;commutativite;neural network	Abstract#R##N##R##N#The ordered weighted averaging (OWA) operator was introduced by Yager.1 The fundamental aspect of the OWA operator is a reordering step in which the input arguments are rearranged in descending order. In this article, we propose two new classes of aggregation operators called ordered weighted geometric averaging (OWGA) operators and study some desired properties of these operators. Some methods for obtaining the associated weighting parameters are discussed, and the relationship between the OWA and DOWGA operators is also investigated. © 2002 Wiley Periodicals, Inc.		Z. S. Xu;Q. L. Da	2002	Int. J. Intell. Syst.	10.1002/int.10045	ordered weighted averaging aggregation operator;combinatorics;monotonic function;computer science;operator;calculus;mathematics;commutative property;artificial neural network	DB	-0.15945139764976884	-21.84152938430871	121192
4c1fad6f5c657d9d9b82bea5d2488a4d63b4881d	on the issue of obtaining owa operator weights	fuzzy set;conjunto difuso;ensemble flou;operateur mathematique;mathematical operator;operador matematico;owa operator	We first investigate the issue of obtaining the weights associated with the OWA aggregation in the situation when we have observed data on the arguments and the aggregated value. We next introduce a family of OWA operators called exponential OWA operators. Finally, we look at a simple procedure for generating the weights given a required degree of orness. © 1998 Elsevier Science B.V.	tensor operator;time complexity	Dimitar Filev;Ronald R. Yager	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00254-0	mathematical optimization;discrete mathematics;computer science;artificial intelligence;mathematics;fuzzy set;algorithm	AI	-0.6707468369922774	-21.926099083424855	121228
efa13067ad6afb062db44e20116abf2c89b7e9a9	lstm model to forecast time series for ec2 cloud price		With the widespread use of spot instances in Amazon EC2, which utilize unused capacity with unfixed price, predicting price is important for users. In this paper, we try to forecast spot instance price by using long short-term memory (LSTM) algorithm to predict time series of the prices. We apply cross validation technique to our data set, and extract some features; this help our model to learn deeply. We make the prediction for 10 regions, and measure the performance using root-mean-square error (RMSE). We apply our data to other statistical models to compare their performance; we found that our model works more efficiently, also the error is decreasing more with cross validation and the result is much faster. Based on our result we try to find the availability zone that less become over on-demand price and less changing price over time, which help users to choose the most stable availability zone.	algorithm;amazon elastic compute cloud (ec2);amazon web services;apply;cross-validation (statistics);long short-term memory;mean squared error;performance;real-time clock;real-time computing;statistical model;time series;vii	Seán Ó Donnchadha;Kyungyong Lee;Hyeokman Kim	2018	2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC/PiCom/DataCom/CyberSciTec.2018.00067	cross-validation;time series;cloud computing;data modeling;machine learning;mean squared error;statistical model;computer science;artificial intelligence	Metrics	8.094809384886675	-16.13834646487455	121354
ab1b87213ccf5894e1ef55e7de9c9ad8f596cd4c	intuitionistic fuzzy hybrid arithmetic and geometric aggregation operators for the decision-making of mechanical design schemes	intuitionistic fuzzy set;intuitionistic fuzzy hybrid weighted arithmetic and geometric aggregation (ifhwaga) operator;intuitionistic fuzzy hybrid ordered weighted arithmetic and geometric aggregation (ifhowaga) operator;decision making;mechanical design scheme	Arithmetic aggregation operators and geometric aggregation operators of intuitionistic fuzzy values (IFVs) are common aggregation operators in the fields of information fusion and decision making. However, their aggregated values imply some unreasonable results in some cases. To overcome the shortcomings, this paper proposes an intuitionistic fuzzy hybrid weighted arithmetic and geometric aggregation (IFHWAGA) operator and an intuitionistic fuzzy hybrid ordered weighted arithmetic and geometric aggregation (IFHOWAGA) operator and discusses their suitability by numerical examples. Then, we propose a multiple attribute decision-making method of mechanical design schemes based on the IFHWAGA or IFHOWAGA operator under an intuitionistic fuzzy environment. Finally, a decision-making problem regarding the mechanical design schemes of press machine is provided as a case to show the application of the proposed method.	intuitionistic logic;numerical analysis;pattern recognition	Jun Ye	2017	Applied Intelligence	10.1007/s10489-017-0930-3	operator (computer programming);computer science;fuzzy logic;ordered weighted averaging aggregation operator;arithmetic	AI	-2.206147250162471	-21.595480505468313	121595
96f20edca35fc142a51d9a56359de21ea7f5e53c	a microscopic investigation into the capacity drop: impacts of longitudinal behavior on the queue discharge rate		The capacity drop indicates that the queue discharge rate is lower than the free-flow capacity. Studies show that the queue discharge rate varies under different traffic conditions. Empirical data show that the queue discharge rate increases as the speed in congestion increases. Insights into the underlying behavioral mechanisms that cause these variable queue discharge rates can help minimize traffic delays and eliminate congestion. However, to our knowledge, few efforts have been devoted to testing impacts of traffic behaviors on the queue discharge rate. This paper tries to fill this gap. We investigate to what extent the acceleration spread and reaction time can influence the queue discharge rate. It is found that the (interdriver) acceleration spread does not reduce the queue discharge rates as much as found empirically. Modeling reaction time might be more important than modeling acceleration for capacity drop in car-following models. A speed-dependent reaction time mechanism for giving variable que...	discharger	Kai Yuan;Victor L. Knoop;Serge P. Hoogendoorn	2017	Transportation Science	10.1287/trsc.2017.0745	mathematics;acceleration;operations management;real-time computing;queue	Theory	9.745880084830608	-9.962509824654433	122036
7c8b627baabc112bc327ee9709ad97486e441d5a	horizontal generalization properties of fuzzy rule-based trading models	settore inf 01 informatica;data mining;fuzzy rule base;trading;evolutionary algorithms;profitability;prediction model;evolutionary algorithm;modeling;generalization capability	We investigate the generalization properties of a data-mining approach to single-position day trading which uses an evolutionary algorithm to construct fuzzy predictive models of financial instruments. The models, expressed as fuzzy rule bases, take a number of popular technical indicators on day t as inputs and produce a trading signal for day t + 1 based on a dataset of past observations of which actions would have been most profitable. The approach has been applied to trading several financial instruments (large-cap stocks and indices), in order to study the horizontal, i.e., cross-market, generalization capabilities of the models.	evolutionary algorithm;evolutionary data mining;exception handling;fuzzy rule;predictive modelling;salt (cryptography)	Célia da Costa Pereira;Andrea Tettamanzi	2008		10.1007/978-3-540-78761-7_10	artificial intelligence;trading strategy;machine learning;data mining;mathematics	ML	6.761349170329937	-19.51472021016515	122117
50711d524e807ebec45fff572c4b471364fa30da	are you thinking what i am thinking? a comparison of decision makers' cognitive maps by means of a new similarity measure	cognitive response elicitation process;new similarity measure;structural similarity;cognitive map.;urban planning decision problem;preorder;different decision maker;decision maker;new measure;decision makers;similarity measure;cognitive structure;fuzzy relation;cognitive map;cognitive maps;relational product;urbs decision support system;decision support system;urban planning;decision support systems;relational algebra;fuzzy set theory;decision problem;mathematical analysis	We propose a new measure for analyzing the structural similarities among the cognitive maps of different decision makers. Upon interpreting the cognitive response elicitation process in the general framework of fuzzy set theory, we are able to develop the similarity measure from straightforward operations on fuzzy relational products. In addition to the mathematical analysis, we present visual representations of cognitive structures and the structural similarities among them. We illustrate the measure’s applicability to an empirical dataset collected in an urban planning decision problem, for which the URBS decision support system has been developed to offer information and advice. The key components of the URBS decision support system are the identification and the comparison of the cognitive structures of the decision makers that were interviewed. We illustrate how the new similarity measure can be used to gain a better understanding of the structural resemblances and differences among the cognitive maps of the decision makers.	cognitive map;cognitive science;decision problem;decision support system;fuzzy set;repertory grid;set theory;similarity measure	Hyeokki Kwon;Il Im;Bartel Van de Walle	2002		10.1109/HICSS.2002.10036	management science;fuzzy logic;knowledge management;decision support system;data mining;cognitive map;decision engineering;similarity measure;fuzzy cognitive map;decision problem;computer science;cognition	HCI	-3.364942905500137	-21.729889029107927	122135
01915b66afc793bd6a5df3ef83f96d3faa49bfc4	application of fuzzy neural network for real estate prediction	modelizacion;hedonic price;aproximacion funcion;fuzzy neural network;fuzzy neural nets;real estate;database;base dato;reseau neuronal flou;price level;devis estimatif;approximation fonction;modelisation;presupuesto estimativo;function approximation;base de donnees;prediction model;estimate;reseau neuronal;modeling;red neuronal;neural network	A FNN prediction model based on hedonic price theory to estimate the appropriate price level for a new real estate is proposed. The model includes a database storing hedonic characteristics and coefficients affecting the real estate price level from recently sold projects that are representative in the local environment. The experimental result shows that the fuzzy neural network prediction model has strong function approximation ability and is suitable for real estate price prediction depending on the quality of the available data.	application domain;applications of artificial intelligence;approximation;artificial neural network;coefficient;database;decision support system;fuzzy rule;hedonic regression;neuro-fuzzy	Jian-Guo Liu;Xiao-Li Zhang;Wei-Ping Wu	2006		10.1007/11760191_173	systems modeling;function approximation;computer science;artificial intelligence;machine learning;predictive modelling;operations research;price level;artificial neural network;real estate	ML	6.324992002423079	-20.59751567017061	122226
0a39ec578228e105ae426cc0810fd7a7a7ed94dd	performance analysis of unorganized machines in streamflow forecasting of brazilian plants		Abstract This work performs an extensive investigation about the application of unorganized machines – extreme learning machines and echo state networks – to predict monthly seasonal streamflow series, associated to three important Brazilian hydroelectric plants, for many forecasting horizons. The aforementioned models are neural network architectures which present efficient and simple training processes. Moreover, the selection of the best inputs of each model is carried out by the wrapper method, using three different evaluation criteria, and three filters, viz ., those based on the partial autocorrelation function, the mutual information and the normalization of maximum relevance and minimum common redundancy method. This study also establishes a comparison between the unorganized machines and two classical models: the partial autoregressive model and the multilayer perceptron. The computational results demonstrate that the unorganized machines, especially the echo state networks, represent efficient alternatives to solve the task.	profiling (computer programming)	Hugo Valadares Siqueira;Levy Boccato;Ivette Luna;Romis de Faissol Attux;Christiano Lyra	2018	Appl. Soft Comput.	10.1016/j.asoc.2018.04.007	redundancy (engineering);mathematics;partial autocorrelation function;machine learning;artificial intelligence;mutual information;artificial neural network;normalization (statistics);streamflow;multilayer perceptron;autoregressive model	Logic	8.74847438700268	-19.868220519784	122393
7ebf9726d74ab05ed2f90e84cfea580f4e9fd90b	the study on the relationship between energy consumption and economy growth in china based on var model	analytical models;consumption;biological system modeling;social development;social activities;power engineering computing;var model consumption economy growth;social sciences computing;energy consumption;human existence;impulse response;predictive models;social sciences computing causality economics energy consumption power engineering computing;economics;economy growth;living standards;china;energy consumption reactive power power generation economics impulse testing economic forecasting macroeconomics cities and towns industrial relations databases electronic mail;granger causality test;var model;granger causality test energy consumption economy growth china var model human existence social activities;economic indicators;causality;reactive power	Energy is the basis of the human existence and social activities, the development of every country needs the support which is supplied by energy. Energy is the development of the national economy and is a necessary and important material foundation to people's livelihood, it is also extremely important to social development and the improvement of people's living standards. Studying the relationship between China's energy consumption and economy growth has an important theoretical and practical significance. In this paper, we use ADF test to test the stationary of energy consumption and GDP based on the data from 1990 to 2006, establish VAR model, then forecast and analysis the impulse response, lastly, use Granger causality test to determine the relationship between energy consumption and economy growth.	causality;stationary process;vector autoregression	He-rui Cui;Di Wang	2009	2009 First International Workshop on Database Technology and Applications	10.1109/DBTA.2009.62	causality;consumption;impulse response;machine learning;china	SE	3.2768796721911935	-14.207477010979357	122429
3f3f405db2e71518a90a8dc440d47536a257eb8f	applying fuzzy logic to measure completeness of a conceptual model	experimental design;mesure floue;concept;analisis numerico;medio ambiente;base donnee;world;matematicas aplicadas;fonctionnelle;mathematiques appliquees;05bxx;qualite;appartenance;coaccion;logique floue;plan experiencia;contrainte;database;base dato;conceptual model;logica difusa;conception;index;calculo automatico;03b52;functional dependency;recherche;analyse numerique;computing;calcul automatique;fuzzy logic;funcional;constraint;numerical analysis;plan experience;functional;quality;indice;environment;indexation;diseno;completitud;design;environnement;monde;pertenencia;mundo;information system;completeness;database design;applied mathematics;membership;completude;investigacion;systeme information;calidad;concepto;sistema informacion	In a computing environment, the success of an information system depends upon the quality of its conceptual model. The importance of measuring quality of a conceptual model in quantitative terms has been emphasized in the research but still the quantitative measures are very scarce in the literature. A new Fuzzy Completeness Index (FCI) is introduced in this paper as a quantitative measure for the quality of a conceptual model. It takes into account completeness of a conceptual model based upon the concept of functional dependencies. For a given conceptual model the incorporation of functional dependencies is mapped onto a TAS Graph and is then measured using the fuzzy membership values and fuzzy hedges. The FCI has been calculated for different conceptual models. It has been illustrated that the quality in terms of completeness can effectively be measured through the FCI based approach. The higher the value of FCI the closer is the conceptual model to the real world in representing functional constraints.	fuzzy logic	Tauqeer Hussain;Mian M. Awais;Shafay Shamail	2007	Applied Mathematics and Computation	10.1016/j.amc.2006.07.053	fuzzy logic;design;conceptual model;computing;numerical analysis;completeness;artificial intelligence;conceptual model;mathematics;functional dependency;constraint;concept;information system;algorithm;database design	Logic	1.4236951510618931	-17.527565925507563	122440
d74807df50bf80361296e5e5c0a8714cc21a4200	set theoretic foundations for fuzzy set theory, and their applications	fuzzy set;set theory;fuzzy set theory;fuzzy logic;category theory;cumulant;theoretical foundation	The search for foundations for fuzzy set theory that are both mathematically elegant and general enough to encompass the practical applications of fuzzy sets has been a long process. Currently the most well-known formalisms are those based on category theory [3]. These category-theoretic formulations, in terms of quasi-topoi [4], are quite complex however, and may be difficult to use for those without expertise in category theory. In contrast, the more pragmatic approach of [11] leads to strong restrictions on the semantics that can be used, essentially requiring that formulae are always evaluated by using the Lukasiewicz valuation. This paper presents the results of work which has provided a simple, set-theoretic basis for the foundations of fuzzy sets. This is achieved by the use of residuated logic, which generalises intuitionistic predicate logic, and it encompasses a wide range of common semantic valuations for fuzzy logic. A set theory is built on top of this underlying logic, using the method of [6] to build a cumulative hierarchy of fuzzy sets. There is an interpretation of classical set theory into the resulting theory.	fuzzy set;set theory	Kevin Lano	1992		10.1007/BFb0023880	fuzzy logic;t-norm fuzzy logics;effective descriptive set theory;universal set;combinatorics;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	Theory	-2.5738940166970865	-23.559306412625048	122511
81061c3c4e7c7cd75843f4cbc9b56f9f9207fb46	evolving trading signals at foreign exchange market		Paper examines the merit of evolutionary algorithms to generate trading signals for trading decisions at financial markets. We focus on foreign-exchange market. It is among the largest financial markets. “Technical” traders base their decisions on a set of technical rules evolved from past market activity. We employ a genetic algorithm to learn a set of profitable trading rules considering transaction costs; each rule generates a ‘buy’, ‘hold’, or ‘sell’ signal using moving average technical rule. We empirically evaluate our approach using exchange rates of four major currency pairs over the period 2000 to 2015. Performance evaluation on out-of-sample data indicates that our approach is able to provide acceptably high returns on investment. Comparison with exhaustive search proves convincing performance of our approach.	foreign exchange service (telecommunications)	Svitlana Galeshchuk;Sumitra Mukherjee	2017		10.1007/978-3-319-60285-1_9	open outcry;flash trading;market microstructure;order;finance;electronic trading;financial system;business;trading turret;alternative trading system;algorithmic trading	ECom	5.110970647612695	-16.92812074692988	122695
c60b6ccb959faa2b4a192b3d924fde1e899fb169	bridging the gap between missing and inconsistent values in eliciting preference from pairwise comparison matrices	ahp;improved consistency;期刊论文;pairwise comparison matrix;connecting path method;missing values	Missing values and inconsistency of pairwise comparisonmatrices (PCM) in AHP are tackled with different methods in extant studies. In this article, we prove that connecting path method (CPM), which is a well-known approach of estimating missing judgments, also guarantees minimal geometric consistency index, and only involves elementary connecting paths by unraveling its connection with logarithmic least square method. The formal proofs suggest that CPM alone is sufficient to fix both missing judgments and inconsistency of PCM inAHP.Based on thiswell groundedmerit, a procedure is proposed to improve consistency for incomplete PCM. The validity of the proposed procedure is illustrated by classical examples, and its advantages over other consistency improving approaches are also discussed in detail.	analysis of algorithms;bridging (networking);cladogram;entity–relationship model;formal proof;google code-in;iteration;jian xu;judgment (mathematical logic);level of detail;pairwise summation;regular expression;word lists by frequency	Kun Chen;Gang Kou;J. Michael Tarn;Yan Song	2015	Annals OR	10.1007/s10479-015-1997-z	econometrics;analytic hierarchy process;data mining;mathematics;statistics	AI	-3.898955139140942	-19.895529604045862	122765
a397d42e062ec40640945fc5a09a9a2d9b0d56bd	pgt: a statistical approach to prediction and mechanism design	statistical approach;game theory;theoretical framework;bayesian approach;behavioral economics;decision theory;solution concept;mechanism design;equilibrium theory	One of the biggest challenges facing behavioral economics is the lack of a single theoretical framework that is capable of directly utilizing all types of behavioral data. One of the biggest challenges of game theory is the lack of a framework for making predictions and designing markets in a manner that is consistent with the axioms of decision theory. An approach in which solution concepts are distribution-valued rather than set-valued (i.e. equilibrium theory) has both capabilities. We call this approach Predictive Game Theory (or PGT). This paper outlines a general Bayesian approach to PGT. It also presents one simple example to illustrate the way in which this approach differs from equilibrium approaches in both prediction and mechanism design settings.		David H. Wolpert;James W. Bono	2010		10.1007/978-3-642-12079-4_39	implementation theory;mechanism design;game theory;positive political theory;decision theory;bayesian probability;computer science;artificial intelligence;machine learning;decision rule;management science;mathematical economics;equilibrium selection;solution concept;behavioral economics;statistics	ECom	-3.9819406874313614	-10.873494192740393	122804
ba8f2945531af5bd89fa1e8bec6c78a4fbba255f	multicriteria group decision making under incomplete preference judgments: using fuzzy logic with a linguistic quantifier	multiple criteria;decision maker;fuzzy logic;multiple objectives;mathematical programming;approximate solution;incomplete preferences;group decision making	Abstract#R##N##R##N#In the face of increasing global competition and complexity of the socioeconomic environment, many organizations employ groups in decision making. Inexact or vague preferences have been discussed in the decision-making literature with a view to relaxing the burden of preference specifications imposed on the decision makers and thus taking into account the vagueness of human judgment. In this article, we present a multiperson decision-making method using fuzzy logic with a linguistic quantifier when each group member specifies incomplete judgment possibly both in terms of the evaluation of the performance of different alternatives with respect to multiple criteria and on the criteria themselves. Allowing for incomplete judgment in the model, however, makes a clear selection of the best alternative by the group more difficult. So, further interactions with the decision makers may proceed to the extent to compensate for the initial comfort of preference specifications. These interactions, however, may not guarantee the selection of the best alternative to implement. To circumvent this deadlock situation, we present a procedure for obtaining a satisfactory solution by the use of a linguistic-quantifier-guided aggregation that implies the fuzzy majority. This is an approach that combines a prescriptive decision method via mathematical programming and a well-established approximate solution method to aggregate multiple objects. © 2007 Wiley Periodicals, Inc. Int J Int Syst 22: 641–660, 2007.	fuzzy logic;quantifier (logic)	Duke Hyun Choi;Byeong Seok Ahn;Soung Hie Kim	2007	Int. J. Intell. Syst.	10.1002/int.20218	fuzzy logic;decision-making;group decision-making;computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm;weighted sum model	AI	-4.507358598098088	-19.57911399845128	122876
2c0a4fd6bc04244d9035f2c9869ebe22499f6569	on the valuation of multistage information technology investments embedding nested real options	heuristic nested variation;accurate nested variation;bs model;custom-tailored binomial model;particular heuristic bs model;embedding nested real options;heuristic roa model;multistage information technology investments;untested heuristic roa model;heuristic bs model;binomial model;nested option	As real options analysis (ROA) is being applied to increasingly complex information technology (IT) investment problems, a concern arises over the use heuristic ROA models that are simpler to apply but can produce overvaluations. A good example is the application of a heuristic nested variation of the Black–Scholes (BS) model to the evaluation of interrelated IT investments as nested options. This particular heuristic BS model could overvalue by more than 100 percent. Using a binomial model that is custom-tailored to a generic IT investment embedding nested options as the “baseline,” we identify conditions under which the degree of overvaluation of this heuristic BS model is severe and unpredictable. Moreover, upon examining the structure of the custom-tailored binomial model, we identify the reason for overvaluation 08 benaroch.pmd 4/21/2006, 12:27 AM 239 240 BENAROCH, SHAH, AND JEFFERY and derive a more accurate nested variation of the BS model. These findings should serve as a cautionary message about the use of untested heuristic ROA models.	baseline (configuration management);black–scholes model;heuristic;multistage amplifier;resource-oriented architecture;value (ethics)	Michel Benaroch;Sandeep Shah;Mark Jeffery	2006	J. of Management Information Systems		financial economics;actuarial science;economics;operations management;black–scholes model	NLP	-3.2264617461432525	-11.850711364475163	122982
3a49361f2d258adb40c7c5aff17a095eaa843703	designing a decision support system to evaluate and select suppliers using fuzzy analytic network process	outsourcing;non linear programming;community development;fuzzy set theory;decision support system;analytic network process;fuzzy sets theory;eigenvectors;supplier selection	0360-8352/$ see front matter 2009 Elsevier Ltd. A doi:10.1016/j.cie.2009.06.008 * Corresponding author. Address: Department of In of Engineering, University of Tehran, North Kargar Tehran, Iran. Tel.: +98 21 88021067; fax: +98 21 880 E-mail addresses: jrazmi@ut.ac.ir (J. Razmi), m.hashemi62@gmail.com (M. Hashemi). Recent information and communication developments caused that global organizations spread out their markets throughout the world. In this environment, local exclusive markets have been replaced with global competitive ones. Therefore, organizations must concentrate on their main operations to survive in such an environment. To do so, managers have intended to cooperate with some financial partners in long-term relations. In this paper, the aim is to develop a fuzzy analytic network process (ANP) model to evaluate the potential suppliers and select the best one(s) with respect to the vendor important factors. Additionally, ANP is developed by fuzzy sets theory to cover the indeterminacy of decisions made in this field. The authors have augmented the model with a non-linear programming model to elicit eigenvectors from fuzzy comparison matrices. Hybridization of these two concepts can model supplier selection problem in all circumstances and reaches the optimal choice. Finally, a numerical sample is used to validate the proposed model. 2009 Elsevier Ltd. All rights reserved.	decision support system;fax;fuzzy set;indeterminacy in concurrent computation;linear programming;nonlinear programming;nonlinear system;numerical analysis;outsourcing;programming model;requirement;selection algorithm;vagueness	Jafar Razmi;Hamed Rafiei;Mahdi Hashemi	2009	Computers & Industrial Engineering	10.1016/j.cie.2009.06.008	mathematical optimization;community development;decision support system;eigenvalues and eigenvectors;computer science;engineering;artificial intelligence;marketing;operations management;machine learning;data mining;mathematics;management science;fuzzy set;management;fuzzy set operations;analytic network process;outsourcing	AI	-4.492435558584782	-16.000806818441408	123025
d0dd02c5890bdd16d8299dea49e895f43702ea0b	forecasting time series with a new architecture for polynomial artificial neural network	forecasting;chaotic time series;polynomial artificial neural network;time series;nonlinear time series;time use;genetic algorithm;artificial neural network	Polynomial artificial neural networks (PANN) have been shown to be powerful for forecasting nonlinear time series. The training time is small compared to the time used by other algorithms of artificial neural networks and the capacity to compute relations between the inputs and outputs represented by every term of the polynomial. In this paper a new structure of polynomial is presented that improves the performance of this type of network considering only non-integers exponents. The architecture adaptation uses genetic algorithm (GA) to find the optimal architecture for every example. Some examples of sunspots and chaotic time series are presented.	artificial neural network;polynomial;time series	Eduardo Gómez-Ramírez;Kaddour Najim;Enso Ikonen	2007	Appl. Soft Comput.	10.1016/j.asoc.2006.01.008	genetic algorithm;forecasting;computer science;artificial intelligence;machine learning;time series;time delay neural network;artificial neural network;statistics	ML	8.759367021810476	-21.015514885336234	123037
97b3559aa203a4fefd24a74a7871a48992fd30f3	econometric modelling of time series relationship between fertility and income for the russian population: methodological issues			time series	Oksana Shubat;Anna Bagirova	2018		10.7148/2018-0020	econometrics;population;fertility;economics	AI	4.546783258604463	-14.032652410460363	123054
4042e90e2852b1f17ee9fcfa16fed6e1cae46846	the optimization model based on support vector machine for the energy-conserving generation dispatch	energy conservation;southern hebei grid optimization model support vector machine energy conserving generation dispatch;optimisation;kernel;support vector machines;rough set theory;training;energy conservation and consumption reduction;data mining;indexes;support vector machines energy conservation optimisation;indexation;optimization;southern hebei grid;support vector machines power generation set theory energy conservation risk management rough sets equations power engineering computing computational modeling mechanical engineering;support vector machine;support vector machine energy conservation and consumption reduction energy conserving generation dispatch rough set theory;optimization model;energy conserving generation dispatch	According to the situation of generation units operating, we establish the index system of energy-conserving generation dispatch and the optimization model based on support vector machine (SVM) .Then we make experiment by the real data of southern Hebei grid, and the result show that SVM can be realized easily and has high accuracy. The model is feasible and effective, and this method can improve the evaluation precision and has good spread ability.	dynamic dispatch;lazy evaluation;mathematical optimization;support vector machine	Cui Yan-bin	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.429	mathematical optimization;engineering;operations management;data mining	Robotics	9.57061444162967	-16.807878542916146	123089
35f50bc53e8ed92668f72da4edf4e948833acd24	soft computing based on maximizing consensus and fuzzy topsis approach to interval-valued intuitionistic fuzzy group decision making	fuzzy topsis;consensus;experts weights;multi attribute group decision making;期刊论文;interval valued intuitionistic fuzzy number;multi choice goal programming	We define the consensus index from the perspective of the ranking of decision information and derive the experts' weights on the basis of the idea of the maximizing consensus.We extend the TOPSIS approach to rank all the alternatives from the perspective of the magnitude of decision information under interval-valued intuitionistic fuzzy environment.Combining with the multi-choice goal programming models, our approach can not only find the optimal alternative(s) but also determine their corresponding optimum quantities. Multi-attribute group decision making (MAGDM) is an important research topic in decision theory. In recent decades, many useful methods have been proposed to solve various MAGDM problems, but very few methods simultaneously take them into account from the perspectives of both the ranking and the magnitude of decision data, especially for the interval-valued intuitionistic fuzzy decision data. The purpose of this paper is to develop a soft computing technique based on maximizing consensus and fuzzy TOPSIS in order to solve interval-valued intuitionistic fuzzy MAGDM problems from such two aspects of decision data. To this end, we first define a consensus index from the perspective of the ranking of decision data, for measuring the degree of consensus between the individual and the group. Then, we establish an optimal model based on maximizing consensus to determine the weights of experts. Following the idea of TOPSIS, we calculate the closeness indices of the alternatives from the perspective of the magnitude of decision data. To identify the optimal alternatives and determine their optimum quantities, we further construct a multi-choice goal programming model based on the derived closeness indices. Finally, an example is given to verify the developed method and to make a comparative analysis.	fuzzy set;soft computing	Xiaolu Zhang;Zeshui Xu	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.08.073	mathematical optimization;consensus;optimal decision;computer science;machine learning;data mining;mathematics;weighted sum model	ECom	-3.4010197253659125	-20.05957521238437	123092
507eae8688abd093865eccd8bb8c340ef7f3a6d7	improving prediction in tac scm by integrating multivariate and temporal aspects via pls regression		We address the construction of a prediction model from data available in a complex environment. We first present a data extraction method that is able to leverage information contained in the movements of all variables in recent observations. This improved data extraction is then used with a common multivariate regression technique: Partial Least Squares (PLS) regression. We experimentally validate this combined data extraction and modeling with data from a competitive multi-agent supply chain setting, the Trading Agent Competition for Supply Chain Management (TAC SCM). Our method achieves competitive (and often superior) performance compared to the state-of-the-art domain-specific prediction techniques used in the 2008 Prediction Challenge competition.		William Groves;Maria L. Gini	2011		10.1007/978-3-642-34889-1_3	artificial intelligence;data mining;operations research	Vision	4.842162641280938	-19.65302547849875	123116
b9c3f0d6422efe847633cdd2869ddc5e8536436f	an interpretation of rough sets in incomplete information systems within intuitionistic fuzzy sets	intuitionistic fuzzy set;membership degree;intuitionistic fuzzy sets;incomplete information;rough sets;incomplete information systems;rough set	The intuitionistic fuzzy rough set is introduced in an attempt to solve certain problems in incomplete information systems. Some properties of the corresponding intuitionistic fuzzy set are studied and application of the intuitionistic fuzzy set is explored by illustrations.	fuzzy set;information system;intuitionistic logic;rough set	Xiao-Ping Yang	2009		10.1007/978-3-642-02962-2_41	mathematical analysis;discrete mathematics;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;machine learning;data mining;fuzzy set;fuzzy set operations	DB	-1.7825187845099364	-22.602094723159677	123210
6cf286ee1774072eee0a00d9eee2782ae17da009	very short term load forecasting using cartesian genetic programming evolved recurrent neural networks (cgprnn)	load forecasting load modeling predictive models forecasting genetic programming recurrent neural networks;cartesian genetic programming evolved recurrent neural network cgprnn;demand side management;uk grid station cartesian genetic programming evolved recurrent neural networks electricity demand electricity generation neuroevolutionary technique load forecasting model cgprnn;recurrent neural nets demand side management genetic algorithms load forecasting power engineering computing;load forecasting;very short term load forecast vstlf cartesian genetic program cgp cartesian genetic programming evolved recurrent neural network cgprnn;power engineering computing;very short term load forecast vstlf;cartesian genetic program cgp;genetic algorithms;recurrent neural nets	Forecasting the electrical load requirements is an important research objective for maintaining a balance between the demand and generation of electricity. This paper utilizes a neuro-evolutionary technique known as Cartesian Genetic Programming evolved Recurrent Neural Network (CGPRNN) to develop a load forecasting model for very short term of half an hour. The network is trained using historical data of one month on half hourly basis to predict the next half hour load based on the 12 and 24 hours data history. The results demonstrate that CGPRNN is superior to other networks in very short term load forecasting in terms of its accuracy achieving 99.57 percent. The model was developed and evaluated on the data collected from the UK Grid station.	cartesian closed category;electrical load;genetic programming;recurrent neural network;requirement	Gul Muhammad Khan;Faheem Zafari;Sahibzada Ali Mahmud	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.181	simulation;genetic algorithm;computer science;artificial intelligence;machine learning	Robotics	9.31316366417247	-18.169566643181103	123283
4fca08cad62ecfd87bb61a72fdbca7859a1a425b	a new fuzzy topsis-todim hybrid method for green supplier selection using fuzzy time function		Today green supply chain is considered all around the world and supplier selection has been changed regarding these green and carbon emission criteria, so green supplier selection has been a major problem in this area. In this study we use fuzzy time function to assist managers in green supplier selection under uncertainty and ambiguity. This function will consider derivation from the goal during the time and by using it, and we will be able to have the best supplier in every period after having some modification in legal limitations for green supplier selection criteria. We use a fuzzy TOPSIS to have better initial weighting in TODIM, a discrete multicriteria method based on prospect theory in uncertainty (known as TODIM in Portuguese) decision making method. The results indicated that our proposed approach can easily and effectively accommodate criteria with gains and loss functions during time and also by using this method we will have a more reasonable predict of our suppliers ranking in future and that will help us in future investment in these suppliers. Finally it has been shown in car industries in Iran.	loss function;parse tree	Alireza Arshadi Khamseh;Mahdi Mahmoodi	2014	Adv. Fuzzy Systems	10.1155/2014/841405	management science;operations research	ML	-4.456684252529982	-15.941459251571537	123504
0e9e5da65693330928ee7abb930aaf9541cad484	contribution of site assessment toward prioritising investment in natural capital	analytic hierarchy process;costs and benefits;agri environment schemes;stewardship;ranking and selection;information sharing;gis;decision making process;cost effective;cost effectiveness;multi attribute utility theory;landscape scale;analytical hierarchy process;payments for ecosystem services;natural capital;south australia;conservation investment	In prioritising investment in natural capital, site-scale indicators are increasingly used to capture finescale variation inherent in complex ecosystems. However, site assessment is costly, has high skill demand, and is time-consuming. We assess the marginal gain associated with including site-scale indicators in metrics typically used by agri-environmental stewardship schemes and payments for ecosystem services. We developed 18 landscape-scale and 14 site-scale indicators to prioritise sites for on-ground works in a real-world conservation auction in South Australia. We used the Analytical Hierarchy Process (AHP) to weight them and multi-attribute utility theory to combine them in quantifying site priority. Bid benefit was calculated as the product of impact of the proposed works and the site priority. Cost-utility analysis was used to rank and select bids with benefits calculated using: i) landscape-scale indicators, and; ii) both landscapeand site-scale indicators. We found that the inclusion of site-scale indicators has limited influence on the ranking and selection of bids for investment when cost of investment is included in the decision-making process. We suggest that, depending on the nature of costs and benefits, and if landholder engagement, information sharing, and trust-building can be achieved in more efficient ways, site assessment may not be necessary. Thereby a significant barrier to the adoption of cost-effective agri-environment schemes and payments for ecosystem services may be eliminated. Crown Copyright 2010 Published by Elsevier Ltd. All rights reserved.	analytical hierarchy;crown group;ecosystem services;marginal model;utility	Neville D. Crossman;Brett A. Bryan;Darran King	2011	Environmental Modelling and Software	10.1016/j.envsoft.2010.04.022	analytic hierarchy process;geomatics;economics;computer science;environmental resource management;mathematics;welfare economics	HCI	-1.8205043073094986	-10.256199964684377	123556
951f6c48bac54eea56e4b0870a8a2fddb3be7f81	continuity and completeness of lattice-valued possibilistic measures	possibilistic distribution;partially ordered set;lattice valued possibilistic measure;continuity from above and from below;completeness;complete lattice;probability measure	Properties such as continuity from above and from below and various kinds of completeness are analysed when investigating set functions, in particular probabilistic and possibilistic measures and the relations of these properties to properties such as (σ)-additivity of probability measures or (complete) maxitivity of possibilistic measures are proved. In this work, the notions of continuity from above and from below are introduced for non-numerical possibilistic measures, taking their values in a complete lattice and at least for some relations valid for real-valued possibilistic measures of their analogies for lattice-valued possibilistic measures are stated and proved.	scott continuity	Ivan Kramosil	2006	Int. J. General Systems	10.1080/03081070600687700	partially ordered set;combinatorics;mathematical analysis;discrete mathematics;probability measure;complete lattice;completeness;mathematics;statistics	Logic	0.2346984012390627	-21.082258439129728	123576
427da4e2e21400dd4f648e0debe2a8e126ed1dc9	fuzzy logic approach to autonomous car parking using matlab	mathematics computing;traffic control;fuzzy logic;decision action approach autonomous car parking matlab fuzzy logic decision making;decision making fuzzy logic traffic control mathematics computing;fuzzy logic matlab humans mathematical model wheels automobiles pressing turning	"""This paper discusses a system that parks automatically an automobile given certain conditions and making decisions based in fuzzy logic. It proposes three models in cascade in order to achieve a """"decision-action"""" approach, so that the car in question does not need an operator."""	autonomous car;autonomous robot;fuzzy logic;matlab	Carlos Daniel Pimentel Flores;Miguel Ángel Hernández Gutiérrez;Rubén Alejos Palomares	2005	15th International Conference on Electronics, Communications and Computers (CONIELECOMP'05)	10.1109/CONIEL.2005.41	fuzzy logic;control engineering;fuzzy electronics;simulation;fuzzy cognitive map;defuzzification;computer science;engineering;artificial intelligence;neuro-fuzzy;fuzzy set operations;fuzzy control language;fuzzy control system	Robotics	7.417708688371128	-13.124312333460802	123730
a4472a2d9718535048eea5681ec14a4ad5601e7d	dea and benchmarks - an application to nordic banks	efficient frontier;data envelope analysis;convex combination	In this paper, Data Envelopment Analysis (DEA) is developed to analyze the efficiencyof a single bank. The inputs are given in terms of cost of personnel, cost of material andexpected cost of credit losses. Outputs concern lending, deposits and gross revenues (interestmargins and non-interest income). The data covers 48 large Nordic banks during the twoyears 1992 and 1993. Fourteen banks are from Denmark, thirteen from Finland, twelve fromNorway and nine from Sweden. For each of these banks, the DEA method is used to form a“reference bank”, which is a convex combination of the best competing banks (those at theefficiency frontier). The three inputs and the three outputs of the reference bank will beused as benchmarks. This procedure implies that one can only say that one single bank isless efficient than its reference bank, not less efficient than another bank. The results showthat 4-7 Nordic banks were situated at the efficiency frontier for those two years. Thesebanks should then be used to form reference banks for other banks, and to set benchmarksfor them. Such benchmarks would have been slightly different, dependent on the “window”to be used, 1992, 1993 or 1992 + 1993. Copyright Kluwer Academic Publishers 1998		Göran Bergendahl	1998	Annals OR	10.1023/A:1018910719517	efficient frontier;actuarial science;convex combination;economics;operations management;data envelopment analysis;mathematics;economy	Crypto	4.910421824129915	-10.384188298162845	123751
87f2865d1e9962e1a4dca71ae690e4e6ca2fff46	short-term management of hydropower reservoirs under meteorological uncertainty by means of multi-stage optimization			mathematical optimization	Steffi Naumann;Dirk Schwanenberg;Divas Karimanzira;Fernando Fan;Christopher Allen	2015	Automatisierungstechnik		engineering;control engineering;systems engineering;hydropower	DB	6.838565792535679	-10.012163858390839	123763
a7eaeb5cacc896c404eb0af5559e35cd77e9e6e8	new multi-criteria decision-making based on fuzzy similarity, distance and ranking		We propose a new method for group decision making by using trapezoidal fuzzy numbers to describe decisions. A team of decision makers (or experts) must choose the most appropriate alternative using fuzzy logic. Each expert will give his opinion about every alternative in accordance with the criteria of choice by a fuzzy number. The decisions taken are aggregated respecting the similarity and dissimilarity (distance) between each pair of opinions, in addition to the hierarchical weights (importance) of each decision-maker (DM). The result is a fuzzy number representing the general appreciation of each alternative. In order to able to choose one, an appropriate ranking method is proposed. In this article we treat four issues, namely, similarity and distance between opinions represented by fuzzy numbers, aggregation of opinions while preserving similarity and in accordance with the hierarchical weight, and finally ranking fuzzy numbers.		M. El Alaoui;H. Ben-Azza;A. Zahi	2016		10.1007/978-3-319-60834-1_15	ranking svm;fuzzy logic;group decision-making;fuzzy set;similarity (network science);mathematics;pattern recognition;artificial intelligence;fuzzy classification;ranking;fuzzy number	HCI	-3.3478417400704124	-20.143181692309206	124175
46b2be829a60b052f654f246e2368944f58613e5	an outranking approach for multi-criteria decision-making problems with interval-valued neutrosophic sets	multi criteria decision making;interval valued neutrosophic sets;outranking;期刊论文;electre iv	In this paper, a novel outranking approach for multi-criteria decision-making (MCDM) problems is proposed to address situations where there is a set of numbers in the real unit interval and not just a specific number with a neutrosophic set. Firstly, the operations of interval neutrosophic sets and their related properties are introduced. Then some outranking relations for interval neutrosophic numbers (INNs) are defined based on ELECTRE IV, and the properties of the outranking relations are further discussed in detail. Additionally, based on the outranking relations of INNs, a ranking approach is developed in order to solve MCDM problems. Finally, two practical examples are provided to illustrate the practicality and effectiveness of the proposed approach. Moreover, a comparison analysis based on the same examples is also conducted.	computation	Hong-yu Zhang;Jian-qiang Wang;Xiaohong Chen	2015	Neural Computing and Applications	10.1007/s00521-015-1882-3	artificial intelligence;data mining;mathematics;algorithm	SE	-3.0046613410926577	-20.668099136806912	124189
13cbbed12c77294cb94d4a70513c84039150ab0a	a fast approach to compute fuzzy values of matrix games with payoffs of triangular fuzzy numbers	i linear programming;s fuzzy sets;i game theory;journal;p uncertainty modeling;期刊论文;s group decisions and negotiations	The aim of this paper is to develop an effective method for solving matrix games with payoffs of triangular fuzzy numbers (TFNs) which are arbitrary. In this method, it always assures that players’ gain-floor and loss-ceiling have a common TFN-type fuzzy value and hereby any matrix game with payoffs of TFNs has a TFN-type fuzzy value. Based on duality theorem of linear programming (LP) and the representation theorem for fuzzy sets, the mean and the lower and upper limits of the TFN-type fuzzy value are easily computed through solving the derived LP models with data taken from 1-cut set and 0-cut set of fuzzy payoffs. Hereby the TFN-type fuzzy value of any matrix game with payoffs of TFNs can be explicitly obtained. Moreover, we can easily compute the upper and lower bounds of any Alfa-cut set of the TFN-type fuzzy value for any matrix game with payoffs of TFNs and players’ optimal mixed strategies through solving the derived LP models at any specified confidence level Alfa. The proposed method in this paper is demonstrated with a numerical example and compared with other methods to show the validity, applicability and superiority. 2012 Elsevier B.V. All rights reserved.	alfa (xacml);cut (graph theory);decision problem;effective method;finite element method;fuzzy associative matrix;fuzzy number;fuzzy set;linear programming;mathematical model;numerical analysis;the matrix;tribe floodnet	Deng-Feng Li	2012	European Journal of Operational Research	10.1016/j.ejor.2012.06.020	mathematical optimization;combinatorics;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;fuzzy associative matrix;fuzzy set operations	AI	-2.1115125697344572	-19.564442096398245	124238
072c8b5c9874f31e31aeccdf7e1604ca569167b7	an analysis of the performance of university-affiliated credit unions	finanza;metodo estadistico;analisis envolvimiento datos;universite;finance;statistical method;credit;credit union;credito;data envelopment analysis;methode statistique;analyse performance;performance analysis;seemingly unrelated regression;university;educational attainment;data envelope analysis;universidad;free disposal hull;analyse enveloppement donnee;analisis eficacia	In this paper we analyze the operating efficiency of a group of university-affiliated credit unions in 1990. We use free disposal hull (FDH) techniques, which generalize data envelopment analysis (DEA) techniques by dispensing with the convexity assumption imposed in DEA, to measure the operating efficiency of university-affiliated credit unions and to compare their efficiency with that of credit unions not affiliated with a university. The purpose of the analysis is to test the hypothesis that university-affiliated credit unions, by virtue of the superior educational attainment of their members, some of whom sit on boards of directors that monitor managements, are thereby better managed and so perform better. In the second stage of the analysis we use seemingly unrelated regression (SUR) techniques to identify exogenous factors that might explain variation in operating efficiency among the university-affiliated credit unions.		H. O. Fried;C. A. Knox Lovell;J. A. Turner	1996	Computers & OR	10.1016/0305-0548(95)00045-3	computer science;data envelopment analysis;mathematics;operations research;statistics	NLP	0.19700367497173402	-14.343256917746999	124251
d0e7df127ecf9d7ab1bcd2cc99953318b51b7980	identification of chiller model in hvac system using fuzzy inference rules with zadeh's implication operator	gradient descent method;chiller;heating ventilating and air conditioning;objective function;mean square error;fuzzy inference;hvac system;fuzzy inference system;improved genetic algorithm;numerical experiment;optimal algorithm;fuzzy model;implication operator	In the heating, ventilating, and air-conditioning (HVAC) system, chiller is the central part and one of the primary energy consumers. For the purpose of saving energy, the identification of the chiller model is of great significance. In this paper, based on fuzzy inference rules with Zadeh's implication operator, the model of chiller in HVAC is identified. The mean square error (MSE) is employed to evaluate the approximating capability of the fuzzy inference system. The objective of the problem is to minimize MSE. Since the Zadeh's implication operator is adopted in the fuzzy inference, the output of the system becomes a continuous but non-smooth function. In addition, the objective function contains many parameters that need to be optimized, consequently, traditional optimization algorithms based on gradient descent method fail to work. Therefore, an improved genetic algorithm (GA) is applied to minimize the MSE. Actual operational data of a chiller in HVAC are gathered to train the fuzzy inference system. Numerical experiment results validate the accuracy and efficiency of proposed fuzzy model and the improved GA algorithm.		Yukui Zhang;Shiji Song;Cheng Wu;Kang Li	2010		10.1007/978-3-642-15621-2_44	gradient descent;mathematical optimization;hvac;chiller;adaptive neuro fuzzy inference system;computer science;fuzzy number;machine learning;control theory;mathematics;mean squared error;fuzzy control system	DB	10.027792102311002	-15.444403788778493	124286
675186c35935a0d285598936d3a42c5404aeb1fd	a note on the use of choquet and sugeno integrals in minimal and maximal covering location problems	optimisation fuzzy set theory;mathematical models choquet integrals sugeno integrals fuzzy integrals sugeno integral choquet integral maximal covering location problem minimal covering location problem minclp mclp decision maker behavior;sugeno integral minimal maximal covering choquet integral;computational complexity;particle swarm optimization;intelligent systems;mathematical model;optimization;informatics;particle swarm optimization mathematical model educational institutions optimization intelligent systems informatics computational complexity	The aim of this paper is to show a potential applicability of some well-known fuzzy integrals, i.e., of the Choquet integral and the Sugeno integral, in Minimal and Maximal Covering location problems, i.e., in MinCLP and MCLP. Possible benefits of the use of Choquet and Sugeno integrals lie in the flexibility of a monotone set function which is the core of the observed integrals and which is being used for modelling Decision Maker's behavior. Mathematical models of Minimal and Maximal Covering location problems are given. Approach based on fuzzy integrals versus the standard two types of operators is discussed. Ideas for the future work and applications are presented.	algorithm;heuristic (computer science);mathematical model;maximal set;particle swarm optimization;sugeno integral;monotone	Aleksandar Takaci;Ivana Stajner-Papuga;Miroslav Maric;Darko Drakulic	2014	2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2014.6923577	mathematical optimization;mathematical analysis;discrete mathematics;fuzzy measure theory;mathematics	Embedded	-1.0192646156198992	-19.968158955647695	124490
31c537033455fa80ef6266f723206695ba01b1b0	fuzzy strength of preference in the graph model for conflict resolution with two decision makers		A new hybrid preference framework of the Graph Model for Conflict Resolution (GMCR) is proposed to facilitate modeling and analysis of conflicts involving two decision makers (DMs) with fuzzy levels of preference. The new preference structure, a hybrid of models of three-level strength of preference and fuzzy preference, provides a more flexible technique to express DMs' relative preferences among states, and is more comprehensive and flexible than existing models. One key contribution of this paper is to extend four graph model stability definitions for fuzzy preferences, consisting of fuzzy Nash stability (FR), fuzzy general metarationality (FGMR), fuzzy symmetric metarationality (FSMR), and fuzzy sequential stability (FSEQ), to the new hybrid preference structure. This extension permits the analysis of the hybrid model, including both strong and weak stabilities. The resulting modeling and analysis system can be applied to complex two-decision-maker conflicts, thereby enhancing the ability of the graph model to provide strategic insights. A specific two decision-maker conflict is utilized to illustrate the practical application of the new hybrid preference framework for GMCR.	decision support system;nash equilibrium	Jing Yu;Keith W. Hipel;D. Marc Kilgour;Liping Fang	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8123186	machine learning;fuzzy logic;artificial intelligence;computer science;conflict resolution;graph	Robotics	-2.8824781072252414	-18.512079984889144	124570
1839e468b64a7110bf5d3dc3debcae60a98a2406	a hybrid fuzzy and neural approach for dram price forecasting	multiple linear regression;forecasting;agregacion;microelectronic fabrication;prevision;fabricacion microelectrica;non linear programming;fuzzy neural nets;control difusa;nonlinear programming;fuzzy number;social decision;pricing;programacion no lineal;fuzzy control;logique floue;polygone;linear regression;logica difusa;industrie electronique;reseau neuronal flou;programmation non lineaire;interseccion;fijacion precios;aggregation;regresion lineal multiple;fuzzy logic;polygon;tariffication;analisis regresion;fuzzy;back propagation network;tarification;regresion multiple;electronics industry;backpropagation algorithm;indexation;industria electronica;regresion lineal;neural;agregation;poligono;algorithme retropropagation;analyse regression;dynamic random access memory;regression analysis;peritaje;regression lineaire multiple;expertise;reseau neuronal;decision colectiva;intersection;dram;decision collective;price;memoire acces direct dynamique;regression multiple;red neuronal difusa;regression lineaire;red neuronal;fixation prix;tarificacion;commande floue;neural network;fabrication microelectronique;algoritmo retropropagacion;multiple regression	The trend in the price of dynamic random access memory (DRAM) is a very important prosperity index in the semiconductor industry. To further enhance the performance of DRAM price forecasting, a hybrid fuzzy and neural approach is proposed in this study. In the proposed approach, multiple experts construct their own fuzzy multiple linear regression models from various viewpoints to forecast the price of a DRAM product. Each fuzzy multiple linear regression model can be converted into two equivalent nonlinear programming problems to be solved. To aggregate these fuzzy price forecasts, a two-step aggregation mechanism is applied. At the first step, fuzzy intersection is applied to aggregate the fuzzy price forecasts into a polygon-shaped fuzzy number, in order to improve the precision. After that, a back propagation network is constructed to defuzzify the polygon-shaped fuzzy number and to generate a representative/crisp value, so as to enhance the accuracy. A real example is used to evaluate the effectiveness of the proposed methodology. According to experimental results, the proposed methodology improved both the precision and accuracy of DRAM price forecasting by 66% and 43%, respectively. 2010 Elsevier B.V. All rights reserved.	aggregate data;autoregressive integrated moving average;backpropagation;consensus (computer science);futures studies;fuzzy number;fuzzy set operations;linear logic;nonlinear programming;nonlinear system;pattern recognition;random access;semiconductor industry;software propagation	Tsuhan Chen	2011	Computers in Industry	10.1016/j.compind.2010.10.012	econometrics;defuzzification;nonlinear programming;computer science;linear regression;artificial intelligence;fuzzy number;machine learning;fuzzy set operations;artificial neural network;algorithm	AI	4.937083508247649	-21.119368426014777	124678
4d41faf1856add0466c92eae8ae53f253ac70ad1	improved online estimation methods for a feedback-based freeway ramp metering strategy	nonlinear filters;estimation theory;online estimation;critical density estimation;extended kalman filter online estimation feedback based freeway ramp metering critical density estimation;estimation method;road traffic;real time;kalman filters;environmental conditions;traffic flow;road traffic estimation theory feedback kalman filters metering nonlinear filters;ramp metering;traffic control communication system traffic control delay intelligent transportation systems snow rain density measurement fluid flow measurement performance evaluation matlab;feedback;mathematical models;feedback based freeway ramp metering;metering;extended kalman filter;ramps interchanges	The critical density of a freeway link is subject to changes over time owing to such circumstances as environmental conditions (snow, rain, etc.) and traffic incidents. Because of the critical density impacts on the performance of some ramp metering strategies that make use of it as a threshold value for control action, it is necessary to trace the real value of critical density. This paper presents improvements to the methodology for the online estimation of critical density using the extended Kalman filter (EKF) proposed by Ozbay et al. (2006). Basically, critical density and density of the freeway section are chosen as the state variables to be determined using the system output, namely the measurement of traffic flow and occupancy on the downstream freeway link. The effectiveness of the proposed method is evaluated using the feedback-based ramp metering strategy ALINEA (Papageorgiou et al., 1991). A number of simulations are run to investigate the sensitivity of the proposed methodology with respect to initial estimates and time step size selection. Also, the methodology's capability of tracking gradual and sudden changes in real-time critical density is examined. This new methodology provided successful performances based on the macroscopic simulation evaluation using MATLAB	downstream (software development);extended kalman filter;freeway;matlab;performance;ramp simulation software for modelling reliability, availability and maintainability;real-time clock;software metering	Kaan Ozbay;Ilgin Yasar;Pushkin Kachroo	2006	2006 IEEE Intelligent Transportation Systems Conference	10.1109/ITSC.2006.1706776	control engineering;simulation;engineering;control theory	Robotics	9.52518373781146	-12.208260545808	124846
8e8fc25c9cf6c55a6cbd860d563e5f5555e56f3c	associative model for the forecasting of time series based on the gamma classifier		The paper describes a novel associative model for the forecasting of time series in petroleum engineering. The model is based on the Gamma classifier, which is inspired on the Alpha-Beta associative memories, taking the alpha and beta operators as basis for the gamma operator. The objective is to reproduce and predict future oil production in different scenarios in an adjustable time window. The distinctive features of the experimental data set are spikes, abrupt changes and frequent discontinuities, which considerably decrease the precision of traditional forecasting methods. As experimental results show, this classifier-based predictor exhibits competitive performance. The advantages and limitations of the model, as well as lines of improvement, are discussed.	associative model of data;data point;gamma correction;gene prediction;kerrison predictor;sensor;server name indication;time series	Itzamá López-Yáñez;Leonid Sheremetov;Cornelio Yáñez-Márquez	2013		10.1007/978-3-642-38989-4_31	speech recognition;computer science;artificial intelligence;machine learning	ML	8.088010454095613	-20.829678552637983	124853
9029490450550f77c9f96ba7ed7d6c0cfdc62d09	building dynamic correlation network for financial asset returns	filtering;correlation electric shock matrix converters time series analysis atmospheric modeling distortion data models;correlation network;time series dynamic correlation network dynamic correlation matrix estimation financial asset return linear correlation matrix multivariate volatility model dcc garch model generalized autoregressive conditional heteroskedasticity model with dynamic conditional correlation return filtering model fitting japanese stock return;asset returns;dynamic correlation;time series autoregressive processes correlation theory estimation theory matrix algebra modelling network theory graphs stock markets;volatility model correlation network dynamic correlation asset returns filtering;volatility model	This paper studies the dynamic correlation matrix estimation of highly volatile financial returns, which is used to build a dynamic correlation network. The widely used method of calculating time-dependent linear correlation matrices by moving window of a fixed sample period can have fundamental problems when applied to fat-tailed returns. A multivariate volatility model, DCC-GARCH, is employed to filter the fat-tailed returns and estimate the dynamic correlation of returns in order to overcome such difficulties. The time-dependent correlation matrices are calculated and compared with the ones that are calculated by the traditional calculation method to highlight the advantages of the proposed dynamic correlation based method. As a case study, the model is fitted to the Japanese stock returns to analyze dynamic changes in the correlation matrix. The method is not limited to financial returns, but can also be applied to build a dynamic correlation network of other time series data with high volatility.	data compression;distortion;high-level programming language;quantum fluctuation;software propagation;time series;volatility	Takashi Isogai	2015	2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2015.39	filter;computer vision;econometrics;computer science;partial correlation;correlation swap;statistics	DB	4.249841397399444	-11.569403993913273	125102
73f368967147a53690690260eb4d8521e647b321	lbf: a labeled-based forecasting algorithm and its application to electricity price time series	forecasting;pattern clustering;time series data mining economic forecasting pattern clustering power engineering computing power markets;economic forecasting;electricity prices;data mining labeled based forecasting algorithm electricity price time series prediction data clustering technique energy related time series prediction;training;prediction algorithms;neighbourhood;predictive models data mining computer science time series analysis artificial neural networks wavelet transforms application software labeling prediction algorithms testing;time series;data mining;neighbourhood clustering time series forecasting;power markets;power engineering computing;time series analysis;clustering;clustering algorithms;electricity	A new approach is presented in this work with the aim of predicting time series behaviors. A previous labeling of the samples is obtained utilizing clustering techniques and the forecasting is applied using the information provided by the clustering. Thus, the whole data set is discretized with the labels assigned to each data point and the main novelty is that only these labels are used to predict the future behavior of the time series, avoiding using the real values of the time series until the process ends. The results returned by the algorithm, however, are not labels but the nominal value of the point that is required to be predicted. The algorithm based on labeled (LBF) has been tested in several energy-related time series and a notable improvement in the prediction has been achieved.	algorithm;cluster analysis;data point;discretization;linear programming relaxation;time series	Francisco Martínez-Álvarez;Alicia Troncoso Lora;José Cristóbal Riquelme Santos;Jesús S. Aguilar-Ruiz	2008	2008 Eighth IEEE International Conference on Data Mining	10.1109/ICDM.2008.129	computer science;data science;machine learning;economic forecasting;time series;data mining;mathematics;cluster analysis;statistics	DB	8.397853015134025	-17.516734859592518	125156
aec544ebe577b988520fc509e9e4344d894e7aa2	use of control charts with regression analysis for autocorrelated data in the context of logistic financial budgeting		Abstract The aim of this paper is to explore whether the use of control charts with regression analysis is an effective way to evaluate financial budget requests (autocorrelated data) in the transport logistics sector. First, the variables are selected. Second, a regression analysis is performed to model the financial variables. Third, three types of traditional control charts are tested (individuals, CUSUM and EWMA), using simulation to monitor the regression scaled residuals. The results show that the individual control chart of 2.7-sigma offers an appropriate performance for the context of this study. This paper provides new evidence regarding a type of variable and context not reported in the literature. In addition, it proposes a control chart approach of scaled regression residuals, with two differentiators: (1) residuals offer better practical interpretation and (2) regressions do not incorporate the time variable, as traditionally occurs, but a missionary process variable (loading units) and a control one.	autocorrelation;chart	Jorge Iván Pérez Rave;Leandro Muñoz-Giraldo;Juan Carlos Correa-Morales	2017	Computers & Industrial Engineering	10.1016/j.cie.2017.08.015	econometrics;autocorrelation;engineering;regression analysis;operations management;process variable;statistics;cross-sectional regression;ewma chart;finance;cusum;control chart	SE	5.402206135484783	-15.071093581656342	125190
fd9d591be91e14133501e1c236c6929df7fae4a0	the consistent value of fuzzy games	balanced contributions;engineering;game theory;fuzzy set;procesamiento informacion;coalitional games;dynamical processes;conjunto difuso;teoria juego;ensemble flou;theorie jeu;potential;ingenierie;shapley value;information processing;ingenieria;sistema difuso;systeme flou;traitement information;potential function;consistency;fuzzy system;self reduced game	In the framework of fuzzy games, we offer an extension of the reduced game introduced by Hart and Mas-Colell, which we name the self-reduced game. According to consistency which related to the self-reduced game, we provide a definition of the consistent value which is a generalization of the Shapley value of fuzzy games. We adopt three existing concepts from coalitional game theory and reinterpret them in the framework of fuzzy games. The first one is that there exists a unique potential function and the resulting payoff vector coincides with the consistent value. Second, based on the properties of balanced contributions and consistency, we offer several axiomatizations of the consistent value. Finally, we propose a dynamic process to illustrate that the consistent value can be reached by players who start from an arbitrary efficient payoff vector and make successive adjustments.	fuzzy logic	Yan-An Hwang;Yu-Hsien Liao	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.10.003	bondareva–shapley theorem;combinatorial game theory;game theory;example of a game without a value;potential;information processing;computer science;artificial intelligence;machine learning;repeated game;mathematics;stochastic game;shapley value;fuzzy set;mathematical economics;consistency;sequential game;symmetric game;fuzzy control system	AI	-0.9192598534235193	-19.4274644773793	125288
7447f11875a35e82f528524c1627c60b6dd2dfd0	type-2 aggregation operators	aggregation operatorfuzzy truth valuestype 2 fuzzy setstype 2 aggregation operatorn dimensional fuzzy setfuzzy multiset	The paper deals with an extension of aggregation operators from the set of real numbers (or interval [0, 1]) to the set of fuzzy truth values (fuzzy sets in [0, 1]). We define so-called type-2 aggregation operator and show that an extension of ordinary aggregation operator by convolution is a type-2 aggregation operator. Finally we show that ordinary aggregation operator, as well as aggregation operator for intervals and for n-dimensional intervals are special cases of our type-2 aggregation operator.	aggregate data;approximation algorithm;computation;convolution;formal system;fuzzy logic;fuzzy set;online aggregation;operator overloading;type-2 fuzzy sets and systems	Zdenko Takác	2013		10.2991/eusflat.2013.24	combinatorics;discrete mathematics;algorithm	DB	-0.5798491315070506	-22.48864911621806	125312
06c367e47d7ba083f8202b439695ce8369e69e75	fuzzy dominance: a new approach for ranking fuzzy variables via credibility measure	fuzzy dominance;fuzzy theory;credibility theory;fuzzy variable;journal;triangular fuzzy variable;credibility measure	Comparison of fuzzy variables is considered one of the most important topics in fuzzy theory. A new approach for ranking fuzzy variable via credibility measure — fuzzy dominance is presented in this paper. Some basic properties of fuzzy dominance are investigated. As an illustration, the cases of fuzzy dominance rule for triangular fuzzy variables are examined.		Jin Peng;Qin Jiang;Congjun Rao	2007	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488507004583	fuzzy logic;discrete mathematics;credibility theory;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;statistics	Robotics	-2.359594248400455	-20.56125480172593	125346
c1d27edcc710ae757ec5fa1b2b50f442410ba34e	rejoinder - causes and implications of some bidders not conforming to the sealed-bid abstraction	structural model;measurement and inference;online auction;econometrics;empirical io methods;auctions	T paper presents the authors’ rejoinder to Zeithammer and Adams [Zeithammer, R., C. Adams. 2010. The sealed-bid abstraction in online auctions. Marketing Sci. 29(6) 964–987]. This rejoinder clarifies and qualifies conclusions of the original paper and makes suggestions for fruitful areas of future research. In particular, the original paper shows that bidding style can make a big difference in managerial decisions, but a structural model would be necessary to make confident predictions under different reserve prices. The rejoinder also clarifies the interpretation of feedback as a measure of bidder experience, and the relationship between bidder experience and bidding style.	anomaly detection at multiple scales	Robert Zeithammer;Christopher Adams	2010	Marketing Science	10.1287/mksc.1100.0597	economics;marketing;operations management;common value auction;statistics;commerce	AI	-2.351727184086231	-12.44540799267772	125385
d5c35ff79e213087b10096f768ca077adccac317	a measure of perceived performance to assess resource allocation	classificacio ams 94 information and communication circuits 94d05 fuzzy sets and logic;performance evaluation;info eu repo semantics article;programacio matematica;arees tematiques de la upc matematiques i estadistica matematica aplicada a les ciencies;classificacio ams 90 operations research mathematical programming 90c mathematical programming;fuzzy operator;arees tematiques de la upc matematiques i estadistica investigacio operativa optimitzacio;informacio teoria de la;reasoning under uncertainty;programming mathematics;info eu repo semantics submittedversion;article;information theory;similarity index	Performance measurement is a key issue when a company is designing new strategies to improve resource allocation. This paper offers a new methodology inspired by classic importance-performance analysis that provides a global index of importance versus performance for firms. This index compares two rankings of the same set of features regarding importance and performance, taking into account under-performing features. The marginal contribution of each feature to the proposed global index defines a set of iso-curves that represents an improvement in the importance-performance analysis diagram. The defined index, together with the new version of the diagram, will enable the assessment of a firm’s overall performance and therefore enhance decision making in the allocation of resources. The proposed methodology has been applied to a Taiwanese multi-format retailer and managerial perceptions of performance and importance are compared to assess the firm’s overall performance.		Josep M. Sayeras;Núria Agell;Xari Rovira;Mónica Sánchez;John A. Dawson	2016	Soft Comput.	10.1007/s00500-015-1696-3	mathematical optimization;information theory;computer science;artificial intelligence;mathematics;operations research;statistics	Web+IR	-2.692361388463527	-16.49607873157415	125463
c03ff784c47197a460f0aee3352a4a20d6fddddc	neuro fuzzy modelling for prediction of consumer price index		Economic indicators such as Consumer Price Index (CPI) have frequently used in predicting future economic wealth for financial policy makers of respective country. Most central banks, on guidelines of research studies, have recently adopted an inflation targeting monetary policy regime, which accounts for high requirement for effective prediction model of consumer price index. However, prediction accuracy by numerous studies is still low, which raises a need for improvement. This manuscript presents findings of study that use neuro fuzzy technique to design a machine-learning model that train and test data to predict a univariate time series CPI. The study establishes a matrix of monthly CPI data from secondary data source of Tanzania National Bureau of Statistics from January 2000 to December 2015 as case study and thereafter conducted simulation experiments on MATLAB whereby ninety five percent (95%) of data used to train the model and five percent (5%) for testing. Furthermore, the study use root mean square error (RMSE) and mean absolute percentage error (MAPE) as error metrics for model evaluation. The results show that the neuro fuzzy model have an architecture of 5:74:1 with Gaussian membership functions (2, 2, 2, 2, 2), provides RMSE of 0.44886 and MAPE 0.23384, which is far better compared to existing research studies.	approximation error;artificial intelligence;compiler;experiment;matlab;machine learning;mean squared error;neuro-fuzzy;residual (numerical analysis);simulation;test data;time series	Godwin Ambukege;Godfrey Justo;Joseph Mushi	2017	CoRR	10.5121/ijaia.2017.8503	neuro-fuzzy;machine learning;architecture;consumer price index;artificial intelligence;inflation targeting;computer science;mean absolute percentage error;economic indicator;test data;mean squared error	HCI	8.118044431437976	-18.041086712215435	125660
0662bd368a7516a7521eaf1c2dfa4926e1996c39	generalized atanassov's operators defined on lattice fuzzy multisets	aggregation function;restricted equivalence function;interval valued lattice fuzzy set;qowa operator;lattice multiset	In this paper the concept of an ordered weighted quasi-average operator (QOWA) on the one hand and that of an n-dimensional Atanassov’s operator on the other, are extended from fuzzy multisets on [0, 1] to fuzzy multisets on any complete lattice endowed with a t-norm and a t-conorm. We show that, in the case of a distributive lattice, both operators provide particular cases of OWA operators defined on the lattice. In addition, a class of operators including Atanassov’s ones is defined on fuzzy lattice multisets. This new approach allows us to build a kind of n-ary aggregation functions for complete lattices which generalizes OWA operators.	aggregate function;t-norm;tensor operator	Inmaculada Lizasoain;Gustavo Ochoa	2014	Inf. Sci.	10.1016/j.ins.2014.03.061	combinatorics;mathematical analysis;discrete mathematics;mathematics;map of lattices	DB	-0.9067892262521774	-23.029370406548477	125877
e41fefdd72b911e82d8faa51a38ca790297f3583	risk assessment for build-operate-transfer projects: a dynamic multi-objective programming approach	analyse risque;dynamic programming;multiobjective programming;programmation multiobjectif;programacion dinamica;imulti objective programming;utilite conversationnelle;risk analysis;interactive utility;dynamic program;iterative algorithm;analisis riesgo;multi objective programming;taiwan;asie;mathematical programming;modele utilite dependente;programmation dynamique;model development;risk assessment;risk measure;programmation mathematique;article;programacion matematica;evaluation risque;asia;build operate transfer;bot;programacion multiobjetivo	This study uses a dynamic multi-objective programming approach to establish a risk assessment model, and develops an iterative algorithm for the model solution. The results obtained show that the sum of the interactive utility value could determine whether or not the interactive relationship is characterized by independence among negotiators. In addition, our numerical example shows that the risk measurement model developed can re4ect risk assessment made by the negotiation group for certain events, and can analyze interaction characteristics among negotiators. ? 2003 Elsevier Ltd. All rights reserved.	algorithm;interactive proof system;iterative method;numerical analysis;risk assessment	Chao-Chung Kang;Cheng-Min Feng;Haider A. Khan	2005	Computers & OR	10.1016/j.cor.2003.11.026	risk assessment;mathematical optimization;simulation;risk analysis;artificial intelligence;dynamic programming;mathematics;iterative method	AI	-0.17645648132693784	-16.06953590352259	126017
6fef8ef338f1887097131e9588d18aec71260d67	cell formation using fuzzy relational clustering algorithm	computer aided analysis;cluster algorithm;frc algorithm;mixed variable data;group technology;cellular manufacturing system;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;dissimilarity matrix;fuzzy relation;modelo matematico;algorithme;identificacion sistema;data clustering;algorithm;system identification;cellular manufacturing systems;cell formation;mathematical model;analisis asistido;applied mathematics;manufacturing system;identification systeme;cellular manufacturing;algoritmo	Cellular manufacturing is a useful way to improve overall manufacturing performance. Group technology is used to increase the productivity for manufacturing high quality products and improving the flexibility of manufacturing systems. Cell formation is an important step in group technology. It is used in designing good cellular manufacturing systems. The key step in designing any cellular manufacturing system is the identification of part families and machine groups for the creation of cells that uses the similarities between parts in relation to the machines in their manufacture. There are two basic procedures for cell formation in group technology. One is part-family formation and the other is machine–cell formation. In this paper, we apply a fuzzy relational data clustering algorithm to form part families and machine groups. A real data study shows that the proposed approach performs well based on the grouping efficiency proposed by Chandrasekharan and Rajagopalan. © 2011 Elsevier Ltd. All rights reserved.	algorithm;cell signaling;cluster analysis;display resolution;frame rate control;fuzzy logic;level of measurement;national supercomputer centre in sweden;xslt/muenchian grouping;yang	Wen-Liang Hung;Miin-Shen Yang;E. Stanley Lee	2011	Mathematical and Computer Modelling	10.1016/j.mcm.2010.12.056	system identification;mathematical model;mathematics;cluster analysis;algorithm	Robotics	0.23670725871306272	-15.861291631084514	126058
9a4111436c40201323a62bd8e0cdc95758fc9eae	a hybrid arima-denfis method for wind speed forecasting	denfis wind speed forecasting arima;fuzzy reasoning;autoregressive moving average processes;wind power;wind power autoregressive moving average processes forecasting theory fuzzy reasoning;forecasting mathematical model wind speed predictive models time series analysis data models training data;forecasting theory;error measures hybrid arima denfis method wind speed forecasting autoregressive integrated moving average dynamic evolving neural fuzzy inference system ndbc wind speed data	This paper proposes a hybrid autoregressive integrated moving average - dynamic evolving neural-fuzzy inference system (ARIMA-DENFIS) model for wind speed forecasting. The theory of ARIMA, DENFIS and the hybrid of the two are discussed. The proposed model is evaluated with NDBC wind speed data and the results show that the proposed hybrid ARIMA-DENFIS model outperforms DENFIS model in most of the cases. It has comparable or better error measures than ARIMA model. In addition, when the forecasting horizon increases, the advantage of the proposed ARIMA-DENFIS model becomes more significant.	autoregressive integrated moving average;autoregressive model;inference engine;mean squared error	Ren Ye;Ponnuthurai Nagaratnam Suganthan;Narasimalu Srikanth;Soham Sarkar	2013	2013 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2013.6622503	wind power;econometrics;autoregressive integrated moving average;simulation;machine learning	Robotics	8.417703820748681	-19.407792590102385	126179
1a56e39d6d67919a82df99e8360e95efbf85e2b6	mining television audience rating based on fuzzy cognitive map	prediction television playback volume fuzzy cognitive map fuzzy logic neural network nonlinear system dynamic data linear regression algorithm coarse weight matrix model training weight algorithm television audience rating mining;regression analysis cognitive systems data mining fuzzy reasoning fuzzy set theory;data mining;television audience rating;fuzzy cognitive map;tv satellites satellite broadcasting data models films training ethics;data mining fuzzy cognitive map television audience rating	The Fuzzy Cognitive Map (FCM) is the product of the combination of fuzzy logic and neural network, and is suitable for the description, prediction and control of the nonlinear system based on dynamic data. The linear regression algorithm is used to obtain the coarse weight matrix model of fuzzy cognitive map, and then a training weight algorithm is used to refine the coarse weight matrix model. The fuzzy cognitive map model is applied to mine the television audience rating, realizing and prediction television playback volume. The experimental result shows that the modeling method is effective.	algorithm;artificial neural network;dynamic data;fuzzy cognitive map;fuzzy logic;nonlinear system	Nan Ma;Hao Wang;Qin He;Tao Yao	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381921	fuzzy cognitive map;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining	Robotics	5.086919529762456	-23.14101884116573	126377
c990238aeb2e4fd1e9f8d5338dd21a84a42dcdcb	extending concordance and discordance relations to hierarchical sets of criteria in electre-iii method	hierarchical criteria;multiple criteria decision making;partial preorder;outranking method	In many real-world multiple criteria decision problems, the family of criteria has a hierarchical structure presented in the form of a tree. The leaves of the tree correspond to elementary criteria on which a finite set of alternatives is directly evaluated. Evaluations of alternatives on elementary criteria are aggregated to form a sub-criterion at an upper level of the tree. Then, evaluations of alternatives on sub-criteria having the same predecessor in the hierarchy tree are aggregated again in a sub-criterion of a higher level, and so on, until the aggregation at the general goal criterion, which is the root of the tree, where the alternatives are finally ranked from the best to the worst. At each node of the tree, above the leaves, we are considering the aggregation of multiple criteria evaluations using the ELECTRE-III method, based on building and exploiting outranking relations for each pair of alternatives. As the result of ELECTRE-III is a partial preorder of alternatives, the sub-criteria are ordering the alternatives just partially. Therefore, in this paper we propose a new way of calculating concordance and discordance indices that take part in the definition of an outranking relation aggregating the (partially ordered) evaluations on sub-criteria. A robustness analysis has been performed to analyze the behavior of the proposed method in different settings.	concordance (publishing)	Luis Del Vasto Terrientes;Aïda Valls;Roman Slowinski;Piotr Zielniewicz	2012		10.1007/978-3-642-34620-0_9	mathematical optimization;combinatorics;discrete mathematics;mathematics	NLP	-3.496484639004417	-19.974264372025306	126397
a2e07f968d2dddaf2a43323f2a7583727721cb0a	portfolio optimization with disutility-based risk measure	mean risk model for portfolio optimization;utility functions;quantile based risk measures	In this paper we propose a quantile-based risk measure which is defined using the modified loss distribution according to the decision maker’s risk and loss aversion. The properties related to different classes of disutility functions are established. A portfolio selection model in the Mean-Risk framework is proposed and equivalent formulations of the model generating the same efficient frontier are given. The advantages of this approach are investigated using real world data from NYSE. The differences between the efficient frontier of the proposed model and the classical Mean-Variance and Mean-CVaR are quantified and interpreted. Extensive experiments show that the efficient portfolios obtained by using the proposed model exhibit lower risk levels and an increased satisfaction compared to the other two Mean-Risk models. © 2015 Elsevier B.V. and Association of European Operational Research Societies (EURO) within the International Federation of Operational Research Societies (IFORS). All rights reserved.	cvar;experiment;international federation of operational research societies;mathematical optimization;mean squared error;operations research;risk aversion;risk measure;utility	Cristinca Fulga	2016	European Journal of Operational Research	10.1016/j.ejor.2015.11.012	financial economics;efficient frontier;actuarial science;economics;expected shortfall;portfolio optimization;spectral risk measure;dynamic risk measure;distortion risk measure;welfare economics;coherent risk measure	AI	2.1186957418267367	-11.033229338487219	126502
27d9f8106a7552b48a131a51628fbdae180ef0e3	lattice representations of interval-valued fuzzy sets	cuts;lattice valued fuzzy sets;interval valued fuzzy sets	In this paper, a solution of the problem of the interval-valued fuzzy sets synthesis, is given. The codomain is considered to be a lattice of intervals. The necessary and sufficient conditions for a family of subsets to be a family of cut sets of an interval-valued fuzzy set are given. © 2013 Elsevier B.V. All rights reserved.	crystal structure;embedded system;fuzzy set	Marijana Gorjanac-Ranitovic;Aleksandar Petojevic	2014	Fuzzy Sets and Systems	10.1016/j.fss.2013.07.006	fuzzy logic;combinatorics;mathematical analysis;discrete mathematics;membership function;family of sets;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;disjoint sets;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	AI	-0.03617182077923215	-22.70701511740555	126543
1882365ece486155313996109ca6f958f6d1a8b7	causal modeling alternatives in operations research: overview and application	bayesian network;sample size;tqm;delivery performance;structural equation modeling;structural equation model;operations research;model complexity;total quality management;manufacturing;causal modeling;search model;process analysis;hb economic theory kozgazdasagtudomany;causal models;bayesian networks	This paper uses the relationships between three basic, fundamental and proven concepts in manufacturing (resource commitment to improvement programs, flexibility to changes in operations, and customer delivery performance) as the empirical context for reviewing and comparing two casual modeling approaches (structural equation modeling and Bayesian networks). Specifically, investments in total quality management (TQM), process analysis, and employee participation programs are considered as resource commitments . The paper begins with the central issue of the requirements for a model of associations to be considered causal . This philosophical issue is addressed in reference to probabilistic causation theory. Then, each method is reviewed in the context of a unified causal modeling framework consistent with probabilistic causation theory and applied to a common dataset . The comparisons include concept representation, distribution and functional assumptions, sample size and model complexity considerations, measurement issues, specification search, model adequacy, theory testing and inference capabilities . The paper concludes with a summary of relative advantages and disadvantages of the methods and highlights the findings relevant to the literature on TQM and on-time deliveries . © 2003 Elsevier B .V. All rights reserved.	bayesian network;causal filter;causality;operations research;probabilistic causation;requirement;structural equation modeling	Ronald D. Anderson;Gyula Vastag	2004	European Journal of Operational Research	10.1016/S0377-2217(02)00904-9	structural equation modeling;econometrics;mathematical optimization;total quality management;computer science;operations management;bayesian network;mathematics;management science;statistics;causal model	AI	-1.1193709020378584	-13.098376111792568	126653
fac11ad82dfbdeddba407b1f0872fb92fe734703	calibration of traffic dynamics models with data mining	traffic engineering computing automated highways calibration data mining digital simulation road traffic;traffic simulation;mesoscopic traffic simulator;neural networks;road traffic;k means;dynamic model;automated highways;traffic control;parametric calibration precision traffic dynamics model calibration data mining speed density relationship mesoscopic traffic simulator;traffic flow;joints;data mining;speed density relationship;artificial neural networks;agglomerative hierarchical clustering;artificial neural networks joints;traffic engineering computing;calibration;traffic dynamics model calibration;parametric calibration precision;digital simulation	Speed-density relationships are one of models used by a mesoscopic traffic simulator to represent traffic dynamics. While the classical speed-density relationships provide a useful insight into the traffic dynamics problem and have theoretical value to traffic flow, for such applications they are limited This paper focuses on calibrating parameters for the speed-density relationships by using data mining methods such as locally weighted regression, k -means, k -nearest neighborhood classification and agglomerative hierarchical clustering. Meanwhile, in order to improve the precision of the parametric calibration, we also utilize densities and flows as variables to calibrate parameters. The proposed approach is tested with sensor data from the 3rd ring road in Beijing. The test results show that the proposed algorithm has great performance on the parametric calibration of the speed-density relationships.	algorithm;cluster analysis;data mining;hierarchical clustering;mesoscopic physics	Zhu Jiang;Yan Zhang;YongXuan Huang;JiSheng Li	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633861	calibration;simulation;computer science;machine learning;traffic flow;data mining;artificial neural network;k-means clustering	ML	9.463513457000104	-20.554118897528923	126661
100d40a3c051c64bfb347b3dd651459f081d0a0e	risk analysis of water resource based on possibility-probability distribution	interior outer set model risk analysis water resource possibility probability distribution fuzzy risk assessment fuzzy expected value crisp value fuzzy set crisp set multi valued risk set valued risk;finite element methods;water resource;model selection;fuzzy set;water resource risk;fuzzy expected value;crisp set;crisp value;risk analysis;probability density function;fuzzy risk assessment;water resources;biological system modeling;data mining;risk analysis water resources fuzzy sets fuzzy systems geography risk management equations probability distribution arithmetic performance analysis;fuzzy set theory;possibility probability distribution;fuzzy risk;statistical distributions;expected value;set valued risk;risk preference;probability distribution;interior outer set model;risk assessment;cities and towns;a level;water resource risk interior outer set model a level fuzzy risk fuzzy expected value;multi valued risk;water resources fuzzy set theory risk analysis statistical distributions	To perform fuzzy risk assessment, the simplest way is to calculate the fuzzy expected value and convert fuzzy risk into non-fuzzy risk, i. e. , a crisp value. In doing so, there is a transition from a fuzzy set to crisp set. Therefore, the fuzzy risk can be a multi-valued risk or set-valued risk. Calculation of the fuzzy expected value of Yiwu city’s water resource risk has been performed based on the interior-outer set model. Selection of an α value depends on the confidence in different groups of people, while selection of a conservative risk value or venture risk value depends on the risk preference of these people.	fuzzy set;risk assessment	Lihua Feng	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.36	probability distribution;mathematical optimization;discrete mathematics;defuzzification;expected shortfall;entropic value at risk;value of information;data mining;mathematics;dynamic risk measure;fuzzy set;time consistency;statistics	Robotics	0.882538829244842	-12.387507964639779	126725
a80a4a51d07b65fd292c91d3be3ca9db8c9f2421	estimating the technical efficiency of health care systems: a cross-country comparison using the directional distance function	directional distance function;data envelopment analysis;or in health services;health systems;undesirable outputs	Economic activity produces not only desirable outputs but also undesirable outputs. Undesirable outputs are usually omitted from efficiency assessments (i.e., applications of Data Envelopment Analysis) which fail to express the true production process. The directional distance function model has been used for handling asymmetrically both desirable and undesirable outputs in the assessment process. In the present paper, we apply a generalized directional distance function to measure the efficiency of the health systems of 171 countries. We incorporate both desirable and undesirable outputs into the efficiency assessment without transforming the latter type of outputs into inputs or into their inverse form, as is done in most of the extant studies that deal with the measurement of health efficiency. The methodology that we apply introduces a modified definition of the efficiency score which yields results consistent with those obtained from radial DEA models. In addition, our results are independent of the length of the direction vector.	algorithmic efficiency;consistency model;data envelopment analysis;function model;radial (radio)	Gang Cheng;Panagiotis D. Zervopoulos	2014	European Journal of Operational Research	10.1016/j.ejor.2014.05.007	econometrics;computer science;data envelopment analysis;mathematics;welfare economics;statistics	ML	-1.7480286231968303	-13.723313543219136	126980
8c09ab9b4ecda8e1ddc1e44637dcf949baf9c206	semantics of quotient operators in fuzzy relational databases	fuzzy relational database;fuzzy relation;relational database;query evaluation;fuzzy database	"""The quotient operator plays an important role for query evaluation in relational databases. Its extension to fuzzy databases raises the question of the intended interpretation of the intermediary degrees attached to the items in the fuzzy relations. This note points out that the fuzzy quotient operator should be defined in slightly different ways depending on the possible interpretations of the degrees: levels of satisfaction of a gradual property, levels of importance of a required property, uncertainty pertaining to the membership of an element to a subset. (*) This short paper is a revised and slightly expanded version of a note entitled """"Quotient operators in fuzzy relational databases"""" presented at the Second European Congress on Intelligent Techniques and Soft Computing (EUFIT'94), Aachen, Germany, September 20-24, 1994 and which appears pp. 357-360 in the Proceedings."""	interpretation (logic);relational database;soft computing	Didier Dubois;Henri Prade	1996	Fuzzy Sets and Systems	10.1016/0165-0114(95)00117-4	fuzzy logic;discrete mathematics;relational model;relational calculus;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;relational database;fuzzy classification;computer science;fuzzy number;fuzzy measure theory;data mining;database;mathematics;fuzzy set;fuzzy associative matrix;conjunctive query;fuzzy set operations	DB	-2.191896014630116	-23.785845334531178	127017
89df9440efdf7e0f3c82080d4d4261897c52425c	a multi-objective evolution algorithm based oil field stimulation measure programming	decision support;geologic measurements optimization programming biological system modeling biological neural networks predictive models prediction algorithms;pareto optimisation;evolutionary computation;neural nets;oil drilling;pareto optimal;multi objective evolution algorithm;cost reduction;biological system modeling;multi objective optimization;prediction algorithms;wavelet neural network;profitability cost reduction evolutionary computation neural nets oil drilling pareto optimisation;wavelet neural network wnn;programming model;geologic measurements;economic profit multiobjective evolution algorithm oil field stimulation measure programming mature oil field economical profit ex ante wavelet neural network ex post wavelet neural network oil well production oil block production pareto optimal solution aggregated fitness measures cost reduction;predictive models;optimization;profitability;prediction model;measure programming;pareto optimal solution;programming;pareto optimality;biological neural networks;pareto optimal measure programming multi objective evolution algorithm wavelet neural network wnn;neural network	A multi-objective evolution algorithm based oil field stimulation measure programming is presented in this paper. Stimulations are very important measure for mature oil field to maintain stable oil yield. Stimulation measure programming can reduce cost and increase economical profit. Ex-ante and ex-post wavelet neural network models for oil well or block production was constructed first. Then predict models based stimulation measure programming models was constructed. These models are usually constrained multi-objective optimizations. Obtained Pareto optimal solutions using multi-objective evolution algorithm are used for ex-ante decision support and ex-post evaluation. Multi-objective evolution algorithm was used to obtain Pareto optimal set of oil field stimulation measure programming. All of oil well's stimulation serial-number is encoded into an integer array as chromosome. Population consists of feasible chromosomes. Then two aggregated fitness measures are used to evaluate each individual's fitness, one is based on dominant count to achieve proximity, another is based on distance to maintain population's diversity.	algorithm;decision support system;evolution;fitness function;pareto efficiency;population;wavelet	Bixin Hu	2011	2011 Fifth International Conference on Genetic and Evolutionary Computing	10.1109/ICGEC.2011.80	mathematical optimization;computer science;artificial intelligence;machine learning;predictive modelling;artificial neural network	Robotics	7.742589562562656	-18.744922395800476	127112
0f7f6f2935a2cda9ad98826d2b6bfe382f98ab5e	decision support for it investment projects - a real option analysis approach based on relaxed assumptions	330 wirtschaft;black scholes model;ddc 330;assumptions;characteristics of it investment projects;real option analysis;simulation model;business value of it investment projects;it investment project decisions	Managerial flexibilities have to be taken into account in ex-ante decision-making on IT investment projects (ITIPs). In many papers of the IS literature, standard financial option pricing models are used to value such managerial flexibilities. Based on a review of the related literature, the paper critically discusses the assumptions of the most frequently used financial option pricing model, namely the Black–Scholes model, arguing for relaxed assumptions that better represent the characteristics of ITIPs. The authors find that existing real option analysis approaches featured in the IS, Finance, and Economics literature are unable to consider more than two of our relaxed assumptions. Consequently, they present their own approach in form of a simulation model for the valuation of real options in ITIPs which offers a better representation of the characteristics of ITIPs by taking the discounted cashflows and the runtime to be uncertain as well as the market to be incomplete. Based on these modifications of the Black–Scholes model’s assumptions, it is found that the resulting option value contains idiosyncratic risk that has to be taken into account in ITIP decision making. For the realistic case of risk averse decision makers, the consideration of idiosyncratic risk usually leads to a lower riskadjusted option value, compared to one calculated by means of the Black–Scholes model. This confirms the perception of managers who feel that financial option pricing models frequently overvalue ITIPs and hence may induce flawed investment decisions.	backdrop cms;black–scholes model;markov chain;openbsm;programming paradigm;relevance;resource-oriented architecture;risk aversion;simulation;theory;value (ethics);word lists by frequency	Marcel Philipp Müller;Sebastian Stöckl;Steffen Zimmermann;Bernd Heinrich	2016	Business & Information Systems Engineering	10.1007/s12599-016-0423-7	financial economics;datar–mathews method for real option valuation;actuarial science;economics;marketing;asian option;black–scholes model;simulation modeling;finance;management	AI	-3.294683170497176	-12.1612019856186	127298
5fb832e9cf4c9ba7a3fbe5876c113af34725606f	the financial structure and high-tech industries development in china		The transformation of economic development approach demands to develop high-tech industries, while high-tech industries can’t develop without financial support, in which different financial structures exists differences in the efficiency of high-tech industries development and support. For this reason, this paper firstly theoretically described the mechanism of financial structure’s effect on high-tech industries, and then used principal component analysis with 1990 to 2007 economic data which constructed two financial structure factors, and places them in the same regression model with the rate of high-tech industries. It found that the relationships of both two factors and the rate of high-tech industries are positive, also for the one-way Granger-causality, but the total factor for one-way causality has larger impact relative to the structure factor. So this paper suggested that it’s necessary to focus on the total accumulation of financial assets and make full use of resources allocation and the positive role of high-tech industries in order to develop China’s high-tech industries.	causality;one-way function;principal component analysis;tree accumulation	Liuyong Yang;Shensheng Mo;Anqi Zhou	2010	JDCTA		geography of finance;computer science;financial intermediary;financial system	HCI	2.8309584522733178	-14.268621516174129	127311
44e992e56fda1b8eddad9c2bba1ff65cf948c4de	a new algorithm based on the continuous ordered weighted geometric operator	eigenvalues and eigenfunctions;sensitivity analysis geometry mathematical operators matrix algebra;geometry;sensitivity analysis interval reciprocal comparison matrices a new algorithm the c owg operator;matrix algebra;data mining;the c owg operator;mathematical operators;interval reciprocal comparison matrices;attitudinal character continuous ordered weighted geometric operator interval reciprocal comparison matrices sensitivity analysis;a new algorithm;matrix decomposition;sensitivity analysis;chromium;attitudinal character;chromium decision making matrix decomposition sensitivity analysis fuzzy systems mathematics information science concrete delta modulation linear programming;europe;continuous ordered weighted geometric operator	Based on the continuous ordered weighted geometric (C-OWG) operator, a new algorithm is proposed to derive the weights from interval reciprocal comparison matrices. The obtained weights are dependent on the corresponding expected reciprocal comparison matrix associated with an attitudinal character. In order to make the weights be reliable, a method of making the expected reciprocal comparison matrices own consistency or acceptable consistency is given. Numerical results are further calculated to show the feasibility of the new algorithm and compare with other known procedure. A sensitivity analysis with respect to the attitudinal character is made and shown graphically.	algorithm;null character;numerical analysis	Fang Liu	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.317	mathematical optimization;combinatorics;chromium;discrete mathematics;data mining;mathematics;matrix decomposition;sensitivity analysis	Robotics	-2.3684814243349352	-20.29464711055211	127376
9d0c16e049a02fb14e16de44637838559aa74ab5	decision-making in management information systems research: the utility of policy capturing methodology	eleccion;entreprise;development;comportement individuel;employe;empresa;desarrollo;methode;prise decision;systeme information gestion;scenario;comportamiento individual;argumento;developpement;firm;ejemplo;employee;policy capturing;choix;toma decision;management information system;metodo;example;empleado;method;individual behavior;choice;exemple	Abstract Although decision-making represents a fundamental issue in management information systems (MIS), obtaining accurate assessments of the factors affecting employees' decisions may be difficult using traditional methods such as ratings and rankings. Policy capturing, a little-used method in MIS, represents a potentially important alternative to more traditional methods. After demonstrating that policy capturing has been underutilized in MIS, the paper illustrates the use of policy capturing in two decision-making contexts—computer training and software selection. These two studies contrast policy capturing results with more traditional methods, and draw implications for research.	information systems research;management information system;systems theory	Joseph J. Martocchio;Jane Webster;Charles R. Baker	1993	Behaviour & IT	10.1080/01449299308924386	psychology;method;computer science;scenario;management information systems;management;social psychology;operations research	AI	-1.3140456136529663	-14.610677324869048	127489
34c12242d5b25ea48a79bc53d80bc31c27875c1e	application of mixture of experts to construct real estate appraisal models	real estate;real estate appraisal;mixture of experts;statistical test;nonparametric test;multilayer perceptron;statistical tests;general linear model;matlab	Several experiments were conducted in order to investigate the usefulness of mixture of experts (ME) approach to an online internet system assisting in real estate appraisal. All experiments were performed using 28 realworld datasets composed of data taken from a cadastral system and GIS data derived from a cadastral map. The analysis of the results was performed using recently proposed statistical methodology including nonparametric tests followed by post-hoc procedures designed especially for multiple 1×n and n×n comparisons. GLM (general linear model) architectures of mixture of experts achieved better results for ME with an adaptive variance parameter for each expert, whereas MLP (multilayer perceptron) architectures for standard mixtures of experts.	experiment;general linear model;generalized linear model;geographic information system;hoc (programming language);memory-level parallelism;multilayer perceptron	Magdalena Graczyk;Tadeusz Lasota;Zbigniew Telec;Bogdan Trawinski	2010		10.1007/978-3-642-13769-3_71	statistical hypothesis testing;artificial intelligence;machine learning;data mining;statistics	ML	7.537742429060708	-23.948021116176946	127543
f66d4ebdba18c3a2138fc10d80c2a40000cd8b6c	comparing minimax and product in a variety of games	technical report	This paper describes comparisons of the minimax backup rule and the product back-up rule on a wide variety of games, including P-games, G-games, three-hole kalah, Othello, and Ballard’s incremental game. In three-hole kalah, the product rule plays better than a minimax search to the same depth. This is a remarkable result, since it is the first widely known game in which product has been found to yield better play than minimax. Furthermore, the relative performance of minimsx and product is related to a parameter called the rate of heuristic flaw (rhf). Thus, rhf has potential use in predicting when to use a back-up rule other than	backup;flaw hypothesis methodology;heuristic;minimax	Ping-Ching Chi;Dana S. Nau	1987			minimax;mathematical optimization;example of a game without a value;expectiminimax tree;computer science;artificial intelligence;technical report;machine learning;negamax;statistics	AI	-4.231504314641766	-10.416473629042695	127552
affed3470ff4ec9da575288f7c6b0ea6568a5e81	comparative analysis of machine learning algorithms to urban traffic prediction		Machine learning is currently a hot research topic and applied in intelligence transportation system to discover new valuable knowledge and patterns. In this paper, we extract trajectory information from popular traffic simulator and apply it into four different machine learning methods. In the case of the Gangnam district in Seoul, the Gradient Boosting Regression has better fit with lower values of RMSE (Root Mean Square Error).	algorithm;gradient boosting;machine learning;mean squared error	Yong-Ju Lee;Ok-Gee Min	2017	2017 International Conference on Information and Communication Technology Convergence (ICTC)	10.1109/ICTC.2017.8190846	boosting (machine learning);gradient boosting;machine learning;algorithm design;artificial intelligence;mean squared error;computer science	ML	9.392682089223932	-20.1927266160872	127637
62ee099a1fa2c931d788dd35b8f55461e9f8adaa	deriving the priority weights from incomplete hesitant fuzzy preference relations in group decision making	incomplete hesitant fuzzy preference relations;priority weights;multiplicative consistency;additive consistency;group decision making	The concept of hesitant fuzzy preference relation (HFPR) has been recently introduced to allow the decision makers (DMs) to provide several possible preference values over two alternatives. This paper introduces a new type of fuzzy preference structure, called incomplete HFPRs, to describe hesitant and incomplete evaluation information in the group decision making (GDM) process. Furthermore, we define the concept of multiplicative consistency incomplete HFPR and additive consistency incomplete HFPR, and then propose two goal programming models to derive the priority weights from an incomplete HFPR based on multiplicative consistency and additive consistency respectively. These two goal programming models are also extended to obtain the collective priority vector of several incomplete HFPRs. Finally, a numerical example and a practical application in strategy initiatives are provided to illustrate the validity and applicability of the proposed models.		Yejun Xu;Lei Chen;Rosa M. Rodríguez;Francisco Herrera;Huimin Wang	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.01.047	group decision-making;artificial intelligence;data mining	DB	-3.0856494655111035	-19.924829071858102	127781
85085dd99d1945e7dd438dbc59bb19104033ecf0	an intelligent analysis for rural settlement distribution based on gaussian mixture models: a case study of kengzi village		In light of the booming rural construction industry in China, the recurring problem of the “village sameness” reflects the inadequate understanding of inherent characteristics of traditional settlements. To discern the internal logic of historic rural settlements from historical spatial data, this study proposed a machine learning method based on the Gaussian mixture model (GMM) to examine the settlement distribution sensitivity for every geographical variable, and to perform a multivariate regression analysis on the nonlinear non-monotonic relationship between variables and the land usage development potential. In accordance with the abstracted spatial rules, the model was also used for predicting the spatial trends to support regional planning activities.	general linear model;google map maker;machine learning;mixture model;nonlinear system	Xi Yang;Fuan Pu;Guiming Luo	2018	2018 IEEE 17th International Conference on Cognitive Informatics & Cognitive Computing (ICCI*CC)	10.1109/ICCI-CC.2018.8482033	geography;human settlement;mixture model;econometrics;spatial analysis;multivariate statistics;regional planning;rural settlement	Robotics	6.71013122760204	-17.05808907189482	127851
29b0d95131adc61a0c94527b03349d029e2a110a	an event detection framework for virtual observation system: anomaly identification for an acme land simulation		Based on previous work on in-situ data transfer infrastructure and compiler-based software analysis, we have designed a virtual observation system for real time computer simulations. This paper presents an event detection framework for a virtual observation system. By using signal processing and detection approaches to the memory-based data streams, this framework can be reconfigured to capture high-frequency events and low-frequency events. These approaches used in the framework can dramatically reduce the data transfer needed for in-situ data analysis (between distributed computing nodes or between the CPU/GPU nodes). In the paper, we also use a terrestrial ecosystem system simulation within the Earth System Model to demonstrate the practical values of this effort.	algorithm;anomaly detection;central processing unit;climate model;compiler;computer simulation;data-flow analysis;dataflow;decimation (signal processing);digital signal processing;distributed computing;earth system science;graphics processing unit;machine learning;observable;signal processing;terrestrial television;x.690	Zhuo Yao;Dali Wang;Yifan Wang;Fengming Yuan	2018		10.1007/978-3-319-93701-4_4	software analysis pattern;distributed computing;earth system science;data stream mining;data transmission;signal processing;central processing unit;computer science	HPC	7.2285205801510095	-12.095179560126095	127927
27cf70518c0ca950325c3015a9df4075e002e553	machine learning for stock selection	competitive learning;risk adjustment;machine learning;stock selection;profitability	In this paper, we propose a new method called Prototype Ranking (PR) designed for the stock selection problem. PR takes into account the huge size of real-world stock data and applies a modified competitive learning technique to predict the ranks of stocks. The primary target of PR is to select the top performing stocks among many ordinary stocks. PR is designed to perform the learning and testing in a noisy stocks sample set where the top performing stocks are usually the minority. The performance of PR is evaluated by a trading simulation of the real stock data. Each week the stocks with the highest predicted ranks are chosen to construct a portfolio. In the period of 1978-2004, PR's portfolio earns a much higher average return as well as a higher risk-adjusted return than Cooper's method, which shows that the PR method leads to a clear profit improvement.	air traffic control radar beacon system;competitive learning;machine learning;prototype;selection algorithm;simulation	Robert J. Yan;Charles X. Ling	2007		10.1145/1281192.1281307	computer science;machine learning;competitive learning;profitability index	ML	5.130161507742406	-18.62648011654789	128098
36a7378f95b228b692a7684b3f919cf05e046d28	macroscopic modeling and control of reversible lanes on freeways	reversible traffic lanes;freeway management systems;traffic signal controllers;macroscopic traffic flow;automobiles;traffic control;macroscopic modeling centenario bridge spain se 30 freeway seville discrete optimization discrete model predictive control logic based controller metanet second order traffic flow model freeways reversible lanes control;mathematical model traffic control automobiles numerical models computational modeling adaptation models;computational modeling;macroscopic traffic flow modeling reversible lanes traffic control mpc metanet;mathematical model;traffic models;algorithms;road traffic discrete systems predictive control;numerical models;adaptation models;metanet computer program	This paper proposes a macroscopic model and two control algorithms for the dynamic operation of reversible lanes on freeways. The proposed model is an extension of the second-order traffic flow model METANET. The reversible lanes are modeled like variable lane drops (taking into account that the cars in the closed/opened lanes need a certain time to leave/enter the corresponding segments). Based on this model, two kinds of dynamic controllers have been developed. The first one is an easy-to-implement logic-based controller that takes into account the congestion lengths generated by the reversible lane bottleneck and uses this information for the dynamic operation of the lanes. The second one is a discrete model predictive control that minimizes the total time spent of the modeled network within some constraints for the maximum values of the generated bottleneck queues. The discrete optimization is carried out via evaluation of the cost function for all the leaves in a reduced search tree. The proposed model and control algorithms are simulated and tested using loop detector data collected over a section of the SE-30 freeway in Seville, Spain. The modeled network includes the Centenario Bridge, which is a bottleneck with a reversible lane that creates recurrent congestion during the morning rush-hour period. The results show that the proposed model is able to reproduce traffic congestion due to the reversible lanes and that all the proposed controllers (which can be computed in a short time) substantially reduce this congestion.	algorithm;discrete optimization;freeway;mathematical optimization;metanet software;netware file system;network congestion;search tree	José Ramón Domínguez Frejo;Ioannis Papamichail;Markos Papageorgiou;Eduardo F. Camacho	2016	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2015.2493127	control engineering;simulation;computer science;engineering;mathematical model;mathematics;transport engineering;computational model;algorithm;statistics	Robotics	9.914933341871583	-10.74158086147498	128109
51548c0435a05eb92d4901449e4c352dea071ed8	a grey-based decision-making approach to the supplier selection problem	calcul scientifique;multiple attribute decision making madm;computer aided analysis;selection problem;problema seleccion;linguistic variable;matematicas aplicadas;analyse assistee;modele mathematique;analisis sistema;mathematiques appliquees;grey possibility degree;etude methode;modelo matematico;prise decision;estudio metodo;decision maker;multiple attribute decision making;mathematical analysis;computacion cientifica;grey number;mathematical model;system analysis;analisis asistido;analyse systeme;method study;information system;scientific computation;applied mathematics;toma decision;systeme information;supplier selection;sistema informacion;probleme selection	Supplier selection is a multiple-attribute decision-making (MADM) problem. Since the decision makers (DMs) such as preferences on alternatives or on the attributes of suppliers are often uncertain, supplier selection becomes more difficult. Grey theory is one of the methods used to study uncertainty, being superior in the mathematical analysis of systems with uncertain information. In this paper, we propose a new grey-based approach to deal with the supplier selection problem. The work procedure is as follows: firstly, the weights and ratings of attributes for all alternatives are described by linguistic variables that can be expressed in grey numbers. Secondly, using a grey possibility degree, the ranking order of all alternatives is determined. Finally, an example of a selection problem of supplier was used to illustrate the proposed approach. c © 2006 Elsevier Ltd. All rights reserved.	decision problem;selection algorithm	Guodong Li;Daisuke Yamaguchi;Masatake Nagai	2007	Mathematical and Computer Modelling	10.1016/j.mcm.2006.11.021	decision-making;mathematical analysis;applied mathematics;artificial intelligence;mathematical model;control theory;mathematics;system analysis;operations research;information system	AI	-0.8030640725739306	-16.411779869603684	128194
124e50fefc63449fe0016f3b03fbcb936ace5ff7	evolutionary approach to calibration of cellular automaton based traffic simulation models	traffic simulation;surveillance;road traffic;microscopy;real time information;manually tuned model evolutionary calibration approach cellular automaton microscopic traffic simulation models transportation engineering transportation planning practices genetic algorithm human expert surveillance technologies loop detectors;roads;loop detectors;genetic algorithms;vehicles data models calibration roads microscopy genetic algorithms sociology;vehicles;cellular automata;calibration;sociology;data models;road traffic calibration cellular automata genetic algorithms	Microscopic traffic simulation models have become very popular in the evaluation of transportation engineering and planning practices in the past few decades. To achieve high fidelity and credibility of simulations, a model calibration and validation must be performed prior to deployment of the simulator. In this paper, we proposed an effective calibration method of the microscopic traffic simulation model. The model is based on the cellular automaton, which allows fast large-scale real-time simulation. For its calibration, we utilized a genetic algorithm which is able to optimize different parameters much better that a human expert. Furthermore, it is possible to readjust the model to given field data coming from standard surveillance technologies such as loop detectors in our case. We have shown that the precision of simulations can be increased by 20 % with respect to a manually tuned model.	cellular automaton;genetic algorithm;iterative and incremental development;real-time clock;sensor;simulation;software deployment	Pavol Korcek;Lukás Sekanina;Otto Fucík	2012	2012 15th International IEEE Conference on Intelligent Transportation Systems	10.1109/ITSC.2012.6338759	simulation;engineering;machine learning;transport engineering	Robotics	8.62061185536544	-12.241001541933688	128389
1ec955eb76fa72e295ef69c20c855a46f66549d2	forecasting of telecommunications time-series via an orthogonal least squares-based fuzzy model	forecasting;least squares approximations;orthogonal least squares method telecommunications data fuzzy modeling;time series fuzzy set theory least squares approximations telecommunication computing;forecasting models telecommunications time series orthogonal least squares based fuzzy model telecommunications data prediction ols technique second orthogonal estimator two stage sequential algorithm fuzzy rule input selection;telecommunication computing;time series;orthogonal least squares method;fuzzy set theory;telecommunications data;fuzzy modeling;smoothing methods;computational modeling;vectors;predictive models forecasting vectors computational modeling mathematical model telecommunications smoothing methods;mathematical model;predictive models;telecommunications	An application of fuzzy modeling to the problem of telecommunications data prediction is proposed in this paper. The model building process is a two-stage sequential algorithm, based on the Orthogonal Least Squares (OLS) technique. Particularly, the OLS is first employed to partition the input space and determine the number of fuzzy rules and the premise parameters. In the sequel, a second orthogonal estimator determines the input terms which should be included in the consequent part of each fuzzy rule and calculate their parameters. Input selection is automatically performed, given a large input candidate set. Real world telecommunications data are used in order to highlight the characteristics of the proposed forecaster and to provide a comparative analysis with well-established forecasting models.	fuzzy rule;nonlinear system;ordinary least squares;qualitative comparative analysis;sequential algorithm;time series	Paris A. Mastorocostas;Constantinos S. Hilas;Stergiani C. Dova;Dimitris N. Varsamis	2012	2012 IEEE International Conference on Fuzzy Systems	10.1109/FUZZ-IEEE.2012.6251254	econometrics;forecasting;computer science;fuzzy number;machine learning;time series;mathematical model;mathematics;predictive modelling;fuzzy set;computational model;fuzzy set operations;statistics	Robotics	7.5712767230772595	-21.26994936505133	128402
d702381e41df4ca208dcd0c24a40dc4757d3394c	a review of the construction of hierarchical fuzzy systems	hierarchical system;flatness;sistema experto;fuzzy set;systeme hierarchise;logique floue;base connaissance;conjunto difuso;logica difusa;ensemble flou;fuzzy logic;sistema jerarquizado;base conocimiento;sistema difuso;systeme flou;systeme expert;planeite;fuzzy system;planeidad;knowledge base;expert system	Fuzzy rule-based systems are nowadays one of the most successful applications of fuzzy sets and fuzzy logic. Most applications use a flat set of fuzzy rules. However, in complex applications with a large set of variables, it is not appropriate to define the system with a flat set of rules because, among other problems, the number of rules increases exponentially with the number of variables. Hierarchical fuzzy systems are one of the alternatives presented in the literature to overcome this problem. In this article we review the latest results related with this type of fuzzy system. © 2002 Wiley Periodicals, Inc.	fuzzy control system;fuzzy logic;fuzzy rule;fuzzy set;john d. wiley;rule-based system	Vicenç Torra	2002	Int. J. Intell. Syst.	10.1002/int.10036	fuzzy logic;knowledge base;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;hierarchical control system;fuzzy associative matrix;expert system;fuzzy set operations;algorithm;fuzzy control system;flatness	AI	2.4812389247018136	-23.432324644128173	128551
73a9627fa1a9c858224453d2c20c946c721677c3	a neural stock price predictor using quantitative data	artificial intelligence and neural networks;stock prediction;financial forecasting;signal processing;regression analysis;exploratory study;artificial intelligent;neural network;neural net	Financial forecasting is an example of a signal processing problem which is challenging due to small sample sizes, high noise, non-stationarity, and non-linearity. Neural networks have been very successful in a number of signal processing applications. We discuss fundamental limitations and inherent difficulties when using neural networks for the processing of high noise, small sample size signals. Financial forecasting employing neural networks is a highly significant area for exploratory study. It has long been known that exact prediction is more of an art than a science but attempts can be made to recognize trends and patterns which should help in predicting correctly within an order of magnitude. Our approach uses a typical back propagation neural net and employs various formulas on quantitative data. We picked our training stocks from different categories having varying prices and volumes; this enabled a better analysis while generalizing our findings. While it has not been possible to provide exact predictions, a definite trend is evident in most cases. Statistically-oriented projections of the significance of these findings using standard regression analysis techniques show our approach to be simple yet effective. The historical data was obtained over a time period of 40 days for major stocks on New York Stock Exchange (NYSE). This data was used to train the network while trying out various parameter values and techniques which are discussed below	backpropagation;branch predictor;level of measurement;neural networks;nonlinear system;signal processing;software propagation;stationary process;the new york times	Animesh Chaturvedi;Samanvaya Chandra	2004			regression analysis;data mining;exploratory research;signal processing;artificial neural network;computer science	ML	6.7083767595617125	-20.043974709477272	128795
4b28881e67d6d97801909b8746eece98dabb19a9	a neural network approach to predicting stock exchange movements using external factors	commodity prices;finance;return on investment;dow jones;time series;stock exchange;indexation;financial time series;exchange rate;dow jones industrial average;prediction model;domain specificity;neural network	The aim of this study is to evaluate the effectiveness of using external indicators, such as commodity prices and currency exchange rates, in predicting movements in the Dow Jones Industrial Average index. The performance of each technique is evaluated using different domain specific metrics. A comprehensive evaluation procedure is described, involving the use of trading simulations to assess the practical value of predictive models, and comparison with simple benchmarks that respond to underlying market growth. In the experiments presented here, basing trading decisions on a neural network trained on a range of external indicators resulted in a return on investment of 23.5% per annum, during a period when the DJIA index grew by 13.03% per annum. A substantial dataset has been compiled and is available to other researchers interested in analysing financial time series.	artificial neural network;compiler;domain-specific modeling;experiment;jones calculus;predictive modelling;simulation;time series	Niall O'Connor;Michael G. Madden	2006	Knowl.-Based Syst.	10.1016/j.knosys.2005.11.015	stock exchange;return on investment;computer science;machine learning;time series;predictive modelling;artificial neural network	Web+IR	6.477634066334577	-18.681641882123554	128817
32b3c9f0ef19cc33a38e893ff16dff1d85b7df72	variable precision multigranulation decision-theoretic fuzzy rough sets	multigranulation fuzzy approximation space;variable precision rough sets;three way decisions;期刊论文;lower and upper approximations	This paper studies variable precision multigranulation fuzzy decision-theoretic rough sets in an information system. We firstly review definitions and properties of multigranulation fuzzy rough sets. A novel membership degree based on single granulation rough sets is proposed. Then two operators based on this membership degree are defined. By employing these operators, two types of variable precision multigranulation fuzzy rough sets in an information system are proposed. Finally, inspired by three-way decisions, we propose Type-1 variable precision multigranulation decision-theoretic fuzzy rough sets. © 2015 Elsevier B.V. All rights reserved.	information system;rough set;theory	Tao Feng;Ju-Sheng Mi	2016	Knowl.-Based Syst.	10.1016/j.knosys.2015.10.007	fuzzy logic;operator (computer programming);machine learning;artificial intelligence;rough set;computer science	AI	-1.9059008681016147	-22.003520334289984	129022
1a1f66752882614cb76347b5edea997cf8a4bdd7	robust similarity between hypergraphs based on valuations and mathematical morphology operators	mathematical morphology on hypergraphs;pseudo metric;hypergraphs;similarity;valuation	This article aims at connecting concepts of similarity, hypergraph and mathematical morphology. We introduce new measures of similarity and study their relations with pseudometrics defined on lattices. More precisely, based on various lattices that can be defined on hypergraphs, we propose some similarity measures between hypergraphs based on valuations and mathematical morphology operators. We also detail new examples of these operators. The proposed similarity measures can be used in particular to introduce some robustness, up to some morphological operators. Some examples based on various dilations, erosions, openings and closings on hypergraphs illustrate the relevance of our approach. Potential applications to image comparison are suggested as well. © 2014 Elsevier B.V. All rights reserved.	crystal structure;entropy (information theory);feature selection;matching (graph theory);mathematical morphology;relevance;robustness (computer science);simplicial complex;supra, inc.;turing completeness;value (ethics)	Isabelle Bloch;Alain Bretto;Aurélie Leborgne	2015	Discrete Applied Mathematics	10.1016/j.dam.2014.08.013	combinatorics;discrete mathematics;similarity;valuation;mathematics	Vision	-1.3171355288677842	-23.354389913774998	129084
2646860eb0889969c4fb1a992d1523ee0de57043	on the identification of the global reference set in data envelopment analysis	data envelopment analysis;linear programming;minimum face;global reference set;returns to scale	It is well established that multiple reference sets may occur for a decision making unit (DMU) in the non-radial DEA (data envelopment analysis) s etting. As our first contribution, we differentiate between three types of reference set. First, we introduce the notion of unary reference set (URS) corresponding to a given projection of an ev aluated DMU. The URS includes efficient DMUs that are active in a specif ic convex combination producing the projection. Because of the occurrence of multiple U RSs, we introduce the notion of maximal reference set (MRS) and define it as the union of all the URSs a s ociated with the given projection. Since multiple projections may occur in non-radial DEA models, we further define the union of the MRSs associated with all the proje ctions as unique global reference set (GRS) of the evaluated DMU. As the second contribution, w e propose and substantiate a general linear programming (LP) based approach to identify the GRS . Since our approach makes the identification through the execution of a single pr imal-based LP model, it is computationally more efficient than the existing methods for its ea y implementation in practical applications. Our last contribution is to measure returns to scal e using a non-radial DEA model. This method effectively deals with the occurrence of multiple s upporting hyperplanes arising either from multiplicity of projections or from non-full dimens ionality of minimum face. Finally, an empirical analysis is conducted based on a real–lif e data set to demonstrate the ready applicability of our approach.	data envelopment analysis;linear programming;maximal set;minimal recursion semantics;radial (radio);unary operation	Mahmood Mehdiloozad;S. Morteza Mirdehghan;Biresh K. Sahoo;Israfil Roshdi	2015	European Journal of Operational Research	10.1016/j.ejor.2015.03.029	returns to scale;mathematical optimization;discrete mathematics;economics;linear programming;data mining;data envelopment analysis;mathematics	ML	0.783105233870358	-19.52742137219115	129521
2f87425146af8e9092232821a777e9a4afe43907	foundations of neo-bayesian statistics	bayesian statistics;choquet expected utility;strong law of large numbers;expected utility;robust statistics;subjective expected utility;neyman pearson;satisfiability;law of large numbers;choquet integral;additive functional;neo bayesian statistics choquet integral invariant bi separable preferences affine functions extension comonotonic additive functionals;convex set;probability measure	We study an axiomatic model of preferences, which contains as special cases Subjective Expected Utility, Choquet Expected Utility, Maxmin and Maxmax Expected Utility and many other models. First, we give a complete characterization of the class of functionals representing these preferences. Then, we show that any such functional can be represented as a Choquet integral where is the canonical mapping from the space of bounded [Sigma]-measurable functions into the space of weak*-continuous affine functions on a weak*-compact, convex set of probability measures on [Sigma]. Conversely, any preference relation defined by means of such functionals satisfies the axioms of the model we study. Different properties of the capacity give rise to different models. Our result shows that the idea of Choquet integration is general enough to embrace all the models mentioned above. In doing so, it widens the range of applicability of well-known procedures in robust statistics theory such as the Neyman-Pearson lemma for capacities [P.J. Huber, V. Strassen, Minimax tests and the Neyman-Pearson lemma for capacities, Ann. Statist. 1 (1973) 251-263], Bayes' theorem for capacities [J.B. Kadane, L. Wasserman, Bayes' theorem for Choquet capacities, Ann. Statist. 18 (1990) 1328-1339] or of results like the Law of Large numbers for capacities [F. Maccheroni, M. Marinacci, A strong law of large numbers for capacities, Ann. Probab. 33 (2005) 1171-1178].		Massimiliano Amarante	2009	J. Economic Theory	10.1016/j.jet.2009.04.001	mathematical optimization;mathematical analysis;discrete mathematics;law of large numbers;mathematics;choquet integral;mathematical economics;statistics;choquet theory	Theory	0.6192234861298648	-20.882947966840888	129764
794f71cc9232d10e0c8685b4db0242e8a8cb1d82	selecting between cnc milling, robot milling and dmls processes using a combined ahp and fuzzy approach		Abstract Recent advancements in manufacturing technology allow now a much wider selection of machining processes. Milling with industrial robots or additive manufacturing could now replace traditional milling performed on CNC machine-tools, for certain applications. This work presents a decision-making process for selecting between CNC milling, robot milling and a process of additive manufacturing (DMLS) for a certain class of parts. The AHP method was used for selecting between the three variants of manufacturing processes. The criteria used for AHP were divided into crisp ones and criteria described by linguistic variables. For the last ones, fuzzy inference systems were built to extract measurable information to be used for AHP. Finally, the proposed method was applied for a specific part.	fuzzy logic;robot	Radu-Eugen Breaz;Octavian-Constantin Bologa;Sever Gabriel Racz	2017		10.1016/j.procs.2017.11.439	analytic hierarchy process;artificial intelligence;fuzzy logic;machine learning;robot;machining;computer science;inference	Robotics	-4.276834296264571	-17.70011067035685	129858
2d43dbac46612cbec8d4a4df432f68db7d92b6ac	the dependence between international crude oil price and vietnam stock market: nonlinear cointegration test approach				Le Hoang Anh;Tran X. Phuoc;Ha Thi Nhu Phuong	2019		10.1007/978-3-030-04263-9_51		Robotics	4.135131751988045	-13.759970456845869	129917
81d5a40cfff5c6c1663112afc4fc59db82124f6e	machine learning techniques and use of event information for stock market prediction: a survey and evaluation	stock market prediction model;stock market;machine learning stock markets predictive models internet data mining prediction algorithms neural networks space exploration space technology information technology;learning;neural nets;bepress selected works;financial time series prediction;time series learning artificial intelligence neural nets stock markets;automated event extraction system;machine;time series;stock markets;machine learning techniques;survey prediction market stock information event evaluation learning techniques machine;machine learning;financial time series;techniques;stock;evaluation;prediction model;event;market;automated event extraction system machine learning techniques stock market prediction model financial time series prediction event weighting method;learning artificial intelligence;survey;prediction;event weighting method;information;conference proceeding	This paper surveys machine learning techniques for stock market prediction. The prediction of stock markets is regarded as a challenging task of financial time series prediction. In this paper, we present recent developments in stock market prediction models, and discuss their advantages and disadvantages. In addition, we investigate various global events and their issues on predicting stock markets. From this survey, we found that incorporating event information with prediction model plays very important roles for more accurate prediction. Hence, an accurate event weighting method and a stable automated event extraction system are required to provide better performance in financial time series prediction	machine learning;time series	Paul D. Yoo;Maria H. Kim;Tony Jan	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.1631572	machine;information;prediction;event;computer science;data science;evaluation;machine learning;time series;data mining;predictive modelling;stock;artificial neural network;statistics	Robotics	6.188680342167251	-19.37012854134689	129968
8337cbbc180c799d92a87958e19895fcbb2a663c	a bi-proportional method applied to the spanish congress	calcul scientifique;computer aided analysis;matematicas aplicadas;analyse assistee;modele mathematique;mathematiques appliquees;modelo matematico;bazi;computacion cientifica;proportional allotment;mathematical model;analisis asistido;scientific computation;applied mathematics;political parties;bi proportional allotment	A bi-proportional divisor method is applied to allocate the seats of the 2004 Spanish Congress, thus achieving proportionality relative to the population counts in the fifty-two districts, as well as proportionality relative to the vote counts for the political parties. Also, advantages and disadvantages of the method are discussed.		Victoriano Ramírez-González;Friedrich Pukelsheim;Antonio Palomares;J. Martínez	2008	Mathematical and Computer Modelling	10.1016/j.mcm.2008.05.023	applied mathematics;artificial intelligence;mathematical model;mathematics;operations research	Vision	0.8553272472106389	-14.496180829928623	130020
9770192b69592c6fe1fbb91b96e39769d791e888	stock portfolio selection using learning-to-rank algorithms with news sentiment		In this study, we apply learning-to-rank algorithms to design trading strategies using relative performance of a group of stocks based on investors’ sentiment toward these stocks. We show that learning-to-rank algorithms are effective in producing reliable rankings of the best and the worst performing stocks based on investors’ sentiment. More specifically, we use the sentiment shock and trend indicators introduced in the previous studies, and we design stock selection rules of holding long positions of the top 25% stocks and short positions of the bottom 25% stocks according to rankings produced by learning-to-rank algorithms. We then apply two learning-to-rank algorithms, ListNet and RankNet, in stock selection processes and test long-only and long-short portfolio selection strategies using 10 years of market and news sentiment data. Through backtesting of these strategies from 2006 to 2014, we demonstrate that our portfolio strategies produce risk-adjusted returns superior to the S&P500 index return, the hedge fund industry average performance HFRIEMN, and some sentiment-based approaches without learning-to-rank algorithm during the same period.	algorithm;backtesting;best, worst and average case;learning to rank;news analytics;selection rule;sentiment analysis	Qiang Song;Anqi Liu;Steve Y. Yang	2017	Neurocomputing	10.1016/j.neucom.2017.02.097	trading strategy;learning to rank;stock (geology);long/short equity;mathematics;portfolio insurance;algorithm;portfolio;hedge fund	Web+IR	2.549722040483284	-11.843969942345355	130065
01370b140954fc9890701273a9b29be55dd2eb9d	a test case for application of convolutional neural networks to spatio-temporal climate data: re-identifying clustered weather patterns		Convolutional neural networks (CNNs) can potentially provide powerful tools for classifying and identifying patterns in climate and environmental data. However, because of the inherent complexities of such data, which are often spatio-temporal, chaotic, and non-stationary, the CNN algorithms must be designed/evaluated for each specific dataset and application. Yet to start, CNN, a supervised technique, requires a large labeled dataset. Labeling demands (human) expert time, which combined with the limited number of relevant examples in this area, can discourage using CNNs for new problems. To address these challenges, here we 1) Propose an effective auto-labeling strategy based on using an unsupervised clustering algorithm and evaluating the performance of CNNs in re-identifying these clusters; 2) Use this approach to label thousands of daily large-scale weather patterns over North America in the outputs of a fully-coupled climate model and show the capabilities of CNNs in re-identifying the 4 clustered regimes. The deep CNN trained with 1000 samples or more per cluster has an accuracy of 90% or better. Accuracy scales monotonically but nonlinearly with the size of the training set, e.g. reaching 94% with 3000 training samples per cluster. Effects of architecture and hyperparameters on the performance of CNNs are examined and discussed.	algorithm;artificial neural network;chaos theory;climate model;cluster analysis;convolutional neural network;nonlinear system;stationary process;supervised learning;test case;test set	Ashesh Chattopadhyay;Pedram Hassanzadeh;Saba Pasha	2018	CoRR			ML	8.52965061928293	-23.159347788468754	130136
1a2207aafed6b5014443d151224584771ebe4606	multi-objective linguistic optimization: extensions and new directions using 2-tuple fuzzy linguistic representation model		Multi objective linguistic optimization is a useful mathematical technique to solve problems that interdependent criteria. In such problems, values of the objective functions may be unknown at some points, when the link between the variables and the objective functions are defined linguistically through if-then rules. While solving this type of problems, Tsukamoto based reasoning method has proved useful for converting objective function to a crisp form, and then using the resulting objective function to solve by any traditional optimization technique. However, this method suffers from a drawback that the resulting solution is in numeric form whereas it should have been in linguistic form, owing to the linguistic definition of if-then rules. So, here we propose 2-tuple fuzzy linguistic representation model based method for solving the Multi objective linguistic optimization problem. We demonstrate the novelty of our approach through a suitable example. We also prove that the proposed approach generates unique recommendation in linguistic form.	case-based reasoning;interdependence;mathematical optimization;optimization problem;real life;web framework	Prashant K. Gupta;Pranab K. Muhuri	2017	2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2017.8015772	novelty;artificial intelligence;fuzzy logic;machine learning;tuple;linguistics;mathematics;optimization problem	AI	-4.274132371060412	-23.13836834775616	130303
80e940645ba1e18349eb963d4186ea8de39c2e26	water allocation improvement in river basin using adaptive neural fuzzy reinforcement learning approach	water resource;fuzzy rules;reinforcement learning;optimal water allocation;water allocation;large scale;adaptive neural fuzzy inference system;river basin;water resource management;simulation model;optimization model;historical data;water demand	Optimal use of water is an important objective of water resource development projects all over the world. An integrated approach toward better water resources management in river basins for irrigation planning is needed to find optimal water use policies. In the past, researchers used variables affecting crop pattern and reservoir releases as decision variables (Yeh, 1985). Labadie, 1993, found discrepancies in simulation and optimization models which are important factors in non-adaptive and weak system managements in river basins. These models become more complicated considering conflicting objectives, stochastic hydrology behavior, and uncertain consumptive water use. Labadie, 1993, presented a combined simulation-optimization strategy for river system management. In his studies, decision variable was reservoir release and objective function was maximization of power generation. However, the objective of his study was to assess directly the optimal water use. The other group of studies is concerned with indirect optimization of water use by selecting the best strategies or alternatives in the river basin or even on the farms. Multi-objective methods have been widely used in different water resource projects. Bogardi & Nachtnebel, 1994, used multicriteria decision analysis in the study of water resources management. Other applications of this group can be found in the works of.Karamouz et al., 1992, and Owen et al., 1997. The theory of fuzzy logic provides a mechanism to represent the degree of satisfaction of reservoir objective through the use of fuzzy membership function measures that can be combined in an integrated fashion. The fuzzy approach, alluding to the vagueness or imprecision inherent in problems of this type, has found increasing application in many fields. Fontane et al., 1997, applied reservoir operation based on Fuzzy Logic concept in order to deal with imprecise objectives for the reservoirs located in the monographic area on the Cache la Poudre river basin in the northern Colorado. Sasikumar and Mujumdar, 1998, developed a Fuzzy Waste-Load Allocation Model (FWLAM) for water quality management of a river system using fuzzy multiple objective optimization. Dubrovin et al., 2002, used a new methodology for fuzzy inference and compared it with a traditional (Sugeno style) method, for multipurpose real-time reservoir operation. In these researches, it is implicitly	decision analysis;decision theory;entropy maximization;fuzzy logic;linear algebra;loss function;mathematical optimization;multi-objective optimization;optimization problem;real-time locating system;reinforcement learning;simulation;systems management;vagueness;word lists by frequency	B. Abolpour;M. Javan;M. Karamouz	2007	Appl. Soft Comput.	10.1016/j.asoc.2005.02.007	drainage basin;adaptive neuro fuzzy inference system;computer science;machine learning;simulation modeling;reinforcement learning	AI	3.476134449469618	-19.136017139074895	130347
135c1d6a4e72b05f87a5a678f2008130a54aa4eb	an approximation method for type reduction of an interval type-2 fuzzy set based on α-cuts	zirconium;approximation algorithms;fuzzy set theory approximation theory;fuzzy set theory;fuzzy sets;fuzzy logic;approximation theory;optimization;approximation methods;fuzzy sets approximation methods approximation algorithms fuzzy logic optimization decision making zirconium;approximation algorithm approximation method type reduction interval type 2 fuzzy set α cuts	This paper shows a proposal for Type-reduction of an Interval Type-2 fuzzy set composed from α-cuts done over its primary membership functions. The definition of available Type-reduction methods for Interval Type-2 fuzzy sets are based on an homogeneous subdivision of the universe of discourse, so we propose an approximation algorithm for Type-reduction of an Interval type-2 fuzzy set through its primary α-cuts. Some definitions about the α-cut of a Type-2 fuzzy set are provided and used for computing the centroid of an Interval Type-2 fuzzy set through a mapping of its membership function, instead of its universe of discourse.	approximation algorithm;domain of discourse;fuzzy set;subdivision surface	Juan Carlos Figueroa García	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)		fuzzy logic;mathematical optimization;mathematical analysis;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;approximation algorithm;fuzzy control system	Logic	-0.2279795120955283	-23.600930395782406	130553
7b8ae5d7a000d1443461422cc064a32e0987af2a	integrated traffic and communication performance evaluation of an intelligent vehicle infrastructure integration (vii) system for online travel-time prediction	forecasting;traffic simulation;artificial intelligence ai;travel time prediction;travel time;vehicle infrastructure integration vii;artificial intelligence ai traffic simulation travel time prediction vehicle infrastructure integration vii;neural networks;support vector machines;neural nets;bepress selected works;road traffic;online highway travel time prediction intelligent vehicle infrastructure integration system traffic measurements artificial intelligence paradigms ai paradigms artificial neural networks support vector regression vii ann framework vii svr framework integrated simulation platform greenville sc traffic surveillance traffic management methods traffic simulator paramics network simulator version ns 2 nonrecurrent congestion scenarios sensor based highway travel time prediction methods integrated traffic and communication performance evaluation;support vector machines artificial intelligence automated highways discrete event simulation neural nets regression analysis road traffic;automated highways;support vector regression;traffic control road transportation predictive models support vector machines real time systems artificial intelligence;vehicle infrastructure integration;vehicle infrastructure integration vii artificial intelligence ai traffic simulation travel time prediction;traffic models;artificial intelligence;regression analysis;discrete event simulation	This paper presents a framework for online highway travel-time prediction using traffic measurements that are likely to be available from vehicle infrastructure integration (VII) systems, in which vehicle and infrastructure devices communicate to improve mobility and safety. In the proposed intelligent VII system, two artificial intelligence (AI) paradigms, i.e., artificial neural networks (ANNs) and support vector regression (SVR), are used to determine future travel time based on such information as the current travel time and VII-enabled vehicles' flow and density. The development and performance evaluation of the VII-ANN and VII-SVR frameworks, in both the traffic and communications domains, were conducted using an integrated simulation platform for a highway network in Greenville, SC. In particular, the simulation platform allows for implementing traffic surveillance and management methods in the traffic simulator PARAMICS and for evaluating different communication protocols and network parameters in the communication network simulator, Network Simulator version 2 (ns-2). This paper's findings reveal that the designed communications system can support the travel-time prediction functionality. The findings also demonstrate that the travel-time prediction accuracy of the VII-AI framework was superior to a baseline instantaneous travel-time prediction algorithm, with the VII-SVR model slightly outperforming the VII-ANN model. Moreover, the VII-AI framework was shown to perform reasonably well during nonrecurrent congestion scenarios, which have traditionally challenged sensor-based highway travel-time prediction methods.	algorithm;artificial intelligence;artificial neural network;baseline (configuration management);network congestion;performance evaluation;simulation;support vector machine;telecommunications network;ultima vii part two: serpent isle;vehicle infrastructure integration	Yongchang Ma;Mashrur Chowdhury;Adel Sadek;Mansoureh Jeihani	2012	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2012.2198644	support vector machine;simulation;computer science;engineering;machine learning;transport engineering;operations research;artificial neural network	Mobile	8.827855504036322	-13.663555628139658	130637
582164147ab345c4edd4a15274ffb472232848ce	intuitionistic linguistic group decision-making methods based on generalized compensative weighted averaging aggregation operators		As one of the key research topic in multi-criteria group decision making (MCGDM), aggregation operator has been drawn widespread concern from academics and practitioners. In order to reflect the characteristics of human decision, it is necessary to introduce an operator with compensation ability to close the gap between the theoretical results and experimental results. Based on generalized compensative weighted averaging operator, intuitionistic linguistic generalized compensative weighted averaging (ILGCWA) operator, intuitionistic linguistic generalized compensative ordered weighted averaging (ILGCOWA) operator, and power generalized compensative weighted averaging aggregation (ILPGCWA) operator are developed in this paper. These operators provide two additional parameters to represent decision makers’ attitude and decision makers’ preference for all kinds of alternatives in the aggregation process, respectively. Moreover, some special cases with regard to the generalized parameters p and (lambda ) are investigated in detail in ILGCWA operator and ILGCOWA operator. Some examples are employed to illustrate the effectiveness of the proposed methods, which can be applied to solve MCGDM problem with intuitionistic linguistic information.		Lidong Wang;Yanjun Wang;Arun Kumar Sangaiah;Binquan Liao	2018	Soft Comput.	10.1007/s00500-017-2734-0	artificial intelligence;discrete mathematics;machine learning;operator (computer programming);group decision-making;linguistics;ordered weighted averaging aggregation operator;mathematics;rule-based machine translation	NLP	-3.213259358538068	-21.539367472194538	130696
8f34e0248350ca00a6a484fc8452a9c13571eddc	deal effect curve and promotional models - using machine learning and bootstrap resampling test		Promotional sales have become in recent years a paramount issue in the marketing strategies of many companies, specially in the current economic situation. Empirical models of consumer promotional behavior, mostly based on machine learning methods, are becoming more usual than theoretical models, given the complexity of the promotional interactions and the availability of electronic recordings. However, the performance description and comparison among promotion models are usually made in terms of absolute and empirical values, which is a limited handling of the information. Here we first propose to use a simple nonparametric statistical tool, thepaired bootstrap resampling , for establishing clear cut-off test based comparisons among methods for machine learning based promotional models, by simply taking into account the estimated statistical distribution of the actual risk. The method is used to determine the existence of actual statistically significant differences in the performance of different machine design issues for multilayer perceptron based marketing models, in a real database of everyday goods (milk products). Our results show that paired bootstrap resampling is a simple and effective procedure for promotional modeling using machine learning techniques.	bootstrapping (statistics);effective method;interaction;machine learning;multilayer perceptron;resampling (statistics);statistical model	Cristina Soguero-Ruíz;Francisco Javier Gimeno-Blanes;I. Mora-Jiménez;María Pilar Martínez-Ruiz;José Luis Rojo-Álvarez	2012			artificial intelligence;machine learning;computer science;statistics;bootstrapping (statistics);bootstrap aggregating	AI	-2.018128521969973	-11.345164541081708	130740
ee18f09f891e9e6f59efb95e36c71db6f6433441	traffic flow stabilization strategy for mitigating automated and human driven vehicles interactions		One of key challenges in the area of automated vehicles or self-driving cars is how to ensure smooth interactions between the automated vehicles and human driven vehicles. This is because it would be inevitable to have both automated vehicles and human driven vehicles until market penetration of automated vehicle reaches 100 percent. Our paper proposed traffic flow stabilization strategy based on optimal control theory and evaluated its performance using a microscopic traffic simulation tool under varying automated vehicle market penetrations. The simulation results indicated that the proposed approach effectively improves traffic flow stability when compared to the base case under adaptive cruise control algorithm.		B. Brian Park;Seongah Hong	2018	IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2018.8591075		Robotics	9.36870400419164	-9.96420763678253	130820
7fde68e2ae971096d79315ac568c7edc5135ac7b	economic order quantity in fuzzy sense for inventory without backorder model	optimisation;fuzzy set;theorie ensemble flou;optimizacion;fuzzy total cost;numero difuso;fuzzy number;logique floue;nombre flou;logica difusa;fuzzy set theory;inventory model;fuzzy economic order quantity;administracion deposito;fuzzy logic;optimization problem;ciencias economicas;economic order quantity;triangular fuzzy number;gestion stock;optimization;fuzzy inventory model;sciences economiques;extension principle;economics;inventory control	This paper investigates a group of computing schemas for economic order quantity as fuzzy values of the inventory without backorder. We express the fuzzy order qunatity as the normal triangular fuzzy number (q~, q0, q2), and then solve the aforementioned optimization problem. We find that, after defuzzification, the total cost is slightly higher than in the crisp model; however, it permits better use of the economic fuzzy quantities arising with changes in orders, deliveries, and sales. (~) 1999 Elsevier Science B.V. All rights reserved.	defuzzification;economic order quantity;fuzzy number;mathematical optimization;optimization problem	Huey-Ming Lee;Jing-Shing Yao	1999	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00227-3	mathematical optimization;defuzzification;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set;mathematical economics	AI	-0.15128924524585632	-16.560558642664127	130879
212abf6b34ad7afe00f5da4c26488c3f0b93975d	determination of optimal bidding profit rate by fuzzy set theory	fuzzy set theory;profitability	This paper deals with the problem of application of utility theory by the probabilistic-possibilistic approach in the civil engineering industry. The procedure for determination of the optimal financial result in a case of bidding by a contractor is presented. The utility function is defined as a maximum expected utility. The example in the paper illustrates the problem.	fuzzy set;set theory	Zivojin Prascevic;Sonja Petrovic-Lazarevic	1997	Cybernetics and Systems	10.1080/019697297126137	computer science;artificial intelligence;fuzzy set;profitability index	Theory	-3.4884423122515167	-16.620665070820177	130944
f5094e57514f7688bdc1d23d598d99c0b15988d7	intelligent decision support system for real-time water demand management	neural networks;multi agent systems;decision support system;water demand management	Environmental and demographic pressures have led to the current importance of Water Demand Management (WDM), where the concepts of efficiency and sustainability now play a key role. Water must be conveyed to where it is needed, in the right quantity, at the required pressure, and at th e right time using the fewest resources. This paper shows how modern Artificial Intelligence (AI) techniques can be applied on this issue from a holistic perspective. More specifically, the multi-agent methodology ha s been used in order to design an Intelligent Decision Support System (IDSS) for real-time WDM. It determines the optimal pumping quantity from the storage reservoirs to the pointsof-consumption in an hourly basis. This application integrates advanced f or casting techniques, such as Artificial Neural Networks (ANNs), and other componen ts within the overall aim of minimizing WDM costs. In the tests we have performed, the system achieves a large reduction in these costs. Moreover, the multi-agent environment has demonstrated to propose an appropriate framework to tackle this issue.	analysis of algorithms;artificial intelligence;artificial neural network;holism;intelligent decision support system;multi-agent system;neural networks;pumping (computer systems);real-time clock;real-time transcription;requirement;severo ornstein;time series;wavelength-division multiplexing	Borja Ponte;David de la Fuente;José Parreño;Raúl Pino	2016	Int. J. Comput. Intell. Syst.	10.1080/18756891.2016.1146533	decision support system;intelligent decision support system;computer science;artificial intelligence;machine learning;management science;operations research;artificial neural network	AI	9.964899145434297	-14.664894392050115	131042
cd51344b7c09d1e3a4619dbd5504e39ebaad2096	pricing american parisian options and its application in the valuation of convertible bonds	finance;pricing;stock markets;error analysis;option prices its application forward shooting grid method american moving window parisian option american consecutive parisian option empirical pricing study chinese convertible bonds market;forward shooting grid method;convertible bonds forward shooting grid method american parisian options;convertible bonds;europe;american parisian options;pricing educational institutions europe finance error analysis stock markets	In this paper, we study the problem of pricing American Parisian options based on the forward shooting grid method. We verify the validity of forward shooting grid method by exploiting the relationships among option values of American cumulative Parisian option, American moving window Parisian option and American consecutive Parisian option. We also consider the effects of the trigger conditions and volatilities on the option prices. Then using the simulation method proposed, we present an empirical pricing study of the Chinese convertible bonds market. The results show that the simulated values agree much better with the market values.	simulation;value (ethics)	Chunli Chu;Dongmei Guo;Yi Hu	2013	2013 Sixth International Conference on Business Intelligence and Financial Engineering	10.1109/BIFE.2013.41	financial economics;actuarial science;asian option;finance;business	DB	3.0112781426934707	-11.744719888038095	131150
1a5534c4dc46dfb80be5773e289c5647397b3c17	parameter reduction of intuitionistic fuzzy soft sets and its related algorithms		Contribution of intuitionistic fuzzy soft set (IFSS) in uncertain real-life applications is inevitable. Computation with IFSS may be complicated by the use of less important parameters. However, there has been a little focus on parameter reduction of IFSSs. In this paper, we introduce two different parameter reduction algorithms in IFSSs to satisfy the different needs of decision makers. The first algorithm is based on selection of a set of parameters whose combined contribution is less important in the decision-making process. The second approach selects parameter(s) which has less deviation in comparison to the other parameters. Finally, the proposed algorithms have been demonstrated using illustrative numerical examples. This study also preserves the decision abilities while reducing the redundant parameters.	algorithm	Sumonta Ghosh;Sujit Das	2015		10.1007/978-81-322-2695-6_34	combinatorics;discrete mathematics;mathematics;fuzzy set operations	NLP	-3.1651032782541373	-20.207339441848816	131414
883b29c31b86d463611d4e2a990c6ec18084a97f	interval-valued 2-tuple linguistic induced continuous ordered weighted distance measure and its application to multiple attribute group decision making		This paper aims to propose a new distance measure, the interval-valued 2-tuple linguistic induced continuous ordered weighted distance (IT-ICOWD) measure, which consists of the intervalvalued 2-tuple linguistic induced continuous ordered weighted averaging (IT-ICOWA) operator and the ordered weighted distance (OWD) measure. In these operators, we consider the risk attitude of decision maker. Furthermore, we discuss some desired properties and various special cases of the ITICOWD measure. Additionally, a method of multiple attribute group decision making (MAGDM) in interval-valued 2-tuple linguistic environment is developed on the basis of the IT-ICOWD measure. Through this method, we obtain three simple and exact formulae to determine the order-inducing variables of the IT-ICOWD measure, the weighting vector of decision makers and the weighting vector of attributes, respectively. At last, a numerical example is presented to illustrate the practicability and feasibility of proposed method.	numerical analysis;risk aversion	Xi Liu;Bing Han;Huayou Chen;Ligang Zhou	2018	Informatica, Lith. Acad. Sci.		machine learning;artificial intelligence;computer science;tuple;group decision-making	AI	-2.8389273200865865	-20.748410627765598	131428
96ba40e1369185e020c27531d64b77ca79affbe5	an experiment of burn-in time reduction based on parametric test analysis	kernel;support vector machines;cost saving burn in time reduction parametric test analysis unreliable parts burn in periods expensive equipment parametric test data parametric test models burn in failures multivariate parametric test spaces burn in cycle burn in experiment 3 axis accelerometer design;testing;test equipment failure analysis;testing data models predictive models vectors support vector machines buildings kernel;failure analysis;vectors;predictive models;test equipment;buildings;data models	Burn-in is a common test approach to screen out unreliable parts. The cost of burn-in can be significant due to long burn-in periods and expensive equipment. This work studies the potential of using parametric test data to reduce the time of burn-in. The experiment focuses on developing parametric test models based on test data collected after 10 hours of burn-in to predict parts likely-to-fail after 24 and 48 hours of burn-in. Our study shows that 24-hour and 48-hour burn-in failures behave abnormally in multivariate parametric test spaces after 10 hours of burn-in. Hence, it is possible to develop multivariate test models to identify these likely-to-fail parts early in a burn-in cycle. This study is carried out on 8 lots of test data from a burn-in experiment based on a 3-axis accelerometer design. The study shows that after 10 hours of burn-in, it is possible to identify a large portion of all parts that do not require longer burn-in time, potentially providing significant cost saving.	24-hour clock;binary classification;burn-in;failure cause;signature;test data;test vector;you watch the k foundation burn a million quid	Nik Sumikawa;Li-C. Wang;Magdy S. Abadir	2012	2012 IEEE International Test Conference	10.1109/TEST.2012.6401595	reliability engineering;data modeling;support vector machine;failure analysis;kernel;simulation;computer science;engineering;predictive modelling;software testing;forensic engineering	SE	9.82716888504197	-15.316329246559402	131491
fd066d674a0594830b0071fc46832a48d5293292	analysis on supply and demand of shared autonomous vehicles considering household vehicle ownership and shared use		This research focuses on people's intention for use of shared autonomous vehicles (SAVs) as well as the ownership and shared use of their private vehicles in Meito Ward, Nagoya area of Japan. The nested logit model of mode choice is developed with person trip survey data while the multinomial logit model of autonomous vehicle ownership and shared use is developed with stated preference survey data. The former provides demand forecast for SAVs while the latter for the supply. The results of simulation analysis suggest that 20 to 30 % of trips are served by SAVs, and that 50 to 70% of the vehicles provided by households are sufficient to serve the demand without significant waiting time.	autonomous robot;challenge–response spam filtering;discrete choice;job stream;mathematical optimization;multinomial logistic regression;simulation	Mingyang Hao;Toshiyuki Yamamoto	2017	2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)	10.1109/ITSC.2017.8317920	trips architecture;simulation;logistic regression;survey data collection;multinomial logistic regression;mode choice;demand forecasting;microeconomics;engineering;supply and demand	Robotics	-1.4738517055926394	-10.389595501524992	131745
60e95eb8a2228629c88c5801a7ef7bf42e308552	multi-attribute group decision making method based on neutrosophic trapezoidal fuzzy linguistic frank aggregation operators				Peide Liu;Junlin Liu;Yanchang Chu;Yuming Zhang	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-171278	discrete mathematics;artificial intelligence;fuzzy logic;mathematics;operator (computer programming);machine learning;group decision-making	Robotics	-1.5219085457095913	-21.738157846025715	131789
360182148ac2206b7aca40de551985f6b69446ec	self projecting time series forecast: an online stock trend forecast system	forecast;linear transfer function;time series;forecasts;journal article;box jenkins methodology;conference paper;time series analysis;stock trends;self projecting;arima;self projecting forecasting	This paper explores the applicability of time series analysis for stock trend forecast and presents the Self projecting Time Series Forecasting (STSF) System which we have developed. The basic idea behind this system is the online discovery of mathematical formulae that can approximately generate historical patterns from given time series. SPTF offers a set of combined prediction functions for stocks, including Point Forecast and Confidence Interval Forecast, where the latter could be considered as a subsidiary index of the former in the process of decision making. We propose a new approach to determine the support line and resistance line that are essential for market assessment. Empirical tests have shown that the hit rate of the prediction is impressively high if the model is properly selected, indicating a good accuracy and efficiency of this approach. The numerical forecast result of STSF is superior to normal descriptive investment recommendation offered by most web brokers. Furthermore, SPTF is an online system and investors and analysts can upload their real time data to get the forecast result on the web.	time series	Ke Deng;Hong Shen;Hui Tian	2006	IJCSE	10.1504/IJCSE.2006.009934	econometrics;forecast skill;forecast error;forecast verification;time series;operations research;statistics	ML	5.159184202525214	-16.96301149167153	131842
cf8c8deceb9b221eb8acc8abb21da8b76c231ce8	measure of similarity between interval-valued fuzzy numbers for fuzzy recommendation process based on quadratic-mean operator	quadratic mean operator;interval valued fuzzy numbers;fuzzy number;decision maker;fuzzy recommendation process;similarity measure	Interval-valued fuzzy numbers are very useful for representing a decision-maker's evaluations of parameters or variables associated with real-world problems. This study presents a new similarity measure that is based on the quadratic-mean operator to solve problems that involve the measurement of similarity between interval-valued fuzzy numbers. Some properties of the proposed similarity measure have been demonstrated, and 15 sets of interval-valued fuzzy numbers are adopted to compare the proposed method with existing similarity measures. The results of the comparison indicate that the proposed similarity measure is better than existing methods. Finally, the proposed similarity measure is used to deal with fuzzy recommendation process.		Shi-Jay Chen	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.08.027	fuzzy logic;decision-making;discrete mathematics;membership function;defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	DB	-2.5154124796475514	-20.878545486508788	132092
6696dccfe3f931f4436294237354c5883f70a37e	investment project assessment by a magdm method based on the ranking of interval type-2 fuzzy sets			fuzzy set	Lintao Zhou;Yanfeng Wang;Yong Jiang	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-171403	machine learning;artificial intelligence;fuzzy set;mathematics;ranking	Robotics	-1.4787479064025608	-21.40334585854978	132165
24d38473cd5ebc83094c5b3af98ccafe8b23d56c	minimum weighted minkowski distance power models for intuitionistic fuzzy madm with incomplete weight information	intuitionistic fuzzy set;uncertainty modeling;distance measure;multi objective programming;multi attribute decision making	Owing to more vague concepts frequently represented in decision data, intuitionistic fuzzy sets (IFSs) are more flexibly used to model real-life decision situations. At the same time, with ever increasing complexity in many decision situations in reality, there are often some challenges for a decision maker to provide complete attribute preference information, i.e., the weights may be completely unknown or partially known. The aim of this paper is to develop an effective method for solving intuitionistic fuzzy multi-attribute decision making (MADM) problems with incomplete weight information. In this method, ratings of alternatives on attributes are expressed with IFSs. The multi-objective programming models are established to calculate unknown weights by using weight information partially known a priori. The derived minimum weighted Minkowski distance power models are used to determine the unknown weights and to generate the ranking order of the alternatives simultaneously. The proposed models are easily extended to intuitionistic fuzzy MADM problems with different weight information structures. An example of the supplier selection problem is examined to demonstrate applicability and flexibility of the proposed models and method.		Deng-Feng Li;Shu-Ping Wan	2017	International Journal of Information Technology and Decision Making	10.1142/S0219622014500321	mathematical optimization;discrete mathematics;machine learning;mathematics	Theory	-3.3594239077039636	-19.714375847305472	132189
ce7e16f4b05fb8902fc1a9b8a5e8b05d588519a8	a two tiered finite mixture modelling framework to cluster customers on eftpos network		This paper proposes a framework to build a clustering model of customers of the retailers on the EFTPOS network of a major bank in Australia. The framework consists of two clustering tiers using Finite Mixture Modelling (FMM) that segments customers based on their probabilities of generating transactions of different categories. The first tier generates the transaction categories and the second tier segments the customers, each with a vector of the fractions of their transaction categories as parameters. For each tier, we determine the optimal number of clusters based on the Minimum Message Length (MML) criterion. With the premise that the most valuable customer segment is one that is most likely to generate the most valuable transaction category, we rank the customer segments based on their respective joint probabilities with the most valuable transaction category. By doing so, we are able to reveal the relative value of each customer segment.	eftpos	Yuan Jin;Grace Rumantir	2015		10.1007/978-3-319-26350-2_24	real-time computing;simulation;data mining;business	NLP	3.3007124700635777	-16.638957341377836	132441
7cda4ab7bcd4f2b74ec85074f675281f4283df62	on policy capturing with fuzzy measures	decision models;mesure floue;multicriteria analysis;integrale choquet;valeur shapley;analyse jugement;fuzzy set;theorie ensemble flou;fuzzy measure;linear regression;conjunto difuso;regression model;ensemble flou;statistical method;fuzzy set theory;fuzzy sets;investissement strategique;modelo regresion;analisis regresion;shapley value;telecomunicacion;choquet integral;modele regression;telecommunication;analyse regression;regression analysis;analisis multicriterio;analyse multicritere;interaction effect;valor shapley;judgment analysis	Policy capturing methods generally apply linear regression analysis to model human judgment. In this paper, we examine the application of fuzzy set and fuzzy measure theories to obtain subjective descriptions of cue importance for policy capturing. At the heart of the approach is a method of learning fuzzy measures. The Shapley values associated with the fuzzy measures provide a basis for comparison with the results of linear regression. However, the fuzzy measure-theoretical approach provides additional insight into interaction effects corresponding to the nonlinear, noncompensatory nature of the underlying decision model. To illustrate the methodology, we estimated the importance of factors and the interactions among them that influence decisions related to strategic investments in telecommunications infrastructure and compared the results from the fuzzy approach to those obtained from traditional statistical methods.	fuzzy measure theory	Divakaran Liginlal;Terence T. Ow	2005	European Journal of Operational Research	10.1016/j.ejor.2004.02.023	econometrics;membership function;defuzzification;adaptive neuro fuzzy inference system;computer science;fuzzy measure theory;data mining;mathematics;fuzzy set;statistics	Vision	-1.1166036878322305	-15.412956985203836	132594
83d0a3a49d920da54d3cc26bef1a166901585057	multiple attractors and business fluctuations in a nonlinear macro-model with equity rationing	discrete time;dynamic system;time series;bifurcation analysis;nonlinear dynamics;path dependence;multiple attractors;business fluctuations	A stylized model of business fluctuations is developed, where investment and debt accumulation are responsible for the endogenous dynamics of income. Despite the simplicity of the model, the resulting nonlinear, two-dimensional discrete-time dynamical system displays a wide range of possible dynamic outcomes. If a key parameter, representing the propensity to invest, shifts exogenously from low to high values, a transition across qualitatively different long-run scenarios is observed, associated to different levels of economic activity. Moreover, coexistence of attractors and path-dependence characterize the dynamics for an intermediate range of such a parameter. The impact of exogenous disturbances on such situations results in aperiodic time series subject to unpredictable booms and slumps.	equity crowdfunding;nonlinear system	Roberto Dieci;Mauro Gallegati	2011	Mathematical and Computer Modelling	10.1016/j.mcm.2010.12.016	discrete time and continuous time;simulation;nonlinear system;dynamical system;time series;mathematics;mathematical economics;statistics	Vision	0.08842930985599524	-10.071745586775345	133002
6a537c6c922808870c26ed28740ad24f99ac82e6	new measures of weighted fuzzy entropy and their applications for the study of maximum weighted fuzzy entropy principle	fuzzy set;maximum entropy principle;fuzzy entropy;partial information;maximum fuzzy entropy principle;membership function;weighted fuzzy entropy	Keeping in view the non-probabilistic nature of experiments, two new measures of weighted fuzzy entropy have been introduced and to check their authenticity, the essential properties of these measures have been studied. Under the fact that measures of entropy can be used for the study of optimization principles when certain partial information is available, we have applied the existing as well as the newly introduced weighted measures of fuzzy entropy to study the maximum entropy principle.		Om Parkash;P. K. Sharma;Renuka Mahajan	2008	Inf. Sci.	10.1016/j.ins.2007.12.003	mathematical optimization;joint entropy;combinatorics;mathematical analysis;information diagram;binary entropy function;rényi entropy;membership function;transfer entropy;maximum entropy probability distribution;fuzzy classification;computer science;artificial intelligence;principle of maximum entropy;fuzzy number;fuzzy measure theory;mathematics;maximum entropy thermodynamics;fuzzy set;joint quantum entropy;maximum entropy spectral estimation;conditional entropy	NLP	-2.7027198395990393	-22.757092857208008	133046
9ef9fb009585101c7227baebdb7f0d6f1c9e4e42	neural networks supporting causal reasoning in traffic telematics	neural networks;causal reasoning;data mining;intelligent information systems;genetic algorithms;neural network	In recent years neural networks (NN) made their way to the science of traffic telematics. Approaches for their implementation are quite different, depending on the amount of available data sources and on data resolution (i.e. sampling times). Recently the trend goes to fuse different aspects of system observations to make decisions more secure and reliable. A reason for that is the increasing availability of various data coming from different sources (e.g. weather, events).#R##N##R##N#Weather, concerts as well as higher traffic flows caused by vacationists occur timely and spatially separated. The dimension of designable models grows and when a traffic indicator such as flow needs to be mapped adequately there is additional and meta-information which can be included in the process of system perception.#R##N##R##N#Two approaches are introduced where NN are used to map a given system with the goal to classify or predict a traffic flow, respectively. Observation data of this indicator was only available for training and testing. Decisions are based on additional and meta-information, named as the ability for causal reasoning. The first approach uses feed-forward NN, the second approach uses a Self-Organizing Map (SOM), leading to better visual causal understanding.	artificial neural network;causality;telematics	Werner Toplak;Johannes Asamer;K. Din	2007			computer science;artificial intelligence;machine learning;data mining	ML	8.672020125160214	-22.30283192932318	133117
1c4cb2ddbb4ababbcc59af40a5dd1d5d205c7d95	a novel bp neural network model for traffic prediction of next generation network	transport protocols autoregressive moving average processes backpropagation internet neural nets;convergence;neural nets;next generation network;autoregressive moving average processes;transfer functions;training;internet protocol version 6 network traffic prediction next generation network prediction neural network model backpropagation arma time sequence model time sequence models ip6 network traffic prediction transfer function;traffic prediction;backpropagation;network traffic prediction;ip6 network traffic prediction;transport protocols;artificial neural networks;computational modeling;internet;time sequence models;transfer function;network traffic;next generation network prediction;bp neural network;arma time sequence model;transfer function network traffic bp neural network traffic prediction;predictive models;neurons;neural network model;internet protocol version 6;neural networks predictive models telecommunication traffic traffic control next generation networking transfer functions neurons computer networks convergence streaming media;neural network	Network traffic prediction is an important research aspect of network behavior. Conventionally, ARMA time sequence model is usually adopted in network traffic prediction. However, the parameters used in normal time sequence models are difficult to be estimated and the nonstationary time sequence problem can not be processed using ARMA time sequence model. The neural network techniques may memory large quantity of characteristics of data set by learning previous data, and is suitable for solving these problems with large complexity. IP6 network traffic prediction is just the problem with nonlinear feature and can be solved using appropriate neural network model. In this paper, according to the daily cycle characteristic of IPv6 network traffic, a novel transfer function is designed, which has lots of advantages such as fast convergence and high precision. Based on the new transfer function, an improved BP neural network model is produced, and a IPv6 network traffic prediction system is implemented. Using this new BP neural network model to process the actual data, the results present that our model has a faster learning ability and has a higher precision compared with previous BP neural network model. Therefore, this BP neural network model can be used for normal traffic prediction in current IPv6 network.	artificial neural network;complexity;network model;network packet;network traffic control;next-generation network;nonlinear system;time series;transfer function	Li Zhu;Lei Qin;Kouying Xue;Xinyan Zhang	2009	2009 Fifth International Conference on Natural Computation	10.1109/ICNC.2009.673	traffic generation model;probabilistic neural network;computer science;artificial intelligence;recurrent neural network;machine learning;time delay neural network;network simulation;computer network;network traffic simulation	ML	9.67998407969869	-21.478778150104056	133203
f826a09743f5b5fc7f09d6f89f9d879fba664339	new research of coefficient-fuzzy linear programming	optimal solution;membership function;fuzzy linear programming	In this paper, we make a further research on the coefficient-fuzzy linear programming. The constraint satisfactory function  method and its deficiency are discussed. We introduced a new concept of the membership of constraint. Based on the new concept,  the optimal solution varies with the membership of constraint. A new method is presented and an illustrative numerical example  is provided to demonstrate the feasibility and efficiency of the proposed approach.  	coefficient;linear programming	Hua Pan;Haiquan Qiu;Bing-yuan Cao;Yongchao Hou	2007		10.1007/978-3-540-71441-5_69	mathematical optimization;constraint programming;discrete mathematics;basic solution;integer programming;membership function;criss-cross algorithm;second-order cone programming;linear-fractional programming;fuzzy classification;computer science;linear programming;artificial intelligence;branch and price;machine learning;goal programming	Theory	-0.9431955515029308	-18.160622851161072	133291
70dde6776de53ee5d689606507986dacc6020146	aggregation operators in interval-valued fuzzy and atanassov's intuitionistic fuzzy set theory	science general;intuitionistic fuzzy set;additive generator;aggregation operator;interval valued fuzzy set;atanassov s intuitionistic fuzzy set;t norm;uninorm;owa operator	In this paper we give an overview of some recent advances on aggregation operators on L^I, where L^I is the underlying lattice of interval-valued fuzzy set theory (which is equivalent to Atanassov’s intuitionistic fuzzy set theory). We discuss some special classes of t-norms on L^I and their properties. We show that the t-representable t-norms, which are constructed as a pair of t-norms on [0, 1], are not the t-norms with the most interesting properties. We study additive generators of t-norms on L^I , uninorms on L^I and generators of uninorms on L^I . We give the general deﬁnition and some special classes of aggregation operators on L^I . Finally we discuss the generalization of Yager’s OWA operators to interval-valued fuzzy set theory.	fuzzy set;set theory	Glad Deschrijver;Etienne E. Kerre	2008		10.1007/978-3-540-73723-0_10	mathematical analysis;discrete mathematics;mathematics;fuzzy set operations	Logic	-0.9171233318574058	-23.254194295165075	133296
daa8d5b83fede84689becbed13d725e56f225ac1	transfer information energy: a quantitative causality indicator between time series		We introduce an information-theoretical approach for analyzing cause-effect relationships between time series. Rather than using the Transfer Entropy (TE), we define and apply the Transfer Information Energy (TIE), which is based on Onicescu’s Information Energy. The TIE can substitute the TE for detecting cause-effect relationships between time series. The advantage of using the TIE is computational: we can obtain similar results, but faster. To illustrate, we compare the TIE and the TE in a machine learning application. We analyze time series of stock market indexes, with the goal to infer causal relationships between them (i.e., how they influence each other).	causality;time series	Angel Cataron;Razvan Andonie	2017		10.1007/978-3-319-68612-7_58	machine learning;econometrics;stock market index;transfer entropy;artificial intelligence;causality;computer science	Logic	6.94646030467495	-16.509289696183306	133352
137c4cdfb3920dbb9b4dab14c2923cde17a3299c	fuzzy rough sets, fuzzy preorders and fuzzy topologies	tc axiom;fuzzy topology;fuzzy preorder;fuzzy rough set	Given an arbitrary relation R on a nonempty set X, putting R(A) = {x : (x, y) ∈ R, for some y ∈ A}, A ⊆ X, defines a Kuratowski saturated closure operator R on X iff R is a preorder, i.e., R is reflexive and transitive (a Kuratowski closure operator k : 2 → 2 on X is called saturated if k satisfies k(∪Aj) = ∪k(Aj), Aj ⊆ X, j ∈ J , in place of the usual requirement k(A1 ∪A2) = k(A1) ∪ k(A2), A1, A2 ⊆ X).		S. P. Tiwari;Arun K. Srivastava	2013	Fuzzy Sets and Systems	10.1016/j.fss.2012.06.001	fuzzy logic;t-norm fuzzy logics;combinatorics;discrete mathematics;topology;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-0.5335675792853469	-23.168977025813756	133411
d133bac2e0c40a65938b5c7c9e22961b6cf149b1	flood water level prediction modeling using nnarx structure for sg pahang basin	training;artificial neural networks;computational modeling;mathematical model;predictive models;floods	There were a total of 58 events of natural disaster in Malaysia for the period between years 1980 to 2010 that claiming a total of 1,239 lives of the 640,000 people affected. These data were based on statistics provided by United Nation Officer for Disaster Risk Reduction (UNISDR). From all different categories of natural disasters considered, flood accounted for over half the registered events. Floods contribute to 8 out of 10 disaster events with the highest human exposure and affect over 85 % of all the disaster-stricken people. Floods are thus the primary hazard which affecting Malaysia, in particular the west coast of Peninsular. Therefore, an accurate and reliable flood prediction model is very much needed to provide early warning for residents nearby flood locations for evacuation purposes. However, current trends in flood prediction only involve flood modeling because no prediction time was mentioned and discussed. Furthermore, in Malaysia there is none of flood model or flood prediction model developed yet. An existing system in the Department of Irrigation and Drainage Malaysia is only the alarming system which alarms the users only when the water level exceeds the danger limit. Based on these scenarios, the research objective is to obtain a flood water level prediction model for Pahang flood prone area using Neural Network Autoregressive Model with Exogenous Input (NNARX) structure. The samples used for model training, model validation and model testing were carefully selected. In order to obtain good flood water level prediction model, all samples must be the data when flood events happened. All samples were real-time data that were obtained from the Department of Irrigation and Drainage Malaysia upon special request. From carefully selected samples, several optimal flood prediction times were suggested for flood location in Pahang. Model validation and model testing were conducted to observe the prediction performances. The optimal prediction time was selected based on the results of prediction performances. Results show NNARX model successfully predicted flood water level ahead of time.	artificial neural network;autoregressive model;flood fill;mathematical model;numerical weather prediction;performance;real-time computing;real-time data;syn flood	Fazlina Ahmat Ruslan;Abd. Manan Samad;Mazidah Tajjudin;Ramli Adnan	2015	2015 IEEE International Conference on Control System, Computing and Engineering (ICCSCE)	10.1109/ICCSCE.2015.7482245	meteorology;geography;hydrology;flood forecasting;100-year flood	ML	9.782163794741296	-19.155496327277895	133895
e706d59a6354f38d0bf34e785598d0b13220af08	on structure of generalized intuitionistic fuzzy rough sets	intuitionistic fuzzy set;distributive lattice;fuzzy set;intuitionistic fuzzy sets;lattices;bismuth;rough set theory;data mining;fuzzy set theory;fuzzy sets;cognition;if rough sets;approximation methods;fuzzy rough sets;rough set theory fuzzy set theory;rough set;intuitionistic fuzzy sets fuzzy rough sets if rough sets;fuzzy sets rough sets lattices set theory information science uncertainty humans	Intuitionistic fuzzy sets, originally proposed by Atanassov in 1986, are an attractive extension of fuzzy sets, which enriches the latter with extra features to represent uncertainty. The concept of IF rough sets comes from the combination of IF sets and rough sets. This paper studies axiomatic characterization of IF rough sets. The lower and upper approximations are respectively characterized by two simple axioms. We also consider lattice theoretical properties of IF rough sets and show that the set of all definable IF sets is a completely distributive lattice.	approximation;axiomatic system;fuzzy set;intuitionistic logic;rough set	Guilong Liu;Jie Liu	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255087	combinatorics;mathematical analysis;discrete mathematics;rough set;membership function;type-2 fuzzy sets and systems;computer science;machine learning;mathematics;fuzzy set;dominance-based rough set approach	Robotics	-1.2911509194079263	-23.32415934483657	133970
3a42bab9fcdf6a16327eecef2feb5caf50e55310	fuzzy claim reserving in non-life insurance	fuzzy numbers;fuzzy logic;expected value of a fuzzy number;value of a fuzzy number;claims provisions;insurance	This paper develops several expressions to quantify claim provisions to account in financial statements of a non-life insurance company under the hypothesis of a fuzzy environment. Concretely, by applying the expected value of a fuzzy number and the more general concept of value of a fuzzy number to the ANOVA claim predicting model [2] we estimate claim reserves to account in insurer’s balance sheet and income account. Key-words: Fuzzy logic; Fuzzy numbers; Value of a Fuzzy Number, Expected Value of a Fuzzy Number; Insurance; Claims provisions	defuzzification;embedded system;fuzzy concept;fuzzy logic;fuzzy number;fuzzy set;risk aversion;set theory	Jorge de Andrés Sánchez	2014	Comput. Sci. Inf. Syst.	10.2298/CSIS121225045A	fuzzy logic;membership function;insurance;defuzzification;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;fuzzy measure theory;data mining	AI	-2.0266421536824133	-17.953698057280885	133989
f9b5540ad06d1128a3d7663136c8e280213a591f	characterizing and modeling package dynamics in express shipping service network	express shipping service network traffic dynamics model nationwide expressnet china network structure spatial traffic dynamics temporal traffic dynamics extended markov model emm package delivery process dynamics package delay prediction performance metric;delays companies markov processes predictive models schedules optimization transportation;companies;transportation;schedules;predictive models;optimization;markov processes;delays;traffic goods distribution markov processes	Along with the increasing prosperity of market economy and the growth of online retail, express shipping service (e.g. FedEx, UPS) is playing an increasingly important role in our daily lives. A thorough understanding of the network structure and the package traffic dynamics of largescale express shipping service network (ExpressNet) is essential for performance evaluation, network optimization, and user experience enhancement. Moreover, it would also be interesting and helpful to investigate how express shipping service reflects people's daily lives. In this paper, we propose systematic work to characterize and model the traffic dynamics in a nationwide ExpressNet. We collect 16 million delivery traces over 4 months in China, and examine its characteristics from a wide range of perspective, including network structure, temporal and spatial traffic dynamics, which provide important insights into express companies to better understand the network performance. On top of that, we develop an Extended Markov Model (EMM) to capture the dynamics of package delivery process and further predict the package delay, which is a major performance metric that both customers and express companies are concerned about. Data-based evaluation shows our model can achieve 91% prediction accuracy.	markov chain;markov model;mathematical optimization;network performance;online shopping;performance evaluation;tracing (software);uninterruptible power supply;user experience	Xu Tan;Yuanchao Shu;Xie Lu;Peng Cheng;Jiming Chen	2014	2014 IEEE International Congress on Big Data	10.1109/BigData.Congress.2014.29	real-time computing;simulation;engineering;operations management	Metrics	8.537555514225126	-12.907993196857939	134039
5567eb28b5e6424d0cc85f05cca738c8fead190b	general regression neural network and artificial-bee-colony based general regression neural network approaches to the number of end-of-life vehicles in china		Establishing the number of vehicles that will reach the end of their useful lives in the coming years will substantially affect recycling management and recycling policy. Thus, how to construct a reasonable, accurate model to forecast a product’s end of life is important for recycling management. To improve forecast accuracy for vehicle end of life, this paper proposes two approaches: a general regression neural network (GRNN) and an optimized GRNN based on an artificial bee colony. These approaches are applied to forecast the number of end-of-life vehicles (ELVs) in China. In addition, the proposed models are used to predict the number of ELVs that will appear in China from 2016 to 2020 by combining the forecasting data for the main factors that influence the number of such vehicles. Theoretical and simulation results indicate that the described approaches are effective and feasible. This paper provides practical data support and a better theoretical model for researchers, government managers, and industrial engineers faced with the problems posed by ELVs.	artificial bee colony algorithm;artificial neural network;end-of-life (product);simulation;theory	Fang Xin;Songyuan Ni;Hongliang Li;Xuesheng Zhou	2018	IEEE Access	10.1109/ACCESS.2018.2814054	general regression neural network;machine learning;china;computer science;distributed computing;government;data modeling;artificial intelligence	Robotics	7.676914534802245	-17.951447108854904	134068
4750276f0b2dbc606598ef1c5819e3b6c7a1c5cb	consensus reaching model in the complex and dynamic magdm problem	consensus;multiple attribute decision making;individual alternatives;group decision making;dynamic context;individual attributes	In classical multiple attribute group decision making (MAGDM), decision makers evaluate predefined alternatives based on predefined attributes. In other words, the set of alternatives and the set of attributes are fixed throughout the decision process. However, real-world MAGDM problems (e.g., the decision processes of the United Nations Security Council) frequently have the following features. (1) Decision makers have different interests, and they thus use individual sets of attributes to evaluate the individual alternatives. In some situations, the individual sets of attributes may be heterogeneous. (2) In the decision process, decision makers do not have to reach a consensus regarding the use of the set of attributes. Instead, decision makers hope to find an alternative that is approved by all or most of them. (3) Finally, both the individual sets of attributes and the individual sets of alternatives can change dynamically in the decision process. By incorporating the above practical features into MAGDM, this study defines a complex and dynamic MAGDM problem, and proposes its resolution framework. In the resolution framework, a selection process in the context of heterogeneous attributes is proposed that obtains the ranking of individual alternatives and a collective solution. In addition, a consensus process is developed that generates adjustment suggestions for individual sets of attributes, individual sets of alternatives and individual preferences, thus helping decision makers reach consensus. Compared with existing MAGDM models, this study provides a flexible framework to form an approximate decision model to real-world MAGDM problems. © 2016 Elsevier B.V. All rights reserved.	approximation algorithm;computing with words and perceptions;decision support system;dynamic energy budget;electron;environmental resource management;formal language;fuzzy logic;fuzzy set;internet;interval arithmetic;karloff–zwick algorithm;knowledge-based systems;landau–yang theorem;omega;qualitative comparative analysis;simulation;social network analysis;soft computing;springer (tank);utility;yang;zionts–wallenius method	Yucheng Dong;Hengjie Zhang;Enrique Herrera-Viedma	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.05.046	group decision-making;consensus;computer science;data mining;management science;weighted sum model	AI	-4.4804864071974375	-20.41173171213526	134196
d547198e994ad736b6ac3375c938d5d292fb3b75	a fuzzy cognitive map model for estimating the repercussions of greek psi on cypriot bank branches in greece		Recently, Greece experienced a financial crisis unprecedented in its modern history. In May of 2010 Greece signed a bailout memorandum with Troika (a tripartite committee constituted by the European Central Bank, the European Commission and the International Monetary Fund). In February of 2012, they proceeded to a second bailout package along with a debt restructuring deal that included a private sector involvement (PSI). The overall loss, for the private investors, was equivalent to around 75%. Due to the strong economic ties between Greece and Cyprus, PSI had a substantial impact on the Cypriot economy. A fuzzy cognitive map (FCM) system has been developed and used to study the repercussions of the Greek PSI on the economic dynamics of Cyprus and more specifically on the probability of cutting off the Cypriot Bank branches that operate in Greece. The system allows one to observe how a change on some parameters can affect the stability of the rest of the parameters. Different promising scenarios were implemented, scaling the percentage of PSI from 0% to 80%.	fuzzy cognitive map;image scaling;memorandum	Maria Papaioannou;Costas Neocleous;Christos N Schizas	2013		10.1007/978-3-642-41142-7_60	machine learning;debt restructuring;economic policy;financial crisis;artificial intelligence;computer science;private sector involvement;intelligent decision support system;fuzzy cognitive map;commission;bailout;memorandum	Robotics	3.662906869552193	-15.142543301043423	134223
b0f38e810c39f03093ba991ff2ef32aeeb828ee8	two new characterizations of universal integrals on the scale |0,1]	business and management;fuzzy measure;non additive integral;universal integral	The concept of universal integral, recently proposed and ax iomatized, encompasses several integrals, including the Choquet, Shilkret a nd Sugeno integrals. In this paper we present two new axiomatizations of universal i ntegrals on the scale [0, 1]. In the first characterization, we look at universal integra ls on the scale[0, 1] as families of aggregation functions F satisfying some desired properties. The second characterization is given in the framing in which the original definition of universal integral was provided.	aggregate function;cartesian closed category;decision analysis;framing (world wide web);sugeno integral	Salvatore Greco;Radko Mesiar;Fabio Rindone	2014	Inf. Sci.	10.1016/j.ins.2013.12.056	volume integral;mathematical analysis;discrete mathematics;calculus;mathematics;order of integration;darboux integral;multiple integral	Theory	0.6078666312012291	-21.260838660915176	134381
01688fe3c3a4f748bd6ca4f72f6724ea40afd811	the dao of the sceptic and the spiritual: attitudinal and cultural influences on preferences for sustainable tourism services in the domestic chinese tourism market	domestic tourism;attitudinal variables;tourist preferences;non spirituality dimensions;images of nature;sceptical attitudes;cultural values;choice experiment methods;destination choice;sustainability;future opportunities;spiritual;scepticism;divine designer;tourists;factor analysis;culture;cultural influences;robustness;services management;middle classes;willingness to pay;china;sustainable development;preferences;sustainable tourism;dao;natural attractions	The study assesses Chinese tourist preferences for southwest China destination attributes – including the availability of sustainable tourism services (STS). Using the choice experiment method, the influence of attitudinal variables and of cultural values on preferences is also estimated. Data are collected from 213 middle class respondents in Beijing and Chengdu. STS is the only destination attribute which has no clear influence on destination choice. Factor analysis singled out two ‘positive’ attitudinal dimensions (in favour of sustainability, concern for losing future opportunities) and one ‘negative’ dimension (sceptical attitude on STS). Only the sceptical attitude on STS significantly influenced destination choice, and substantially reduced willingness-to-pay (WTP) for STS. Based on images of nature analysis of cultural values by factor analysis, robustness by divine designer and a non-spirituality dimension displayed substantial impacts on destination preferences. The less spiritual the images of nature of a Chinese middle class tourist, the less s/he prefers – and is willing to pay for – trips featuring more natural attractions and more STS.	eclipse;factor analysis;value (ethics);variable (computer science)	Jan Barkmann;Jiong Yan;Anne-Kathrin Zschiegner;Rainer Marggraf	2010	IJSTM	10.1504/IJSTM.2010.032083	economics;marketing;advertising;factor analysis;management;sustainability;sustainable development;china;culture;robustness	Web+IR	-2.439128503423859	-10.535769315426803	134434
587318095e3130c729049c114135f0133ec9041c	financial series prediction: comparison between precision of time series models and machine learning methods		Precise financial series predicting has long been a difficult problem because of unstableness and many noises within the series. Although Traditional time series models like ARIMA and GARCH have been researched and proved to be effective in predicting, their performances are still far from satisfying. Machine Learning, as an emerging research field in recent years, has brought about many incredible improvements in tasks such as regressing and classifying, and it’s also promising to exploit the methodology in financial time series predicting. In this paper, the predicting precision of financial time series between traditional time series models and mainstream machine learning models including some state-of-the-art ones of deep learning are compared through experiment using real stock index data from history. The result shows that machine learning as a modern method far surpasses traditional models in precision.	autoregressive integrated moving average;deep learning;machine learning;performance;time series	Xin-Yao Qian;Shan Gao	2017	CoRR		stock market index;artificial intelligence;machine learning;econometrics;deep learning;mathematics;finance;exploit;autoregressive conditional heteroskedasticity;autoregressive integrated moving average	ML	6.682290592752898	-19.813692905086462	134638
93bac5972cadef8f0cc63d40a058e029354dbc40	short-term traffic flow forecasting: an experimental comparison of time-series analysis and supervised learning	traffic forecasting;forecasting predictive models time series analysis graphical models data models computational modeling training;probability;support vector machines;kalman filters;traffic flow;time series;traffic engineering computing kalman filters learning artificial intelligence probability regression analysis support vector machines time series;algorithms;traffic engineering computing;regression analysis;learning artificial intelligence;traffic forecasting intelligent transportation systems support vector machines;methodology;sarimax computer model;kalman filter short term traffic flow forecasting time series analysis supervised learning prediction algorithm probabilistic graphical model support vector regression sarima model	The literature on short-term traffic flow forecasting has undergone great development recently. Many works, describing a wide variety of different approaches, which very often share similar features and ideas, have been published. However, publications presenting new prediction algorithms usually employ different settings, data sets, and performance measurements, making it difficult to infer a clear picture of the advantages and limitations of each model. The aim of this paper is twofold. First, we review existing approaches to short-term traffic flow forecasting methods under the common view of probabilistic graphical models, presenting an extensive experimental comparison, which proposes a common baseline for their performance analysis and provides the infrastructure to operate on a publicly available data set. Second, we present two new support vector regression models, which are specifically devised to benefit from typical traffic flow seasonality and are shown to represent an interesting compromise between prediction accuracy and computational efficiency. The SARIMA model coupled with a Kalman filter is the most accurate model; however, the proposed seasonal support vector regressor turns out to be highly competitive when performing forecasts during the most congested periods.	algorithm;baseline (configuration management);graphical model;kalman filter;seasonality;supervised learning;support vector machine;time series	Marco Lippi;Matteo Bertini;Paolo Frasconi	2013	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2013.2247040	kalman filter;support vector machine;simulation;computer science;artificial intelligence;machine learning;traffic flow;time series;probability;methodology;data mining;mathematics;regression analysis;statistics	ML	8.710003254969441	-13.983918058990476	134650
8a162e9dbb66bb83c3d1033c71c31bfa22527d31	a framework for managing a portfolio of socially responsible investments	social responsibility;socially responsible investing;socially responsible;multi criteria analysis;portfolio management;sri;socially responsible investments;sustainable development;socially responsible investments sri	In this paper we present and illustrate using real-life data a framework for managing an investment portfolio in which the investment opportunities are described in terms of a set of attributes and part of this set is intended to capture the effects on society. Here we link with the emerging literature on SRI: Socially Responsible Investment. Given the multivarious descriptions of the individual investment opportunities we show how these can be combined into portfolios with the same attributes at the portfolio level. Also we show how a manager can systematically be supported in the choice between different portfolio profiles. As part of the framework we use multi-criteria decision tools.	aggregate data;data compression;decision problem;feasible region;goal programming;level of measurement;modern portfolio theory;real life;redundancy (engineering);stepwise regression	Winfried Hallerbach;Haikun Ning;Aloy Soppe;Jaap Spronk	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00172-3	economics;finance;social responsibility;application portfolio management;commerce;project portfolio management	ML	-4.370078928596405	-15.25749420607128	134758
5b9a8f8240a5db9509f6cc62418a8365fefcec2b	portfolio value-at-risk optimization for asymmetrically distributed asset returns	value at riskmathematical methods;robust risk measures;finance;fat tail;markowitz mean variance optimization;multivariate normal;risk management;value at risk;portfolio allocation;robust optimization;mean variance;portfolio optimization;portfolio management;risk measure;coherent risk measure;optimal portfolio;asymmetric distributions;partitioned value at risk	We present a new approach to portfolio optimization by separating asset return distributions into positive and negative half-spaces. The approach minimizes a so-called Partitioned Value-at-Risk (PVaR) measure by using the statistical information from the two half-spaces respectively. We show that the proposed PVaR approach is a significant improvement in several important aspects when compared to Markowitz mean-variance optimization approach. First, our approach, which accommodates ambiguous asymmetric return distributions and captures portfolio risk in higher moments, does not require asset distributions being elliptically symmetric or multivariate normal. Second, using simulated and real data, our approach generates better risk-return tradeoffs in the optimal portfolios. The difference between the two approaches increases in the degree of asymmetry in the underlying asset distributions. Third, when given the support of asset returns, our PVaR measure becomes a coherent risk measure proposed by Artzner et al. (1999) whereas conventional risk measures such as variance and VaR fail to do so. Moreover, our PVaR measure is an asymmetric risk measure, which is different from symmetric risk measures like variance and worst-case mean-covariance VaR (WVaR). Therefore, our proposed PVaR is a significant addition to the existing portfolio risk measures. We believe that the PVaR approach can be very useful for better portfolio allocations than the mean-variance or other symmetric risk-metrics approach during market downturns when asset return distributions are often fat-tailed or skewed. ∗Department of Decision Sciences, NUS Business School, National University of Singapore. Email: joelgoh@nus.edu.sg †Finance and Quantitative Finance Unit, Singapore Management University. Email: kgl@smu.edu.sg ‡Department of Decision Sciences, NUS Business School, National University of Singapore. Affiliated with NUS Risk Management Institute and Singapore-MIT Alliance. Email: melvynsim@nus.edu.sg. The research of the author is supported by Singapore-MIT Alliance and NUS academic research grant R-314-000-068-122. §Department of Finance, NUS Business School, National University of Singapore. Affiliated with NUS Risk Management Institute. Email: weina@nus.edu.sg. The research is supported by a NUS academic research grant.	mathematical optimization;risk measure;value at risk	Joel Goh;Kian Guan Lim;Melvyn Sim;Weina Zhang	2012	European Journal of Operational Research	10.1016/j.ejor.2012.03.012	financial economics;efficient frontier;multivariate normal distribution;robust optimization;fat-tailed distribution;actuarial science;economics;risk management;finance;portfolio optimization;coherent risk measure;value at risk	ML	2.1811605776808416	-10.983396901860909	134800
b49a2cd3013d61d953b5c1dce3de3cc0b0da8c51	the role of multiplier bounds in fuzzy data envelopment analysis	fuzzy data;epsilon;data envelopment analysis;weak frontier;article	The non-Archimedean epsilon ε is commonly considered as a lower bound for the dual input weights and output weights in multiplier data envelopment analysis (DEA) models. The amount of ε can be effectively used to differentiate between strongly and weakly efficient decision making units (DMUs). The problem of weak dominance particularly occurs in Fuzzy Data Envelopment Analysis where the reference set is fully or partially defined in terms of fuzzy numbers. In this paper, we propose a new four-step fuzzy DEA method to re-shape weakly efficient frontiers along with revisiting the efficiency score of DMUs in terms of perturbing the weakly efficient frontier. This approach eliminates the non-zero slacks in fuzzy DEA while keeping the strongly efficient frontiers unaltered. In comparing our proposed algorithm to an existing method in the recent literature we show three important flaws in their approach that our method addresses. Finally, we present a numerical example in banking with a combination of crisp and fuzzy data to illustrate the efficacy and advantages of the proposed approach. ‎Keywords‎: ‎Data envelopment analysis‎; ‎Epsilon‎; Fuzzy data‎; Weak frontier‎. ‎ JEL Classification C61 D20 D80 ‎		Adel Hatami-Marbini;Per J. Agrell;Hirofumi Fukuyama;Kobra Gholami;Pegah Khoshnevis	2017	Annals OR	10.1007/s10479-017-2404-8	econometrics;mathematical optimization;computer science;operations management;data envelopment analysis;mathematics	AI	-2.1724730776765506	-15.6986710535281	134982
aaf5e728ea42ee57e7712265b2dcaac970c8ed61	nonequilibrium dynamics of dissipative systems and its application to a macroeconomic system	time scale;long period;stochastic process;history;time measurement;measurement units;integrodifferential equations;economic forecasting;econometric model;trajectory;stochastic processes;macroeconomics econometrics history economic forecasting integrodifferential equations stochastic processes trajectory predictive models measurement units time measurement;macroeconomics;central bank;initial condition;predictive models;econometrics;evolutionary process;dissipative system;nonequilibrium dynamics	A form of nonequilibrium dynamics for a macroeconomic system is developed, in which each of the economic subjects working as interdependent decisionmakers acts so as to maintain the balance between the incoming and the outgoing monetary flows, except at the central bank. The nonequilibrium dynamics, which is based upon the balancing of monetary flows at every instantaneous time at each economic subject except the central bank, is described as a stochastic process since an outside observer cannot know or tell the reasons for particular choices on the part of the interdependent decisionmakers. A sample trajectory of the evolutionary process can be traced by referring only to the initial condition of the macroeconomic system at an arbitrary initial time, exhibiting a distinct contrast to those of various econometric models in which knowledge of the past history over a long period is needed for making a prediction of the future. The macroeconomic system evolves in the direction along which the ratio of the total material dissipation rate per unit time to the total material stock present in the system, both measured in common units of currency, decreases with time on a macroscopic time scale, even if some of the economic subjects undergo bankruptcies during the course of time evolution.	dissipative system;econometric model;initial condition;interdependence;stochastic process	Koichiro Matsuno	1978	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1978.4310016	stochastic process;econometrics;mathematical optimization;dissipative system;trajectory;units of measurement;control theory;predictive modelling;mathematical economics;econometric model;initial value problem;statistics;time	Visualization	5.1501103808655575	-12.337621391602518	135112
34fd1c7a8a93a585dbfc128be2010ee95f227c22	ensemble based ranking of decision making units	ranking decision making units;ensembles;keywords data envelopment analysis	One of the problems with data envelopment analysis (DEA) is that it results in too many decision making units (DMUs) as efficient. This leads to a problem of discrimination among the efficient units. Model misspecification and unrestricted weight flexibility are two main reasons for the discrimination problem. In this paper, we propose and test a model averaging ensemble approach that results in unique DMU rankings. We also prove that ensemble based ranking of DMUs will always result in equal or fewer efficient DMUs than any other single DEA model considered in the ensemble.	data envelopment analysis	Parag C. Pendharkar	2013	INFOR	10.3138/infor.51.3.151	econometrics;machine learning;data mining;mathematics	AI	2.1043607453883446	-16.694547109080936	135238
5d392b8bc9eccfa68c28cb88a3048b92841c2597	an axiomatic definition of divergence for intuitionistic fuzzy sets	intuitionistic fuzzy set;dissimilarity measure;distance;divergencemeasure	An axiomatic definition of divergence measure for intuitionistic fuzzy sets (IFSs, for short) is presented in this work, as a particular case of dissimilarity between IFSs. As the concept of divergence measure is more restrictive, it has particular properties which are studied. Furthermore, the relationships among IF-divergences, dissimilarities and distances are studied. We also provide some methods for building divergence measure for IFSs. They will allow us to conclude this work with a classification of the usual functions used in the literature for measuring the difference between intuitionistic fuzzy sets in two classes: which are divergence measures between IFSs and which are not.	axiomatic system;fuzzy set;intuitionistic logic	Ignacio Montes;Vladimír Janis;Susana Montes	2011		10.2991/eusflat.2011.38	mathematical analysis;discrete mathematics;topology;mathematics	ML	-0.1420892208412135	-21.200762631823704	135468
5b2e7a3fe0cc99ff03ec251910c8bf8b786ece02	multi criteria decision making in fuzzy description logics: a first step	multi criteria decision making;fuzzy description logic	Fuzzy Description Logics are logics which allow to deal with structured knowledge affected by vagueness. Although a relatively important amount of work has been carried out in the last years, fuzzy DLs are open to be extended with several features worked out in other fields. In this work, we start addressing the problem of incorporating Multi-Criteria Decision Making (MCDM) into fuzzy Description Logics and, thus, start an investigation about offering the possibility of a fuzzy ontology assisted approach to decision making.	defuzzification;description logic;emoticon;fuzzy logic;fuzzy set;geographic information system;interdependence;t-norm;vagueness;weight function	Umberto Straccia	2009		10.1007/978-3-642-04595-0_10	fuzzy logic;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;knowledge management;artificial intelligence;data mining;mathematics;fuzzy set operations	AI	-4.029047811360462	-21.868009304580667	135479
1068e481b932d33cacb55b30f0ed9647a7434946	a determination coefficient for fuzzy random variables in a fuzzy arithmetic-based linear model	fuzzy random variable;linearity;fuzzy set;gas insulated transmission lines;linear regression;regression model;random variables;fuzzy set theory;fuzzy sets;regression analysis arithmetic fuzzy set theory random processes;fuzzy set determination coefficient fuzzy random variable fuzzy arithmetic based linear model regression model;gold;linear model;random processes;random variable;virtual colonoscopy;determination coefficient;arithmetic;fuzzy arithmetic based linear model;random variables fuzzy sets linear regression gas insulated transmission lines arithmetic least squares methods virtual colonoscopy gold context modeling linearity;regression analysis;correlation coefficient;context modeling;least squares methods	A natural way of quantifying the degree of linear dependence between two fuzzy random variables in certain models is analyzed. In these models, the linear relationship is formalized in terms of a regression model based on the usual arithmetic for fuzzy sets. The degree of linear relationship is proposed to be measured by means of a kind of determination coefficient, that is, through the proportion of variability of the response fuzzy random variable explained by the regression model. Some properties of both the models and the determination coefficient are analyzed in order to verify the suitability of this coefficient as a measure of linear dependence. Finally, the meaning of a correlation coefficient which mimics the usual one for real-valued random variables is discussed.	coefficient;fuzzy set;heart rate variability;linear model	Ana Colubi;Norberto Corral;Gil González-Rodríguez;Manuel Montenegro	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295412	stochastic process;simple linear regression;econometrics;proper linear model;discrete mathematics;computer science;linear regression;fuzzy number;linear predictor function;coefficient matrix;bayesian multivariate linear regression;linear model;mathematics;fuzzy set;path coefficient;coefficient of determination;statistics	SE	0.9113254632640159	-20.914805408852768	135612
ac97465fc68bca31e1d71408fdcfa2060fc34dba	probabilistic graded rough set and double relative quantitative decision-theoretic rough set	relative quantitative information;probabilistic rough set;graded rough set;absolute quantitative information;probabilistic graded rough set	The probabilistic rough set (PRS) model ignores absolute quantitative information i.e., overlap between equivalence class and basic set. And graded rough set (GRS) model cannot reflect the distinctive degrees of information. In order to overcome these defects, this paper proposes the probabilistic graded rough set (PGRS), which is an extension of Pawlak's rough set and GRS. What is more, we propose double relative quantitative decision-theoretic rough set (Drq-DTRS) models, which essentially indicate the relative and absolute quantification.	rough set;theory	Bowen Fang;Bao Qing Hu	2016	Int. J. Approx. Reasoning	10.1016/j.ijar.2016.03.004	discrete mathematics;rough set;topology;mathematics;dominance-based rough set approach	Logic	-0.5330038054196921	-20.350787610584295	135827
962dc7fb7d948292ecb880436202dc1a86d70772	stein-rule combination forecasting on rfid based supply chain		Radio frequency identification technology has been applied in many fields, especially in logistics operations and supply chain management. Supply chain coordination among partners, which is the core part of supply chain management, can be more practical and effective through sharing real-time product data along the supply chain tracked by RFID technology. This paper focused on the study of the supply chain collaborative forecasting process by sharing RFID real-time data. The collaborative forecasting process among supply chain partners based on the sharing RFID product data is discussed for product demand decision in the paper at first. Then, a Stein-rule combination-forecasting model is proposed to integrate the forecasting knowledge and coordinate forecasting process between the retailers and manufactures shared the RFID data in the supply chain. Moreover, in order to enhance collaborative forecasting precision an error correction combination-forecasting model is discussed. Finally, the outcomes of mathematics simulation verify that the forecast combinations with Stein-rule estimation rules and error correction algorithms are effective to improve forecast precision and coordinate RFID-based supply chain.	radio-frequency identification	Wenjie Wang;Qi Xu;Dandan Fan	2018	APJOR	10.1142/S0217595918400018	radio-frequency identification;industrial engineering;supply chain management;mathematical optimization;error detection and correction;mathematics;supply chain	HCI	-2.9395575691571	-12.73065027992255	135974
e3462d361af4f5d638e8075a7c89d7da12e9a3cf	linguistic rough sets		We introduce linguistic rough set (LRS) by integrating linguistic quantifiers in the rough set framework. The proposed LRS is inspired by the ways in which humans process imprecise information. It operates directly with the linguistic summaries and caters to imprecision implicit in the real world with partial knowledge. The measures of LRS are developed and its properties are investigated in detail. An approach is proposed for approximation of fuzzy concepts with the proposed LRS. This approach is applied in a real world case-study on the credit scoring analysis problem.	rough set	Manish Agarwal;Themis Palpanas	2016	Int. J. Machine Learning & Cybernetics	10.1007/s13042-014-0297-2	artificial intelligence;data mining;mathematics;algorithm	AI	-4.182816402960433	-23.2187063254344	136297
d2035ee14f13a0f273a975365df7aa30689dd962	a simplified neuro-fuzzy inference system (s-nfis) tool for criteria matching	neuro fuzzy inference system	A Simplified Neuro-Fuzzy Inference System (S-NFIS) tool for evalnating counter-party (e.g. a customer and a vendor) criteria is proposed. The proposed S-NFIS, which derived and made unique from Jang's [3] ANFIS structure, allows one party (e.g. a vendor) to define a set of rules with specific weight derived from its business objectives. The rules represent this party's criteria on targeted market segment. Similarly, a set of criteria from the counterparty (e.g. a customer), representing the desired purchasing goals in rules from, is also incorporated. The proposed S-NFIS utilises the degree of membership and the concept of weight to compute the summation of all possible first-order equations generated from the SNFIS structure. The output from S-NFIS, expressed as a percentage offers an overall indication of the customer's demand and vendor's supply criteria mismatches. Based on the size of the gap, either of the party can therefore make own appropriate action towards each other. An illustrative example is presented to demonstrate the computational procedures of the proposed S-NFIS in a typical automotive transactional process.	inference engine;neuro-fuzzy	Alex Tze Hiang Sim;Vincent Cheng-Siong Lee	2003			engineering;marketing;operations management;data mining	Logic	-4.530357371833413	-14.083349612861578	136368
cc2e90ef925b140ff3774b638c5f2da4e122d0a6	daily multivariate forecasting of water demand in a touristic island with the use of artificial neural network and adaptive neuro-fuzzy inference system	forecasting;anfis daily multivariate forecasting touristic island artificial neural network adaptive neuro fuzzy inference system intelligent internet communication technology water management socioeconomic variable mediterranean touristic resort daily water demand forecasting;anfis;artificial neural networks water resources meteorology testing adaptive systems adaptation models forecasting;water supply forecasting theory fuzzy neural nets fuzzy reasoning socio economic effects;water resources;ann;testing;artificial neural networks;water demand forecasting;adaptive systems;ict;anfis water demand forecasting ict ann;adaptation models;meteorology	Water demand forecast has emerged as an imperative component of intelligent Internet and Communication Technologies based methodologies of water management. The need of increased time resolution of forecast in order to implement such methodologies is driving stakeholders to long for new more specialized forecast approaches that will take into account the special drivers of water demand in each case study. Advanced techniques have the ability to overcome the nonlinearity issues commonly met when investigating the complex relationship of water demand and weather, socioeconomic and other variables. In this article we present two approaches, an Artificial Neural Network and an Adaptive Neuro-Fuzzy Inference System, for forecasting a Mediterranean touristic resort daily water demand based on weather variables, tourism and leakage. Both models seem to have an adequate response, though ANFIS can more smoothly catch winter non-touristic water demand profile.	adaptive neuro fuzzy inference system;artificial neural network;imperative programming;inference engine;internet;neuro-fuzzy;nonlinear system;smoothing;spectral leakage	Dimitris Kofinas;Elpiniki I. Papageorgiou;Chrysi S. Laspidou;Nikolaos Mellios;Konstantinos Kokkinos	2016	2016 International Workshop on Cyber-physical Systems for Smart Water Networks (CySWater)	10.1109/CySWater.2016.7469061	simulation;demand forecasting;geography;hydrology;artificial intelligence	AI	6.8770671683973	-17.0781369181055	136456
4bf80fdb2a1526fdf8b36fe72278035a38a4f65a	fuzzy rough sets determined by fuzzy implication operators	approximation operator;probability density function;rough set theory;fuzzy sets rough sets fuzzy logic fuzzy set theory uncertainty set theory fuzzy systems boundary conditions mathematics physics;data mining;serial fuzzy approximation space;fuzzy set theory;fuzzy sets;fuzzy sigma algebra;fuzzy rough set;mathematical operators;indexes;artificial neural networks;rough set theory fuzzy set theory mathematical operators;approximation methods;fuzzy sigma algebra fuzzy rough set fuzzy implication operator approximation operator serial fuzzy approximation space;fuzzy implication operator;rough set	In this paper, determined by a fuzzy implication operator, a general type of dual pair of lower and upper fuzzy rough approximation operators induced by a fuzzy approximation space is first introduced. Properties of the approximation operators are then examined. Finally, it is proved that the class of all definable sets induced from a serial fuzzy approximation space forms a fuzzy σ-algebra.	rough set	Wei-Zhi Wu	2009		10.1109/GRC.2009.5255055	fuzzy logic;combinatorics;mathematical analysis;discrete mathematics;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system	AI	-0.9709493970698116	-22.996163812282475	136517
a9166b50d5ecb6799fa33c211b3cdb9b04c31fd0	genetic algorithm optimization for determining fuzzy measures from fuzzy data		Fuzzy measures and fuzzy integrals have been successfully used in many real applications. How to determine fuzzy measures is a very difficult problem in these applications. Though there have existed some methodologies for solving this problem, such as genetic algorithms, gradient descent algorithms, neural networks, and particle swarm algorithm, it is hard to say which one is more appropriate and more feasible. Each method has its advantages. Most of the existed works can only deal with the data consisting of classic numbers which may arise limitations in practical applications. It is not reasonable to assume that all data are real data before we elicit them from practical data. Sometimes, fuzzy data may exist, such as in pharmacological, financial and sociological applications. Thus, we make an attempt to determine a more generalized type of general fuzzy measures from fuzzy data by means of genetic algorithms and Choquet integrals. In this paper, we make the first effort to define the  rules. Furthermore we define and characterize the Choquet integrals of interval-valued functions and fuzzy-number-valued functions based on  rules. In addition, we design a special genetic algorithm to determine a type of general fuzzy measures from fuzzy data.	fuzzy logic;fuzzy measure theory;genetic algorithm	Li Chen;Zengtai Gong;Gang Duan	2013	J. Applied Mathematics	10.1155/2013/542153	fuzzy logic;mathematical optimization;membership function;defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	DB	-1.949344854277157	-21.187853257021615	136569
b4e4fb1232598ba2da2ff288b9205eebee92c306	determination the number of hidden nodes of recurrent neural networks for river flow and stock price forecasting	time series forecasting;time varying;time series bayes methods data analysis economic forecasting geophysics computing mathematics computing recurrent neural nets rivers stock markets;mathematics computing;rivers;efficiency index;economic forecasting;stock price forecasting;bayes methods;efficiency index recurrent neural networks river flow forecasting stock price forecasting time series forecasting time series data analysis bayesian information criterion;river flow forecasting;time series;stock markets;data analysis;time series data analysis;geophysics computing;stock price;indexation;time series data;recurrent neural nets;recurrent neural networks;recurrent neural network;bayesian information criterion;recurrent neural networks rivers neurons bayesian methods predictive models technology forecasting time series analysis output feedback neurofeedback least squares methods	The classic approach to time series forecasting is to undertake an analysis of the time series data. Recurrent neural networks (RNNs) are designed to learn sequential or time-varying patterns. Due to their dynamic nature, so RNNs are suitable for time series forecasting. As the number of nodes in the input and output layers are application - dependent, the problem reduces to how to optimally choose the number of hidden nodes. This is achieved by using the Bayesian Information Criterion via the number of hidden nodes of recurrent neural network. It is also proved that there is a close link between the Bayesian Information Criterion and Efficiency Index, which indicates how good the model when it is used for the river flow and stock price data set.	artificial neural network;bayesian information criterion;input/output;neural networks;recurrent neural network;time series	Suwarin Pattamavorakun;Suwat Pattamavorakun	2007	5th ACIS International Conference on Software Engineering Research, Management & Applications (SERA 2007)	10.1109/SERA.2007.79	computer science;machine learning;economic forecasting;time series	ML	9.755341881754754	-21.2458910492917	136617
17dbaf9fa99b07d24c39701b880598d89f4d743e	applying undistorted neural network sensitivity analysis in iris plant classification and construction productivity prediction	mlp neural networks;productivity study;neural network sensitivity analysis;feature reduction;mlp neural network;civil engineering;sensitivity analysis;concrete construction;prediction accuracy;decision process;multi layer perceptron;neural network	The present research focuses on the development and applications of a sensitivity analysis technique on multi-layer perceptron (MLP) neural networks (NN), which eliminates distortions on the sensitivity measures due to dissimilar input ranges with different units of measure for input features of both continuous and symbolic types in NN’s practical engineering applications. The effect of randomly splitting the dataset into training and testing sets on the stability of a MLP network’s sensitivity is also observed and discussed. The IRIS-UCI dataset and a real concreting productivity dataset serve as case studies to illustrate the validity of the undistorted sensitivity measure proposed. The results of the two case studies lead to the conclusion that the sensitivity measures accounting for the relevant input range for each input feature are more accurate and effective for revealing the relevance of each input feature and identifying less significant ones for potential feature reduction on the model. The MLP NN model obtained in such a way can give not only high prediction accuracy, but also valid sensitivity measures on its input features, and hence can be deployed as a predictive tool for supporting the decision process on new scenarios within the engineering problem domain.	artificial neural network	Ming Lu;Daniel S. Yeung;Daniel S. Yeung	2006	Soft Comput.	10.1007/s00500-005-0469-9	computer science;artificial intelligence;machine learning;data mining;multilayer perceptron;sensitivity analysis;artificial neural network	NLP	6.3991187631356565	-22.68150845510335	137085
cceb157a568b982fcff93dfa17f00e1c038365e2	building behavior scoring model using genetic algorithm and support vector machines	customer behavior;data mining;commercial banks;financial institutions;genetic algorithm;classifiers;feature selection;support vector machine;classification accuracy;article;multi class support vector machines;behavior scoring	In the increasingly competitive credit industry, one of the most interesting and challenging problems is how to manage existing customers. Behavior scoring models have been widely used by financial institutions to forecast customer’s future credit performance. In this paper, a hybrid GA+SVM model, which uses genetic algorithm (GA) to search the promising subsets of features and multi-class support vector machines (SVM) to make behavior scoring prediction, is presented. A real life credit data set in a major Chinese commercial bank is selected as the experimental data to compare the classification accuracy rate with other traditional behavior scoring models. The experimental results show that GA+SVM can obtain better performance than	genetic algorithm;real life;support vector machine	Defu Zhang;Qingshan Chen;Lijun Wei	2007		10.1007/978-3-540-72586-2_69	support vector machine;genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection	AI	5.7405644176449755	-19.40439893369459	137144
2a0981964636bd6bb3ec1eda83d0b5d8dccd9f6e	chance-constrained programming on sugeno measure space	gλ random variable;random number generator;α optimistic value;journal;optimization problem;hybrid approach;expected value;approximate solution;a pessimistic value;a optimistic value;random variable;sugeno chance constrained programming;g λ random variable;sugeno measure;g l random variable;probability measure;α pessimistic value	Uncertain programming is a theoretical tool to handle optimization problems under uncertain environment, it is mainly established in probability, possibility, or credibility measure spaces. Sugeno measure space is an interesting and important extension of probability measure space. This motivates us to discuss the uncertain programming based on Sugeno measure space. We have constructed the first type of uncertain programming on Sugeno measure space, i.e. the expected value models of uncertain programming on Sugeno measure space. In this paper, the second type of uncertain programming on Sugeno measure space, i.e. chance-constrained programming on Sugeno measure space, is investigated. Firstly, the definition and the characteristic of @a-optimistic value and @a-pessimistic value as a ranking measure are provided. Secondly, Sugeno chance-constrained programming (SCCP) is introduced. Lastly, in order to construct an approximate solution to the complex SCCP, the ideas of a Sugeno random number generation and a Sugeno simulation are presented along with a hybrid approach.		Hong Zhang;Minghu Ha;Hong-Jie Xing	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.03.029	random variable;optimization problem;mathematical optimization;discrete mathematics;probability measure;control theory;mathematics;expected value;statistics	HCI	-1.40993609847195	-19.819694087348907	137170
2b4b9d5ce3d9ead3f9ae355557a02c7e27ca1d7b	intuitionistic fuzzy approximation spaces induced by intuitionistic fuzzy topologies	intuitionistic fuzzy preorder;intuitionistic fuzzy approximation;intuitionistic fuzzy topology	We investigate intuitionistic fuzzy approximation spaces induced by intuitionistic fuzzy topologies, and prove that there exists a one-to-one correspondence between the set of all preordered intuitionistic fuzzy relations and the set of all intuitionistic fuzzy topologies satisfying appropriate condition.	approximation;intuitionistic logic;one-to-one (data model)	Sang Min Yun;Seok-Jong Lee	2016	2016 Joint 8th International Conference on Soft Computing and Intelligent Systems (SCIS) and 17th International Symposium on Advanced Intelligent Systems (ISIS)	10.1109/SCIS-ISIS.2016.0168	mathematical analysis;discrete mathematics;topology;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;fuzzy set operations	Embedded	-0.9363256199536291	-23.0605296444439	137234
65615ff93940062afdea540f63d2980628ec7fd3	improving option pricing with the product constrained hybrid neural network	computer crashes;neural networks;financial management;neural nets;boundary conditions;pricing;financial management pricing neural nets;option pricing;indexing terms;artificial neural networks;product constrained hybrid neural network;stochastic processes;boundary condition;neural net work;boundary conditions option pricing product constrained hybrid neural network financial markets artificial neural networks;financial market;financial markets;neural networks computer;models economic neural networks computer;security;pricing neural networks artificial neural networks boundary conditions globalization computer crashes security stochastic processes costs australia;globalization;models economic;australia;artificial neural network;neural network	In the past decade, many studies across various financial markets have shown conventional option pricing models to be inaccurate. To improve their accuracy, various researchers have turned to artificial neural networks (ANNs). In this work a neural network is constrained in such a way that pricing must be rational at the option-pricing boundaries. The constraints serve to change the regression surface of the ANN so that option pricing accuracy is improved in the locale of the boundaries. These constraints lead to statistically and economically significant out-performance, relative to both the most accurate conventional and nonconventional option pricing models.	artificial neural network;confidence limit;feature selection;gamma correction;hybrid neural network;platform controller hub;resampling (statistics);volatility	Paul Lajbcygier	2004	IEEE Transactions on Neural Networks	10.1109/TNN.2004.824265	actuarial science;boundary value problem;computer science;machine learning;artificial neural network;financial market	DB	5.813785445485102	-16.746273330734937	137302
8fbda26932b6a2b6c5e4588c6f4c11b57356253f	classification of hard-to-recover reserves based on fcm and combination weighting approach	reserves classification;fuzzy means fcm;hard to recover reserves;combination weighting approach	Currently,the classification criterion of reserves are determined by the scope of the values of criteria such as geological attributes,reservoir phydical parameters and etc.,which require all attribute values of one block should be just right in the existing range of criteria,otherwise it would be difficult to divide the hard-to-recover reserves into different categories. To solve this problem,this paper combines with Fuzzy c-Means clustering algorithm( FCM) and combination weighting approach to classify hard-to-recover reserves. First,FCM is used to automatically search for the optimal category number of reserves based on effect indexes. Then,a combination weighting model is established based on the minimal error-sum of deviation of subjective weights and deviation of objective weights,which is used to compute the weights of attributes and the values of effect indexes. Finally, the categories that blocks belonge to is judged according to the result of FCM. To verify the validity of model, this paper applies it to the classification problem of hard-to-recover reserves from an oil field in the 10th Oil Production Plant of PetroChina Daqing Oilfield LLC,which would conduct the rolling development of hard-to-recover reserves.	fuzzy cognitive map	Zhi Li;Juan Yang;Kejun Zhu	2013			mathematical optimization;pattern recognition;data mining;mathematics	NLP	3.7208704386613385	-23.90628138114162	137324
7f1e76a88b25e5470e3fbf0e48b8e8d49483967e	fuzzy measures for a correlation coefficient of fuzzy numbers under tw(the weakest t-norm)-based fuzzy arithmetic operations	fuzzy set;fuzzy measure;fuzzy number;tw based fuzzy arithmetic operations;exact solution;fuzzy sets;mathematical programming;t w based fuzzy arithmetic operations;extension principle;correlation;correlation coefficient	"""Recently, the sup-min convolution based on Zadeh's extension principle has been used by Liu and Kao [Fuzzy measures for correlation coefficient of fuzzy numbers, Fuzzy Sets and Systems 128 (2002) 267-275], to calculate a fuzzy correlation coefficient. They used a mathematical programming approach to derive fuzzy measures based on the classical definition of the correlation coefficient. It is well known that T""""W (the weakest t-norm)-based addition and multiplication preserve the shape of L-R fuzzy numbers. In this paper, we consider the computational aspect of the T""""W-based extension principle when the principle is applied to a correlation coefficient of L-R fuzzy numbers. We give the exact solution of a fuzzy correlation coefficient without programming or the aid of computer resources."""	coefficient;fuzzy measure theory	Dug Hun Hong	2006	Inf. Sci.	10.1016/j.ins.2004.11.005	fuzzy logic;mathematical analysis;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-0.9699609088670776	-21.61721029079111	137436
62aed1c55ab1833436b5c44270680d3127b01eec	a case-based reasoning system for pcb principal process parameter identification	case base reasoning;lead time;parameter identification;parameter selection;process parameters;nearest neighbor;principal parameter identification;feature selection;nearest neighbor search;printed circuit board;case based reasoning;product quality	The Printed Circuit Board (PCB) manufacturing process usually consists of lengthy production activities. Each activity is controlled by a number of process parameters. Although numerous process parameters must be determined before fabrication, only a number of parameters called principal process parameters because they affect the quality of a PCB product. As long as the principal process parameters are identified efficiently and controlled well, the manufacturing lead-time can be shortened and the quality of the new PCB product can be assured. This research proposes a Case-Based Reasoning (CBR) system to infer the principal process parameters for a new PCB product. Each case in the case-base stores design specifications, process parameters, and the corresponding production quality specifications. A Significant Nearest Neighbor (SNN) search is developed to retrieve similar cases from a case-base. A Mutual Correlation Parameter Selection (MCPS) method and a correlation-based parameter setting method are developed to identify the principal parameters and infer their reasonable value range. A set of experiments and a practical implementation case are demonstrated to show the efficiency and accuracy of the proposed system. 2006 Elsevier Ltd. All rights reserved.	artificial neural network;case-based reasoning;experiment;knowledge management;microsoft cordless phone system;national supercomputer centre in sweden;printed circuit board;profiling (computer programming);reasoning system	Chieh-Yuan Tsai;Chuang-Cheng Chiu	2007	Expert Syst. Appl.	10.1016/j.eswa.2006.02.014	case-based reasoning;computer science;machine learning;pattern recognition;data mining;nearest neighbor search;printed circuit board;feature selection;statistics	AI	4.686376583131315	-20.29528556354189	137538
bcc452769c75bcce097ee585f89709cffa511584	stochastic lagrangian traffic flow modeling and real-time traffic prediction	estimation;stochastic processes;mathematical model;predictive models;vehicles;data models;real time systems	Lagrangian Traffic model which follows a platoon of vehicle has the benefit of convenient utilization of individual vehicle data and easy distribution of traffic information to the drivers. The objective of this paper is to develop a stochastic Lagrangian traffic flow model which uses probe vehicle data to predict traffic status. The proposed probing method tracks vehicles in pairs to collect speed and spacing between vehicles. The traffic flow model utilizes unscented Kalman filter (UKF) with dual estimation to update model parameters and estimated current traffic in real-time. The proposed model was validated by empirical highway traffic data, and the result showed that the stochastic model has an overall 20% improvement in estimating current traffic state comparing to the estimation from deterministic model. The predictive ability of the model with an average of 15% error for 3-sec prediction can be used to compensate for the latency of data processing in real-time application. This paper also demonstrated a new method to predict the unexpected jam traffic phase using estimated model parameter with an average lead time of 6.76 sec, which allows drivers to be prepared for potential stop-and-go traffic.	image resolution;jam;kalman filter;real-time clock;real-time computing;sensor	Kang-Ching Chu;Romesh Saigal;Kazuhiro Saitou	2016	2016 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2016.7743383	traffic generation model;control engineering;econometrics;simulation;engineering	Robotics	9.694535581834321	-11.611103437349806	137730
8add57a14ae7f05039b1f10ef0b768878c755879	a system for airport weather forecasting based on circular regression trees		Abstract This paper describes a suite of tools and a model for improving the accuracy of airport weather forecasts produced by numerical weather prediction (NWP) products, by learning from the relationships between previously modelled and observed data. This is based on a new machine learning methodology that allows circular variables to be naturally incorporated into regression trees, producing more accurate results than linear and previous circular regression tree methodologies. The software has been made publicly available as a Python package, which contains all the necessary tools to extract historical NWP and observed weather data and to generate forecasts for different weather variables for any airport in the world. Several examples are presented where the results of the proposed model significantly improve those produced by NWP and also by previous regression tree models.	decision tree learning;machine learning;numerical analysis;numerical weather prediction;python	Pablo Rozas Larraondo;Iñaki Inza;José Antonio Lozano	2018	Environmental Modelling and Software	10.1016/j.envsoft.2017.11.004	software;management science;data mining;simulation;python (programming language);decision tree;computer science;weather forecasting;numerical weather prediction	AI	7.278750771417783	-22.17530087306722	137769
761ab68b6d02f8726a4e69977ca4b0143cb822f7	multiple criteria decision analysis using a likelihood-based outranking method based on interval-valued intuitionistic fuzzy sets	likelihood based preference function;interval valued intuitionistic fuzzy set;outranking method;multiple criteria decision analysis;likelihood	The purpose of this paper is to develop a likelihood-based outranking method for handling multiple criteria decision analysis problems based on interval-valued intuitionistic fuzzy sets. Using the concept of likelihood of the interval-valued intuitionistic fuzzy preference relations, this paper determines certain generalized criteria and the corresponding likelihood-based preference functions. Based on some useful concepts of comprehensive preference indices, concordance indices, counter-likelihood-based preference functions, and discordance indices, this paper conducts a concordance-discordance analysis with concordance and discordance outranking relationships to determine a global Boolean matrix and acquire partial ranking orders of the alternatives. Alternatively, this paper determines complete ranking orders of the alternatives using the concepts of net concordance indices, net discordance indices, and mean outranking values. The feasibility and applicability of the proposed method are illustrated with a practical multiple criteria decision-making application concerning the selection of a suitable bridge construction method. Finally, comparative discussions with different decision-making methods are conducted to verify the effectiveness and advantages of the proposed method in aiding decision making.	decision analysis;fuzzy set	Ting-Yu Chen	2014	Inf. Sci.	10.1016/j.ins.2014.07.003	mathematical optimization;data mining;mathematics;likelihood function;multiple-criteria decision analysis;statistics	AI	-3.467312771847767	-19.94388933713759	137813
6cdc7b6d60af850a5f5de97773e30a103d29e57b	point-sensitive aggregation operators: functional equations and applications to social choice		In this paper we study aggregation operators that are point-sensitive which means that their values depend on the point where the functions to be aggregated are defined, as well as on the values of those functions at that point. This analysis gives rise to consider several functional equations that appear in a natural way. Further applications in mathematical social choice also appear as a by-product. In particular, we characterize the representation of certain social choice rules by means of specific numerical functions.		Juan Carlos Candeal;Esteban Induráin	2017	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488517500428	functional equation;operator (computer programming);mathematics;discrete mathematics;social choice theory	Arch	0.5297621060945199	-21.090240327373134	138388
550d363b95bfdf75e144dab5198621b96a9d1e07	pm-10 forecasting using neural networks model	neural networks model;forecasting;neural nets air pollution correlation methods environmental science computing health hazards mean square error methods;root mean square error pm 10 forecasting neural networks model air pollutant human health emission source meteorological factor geographical factor pm 10 concentration correlation analysis environmental factor;neural networks;human health;air pollutant;neural nets;emission source;root mean square error;correlation methods;environmental science computing;pm 10;artificial neural networks;geographical factor;air pollution;environment;meteorological factor;mean square error methods;error rate;cities and towns;predictive models;atmospheric modeling;neural network model;pm 10 forecasting;health hazards;pm 10 concentration;neural networks predictive models weather forecasting humans meteorology system testing air pollution protection environmental factors monitoring;air pollutants;pm 10 forecasting neural networks environment;data models;neural network;environmental factor;correlation analysis	PM-10 is one of major air pollutants which affect on human health. Since PM-10 comes from various emission sources and its level of concentration is largely dependent on meteorological and geographical factors of the local region, the forecasting of PM-10 concentration is of great interest to protect daily human health. In this study, the dependent variables on PM-10 concentration were derived from the correlation analysis between PM-10 and meteorological as well as environmental factors based on the observations at the monitoring stations. Using the potential variables on the PM-10 level, the neural network model was developed and tested. The root mean square errors of the prediction in test runs were 0.064 to 0.077 and the test results implied that the system could be used in real forecasting within 10% error rates.	mean squared error;network model;neural networks	S. H. Yu;Y. S. Koo;E. Y. Ha;H. Y. Kwon	2008	2008 International Conference on Computational Intelligence for Modelling Control & Automation	10.1109/CIMCA.2008.173	data modeling;atmospheric model;particulates;forecasting;word error rate;computer science;machine learning;mean squared error;predictive modelling;natural environment;operations research;artificial neural network;air pollution	Robotics	9.839443244474575	-19.13380511659347	138471
fc1d331cf7539b7058e4e4c4e2466657251a812a	a combined pca-mlp model for predicting stock index	mape;stock market;multilayer perceptron;multilayer perceptron mlp;stock price;principal component analysis pca;principal component analysis;indexation;prediction;multicollinearity	Predicting stock prices is a challenging and daunting task due to the complexity of the stock market. In this study, a combined model is proposed to explore market tendency. Prediction of daily closing price using the variables daily opening price, high, low and volume of transaction is done. In this approach, the predictor variables are multi collinear in nature which is overcome by using Principal Component Analysis (PCA) which resulted in a new set of independent variables that are taken for predicting the stock prices using Multilayer Layer Perceptron (MLP) model. To evaluate the prediction ability of the model, we compare the performance of models using a common error measure. The empirical results reveal that the proposed approach is a promising alternate to stock market prediction.	closing (morphology);kerrison predictor;memory-level parallelism;perceptron;principal component analysis	K. V. Sujatha;S. Meenakshi Sundaram	2010		10.1145/1858378.1858395	financial economics;econometrics;economics;machine learning	NLP	6.213586849698476	-18.273734829477373	138508
e388c25f2bcca12518da3e6bbb55d0800ad1d560	maximum length weighted nearest neighbor approach for electricity load forecasting	optimization technique maximum length weighted nearest neighbor approach electricity load forecasting time series forecasting mlwnn sequence similarity;artificial neural networks;artificial neural networks world wide web;time series load forecasting optimisation;world wide web	In this paper we present a new approach for time series forecasting, called Maximum Length Weighted Nearest Neighbor (MLWNN), which combines prediction based on sequence similarity with optimization techniques. MLWNN predicts the 24 hourly electricity loads for the next day, from a time sequence of previously electricity loads up to the current day. We evaluate MLWNN using electricity load data for two years, for three countries (Australia, Portugal and Spain), and compare its performance with three state-of-the-art methods (weighted nearest neighbor, pattern sequence-based forecasting and iterative neural network) and with two baselines. The results show that MLWNN is a promising approach for one day ahead electricity load forecasting.	artificial neural network;homology (biology);iterative method;mathematical optimization;sequence alignment;time series;weather research and forecasting model;wnn	Tommaso Colombo;Irena Koprinska;Massimo Panella	2015	2015 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2015.7280809	probabilistic forecasting;computer science;artificial intelligence;machine learning;data mining;artificial neural network	Vision	9.148903096036758	-20.586956363523427	138648
dfe2b467551d4053f757c1992b2286cff425ad3a	an approach to calculate optimal window-width serving for the information diffusion technique	optimal diffusion function estimation;probability density function;optimal window width;information diffusion	The earliest application of information diffusion theory in China is Wang’s (1985) utilization of information allocation to present a method for assessing the vibrational trend of arenaceous soil. After that, Huang and Wang (1992; 1995) successfully compiled the integrated map of geological disaster for the Boroughs of Lanzhou City, which is an international corporation project of China and the European Community, by means of information diffusion theory. The Taisanmiao landslide, which is located in Jiaosuwan, Tianshui city of China, was in a dangerous situation because of a rainstorm on 11th August, 1990. Wang et al. on engineering geology successfully employed the technique of processing fuzzy information to predict exactly the moving tendency of the landslide. They advised the local government to take a series of measures to lower the extent of the landslide based on their prediction, which meant the local government saved more than RMB 40 million (the cost of moving the people out) (Wang, 1996). Although those methods mentioned above were not exactly referred to as information diffusion estimation at that time, they are actually in the frame of the information diffusion theory. The theory was later induced and proposed by Huang and Wang (1995, 1997).	compiler;fuzzy logic;interaction soil-biosphere-atmosphere;iterative method;mean squared error	Xinzhou Wang;Yangsheng You;Yongjing Tang	2004	Int. J. General Systems	10.1080/03081070310001633554	econometrics;mathematical optimization;probability density function;computer science;mathematics;statistics	AI	7.496934455985824	-14.367824079930427	138653
d8f5b6e456ef689b26a75d24b24492ce9ffc4b05	use of the displaced worst compromise in interactive multiobjective programming	multiobjective programming;programmation multiobjectif;general and miscellaneous mathematics computing and information science;decision process decision making management science interactive multiobjective programming linear programming displaced worst compromise reference point;computerized processing;tratamiento informatico;management science linear programming;display devices;linear programming councils cybernetics;prise decision;mathematical logic;reference point;interactive display;algorithme;multiple objective linear programming;algorithm;programacion lineal;dynamics;mechanics;linear programming;preferencia;programmation lineaire;linear program;algorithms;decision process;preference;programming 990210 supercomputers 1987 1989;toma decision;interactive display devices;traitement informatique;management science;algoritmo;programacion multiobjetivo	The development and application of an interactive multiple-objective linear programming algorithm are considered. The displaced worst compromise is used as a reference point, to select the most preferred decision alternative. This point represents the undesired target values, which a decision-maker would prefer to avoid. To reflect the dynamics of a decisionmaker's preferences, use is made of the concept of a displaced point which captures changes of reference during the decision process. An illustrative example is presented to show the process of reaching a final compromise decision. >	multi-objective optimization	Wojtek Michalowski	1988	IEEE Trans. Systems, Man, and Cybernetics	10.1109/21.7497	mathematical optimization;dynamics;mathematical logic;computer science;linear programming;artificial intelligence;machine learning;mathematics;algorithm;display device	Robotics	0.2315355806298279	-16.652196419321037	138742
9b1248a17e5fddbbdc14a4b0a3c3f528910e5bd6	risk management of banking industry in taiwan	capital adequacy ratio;banking industry;default risk;risk management;liquidity;taiwan;market risk;franchise value;insolvency risk	This study focuses upon the factors affecting the risk-taking behaviour of Taiwan’s banking industry during the period from 1994 through 2003, and also analyse separately the differences of new-private and state-owned banks, preand post-New Basel Capital Accord. It further compares the differences of risk-taking behaviour in Taiwan’s banking industry. The results show that the more liquidity, the higher the total risk as well as the market risk. The higher the number of rational investors, franchise value and capital adequacy ratio, the less default risk for Taiwan’s banking industry. Furthermore, the larger the number of rational investors and higher the franchise value, the less market risk and insolvency risk. However, the larger the bank size, the more default risk, market risk and insolvency risk in Taiwan’s banking industry. By implementing the New Basel Capital Accord, for state-owned and new-private banks, the results are consistent. This study infers that the banks in Taiwan are definitely affected by rational investors, franchise value, capital adequacy ratio, size and liquidity.	risk aversion;risk management	Shu Ling Lin	2007	IJSTM	10.1504/IJSTM.2007.013928	risk-weighted asset;liquidity risk;capital adequacy ratio;economics;risk management;risk-adjusted return on capital;finance;economic capital;financial system;market liquidity;market risk;operational risk;financial risk management	AI	2.7832043631780428	-13.713043676667198	138924
f4c4cfed57adcfb68f1713f19360f20bb16c8fcd	a non-linear forecasting system for the ebro river at zaragoza, spain	forecasting;measurement;rivers;water efficient use;specifications;environmental systems;kalman filter;estimation algorithm;system identification;identification;environment;linear model;floods;transformation;parameter estimation;flow measurement;models;systems identification	This paper addresses the problem of modelling and forecasting river flows and levels based on flood routing type models. Though this is generally considered as a non-linear problem, very often it is treated by linear models. A forecasting system is built for the level and flow measurements registered in the Ebro River at the station of Zaragoza (Spain), with the main purpose of preventing floods in an early stage of development. The model takes advantage of the wealth of data available at the Ebro Hydrographical Confederation and is non-linear in essence. The system is obtained by application of system identification tools, starting from a linear specification and relating the parameters of the model estimated to some transformation of the input in the system. Such transformation requires the application of a Kalman Filter in a particular set up and the full estimation algorithm involves an iterative procedure. The model is fully developed on a data set and is thoroughly validated on a different span of data.		Diego J. Pedregal;R. Rivas;Vicente Feliú Batlle;Luis Sánchez;A. Linares-Saez	2009	Environmental Modelling and Software	10.1016/j.envsoft.2008.09.010	transformation;identification;kalman filter;flow measurement;system identification;forecasting;hydrology;linear model;mathematics;natural environment;estimation theory;measurement;statistics	Robotics	8.85400705693612	-13.655179268446958	138956
8f708b7ec2c58c7fcd96ed9d2d5dd974d84d97a5	a generalized model for data envelopment analysis	generalized dea;generic model;production system;decision maker;efficient frontier;data envelopment analysis;relative efficiency;convex cone;decision making unit;convex hull;data envelope analysis;free disposal hull	Data envelopment analysis (DEA) is a method to estimate a relative efficiency of decision making units (DMUs) performing similar tasks in a production system that consumes multiple inputs to produce multiple outputs. So far, a number of DEA models have been developed: The CCR model, the BCC model and the FDH model are well known as basic DEA models. These models based on the domination structure in primal form are characterized by how to determine the production possibility set from a viewpoint of dual form; the convex cone, the convex hull and the free disposable hull for the observed data, respectively.#R##N##R##N#In this study, we suggest a model called generalized DEA (GDEA) model, which can treat the above stated basic DEA models in a unified way. In addition, by establishing the theoretical properties on relationships among the GDEA model and those DEA models, we prove that the GDEA model makes it possible to calculate the efficiency of DMU incorporating various preference structures of decision makers. Furthermore, we propose a dual approach to GDEA, GDEAD and also show that GDEAD can reveal domination relations among all DMUs.	data envelopment analysis	Yeboon Yun;Hirotaka Nakayama;Tetsuzo Tanino	2004	European Journal of Operational Research	10.1016/S0377-2217(03)00140-1	mathematical optimization;operations management;data envelopment analysis;mathematics;mathematical economics	Theory	-2.1653297136404337	-17.811727344042644	139078
e5b05be728345c9c377c1322e08eb77188f6b8d7	many-dimensional observables on łukasiewicz tribe: constructions, conditioning and conditional independence		Probability on collections of fuzzy sets can be developed as a generalization of the classical probability on σ-algebras of sets. A Lukasiewicz tribe is a collection of fuzzy sets which is closed under the standard fuzzy complementation and under the pointwise application of the Lukasiewicz t-norm to countably many fuzzy sets. An observable is a fuzzy set-valued mapping defined on a σ-algebra of sets and satisfying some additional properties; formally, the role of an observable is in a sense analogous to that of a random variable in classical probability theory. This article aims at studying and surveying some properties of observables on a Lukasiewicz tribe of fuzzy sets with a special focus on many-dimensional observables. Namely, the definition and basic construction techniques of observables are discussed. A method for a reasonable construction and interpretation of a joint observable is proposed. Further, the contribution contains results concerning conditioning of observables. We continue in our study [3] of conditional independence in this framework and conclude that semi-graphoid properties are preserved.	observable	Tomás Kroupa	2005	Kybernetika		fuzzy logic;mathematics;mathematical optimization;discrete mathematics;algebra;conditional independence;fuzzy set;type-2 fuzzy sets and systems;random variable;t-norm;membership function;pointwise	Crypto	0.5080698652083147	-20.870945259425383	139343
250f2832f318ff929fc0c013659e587dc6a41f51	a decision support system for cancer differentiation therapy with game-theoretic rough sets		In this paper, we propose a mechanism for cancer cell differentiation therapy based on a game-theoretic rough set model with intuitionistic fuzzy set. We introduce an intuitionistic fuzzy relation with the help of fuzzy compatibility relation. The approximation space classification is controlled with a pair of intuitionistic fuzzy thresholds. It provides a flexibility to set up tolerance level depending on different requirements. The threshold parameters act as players and pay-offs are determined based on the shifting of equivalent classes from boundary region to positive or negative region.	approximation;computational complexity theory;decision support system;experiment;fuzzy set;game theory;information;mike lesser;real life;requirement;rough set	Sibasis Bandyopadhyay;JingTao Yao	2017	2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2017.8122691	machine learning;fuzzy logic;computer science;decision support system;fuzzy set;artificial intelligence;rough set;differentiation therapy	Robotics	-1.4729497734993793	-20.66045185315664	139659
b0f60c82a5ca60d39df9962b4683f6139d027feb	ranking of fuzzy numbers by a new metric	fuzzy regression;fuzzy set;fuzzy numbers;distance measure;fuzzy number;operations research;satisfiability;decision analysis;engineering system;ranking function;metric distance;fuzzy rule base;ranking	In this paper, a new approach for comparison among fuzzy numbers based on new metric distance (DTM) is proposed. All reasonable properties of ranking function are proved. At first, the distance on the interval numbers based on convex hall of endpoints is proposed. The existing distance measures for interval numbers, (Bardossy and Duckstein in Fuzzy rule-based modeling with applications to geophysical, biological and engineering systems. CRC press, Boca Raton, 1995; Diamond in Info Sci 46:141–157, 1988; Diamond and Korner in Comput Math Appl 33:15–32, 1997; Tran and Duckstein in Fuzzy Set Syst 130:331–341, 2002; Diamond and Tanaka Fuzzy regression analysis. In: Slowinski R (ed) Fuzzy sets in decision analysis, operations research and statistics. Kluwer, Boston, pp 349–387, 1998) do not satisfy the properties of a metric distance, while the proposed distance does. It is extended to fuzzy numbers and its properties are proved in detail. Finally, we compare the proposed definition with some of the known ones.	cyclic redundancy check;decision analysis;fuzzy number;fuzzy rule;fuzzy set;mit engineering systems division;operations research;ranking (information retrieval);rule-based modeling	Tofigh Allahviranloo;M. Adabitabar Firozja	2010	Soft Comput.	10.1007/s00500-009-0464-7	fuzzy logic;mathematical optimization;combinatorics;discrete mathematics;membership function;defuzzification;decision analysis;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations;statistics	AI	-0.792410262384552	-21.045499393088097	139719
b37a54ae00b9874654bd5723af4eaaf01088bf0b	for multi-interval-valued fuzzy sets, centroid defuzzification is equivalent to defuzzifying its interval hull: a theorem		In the traditional fuzzy logic, the expert’s degree of certainty in a statement is described either by a number from the interval [0, 1] or by a subinterval of such an interval. To adequately describe the opinion of several experts, researchers proposed to use a union of the corresponding sets – which is, in general, more complex than an interval. In this paper, we prove that for such set-valued fuzzy sets, centroid defuzzification is equivalent to defuzzifying its interval hull. As a consequence of this result, we prove that the centroid defuzzification of a general type-2 fuzzy set can be reduced to the easier-to-compute case when for each x, the corresponding fuzzy degree of membership is convex. 1 Formulation of the Problem Outline of this section. Our main objective is to come up with a centroid defuzzification formula for multi-interval-valued fuzzy sets. Before we start describing our results and algorithms, let us briefly recall why we need centroid defuzzification and why we need multi-interval-valued fuzzy sets. To explain this need: – we will start with the regular fuzzy sets, – then we explain the need for interval-valued fuzzy sets, and – the need for multi-interval-valued fuzzy sets; – finally, we explain the need for centroid defuzzification for all these types of fuzzy sets. Need for interval-valued fuzzy sets: a brief reminder. In the traditional fuzzy logic, an expert describes his or her degree of confidence in different statements by a number from the interval [0, 1]. In particular, for statements like “x is small” corresponding to different values x, the corresponding degree μ(x) form a membership function describing the imprecise (fuzzy) concept like “small”; see, e.g., [1, 6]. In many practical situations, experts are not comfortable describing their degree of confidence by an exact number; they feel more comfortable describing 2 V. Kreinovich and S. Sriboonchitta their degree of confidence by an interval – e.g., by an interval [0.7, 0.8]. In particular, for statements like “x is small”, the corresponding interval-valued degrees of confidence [ μ(x), μ(x) ] form an interval-valued membership function. The intuitive meaning of this membership function is that in principle, we can have many different number-valued membership functions μ(x) as long as μ(x) ∈ [ μ(x), μ(x) ] for every x. Another case when an interval-valued membership function naturally appears is when we ask several experts. For the same value x, different experts give, in general, different degrees of confidence μ1(x), . . . , μn(x). When experts are equally good, there is no reason to select one of these values, it make more sense to consider the interval [ min i μi(x),max i μi(x) ] spanned by these values. This smallest interval containing the values μ1(x), . . . , μn(x) is also known as the interval hull of the corresponding finite set {μ1(x), . . . , μn(x)}. Need for multi-interval-valued fuzzy sets. Once each expert provides his or her degree μi(x) or interval-valued degree [ μ i (x), μi(x) ] , then, instead of taking the interval hull of all these degrees, we can get a more adequate description of the experts’ opinions if we simply take the union of these values and intervals. Such unions are known as multi-intervals. If for each x, the experts’ degrees of confidence in the corresponding statement “x is small” is described by a multiinterval M(x), then we get a multi-interval-valued membership function M(x); see, e.g., [7]. Centroid defuzzification for regular fuzzy sets. In control (or, more generally, decision) applications, when for each possible value x of control, we know the degree μ(x) to which this value is reasonable, we then need to decide which control value c to apply. In fuzzy applications, we usually select the value c for which the weighted mean square deviation from this value is the smallest possible:	algorithm;allen's interval algebra;convex function;defuzzification;emoticon;fuzzy concept;fuzzy logic;fuzzy set;maxima and minima;mean squared error;membership function (mathematics);vladik kreinovich	Vladik Kreinovich;Songsak Sriboonchitta	2016		10.1007/978-3-319-62434-1_17	mathematical optimization;mathematical analysis;discrete mathematics;defuzzification;mathematics	AI	-1.7933035773039383	-22.964637500880016	139730
9edcfea5b8b26af2885ddbaa22f4ac5f59a1877b	on aggregation models leading to valued preference relations	generic model;multi dimensional	Abstract   Various methods have been proposed in the literature for aggregating multi-dimensional preferences into a valued preference relation. Among these methods one can cite ELECTRE III and IV or PROMETHEE; the Condorcet voting method, in a different field of application, can also be viewed as pertaining to this framework. The above mentioned methods generally result in valued preference relations without special desirable properties such as transitivity or acyclicity. Our aim is to describe and characterize a general model where the overal valued preference is obtained through the aggregation of “differences of preferences” along the various viewpoints. The relevance of the model for understanding the principles on which some of the existing methods are built will be discussed.		Marc Pirlot;Denis Bouyssou	1999	Electronic Notes in Discrete Mathematics	10.1016/S1571-0653(04)00041-1	data mining;mathematics	Theory	-3.65198303607232	-22.619394510637143	139858
c0dd0767839d1b66e6bfeb3fcf32418c4f664620	a new approach to measure volatility in energy markets	volatility;power markets;entropy	Several measures of volatility have been developed in order to quantify the degree of uncertainty of an energy price series, which include historical volatility and price velocities, among others. This paper suggests using the permutation entropy, topological entropy and the modified permutation entropy as alternatives to measure volatility in energy markets. Simulated data show that these measures are more appropriate to quantify the uncertainty associated to a time series than those based on the standard deviation or other measures of dispersion. Finally, the proposed method is applied to some typical electricity markets: Nord Pool, Ontario, Omel and four Australian markets.	nonlinear system;randomness;time series;topological entropy;volatility	María del Carmen Ruiz;Antonio Guillamón;Antonio Gabaldón	2012	Entropy	10.3390/e14010074	forward volatility;volatility swap;econometrics;entropy;implied volatility;volatility;volatility smile;volatility risk premium;mathematical economics;stochastic volatility;sabr volatility model;thermodynamics;physics;quantum mechanics	AI	3.364605653502478	-12.32949702057262	139974
4d06c5934979f603ad12dd29abd8f634188f25e5	improvement and evaluation of estimation of time series data of daily life		This paper improves the estimation of the amounts of sewage flow, which is one of daily life data, in order to manage them efficiently. The amounts of flow of a typical day are tried to be adjusted to those of a non-regular day. A typical (non-regular, respectively) day is a non-rainy day having good data and no (a few) outliers. The values for the adjustment are tried to be estimated by using the multiple regression analysis. It is shown that the estimation can be improved, and these values can be estimated by using the temperature of that day, the amount of the rain fall of the previous day, and the day type, which distinguishes a weekday, Saturday, Sunday, and a national holiday. The estimation is tried to be used in estimating the data of a regular day. It is experimentally shown that the estimation works well.		Teruhisa Hochin;Hiroki Nomiya	2017	IJNDC	10.2991/ijndc.2017.5.4.5	time series;regression analysis;statistics;computer science	Visualization	8.372936610473896	-16.47503950435284	140030
8f2f6a00dc29db90d5c4a421d6b07fe59d742cca	flexible selection among objects: a framework based on fuzzy sets	fuzzy set	Up to now, most of the retrieving systems are founded on a Boolean selection mechanism. It appears that this way of doing is not powerful enough to deal with some applications, especially when the size (number) of the results must be controlled. In that case, some kind of flexibility is needed in query expression. In this paper, we suggest the use of a fuzzy sets based approach. The basic principles of this approach are presented and compared to more conventional solutions providing only limited extensions. Moreover, the implementation aspects related to our approach are discussed to show that reasonable performances can be expected.	fuzzy set;performance	Patrick Bosc;M. Galibourg	1988		10.1145/62437.62483	defuzzification;type-2 fuzzy sets and systems;computer science;fuzzy number;machine learning;data mining;fuzzy set;fuzzy set operations;algorithm	PL	-3.1438534306413137	-22.83015656593734	140122
5196643a3ee5cf9d1efaa24968f48c83e4167308	onn-person noncooperative multicriteria games described in strategic form	decision support;multicriteria decision making;noncooperative game theory;noncooperative games;decision support system	This paper undertakes the problem of multicriteria decision support in conflict situations described as a noncooperative game. Construction of such a decision support requires the development of the noncooperative game theory to be generalized for the multicriteria case. New theoretical results in this case are presented. Features of the multicriteria noncooperative games are shown with use of a duopoly model in case of two goods and two criteria of each player. Concepts of the decision support are discussed.		Lech Krus;Piotr Bronisz	1994	Annals OR	10.1007/BF02032669	decision support system;economics;computer science;management science;mathematical economics;welfare economics	ECom	-3.7411058957104513	-16.30182360872283	140202
e15af0da3740e0422531532da3128648ee40c248	fuzzy multi-criteria decision making approach for transport projects evaluation in istanbul	multicriteria analysis;evaluacion proyecto;courant marin;rail transportation;corriente marina;systeme aide decision;multi criteria decision making;decision borrosa;logique floue;logica difusa;decision floue;sistema ayuda decision;prise decision;transport system;tunnels;fuzzy logic;sea current;transporte ferroviaro;decision support system;evaluation projet;delphi method;analisis multicriterio;analyse multicritere;tunnel;project evaluation;transport ferroviaire;toma decision;tunel;fuzzy decision	Istanbul’s current transport systems’ capacity that stays inadequate faced with the uncontrolled growth of population has required the evaluation of several transport projects among which the construction of a new bridge, the construction of an under-water railway tunnel and the improvement of the current sea transport are cited as the most popular ones. This paper presents a robust two-phase fuzzy decision framework, which integrates the fuzzy Delphi method and a hierarchical distance-based fuzzy multi-criteria decision making (MCDM) approach, for evaluating these transport alternatives based on a comprehensive list of quantitative and qualitative performance attributes.	delphi method;two-phase locking;uncontrolled format string	E. Ertugrul Karsak;S. Sebnem Ahiska	2005		10.1007/11424925_33	fuzzy logic;tunel assay;decision support system;program evaluation;delphi method;computer science;artificial intelligence;operations research;statistics	SE	-0.11805948593807153	-15.922974835999874	140236
39aca98b600922c6c7e11c4f0f46ca36d6291362	extension of information axiom from ordinary to intuitionistic fuzzy sets: an application to search algorithm selection	information axiom;uninformed search algorithms;intuitionistic fuzzy sets;ordinary fuzzy sets	Information axiom aims at minimizing the information content of the design in order to determine the best alternative satisfying the required design characteristics. In the literature, information axiom has been extended to fuzzy environment in order to capture impreciseness and vagueness in decision making problems. In the scope of this study, the ordinary fuzzy information axiom has been extended to intuitionistic fuzzy sets which attempt defining a fuzzy set with its membership, non-membership and hesitance. Thus, reflection of decision makers’ hesitancy in information axiom has been better provided. In this paper, to illustrate the applicability of the proposed approach, triangular intuitionistic fuzzy information axiom is applied to uninformed search algorithm selection problem including unweighted, weighted, and multi-expert evaluation cases. The obtained results show the validity and efficiency of the proposed model. One-at-a-time sensitivity analysis is also applied to reveal the robustness of the given decision.	algorithm selection;brute-force search;fuzzy set;intuitionistic logic;search algorithm;selection algorithm;self-information;vagueness	Cengiz Kahraman;Sezi Çevik Onar;Selçuk Çebi;Basar Öztaysi	2017	Computers & Industrial Engineering	10.1016/j.cie.2016.12.012	decision model;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;mathematics;fuzzy set operations;algorithm	AI	-2.808813292151248	-21.254889934071578	140255
11c2ce2dc8b346e4f324aa272ea732b93041e0ff	commutativity as prior knowledge in fuzzy modeling	mesure floue;fuzzy system models;fuzzy measure;conmutatividad;fuzzy rules;prior knowledge;satisfiability;integration;integracion;connaissance a priori;sistema difuso;fuzzy measures;systeme flou;commutativity;62p12;28xx;fuzzy system;fuzzy model;commutativite	This paper faces with the integration of mathematical properties satisfied by the system as prior knowledge in fuzzy modeling (FM), focusing on the commutativity as a starting point. The underlying idea is to reward the rules in each input fuzzy region that provide good commutativity degrees respecting its complementary —commutatively related— input fuzzy region. With this aim, the similarity between the outputs in both regions will be obtained. The experimental results show the accuracy improvement gained by the proposed method.	algorithmic efficiency;complex systems;computation;fm broadcasting;information	Pablo Carmona;Juan Luis Castro;Jose Manuel Zurita	2005	Fuzzy Sets and Systems	10.1016/j.fss.2004.11.004	computer science;artificial intelligence;mathematics;commutative property;fuzzy set operations;algorithm;fuzzy control system;satisfiability	AI	2.222283828298036	-22.731893765891957	140296
7d668480e3b86bb54d11e2d76966f9b959dc6b7a	on some methods for performance ranking and correspondence analysis in the dea context	modelizacion;evaluation performance;analisis envolvimiento datos;analyse amas;entrada salida;decomposition valeur singuliere;performance evaluation;dea performance ranking performance correspondence;systeme aide decision;evaluacion prestacion;singular value decomposition;performance comparison;prise de decision;sistema ayuda decision;dea;input output;modelisation;decision support system;hierarchical classification;cluster analysis;estudio caso;data envelopment analysis;correspondence analysis;performance correspondence;etude cas;classification hierarchique;analisis cluster;decomposicion valor singular;decision making unit;toma decision;modeling;clasificacion jerarquizada;entree sortie;analyse enveloppement donnee;performance ranking	Two novel methods named performance baseline and performance correspondence matrices are proposed to evaluate the performance of decision making units (DMUs) based on the techniques of singular value decomposition (SVD). The performance baseline matrix can be used to rank all the DMUs because it provides a common basis for performance comparison. The performance correspondence matrix can be used to conduct performance cluster analysis, with which to explore the structure of input/output variables that are associated with DMUs. The analysis can reveal the performance difference of the DMUs and the key input/output variables determining the efficiency of a certain DMU, and provides valuable quantitative information for adjusting variables to improve efficiency of the DMU. Three case studies are presented to demonstrate that the proposed methods in this work are effective and easy to use and can provide insights into proper selection of input/output variables for performance comparison to avoid over manipulating DEA models in practice.	correspondence analysis	Chi-Ming Tsou;Deng-Yuan Huang	2010	European Journal of Operational Research	10.1016/j.ejor.2009.09.010	input/output;econometrics;systems modeling;decision support system;computer science;data mining;data envelopment analysis;mathematics;cluster analysis;correspondence analysis;singular value decomposition;operations research	Vision	-0.6211735643256944	-15.715446956550384	140313
57dd2539b1f34c5d44a82071af6a7e762b0bd1d0	an interactive fuzzy satisficing method for multiobjective block angular linear programming problems with fuzzy parameters	multiobjective programming;optimum pareto;programmation multiobjectif;fuzzy parameter;fuzzy programming;fuzzy number;metodo descomposicion;fuzzy objective;level set;methode decomposition;interactive method;decision maker;objective function;parametre flou;decomposition method;large scale;programacion lineal;membership function;linear programming;programmation lineaire;linear program;objectif flou;programmation floue;escala grande;lsmolp;pareto optimal solution;large scale multiobjective linear programming;pareto optimum;methode interactive;optimo pareto;pareto optimality;programacion difusa;echelle grande;programacion multiobjetivo	Abstract   In this paper, by considering the experts’ imprecise or fuzzy understanding of the nature of the parameters in the problem-formulation process, large-scale multiobjective block-angular linear programming problems involving fuzzy parameters characterized by fuzzy numbers are formulated. Using the   α  -level sets of fuzzy numbers, the corresponding nonfuzzy   α  -programming problem is introduced. The fuzzy goals of the decision maker for the objective functions are quantified by eliciting the corresponding membership functions including nonlinear ones. Through the introduction of an extended Pareto optimality concept, if the decision maker specifies the degree   α   and the reference membership values, the corresponding extended Pareto optimal solution can be obtained by solving the minimax problems for which the Dantzig–Wolfe decomposition method is applicable. Then a linear programming-based interactive fuzzy satisficing method for deriving a satisficing solution for the decision maker efficiently from an extended Pareto optimal solution set is presented along with an illustrative numerical example.	angularjs;linear programming	Masatoshi Sakawa;Kosuke Kato	2000	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00452-7	decision-making;mathematical optimization;discrete mathematics;decomposition method;membership function;defuzzification;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;linear programming;level set;fuzzy number;machine learning;goal programming;mathematics;fuzzy set;fuzzy set operations	AI	-0.38944306524964245	-16.845971326698447	140575
84fb1687f38364d842b370928fcb6a7d703b16f8	operators on intuitionistic fuzzy relations	electronic mail;lattices;uncertainty;fuzzy sets;yttrium;yttrium electronic mail decision making lattices fuzzy sets uncertainty	In the paper properties of intuitionistic fuzzy relations are considered and preservation of some properties by operations, including lattice operations, composition and related operators are studied. Properties of intuitionistic fuzzy relations, namely reflexivity, irreflexivity, connectedness, symmetry, antisymmetry, perfect antisymmetry and transitivity are considered. Moreover, the authors study assumptions under which intuitionistic fuzzy preference relations and related operators fulfil these properties.	antisymmetry;fuzzy logic;fuzzy set;intuitionistic logic;unary operation;vertex-transitive graph	Barbara Pekala;Urszula Bentkowska;Humberto Bustince;Javier Fernández;Mikel Galar	2015	2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2015.7337956	combinatorics;discrete mathematics;uncertainty;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;yttrium;lattice;mathematics;fuzzy set;fuzzy set operations;statistics	SE	-1.2220795118921215	-23.138263317922643	140738
4bac7d71b7ce6ca6faa5dc93808af0849fc3ec30	uniform fuzzy relations and fuzzy functions	complete residuated lattice;engineering;fuzzy equivalence;fuzzy set;procesamiento informacion;control difusa;perfect fuzzy function;fuzzy control;fuzzy relation;conjunto difuso;ensemble flou;raisonnement;ingenierie;approximate reasoning;particion;partial fuzzy function;information processing;razonamiento;partition;uniform fuzzy relation;ingenieria;sistema difuso;residuated lattice;06bxx;systeme flou;reasoning;traitement information;fuzzy system;fuzzy partition;commande floue	In this paper we introduce and study the concepts of a uniform fuzzy relation and a (partially) uniform F-function. We give various characterizations and constructions of uniform fuzzy relations and uniform F-functions, we show that the usual composition of fuzzy relations is not convenient for F-functions, so we introduce another kind of composition, and we establish a mutual correspondence between uniform F-functions and fuzzy equivalences. We also give some applications of uniform fuzzy relations in approximate reasoning, especially in fuzzy control, and we show that uniform fuzzy relations are closely related to the defuzzification problem. © 2008 Elsevier B.V. All rights reserved.	approximation algorithm;defuzzification;fuzzy control system	Miroslav Ciric;Jelena Ignjatovic;Stojan Bogdanovic	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.07.006	partition;fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;reason;algorithm;fuzzy control system	AI	1.2030431951909724	-22.68146893212412	140800
c5414584f617895933985dbdf719c27a1f558f89	measuring inequality and segregation		In this paper, I introduce the Divergence Index, a conceptually intuitive and methodologically rigorous measure of inequality and segregation. The index measures the difference between a distribution of interest and another empirical, theoretical, or normative distribution. The Divergence Index provides flexibility in specifying a theoretically meaningful basis for evaluating inequality. It evaluates how surprising an empirical distribution is given a theoretical distribution that represents equality. I demonstrate the unique features of the new measure, as well as deriving its mathematical equivalence with Theil’s Inequality Index and the Information Theory Index. I compare the dynamics of the measures using simulated data, and an empirical analysis of racial residential segregation in the Detroit, MI, metro area. The Information Theory Index has become the gold standard for decomposition analyses of segregation. I show that although the Information Theory Index can be decomposed for subareas, it is misleading to interpret the results as segregation. The Divergence Index addresses the limitations of existing measures and accurately decomposes segregation across contexts and nested levels of geography. By creating an alternative measure, I provide a distinct lens, which enables richer, deeper, more accurate understandings of inequality and segregation. ∗Thank you to Richard Breen, Scott Page, and Russell Golman for their valuable feedback. An earlier version of this paper was included as Essay 1 in: Roberto, Elizabeth. 2015. “The Boundaries of Spatial Inequality: Three Essays on the Measurement and Analysis of Residential Segregation.” PhD thesis, Yale University. †Department of Sociology, Princeton University, Princeton, NJ, USA 1 ar X iv :1 50 8. 01 16 7v 1 [ st at .M E ] 5 A ug 2 01 5 Social inequality is a concept that describes the uneven distribution of resources, life conditions, opportunities, or outcomes across individuals, groups, or social classes. A variety of measures seek to answer a seemingly simple question: how unequal is the distribution? All measures of inequality have an implied or explicit notion of equality (Coulter 1989), such as uniformity across individuals, maximal diversity of groups, or randomly occurring events. They evaluate the degree of inequality in a distribution by measuring it against a comparative reference. For example, consider studying gender inequality across academic majors at a university. If the student population is 75% women and 25% men, and among engineering majors 25% are women and 75% are men, is the major segregated? To measure inequality in terms of diversity, we can use the gender diversity of the university as the comparative reference that represents equality. Although the relative proportion of men and women differs in the engineering major and the overall student population, both have a 3 to 1 mix of genders. Since the major has the same level of gender diversity as the university, we would conclude that it is not segregated. However, the gender proportions within the engineering major are unexpected given the university context. Men are over-represented and women are under-represented relative to their overall proportions at the university. Rather than comparing gender diversity, we can measure inequality as the difference between the actual proportion of each gender in the engineering major and the overall student population. Equality is defined by the university’s gender distribution – a major that has the same gender distribution as the university is not segregated. Given the striking difference between the gender proportions of the engineering major and the university, we would conclude that the major is segregated. A comparative reference is typically hard wired into an inequality measure, and it is often not transparent without inspecting the mathematics that underly the measure. As a result, choosing a particular measure also establishes a theoretical definition of equality. Measures should be selected and evaluated with this in mind. A measure’s comparative reference affects assessments of whether one distribution is more or less unequal than another, and thus has important implications for our understanding of inequality. To quote Allison: The decision to rank one distribution as more unequal than another has theoretical as well as methodological implications. In fact, the choice of an inequality measure is properly regarded as a choice among alternative definitions of inequality rather than a choice among alternative ways of measuring a single theoretical construct (Allison 1978:865). The aim of this paper is to improve upon existing measures of inequality and segregation by proposing a new measure of inequality: the Divergence Index. It is derived from an information theoretic measure, relative entropy. The index measures the difference between a distribution of interest and another empirical, theoretical, or normative distribution. In contrast to other measures, the comparative reference is not fixed, it can be specified in a theoretically meaningful way. It is particularly useful for comparing inequality over time, place, or cohort, across counterfactual scenarios, or against a normative standard. I will show that the divergence index is both conceptually intuitive and methodologically rigorous. I will make a normative claim that the Divergence Index is a better measure of inequality and segregation. That, of course, need not be true for the index to be of value. By creating an alternative measure, I provide a distinct lens, which enables richer, deeper, more accurate understandings of inequality and segregation. I begin by describing four types of inequality measures. I then review the desirable properties of inequality measures, as identified in previous research. I describe three existing measures – the Dissimilarity Index, Theil’s Inequality Index, and the Information Theory Index – and summarize their desirable properties and limitations. Next, I introduce my new measure – the Divergence Index – and evaluate it against the same criteria. I demonstrate the unique features of the new measure, as well as deriving the mathematical equivalence between the Divergence Index and Theil’s Inequality Index and the Information Theory Index. Finally, I compare the dynamics of the measures using simulated and empirical data.	circuit complexity;counterfactual conditional;information theory;kullback–leibler divergence;maximal set;mind;population;randomness;social inequality;theil index;theoretical definition;turing completeness;utility functions on indivisible goods	Elizabeth Roberto	2015	CoRR		econometrics;generalized entropy index;mathematics;statistics	ML	-1.1503237750079909	-12.482010739690637	140972
e3496d7d7ff0c3798e2ce9ed9c36e9a6ac7afe07	pythagorean fuzzy interaction muirhead means with their application to multi-attribute group decision-making		Due to the increased complexity of real decision-making problems, representing attribute values correctly and appropriately is always a challenge. The recently proposed Pythagorean fuzzy set (PFS) is a powerful and useful tool for handling fuzziness and vagueness. The feature of PFS that the square sum of membership and non-membership degrees should be less than or equal to one provides more freedom for decision makers to express their assessments and further results in less information loss. The aim of this paper is to develop some Pythagorean fuzzy aggregation operators to aggregate Pythagorean fuzzy numbers (PFNs). Additionally, we propose a novel approach to multi-attribute group decision-making (MAGDM) based on the proposed operators. Considering the Muirhead mean (MM) can capture the interrelationship among all arguments, and the interaction operational rules for PFNs can make calculation results more reasonable, to take full advantage of both, we extend MM to PFSs and propose a family of Pythagorean fuzzy interaction Muirhead mean operators. Some desirable properties and special cases of the proposed operators are also investigated. Further, we present a novel approach to MAGDM with Pythagorean fuzzy information. Finally, we provide a numerical instance to illustrate the validity of the proposed model. In addition, we perform a comparative analysis to show the superiorities of the proposed method.	aggregate data;forward secrecy;fuzzy logic;fuzzy set;norm (social);numerical analysis;qualitative comparative analysis;vagueness	Yuan Xu;Xiaopu Shang;Jun Wang	2018	Information	10.3390/info9070157	machine learning;fuzzy logic;artificial intelligence;inequality;computer science;group decision-making;fuzzy set;operator (computer programming);pythagorean theorem;fuzzy number;vagueness	AI	-3.1562321990431292	-20.906816229066628	141014
422769ed678da3560ffae44ee2acf3e0c8ff005d	combination of evidence with different weighting factors: a novel probabilistic-based dissimilarity measure approach	invalidation problem;期刊论文;dempster shafer theory	To solve the invalidation problem of Dempster-Shafer theory of evidence (DS) with high conflict in multisensor data fusion, this paper presents a novel combination approach of conflict evidence with different weighting factors using a new probabilistic dissimilarity measure. Firstly, an improved probabilistic transformation function is proposed to map basic belief assignments (BBAs) to probabilities. Then, a new dissimilarity measure integrating fuzzy nearness and introduced correlation coefficient is proposed to characterize not only the difference between basic belief functions (BBAs) but also the divergence degree of the hypothesis that two BBAs support. Finally, the weighting factors used to reassign conflicts on BBAs are developed and Dempster’s rule is chosen to combine the discounted sources. Simple numerical examples are employed to demonstrate the merit of the proposedmethod.Through analysis and comparison of the results, the new combination approach can effectively solve the problem of conflict management with better convergence performance and robustness.	coefficient;numerical analysis	Mengmeng Ma;Ji-yao An	2015	J. Sensors	10.1155/2015/509385	dempster–shafer theory;machine learning;data mining;mathematics;statistics	AI	-3.954437403872062	-20.700379689096405	141122
4c38acc2aaca4d4e63b0be9fa377af19f5328228	comparative evaluation of genetic algorithm and backpropagation for training neural networks	interpolation;search algorithm;evolutionary programming;chaotic time series;global search algorithms;backpropagation;ease of use;neural network training;epoch;financial market;genetic algorithm;genetic algorithms;neural network	In view of several limitations of gradient search techniques (e.g. backpropagation), global search techniques, including evolutionary programming and genetic algorithms (GAs), have been proposed for training neural networks (NNs). However, the eectiveness, ease-of-use, and eciency of these global search techniques have not been compared extensively with gradient search techniques. Using ®ve chaotic time series functions, this paper empirically compares a genetic algorithm with backpropagation for training NNs. The chaotic series are interesting because of their similarity to economic and ®nancial series found in ®nancial markets. Ó 2000 Published by Elsevier Science Inc. All rights reserved.	artificial neural network;backpropagation;central processing unit;computation;david b. fogel;evolutionary programming;genetic algorithm;gradient;nonlinear system;open road tolling;radial (radio);radial basis function;simulated annealing;software release life cycle;tabu search;time series	Randall S. Sexton;Jatinder N. D. Gupta	2000	Inf. Sci.	10.1016/S0020-0255(00)00068-2	mathematical optimization;genetic algorithm;computer science;artificial intelligence;machine learning;artificial neural network	AI	7.454688243506507	-20.808998167413623	141225
5c7eb316ae593046d730763c0bb6797e3714fc57	"""dealing with the """"don't know"""" answer in risk assessment"""	aggregation;owa—ordered weighted averaging;ofnwa—ordered fuzzy numbers weighted averaging.;risk assessment;decision making;fuzzy numbers;uncertainty;fuzzy number;incomplete information	Decision making often deals with incomplete and uncertain information. Uncertainty concerns the level of confidence associated with the value of a piece of information, while incompleteness derives from the unavailability of data. Fuzzy numbers capture the uncertainty of information, but they are not able to explicitly represent incompleteness. In this paper we discuss an extension of fuzzy numbers, called fuzzy numbers with indeterminateness, and show how they can be used to model decision process involving incomplete information. In particular, the paper focuses on the “Don’t Know” answer to questionnaires and develops an aggregation model that accounts for these type of answers. The main contribution lies in the formalization of the interrelationships between the risk of a decision and the incompleteness of the information on which it is made.	convex function;fuzzy number;object composition;primer;property (philosophy);risk assessment;unavailability	Gerardo Canfora;Luigi Troiano	2003			complete information;weighted product model;unavailability;data mining;weighted sum model;computer science;confidence interval;risk assessment;machine learning;artificial intelligence;fuzzy number	AI	-0.5468910613349892	-18.748899714360213	141347
d1fcad8dd2c6cefd0bd8c38878ca71716da3c5c1	player retention in league of legends: a study using survival analysis		Multi-player online esports games are designed for extended durations of play, requiring substantial experience to master. Furthermore, esports game revenues are increasingly driven by in-game purchases. For esports companies, the trends in players leaving their games therefore not only provide information about potential problems in the user experience, but also impacts revenue. Being able to predict when players are about to leave the game - churn prediction - is therefore an important solution for companies in the rapidly growing esports sector, as this allows them to take action to remedy churn problems.  The objective of the work presented here is to understand the impact of specific behavioral characteristics on the likelihood of a player continuing to play the esports title League of Legends. Here, a solution to the problem is presented based on the application of survival analysis, using Mixed Effects Cox Regression, to predict player churn. Survival Analysis forms a useful approach for the churn prediction problem as it provides rates as well as an assessment of the characteristics of players who are at risk of leaving the game. Hazard rates are also presented for the leading indicators, with results showing that duration between matches played is a strong indicator of potential churn.	behavioral pattern;design rationale;download;hazard symbol;kerrison predictor;lol;proportional hazards model;purchasing;rework (electronics);user experience;virtual world	Simon Demediuk;Alexandra Murrin;David Bulger;Michael Hitchens;Anders Drachen;William L. Raffe;Marco Tamassia	2018		10.1145/3167918.3167937	proportional hazards model;business;user experience design;league;survival analysis;marketing;business intelligence;economic indicator;revenue	ML	-1.7858626435949785	-10.664623666817604	141400
149bb90e0ce6d6d45945325ded8d2f0de85a62f5	a fuzzy rule-based trading agent: analysis and knowledge extraction	online learning;knowledge extraction;time se- ries data;future trade;fuzzy systems;decision support;fuzzy system;time series;decision making process;statistical test;knowledge base;profitability	 In this paper, we show how a fuzzy rule-based system is developed for trading in a futures market. By our fuzzy rule-based system, an agent determines whether it should buy a futures spot or not based on the time series of both the spot and the futures prices. The fuzzy rule-based system is fine-tuned so that the amount of profit is maximized. Since a fuzzy system is used as a decision making tool, the decision making process by the learning agent can be linguistically interpreted. The performance of the fuzzy rule-based system is evaluated in a virtual stock market.We also try to extract a knowledge base from the fuzzy rule-based system after it is fully trained. Statistical test shows the effectiveness of the extracted knowledge as a human decision support.	fuzzy rule	Tomoharu Nakashima;Takanobu Ariyama;Hiroko Kitano;Hisao Ishibuchi	2005		10.1007/10966518_19	knowledge extraction;decision-making;artificial intelligence;business;knowledge base;data mining;profitability index;machine learning;stock market;fuzzy rule;fuzzy control system;decision support system	AI	3.975881644717558	-18.710946886270165	141423
d9333967c0c82e6a36c034fcae39eeda6ec28d46	investing in real-world equity markets with an ahp-based decision framework	behavioral-judgmental perspective;system;decision support system;bounded rationality;decision problem;analytic hierarchy process	A decision framework based on the Analytic Hierarchy Process is presented to evaluate and select equity portfolios under a given investment horizon. It is shown that a hierarchical structure can readily accommodate the complexity and information-computation constraints of this decision problem in real-world stock markets, ranging from a manydimensional investment environment, diverse asset characteristics, individual preferences, perceptions, judgments, expectations, and experience, and multiple qualitative or quantitative criteria. Once the weights obtained from pairwise comparisons in each level of the hierarchy are synthesized with reference to the decision objective, a rationally-determined portfolio emerges through the correspondingly prioritized allocation of equities over a given wealth. Our decision framework offers a flexible and readily applicable addition to the financial practitioner’s menu of equity selection techniques, and suggests fruitful extensions to optimize the use of knowledge under ...		Michael Tow Cheung;Ziqi Liao	2009	Journal of Decision Systems	10.3166/jds.18.1149-163	inversion;judgment;stock exchange;analytic hierarchy process;decision support system;economics;rationality;investment;computer science;artificial intelligence;operations management;decision problem;system;management;perception;welfare economics;equity;financial market;hierarchy;bounded rationality	ECom	-3.942271577482033	-13.607939202650236	141433
c5cf72f4fceca58b295f9e3d750d9ec9b85ab675	applications of the fuzzy weighted average of fuzzy numbers in decision making models	fuzzy number	The paper deals with the operation of fuzzy weighted average of fuzzy numbers. The operation can be applied to the aggregation of partial fuzzy evaluations in fuzzy models of multiple criteria decision making and to the computation of expected fuzzy evaluations of alternatives in discrete fuzzy-stochastic models of decision making under risk. Normalized fuzzy weights figuring in the operation have to form a special structure of fuzzy numbers; its properties will be studied and the practical procedures for setting them will be proposed. A fuzzy weighted average of fuzzy numbers with normalized fuzzy weights will be defined, an effective algorithm of its calculation will be described, and its uncertainty will be studied.	algorithm;computation;decision theory;fuzzy logic;fuzzy number;stochastic process	Ondrej Pavlacka;Jana Talasová	2007			fuzzy logic;fuzzy set operations;fuzzy measure theory;ordered weighted averaging aggregation operator;machine learning;mathematical optimization;defuzzification;mathematics;fuzzy number;membership function;fuzzy classification;artificial intelligence	AI	-1.7496570770472997	-20.984892413355293	141473
0f7307db78d504ec258ee3360d745b1e4309feeb	towards predicting first daily departure times: a gaussian modeling approach for load shift forecasting		This work provides two statistical Gaussian forecasting methods for predicting First Daily Departure Times (FDDTs) of everyday use electric vehicles. This is important in smart grid applications to understand disconnection times of such mobile storage units, for instance to forecast storage of non dispatchable loads (e.g. wind and solar power). We provide a review of the relevant state-of-the-art driving behavior features towards FDDT prediction, to then propose an approximated Gaussian method which qualitatively forecasts how many vehicles will depart within a given time frame, by assuming that departure times follow a normal distribution. This method considers sampling sessions as Poisson distributions which are superimposed to obtain a single approximated Gaussian model. Given the Gaussian distribution assumption of the departure times, we also model the problem with Gaussian Mixture Models (GMM), in which the priorly set number of clusters represents the desired time granularity. Evaluation has proven that for the dataset tested, low error and high confidence (≈ 95%) is possible for 15 and 10 minute intervals, and that GMM outperforms traditional modeling but is less generalizable across datasets, as it is a closer fit to the sampling data. Conclusively we discuss future possibilities and practical applications of the discussed model.	approximation algorithm;artificial intelligence;computer cluster;gaussian (software);gaussian elimination;google map maker;mixture model;point of interest;propagation of uncertainty;sampling (signal processing);software propagation	Nicholas H. Kirk;Ilya Dianov	2015	CoRR		econometrics;simulation;computer science;machine learning;statistics	ML	9.238166849037807	-16.318682281636182	141712
690651d391bcb32b4d0e3104d6e89aa469a9af48	the impact of golden cross and death cross frequency on stock returns in pre- and post-financial crisis		Technical analysis in stock market seems to be helpful for investors to survive the financial crisis. To verify this point, we try to explore whether the moving average (MA) strategy is related to the investment performance. Here we apply the Golden Cross (Death Cross), a technical indicator signalling the time to buy (sell) when the short term moving average crosses above (below) the long term moving average, to 400 listed firms in the Taiwan stock market based on the approach of [8]. We find that Golden Cross (Death Cross) is positively (negatively) related to stock returns. However, the occurrences of Golden Cross and Death Cross before and after financial crisis are not significantly related to stock returns.		Yung-Shun Tsai;Chun-Ping Chang;Shyh-Weir Tzang	2017		10.1007/978-3-319-61542-4_70	computer science;finance;computer security;financial crisis;moving average;signalling;stock market;technical indicator;technical analysis;investment performance	ML	3.1412908291857717	-13.692974423844404	141862
96dc4229e8dfb08b265100024ed0b9e666434fb7	fuzzy analysis and prediction of commit activity in open source software projects	commit activity prediction;arima model;oss projects;software evolution prediction;time variant difference parameter;autoregressive integrated moving average models;fuzzy analysis;open source software projects;fuzzy time series based prediction method;development community	Autoregressive integrated moving average (ARIMA) models are the most commonly used prediction models in the previous studies on software evolution prediction. This study explores a prediction method based on fuzzy time series for predicting the future commit activity in open source software (OSS) projects. The idea to choose fuzzy time series based prediction method is due to the stochastic nature of the OSS development process. Commit activity of OSS project indicates the activeness of its development community. An active development community is a strong contributor to the success of OSS project. Therefore commit activity prediction is an important indicator to the project managers, developers, and users regarding the evolutionary prospects of the project in future. The fuzzy time series-based prediction method is of order three and uses time variant difference parameter on the current state to forecast the next state data. The performance and suitability of computational method are examined in comparison with that of ARIMA model on a data set of seven OSS systems. It is found that the predicted results of the computational method outperform various ARIMA models. Towards the end, a commit prediction model is used for each project to analyse the trends in their commit activity.	open-source software	Munish Saini;Kuljit Kaur Chahal	2016	IET Software	10.1049/iet-sen.2015.0087	autoregressive integrated moving average;systems engineering;engineering;data science;data mining	SE	6.558202187816305	-18.478013415367613	141864
8a9012faca51e22781ef11c2635d36c86070f163	seismic damage prediction of multistory building using gis and artificial neural network	seismic prediction;building;geographic information system;seismology;neural nets;earthquake damage;structure vulnerability;artificial neural networks earthquakes buildings geographic information systems neurons acceleration cities and towns;ann;spatial analysis seismic damage prediction multistory building integrated gis artificial neural network geographic information system earthquake damage ann earthquake intensity peak acceleration value;geographic information system gis;integrated gis;earthquakes;acceleration;artificial neural networks;geographic information systems;cities and towns;peak acceleration value;seismic damage prediction;neurons;spatial analysis;seismology building earthquakes geographic information systems neural nets;geographic information system gis ann structure vulnerability seismic prediction;buildings;earthquake intensity;artificial neural network;multistory building	An integrated GIS and Artificial neural network analysis model for earthquake-damaged, which couples geographic information systems(GIS) with artificial neural networks (ANN) to predict the seismic damage to multistory buildings based on earthquake intensity and adopt the peak acceleration value, is presented here. ANN is used to learn the patterns of development in the region and test the predictive capacity of the model, while GIS is used to develop the spatial, and perform spatial analysis on the results. The ANN combined with GIS was found to have a great potential to predict seismic damage.	algorithm;artificial neural network;decision support system;geographic information system;spatial analysis	Jun-Jie Wang;Hui-Ying Gao;Ming-Qiong Liu	2010	2010 Sixth International Conference on Natural Computation	10.1109/ICNC.2010.5584603	structural engineering;seismology;geography;geotechnical engineering	Robotics	9.65704873074817	-19.24024142045755	142036
f0a14d225ef71b2f78b2dc1f56891a0d50a4759e	study on resident behaviors of selecting saving bank	bank;banking;mathematics;information science;resource allocation;information technology;resource management;biological system modeling;site selection;portfolios;attractor;psychology;data mining;inspection;fuzzy set theory;deposit fuzzy mathematics attractor optimal site selection;resource allocation banking fuzzy set theory;fuzzy membership grade;data envelopment analysis;resident saving allocation;optimal site selection;mathematical model;fuzzy mathematics;deposit;economics;mathematical model information technology mathematics psychology gaussian distribution information science data envelopment analysis banking inspection portfolios;gaussian distribution;fuzzy membership grade bank fuzzy mathematics resident saving allocation	The location of a bank has a direct effect on saving behavior of residents. Study on the relations between the allocation of the deposit and the locations of the banks have an urgent meaning. To this end, fuzzy mathematics is used. The allocation of resident savings in each bank is expressed as a fuzzy subset, and each bank is given a attractor, which denotes the capability of capturing deposit within surrounding areas, and based on the attractor, fuzzy membership grade for each fuzzy subset are calculated. Then the results are normalized to ensure that every deposit is not done in two different banks. A real case has been checked with above model and it showed a good performance, and this model could be used as guidance for optimal site selection of bank.			2009		10.1109/IFITA.2009.535	actuarial science;economics;operations management;data mining	AI	-2.5535090122291835	-14.84093610100699	142047
77ce2c40c7ee2557a9581a7e41ef6b5ff0a87bab	preference-based performance measures for time-domain global similarity method		For Time−Domain Global Similarity (TDGS) method, which transforms the data cleaning problem into a binary classification problem about the physical similarity between channels, directly adopting common performance measures could only guarantee the performance for physical similarity. Nevertheless, practical data cleaning tasks have preferences for the correctness of original data sequences. To obtain the general expressions of performance measures based on the preferences of tasks, the mapping relations between performance of TDGS method about physical similarity and correctness of data sequences are investigated by probability theory in this paper. Performance measures for TDGS method in several common data cleaning tasks are set. Cases when these preference−based performance measures could be simplified are introduced. 1Corresponding author.	binary classification;correctness (computer science);global variable;plasma cleaning	Ting Lan;Jian Liu;Hong Qin	2017	CoRR	10.1088/1748-0221/12/12/C12008	particle physics;physics;probability theory;binary classification;correctness;time domain;expression (mathematics);artificial intelligence;pattern recognition;communication channel	DB	-4.443779492665834	-21.75404495427714	142093
89ac24207a1bf67d3398561d2a0fd59b068ac64f	a particle swarm optimization based algorithm for fuzzy bilevel decision making with constraints-shared followers	fuzzy set;fuzzy sets;decision problem;particle swarm optimizer;particle swarm optimization;bilevel multiple follower programming;conference proceeding	In a bilevel decision problem, decision making may involve multiple followers and fuzzy demands. This research focuses on the problem of fuzzy linear bilevel decision making with multiple followers who share common constraints (FBCSF). Based on the ranking relationship among fuzzy sets defined by cut set and satisfactory degree α, a FBCSF model is presented and a particle swarm optimization based algorithm is developed. The experiments reveal that solutions obtained by this algorithm are reasonable and stable.	algorithm;cut (graph theory);decision problem;experiment;fuzzy set;mathematical optimization;particle swarm optimization	Ya Gao;Guangquan Zhang;Jie Lu	2009		10.1145/1529282.1529519	mathematical optimization;multi-swarm optimization;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;fuzzy set;bilevel optimization;fuzzy set operations	AI	-1.7006405796955766	-18.751800714737254	142110
1ed40ac3e903ae84f4f4d52f88bd36388aee7a8f	composite of support vector regression and evolutionary algorithms in car-rental revenue forecasting	holt winters hw model;evolutionary computation;car rental industry;revenue management;support vector regression svr;support vector machines;immune algorithm;pricing;support vector regression;performance index;time series;indexing terms;time series evolutionary computation pricing regression analysis rental support vector machines;seasonality;regression analysis;profitability;rental;evolutionary algorithm;immune algorithm ia;seasonal holt winters shw model;immune algorithm support vector regression evolutionary algorithms car rental revenue forecasting revenue management monthly revenue per unit benchmarking performance index annual pricing nonlinear regression time series problems;nonlinear regression;revenue management support vector regression svr immune algorithm ia holt winters hw model seasonal holt winters shw model car rental industry	In the car-rental industry, it is generally assumed that a 1% increase in price contributes an 8% profit increment. Therefore, pricing is of priority concern in revenue management. Accurately forecasting monthly revenue per unit (RPU) (per car) has received increasing attention owing to its ability to provide a benchmarking performance index for annual pricing. However, RPU in the car-rental industry suffers serious nonlinearity. Recently, support vector regression (SVR) has been successfully applied to solve nonlinear regression and time series problems. This study elucidates the feasibility of using SVR to forecast RPU. Moreover, the parameters of a SVR model are derived using the immune algorithm (IA). Subsequently, RPU data from DTAG (Dollar Thrifty Automotive Group, USA) are used to illustrate the proposed SVRIA (support vector regression with immune algorithm) model. The empirical results reveal that the proposed model outperforms the other two models, namely the Holt-Winters (HW) model and the seasonal HW (SHW) model. Consequently, the SVRIA model provides a promising alternative method of forecasting RPU. Index Terms-Support vector regression (SVR); immune algorithm (IA); Holt-Winters (HW) model; seasonal Holt-Winters (SHW) model; car-rental industry; revenue management.	beverton–holt model;complexity economics;evolutionary algorithm;nonlinear system;seasonality;support vector machine;time series	Wei-Chiang Hong;Young-Jou Lai;Ping-Feng Pai;Shao-Lun Lee;Shun-Lin Yang	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424836	support vector machine;simulation;computer science;machine learning;evolutionary algorithm;operations research;evolutionary computation	ML	6.703700287064336	-18.502131061155346	142127
306d46e9752553036675a2c29fdc7b8f00567009	genetically engineered decision trees: population diversity produces smarter trees	genetic engineering;decision tree;data analysis data mining marketing;computers computer science artificial intelligence genetic algorithms;statistics;marketing estimation statistical techniques decision trees;statistics data analysis data mining;artificial intelligence genetic algorithms;estimation statistical techniques decision trees computers computer science	When considering a decision tree for the purpose of classification, accuracy is usually the sole performance measure used in the construction process. In this paper, we introduce the idea of combining a decision tree’s expected value and variance in a new probabilistic measure for assessing the performance of a tree. We develop a genetic algorithm for constructing a tree using our new measure and conduct computational experiments that show the advantages of our approach. Further, we investigate the effect of introducing diversity into the population used by our genetic algorithm. We allow the genetic algorithm to simultaneously focus on two distinct probabilistic measures—one that is risk averse and one that is risk seeking. Our bivariate genetic algorithm for constructing a decision tree performs very well, scales up quite nicely to handle data sets with hundreds of thousands of points, and requires only a small percent of the data to generate a high-quality decision tree. We demonstrate the effectiveness of our algorithm on three large data sets.		Zhiwei Fu;Bruce L. Golden;Shreevardhan Lele;S. Raghavan;Edward A. Wasil	2003	Operations Research	10.1287/opre.51.6.894.24919	genetic engineering;c4.5 algorithm;influence diagram;decision tree model;decision tree learning;computer science;data science;machine learning;decision tree;incremental decision tree;data mining;mathematics;id3 algorithm;algorithm;statistics	ML	4.016632021459459	-17.647828218190064	142209
0dd7db73b6192135b14bab930c6d73c601e432b3	filtering financial time series by least squares		Modeling of financial time series with artificial intelligence is difficult due to the random nature of the data. The moving average filter is a common and simple form of filter to reduce this noise. There are several of these noise reduction methods used throughout the financial security trading community. The major issue with these filters is the lag between the filtered data and the noisy data. This lag only increases as more noise reduction is desired. In the present marketplace, where investors are competing for quality and timely information, lag can be a hindrance. This paper proposes a new moving average filter derived with the aim of maximizing the level of noise reduction at each delay. Comparison between five different methods has been done and experiment results have shown that our method is a superior noise reducer to the alternatives over short and middle range lag periods.	least squares;time series	Adrian Letchford;Junbin Gao;Lihong Zheng	2013	Int. J. Machine Learning & Cybernetics	10.1007/s13042-012-0081-0	econometrics;simulation;engineering;statistics	ML	5.1899256949826	-16.510940614397487	142222
721f853839e04f224d25198abf49724a88843adc	interval valued intuitionistic fuzzy sets in \(\gamma \) -semihypergroups		The notion of interval valued intuitionistic fuzzy sets was introduced by Atanassov and Gargov as a generalization of the notion of intuitionistic fuzzy sets and interval valued fuzzy sets. In this paper, we initiate a study on interval valued intuitionistic fuzzy sets in \(\Gamma \)-semihypergroups. We introduce the notions of interval valued intuitionistic fuzzy left (right, two sided) \(\Gamma \)-hyperideal, interval valued intuitionistic fuzzy bi-\(\Gamma \)-hyperideal and interval valued intuitionistic fuzzy (1,2) \(\Gamma \)-hyperideal in a \(\Gamma \)-semihypergroup and some properties of them are obtained. We use the interval valued intuitionistic fuzzy left, right, two-sided and bi-\(\Gamma \)-hyperideals to characterize some classes of \(\Gamma \)-semihypergroups. We also introduce the notion of an interval valued intuitionistic fuzzy \(M\) (resp. \(N\))-hypersystem of a \(\Gamma \)-semihypergroup and some properties of them are investigated.	fuzzy set	Saleem Abdullah;Aslam Muhammad;Kostaq Hila	2016	Int. J. Machine Learning & Cybernetics	10.1007/s13042-014-0250-4	discrete mathematics;mathematics;algorithm	AI	-0.060111650486018034	-21.319606719649343	142332
4cb16f6be9799ea7f2dc8832473384f6b57774af	belief structures, weight generating functions and decision-making	uncertainty;measures;dempster shafer	We describe the Dempster–Shafer belief structure and provide some of its basic properties. We introduce the plausibility and belief measures associated with a belief structure. We note that these are not the only measures that can be associated with a belief structure. We describe a general approach for generating a class of measures that can be associated with a belief structure using a monotonic function on the unit interval, called a weight generating function. We study a number of these functions and the measures that result. We show how to use weight-generating functions to obtain dual measures from a belief structure. We show the role of belief structures in representing imprecise probability distributions. We describe the use of dual measures, other then plausibility and belief, to provide alternative bounding intervals for the imprecise probabilities associated with a belief structure. We investigate the problem of decision making under belief structure type uncertain. We discuss two approaches to this decision problem. One of which is based on an expected value of the OWA aggregation of the payoffs associated with the focal elements. The second approach is based on using the Choquet integral of a measure generated from the belief structure. We show the equivalence of these approaches.		Ronald R. Yager	2017	FO & DM	10.1007/s10700-016-9236-x	discrete mathematics;uncertainty;dempster–shafer theory;measure;belief structure;artificial intelligence;machine learning;fuzzy measure theory;mathematics;statistics	Logic	-0.8470840567497643	-20.19232912589649	142370
9a5ee1338e2a37ebe10425bbe6c70aae24ad828c	reconstruction of bifurcation diagrams using extreme learning machines	neural nets;bifurcation;chaos;fractal image compression bifurcation diagrams reconstruction extreme learning machines elm principal component analysis pca coefficient vector time series predictor parameter estimation structural similarity extraction method;nonlinear prediction;bifurcation vectors image reconstruction mathematical model principal component analysis neurons;extreme learning machine chaos nonlinear prediction bifurcation diagram;time series;vectors;extreme learning machine;bifurcation diagram;principal component analysis;parameter estimation;vectors bifurcation learning artificial intelligence neural nets parameter estimation principal component analysis time series;learning artificial intelligence	We describe a method for reconstructing bifurcation diagrams by using extreme learning machines (ELM). Principal component analysis (PCA) is performed for the coefficient vector obtained by training the time-series predictor. From the results of PCA, we estimate the number of significant parameters of the target system, reconstruct the bifurcation diagram, and compare it with the original one. The results show that the computation time required by ELM is considerably shorter than that required by conventional methods. In addition, we quantitatively evaluate the accuracy of reconstruction of bifurcation diagrams using a structural similarity extraction method based on fractal image compression.	bifurcation diagram;bifurcation theory;coefficient;computation;fractal compression;image compression;kerrison predictor;principal component analysis;structural similarity;time complexity;time series	Yuta Tada;Masaharu Adachi	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.196	machine learning;time series;pattern recognition;mathematics;estimation theory;artificial neural network;statistics;bifurcation diagram;principal component analysis	Robotics	9.627864677198385	-21.632563902897175	142422
f5f5eb0b2fa648138c59d43e2037d8f0a6e4b4cf	credit default swaps: implied ratings versus official ones	secs s 06 metodi mat dell economia e scienze attuariali e finanziarie	Recently, in line with the progressive development of the credit derivatives market, the academic research has begun to explore the relationship between Credit Default Swap market and rating events. In this paper, following a market based approach, we calibrate an Implied Rating model on Credit Default Swap market spreads. The non parametric mapping of Implied Ratings is calibrated on a large data set of Credit Default Swap quotes that includes the years of financial turmoils. This allows also to investigate the existence of possible differences between normal and abnormal market conditions. Unlike other models, the one proposed considers a linear penalty function which allows to evaluate market quotes in a neutral way and to formalize a more computationally efficient programming model. We compare the behaviors of credit rating agencies in different markets (EU and USA) and in different sub-periods, in order to analyze whether Implied Rating changes anticipate or follow the effective rating changes supplied by Fitch Ratings, Moody’s and Standard and Poor’s.	algorithmic efficiency;convex function;fitch notation;penalty method;performance rating;programming model	Rosella Castellano;Rosella Giacometti	2012	4OR	10.1007/s10288-011-0195-3	credit default swap index;credit rating;credit derivative;actuarial science;bond credit rating;credit event;credit risk;mathematics;itraxx;credit default swap;credit valuation adjustment	AI	1.9185916330710029	-10.657795902863677	142440
054f4af68a463fa04d9568fc977c1a6a72dffe8c	some results on fuzzy commutative and fuzzy cyclic subgroups	subgrupo;subgroup;conmutatividad;logique floue;logica difusa;sous groupe;fuzzy logic;comparative study;sous groupe cyclique;commutativity;sous groupe flou;commutativite	In this paper an attempt has been made to study different quasi-fuzzy commutativity for different fuzzy subgroups. Interrelationship and comparative studies on these structures have been made. Interesting results have been found and discussed briefly.		Sandeep Kumar Bhakat	2002		10.1007/3-540-45631-7_72	fuzzy logic;discrete mathematics;computer science;comparative research;mathematics;subgroup;commutative property;algorithm	NLP	1.4059063202964823	-22.698161443649518	142706
5f2ab6546fc9048690e801337eb5d233e599f2bb	an optimization approach to services sales forecasting in a multi-staged sales pipeline	services sales management;forecasting;sales forecasting;sales prediction;optimization and machine learning;it services;services enterprise sales	Services organization manage a pipeline of sales opportunities with variable enterprise sales engagement lifespan, maturity levels (belonging to progressive sales stages), and contract values at any given point in time. Accurate forecasting of contract signings by the end of a time period (e.g., a quarter) is a desire for many services organizations in order to get an accurate projection of incoming revenues, and to provide support for delivery planning, resource allocation, budgeting, and effective sales opportunity management. While the problem of sales forecasting has been investigated in its generic context, sales forecasting for services organizations entails the consideration of additional complexities, which has not been thoroughly investigated: (i) considering opportunities in multi-staged sales pipeline, which means providing stage-specific treatment of sales opportunities in each group, and (ii) using the information of the current pipeline build-up, as well as the projection of the pipeline growth over the remaining time period before the end of the target time period in order to make predictions. In this paper, we formulate this problem, considering the service-specific context, as a machine learning problem over the set of historical services sales data. We introduce a novel optimization approach for finding the optimized weights of a sales forecasting function. The objective value of our optimization model minimizes the average error rates for predicting sales based on two factors of conversion rates and growth factors for any given point in time in a sales period over historical data. Our model also optimally determines the number of historical periods that should be used in the machine learning framework to predict the future revenue. We have evaluated the presented method, and the results demonstrate superior performance (in terms of absolute and relative errors) compared to a baseline state of the art method.	baseline (configuration management);capability maturity model;conversion marketing;data point;experiment;machine learning;mathematical optimization;mean squared error;nonlinear system;opportunity management;optimization problem;pipeline (computing);population;program optimization;sparse matrix;tropical cyclone track forecasting	Aly Megahed;Peifeng Yin;Hamid R. Motahari Nezhad	2016	2016 IEEE International Conference on Services Computing (SCC)	10.1109/SCC.2016.98	sales order;marketing;operations management;sales and operations planning;sales management;data mining;target income sales;business;like for like	DB	5.7727206136222495	-14.536895207382926	142788
60af709bb382e9e73025ed45f4f559638c54ea5b	the impact of stock market volatility on corporate bond credit spreads	gestion previsionnelle;forecasting;garch;australie;oceanie;garch model;stock market;credit spread;corporate bonds;option pricing;forecasting management;time series;statistical model;volatilite implicite;stock markets;marche valeurs;modele garch;time series analysis;indexation;serie temporelle;modele statistique;serie temporal;implied volatility;modelo estadistico;stock market volatility;gestion provisional;oceania;conditional heteroscedasticity;yield curve;australia;var	There has been a rapid increase in the number of corporate bonds issued in Australia since the middle of 1998. This increase has stimulated interest in characterising the yield curves and the factors that determine changes in these spreads. The focus of this paper is on measuring any impact of stock market volatility on spreads using two different measures. One measure is based on volatility implied from options prices while the other is derived from a conditional heteroscedastic volatility model of changes in a stock market index. It is found that the former has no significant impact on spreads but the latter is both significant and stable over time. This impact is estimated to be negative implying that an increase in volatility cause a decrease in corporate bond spreads.	volatility	Ronald Bewley;David Rees;Paul Berg	2004	Mathematics and Computers in Simulation	10.1016/S0378-4754(03)00102-2	forward volatility;volatility swap;autoregressive conditional heteroskedasticity;implied volatility;volatility smile;time series;mathematics;statistics	ML	3.3924267476913146	-13.169361163253846	142870
305c8ee830ca3c7f4bdf89f078d458dba50e6eb2	reasoning with prioritized data by aggregation of distance functions	distance function	We introduce a general framework for reasoning with prioritized data by aggregation of distance functions, study some basic properties of the entailment relations that are obtained, and relate them to other approaches for maintaining uncertain information.		Ofer Arieli	2008			metric;computer science;machine learning;data mining	DB	-2.615154922933604	-22.58182249152405	142968
f86f0d3d3f576d78bd4b1625bcde3e23a14ebeee	a hybrid-learning based broker model for strategic power trading in smart grid markets		Smart Grid markets are dynamic and complex, and brokers are widely introduced to better manage the markets. However, brokers face great challenges, including the varying energy demands of consumers, the changing prices in the markets, and the competitions between each other. This paper proposes an intelligent broker model based on hybrid learning (including unsupervised, supervised and reinforcement learning), which generates smart trading strategies to adapt to the dynamics and complexity of Smart Grid markets. The proposed broker model comprises three interconnected modules. Customer demand prediction module predicts short-term demands of various consumers with a data-driven method. Wholesale market module employs a Markov Decision Process for the one-day-ahead power auction based on the predicted demand. Retail market module introduces independent reinforcement learning processes to optimize prices for different types of consumers to compete with other brokers in the retail market. We evaluate the proposed broker model on Power TAC platform. The experimental results show that our broker is not only is competitive in making profit, but also maintains a good supply-demand balance. In addition, we also discover two empirical laws in the competitive power market environment, which are: 1. profit margin shrinks when there are fierce competitions in markets; 2. the imbalance rate of supply demand increases when the ∗Corresponding author Email addresses: xw357@uowmail.edu.au (Xishun Wang), minjie@uow.edu.au (Minjie Zhang), fren@uow.edu.au (Fenghui Ren) Preprint submitted to Elsevier December 5, 2016 market environment is more competitive.	email;markov chain;markov decision process;reinforcement learning	Xishun Wang;Minjie Zhang;Fenghui Ren	2017	Knowl.-Based Syst.	10.1016/j.knosys.2016.12.008	trading strategy;data mining;profit margin;market environment;reinforcement learning;search engine;computer science;smart grid;markov decision process;microeconomics;artificial intelligence;supply and demand	AI	5.4976843509078535	-13.636352110271247	142991
91d4cd60332efb13978f4a9e1094b5052f935b59	non-parametric interval forecast models from fuzzy clustering of numerical weather predictions	unsupervised learning;pattern clustering;nonparametric statistics;weather forecasting;fuzzy set theory;uncertainty kernel predictive models wind forecasting fitting;statistical distributions;geophysics computing;sampling variation nonparametric interval forecast model fuzzy clustering numerical weather prediction clustering method post processing technique nwp system relevant information discovery fuzzy c means clustering unsupervised learning method fuzzy set discovery weather forecast situation forecast uncertainty pattern distribution fitting method statistical prediction intervals probabilistic forecast post processing method cross fold validation;weather forecasting fuzzy set theory geophysics computing nonparametric statistics pattern clustering statistical distributions unsupervised learning	Clustering methods are proposed and evaluated as post-processing techniques that can model the uncertainty of forecasts provided by Numerical Weather Prediction (NWP) systems. These techniques try to discover relevant information about forecast uncertainty that is inherent in the performance records of the system. We investigate the application of Fuzzy C-means clustering as a powerful unsupervised learning method to discover fuzzy sets of weather forecast situations which represent different forecast uncertainty patterns. These patterns are then utilized by different distribution fitting methods to obtain statistical prediction intervals which can express the expected accuracy of the NWP system output. Three years of weather forecast records in two weather stations are used in a set of experiments to empirically study the application of the proposed approach. Skills of the probabilistic forecasts obtained by these post-processing methods are investigated by considering cross fold validation and sampling variations. Results demonstrate that the Prediction intervals generated by the proposed procedure have a higher skill compared to baseline methods.	baseline (configuration management);cluster analysis;experiment;fuzzy clustering;fuzzy set;interval arithmetic;numerical method;numerical weather prediction;sampling (signal processing);unsupervised learning;video post-processing	Ashkan Zarnani;Petr Musílek	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608480	econometrics;forecast skill;geography;forecast error;machine learning;ensemble forecasting;global forecast system;data mining;cluster analysis	ML	8.132639896877214	-22.56836822555836	143002
bed29576d23e23e5fa065037fdf0481232bd6752	rough operations and uncertainty measures on r0-algebras (nilpotent minimum algebras)	r0 algebra;plausibility measure r0 algebra lower approximate operation upper approximate operation belief measure;rough set theory boolean algebra;boolean algebra fuzzy logic uncertainty set theory measurement uncertainty;lower approximate operation;plausibility measure;plausibility measure rough operation uncertainty measure r0 algebra nilpotent minimum algebra lower approximate operation upper approximate operation boolean partition belief measure;upper approximate operation;belief measure	In this paper, we define a lower approximate operation and an upper approximate operation based on a Boolean partition on R0-algebras and discuss their properties. We then introduce a pair of belief measure and plausibility measure on R0-algebras and investigate the relationship between rough operations and belief measure and plausibility measure.	approximation algorithm;fuzzy control system;information system;plausibility structure	Xia Wu;Xiaoling Liu;Li Zhou;Jialu Zhang	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381948	boolean algebra;combinatorics;mathematical analysis;discrete mathematics;mathematics;two-element boolean algebra	Robotics	-1.2198527207789505	-22.8185789367165	143170
365b6db20da970018d0f0fe1c52902fbe7a4f6a8	a logic deduction of expanded means-end chains	qualitative method;consumidor;analisis cualitativo;metodologia;modele mathematique;concepcion sistema;consommateur;data collection;qualitative analysis;modelo matematico;prise decision;marketing decision support system;set theory;methodologie;marketing strategy;analyse qualitative;methode qualitative;consumer;system design;theory;teoria;means end chain;mathematical model;logic deduction;information system;methodology;toma decision;deduccion;conception systeme;systeme information;theorie;sistema informacion;deduction	This study, based on the expanded means–end chain (MEC) theory, proposes a logic deduction procedure for qualitative analyses as a conceptual structure for establishing information systems. Through the expressions of variable matrix and set theory, the illustrations of data collection and programming procedures reveal the logic deduction framework for analyzing the linkages of consumer value satisfaction and product attribute design. This allows MEC methodology to have an easier programming procedure for creating a related marketing decision support system and thus effective marketing strategies can be developed.	authorization;hardware-assisted virtualization;information science;information system;marketing decision support system;national supercomputer centre in sweden;natural deduction;programming language;serial digital video out;set theory	Chin-Feng Lin;Hsien-Tang Tsai;Chen-Su Fu	2006	J. Information Science	10.1177/0165551506059218	qualitative research;artificial intelligence;algorithm;statistics	AI	-1.1031928319100113	-15.325434694079345	143240
1969122932c0bd8304bd403de66295697062e70f	an intuitionistic fuzzy linear programming method for logistics outsourcing provider selection	intuitionistic fuzzy linear programming;intuitionistic fuzzy preference relation;group decision making;topsis technique for order preference by similarity to ideal solution;logistics outsourcing provider	In order to reduce costs and enhance their core competitiveness, many companies tend to choose the logistics outsourcing. The selection of logistics outsourcing provider plays an important role for the success of outsourcing. In this paper, we formulate the logistics outsourcing provider selection as a kind of group decision making (GDM) problems with intuitionistic fuzzy preference relations (IFPRs). A new intuitionistic fuzzy linear programming method is proposed for solving such problems. First, we construct an intuitionistic fuzzy linear programming model to derive priority weights from IFPRs. Depended on the construction of non-membership functions, this intuitionistic fuzzy linear programming model is solved by the developed three kinds of approaches including the optimistic, pessimistic and mixed approaches. Then by the idea of TOPSIS (Technique for order preference by similarity to ideal solution), the experts’ weights are determined objectively. Combining the experts’ weights with the derived priority weights, the corresponding method for GDM with IFPRs is presented. An example of logistics outsourcing provider selection is provided to illustrate the proposed method. Finally, the intuitionistic fuzzy programming method is further generalized to the case of more general membership and non-membership functions.	intuitionistic logic;linear programming;logistics;outsourcing;programming model	Shu-Ping Wan;Feng Wang;Li-Lian Lin;Jiu-Ying Dong	2015	Knowl.-Based Syst.	10.1016/j.knosys.2015.02.027	group decision-making;knowledge management;data mining	AI	-2.9607193275334778	-19.48274801752275	143581
c9e041e0d2072316ec0651cd62dbccdd07a6f68e	insdecm - an interactive procedure for stochastic multicriteria decision problems	multiple criteria analysis;multicriteria analysis;uncertainty modeling;stochastic dominance;systeme aide decision;dominancia estocastica;sistema ayuda decision;prise decision;decision maker;decision problem;decision support system;hierarchical classification;probability distribution;classification hierarchique;dominance stochastique;analisis multicriterio;analyse multicritere;approche interactive;toma decision;clasificacion jerarquizada;interaction technique;interactive approach	A new interactive technique for a discrete stochastic multiattribute decision making problem is proposed in this paper. It is assumed that performance probability distribution for each action on each attribute is known. Two concepts are combined in the procedure: stochastic dominance and interactive approach. The first one is employed for generating efficient actions and constructing rankings of actions with respect to attributes. The second concept is used when the communication between the DM and the model is conducted. It is assumed that decision maker’s restrictions are defined by specifying minimal or maximal values of scalar criteria measuring either expected outcome or variability of outcomes. As such restrictions are, in general, not consistent with stochastic dominance rules, we suggest verifying this consistency and asking the decision maker to redefine inconsistent restrictions.	decision problem	Maciej Nowak	2006	European Journal of Operational Research	10.1016/j.ejor.2005.02.016	probability distribution;decision-making;mathematical optimization;decision support system;computer science;artificial intelligence;stochastic dominance;machine learning;decision problem;data mining;mathematics;interaction technique;statistics	Robotics	-0.7669966019860532	-15.790715063488419	143614
791e791b58e9b89dddd683cd3155ff903be2733c	neural network model based on fuzzy artmap for forecasting of highway traffic data	. fuzzy artmap;atis.;travel cost estimates;neural network model	In this article, a neural network model is presented for forecasting the average speed values at highway traffic detectors locations using the Fuzzy ARTMAP theory. The performance of the model is measured by the deviation between the speed values provided by the loop detectors and the predicted speed values. Different Fuzzy ARTMAP configuration cases are analysed in their training and testing phases. Some adhoc mechanisms added to the basic Fuzzy ARTMAP structure are also described to improve the entire model performance. The achieved results make this model suitable for being implemented on advanced traffic management systems (ATMS) and advanced traveller information system (ATIS).	artificial neural network;automatic transmitter identification system (television);information system;network model;next-generation network;requirement;sensor;simulation;system requirements	Daniel Boto-Giralda;Miriam Antón-Rodríguez;Francisco Javier Díaz Pernas;José Fernando Díez Higuera	2006			computer science;neuro-fuzzy;machine learning;artificial neural network	Robotics	9.043081290907747	-13.586354039665142	143666
2806f12b07c2d58d0297dba850ff6f4f62bec237	investigation of forecasting methods for the hourly spot price of the day-ahead electric power markets	forecasting;support vector machines;computational modeling;predictive models;electricity supply industry;load modeling;data models	Forecasting hourly spot prices for real-time electricity usage is a challenging task. This paper investigates a series of forecasting methods to 90 and 180 days of load data collection acquired from the Iberian Electricity Market (MIBEL). This dataset was used to train and test multiple forecast models. The Mean Absolute Percentage Error (MAPE) for the proposed Hybrid combination of Auto Regressive Integrated Moving Average (ARIMA) and Generalized Linear Model (GLM) was compared against ARIMA, GLM, Random forest (RF) and Support Vector Machines (SVM) methods. The results indicate significant improvement in MAPE and correlation co-efficient values for the proposed hybrid ARIMA-GLM method.	autoregressive integrated moving average;electricity price forecasting;generalized linear model;radio frequency;random forest;real-time clock;scalability;support vector machine;volatility	Radhakrishnan Angamuthu Chinnathambi;Prakash Ranganathan	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840962	financial economics;econometrics;engineering;operations management	Robotics	8.404366313691849	-17.57726920981686	144039
323bfd9b9a6d3228cefea73d4ffec1783ea19dea	fuzzy logic based game theory applications in multi-criteria decision making process	game theory;multi criteria analysis;fuzzy sets	Decision process can be summarized as a process that helps for choosing the optimal alternatives according to suggested objectives by evaluating all environmental effects in problem solving. Nowadays, since environmental effects are more complex, imprecise and multilateral, fuzzy set theory and game theory are widely preferred instruments in decision making process. The aim of this study is to present a hybrid multi-criteria decision making approach which uses artificial intelligence techniques such as fuzzy logic and game theory. This process is considered in two person non-constant sum game theory perspective. The methods in literature about related topics (such as scenario planning and fuzzy TOPSIS) are examined and a hybrid decision making methodology that comprises many decision methods is formed. All phases of this approach are executed in game theory perspective by evaluating mutual strategies of players. In the study, the methodology is explained and a fictitious international disagreement case is used as a numerical example to demonstrate the validity and applicability.	artificial intelligence;fuzzy logic;fuzzy set;game theory;numerical analysis;problem solving;scenario planning;set theory	Hakan Soner Aplak;Orhan Türkbey	2013	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-2012-0642	wald's maximin model;implementation theory;game theory;positive political theory;optimal decision;influence diagram;decision theory;decision analysis;decision field theory;decision engineering;computer science;artificial intelligence;machine learning;decision rule;mathematics;management science;fuzzy set;algorithmic game theory;evidential reasoning approach;evidential decision theory;weighted sum model	AI	-4.012740094153482	-17.724593570598064	144119
db3a91d8e59d77ac8341dcb21665e4ba99b7f6c0	multiple attribute decision making based on interval-valued intuitionistic fuzzy sets, linear programming methodology, and the extended topsis method		In recent years, some multiple attribute decision making (MADM) methods have been presented based on interval-valued intuitionistic fuzzy sets (IVIFSs). In this paper, we propose a new MADM method based on IVIFSs, the linear programming (LP) methodology, and the extension of the technique for order preference by similarity to ideal solution (TOPSIS) method, where the LP methodology is used to obtain optimal weights of attributes. The proposed method can overcome the drawbacks of the existing methods for MADM in interval-valued intuitionistic fuzzy (IVIF) environments.	fuzzy set;linear programming;software development process	Cheng-Yi Wang;Shyi-Ming Chen	2017	Inf. Sci.	10.1016/j.ins.2017.02.045	mathematical optimization;discrete mathematics;data mining;mathematics	Theory	-2.5656894111697275	-20.315128388785062	144256
134aef5eee1c3bc8e9c8144bdde2f4ac817721e9	functional echo state network for time series classification		Echo state networks (ESNs) are a new approach to recurrent neural networks (RNNs) that have been successfully applied in many domains. Nevertheless, an ESN is a predictive model rather than a classifier, and methods to employ ESNs in time series classification (TSC) tasks have not yet been fully explored. In this paper, we propose a novel ESN approach named functional echo state network (FESN) for time series classification. The basic idea behind FESN is to replace the numeric variable output weights of an ESN with time-varying output-weight functions and introduce a temporal aggregation operator to the output layer that can project temporal signals into discrete class labels, thereby transforming the ESN from a predictive model into a true classifier. Subsequently, to learn the output-weight functions, a spatio-temporal aggregation learning algorithm is proposed based on orthogonal function basis expansion. By leveraging the nonlinear mapping capacity of a reservoir and the accumulation of temporal information in the time domain, FESN can not only enhance the separability of different classes in a high-dimensional functional space but can also consider the relative importance of temporal data at different time steps according to dynamic output-weight functions. Theoretical analyses and experiments on an extensive set of UCR data were conducted on FESN. The results show that FESN yields better performance than single-algorithm methods, has comparable accuracy with ensemble-based methods and exhibits acceptable computational complexity. Interestingly, for some time series datasets, we visualized some interpretable features extracted by FESN via specific patterns within the output-weight functions.	echo state network;time series	Qianli Ma;Lifeng Shen;Weibiao Chen;Jiabin Wang;Jia Wei;Zhiwen Yu	2016	Inf. Sci.	10.1016/j.ins.2016.08.081	computer science;artificial intelligence;machine learning;data mining;algorithm	AI	8.921195468855203	-22.03521227096376	144296
8eabbff2302ea7195879d1a274f0071554c799a8	two short proofs regarding the logarithmic least squares optimality in chen, k., kou, g., tarn, j. m., song, y. (2015): bridging the gap between missing and inconsistent values in eliciting preference from pairwise comparison matrices, annals of operations research 235(1): 155-175			bridging (networking);entity–relationship model;least squares;operations research	Sándor Bozóki	2017	Annals OR	10.1007/s10479-016-2396-9	mathematical optimization;computer science;artificial intelligence;mathematics;algorithm;statistics	Vision	0.7026069403803183	-18.208811121481443	144309
4d3ca705af1f1ff9d641b90375f8d32d411c65f5	quantile induced heavy ordered weighted averaging operators and its application in incentive decision making		To integrate incentives into the information aggregation process in decision making, we propose a new type of aggregation operator, denominated as the quantile induced heavy ordered weighted averaging (QI-HOWA) operator in this paper. A primary characteristic of this type of operator is that the quantile variable can be used not only to measure the relative performance of alternatives but also to facilitate the incentive preference expression of the decision maker. We further provide a calculation technology of the QI-HOWA weights, in which various incentive preferences of the decision maker can be considered through parameter adjustment. In addition, we discuss certain properties of the QI-HOWA operator and note the extent to which they are effective. Finally, a numerical example regarding the selection of optimal alternatives by incentive measures is provided, and the aggregations are compared with those of ordered weighted averaging and unweighted averaging operators to illustrate the validity of the QI-HOWA operator.	ordered weighted averaging aggregation operator	Ping-Tao Yi;Weiwei Li;Yajun Guo;Danning Zhang	2018	Int. J. Intell. Syst.	10.1002/int.21945	mathematics;data mining;operator (computer programming);quantile;mathematical optimization;incentive	AI	-3.0564156724818394	-19.136908685296202	144314
a64f26b3cddc78dda7b4652e35f23af617d15a65	forecasting stochastic neural network based on financial empirical mode decomposition	multiscale mcid analysis;emd stnn forecasting model;stock market fluctuation;stochastic time strength function;empirical mode decomposition	In an attempt to improve the forecasting accuracy of stock price fluctuations, a new one-step-ahead model is developed in this paper which combines empirical mode decomposition (EMD) with stochastic time strength neural network (STNN). The EMD is a processing technique introduced to extract all the oscillatory modes embedded in a series, and the STNN model is established for considering the weight of occurrence time of the historical data. The linear regression performs the predictive availability of the proposed model, and the effectiveness of EMD-STNN is revealed clearly through comparing the predicted results with the traditional models. Moreover, a new evaluated method (q-order multiscale complexity invariant distance) is applied to measure the predicted results of real stock index series, and the empirical results show that the proposed model indeed displays a good performance in forecasting stock market fluctuations.	artificial neural network;biological neural networks;electromechanical dissociation;embedded system;embedding;hilbert–huang transform;processing (action);projections and predictions;regression analysis;stochastic neural network	Jie Wang	2017	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2017.03.004	econometrics;hilbert–huang transform;machine learning	ML	7.792822193919621	-19.969847326822784	144546
6dcdb29319b4aa630ab8b07e2cf46b77b6176740	verhulst model of interval grey number based on information decomposing and model combination		Grey Verhulst models are often employed to simulate the development tendency with the characteristic of saturated process of S curve. However, the uncertainty of interval grey numbers will be increased since the boundaries of interval grey number are extended by the Axiom of nondecreasing grey degree in the existing Verhulst modeling method. In this paper, the interval grey number is divided into two real number parts, that is, “white” and “grey” parts.Then the “white” and “grey” parts are simulated and forecasted by building the grey Verhulst model and DGM (1, 1) model, respectively. To some degree, this method resolves the issue of amplifying the range of interval grey number. Finally, an example is used to compare the simulation performance between the new model and the traditional model, and the results show that the new model is superior to the other model.	simulation	Bo Zeng;Chuan Li;Guo Chen;Wang Zhang	2013	J. Applied Mathematics	10.1155/2013/472065	calculus;mathematics	AI	3.4466969774676146	-21.47409676610229	144809
8083cbfeb6b4a2c1974b9d9a9f88db4732b9d30c	adaptive-expectation based multi-attribute fts model for forecasting taiex	fuzzy time series;stock index futures forecasting;universe of discourse;time series;stock index futures;fuzzy clustering;fuzzy rule base;stock price;clustering method;indexation;literature review;population growth;adaptive expectation model	In recent years, there have been many time series methods proposed for forecasting enrollments, weather, the economy, population growth, and stock price, etc. However, traditional time series, such as ARIMA, expressed by mathematic equations are unable to be easily understood for stock investors. Besides, fuzzy time series can produce fuzzy rules based on linguistic value, which is more reasonable than mathematic equations for investors. Furthermore, from the literature reviews, two shortcomings are found in fuzzy time series methods: (1) they lack persuasiveness in determining the universe of discourse and the linguistic length of intervals, and (2) only one attribute (closing price) is usually considered in forecasting, not multiple attributes (such as closing price, open price, high price, and low price). Therefore, this paper proposes a multiple attribute fuzzy time series (FTS) method, which incorporates a clustering method and adaptive expectation model, to overcome the shortcomings above. In verification, using actual trading data of the Taiwan Stock Index (TAIEX) as experimental datasets, we evaluate the accuracy of the proposedmethod and compare the performancewith the (Chen, 1996 [7], Yu, 2005 [6], and Cheng, Cheng, &Wang, 2008 [20]) methods. The proposedmethod is superior to the listing methods based on average error percentage (MAER). © 2009 Elsevier Ltd. All rights reserved.	autoregressive integrated moving average;closing (morphology);cluster analysis;data point;domain of discourse;entity–relationship model;fleet telematics system;fuzzy cognitive map;fuzzy rule;linguistic value;time series	Jing-Wei Liu;Tai-Liang Chen;Ching-Hsue Cheng;Yao-Hsien Chen	2010	Computers & Mathematics with Applications	10.1016/j.camwa.2009.10.014	econometrics;actuarial science;fuzzy clustering;machine learning;time series;domain of discourse;mathematics;population growth;statistics	AI	6.718115461269667	-20.07384508585197	144818
55d5ec92226d70130db67c91e979fca5b982bf8e	stature, obesity, and portfolio choice		Copyright: © 2016 INFORMS Abstract. Using multiple U.S. and European data sources, we show that observed physical attributes are related to participation in financial markets. Specifically, we find that individuals who are relatively tall and of normal weight are more likely to hold stocks in their financial portfolios. We consider several potential mechanisms that could drive the relation between physical attributes and portfolio decisions. We find that teenage social experiences as well as genetic and prenatal endowments that are fixed at birth are the two channels through which height affects financial decisions. Furthermore, we find that the relation between body mass index and portfolio decisions is largely driven by education and race.	brain–computer interface;experience;fo (complexity);graham scan;human body weight;human height;institute for operations research and the management sciences;kerrison predictor;modern portfolio theory;nl (complexity);risk aversion	Jawad M. Addoum;George Korniotis;Alok Kumar	2017	Management Science	10.1287/mnsc.2016.2508	financial economics;actuarial science;economics;marketing;finance;microeconomics;behavioral economics	ECom	-0.6408734871727964	-10.413815139128742	144915
e5678d4156d5219b23d4eb0b725e3a2b375ac9dc	software project portfolio selection a modern portfolio theory based technique	modern portfolio theory		modern portfolio theory	Hélio R. Costa;Márcio de Oliveira Barros;Ana Regina Cavalcanti da Rocha	2010			application portfolio management;systems engineering;separation property;post-modern portfolio theory;computer science;project portfolio management;efficient frontier;portfolio optimization;modern portfolio theory;black–litterman model	Logic	4.468564533329833	-10.086318135848629	145069
e791e156c8bc617382f667cdabe9b52a1f0ca1eb	non-stationary bayesian learning for global sustainability		An increasingly warming planet calls for widespread use of sustainable energy sources like solar energy. To meet the rising energy demand, the focus of state-of-the-art solar energy models on local predictions is no longer sufficient as it only leads to local optimization of solar resources. Hence, a new class of models is needed that can provide a global response towards sustainability. In this paper, therefore, we propose a new approach that models cloud movement as a multilayer network and then performs parameter learning on it to generate short-term predictions of cloud fraction/solar irradiance simultaneously at a large number of locations. These learned parameters capture the spatio-temporal interdependencies of solar energy which can allow power-grid operators and policy-makers at different locations to know who impacts the solar energy of whom. Our results indicate a Root Mean Square Error (RMSE) of 8-18% in one-hour cloud fraction prediction. Finally, using our network approach, we show that the cloud movement likely follows a power law distribution, an important domain knowledge discovery that may be useful for future models. A major consequence of our approach is that it can enable power-grid operators/policy-makers to see beyond the local boundaries of their respective geographical locations.	cloud fraction;interdependence;machine learning;mathematical optimization;mean squared error;stationary process	Kartikeya Bhardwaj;Radu Marculescu	2017	IEEE Transactions on Sustainable Computing	10.1109/TSUSC.2017.2716823	mathematical optimization;operator (computer programming);solar irradiance;remote sensing;cloud fraction;solar energy;local search (optimization);sustainability;cloud computing;bayesian inference;geography	Vision	9.096377325072712	-16.34499692550873	145217
22152b16cf746e15bb6678f8a59ca6bd4e2c3830	cots evaluation using modified topsis and anp	selection model;model selection;metodo analitico;empirical study;matematicas aplicadas;mathematiques appliquees;perforation;topsis;multiple criteria decision making;performance;solution similitude;similarity solution;selection modele;critere multiple;anp;prise decision;multiple criteria;feasibility;solucion semejanza;seleccion modelo;estudio caso;analytic network process;evaluation criteria;analytical method;etude cas;methode analytique;evaluation;evaluacion;cots;rendimiento;applied mathematics;toma decision;practicabilidad;faisabilite	This paper models the COTS evaluation problem as an MCDM problem and proposes a five-phase COTS selection model, combining the technique of ANP (analytic network process) and modified TOPSIS (technique for order performance by similarity to idea solution). This article discusses using the ANP to determine the relative weights of multiple evaluation criteria. The modified TOPSIS approach is used to rank competing products in terms of their overall performance. To illustrate how the approach is used for the COTS evaluation problem, an empirical study of a real case is conducted. The case study demonstrates the effectiveness and feasibility of the proposed evaluation procedure.		Huan-Jyh Shyur	2006	Applied Mathematics and Computation	10.1016/j.amc.2005.11.006	topsis;feasibility study;applied mathematics;performance;artificial intelligence;evaluation;mathematics;empirical research;operations research;model selection;analytic network process;statistics	NLP	-0.6953364432372702	-15.945761958130765	145472
f00110f89da8893a5f6cdadb6835b5d130fdeb25	the distribution characteristic of travel time reliability in beijing road network	reliability;mathematics;automobiles;travel time;fluctuations;road network;temporal spatial distribution characteristics;beijing road network reliability;transportation automobiles reliability spatiotemporal phenomena;distributed computing;design optimization;computer networks;stability;telecommunication traffic;stability telecommunication traffic fluctuations road vehicles dispersion computer networks distributed computing design optimization road transportation mathematics;spatial distribution;indexation;transportation;floating car data beijing road network reliability temporal spatial distribution characteristics travel time reliability index mathematical model;floating car data;spatiotemporal phenomena;mathematical model;travel time reliability index;road transportation;dispersion;road vehicles	To evaluate the reliability of the road network, the paper designed a mathematic model of travel time reliability index. Using the travel time reliability index, the paper analyzed the temporal and spatial distribution characteristics of Beijing road network reliability based on the floating car data. The results show that the travel time reliability is the lowest at the afternoon peak hour and there are obvious differences between different days and different areas.		Huimin Wen;Xuejie Liu;Yong Gao	2009	2009 International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2009.165	transport;dispersion;multidisciplinary design optimization;simulation;floating car data;stability;mathematical model;reliability;mathematics;statistics	DB	8.489219149466546	-11.585099153204688	145485
49bfbea119ce1b3082ac1e376bd170c9dce2391a	improving forecasting performance by exploiting expert knowledge: evidence from guangzhou port	subjective judgment;container throughput forecast;expert knowledge	Expert knowledge has been proved by substantial studies to be contributory to higher forecasting performance; meanwhile, its application is criticized and opposed by some groups for biases and inconsistency inherent in experts' subjective judgment. This paper proposes a new approach to improving forecasting performance, which takes advantage of expert knowledge by constructing a constraint equation rather than directly adjusting the predicted values by experts. For the comparison purpose, the proposed approach, together with several widely used models including ARIMA, BP-ANN and the judgment model (JM), is applied to forecasting the container throughput of Guangzhou Port, which is one of the most important ports of China. Forecasting performances of the above models are compared and the results clearly show superiority of the proposed approach over its rivals, which implies that expert knowledge will make positive contribution as long as it is used in a right way.	artificial neural network;autoregressive integrated moving average;delphi method;ethernet over twisted pair;expert system;nl (complexity);norm (social);numerical aperture;performance;statistical model;throughput	Anqiang Huang;Han Qiao;Shouyang Wang;John Liu	2016	International Journal of Information Technology and Decision Making	10.1142/S0219622016500085	artificial intelligence;marketing;operations management;machine learning;data mining;management;operations research	AI	-3.5810909362975836	-15.308753338450934	145738
67b7f6251c6e805e2cf08549dd4413960958b206	fuzzy weber sets and lovász extensions of cooperative games		This paper investigates fuzzy extensions of cooperative games and the coincidence of the solutions for fuzzy and crisp games. We first show that an exact game has an exact fuzzy extension such that its fuzzy core coincides with the core. For games with empty cores, we exploit Lovász extensions to establish the coincidence of Weber sets for fuzzy and crisp games.		Nobusumi Sagara	2014		10.1007/978-3-319-08795-5_25	artificial intelligence;mathematics;mathematical economics	AI	-0.05652167776367921	-19.416324401138983	146087
324d4d781571170daa9de7b6c6c541cb44b15171	a robust additive consistency-based method for decision making with triangular fuzzy reciprocal preference relations	decision making;triangular fuzzy reciprocal preference relation;additive consistency;goal programming model	To express uncertain information in decision making, triangular fuzzy reciprocal preference relations (TFRPRs) might be adopted by decision makers. Considering consistency of this type of preference relations, this paper defines a new additive consistency concept, which can be seen as an extension of that for reciprocal preference relations. Then, a simple method to calculate the triangular fuzzy priority weight vector is introduced. When TFRPRs are inconsistent, a linear goal programming framework to derive the completely additive consistent TFRPRs is provided. Meanwhile, an improved linear goal programming model is constructed to estimate the missing values in an incomplete TFRPR that can address the situation where ignored objects exist. After that, an algorithm for decision making with TFRPRs is presented. Finally, numerical examples and comparison analysis are offered.	utility functions on indivisible goods	Fanyong Meng;Xiaohong Chen	2018	FO & DM	10.1007/s10700-016-9262-8	machine learning;data mining;mathematics	ECom	-3.185068870181482	-20.045412191455764	146147
9b918c3dfdddcdfd4299190931dffa49456d2dd3	an interval extension of homogeneous and pseudo-homogeneous t-norms and t-conorms	interval homogeneity;t norms;t conorms;pseudo homogeneity	In this paper, the notion of homogeneity is extended to the interval-valued setting. In particular, we focus on the case of interval-valued t-norms and t-conorms and we characterize interval-valued homogeneous t-norms and t-conorms of interval-valued order. We also introduce the notions of interval-valued pseudo-homogeneous t-norm and interval pseudo-homogeneous t-conorm are introduced and characterized. We illustrate our results with two examples, one in image processing and the other one in decision making.	interval arithmetic;t-norm	Lucelia Lima Costa;Benjamín R. C. Bedregal;Humberto Bustince;Edurne Barrenechea Tartas;Marcus P. da Rocha	2016	Inf. Sci.	10.1016/j.ins.2015.11.031	mathematical analysis;discrete mathematics;mathematics	AI	0.11134307747146485	-22.114855839658667	146225
0597e93a5f0f58101e5b0081ac523c433b0ccf15	selection of appropriate payment methods for e-government - model and application	decision models;e government;economic efficiency;e payment;decision maker;multi dimensional;decision model	Not only in e-business but also in e-government, the success of online services liable for costs often depends on the convenience of the payment process. To find out the most suitable payment methods for a given egovernment service, the authors develop a methodical approach for egovernment decision makers. The multi-dimensional decision model takes into account various requirements such as security, economic efficiency, and specific requirements for the e-government service in question. The following paper illustrates the decision model with the case of the statistics shop of the German federal statistical office. A sample of payment methods is analysed and evaluated according to different criteria. Then, the most suitable payment methods for the online statistics shop are selected in accordance to the decision model.		Georg Wittmann;Markus Breitschaft;Thomas Krabichler;Ernst Stahl	2007		10.1007/978-3-540-74444-3_17	decision model;optimal decision;actuarial science;decision analysis;marketing;decision rule;management;computer security;commerce	Web+IR	-3.8739079303424777	-14.551106405807381	146276
5ab5fafd8f3c9c2a53ca472761f4b334ebe4a804	cluster-based aggregate forecasting for residential electricity demand using smart meter data	forecasting;measurement;support vector regression;linear regression;machine learning;energy consumption;clustering;aggregates;forecasting energy consumption measurement aggregates smart meters load modeling predictive models;predictive models;multi layer perceptron;smart meter;load modeling;electricity load forecasting;cbaf residential electricity demand forecasting smart meter data cluster based aggregate forecasting industrial demand national demand electricity consumption behavior correlation based feature selection;smart meters;smart meters feature selection load forecasting pattern clustering power consumption power engineering computing	While electricity demand forecasting literature has focused on large, industrial, and national demand, this paper focuses on short-term (1 and 24 hour ahead) electricity demand forecasting for residential customers at the individual and aggregate level. Since electricity consumption behavior may vary between households, we first build a feature universe, and then apply Correlation-based Feature Selection to select features relevant to each household. Additionally, smart meter data can be used to obtain aggregate forecasts with higher accuracy using the so-called Cluster-based Aggregate Forecasting (CBAF) strategy, i.e., by first clustering the households, forecasting the clusters' energy consumption separately, and finally aggregating the forecasts. We found that the improvement provided by CBAF depends not only on the number of clusters, but also more importantly on the size of the customer base.	24-hour clock;aggregate data;aggregate function;cluster analysis;feature selection;smart meter	Tri Kurniawan Wijaya;Matteo Vasirani;Samuel Humeau;Karl Aberer	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363836	simulation;demand forecasting;engineering;marketing;operations management	Robotics	8.703254661502275	-17.31497341495432	146406
eb0a3e69bcc2e26a3aefc68aee1d918af6eed12c	automated credit rating prediction in a competitive framework	financial bond credit rating;competitive analysis;automated credit rating prediction	Automated credit rating prediction (ACRP) algorithms are used to predict the ratings of bonds without having to trust one rating agency, like Moody’s, Fitch or Su0026P. Nevertheless, for the moment, the accuracy of ACRP algorithms is investigated by empirical tests. In this paper, the framework for a competitive analysis is set and afterwards in this framework, the definition of competitive ACRP algorithms and its demonstration is given. In this way, for a competitive ACRP algorithm, a worst-case guarantee concerning the misclassification error is offered. Furthermore, several ACRP algorithms from the literature are compared according their competitiveness.		Claude Gangolf;Robert Dochow;Günter Schmidt;Thomas Tamisier	2016	RAIRO - Operations Research	10.1051/ro/2016030	competitive analysis;mathematical optimization;credit rating;bond credit rating;mathematics;credit enhancement	NLP	4.272852070536362	-10.363256183258228	146575
66882c89fa9454ea88bd3fb8e59d790b03161687	the interval-valued fuzzy sets based on flou sets	cut set flou set nested set interval valued fuzzy set;fuzzy set theory boolean algebra;abstracts cognition iron gold;flou nested sets interval valued fuzzy sets flou σ algebras	In this paper, the connection between Flou sets and interval-valued fuzzy sets is established. Firstly, by using the concept of Flou sets, Flou σ-algebras and Flou nested sets are presented. Secondly, it is shown that an interval-valued fuzzy set can be considered as an equivalent class of Flou nested set. Finally, the cut set of an interval-valued fuzzy set is introduced and its properties are discussed.	cut (graph theory);fuzzy set;rough set	Hong-Xia Li;Kun Liu;Zeng-Tai Gong	2013	2013 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2013.6890472	combinatorics;mathematical analysis;discrete mathematics;type-2 fuzzy sets and systems;fuzzy number;mathematics;fuzzy set;fuzzy set operations	Robotics	-0.96273037149942	-23.176993238768475	146584
4557251d562730bb488c82b570386d71ab006923	wind energy prediction and monitoring with neural computation	dimension reduction;support vector regression;wind prediction;wind energy;self organizing feature maps;time series monitoring	Wind energy has an important part to play as renewable energy resource in a sustainable world. For a reliable integration of wind energy high-dimensional wind time-series have to be analyzed. Fault analysis and prediction are an important aspect in this context. The objective of this work is to show how methods from neural computation can serve as forecasting and monitoring techniques, contributing to a successful integration of wind into sustainable and smart energy grids. We will employ support vector regression as prediction method for wind energy time-series. Furthermore, we will use dimension reduction techniques like self-organizing maps for monitoring of high-dimensional wind time-series. The methods are briefly introduced, related work is presented, and experimental case studies are exemplarily described. The experimental parts are based on real wind energy time-series data from the National Renewable Energy Laboratory (NREL) western wind resource data set. & 2012 Elsevier B.V. All rights reserved.	a library for support vector machines;artificial neural network;computation;dimensionality reduction;dynamical system;kramer graph;machine learning;mathematical optimization;organizing (structure);randomness;real-time clock;self-organization;self-organizing map;smart tv;support vector machine;time series;volatile memory;xfig	Oliver Kramer;Fabian Gieseke;Benjamin Satzger	2013	Neurocomputing	10.1016/j.neucom.2012.07.029	wind power;support vector machine;simulation;computer science;machine learning;dimensionality reduction	AI	9.11678198775934	-18.779108205592628	146828
6c9c31fc4ee7afff839f2afa66e6f04eb7747409	the topsis method in the interval type-2 fuzzy setting		The technique for establishing order preference by similarity to the ideal solution (TOPSIS) now is probably one of most popular method for Multiple Criteria Decision Making (MCDM). The method was primarily developed for dealing with real-valued data.		Ludmila Dymova;Pavel V. Sevastjanov;Anna Tikhonenko	2015		10.1007/978-3-319-32152-3_41	mathematical optimization;fuzzy logic;multiple-criteria decision analysis;mathematics;topsis	Logic	-2.594860962490581	-20.809432948875127	147082
5ae0684f6134af3a09fdb691945052124747d99d	constructive and axiomatic approaches to hesitant fuzzy rough set	approximation operator;hesitant fuzzy set;hesitant fuzzy relation;hesitant fuzzy rough set	Hesitant fuzzy set is a generalization of the classical fuzzy set by returning a family of the membership degrees for each object in the universe. Since how to use the rough set model to solve fuzzy problems plays a crucial role in the development of the rough set theory, the fusion of hesitant fuzzy set and rough set is then firstly explored in this paper. Both constructive and axiomatic approaches are considered for this study. In constructive approach, the model of the hesitant fuzzy rough set is presented to approximate a hesitant fuzzy target through a hesitant fuzzy relation. In axiomatic approach, an operators-oriented characterization of the hesitant fuzzy rough set is presented, that is, hesitant fuzzy rough approximation operators are defined by axioms and then, different axiom sets of lower and upper hesitant fuzzy set-theoretic operators guarantee the existence of different types of hesitant fuzzy relations producing the same operators.	rough set	Xibei Yang;Xiaoning Song;Yunsong Qi;Jingyu Yang	2014	Soft Comput.	10.1007/s00500-013-1127-2	mathematical analysis;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;mathematics;fuzzy set;fuzzy set operations	Logic	-1.4570495862900368	-22.952299088893604	147105
f76a2b91a4141cb898f7e2237cb25a96a5265a55	comparing oversampling techniques to handle the class imbalance problem: a customer churn prediction case study		Customer retention is a major issue for various service-based organizations particularly telecom industry, wherein predictive models for observing the behavior of customers are one of the great instruments in customer retention process and inferring the future behavior of the customers. However, the performances of predictive models are greatly affected when the real-world data set is highly imbalanced. A data set is called imbalanced if the samples size from one class is very much smaller or larger than the other classes. The most commonly used technique is over/under sampling for handling the class-imbalance problem (CIP) in various domains. In this paper, we survey six well-known sampling techniques and compare the performances of these key techniques, i.e., mega-trend diffusion function (MTDF), synthetic minority oversampling technique, adaptive synthetic sampling approach, couples top-N reverse k-nearest neighbor, majority weighted minority oversampling technique, and immune centroids oversampling technique. Moreover, this paper also reveals the evaluation of four rules-generation algorithms (the learning from example module, version 2 (LEM2), covering, exhaustive, and genetic algorithms) using publicly available data sets. The empirical results demonstrate that the overall predictive performance of MTDF and rules-generation based on genetic algorithms performed the best as compared with the rest of the evaluated oversampling methods and rule-generation algorithms.	customer relationship management;genetic algorithm;k-nearest neighbors algorithm;oversampling;performance;predictive modelling;sampling (signal processing);synthetic intelligence	Adnan Amin;Sajid Anwar;Awais Adnan;Muhammad Nawaz;Newton Howard;Junaid Qadir;Ahmad Y. A. Hawalah;Amir Hussain	2016	IEEE Access	10.1109/ACCESS.2016.2619719	sampling;genetic algorithm;prediction;computer science;machine learning;data mining;predictive modelling;customer satisfaction;customer retention	AI	7.112394572708046	-22.671885799302757	147111
ef006bfe53793e093dc17983fb8497a59fdf068c	dynamic decision support in the internet marketing management		The article deals with the problem of selecting an advertisement variant on the basis of dynamically-changing values of evaluation criteria. Therefore, a framework, used in an online environment, of a dynamic multi-criteria decision analysis (DMCDA) has been prepared. The framework was based on the PROMETHEE method which makes it possible to carry out a very detailed analysis of a decision process and obtained solutions. While applying the prepared framework, a number of ad variants were considered on the basis of the data collected during a subjective study and a field experiment. In the course of solving the decision problem, the advertiser’s and website operator’s perspectives as well as two aggregation strategies of dynamic data were considered. As a result, the following was obtained: partial rankings of variants, global rankings considering the advertiser’s and publisher’s points of view, GDSS rankings pointing to compromise solutions by merging the two points of view. The obtained solutions were verified by means of: examining correlation coefficients, a GAIA analysis and an analysis of ranking robustness to preference changes. The end result was that the most satisfying advertiser and publisher were determined.	digital marketing	Pawel Ziemba;Jaroslaw Jankowski;Jaroslaw Watrobski	2018	Trans. Computational Collective Intelligence	10.1007/978-3-319-90287-6_3	robustness (computer science);decision analysis;data mining;field experiment;decision problem;ranking;dynamic data;decision support system;compromise;computer science	ECom	-3.9588482063332733	-18.452508025112135	147116
c79f6861ad03707736b681bb31949664b0c992b7	put currency option pricing under uncertain environments		Based on an uncertain currency model, put currency option pricing problem is discussed. Furthermore, we derive the pricing formulas of European and American put currency option. Meanwhile, this paper analyzes the relationship between pricing formulas and the relevant parameters. At last, two numerical examples presented in this paper illustrate the pricing formulas.	capability maturity model;entity–relationship model;numerical analysis	Xiao Wang;Yufu Ning	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8392974	mathematical optimization;computer science;financial economics;valuation of options;currency;differential equation	DB	1.0922107821492752	-11.972770980911514	147339
732e3ed2c6e7a34ba7300902cba4a243cdb5561d	towards intuitive understanding of the cauchy deviate method for processing interval and fuzzy uncertainty		One of the most efficient techniques for processing interval and fuzzy data is a Monte-Carlo type technique of Cauchy deviates that uses Cauchy distributions. This technique is mathematically valid, but somewhat counterintuitive. In this paper, following the ideas of Paul Werbos, we provide a natural neural network explanation for this technique. Keywords— Cauchy deviate method, fuzzy uncertainty, interval uncertainty, Monte-Carlo simulations, neural networks 1 Formulation of the Problem: Cauchy Deviate Method and Need for Intuitive Explanation 1.1 Practical Need for Uncertainty Propagation In many practical situations, we are interested in the value of a quantity y which is difficult or even impossible to measure directly. To estimate this difficult-to-measure quantity y, we measure or estimate related easier-to-measure quantities x1, . . . , xn which are related to the desired quantity y by a known relation y = f(x1, . . . , xn). Then, we apply the relation f to the estimates x̃1, . . . , x̃n for xi and produce an estimate ỹ = f(x̃1, . . . , x̃n) for the desired quantity y. In the simplest cases, the relation f(x1, . . . , xn) may be an explicit expression: e.g., if we know the current x1 and the resistance x2, then we can measure the voltage y by using Ohm’s law y = x1 · x2. In many practical situations, the relation between xi and y is much more complicated: the corresponding algorithm f(x1, . . . , xn) is not an explicit expression, but a complex algorithm for solving an appropriate non-linear equation (or system of equations). Estimates are never absolutely accurate: • measurements are never absolutely precise, and • expert estimates can only provide approximate values of the directly measured quantities x1, . . . , xn. In both cases, the resulting estimates x̃i are, in general, different from the actual (unknown) values xi. Due to these estimation errors ∆xi def = x̃i − xi, even if the relation f(x1, . . . , xn) is exact, the estimate ỹ = f(x̃1, . . . , x̃n) is different from the actual value y = f(x1, . . . , xn): ∆y def = ỹ − y 6= 0. (In many situations, when the relation f(x1, . . . , xn) is only known approximately, there is an additional source of the approximation error in y caused by the uncertainty in knowing this relation.) It is therefore desirable to find out how the uncertainty ∆xi in estimating xi affects the uncertainty ∆y in the desired quantity, i.e., how the uncertainties ∆xi propagate via the algorithm f(x1, . . . , xn). 1.2 Propagation of Probabilistic Uncertainty Often, we know the probabilities of different values of ∆xi. For example, in many cases, we know that the approximation errors ∆xi are independent normally distributed with zero mean and known standard deviations σi; see, e.g., [16]. In this case, we can use known statistical techniques to estimate the resulting uncertainty ∆y in y. For example, since we know the probability distributions, we can simulate them in the computer, i.e., use the Monte-Carlo simulation techniques to get a sample population ∆y, . . . , ∆y of the corresponding errors ∆y. Based on this sample, we can then estimate the desired statistical characteristics of the desired approximation error ∆y. 1.3 Propagation of Interval Uncertainty In many other practical situations, we do not know these probabilities, we only know the upper bounds ∆i on the (absolute values of) the corresponding measurement errors ∆xi: |∆xi| ≤ ∆. In this case, based on the known approximation x̃i, we can conclude that the actual (unknown) value of i-th auxiliary quantity xi can take any value from the interval xi = [x̃i −∆i, x̃i + ∆i]. (1) To find the resulting uncertainty in y, we must therefore find the range y = [y, y] of possible values of y when xi ∈ xi: y = f(x1, . . . ,xn) def = {f(x1, . . . , xn) |x1 ∈ x1, . . . , xn ∈ xn}. (2) Computations of this range under interval uncertainty is called interval computations; see, e.g., [4, 5]. The corresponding computational problems are, in general, NP-hard [9]. Crudely speaking, this means that, in general, such problems require a large amount of computation time – and that therefore faster methods are needed. 1.4 Propagation of Fuzzy Uncertainty In many practical situations, the estimates x̃i come from experts. Experts often describe the inaccuracy of their estimates in terms of imprecise words from natural language, such as “approximately 0.1”, etc. A natural way to formalize such words is to use special techniques developed for formalizing this type of estimates – specifically, the technique of fuzzy logic; see, e.g., [6, 15]. In this technique, for each possible value of xi ∈ xi, we describe the degree μi(xi) to which this value is possible. For each degree of certainty α, we can determine the set of values of xi that are possible with at least this degree of certainty – the α-cut xi(α) = {x |μ(x) ≥ α} of the original fuzzy set. Vice versa, if we know α-cuts for every α, then, for each object x, we can determine the degree of possibility that x belongs to the original fuzzy set [2, 6, 12, 13, 15]. A fuzzy set can be thus viewed as a nested family of its (interval) α-cuts. We already know how to propagate interval uncertainty. Thus, to propagate this fuzzy uncertainty, we can therefore consider, for each α, the fuzzy set y with the α-cuts y(α) = f(x1(α), . . . ,x1(α)); (3) see, e.g., [2, 6, 12, 13, 15]. So, from the computational viewpoint, the problem of propagating fuzzy uncertainty can be reduced to several interval propagation problems. 1.5 Need for Faster Algorithms for Uncertainty Propagation Summarizing the above analysis, we can conclude that in principle, we need to consider three possible types of uncertainty propagation: situations when we propagate probabilistic, interval, and fuzzy uncertainty. For probabilistic uncertainty, there exist reasonable efficient uncertainty propagation algorithms such as Monte-Carlo simulations. In contrast, the problems of propagating interval and fuzzy uncertainty are, in general, computationally difficult. It is therefore desirable to design faster algorithms for propagating interval and fuzzy uncertainty. The computational problem of propagating fuzzy uncertainty can be naturally reduced to the problem of propagating interval uncertainty. Because of this reduction, in the following text, we will mainly concentrate on faster algorithms for propagating interval uncertainty. 1.6 Linearization Situations: Description Due to the approximation errors ∆xi = x̃i− xi, the unknown (actual) values xi = x̃i−∆xi of the input quantities xi are, in general, different from the approximate estimates x̃i. In many practical situations, the approximation errors ∆xi are small – e.g., when the approximations are obtained by reasonably accurate measurements. In such situations, we can ignore terms which are quadratic (and of higher order) in ∆xi. 1.7 Linearization Situations: Analysis In the above situations, we can expand the expression for ∆y = ỹ − y = f(x̃1, . . . , x̃n)− f(x1, . . . , xn) = f(x̃1, . . . , x̃n)− f(x̃1 −∆x1, . . . , x̃n −∆xn) (4) in Taylor series in ∆xi and keep only the linear terms in this expansion. In this case, we get ∆y = c1 ·∆x1 + . . . + cn ·∆xn, (5) where we denoted	approximation algorithm;approximation error;artificial neural network;carrier-to-noise ratio;computation;computational problem;degree (graph theory);existential quantification;fuzzy concept;fuzzy logic;fuzzy set;interval arithmetic;interval propagation;linear equation;monte carlo method;np-hardness;natural language;nonlinear system;ohm's law;paul werbos;propagation of uncertainty;quadratic function;simulation;software propagation;time complexity;uncertainty principle	Vladik Kreinovich;Hung T. Nguyen	2009				Robotics	0.6762300024084411	-18.526804796282054	147477
62ba67fc4f13fd33f8616eb8909c182519665a29	exchange rate forecasting with optimum singular spectrum analysis	ghodsi mansi yarmohammadi masoud 时间序列预测 奇异谱分析 汇率 ssa技术 预测性能 挑战性 吸引力 奇异值 exchange rate forecasting with optimum singular spectrum analysis	Forecasting exchange rate is undoubtedly an attractive and challenging issue that has been of interest in different domains for many years. The singular spectrum analysis (SSA) technique has been used as a promising technique for time series forecasting including exchange rate series. The SSA technique is based upon two main choices: Window length, L, and the number of singular values, r. These values are very important for the reconstruction stage and forecasting purposes. Here the authors consider an optimum version of the SSA technique for forecasting exchange rates. The forecasting performances of the SSA technique for one-step-ahead forecast of six exchange rate series are used to find the best L and r.		Mansi Ghodsi;Masoud Yarmohammadi	2014	J. Systems Science & Complexity	10.1007/s11424-014-3303-6	econometrics;mathematics;mathematical economics	Logic	4.420554863038299	-13.529639877389915	147558
41251a9eb654be55f6047798376f2084124f7694	empirical mode decomposition based adaboost-backpropagation neural network method for wind speed forecasting	forecasting;forecasting predictive models wind speed time series analysis autoregressive processes neurons benchmark testing;wind forecasting empirical mode decomposition ensemble method adaboost backpropagation neural network;autoregressive processes;time series analysis;wind speed;wind backpropagation learning artificial intelligence neural nets regression analysis statistical testing weather forecasting;predictive models;neurons;benchmark testing;wind speed forecasting empirical mode decomposition adaboost backpropagation neural network method emd bpnn regression tree statistical tests	Wind speed forecasting is a popular research direction in renewable energy and computational intelligence. Ensemble forecasting and hybrid forecasting models are widely used in wind speed forecasting. This paper proposes a novel ensemble forecasting model by combining Empirical mode decomposition (EMD), Adaptive boosting (AdaBoost) and Backpropagation Neural Network (BPNN) together. The proposed model is compared with six benchmark models: persistent, AdaBoost with regression tree, BPNN, AdaBoost-BPNN, EMD-BPNN and EMD-AdaBoost with regression tree. The comparisons undergoes several statistical tests and the tests show that the proposed EMD-AdaBoost- BPNN model outperformed the other models significantly. The forecasting error of the proposed model also shows significant randomness.	adaboost;artificial neural network;backpropagation;benchmark (computing);computational intelligence;decision tree learning;ensemble forecasting;hilbert–huang transform;persistent data structure;randomness	Ye Ren;Xueheng Qiu;Ponnuthurai Nagaratnam Suganthan	2014	2014 IEEE Symposium on Computational Intelligence in Ensemble Learning (CIEL)	10.1109/CIEL.2014.7015741	probabilistic forecasting;econometrics;engineering;machine learning;pattern recognition	AI	8.425423943990856	-19.41056128921559	147835
b79936acf44bb023a639ade525c9b786d1152931	fine-grained multi-factor hail damage modelling		A fine-grained multi-factor estimation of crop-hail damage is required to progress from manual inspection of crops post-event to automated assessment and accurate forecasting of the expected impact on agricultural areas. Such automated processes will enable more accurate claims processing, improve customer satisfaction, and reduce insurance losses. This paper demonstrates the value of Gaussian Processes for the construction of such a multi-factor model of crop-hail damage. This is underpinned by a survey of public datasets, and a description of the target dataset to support an operational crop-hail damage model.	customer relationship management;experiment;gaussian process;signal-to-noise ratio;while	Melanie E. Roberts;Shrihari Vasudevan	2015	2015 Conference on Technologies and Applications of Artificial Intelligence (TAAI)	10.1109/TAAI.2015.7407101	simulation;engineering;data mining;operations research	AI	4.8542449576189695	-16.24859889948518	147844
0bb94c3e323e919ff61a6e1baa5d3f7046f92094	mathematical fuzzy logic: an invitation to interesting research areas	mathematical fuzzy logic;fuzzy sets;fuzzy logic	The paper discusses some core topics which present open problems in the field of mathematical fuzzy logic and in the foundations of fuzzy set theory. It may reasonably be assumed that these problems shall have great influence for the future development of fuzzy logic within the next decade.	fuzzy logic;fuzzy set;set theory	Siegfried Gottwald	2007	Fundam. Inform.		fuzzy logic;t-norm fuzzy logics;combs method;fuzzy electronics;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control language;algorithm;fuzzy control system	AI	-1.9368728525408287	-23.65536035310551	148007
03cc8f8847f18aca37618a3fbaa1c493240e5eee	computation of the optimal tolls on the traffic network	bilevel optimization;toll problem;fuzzy optimization	The present paper is devoted to the computation of optimal tolls on a traffic network that is described as fuzzy bilevel optimization problem. As a fuzzy bilevel optimization problem we consider bilinear optimization problem with crisp upper level and fuzzy lower level. An effective algorithm for computation optimal tolls for the upper level decision-maker is developed under assumption that the lower level decision-maker chooses the optimal solution as well. The algorithm is based on the membership function approach. This algorithm provides us with a global optimal solution of the fuzzy bilevel optimization problem.		Alina Budnitzki	2014	European Journal of Operational Research	10.1016/j.ejor.2013.10.059	mathematical optimization;fuzzy transportation;mathematics;mathematical economics;bilevel optimization;algorithm	Theory	0.16413439089800955	-17.442984202997838	148133
41158cc4ab98ee1ae1cd8eeda5ac1e06b2c1663f	application of neural networks in investments: a case of belex15 stock index	neural nets;time series;investment;stock markets;investments neural networks time series prediction stock index;investment prediction neural network application belex15 stock index time series representation stock index values serbia republic;investment biological neural networks indexes neurons educational institutions time series analysis;time series investment neural nets stock markets	The time series represent a line of observations of unintentional variable in different time periods. The aim of the time series is, on the basis of those observations, to forecast the values of the variable. Further reactions should be predicted on the basis of the data from the past. The research covers the sample representing the Serbian (BELEX15) stock index. The aim of this study is the survey of the algorithm for predicting the reaction of the problem of this type by using the neural networks in function of prediction of the daily stock index values in the emerging market of the Republic of Serbia. The proposal of the algorithm is implemented by software and the results are significant both to academics and professionals in the subject area, especially having in mind the possibility of prediction in investments.	algorithm;artificial neural network;mind;time series	Nebojsa M. Ralevic;Natasa Glisovic;Jelena S. Kiurski;Vladimir Dj. Djakovic;Goran B. Andjelic	2013	2013 IEEE 11th International Symposium on Intelligent Systems and Informatics (SISY)	10.1109/SISY.2013.6662563	financial economics;actuarial science;economics;economy	Robotics	7.238140682717265	-18.6928219476852	148138
c3b44f350b364e78a1395a218a6947aee94073bf	yet another note on the range of null-additive fuzzy and non-fuzzy measures	fuzzy measure;atom;additive set function;finite set function;exhaustive set function	In this paper, we point out that the ‘only if’ part of Lemma 1 in a previous paper by Pap [The range of null-additive fuzzy and non-fuzzy measures Fuzzy Sets and Systems 65 (1994) 105–115] is incorrect. We give a correct necessary and sufficient condition and its proof. © 2009 Elsevier B.V. All rights reserved.	fuzzy sets and systems;fuzzy measure theory;utility functions on indivisible goods;yet another	Sun Bo;Congxin Wu	2010	Fuzzy Sets and Systems	10.1016/j.fss.2009.05.006	fuzzy logic;combinatorics;mathematical analysis;discrete mathematics;atom;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations	AI	0.28916150117642747	-21.96247051538197	148304
5d5575696da726944da9b69de5d61c0dd74c129c	conflict resolution for free flight considering degree of danger and concession	conflict resolution	In this study a conflict resolution technique based on danger and concession considerations is presented for free flight paradigm. A danger function which assigns a danger value for the conflict situation, and a concession function which assigns a concession value for the path followed by the aircraft are constructed. The danger and concession values are input to a fuzzy decision module. This module outputs the amount of deviation from the optimal path and the conflict is solved following these deviations. The method presented here is the third method we have been studying regarding to the conflict resolution problem. Its results are presented with a comparison to our other two studies.	programming paradigm	Mustafa Suphi Erden;Kemal Leblebicioglu	2004			computer science;operations research;computer security	EDA	7.2604758719267455	-12.864454881129573	148305
0e33b697a19ad724d4d96dac38a08338eef32926	a multiple criteria decision-making approach for the selection of stocks	modelizacion;forecasting;multicriteria analysis;reliability;project management;information systems;bolsa valores;maintenance;process selection;multiple criteria decision making;portfolio selection;soft or;information technology;seleccion cartera;seleccion proceso;packing;prise de decision;operations research;location;choix procede;investment;journal;journal of the operational research society;bourse valeurs;inventory;selection portefeuille;stock exchange;purchasing;modelisation;history of or;logistics;stock selection;marketing;scheduling;bibliographic review;portfolio management;preferencia;production;communications technology;preference;gestion cartera;analisis multicriterio;computer science;operational research;analyse multicritere;gestion portefeuille;toma decision;application;modeling;applications of operational research;or society;jors;management science;infrastructure	A fundamental principle of modern portfolio theory is that portfolio selection decisions are generally made using two criteria, corresponding to the first two moments of return distributions, namely the expected returnportfolio variance. One criticism over this theory, which has often been addressed both by practitioners and academics, is that it fails to embody all the decision-maker’s objectives, through the various stages of the decision process. The aim of this paper is to present an alternative methodological approach for modeling one of the most crucial phases of the portfolio management process, the security selection phase. The main characteristic of the proposed approach is that it fully takes into account the inherent multi-dimensional nature of the problem, although allowing the decision-maker to incorporate his preferences in the decision process. The validity of the proposed approach is tested through an illustrative application in Athens Stock Exchange. Besides, a detailed categorized bibliography is provided, relative to the application of the techniques of multiple criteria decision making to the problems and issues of portfolio management. Journal of the Operational Research Society (2010) 61, 1273–1287. doi:10.1057/jors.2009.74 Published online 5 August 2009	categorization;game-maker;modern portfolio theory	Panagiotis Xidonas;George Mavrotas;John E. Psarras	2010	JORS	10.1057/jors.2009.74	project management;logistics;stock exchange;inventory;economics;forecasting;investment;marketing;operations management;finance;reliability;management science;location;management;operations research;information technology;multiple-criteria decision analysis;separation property;scheduling;application portfolio management	AI	-1.0338433189715797	-14.082137064257724	148344
5faa4b018a097444c67c9846fb95b39af46346c4	network security situation element extraction based on projection pursuit regression	analytical models;ubiquitous computing convergence nonlinear programming particle swarm optimisation polynomials regression analysis security of data;convergence;nonlinear programming;security optimization analytical models polynomials data mining indexes data models;data mining;polynomials;particle swarm optimization network security situation element extraction model projection pursuit regression network security situation awareness nonlinear optimization problem projection direction polynomial coefficient optimization variable projective index function convergence speed;indexes;ubiquitous computing;regression analysis;optimization;security;particle swarm optimisation;security of data;data models	Situation element extraction is a significant step of network security situation awareness. This paper presents a network security situation element extraction model based on projection pursuit regression. A nonlinear optimization problem is proposed in the model, in which the projection direction and the polynomial coefficients are optimization variables, and projective index function is target function. Due to the high precision and fast convergence speed, the particle swarm optimization is introduced to solve the optimization problem. The simulation experimental results show that this model is efficient.	coefficient;computation;entity–relationship model;mathematical optimization;network security;nonlinear programming;nonlinear system;optimization problem;pp (complexity);particle swarm optimization;polynomial;simulation	Bin Wen;Wenzhong Guo;Guolong Chen	2012	2012 Sixth International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing	10.1109/IMIS.2012.63	database index;data modeling;mathematical optimization;multi-swarm optimization;convergence;nonlinear programming;computer science;information security;theoretical computer science;machine learning;ubiquitous computing;regression analysis;polynomial	EDA	4.136968930596761	-22.64787958099982	148418
628a5afbf9bd6836b9db9a3f98c9f4bceab211fe	framework of group decision making with intuitionistic fuzzy preference information	maintenance engineering;additives;vectors;aggregates;china scholarship council intuitionistic fuzzy preference information operation research and management science intuitionistic fuzzy preference relations ifpr intuitionistic fuzzy group decision making framework consistency checking method multiplicative consistency iterative procedure consensus reaching procedure;scholarships;decision making vectors educational institutions aggregates scholarships additives maintenance engineering;operations research distributed decision making fuzzy set theory iterative methods	Group decision making is an essential activity in various fields of operations research and management science. This paper focuses on the intuitionistic fuzzy group decision making problem in which all the experts use the intuitionistic fuzzy preference relations (IFPRs) to express their preferences. To start our discussion, we first propose the novel framework of intuitionistic fuzzy group decision making and clarify the difficulties in deriving the final result which is accepted by all individuals in the group. Next, a consistency checking method, which is based on the multiplicative consistency, is developed to check the consistency of each IFPR furnished by the group of experts. For those IFPRs that do not have the acceptable consistency, an iterative procedure is proposed to improve the consistency. Furthermore, after introducing a novel consensus measure, an interesting consensus-reaching procedure is developed to help the group to find a solution which is accepted by most members in the group. Finally, in order to make our approaches more applicable, a step-by-step algorithm is given. A numerical example concerning the selection of outstanding Ph.D. students for the China Scholarship Council is given to illustrate and validate the proposed approaches.	algorithm;fuzzy set;intuitionistic logic;iterative method;management science;numerical analysis;operations research	Huchang Liao;Zeshui Xu;Xiao-Jun Zeng;José M. Merigó	2015	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2014.2348013	maintenance engineering;food additive;knowledge management;artificial intelligence;data mining;mathematics;management science	SE	-2.835363267766009	-19.752132645783483	148448
64e2a26e9f97471ef00a585720ed28d3d01611c4	multiple attribute decision making with completely unknown weights based on cumulative prospect theory and grey system theory	prospect theory;grey correlation deep coefficient;multiple attribute decision making;completely unknown weights	The purposes of this paper are to study multiple attribute decision making problems by considering the behavioral characteristics of decision makers where the attribute weights are completely unknown. To determine the attribute weights, an optimization model based on prospect theory and the grey relation deep coefficient, from which the attribute weights can be determined, was established. The value function and decision weight function were used to calculate the overall prospect values of attributes for each alternative, and then rank the alternatives to select the most desirable one in accordance with the scores. In order to verify this method, it was used to study an illustrative example using, with the results demonstrating its feasibility and effectiveness. And it can be drawn the conclusion that the proposed method can be applied to decision making problems when the attribute weights are completely unknown while considering the decision maker's behavior at the same time.	attribute grammar;bellman equation;coefficient;constraint algorithm;governance, risk management, and compliance;mathematical optimization;nonlinear programming;nonlinear system;systems theory;weight function	Aihua Li;Zhangyan Zhao;Mengke Wen	2016		10.1145/3028842.3028848	variable and attribute;artificial intelligence;operations management;data mining;mathematics	ML	-4.044467527583693	-18.216862864329364	148460
b34ea98289e4610df9fbd099e08342686ed4e216	an enhanced fuzzy time series forecasting method based on artificial bee colony		In recent years, several forecasting methods have been proposed for the analysis of fuzzy time series. Determination of fuzzy relations and establishing interval lengths, which is used in partition of universe of discourse, can be considered as the two of main elements affecting the forecasting performance of these forecasting methods. In the literature, along with the studies in which interval lengths are determined subjectively, algorithms such as genetic algorithms and particle swarm optimization have been utilized. In this study, a new fuzzy time series forecasting method which uses Artificial Bee Colony (ABC) algorithm for the determination of interval lengths for the first time in the literature is proposed. To obtain forecasts, this new method makes use of fuzzy logic relationship tables in determining the fuzzy relations and also uses estimating based on next state (EBN) for training set and master voting (MV) scheme for test set. The new proposed method is applied to three various time series and when compared with the existing methods better results are obtained with regard to both training and test set.	artificial bee colony algorithm;time series	Ufuk Yolcu;Ozge Cagcag;Çagdas Hakan Aladag;Erol Egrioglu	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-130933	artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy set operations	Robotics	3.9229500398331867	-23.916816858778635	148464
c4a7a9e44c211660f4c0cde3ee5c7436dc4c9f9a	the micro individual characteristics in non-motorized traffic mixed of bicycles and mopeds	bicycles and mopeds;non motorized traffic mixed;micro individual characteristics	In order to understand the overtaking behavior of, overtaking physical separation events between bicycles and mopeds was taken out. According to the analytical model of road bicycle passing events vehicle spatial features, the bicycle passing events into free events overtaking, adjacent events and blocked events, and based on the bicycle traffic flow velocity distribution characteristics and bicycle space distribution probability, an analytic model of 3 kinds of passing events was taken out. According to the passing events’ survey results on the physically separated nonmotorized lanes of Shanghai, the model is calibrated and verified and the passing events’ overtaking number calculation results was according with the real situation. The model stream the results show that the parameters for the bicycle traffic, road bicycle, bicycle traffic flow in the running speed, traffic flow speed standard deviation and lane number significantly influence of 3 kinds of overtaking event number. This paper aims to present the micro individual characteristics in non-motorized traffic mixed of bicycles and mopeds.	experiment;glossary of computer graphics;left 4 dead 2;moped;mathematical optimization;velocity (software development)	Xiaohong Chen;Xueli Fang	2014	JNW	10.4304/jnw.9.10.2782-2787	simulation	Metrics	9.909610338555394	-10.235607214066338	148543
2806c691fccd9f3508f7303c72701b0b49cd1d36	index matrices as a decision-making tool for job appointment		The paper explores the process of decision making, related to the appointment of the human factor in an incomplete information environment. We propose for the first time a new approach to optimization of the process of appointment and reappointment, based on partial knowledge about the values of evaluation criteria of the human resources over time, using the apparatuses of index matrices and of intuitionistic fuzzy sets.		Velichka Traneva;Vassia Atanassova;Stoyan Tranev	2018		10.1007/978-3-030-10692-8_18	operations research;complete information;fuzzy set;matrix (mathematics);human resources;computer science	Theory	-4.076456091835868	-17.448945838584425	148589
571a88a37f48ebffbe2f27f59661b29e247acc4c	risk factor analysis and portfolio immunization in the corporate bond market	analyse risque;factor riesgo;term structure;banking;analisis factorial;risk factor;scienze economiche e statistiche;bolsa valores;risk analysis;corporate bonds;secteur bancaire;credit;facteur risque;immunization;bourse valeurs;stock exchange;analisis riesgo;risk factors;analyse factorielle;factor model;credito;factor analysis;discipline marche;multi factor model;credit risk	In this paper we develop a multi-factor model for the yields of corporate bonds. The model allows the analysis of factors which in°uence the changes in the term structure of corporate bonds. More than 98% of the variability in the corporate bond market is captured by the model, which is then used to develop credit risk immunization strategies. Empirical results are given for the U.S. market using data for the period 1992-1999. ¤Department of Mathematics, Statistics, Informatics and Applications,University of Bergamo, Bergamo, Italy. Email: marida@unibg.it yDepartment of Mathematics, Statistics, Informatics and Applications,University of Bergamo, Bergamo, Italy. Email: rosella@unibg.it zDepartment of Public and Business Administration, University of Cyprus, Nicosia, Cyprus, and Senior Fellow, The Wharton School, University of Pennsylvania, Philadelphia, PA. Email: zenioss@ucy.ac.cy	email;factor analysis;informatics;risk aversion;spatial variability	Marida Bertocchi;Rosella Giacometti;Stavros A. Zenios	2005	European Journal of Operational Research	10.1016/j.ejor.2003.08.047	financial economics;bond market;actuarial science;economics;bond market index;economy;factor analysis;risk factor	ML	3.812309342494777	-12.394786965044007	148646
787e0260382e872fc5d01010cec4a12cdc933056	an approach to budget allocation for an aerospace company - fuzzy analytic hierarchy process and artificial neural network	budget allocation;decision maker;tangible criteria;artificial intelligent;sensitivity analysis;intangible criteria;artificial intelligence;fuzzy analytic hierarchy process;artificial neural network	Budgetary allocations of resources are made in all businesses, but their volume and composition vary; and efficient budget allocation is fundamental to flow in businesses. The objective of the allocation problem is to determine the required budget for each department (or section) of a company so as to maximize the sum of the company's benefits. The purpose of this paper is to find a suitable degree of fuzziness for preference rankings and to demonstrate an example of budget allocation using artificial intelligence programming, and fuzzy analytic hierarchy process (FAHP). An efficient budget allocation method using FAHP will be provided for businesses. This method is suitable for use in evaluating proposed policies (including tangible and intangible information). A comparison between FAHP and artificial neural network (ANN) will be also made in this paper. An aerospace company's budget allocation problem is investigated as a case study in this research, which will illustrate how to solve this problem. The case study utilizes a two-stage interview (semi-structured interview and in-depth interview) to select their budget allocations given a number of tangible and intangible criteria. The results from the case study are pertinent to other real-world allocation problems that share many of the characteristics of problems, such as decision makers' subjective opinions.	analytical hierarchy;artificial neural network	Yu-Cheng Tang	2009	Neurocomputing	10.1016/j.neucom.2009.03.020	decision-making;computer science;machine learning;management science;operations research;sensitivity analysis;artificial neural network	AI	-3.935121107607613	-16.763742898025644	148676
316586c7a1958061a8c4316e3567d9eb41a655a4	visual predictions of currency crises using self-organizing maps	self organizing maps;currency crises visual prediction self organizing maps nonparametric neural network based visualization som based model;computer crashes;currency crises;training;foreign exchange trading;biological system modeling;probit analysis;currency crisis;currency crisis prediction visualization self organizing maps probit analysis evaluation early warning;biological system modeling training predictive models computer crashes accuracy data models neurons;early warning;accuracy;visualization;self organising feature maps;probit model;self organising feature maps foreign exchange trading;predictive models;visual prediction;self organized map;evaluation;nonparametric neural network based visualization;som based model;neurons;prediction;data models;neural network	Throughout the 1990s, four global waves of financial turmoil occurred. The beginning of the 21st century has also suffered from several crisis episodes, including the severe sub prime crisis. However, to date, the forecasting results are still disappointing. This paper examines whether new insights can be gained from the application of the Self-Organizing Map (SOM) – a non-parametric neural network-based visualization tool. We develop a SOM-based model for prediction of currency crises. We evaluate the predictive power of the model and compare it with that of a classical probit model. The results indicate that the SOM-based model is a feasible tool for predicting currency crises. Moreover, its visual capabilities facilitate the understanding of the factors and conditions that contribute to the emergence of currency crises in various parts of the world.		Peter Sarlin;Dorina Marghescu	2010		10.1109/ICDMW.2010.55	probit model;simulation;computer science;artificial intelligence;machine learning;data mining;artificial neural network;statistics	ECom	7.373653043886361	-19.342475094099207	148704
e13461a21fa1dd6e810aeadb92ca8d6b966c4e9d	modelling and predicting partial orders from pairwise belief functions	label ranking;multilabel classification;belief functions;paired comparisons;dempster shafer theory;partial orders	In this paper, we introduce a generic way to represent and manipulate pairwise information about partial orders (representing rankings, preferences, . . . ) with belief functions. We provide generic and practical tools to make inferences from this pairwise information, and illustrate their use on the machine learning problems that are label ranking and multi-label prediction. Our approach differs from most other quantitative approaches handling complete or partial orders, in the sense that partial orders are here considered as primary objects and not as incomplete specifications of ideal but unknown complete orders.	algorithm;application domain;complete (complexity);interval arithmetic;linear programming;machine learning;maximal set;multi-label classification;plausibility structure;semiconductor industry	Marie-Hélène Masson;Sébastien Destercke;Thierry Denoeux	2016	Soft Comput.	10.1007/s00500-014-1553-9	dempster–shafer theory;machine learning;pattern recognition;data mining;mathematics;statistics	AI	2.478141518402349	-18.882587383319294	148868
af46105c4fe320d2264014aa18f749afe4a243bc	"""comments on """"distinguishability quantification of fuzzy sets"""""""	fuzzy set;possibility measure;distinguishability measure;similarity measure	This paper presents some comments on the above paper regarding the manipulation of Gaussian fuzzy sets and provides two useful generalized formulas for both of similarity and similarity/possibility relationships of Gaussian fuzzy sets with different widths.	fuzzy set	Hesham A. Hefny	2007	Inf. Sci.	10.1016/j.ins.2007.02.027	mathematical analysis;discrete mathematics;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;fuzzy measure theory;mathematics;fuzzy set	DB	-0.29042869979545305	-22.1081807409514	148887
6f80431c3396a90396750cfdd218ba18e4c0e6f8	measuring consensus in a preference-approval context	preference approval;consensus;approval voting;kemeny metric;hamming metric	We consider measuring the degree of homogeneity for preference-approval profiles which include the approval information for the alternatives as well as the rankings of them. A distance-based approach is followed to measure the disagreement for any given two preference-approvals. Under the condition that a proper metric is used, we propose a measure of consensus which is robust to some extensions of the ordinal framework. This paper also shows that there exists a limit for increasing the homogeneity level in a group of individuals by simply replicating their preferenceapprovals.	consensus (computer science);ordinal data	Bora Erdamar;José Luis García-Lapresta;David Pérez-Román;M. Remzi Sanver	2014	Information Fusion	10.1016/j.inffus.2012.02.004	mathematical optimization;hamming distance;consensus;computer science;approval voting;data mining;mathematics	Web+IR	1.6325826438763322	-19.098665183578415	148951
8e57c4b3669133e09961df068b7ddd357a2059eb	taiwanese 3g mobile phone demand forecasting by svr with hybrid evolutionary algorithms	genetic algorithm simulated annealing ga sa;general regression neural networks grnn;support vector regression svr;hybrid evolutionary algorithm;autoregressive integrated moving average arima;non linear regression;support vector regression;time series;simulated annealing;third generation 3g mobile phone;mobile phone;arima model;moving average;general regression neural network;base station;genetic algorithm;hybrid algorithm;demand forecasting	Keywords: Demand forecasting Genetic algorithm–simulated annealing (GA–SA) Support vector regression (SVR) Autoregressive integrated moving average (ARIMA) General regression neural networks (GRNN) Third generation (3G) mobile phone a b s t r a c t Taiwan is one of the countries with higher mobile phone penetration rate in the world, along with the increasing maturity of 3G relevant products, the establishments of base stations, and updating regulations of 3G mobile phones, 3G mobile phones are gradually replacing 2G phones as the mainstream product. Therefore, accurate 3G mobile phones demand forecasting is desirable and necessary to communications policy makers and all enterprises. Due to the complex market competitions and various subscribers' demands, 3G mobile phones demand forecasting reveals highly non-linear characteristics. Recently, support vector regression (SVR) has been successfully employed to solve non-linear regression and time-series problems. This investigation employs genetic algorithm–simulated annealing hybrid algorithm (GA–SA) to choose the suitable parameter combination for a SVR model. Subsequently, examples of 3G mobile phones demand data from Taiwan were used to illustrate the proposed SVRGA–SA model. The empirical results reveal that the proposed model outperforms the other two models, namely the autoregressive integrated moving average (ARIMA) model and the general regression neural networks (GRNN) model.	artificial neural network;autoregressive integrated moving average;autoregressive model;capability maturity model;evolutionary algorithm;generalization error;genetic algorithm;hybrid algorithm;local optimum;maxima and minima;mobile phone;nonlinear system;numerical analysis;overfitting;simulated annealing;structural risk minimization;support vector machine;time series	Wei-Chiang Hong;Yucheng Dong;Li-Yueh Chen;Chien-Yuan Lai	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.12.066	support vector machine;autoregressive integrated moving average;simulation;genetic algorithm;simulated annealing;hybrid algorithm;demand forecasting;computer science;artificial intelligence;base station;machine learning;time series;moving average;nonlinear regression;statistics	ML	7.104988578665572	-18.356695933363657	149029
ddfbb8dd24c84dc78a899f1f226a6b3d92c96730	a committee machine based soft sensor as an alternative to multiphase flow meter for oil flow rate prediction of the wells		Multiphase flow meters (MFMs) which measure three-phase oil-gas-water flow rates, are being utilized to make available, quick and accurate well test data in different oil production applications, like in remote or unmanned locations, topside and subsea applications. Data acquisition and production monitoring of the wells are done discretely by conventional MFMs, due to radioactive sources and unmanned location due to wells far distance. This study presents the development of committee machine based soft sensor (CMSS), an alternative way to the conventional MFMs. The proposed CMSS combines the results of linear and nonlinear auto-regression exogenous input (ARX/NARX), artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS) for overall oil flow rate prediction of the wells, on the basis of available temperature and pressure measurements of lines. Each ensemble member has a weight factor which is derived in two ways including simple averaging and weighted averaging. In the weighted averaging method, the optimal combination of the weights is obtained by a novel optimization based method called imperialist competitive algorithm (ICA). Experiments on data set of 31 wells in one of the northern Persian Gulf oil fields of Iran proved the effectiveness of the proposed ICA optimized CMSS with an improved accuracy over the individual experts.	adaptive neuro fuzzy inference system;arx;artificial neural network;autoregressive model;coefficient;committee machine;data acquisition;experiment;imperialist competitive algorithm;independent computing architecture;inference engine;mathematical optimization;mean squared error;neuro-fuzzy;nonlinear autoregressive exogenous model;nonlinear system;simulation;soft computing;test data;unmanned aerial vehicle;whole earth 'lectronic link	Shahram Mollaiy-Berneti	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-130941	simulation;artificial intelligence;machine learning	ML	9.55836527849158	-19.305109107707572	149143
88e307f97a547d0dafed2f0ba44e4bab661f704c	modeling the eur/usd index using ls-svm and performing variable selection		As machine learning becomes more popular in all fields, its use is well known in finance and economics. The growing number of people using models to predict the market's behaviour can modify the market itself so it is more predictable. In this context, the key element is to find out which variables are used to build the model in a macroeco- nomic environment. This paper presents an application of kernel methods to predict the EUR/USD relationship performing variable selection. The results show how after applying a proper variable selection, very accu- rate predictions can be achieved and smaller historical data is needed to train the model.		Luis Javier Herrera;Alberto Guillén;Rubén Martínez;Carlos García;Héctor Pomares;Oresti Baños;Ignacio Rojas	2015		10.1007/978-3-319-19222-2_14	econometrics;artificial intelligence	NLP	6.155024015109026	-18.240477901172255	149194
9c06dbd877ac87225061df8ad9d6718b5a9a0b23	an extension of the naive bayesian classifier	t norms;naive bayesian classifier;classification;naive bayes classifier;ordered weighted average;aggregation operator;owa operators;owa operator	Our objective here is to provide an extension of the naive Bayesian classifier in a manner that gives us more parameters for matching data. We first describe the naive Bayesian classifier, and then discuss the ordered weighted averaging (OWA) aggregation operators. We introduce a new class of OWA operators which are based on a combining the OWA operators with t-norm s operators. We show that the naive Bayesian classifier can seen as a special case of this. We use this to suggest an extended version of the naive Bayesian classifier which involves a weighted summation of products of the probabilities. An algorithm is suggested to obtain the weights associated with this extended naive Bayesian classifier. 2005 Elsevier Inc. All rights reserved.	aggregate data;algorithm;bayesian network;naive bayes classifier;ordered weighted averaging aggregation operator;statistical classification;t-norm	Ronald R. Yager	2006	Inf. Sci.	10.1016/j.ins.2004.12.006	naive bayes classifier;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics	ML	-0.952363795007168	-21.655580775957155	149244
0e9ea86f901121917528be2d99b766d07daa75bc	investor sentiment, disagreement, and the breadth-return relationship			aggregate data;calculus of variations;coefficient;cross section (geometry);cross-sectional data;emoticon;entity–relationship model;fo (complexity);nl-complete;theory;time series	Ling Cen;Hai Lu;Liyan Yang	2013	Management Science	10.1287/mnsc.1120.1633		AI	4.633927645052433	-13.44692740426114	149377
de20beb1ec26d6f8bc7cd90e690560ccf003dfe5	evolutionary prediction of total electron content over cyprus	time scale;genetic program;ionosphere;seasonality;total electron content;evolutionary algorithm;neural network;global position ing system	Total Electron Content (TEC) is an ionospheric characteristic used to derive the signal delay imposed by the ionosphere on trans-ionospheric links and subsequently overwhelm its negative impact in accurate position determination. In this paper, an Evolutionary Algorithm (EA), and particularly a Genetic Programming (GP) based model is designed. The proposed model is based on the main factors that influence the variability of the predicted parameter on a diurnal, seasonal and long-term time-scale. Experimental results show that the GP-model, which is based on TEC measurements obtained over a period of 11 years, has produced a good approximation of the modeled parameter and can be implemented as a local model to account for the ionospheric imposed error in positioning. The GP-based approach performs better than the existing Neural Network-based approach in several cases.	approximation;artificial neural network;evolutionary algorithm;gps navigation device;genetic algorithm;genetic operator;genetic programming;global positioning system;heart rate variability;seasonality;source-to-source compiler;spatial variability;total electron content	Alexandros Agapitos;Andreas Konstantinidis;Haris Haralambous;Harris Papadopoulos	2010		10.1007/978-3-642-16239-8_50	total electron content;computer science;machine learning;evolutionary algorithm;ionosphere;artificial neural network;seasonality	AI	9.981338265120792	-18.830649377143157	149417
14a5bd8dbc16eac074903ac71dd6a2803c302898	forecasting complex systems with shared layer perceptrons		We present a recurrent neural network topology, the Shared Layer Percep-tron, which allows robust forecasts of complex systems. This is achieved by several means. First, the forecasts are multivariate, i. e., all observables are forecasted at once. We avoid overfitting the network to a specific observable. The output at time step t, serves as input for the forecast at time step t+1. In this way, multi step forecasts are easily achieved. Second, training several networks allows us to get not only a point forecast, but a distribution of future realizations. Third, we acknowledge that the dynamic system we want to forecast is not isolated in the world. Rather, there may be a multitude of other variables not included in our analysis which may influence the dynamics. To accommodate this, the observable states are augmented by hidden states. The hidden states allow the system to develop its own internal dynamics and harden it against external shocks. Relatedly, the hidden states allow to build up a memory. Our example includes 25 financial time series, representing a market, i. e., stock indices, interest rates, currency rates, and commodities, all from different regions of the world. We use the Shared Layer Perceptron to produce forecasts up to 20 steps into the future and present three applications: transaction decision support with market timing, value at risk, and a simple trading strategy.	complex systems;perceptron	Hans-Jörg von Mettenheim;Michael H. Breitner	2010		10.1007/978-3-642-20009-0_3	artificial intelligence;theoretical computer science;machine learning	HCI	4.279234093659294	-16.381629094812578	149552
8f9333a8bfb37b78f32336c22ce2fde65985aba1	market-based control of epidemics	most rapid approach path market based epidemic control single profit maximizer healing health care provider epidemic outbreak homogeneous network dynamic optimal policies infective nodes permanent immunity sird model reinfection sis model infection threshold;optimal control trajectory economics security humans aggregates analytical models;market based control;optimal policy;optimal control;health care provider;profitability network theory graphs optimal control;profitability;network theory graphs;profit maximization;dynamic optimization	We consider a single profit-maximizer healing or health-care provider that administers treatment to infective nodes during an epidemic outbreak in a homogeneous network. We derive the dynamic optimal policies of the provider for two cases: (a) when infective nodes gain permanent immunity upon treatment (SIRD model), and (b) when treated nodes are again susceptible to future re-infection (SIS model). We show that in the case of the SIRD model the optimal policy is to defer provision of healing until an infection threshold is reached and then to provide healing with maximum intensity. In the case of the SIS model the optimal policy is a Most-Rapid-Approach-Path (MRAP) to an intermediate density of infection.1	emoticon	M. H. R. Khouzani;Santosh S. Venkatesh;Saswati Sarkar	2011	2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton)	10.1109/Allerton.2011.6120184	economics;public economics;operations management;welfare economics	Robotics	-4.0884654525114605	-10.040653482566045	149759
6f2830a7ca106932dfebc918937f713a8c9f813d	on the max-nilpotent t-norm powers of a fuzzy matrix	convergence;max nilpotent composition;oscillation in a finite period;powers of a fuzzy matrix	Fuzzy matrices have been proposed to represent fuzzy relations in finite universes. Various studies have evaluated the powers of a fuzzy matrix with max–min/max-product/max Archimedean t-norm/max t-norm/max-arithmetic mean compositions, indicating that the limiting behavior of the powers of a fuzzy matrix depends on its composition. In this paper, max-nilpotent composition is considered for the fuzzy relations. We demonstrate that the max-nilpotent powers of a fuzzy matrix either converge or oscillate in a finite period. Moreover, the max-nilpotent t-norm powers of a fuzzy matrix A are p-periodic if and only if the powers of an associated Boolean matrix Ac are p-periodic. Finally, necessary and sufficient conditions are proposed for nilpotent fuzzy matrices that exhibit max-nilpotent composition. © 2014 Elsevier B.V. All rights reserved.	converge;fuzzy associative matrix;fuzzy logic;maxima and minima;t-norm	Chia-Cheng Liu;Yan-Kuen Wu;Yung-Yih Lur	2015	Fuzzy Sets and Systems	10.1016/j.fss.2014.08.012	mathematical analysis;discrete mathematics;convergence;computer science;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;algebra	AI	-0.146400958267483	-22.211544908678594	149891
f699182e57d541917a97864717747cc3327f81b1	multivariate forecasting of electricity production using neural network and box-jenkins methodologies	multivariate forecasting;box-jenkins.;neural networks;time series;neural network	The aim of this paper is to prove the validity of an alternative prediction technique to another classical one, which is Box-Jenkins methodology, in order to produce multivariate prediction. In particular, one-step ahead forecasts will be obtained for two time series: thermic and hydraulic power production. These forecasts are based on the past values of those series.	artificial neural network;jenkins;time series	David de la Fuente;Raúl Pino;José Parreño	1998			mathematics;machine learning;artificial intelligence;electricity generation;artificial neural network;box–jenkins;multivariate statistics;hydraulic machinery	ML	7.941898710964022	-19.8254128092262	150188
0cc13201ee2e46f9928378ea1c5d9d35f7238511	fuzzy linguistic reporting in driving simulators	traffic engineering computing computational linguistics fuzzy logic;computability theory;real time;linguistic modeling;driving simulator;computational theory of perception fuzzy linguistic reporting driving simulators intelligent transportation on board systems fuzzy logic;automatic generation;fuzzy logic;traffic engineering computing;computational linguistics;pragmatics vehicles computational modeling wheels roads security sensors	The growth of new intelligent transportation on-board systems has increased dramatically drivers' attention to secondary tasks other than driving. Potential risk of committing mistakes while driving suggests the need to evaluate the onboard systems in order to ensure safe driving practices.	approximation;control theory;driving simulator;integrative level;on-board data handling;pid;prototype;simulation;software release life cycle;user interface	Luka Eciolaza;Gracián Triviño;Beatriz Delgado;Josefa Rojas;Matias Sevillano	2011	2011 IEEE Symposium on Computational Intelligence in Vehicles and Transportation Systems (CIVTS) Proceedings	10.1109/CIVTS.2011.5949529	control engineering;fuzzy electronics;simulation;computer science;artificial intelligence	Embedded	7.367352557834977	-13.168132031631567	150206
fa319c68bf4df218ed123377b9a20e6a9142a9b9	optimal portfolio selection using maximum entropy estimation accounting for the firm specific characteristics		The estimated return and variance for the Markowitz mean-variance optimization have been demonstrated to be inaccurate; thereafter it could make the traditional mean-variance optimization inefficient. This paper applied the Maximum Entropy (ME) principle in portfolio selection while accounting for firm specific characteristics; they are the firm size, return on equity and also lagged 12 months return. Since these characteristics are found not only related to the stock’s expected return, variance and correlation with other stocks, they can be good variables to estimate the weights. Furthermore, this method used Generalized Cross Entropy to shrink portfolio weights to the equal weights; therefore solving the problem of concentrated weights in Markowitz mean-variance framework. Also in our empirical study, six stocks are used to investigate the effect of maximum entropy based methods. The results show that the in-sample forecasts that are in comparison with other traditional methods are good, however, in the out-of-sample forecasts the results are mixed.	entropy estimation	Xue Gong;Songsak Sriboonchitta	2015		10.1007/978-3-319-13449-9_21	financial economics;finance;microeconomics	Mobile	3.1717276144411977	-11.038000876273834	150254
f52ea8c287557e5bfd57c537fa4bd7aac50aacb5	a new multi-criteria weighting and ranking model for group decision-making analysis based on interval-valued hesitant fuzzy sets to selection problems	group decision making analysis;location and supplier selection problems;weighting and ranking model;interval valued hesitant fuzzy sets	The multi-criteria group decision-making methods under fuzzy environments are developed to cope with imprecise and uncertain information for solving the complex group decision-making problems. A team of some professional experts for the assessment is established to judge candidates or alternatives among the chosen evaluation criteria. In this paper, a novel multi-criteria weighting and ranking model is introduced with interval-valued hesitant fuzzy setting, namely IVHF-MCWR, based on the group decision analysis. The interval-valued hesitant fuzzy set theory is a powerful tool to deal with uncertainty by considering some interval-values for an alternative under a set regarding assessment factors. In procedure of the proposed IVHF-MCWR model, weights of criteria as well as experts are considered to decrease the errors. In this regard, optimal criteria’ weights are computed by utilizing an extended maximizing deviation method based on IVHF-Hamming distance measure. In addition, experts’ judgments are taken into account for computing the criteria’ weights. Also, experts’ weights are determined based on proposed new IVHF technique for order performance by similarity to ideal solution method. Then, a new IVHF-index based on Hamming distance measure is introduced to compute the relative closeness coefficient for ranking the candidates or alternatives. Finally, two application examples about the location and supplier selection problems are considered to indicate the capability of the proposed IVHF-MCWR model. In addition, comparative analysis is reported to compare the proposed model and three fuzzy decision methods from the recent literature. Comparing these approaches and computational results shows that the IVHF-MCWR model works properly under uncertain conditions.	centrality;coefficient;computation;decision analysis;fuzzy set;hamming distance;qualitative comparative analysis;set theory;trusted computer system evaluation criteria;via c3	Hossein Gitinavard;S. Meysam Mousavi;Behnam Vahdani	2015	Neural Computing and Applications	10.1007/s00521-015-1958-0	artificial intelligence;machine learning;data mining;mathematics	SE	-3.737513267655045	-19.916397599180662	150264
19c9f94dde957786965e2aea6bee7e5a0a070693	multi-criteria set partitioning for portfolio management: a visual interactive method	decision support;knapsack problems;set partitions;financial management;multi criteria decision making;knapsack problems decision making financial management investment;visual interaction;multicriteria decision making;multicriteria 0 1 knapsack problem;multicriteria set partitioning;decision maker;investment;set partitioning;portfolios decision making marketing and sales humans computational complexity computational intelligence asset management constraint optimization feedback data visualization;knapsack problem portfolio management visual interactive method multi criteria decision support set partitioning;multi dimensional;knapsack problem;multicriteria 0 1 knapsack problem multicriteria set partitioning portfolio management visual interactive method multicriteria decision making hold decision sell decision portfolio assets;multiple constraints;visual interactive method;portfolio management;multi criteria;sell decision;portfolio assets;hold decision	A visual interactive multi-criteria decision-making method for partitioning a portfolio of assets into mutually exclusive categories is presented. The two principal decision categories are hold and sell - portfolio assets in the sell category are considered as potential sale prospects, and the other assets in the portfolio are considered as potential retention prospects. The problem may be mathematically formulated as a multi-criteria 0/1 knapsack problem with multiple constraints. The decision-making method centers on the utilization of several coupled 2D projections of the portfolio in the multi-dimensional criterion space. The decision-maker interacts with these projections in a variety of ways to express and record multi-category (hold, hold-bias, sell-bias, and sell) set partitioning preferences. The decision-maker may also set an aggregated preference threshold that is utilized for partitioning the portfolio into the two principal hold and sell categories. The decision-maker may further fine-tune their preferences and threshold settings so as to achieve a multitude of financial targets.	algorithm;computational complexity theory;decision problem;decision support system;knapsack problem	Raj Subbu;Gregory Russo;Kete Charles Chalermkraivuth;José R. Celaya	2007	2007 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making	10.1109/MCDM.2007.369432	financial management;mathematical optimization;decision support system;replicating portfolio;investment;portfolio optimization;management science;knapsack problem;separation property;welfare economics;project portfolio management	Graphics	-3.7306443181776383	-12.788881430953358	150275
3b5e3cd9dc10b7cbdacfe7d1dfe1e57b20a2357e	comments on determination of redundancies in a set of patterns	uncertainty;information theory uncertainty;information theory			Edward J. McCluskey	1957	IRE Trans. Information Theory	10.1109/TIT.1957.1057403	uncertainty analysis;uncertainty;information theory;data mining;mathematics;sensitivity analysis;statistics	Crypto	1.3069612053708217	-19.254950039391257	150714
87f5be726f68096c9a53dc22c7feb4f786e2dba1	load forecasting accuracy through combination of trimmed forecasts	neural networks;trimmed mean;load forecasting;forecast combination	Neural network (NN) models have been receiving considerable attention and a wide range of publications regarding short-term load forecasting have been reported in the literature. Their popularity is mainly due to their excellent learning and approximation capabilities. However, NN models suffer from the problem of forecasting performance fluctuations in different runs, due to their development and training processes. Averaging of forecasts generated by NNs has been proposed as a solution to this problem. However, this may lead to another problem as odd forecasts may significantly shift the mean resulting in large forecasting inaccuracies. This paper investigates application of a trimming method by removing the α% largest and smallest forecasts and then averaging the rest of the forecasts. A validation set is applied for selecting the best trimming amount for NN load demand forecasts. Performance of the proposed method is examined using a real world data set. Demonstrated results show that although trimmed forecasts are not the best possible ones, they are better than forecasts generated by individual NN models in almost 70% of the cases.		Saima Hassan;Abbas Khosravi;Jafreezal Jaafar;Samir B. Belhaouari	2012		10.1007/978-3-642-34475-6_19	probabilistic forecasting;econometrics;computer science;machine learning;data mining;truncated mean;consensus forecast;artificial neural network;statistics	HCI	8.23529289720109	-20.859717816859533	150725
3e18241372aac9754b105cfc5ab6ead045678e11	the complex fuzzy system forecasting model based on fuzzy svm with triangular fuzzy number input and output	fuzzy theory;fuzzy system forecasting;fuzzy v support vector machine;kernel function;support vector regression;forecasting model;fuzzy support vector machine;journal;multi dimensional;particle swarm optimizer;triangular fuzzy number;particle swarm optimization;support vector machine;wavelet kernel function;fuzzy system;forecasting method	This paper presents a new version of fuzzy support vector machine to forecast the nonlinear fuzzy system with multi-dimensional input variables. The input and output variables of the proposed model are described as triangular fuzzy numbers. Then by integrating the triangular fuzzy theory and v-support vector regression machine, the triangular fuzzy v-support vector machine (TFv-SVM) is proposed. To seek the optimal parameters of TFv-SVM, particle swarm optimization is also applied to optimize parameters of TFv-SVM. A forecasting method based on TFv-SVRM and PSO are put forward. The results of the application in sale system forecasts confirm the feasibility and the validity of the forecasting method. Compared with the traditional model, TFv-SVM method requires fewer samples and has better forecasting precision. 2011 Elsevier Ltd. All rights reserved.	algorithm;approximation;coefficient;experiment;fuzzy control system;fuzzy logic;fuzzy number;fuzzy set;input/output;mathematical optimization;nonlinear system;particle swarm optimization;simulation;support vector machine;uncertain data	Qi Wu;Rob Law	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.02.094	support vector machine;mathematical optimization;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	5.182616076408841	-22.338131752941827	150995
865a80b735ecdb92a90fa169a8723aab7a2144b9	building efficient frontier by cvar minimization for non-normal asset returns using copula theory	minimisation;genetic engineering;empirical study;copula theory;risk analysis gaussian distribution investment minimisation;investments;cvar minimization;finance;fat tail;nsga ii portfolio optimization multi objective evolutionary algorithm conditional expectation copula multivariate distribution value at risk conditional value at risk extreme value theory;risk analysis;student t copula;conditional value at risk;value at risk;multi objective evolutionary algorithm;copula;portfolios;conditional expectation;minimization methods;investment;portfolio optimization;mechanical engineering;extreme value theory;nonnormal asset return;market movement prediction;marginal distribution;efficient frontier;market risk;investment portfolio;nonelliptical multivariate distribution;nsga ii;probability distribution;risk modeling;financial market;mathematical model;multivariate distribution;risk measure cvar minimization nonnormal asset return computational finance optimal portfolio market movement prediction risk modeling financial market investment portfolio copula theory nonlinear interdependence nonelliptical multivariate distribution student t copula marginal distribution gaussian distribution market risk conditional value at risk;risk measure;optimal portfolio;gaussian distribution;nonlinear interdependence;computational finance;reactive power	In the realm of computational finance, the performance of the optimal portfolio largely depends upon its composition and its ability to accurately predict the market movements. Recent empirical studies have shown that the underlying assumption of normality of asset returns for risk modeling is seriously flawed, in view of their asymmetric and fat-tailed behavior. This problem is further aggravated when we delve into the functioning of the financial market and realize that the market parameters have highly non-linear kind of inter-dependence amongst themselves. Any investment portfolio that does not account for these factors and their mutual relationship, will tend to under-perform. This work is a novel attempt, which aims at developing a framework which solves all of these problems in an integrated fashion, without overlooking any of them or pre-assigning lesser importance to any of these issues. The contemporary techniques often neglect one of them, resulting in an incomplete and sometimes even a misleading picture of the market scenario. In this work, copula theory effectively captures the non-linear inter-dependence. The scenarios are generated from a non-elliptical multivariate distribution constructed by a students t-copula assuming marginal distributions as Gaussian in the center and EVT distributed in the tail. For gauging the market risk we have used CVaR (conditional value-at-risk) as the risk measure. The efficient frontier thus resulted by minimizing the CVaR and maximizing the returns, gives a clear insight into how does the composition of the optimal portfolio changes with respect to change in CVaR of the portfolio. Our aim is to prove that much more reliable conclusions will certainly be drawn if a more realistic representation of data can be done using the concept of copulas.	cvar;cobham's thesis;computational finance;extreme value theory;financial risk modeling;marginal model;mike lesser;nonlinear dimensionality reduction;nonlinear system;real life;risk measure;tails;value at risk	Kapil Agrawal	2008	2008 11th IEEE International Conference on Computational Science and Engineering	10.1109/CSE.2008.47	econometrics;actuarial science;computational finance;investment;portfolio optimization;statistics	Robotics	3.8261750259679306	-10.719151210022183	151007
f95b3744152843e5b641d5788237c9f97d58722d	on a new class of implications: (g, min)-implications and several classical tautologies	law of importation;distributivity of fuzzy implications;fuzzy implications;g generators;contraction law	A new class of fuzzy implications, called (g,min)-implications, is introduced by means of the additive generators of continuous Archimedean t-conorms, called g-generators. Basic properties of these implications are discussed. It is shown that the (g,min)-implications are really a new class different from the known (S,N)-, R-, QLand Yager’s f and gimplications. Generalizations of three classical logic tautologies with implications, viz. law of importation, contraction law and distributivity over triangular norms (t-norms) and triangular conorms (t-conorms) are investigated. A series of necessary and sufficient conditions are proposed, under which the corresponding functional equations are satisfied.	approximation algorithm;emoticon;fuzzy control system;moore's law;nl (complexity);numerical aperture;t-norm;utility functions on indivisible goods;viz: the computer game	Huawen Liu	2012	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488512500018	discrete mathematics;mathematics;algorithm	Arch	0.14498062654807242	-22.736751109930896	151154
677768fe5397daf66f6f4ff7f9aea3cde295df07	evolutionary-morphological learning machines for high-frequency financial time series prediction		Abstract A recent study has presented a model, called the increasing-decreasing-linear (IDL) model, which is able to efficiently predict the high-frequency stock market. Nevertheless, a drawback arises from the IDLu0027s learning process, which consists of its costly methodology to circumvent the non-differentiability problem of increasing and decreasing operators. In this sense, trying to reduce the computational cost of the IDL design, we propose evolutionary learning machines, using the genetic algorithm, the particle swarm optimizer, the backtracking search algorithm, the firefly algorithm and the cuckoo search, to design the IDL model. Five relevant high-frequency time series from the Brazilian stock market are used to assess performance, and the achieved results have demonstrated better prediction performance with smaller computational cost when compared to those achieved by the IDL model designed by its classical learning process, as well as to those achieved by some relevant prediction models presented in the literature.	time series	Ricardo de A. Araújo;Nadia Nedjah;José Manoel de Seixas;Adriano Lorena Inácio de Oliveira;Silvio Romero de Lemos Meira	2018	Swarm and Evolutionary Computation	10.1016/j.swevo.2018.03.009	predictive modelling;firefly algorithm;operator (computer programming);backtracking;genetic algorithm;time series;cuckoo search;machine learning;artificial intelligence;computer science;particle swarm optimization	ML	7.525473148442627	-21.05684258730146	151282
bef1c2b001bd7af3cfb1982b0340be1e259db427	a multiple attribute decision making method with interval rough numbers based on the possibility degree	rough set theory decision making number theory possibility theory;decision making vectors approximation methods systems engineering and theory educational institutions equations silicon;alternative ranking multiple attribute decision making possibility degree interval rough numbers;possibility degree matrix multiple attribute decision making problems madm problem interval rough numbers deviation degree attribute weights possibility degree formula	Aiming at the interval rough numbers of multiple attribute decision making (MADM) problems, a method of ranking interval rough numbers based on possibility degree is proposed. First, the deviation degree for interval rough numbers is given, and then an optimal model with maximum deviation to solve the attribute weights is set up. Second, a possibility degree formula of interval rough numbers is proposed and then we do some research on it to find out the desirable properties. Simultaneously, an algorithm to rank the interval rough numbers based on possibility degree matrix is presented. Finally, an example is provided to illustrate the application of the proposed models.	algorithm;degree matrix	Ying-ying Liu;Yue-Jin Lv	2014	2014 10th International Conference on Natural Computation (ICNC)	10.1109/ICNC.2014.6975870	mathematical optimization;combinatorics;discrete mathematics;attribute domain;mathematics;dominance-based rough set approach	Robotics	-2.231673751159439	-20.674337094196478	151369
0275ea43015c2804860d1c2962e00eff63734305	a new extension of fuzzy sets using rough sets: r-fuzzy sets	fuzzy set;fuzzy sets;r fuzzy sets;rough sets;rough set;article	This paper presents a new extension of fuzzy sets: R-fuzzy sets. The membership of an element of a R-fuzzy set is represented as a rough set. This new extension facilitates the representation of an uncertain fuzzy membership with a rough approximation. Based on our definition of R-fuzzy sets and their operations, the relationships between R-fuzzy sets and other fuzzy sets are discussed and some examples are provided.	approximation;fuzzy set;rough set;type-2 fuzzy sets and systems	Yingjie Yang;Chris J. Hinde	2010	Inf. Sci.	10.1016/j.ins.2009.10.004	algebra of sets;discrete mathematics;rough set;topology;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy set;fuzzy set operations;dominance-based rough set approach	AI	-1.117408821995291	-23.51992093575968	151388
78b70be366d05a35814946358e04419e3bf172a6	empirical mode decomposition based lssvm for ship motion prediction	lease square support vector machines;ship motion;empirical mode decomposition	An empirical mode decomposition (EMD) based Lease square support vector machines (LSSVM) is proposed for ship motion prediction. For this purpose, the original ship motion series were first decomposed into several intrinsic mode functions (IMFs), then a LSSVM model was used to model each of the extracted IMFs, so that the tendencies of these IMFs could be accurately predicted. Finally, the prediction results of all IMFs are combined to formulate an output for the original ship motion series. Experiments on chaotic datasets and real ship motion data are used to test the effectiveness of the proposed algorithm.		Bo Zhou;Aiguo Shi	2013		10.1007/978-3-642-39065-4_39	simulation;hilbert–huang transform;data mining	Robotics	8.400138006113915	-18.103035035384707	151444
d9c4baefbeb871a369121a2b7c4035e148ee1b33	modeling dependence with copulas: are real estates and tourism associated?		Several families of copulas are considered in this study to illustrate the correlation between real estate–particularly hospitality real estate investment trust- and the tourism sector. In essence, this study uses Elliptical copulas and Archimedean copulas, and more recent classes, like extreme value copulas and mixed copulas to conduct the experiment. Under a specific data set, it is revealed that the classical classes of copulas i.e., Elliptical and Archimedean, are selected most often for illustrating the dependency, followed by the extreme value class, particularly the Husler-Reiss copula. However, surprisingly, the mixed copula is not entirely preferable for this data set.		Roengchai Tansuchat;Paravee Maneejuk	2018		10.1007/978-3-319-75429-1_25	copula (linguistics);extreme value theory;financial economics;tourism;economics;real estate investment trust	Vision	-0.8540764537103065	-12.30914701408962	151500
4d14cb447f904d396304170df3158bd6a4ab290e	multiplication, distributivity and fuzzy-integral. ii	generalise;multicriteria analysis;pseudo multiplication;expected utility;logique floue;distributivity law;semigrupo;logica difusa;pseudo addition;restricted domain;teoria medida;fuzzy logic;choquet integral;semigroupe;generalized;20m130;generalizado;utilite attendue;analisis multicriterio;semigroup;20m30;fuzzy measures;analyse multicritere;28a25;utilidad espera;theorie mesure;measure theory;sugeno integral 28a25;sugeno integral	Based on results of generalized additions and generalized multiplications, proven in Part I, we first show a structure theorem on two generalized additions which do not coincide. Then we prove structure and representation theorems for generalized multiplications which are connected by a strong and weak distributivity law, respectively. Finally as a last preparation for the introduction of a framework for a fuzzy integral we introduce generalized differences with respect to t-conorms (which are not necessarily Archimedean) and prove their essential properties.	cordic;essence	Wolfgang Sander;Jens Siedekum	2005	Kybernetika		fuzzy logic;mathematical analysis;discrete mathematics;measure;expected utility hypothesis;fuzzy measure theory;mathematics;choquet integral;semigroup;algebra	AI	1.1446787557779081	-21.920294025594785	151537
7164fe8c31abc52c7abed0822d8aec9c8f7ccd16	fuzzy sets and fuzzy logic in the human sciences		The development of fuzzy set theory and fuzzy logic provided an opportunity for the human sciences to incorporate a mathematical framework with attractive properties. The potential applications include using fuzzy set theory as a descriptive model of how people treat categorical concepts, employing it as a prescriptive framework for “rational” treatment of such concepts, and as a basis for analysing graded membership response data from experiments and surveys. However, half a century later this opportunity still has not been fully grasped. This chapter surveys the history of fuzzy set applications in the human sciences, and then elaborates the possible reasons why fuzzy set concepts have been relatively under-utilized therein.	fuzzy logic;fuzzy set	Michael Smithson	2016		10.1007/978-3-319-31093-0_8	fuzzy logic;t-norm fuzzy logics;combs method;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	-3.8576826640834487	-23.81342194797368	151605
0273375938378eb23f41c4a5296560bae330b563	combining artificial neural networks and statistics for stock-market forecasting	stock market;time series;statistical model;transfer function;indexation;artificial neural network	We have developed a stock-market forecasting system based on artificial neural networks. The system has been trained with the Standard & Poor 500 composite indexes of past twenty years. Meanwhile, the system produces the forecasts and adjusts itself by comparing its forecasts with the actual indexes. Since most of stock-market forecasting systems are based on some kind of statistical models, we have also implemented a statistical system based on Box-Jenkins ARIMA(p,d,q) model of time series. We compare the performance of the these systems. It shows that the artificial neural network's forecasting is generally superior to time series but it occasionally produces some very wild forecasting values. We then developed a transfer function model to forecast based on the indexes and the forecasts by the artificial neural networks.	artificial neural network;function model;jenkins;neural networks;statistical model;time series;transfer function	Shaun-Inn Wu;Ruey-Pyng Lu	1993		10.1145/170791.170838	statistical model;computer science;artificial intelligence;machine learning;time series;transfer function;consensus forecast;artificial neural network;statistics	ML	7.788689181580227	-19.81183760884193	152013
bcdc1f508c2eb0977a30c87a4d1e38826678d3be	smart meter data analysis for power theft detection	electrical distribution network;technical loss;smart grid;power theft;statistical decision theory sense;smart meter;statistical model;distribution network;smart meter data analysis;power theft detection	We propose a method for power theft detection based on predictive models for technical losses in electrical distribution networks estimated entirely from data collected by smart meters in smart grids. Although the data sampling rate of smart meters is not sufficiently high to detect power theft with complete certainty, detection is still possible in a statistical decision theory sense, based on statistical models estimated from collected data sets. Even without detailed knowledge of the exact topology of the distribution network, it is possible to estimate a statistical model of the technical losses that allows indirect estimation of the non-technical losses (power theft) with high accuracy. International Conference on Machine Learning and Data Mining in Pattern Recognition (MLDM) This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c ©Mitsubishi Electric Research Laboratories, Inc., 2013 201 Broadway, Cambridge, Massachusetts 02139	acknowledgment index;broadway (microprocessor);data mining;decision theory;international conference on machine learning;pattern recognition;predictive modelling;sampling (signal processing);smart meter;statistical model	Daniel Nikovski;Zhenhua Wang;Alan Esenther;Hongbo Sun;Keisuke Sugiura;Toru Muso;Kaoru Tsuru	2013		10.1007/978-3-642-39712-7_29	econometrics;data mining;computer security	ML	5.727258128068754	-13.060897967084685	152105
042da9bcdda56f2b0988869e2c45061821a1ca47	carbon emissions modeling of china using neural network	neural nets economic indicators environmental economics international trade investment;neural network carbon emission china;investments;carbon dioxide;neural networks;neural nets;european debt crisis carbon emission modeling china neural network gdp export investment subprime mortgage crisis;government;economic indicators carbon dioxide investments neural networks predictive models neurons government;investment;environmental economics;predictive models;carbon emission;neurons;china;neural network;international trade;economic indicators	The aim of this study is to find a model to forecast China's carbon emission using the neural network. Some variables, such as GDP, export, investment in fixed assets and population, are considered. First, the neural network is constructed. Then, considering the effect of sub prime mortgage crisis and European debt crisis, we forecast the emissions in the next 10 years through the network. We find that emissions may be approaching the turning point. At last, the possible reasons are given.	artificial neural network	Peng Liu;Guoxing Zhang;Xutao Zhang;Sujie Cheng	2012	2012 Fifth International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2012.155	economics;investment;computer science;machine learning;economy;market economy;economic growth;artificial neural network	ML	7.371663487670645	-18.81001519536996	152307
928100cbb66809f06667a4f1333002526965bd85	knowledge-driven autonomous commodity trading advisor	bepress selected works;switching kalman filter;commodity trading;trading algorithm knowledge driven autonomous commodity trading advisor financial trading algorithmic trading equity market human trader machine trader qualitative expert knowledge financial information physical constraint rapid technology change cash rich hedge fund human centric asset class complicated environment volatile environment adaptive trading support framework two state switching kalman filter state estimation real time information palm oil trading biofuel expert domain knowledge prediction error single state econometric model back test trading performance;state estimation commodity trading econometrics expert systems financial management investment kalman filters;autonomous trading commodity trading switching kalman filter;switching kalman filter autonomous trading commodity trading;autonomous trading	"""The myth that financial trading is an art has been mostly destroyed in the recent decade due to the proliferation of algorithmic trading. In equity markets, algorithmic trading has already bypass human traders in terms of traded volume. This trend seems to be irreversible, and other asset classes are also quickly becoming dominated by the machine traders. However, for asset that requires deeper understanding of physicality, like the trading of commodities, human traders still have significant edge over machines. The primary advantage of human traders in such market is the qualitative expert knowledge that requires traders to consider not just the financial information, but also a wide variety of physical constraints and information. However, due to rapid technology changes and the """"invasion"""" of cash-rich hedge funds, even this traditionally human-centric asset class is crying for help in handling increasingly complicated and volatile environment. In this paper, we propose an adaptive trading support framework that allows us to quantify expert's knowledge to help human traders. Our method is based on a two-state switching Kalman filter, which updates its state estimation continuously with real-time information. We demonstrate the effectiveness of our approach in palm oil trading, which is becoming more and more complicated in recent years due to its new usage in producing biofuel. We show that the two-state switching Kalman filter tuned with expert domain knowledge can effectively reduce prediction errors when compared against traditional single-state econometric models. With a simple back test, we also demonstrate that even a slight decrease in the prediction errors can lead to significant improvement in the trading performance of a naive trading algorithm."""	algorithm;algorithmic trading;autonomous robot;econometric model;kalman filter;real-time data;traders;volatile memory	Yee Pin Lim;Shih-Fen Cheng	2012	2012 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2012.208	dark liquidity;alternative asset;high-frequency trading;pairs trade;trading strategy;electronic trading;trading turret;alternative trading system;algorithmic trading;low latency	AI	4.343629586581927	-16.46782175539384	152377
cc57777e6ffb72c8d8ac206fa6bbaa5c77232f42	new fuzzy numbers comparison operators in energy effectiveness simulation and modeling systems		Energy efficiency is often a key optimization problem. Many control systems use fuzzy logic and as a result applying compare operators to fuzzy numbers. The article deals with the issue of comparing fuzzy numbers. The similarity relation is most probably the most frequently used and the most difficult to precisely determine the convergence measure. Analysis of the similarity of two objects is a basic assessment tool and constitutes the basis for reasoning by analogy. It also directly affects the energy effectiveness of the universe that it controls. This article presents the methods for determining the similarity used in fuzzy logic. Many of these methods were dedicated only to fuzzy triangular or trapezoidal numbers (Dobrosielski et al. 2017, ChiTsuen Yeh 2017, Abbasbandy and Hajjar 2009). This was a computational inconvenience and posed a question about the axiological basis of this type of comparison. The authors proposed two new approaches for comparing fuzzy numbers using one of the known extensions of fuzzy numbers (Kacprzyk and Wilbik 2009, 2005). This allowed to simplify the operation and eliminate the duality (Zadrożny, 2004).	computation;control system;fuzzy logic;fuzzy number;mathematical optimization;optimization problem;simulation	Wojciech T. Dobrosielski;Jacek Czerniak;Hubert Zarzycki;Janusz Szczepanski	2018		10.7148/2018-0454	relational operator;mathematical optimization;fuzzy number;mathematics	AI	-2.2951339609755825	-22.367220268974528	152504
ec22b762f15226e22eddf475a31430c1a9d7da17	implication structures, fuzzy subsets, and enriched categories	complete residuated lattice;non classical logic;fuzzy set;procesamiento informacion;logica no clasica;category;teoria conjunto;fuzzy relation;conjunto difuso;theorie ensemble;quantale;ensemble flou;set theory;fuzzy set theory;enriched categories;categorie;category theory;information processing;non classical logics;unital quantale;03exx;completitud;implication structure;enriched category;cauchy completeness;sistema difuso;residuated lattice;06bxx;systeme flou;completeness;fuzzy powerset;traitement information;fuzzy function;completude;fuzzy system;18xx;logique non classique	We show that there are natural ways to make a fuzzy set into an enriched category, and that in doing so, some basic results in fuzzy set theory are applications of enriched category theory. Thus we argue that the theory of enriched categories is a useful tool for fuzzy set theorists.	fuzzy set	Dexue Zhang	2010	Fuzzy Sets and Systems	10.1016/j.fss.2009.09.014	discrete mathematics;topology;information processing;fuzzy classification;artificial intelligence;mathematics;fuzzy set;fuzzy control system	NLP	1.2744055322265349	-22.828091932247904	152542
c5c946ee1311299932955cf7a8c83a54ada9fb64	automated customer segmentation based on smart meter data with temperature and daylight sensitivity	temperature sensors resistance heating meteorology smart meters temperature distribution;data granularity automated customer segmentation smart meter data daylight sensitivity temperature sensitivity outdoor temperature household classification system daylight coefficients energy efficient light bulbs;temperature sensors;smart meters daylighting energy conservation;meteorology;temperature distribution;smart meters;resistance heating	Utilities increasingly leverage knowledge on their customer's household characteristics in their energy efficiency programs. Examples of such characteristics include the number of persons per household, their employment status, or the type of dwelling they live in. This information allows utilities to personalize energy efficiency campaigns, which increases participation rates and in turn leads to larger energy savings and higher customer retention. However, gathering this information through surveys is costly and cumbersome. We therefore investigate the possibility to automatically infer household characteristics from electricity consumption data measured by an off-the-shelf smart meter. In this paper, we develop a method to determine the sensitivity of a household to outdoor temperature and the times of sunset/sunrise, and use this information to improve the performance of our household classification system. We further investigate the relevance of different features for such a system. Our evaluation is based on smart meter data collected at a 30-minute granularity in more than 4000 Irish households over a period of 75 weeks. The results show that we can improve accuracy by up to 2.3 percentage points using temperature and daylight coefficients. The characteristics floor area, type of dwelling, and percentage of installed energy-efficient light bulbs particularly benefit from temperature and daylight coefficients. Finally, we investigate the impact of the data granularity on the classification performance and show that semi-hourly or hourly data is required, as it performs on average 6.6 percentage points better than using daily consumption averages.	coefficient;daylight;personalization;relevance;semiconductor industry;smart meter;statistical classification;the times;weatherstar	Christian Beckel;Leyna Sadamori;Silvia Santini;Thorsten Staake	2015	2015 IEEE International Conference on Smart Grid Communications (SmartGridComm)	10.1109/SmartGridComm.2015.7436375	simulation;engineering;electrical engineering;operations management	Robotics	8.821085624625336	-15.800042100973592	152559
a29a1f38f5ce13fb0f36d6de7375aaecd1d8b266	comparison of expert algorithms with machine learning models for real estate appraisal		Machine learning models require numerous training examples to provide reliable predictions of real estate prices. Expert algorithms could be applied wherever only several training samples are available. The accuracy of two expert algorithms based on the sales comparison approach was experimentally examined using real-world data derived from a cadastral system and registry of real estate transactions. The performance of the algorithms was compared with three data driven regression models for property valuation. Statistical analysis of the obtained results was conducted.	algorithm;experiment;expert system;machine learning;value (ethics)	Bogdan Trawinski;Zbigniew Telec;Jacek Krasnoborski;Mateusz Piwowarczyk;Michal Talaga;Tadeusz Lasota;Edward Sawilow	2017	2017 IEEE International Conference on INnovations in Intelligent SysTems and Applications (INISTA)	10.1109/INISTA.2017.8001131	data science;regression analysis;data mining;cost accounting;algorithm design;data modeling;real estate;valuation (finance);real estate appraisal;cadastre;machine learning;algorithm;computer science;artificial intelligence	Robotics	4.730180419585934	-19.110356927327942	152600
bf08ae8e7099e40e3f357d89534275a79cf43a6b	a probabilistic definition of a nonconvex fuzzy cardinality	entropie floue;cardinal number;cardinalite floue;entropia;fuzzy set;fuzzy rules;conjunto difuso;ensemble flou;fuzzy entropy;probabilistic approach;data mining;nombre cardinal;numero cardinal;enfoque probabilista;approche probabiliste;entropie;analyse non convexe;fuzzy set equipotency;entropy;non convex analysis;cardinalite floue relative;fuzzy cardinality;fuzzy relative cardinality;equipotence ensemble flou;equipotency of fuzzy sets;analisis no convexo	The existing methods to assess the cardinality of a fuzzy set with #nite support are intended to preserve the properties of classical cardinality. In particular, the main objective of researchers in this area has been to ensure the convexity of fuzzy cardinalities, in order to preserve some properties based on the addition of cardinalities, such as the additivity property. We have found that in order to solve many real-world problems, such as the induction of fuzzy rules in Data Mining, convex cardinalities are not always appropriate. In this paper, we propose a possibilistic and a probabilistic cardinality of a fuzzy set with #nite support. These cardinalities are not convex in general, but they are most suitable for solving problems and, contrary to the generalizing opinion, they are found to be more intuitive for humans. Their suitability relies mainly on the fact that they assume dependency among objects with respect to the property “to be in a fuzzy set”. The cardinality measures are generalized to relative ones among pairs of fuzzy sets. We also introduce a de#nition of the entropy of a fuzzy set by using one of our probabilistic measures. Finally, a fuzzy ranking of the cardinality of fuzzy sets is proposed, and a de#nition of graded equipotency is introduced. c © 2002 Elsevier Science B.V. All rights reserved.	convex function;data mining;fuzzy set	Miguel Delgado;Daniel Sánchez;Maria J. Martín-Bautista;M. Amparo Vila	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00039-2	fuzzy logic;cardinality;entropy;cardinality;combinatorics;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations	AI	-2.586786448691777	-23.10945169451623	152747
7dd9a05f2ae469762c9e4a96068531c4d7d0c276	a gis-based neuro-fuzzy procedure for integrating knowledge and data in landslide susceptibility mapping	damage;computadora;tratamiento datos;landscapes;computers;analyse risque;maps;fuzzy inference system fis;systeme information geographique;susceptibility;mapa;geographic information system;numerical solution;neural networks;paysage;ordinateur;logique floue;data processing;paisaje;estrategia;traitement donnee;geographic information system gis;endommagement;carte;inventory;algorithme;fuzzy logic;strategy;geographical information systems;natural disasters;accuracy;modelo;glissement terrain;precision;propagacion;asie;province mazandaran;geographic information systems;neuro fuzzy;assessment methods;fuzzy inference system;back propagation algorithm;landslides;risk assessment;algorithms;mapping;modele;expert knowledge;middle east;landslide susceptibility map lsm;climatic condition;reseau neuronal;desmoronamiento tierra;artificial neural network ann;strategie;models;red neuronal;iran;inventaire;propagation;historical data;inventario;moyen orient;artificial neural network;oriente medio;asia;algoritmo	A significant portion of the Mazandaran Province in Iran is prone to landslides due to climatic conditions, excessive rain, geology, and geomorphologic characteristics. These landslides cause damage to property and pose a threat to human lives. Numerous solutions have been proposed to assess landslide susceptibility over regions such as this one. This study proposes an indirect assessment strategy that shares in the advantages of quantitative and qualitative assessment methods. It employs a fuzzy inference system (FIS) to model expert knowledge, and an artificial neural network (ANN) to identify non-linear behavior and generalize historical data to the entire region. The results of the FIS are averaged with the intensity values of existing landslides, and then used as outputs to train the ANN. The input patterns include both physical landscape characteristics (criterion maps) and landslide inventory maps. The ANN is trained with a modified back-propagation algorithm. As part of this study, the strategy is implemented as a GIS extension using ArcGIS(R). This tool was used to create a four-domain landslide susceptibility map of the Mazandaran province. The overall accuracy of the LSM is estimated at 90.5%.	geographic information system;neuro-fuzzy	Mohammad H. Vahidnia;Ali A. Alesheikh;Abbas Alimohammadi;Farhad Hosseinali	2010	Computers & Geosciences	10.1016/j.cageo.2010.04.004	computer science;artificial intelligence;machine learning;accuracy and precision;artificial neural network	AI	9.866168262873169	-23.942992482140077	152782
857737a667fce9083f82053802097f0c842e9183	using a multi-criteria decision aid methodology to implement sustainable development principles within an organization	sustainable development action plan;multi criteria decision aid;sustainable development indicators;choquet integral;electre and choquet integral;electre	The implementation of Sustainable Development (SD) within an Organization is a difficult task. This is due to the fact that it is difficult to deal with conflicting and incommensurable aspects such as environmental, economic and social dimensions. In this paper we have used a Multi-Criteria Decision Aid (MCDA) methodology to cope with these difficulties. MCDA methodology offers the opportunity to avoid monetary valuation of the different dimensions of the SD. These dimensions are not substitutable for one another and all have a role to play. There is an abundance of possible aggregation procedures in MCDA methodology. In this paper we have proposed an innovative method to choose a suitable aggregation procedure for SD problems. Real life case studies of the implementation of an outranking approach (i.e., ELECTRE) and of a mono-criterion synthesis approach (i.e., MAUT approaches based on the Choquet integral) were done to respectively rank 22 SD strategic actions within an expertise Institute and rank 20 practical operational actions to control energy consumption of the Institute’s buildings.	object process methodology;object composition;real life;value (ethics)	Myriam Merad;Nicolas Dechy;Lisa Serir;Michel Grabisch;Frédéric Marcel	2013	European Journal of Operational Research	10.1016/j.ejor.2012.08.019	electre;economics;operations management;mathematics;management science;choquet integral;welfare economics	SE	-3.8775260051480753	-16.597989153442715	153022
aa061e52f662467a9f149de5a9601fc2c0c31252	redefined fuzzy b-algebras	α β fuzzy subalgebra;fuzzy point;β fuzzy subalgebra;b algebra;α	By two relations belonging to (∈) and quasi-coincidence (q) between fuzzy points and fuzzy sets, we define the concept of (α, β)-fuzzy subalgebras where α, β are any two of {∈, q,∈ ∨q,∈ ∧q} with α =∈ ∧q. We state and prove some theorems in (α, β)-fuzzy B-algebras.	definition;fuzzy set;fuzzy subalgebra	A. Zarandi Baghini;Arsham Borumand Saeid	2008	FO & DM	10.1007/s10700-008-9045-y	mathematical analysis;discrete mathematics;topology;mathematics	Logic	-0.2692474195136485	-23.185388482487124	153034
949a6a555fd1719471656fb1f760f1b5ca672f6b	fuzzy decision based combination model for securities investment	investments;fuzzy set;history;uncertainty;fuzzy number;investment decision;fuzzy decision securities investment combination model;biological system modeling;investment;fuzzy set theory;fuzzy risk;fuzzy set theory fuzzy risk fuzzy number parabolic membership function fuzzy decision securities investment combination model parabolic satisfaction degree;combination model of securities investments;decision theory;combination model of securities investments parabolic fuzzy number fuzzy decision;membership function;mathematical model;parabolic satisfaction degree;parabolic membership function;security investments history biological system modeling mathematical model uncertainty;security;parabolic fuzzy number;investment decision theory fuzzy set theory;fuzzy decision	In this paper, the difference of satisfactions for fuzzy yield and fuzzy risk from investors is viewed as a fuzzy number with parabolic membership function, and a fuzzy decision securities investment combination model bases on parabolic satisfaction degree is constructed with fuzzy set and fuzzy decision theories. The experimental results on real-word data show that our new approach is very efficient and effective for the investment decision.	decision theory;fuzzy number;fuzzy set;parabolic antenna	Feng Tian;JieChang Wen	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569717	mathematical optimization;actuarial science;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;investment;fuzzy classification;computer science;artificial intelligence;information security;fuzzy number;linear partial information;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations;statistics	DB	-1.7154111953848428	-19.26391493141518	153055
3d4267cdfe13635483fca43aa7043b76434cb2b8	characterizing and visualizing predictive uncertainty in numerical ensembles through bayesian model averaging	numerical ensembles bayes methods mathematical model predictive models numerical models data visualization statistical visualization uncertainty visualization;numerical ensembles;bayes methods;uncertainty handling;bayes methods mathematical model predictive models numerical models data visualization;algorithms bayes theorem computer graphics computer simulation data interpretation statistical models statistical pattern recognition automated reproducibility of results sensitivity and specificity user computer interface;data visualisation;statistical analysis;visualization strategy predictive uncertainty characterization predictive uncertainty visualization numerical ensemble forecasting bayesian model averaging framework visual strategy ensemble constituents event of interest prediction ground truth observations statistical aggregate;statistical visualization;data visualization;mathematical model;uncertainty handling bayes methods data visualisation learning artificial intelligence statistical analysis;predictive models;numerical models;learning artificial intelligence;uncertainty visualization	Numerical ensemble forecasting is a powerful tool that drives many risk analysis efforts and decision making tasks. These ensembles are composed of individual simulations that each uniquely model a possible outcome for a common event of interest: e.g., the direction and force of a hurricane, or the path of travel and mortality rate of a pandemic. This paper presents a new visual strategy to help quantify and characterize a numerical ensemble's predictive uncertainty: i.e., the ability for ensemble constituents to accurately and consistently predict an event of interest based on ground truth observations. Our strategy employs a Bayesian framework to first construct a statistical aggregate from the ensemble. We extend the information obtained from the aggregate with a visualization strategy that characterizes predictive uncertainty at two levels: at a global level, which assesses the ensemble as a whole, as well as a local level, which examines each of the ensemble's constituents. Through this approach, modelers are able to better assess the predictive strengths and weaknesses of the ensemble as a whole, as well as individual models. We apply our method to two datasets to demonstrate its broad applicability.		Luke J. Gosink;Kevin Bensema;Trenton Pulsipher;Harald Obermaier;Michael Henry;Hank Childs;Kenneth I. Joy	2013	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2013.138	computer science;data science;machine learning;ensemble forecasting;mathematical model;data mining;mathematics;predictive modelling;ensemble learning;data visualization;statistics	Visualization	7.697196766784998	-15.310623306490188	153371
05c0671517bc42e7fe27a54f4d2c132507c895c9	stability analysis of group decision-making under weighted scoring rules	stability degree;weighted scoring rules;inclined angle of vectors;group decision making	ABSTRACTThe result of group decision-making is always unstable, influenced by some uncertain factors. It is necessary to measure and analyse the stability of the result. A measurement based on the inclined angle of two vectors is proposed in this paper, in order to measure the stabilities of the results of weighted scoring rules. The concepts of stability degree and stability angle are given, whose geometric interpretations are displayed in the case of three candidates. Then an extended measurement called the relative stability degree is discussed to analyse the comparability of stability measurements for different numbers of candidates. Furthermore, this measurement and its extension are used to aid the decision-making of new project development in a software company.	discrete optimization;integer programming;nl (complexity);nonlinear system;volume rendering	Fan Wu;Yong Zhao;Yang Chen	2016	Int. J. Systems Science	10.1080/00207721.2015.1128577	mathematical optimization;combinatorics;group decision-making;mathematics;statistics	Logic	-2.190573649504299	-20.834708144570087	153430
34d77ddc32e5dbf626bbf7cee3fb13bf1b210013	cluster-based hierarchical demand forecasting for perishable goods	004 informatik	Cluster analysis of articles based on intra-day sales pattern.Exploiting of article clusters and organizational structure for demand forecasting.DSS for ordering fast-moving substitutable perishable goods.Evaluation based on real data of an industrialized bakery. Demand forecasting is of particular importance for retailers in the context of supply chains of perishable goods and fresh food. Such goods are daily produced and delivered as they need to be provided as fresh as possible and quickly deteriorate. Demand underestimation and overestimation negatively affect the revenues of the retailer. Stock-outs have an undesired impact on consumers while unsold items need to be discarded at the end of the day. We propose a DSS that supports day-to-operations by providing hierarchical forecasts at different organizational levels based on most recent point-of-sales data. It identifies article clusters that are used to extend the hierarchy based on intra-day sales pattern. We apply multivariate ARIMA models to forecast the daily demand to support operational decisions. We evaluate the approach with point-of-sales data of an industrialized bakery chain and show that it is possible to increase the availability while limiting the loss at the same time. The cluster analysis reveals that substitutable items have similar intra-day sales pattern which makes it reasonable to forecast the demand at an aggregated level. The accuracy of top-down forecasts is comparable to direct forecasts which allows reducing the computational costs.		Jakob Huber;Alexander Gossmann;Heiner Stuckenschmidt	2017	Expert Syst. Appl.	10.1016/j.eswa.2017.01.022	predictive analytics;data mining;decision support system;demand management;supply chain;organizational structure;revenue;computer science;microeconomics;autoregressive integrated moving average;demand forecasting	ML	-0.49911393668968174	-10.533681179058037	153460
3a0edea786416be1b5b678caa90fc5ee425cedcb	forecast of tourism demand with the use of fuzzy and cointegration econometric techniques	fuzzy regression analysis;tourism demand;cointegration;error correction models;theil s inequality coefficient	Econometric models for the forecast of tourism demand are developed in this paper. In order to assess the long-run trends concerning principal tourism generating countries, the Johansen's maximum likelihood techniques are applied. For a better assessment of the short-run trends, the estimated error correction terms are introduced to the first difference models to estimate the short-run relationships (Error Correction Models, ECMs). Based on the results given by the ECMs, fuzzy regression models are suggested and then compared in order to provide the forecasting ability of both techniques (Fuzzy and ECMs). Finally, for the evaluation of forecasting performance, the Theil's Inequality Coefficient is applied.		George N. Botzoris;E. G. Varagouli;Vassilios A. Profillidis;Basil K. Papadopoulos;P. Lathiras	2014	J. Comput. Meth. in Science and Engineering	10.3233/JCM-140501	financial economics;econometrics;economics;statistics	Theory	5.472624753806187	-15.762534343980395	153623
998435e9415f956902c7c57c5f30075e51955357	a new method for fuzzy decision making based on ranking generalized fuzzy numbers and interval type-2 fuzzy sets	number theory decision making fuzzy set theory;fuzzy set;interval type 2 fuzzy sets fuzzy decision making fuzzy ranking generalized fuzzy numbers;fuzzy number;decision maker;fuzzy set theory;number theory;machine learning;fuzzy ranking method fuzzy decision making ranking generalized fuzzy numbers interval type 2 fuzzy sets α cuts;type 2 fuzzy set;fuzzy sets decision making gold cybernetics machine learning silicon strontium	This paper presents a new method for fuzzy decision making based on ranking generalized fuzzy numbers and interval type-2 fuzzy sets. First, we propose a new fuzzy ranking method based on the α-cuts of generalized fuzzy numbers and interval type-2 fuzzy sets. The proposed fuzzy ranking method involves a decision-maker's attitude towards risks to calculate the left areas and the right areas of generalized fuzzy numbers and interval type-2 fuzzy sets with the consideration of the heights of generalized fuzzy numbers and interval type-2 fuzzy sets. Based on the proposed fuzzy ranking method, we propose a new method for fuzzy decision making. The proposed fuzzy decision making method is more flexible and more intelligent than the existing fuzzy decision making methods, where it not only considers the decision-maker's attitude towards risks, but also can deal with situations where the evaluating values are represented by interval type-2 fuzzy sets.	fuzzy set;game-maker;national supercomputer centre in sweden;take-grant protection model;type-2 fuzzy sets and systems	Shyi-Ming Chen;Cheng-Yi Wang	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016693	fuzzy logic;number theory;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;linear partial information;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	-1.9657836138110045	-21.14511348613532	153687
da24d82c525bd08c02a6287fab83182e2f7c1f6d	estimating fuzzy weight vector from interval pairwise comparison matrix with various processed matrices		In this paper, the approach to estimate a fuzzy weight vector from an interval comparison matrix is proposed. The interval comparison allows a decision maker to state his/her uncertain judgment as a range, instead of a crisp value. By increasing and decreasing its upper and lower bounds of the interval comparison by the inverse rates, the processed comparison matrices are derived from the given matrix. The membership function of the fuzzy weight is based on the certainty degrees of the interval weight vectors obtained from the processed matrices. The interval weight vector is defined as a closure of the normalized crisp weight vectors each of which is included in an interval comparison matrix. Its certainty degree is represented as the sum of the lower bounds of all the corresponding interval weights.	focal (programming language);interval arithmetic;minimum-weight triangulation;the matrix	Tomoe Entani	2017	2017 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2017.8015605	fuzzy logic;normalization (statistics);inverse;weight;matrix (mathematics);mathematical optimization;upper and lower bounds;pairwise comparison;mathematics;membership function	Robotics	-2.3703078723310926	-20.790526656487106	153801
f38bf80ea4624f802ecb2311b808ccda10edd25f	evidence-theory-based numerical algorithms of attribute reduction with neighborhood-covering rough sets	belief and plausibility functions;covering rough sets;evidence theory;attribute reduction;期刊论文;rough sets;neighborhood	Covering rough sets generalize traditional rough sets by considering coverings of the universe instead of partitions, and neighborhood-covering rough sets have been demonstrated to be a reasonable selection for attribute reduction with covering rough sets. In this paper, numerical algorithms of attribute reduction with neighborhood-covering rough sets are developed by using evidence theory. We firstly employ belief and plausibility functions to measure lower and upper approximations in neighborhood-covering rough sets, and then, the attribute reductions of covering information systems and decision systems are characterized by these respective functions. The concepts of the significance and the relative significance of coverings are also developed to design algorithms for finding reducts. Based on these discussions, connections between neighborhood-covering rough sets and evidence theory are set up to establish a basic framework of numerical characterizations of attribute reduction with these sets.	algorithm;numerical analysis;numerical method;plausibility structure;rough set;set theory	Degang Chen;Wanlu Li;Xiao Zhang;Sam Kwong	2014	Int. J. Approx. Reasoning	10.1016/j.ijar.2013.10.003	combinatorics;discrete mathematics;rough set;computer science;machine learning;data mining;mathematics;dominance-based rough set approach	AI	-1.8545780454854957	-23.336236275235116	153963
7dc77bc99f8a588b649694dda254f9d25c4181eb	evolving participatory learning fuzzy modeling for financial interval time series forecasting		Financial interval time series (ITS) describe the evolution of the maximum and minimum prices of an asset throughout time. These price ranges are related to the concept of volatility. Hence, their accurate forecasts play a key role in risk management, derivatives pricing and asset allocation, as well as supplements the information extracted by the time series of the closing price values. This paper addresses evolving fuzzy systems and financial ITS forecasting considering as the empirical application the main index of the Brazilian stock market, the IBOVESPA. An evolving participatory learning fuzzy model, named ePL-KRLS, is proposed. The model extends traditional ePL approach by considering Kernel functions to the identification of rule consequents parameters as well as a metaheuristic algorithm to automatically set model control parameters. One step ahead interval forecasts is compared against linear and nonlinear time series benchmark methods and with the state of the art evolving fuzzy models in terms of traditional accuracy metrics and quality measures designed for ITS. The results provide evidence for the predictability of of IBOVESPA ITS and significant forecast contribution of ePL-KRLS.	algorithm;autoregressive integrated moving average;autoregressive model;benchmark (computing);closing (morphology);emergence;fuzzy control system;fuzzy rule;interval arithmetic;logic programming;maxima and minima;metaheuristic;nonlinear system;risk management;smoothing;time complexity;time series;volatility	Leandro Maciel;Rafael Vieira;Alisson Porto;Fernando A. C. Gomide;Rosangela Ballini	2017	2017 Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2017.7954826	fuzzy logic;time series;financial economics;predictability;fuzzy control system;stock market;derivative (finance);finance;volatility (finance);asset allocation;economics	AI	7.092505857705031	-19.220808526339333	154118
028feb8cfbbcb45d1e89ac98cc24e48e4d597c03	towards an operational interpretation of membership grades - on h-valued fuzzy sets and their use for fuzzy quantification	operational interpretation;confidence level;fuzzy sets fuzzy set theory computational intelligence shape turning cost accounting proposals probabilistic logic lattices computer science;fuzzy set;hasse diagram operational interpretation membership grades h valued fuzzy sets fuzzy quantification lattice valued fuzzy sets;lattices;turning;computational intelligence;membership grades;h valued fuzzy sets;fuzzy set theory;fuzzy sets;lattice valued fuzzy sets;cost accounting;hasse diagram;shape;membership function;computer science;probabilistic logic;proposals;fuzzy quantification	Advances in fuzzy quantification have rendered possible a consistent interpretation of quantifying expressions involving vague quantifiers and fuzzy arguments (Glockner 2006, Diaz-Hermida et al 2005). However, the assumption of these approaches that the modeller is able to specify precise [0,1]-valued membership functions for the involved fuzzy sets and fuzzy quantifiers can be too strong in certain cases. To alleviate this problem, we extend the existing theory of fuzzy quantification to lattice-valued fuzzy sets which no longer require a specification of precise numerical membership grades. The paper focuses on a special type of so called H-lattices whose Hasse diagram has an hourglass shape. In this setting, we can achieve an operational interpretation of membership values, which can be calculated automatically provided that the modeller (a) decides on the basic tendency of the membership assessments and (b) specifies the salient ordering relationships between the confidence levels. The generalization of the existing theory of fuzzy quantification to H-valued fuzzy sets is a straightforward task and few properties of the models will be lost when turning from [0,1] to the generalized valuations. It is even possible to devise a generic construction which assigns a plausible model of fuzzy quantification to any given H-lattice	emoticon;fuzzy concept;fuzzy set;hasse diagram;modeller;numerical analysis;vagueness	Ingo Glöckner	2007	2007 IEEE Symposium on Foundations of Computational Intelligence	10.1109/FOCI.2007.371535	fuzzy logic;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;computational intelligence;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm	Logic	-2.9716678062790307	-23.814413722392885	154350
8a3c4c7a029b496174b57126f0acbf91aa73a3fa	finding most reliable path with extended shifted lognormal distribution		Travel time uncertainty may cause late arrival and impose a high penalty on travelers. There is a growing interest in modeling travel time uncertainty to optimize the reliability of travel time at the path and network level. Real data analysis finds that the influence factors, including day-of-week, holidays, time-of-day, road grades, traffic states, and so on, often reduce the cumulative probability of travel time even in the same facility type (the same lane number and the same divided type). Thus, a novel aggregate approach is proposed to classify the travel time data based on these influence factors. The distribution with the new aggregate approach is defined as the extended shifted lognormal (ESLN) distribution. KS test indicates that the ESLN distribution can effectively describe travel time, and outperforms normal, lognormal, gamma, and beta distribution. Travel time correlations are calculated between new aggregate groups, which can effectively reduce the complexity compared with the link to link correlations. Finally, the ESLN distribution is used to find the most reliable path in a real-world large-scale network. The comparison results between ESLN distribution and shifted lognormal (SLN) distribution show the effectiveness and improvement of the proposed method in finding the most reliable path.		Zhenzhen Yang;Ziyou Gao;Huijun Sun;Feng Liu;Jiandong Zhao	2018	IEEE Access	10.1109/ACCESS.2018.2878312	statistics;cumulative distribution function;beta distribution;distributed computing;computer science;log-normal distribution	DB	8.633646891981083	-13.122120458002879	154398
3cbc1403419b54a5430e4b54bf6b81e1ed18a046	preference conditions for utility models: a risk-value perspective	measures of risk;functional form;decision maker;risk value tradeoffs;measure of risk;utility models	This paper discusses necessary and sufficient preference conditions for utility models basedon a risk-value framework. These conditions provide additional insights into traditionalutility models regarding decision making by risk-value tradeoffs, and can help decisionmakers identify specific functional forms of utility measure in practice. Copyright Kluwer Academic Publishers 1998		James S. Dyer;Jianmin Jia	1998	Annals OR	10.1023/A:1018959713640	econometrics;decision-making;economics;computer science;mathematical economics;higher-order function;welfare economics	Vision	-1.9364864677302072	-12.62479082161682	154533
b883e5fc550fe97a7d831300e8797d3dba34d97f	grey relational with bp pso for time series forecasting	forecasting;time series forecasting;economic forecasting predictive models stock markets neural networks backpropagation time series analysis computer science information systems cybernetics usa councils;forecasting accuracy performance;cooperative feature selection;training;biological system modeling;grey relational analysis;pso;time series;backpropagation;data mining;stock markets;domain knowledge;kuala lumpur stock exchange;indexes;artificial neural networks;particle swarm optimizer;forecasting theory;particle swam optimization;time series backpropagation forecasting theory grey systems particle swarm optimisation stock markets;grey systems;predictive models;coopeative feature selection;feature selection;forecasting accuracy;backpropagation particle swarm optimization;forecasting accuracy grey relational analysis backpropagation particle swam optimization time series forecasting coopeative feature selection;convergence time;particle swarm optimisation;pso grey relational analysis backpropagation particle swarm optimization time series forecasting cooperative feature selection forecasting accuracy performance kuala lumpur stock exchange;forecast accuracy	This paper proposes an effective hybridization of grey relational analysis (GRA) and Backpropagation Particle Swarm Optimization (BP_PSO) for time series forecasting. The hybridization employs the complementary strength of these two appealing techniques. Additionally the combination of GRA and BP as cooperative feature selection (CFS) has successfully assessed the importance of each input variable and automatically suggest the optimum input numbers for the forecasting task. Therefore it assists the forecaster to choose the optimum number of dominant input factor without a need to acquire expert domain knowledge. It also helps to reduce the interference of irrelevant inputs on the forecasting accuracy performance. To test the effectiveness of the proposed hybrid GRABP_PSO, the dataset of closing price from Kuala Lumpur Stock Exchange (KLSE) is used. The results show that the proposed model, GRBP_PSO out performed BP_PSO model and BP model in term of accuracy and convergence time.	particle swarm optimization;time series	Roselina Sallehuddin;Siti Mariyam Hj. Shamsuddin	2009		10.1109/ICSMC.2009.5346304	computer science;artificial intelligence;machine learning;time series;data mining;feature selection;artificial neural network	DB	6.1114640332985335	-19.569621460916956	154599
06a0c8611fd9e96a87d2f4b07bf473c007be8d91	consistency and consensus improving methods for pairwise comparison matrices based on abelian linearly ordered group	consensus;pairwise comparison matrix;abelian linearly ordered group;consistency	The aim of a valued pairwise comparison matrix is to derive the priority structure over a set of criteria (or alternatives) in decision making. The consistency and consensus of a pairwise comparison matrix should be measured and improved to avoid a misleading priority structure. The basic entries of a pairwise comparison matrix can be described in different forms; hence, different consistency and consensus methods should be developed for different types of matrices. To provide a general framework, the pairwise comparison matrix based on Abelian linearly ordered group is first introduced. A consistency index is defined by constructing the nearest consistent pairwise comparison matrix from an inconsistent one, and two consistency improving methods are introduced. A group pairwise comparison matrix is derived, a consensus index of individual pairwise comparison matrices is defined and two consensus improving methods are developed by introducing a general aggregation operator based on Abelian linearly ordered group. The proposed consistency and consensus methods are convergent and can provide a general framework for existing methods.	consistency model	Meimei Xia;Jian Chen	2015	Fuzzy Sets and Systems	10.1016/j.fss.2014.07.019	mathematical optimization;combinatorics;discrete mathematics;consensus;computer science;mathematics;consistency	Vision	-3.0671075964678187	-19.99827726063798	154635
4a847308b8d9c07930f990c5ad452ead74949cf6	solution of a possibilistic multiobjective linear programming problem	multiobjective programming;fuzzy set;fuzzy programming;multiobjective linear programming;fuzzy number;decision maker;expected value;statistical inference;goal programming;possibility distribution	The estimate of the parameters which define a conventional multiobjective decision making model is a difficult task. Normally they are either given by the Decision Maker who has imprecise information and/or expresses his considerations subjectively, or by statistical inference from the past data and their stability is doubtful. Therefore, it is reasonable to construct a model reflecting imprecise data or ambiguity in terms of fuzzy sets and several fuzzy approaches to multiobjective programming have been developed  1 ;  9 ;  10  ;   11 . The fuzziness of the parameters gives rise to a problem whose solution will also be fuzzy, see  2  ;   3 , and which is defined by its possibility distribution. Once the possibility distribution of the solution has been obtained, if the decision maker wants more precise information with respect to the decision vector, then we can pose and solve a new problem. In this case we try to find a decision vector, which approximates as much as possible the fuzzy objectives to the fuzzy solution previously obtained. In order to solve this problem we shall develop two different models from the initial solution and based on Goal Programming: an Interval Goal Programming Problem if we define the relation “as accurate as possible” based on the expected intervals of fuzzy numbers, as we showed in   [4]  , and an ordinary Goal Programming based on the expected values of the fuzzy numbers that defined the goals. Finally, we construct algorithms that implement the above mentioned solution method. Our approach will be illustrated by means of a numerical example.	linear programming	Mar Arenas Parra;Amelia Bilbao-Terol;Maria Victoria Rodríguez Uría	1999	European Journal of Operational Research	10.1016/S0377-2217(99)00135-6	decision-making;mathematical optimization;statistical inference;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;machine learning;linear partial information;goal programming;mathematics;fuzzy set;fuzzy set operations;algorithm;expected value;statistics	Theory	-0.57370370348437	-18.147187515068566	154710
236b9f1326666b8bfa82f49cc264f801b4e0205f	on special fuzzy implications	engineering;metodo estadistico;residual of conjunction;fuzzy set;procesamiento informacion;39b99;conjunto difuso;ensemble flou;statistical method;03b52;fuzzy connectives;ingenierie;1 lipschitzianity;s n implication;03e72;methode statistique;information processing;03c10;ingenieria;sistema difuso;systeme flou;03c80;special implication;n implication;traitement information;s;fuzzy implication;fuzzy system;general binaries	Special implications were introduced by Hájek and Kohout [Fuzzy implications and generalized quantifiers, Int. J. Uncertain. Fuzziness Knowl.-Based Syst. 4 (1996) 225–233] in their investigations on some statistics on marginals. They have either suggested or only partially answered three important questions, especially related to special implications and residuals of t-norms. In this work we investigate these posers in-depth and give complete answers. Toward this end, firstly we show that many of the properties considered as part of the definition of special implications are redundant. Then, a geometric interpretation of the specialty property is given, using which many results and bounds for such implications are obtained. We have obtained a characterization of general binary operations whose residuals become special. Finally, some constructive procedures to obtain special fuzzy implications are proposed and methods of obtaining special implications from existing ones are given, showing that there are infinitely many fuzzy implications that are special but cannot be obtained as residuals of t-norms. © 2008 Elsevier B.V. All rights reserved. MSC: 03B52; 03E72; 39B99	redundancy (engineering);t-norm	Balasubramaniam Jayaram;Radko Mesiar	2009	Fuzzy Sets and Systems	10.1016/j.fss.2008.11.004	information processing;computer science;artificial intelligence;calculus;mathematics;fuzzy set;algorithm;fuzzy control system	DB	0.33247080655293654	-21.55348872247939	154850
4856a3a2863af2eef672300a60d758cf32bc820a	svr-ga-based adaptive power coal rate modeling and optimization for large coal-fired power units	sampling scale large coal fired power units svr ga based adaptive power coal rate optimization economic performance coal fired power plant nonlinear system configuration complex system configuration support vector regression genetic algorithm boundary parameters load demand operation conditions;large coal fired power units svr ga power coal rate modeling optimization;support vector machines;thermal power stations genetic algorithms power engineering computing regression analysis support vector machines;power engineering computing;thermal power stations;genetic algorithms;regression analysis;abstracts training accuracy prediction algorithms	Power coal rate is an important index to evaluate the overall economic performance of coal-fired power plant. It is however difficult to describe and optimize this feature in different operation conditions because of higher dimension, nonlinear and complex system configuration. An optimized support vector regression (SVR) model was built to predict the power coal rate of power unit, in which the prediction performance of SVR model was optimized by introducing genetic algorithm (GA) to optimize the parameters of SVR model. Considering different boundary parameters, load demand and operation conditions, we built the GA-SVR-based power coal rate model of large coal-fired power unit. The main factors contributing to such model such as the sampling scale, attribute number and specific operators in GA were discussed. The results indicate that the modeling performance is significantly improved in accuracy, searching efficiency and model simplicity; in addition, the model can be conveniently generalized for different types of power units.	complex system;genetic algorithm;mathematical optimization;nonlinear system;sampling (signal processing);software release life cycle;support vector machine;system configuration	Ning-Ling Wang;Shaobo Zhang;Yongping Yang;Degang Chen	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6358970	support vector machine;mathematical optimization;genetic algorithm;computer science;machine learning;regression analysis	HPC	9.671733056665849	-16.91693443617172	154898
c55303b0508ff4356ec3e20b678c9326f50e7ac8	pre-production forecasting of movie revenues with a dynamic artificial neural network	box office forecasting;classification;machine learning;movies;artificial intelligence;dynamic artificial neural networks	The production of a motion picture is an expensive, risky endeavor. During the five-year period from 2008 through 2012, approximately 90 films were released in the United States with production budgets in excess of $100 million. The majority of these films failed to recoup their production costs via gross domestic box office revenues. Existing decision support systems for pre-production analysis and greenlighting decisions lack sufficient accuracy to meaningfully assist decision makers in the film industry. Established models focus primarily upon post-release and post-production forecasts. These models often rely on opening weekend data and are reasonably accurate (~90%) but only if data up until the moment of release is included. A forecast made immediately prior to the debut of a film, however, is of limited use to stakeholders because it can only influence late-stage adjustments to advertising or distribution strategies and little else. In this paper we present the development of a model based upon a Dynamic Artificial Neural Network (DAN2) for the forecasting of movie revenues during the pre-production period. We first demonstrate the effectiveness of DAN2 and show that DAN2 improves box-office revenue forecasting accuracy by 32.8% over existing models. Subsequently, we offer an alternative modeling strategy by adding production budgets, pre-release advertising expenditures, runtime, and seasonality to the predictive variables. This alternative model produces excellent forecasting accuracy values of 94.1%. * Corresponding author. Address: 316P Lucas Hall, Santa Clara University, 500 El Camino Real, Santa Clara, CA 95053-0388, USA. Tel.: +1 408 554 4687; fax: +1 408 554 2331 E-mail addresses: mghiassi@scu.edu (M. Ghiassi), david@davidlio.com (D. Lio), bmoon@scu.edu (B. Moon).	artificial neural network;camino;clara.io;decision support system;fax;like button;mathematical model;offset binary;seasonality	M. Ghiassi;David Lio;Brian Moon	2015	Expert Syst. Appl.	10.1016/j.eswa.2014.11.022	simulation;biological classification;computer science;artificial intelligence;machine learning	HCI	3.6884191601990834	-15.941914578778348	155059
6ca5ffe82a4d8f4a368e4b4f91f9c6ddd8b1bbeb	short-term air quality prediction using a case-based classifier	short term no 2 concentration prediction;case based reasoning cbr;case base reasoning;operant conditioning;back propagation neural network;air monitoring operational data modelling;urban area;data modelling;prediction model;urban air quality;air quality management operational centre;air quality;athens	Elias Kalapanidas and Nikolaos Avouris* University of Patras Electrical and Computer Engineering Department GR-265 00 Rio Patras, Greece email: {ekalap , N.Avouris } @ ee.upatras.gr * Corresponding Author Abstract In the frame of air quality monitoring of urban areas, the task of short-term prediction of key-pollutants concentrations is a daily activity of major importance. Automation of this process is desirable, but development of reliable predictive models with good performance, to support this task in operational basis presents many difficulties. In this paper we present and discuss the NEMO prototype that has been built in order to support short-term prediction of NO2 maximum concentration levels in Athens, Greece. NEMO is based on a case-based-reasoning approach combining heuristic and statistical techniques. The process of development of the system, its architecture and its performance, are described in this paper. NEMO performance is compared with that of a back propagating neural network, and a decision tree (CART). The overall performance of NEMO makes it a good candidate to support air pollution experts in operational conditions.	adobe air;artificial neural network;automation;case-based reasoning;computer engineering;decision tree;elias delta coding;email;heuristic;predictive modelling;prototype;statistical classification;winsock	Elias Kalapanidas;Nikolaos M. Avouris	2001	Environmental Modelling and Software	10.1016/S1364-8152(00)00072-4	data modeling;air quality index;simulation;computer science;engineering;artificial intelligence;operant conditioning;predictive modelling;operations research	Robotics	9.750265880594947	-18.881891471740463	155449
9e4c2740076477ac758c768b42fc1bbe0c055c6b	decision-making for financial trading: a fusion approach of machine learning and portfolio selection		Abstract Forecasting stock returns is an exacting prospect in the context of financial time series. This study proposes a unique decision-making model for day trading investments on the stock market. In this regard, the model was developed using a fusion approach of a classifier based on machine learning, with the support vector machine (SVM) method, and the mean-variance (MV) method for portfolio selection. The modelu0027s experimental evaluation was based on assets from the Sao Paulo Stock Exchange Index (Ibovespa). Monthly rolling windows were used to choose the best-performing parameter sets (the in-sample phase) and testing (the out-of-sample phase). The monthly windows were composed of daily rolling windows, with new training of the classifying algorithm and portfolio optimization. A total of 81 parameter arrangements were formulated. To compare the proposed modelu0027s performance, two other models were suggested: (i) SVM + 1/N, which maintained the process of classifying the trends of the assets that reached a certain target of gain and which invested equally in all assets that had positive signals in their classifications, and (ii) Random + MV, which also maintained the selection of those assets with a tendency to reach a certain target of gain, but where the selection was randomly defined. Then, the portfoliou0027s composition was determined using the MV method. Together, the alternative models registered 36 parameter variations. In addition to these two models, the results were also compared with the Ibovespau0027s performance. The experiments were formulated using historical data for 3716 trading days for the out-of-sample analysis. Simulations were conducted without including transaction costs and also with the inclusion of a proportion of such costs. We specifically analyzed the effect of brokerage costs on buying and selling stocks on the Brazilian market. This study also evaluated the classifieru0027s performance, portfolios’ cardinality, and models’ returns and risks. The proposed main model showed significant results, although demand for trading value can be a limiting factor for its implementation. Nonetheless, this study extends the theoretical application of machine learning and offers a potentially practical approach to anticipating stock prices.	machine learning	Felipe D. Paiva;Rodrigo T. N. Cardoso;Gustavo P. Hanaoka;Saori G Salgado-Moctezuma	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.08.003	limiting factor;support vector machine;machine learning;transaction cost;artificial intelligence;stock market;day trading;portfolio optimization;finance;stock exchange;computer science;portfolio	NLP	5.179705013581779	-18.30595061590395	155622
0c41d52ae135fe7548d24b9673e5f352e2f86814	mid-term electricity price forecasting using svm	forecasting;market price mid term electricity price forecasting svm electricity market new england iso;support vector machines;support vector machines forecasting data models predictive models fuels data preprocessing training data;training data;support vector machines power engineering computing power markets pricing;fuels;performance measures forecasting features preprocessing svm rbf neural networks regression trees;predictive models;data preprocessing;data models	In the modern electricity market, it is very significant to have a precise electricity price forecasting in medium term time horizon. There are few studies done concentrating on the medium term forecasting of the electricity price. Medium term time horizon of electricity price forecasting is very useful and of great importance. It has many applications such as future maintenance scheduling of the power plants, risk management, plan future contracts, purchasing raw materials and determine the market pricing. To forecast the electricity price, some factors are very important, such as choosing the most useful price features that influence the market price, and choosing the proper prediction model that is able to predict the price variations behavior using the historical data. The proposed SVM method and the other employed methods are evaluated using the data from the New England ISO which are published at their official website.	electricity price forecasting;purchasing;risk management plan;scheduling (computing);support vector machine	Abdussalam Mohamed;M. E. El-Hawary	2016	2016 IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2016.7726765	data modeling;support vector machine;training set;demand forecasting;forecasting;computer science;data science;machine learning;data mining;predictive modelling;data pre-processing	AI	8.559882104190308	-17.593282412080992	155745
4b25719a3cbbabdfb88f8208ad15b4265e88505b	computing confidence intervals for output-oriented dea models: an application to agricultural research in brazil	forecasting;reliability;project management;theoretical framework;information systems;normal distribution;maintenance;soft or;information technology;packing;operations research;location;investment;journal;journal of the operational research society;inventory;purchasing;statistical properties;confidence interval;history of or;logistics;marketing;scheduling;production;communications technology;decision making unit;computer science;operational research;efficiency measurement;data envelope analysis;agricultural research;research productivity;applications of operational research;or society;jors;management science;infrastructure	We define and model the research production at Embrapa, the major Brazilian institution responsible for applied agricultural research. The main theoretical framework is Data Envelopment Analysis – DEA. We explore the economic interpretation and the statistical properties of these models to compute confidence intervals for output oriented efficiency measurements, based on a parametric flexible model, defined by the truncated normal distribution. These results provide a better insight on the efficiency classification and allow comparison among the DMUs involved in the evaluation process taking into account inefficiency random variation.	data envelopment analysis	Geraldo da Silva e Souza;Mirian Souza;Eliane Gonçalves Gomes	2011	JORS	10.1057/jors.2010.137	normal distribution;project management;logistics;confidence interval;inventory;economics;forecasting;investment;marketing;operations management;reliability;mathematics;management science;location;management;operations research;information technology;scheduling;statistics	ML	-0.6697643943631647	-13.444586921396002	156283
f2bce2237d5e9217cab4a7ad18a440c4d6dbf951	note on ranking fuzzy triangular numbers	triangular number	In a decision-making problem, the choice between fuzzy alternatives requests a ranking of imprecise values. In the literature this problem has been treated by several authors: the lack of a ‘‘golden choice’’ allows various methods of ranking. Different approaches lead to a confused situation in which there is no procedure able to interpret any problem correctly. Indeed, some methods are counterintuitive or suffer from lack of discrimination between alternatives. The aim of this paper is to propose three new methods of ranking triangular fuzzy numbers that consider the propensity for risk of the decision maker and are able to discriminate confused alternatives. In Section 2 we present the pessimistic method, and in Section 3 the optimistic one. These methods make use of a preference function that expresses the degree to which every alternative is preferred to another. This information could be important, especially when the best alternative is hardly feasible. In Section 4 we show two absolute methods that are equivalent, respectively, to the pessimistic and optimistic ones. In Section 5 a l-combination of the previous methods is shown, so that any decision maker could customize his procedure to his personal features.	fuzzy number	Gisella Facchinetti;Roberto Ghiselli Ricci;Silvia Muzzioli	1998	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199807)13:7%3C613::AID-INT2%3E3.0.CO;2-N	fuzzy logic;triangular number;computer science;artificial intelligence;comparative research;mathematics;operations research;algorithm	Web+IR	-4.190040669550757	-19.66721603707127	156320
3477e0f2e3dd9eb203ac3a553d676dea1f702ebb	roughness measures of intuitionistic fuzzy sets	intuitionistic fuzzy set;intuitionistic fuzzy sets;roughness measures;rough sets;rough set;intuitionistic fuzzy rough sets	In this paper, rough approximations of intuitionistic fuzzy sets with respect to an intuitionistic fuzzy approximation space are first introduced. Basic properties of intuitionistic fuzzy rough sets are then examined. Finally, roughness measures of intuitionistic fuzzy sets are defined and their properties are explored.	approximation;fuzzy set;intuitionistic logic;rough set	Lei Zhou;Wen-Xiu Zhang;Wei-Zhi Wu	2008		10.1007/978-3-540-79721-0_44	mathematical analysis;discrete mathematics;rough set;membership function;defuzzification;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;machine learning;fuzzy set;fuzzy set operations	Logic	-1.0116570538440297	-22.46369133973718	156365
4bc4ba84a972ca00a6cf4df8381acb77c4e038a6	some generalized pythagorean fuzzy bonferroni mean aggregation operators with their application to multiattribute group decision-making		The Pythagorean fuzzy set as an extension of the intuitionistic fuzzy set characterized bymembership and nonmembership degrees has been introduced recently. Accordingly, the square sum of the membership and nonmembership degrees is a maximum of one. The Pythagorean fuzzy set has been previously applied to multiattribute group decision-making. This study develops a few aggregation operators for fusing the Pythagorean fuzzy information, and a novel approach to decision-making is introduced based on the proposed operators. First, we extend the generalized Bonferroni mean to the Pythagorean fuzzy environment and introduce the generalized Pythagorean fuzzy Bonferroni mean and the generalized Pythagorean fuzzy Bonferroni geometric mean. Second, a new generalization of the Bonferroni mean, namely, the dual generalized Bonferroni mean, is proposed by considering the shortcomings of the generalized Bonferroni mean. Furthermore, we investigate the dual generalized Bonferroni mean in the Pythagorean fuzzy sets and introduce the dual generalized Pythagorean fuzzy Bonferroni mean and dual generalized Pythagorean fuzzy Bonferroni geometric mean. Third, a novel approach to multiattribute group decision-making based on proposed operators is proposed. Lastly, a numerical instance is provided to illustrate the validity of the new approach.	fuzzy logic;fuzzy set;numerical analysis	Runtong Zhang;Jun Wang;Xiaomin Zhu;Meimei Xia;Ming Yu	2017	Complexity	10.1155/2017/5937376	operator (computer programming);generalized mean;geometric mean;fuzzy logic;discrete mathematics;group decision-making;mathematics;fuzzy set;bonferroni correction;pythagorean theorem	Robotics	-2.6405699561847937	-20.80873555625965	156636
6000a633b4c32819fc829eeedefdf953d73eb339	model learning and spatial data fusion for predicting sales in local agricultural markets		This research explores the ability to extract knowledge about the associations among agricultural products which allows to improve the prediction of future consumption in the local markets of the Andean region of Ecuador. This commercial activity is carried out using Alternative Marketing Circuits (CIALCO), seeking to establish a direct relationship between producer and consumer prices, and promote buying and selling among family groups. The fusion of information from spatially located heterogeneous data sources allows to establish the best association rules between data sources (several products in several local markets) to infer a significant improvement in spatial prediction accuracy for sales future agricultural products.	association rule learning	Washington R. Padilla;Garcia H. Jesus;José M. Molina López	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455594	machine learning;artificial intelligence;computer science;spatial analysis;kriging;agriculture;association rule learning;producer–consumer problem;marketing	DB	-2.7141069589252744	-11.49561967925197	156816
06672b4affec39e6a71136667659967eb61a626d	international transmission of stock market movements: an adaptive neuro-fuzzy inference system for analysis of taiex forecasting		This study aims to examine the fundamental forces driving stock returns and volatility across the international stock markets. Logistic regression analysis is used to investigate possible highly correlated among 9 international stock markets with stock market of Taiwan. Afterward, the highly correlated stock indices with Taiwan would be selected as the input variables of adaptive network-based fuzzy inference system (ANFIS) model to predict stock prices and their direction of Taiwan Stock Exchange Capitalization Weighted Stock Index. The experimental results of the proposed model are contrasted with other models including the first-order model (Chen in Fuzzy Sets Syst 81(3):311–319, 1996), weighted fuzzy time series model (Yu in 349:609–624, 2005), simple neural network model (Huarng and Yu in Phys A 363(2):481–491, 2006), multivariate model (Huarng et al. in J Travel Tour Mark 21(4):15–24, 2007), ANFIS with volatility causality (Cheng et al. in Neurocomputing 72(16–18):3462–3468, 2009), ANFIS with AR model (Chang et al. in Appl Soft Comput 11:1388–1395, 2011), and artificial bee colony-recurrent neural network model (Hsieh et al. in Applied Soft Computing 11:2510–2525, 2011). Finally, the proposed model produces with lower inaccuracy rate and offers higher direction preciseness than above previous models. The benefit of this methodology depended on its application of a hybrid approach to predict the stock prices and direction with higher accuracy.	adaptive neuro fuzzy inference system;artificial bee colony algorithm;artificial neural network;autoregressive model;causality;entity–relationship model;first-order predicate;fundamental interaction;inference engine;logistic regression;network model;neuro-fuzzy;neurocomputing;recurrent neural network;soft computing;time series;volatility	Mu-Yen Chen;Da-Ren Chen;Min-Hsuan Fan;Tai-Ying Huang	2013	Neural Computing and Applications	10.1007/s00521-013-1461-4	econometrics	NLP	7.162506547382188	-20.198240900986907	156873
c9d32f0c48f6072634485e2073d635235ae273e5	applying fuzzy arithmetic to the system dynamics for the customer-producer-employment model	employment;oscillations;empleo;fuzzy numbers;analisis sistema;numero difuso;parametro variable;system dynamics;relation client fournisseur;fuzzy number;logique floue;nombre flou;logica difusa;prise decision;delay system;decision maker;arithmetique;time delay;dynamical system;parametre variable;fuzzy logic;systeme dynamique;systeme incertain;fuzzy arithmetic;aritmetica;systeme a retard;variable parameter;triangular fuzzy number;arithmetics;t ω weakest t norm operator;system analysis;customer producer employment model;relacion cliente proveedor;regime permanent;analyse systeme;sistema difuso;regimen permanente;systeme flou;information system;sistema con retardo;sistema dinamico;steady state analysis;toma decision;sistema incierto;uncertain system;α cut fuzzy arithmetic;systeme information;fuzzy system;emploi;steady state;sistema informacion;supplier customer relationship	This paper presents a system dynamics analysis based on the application of fuzzy arithmetic. Traditional crisp system dynamics observe that some variables/parameters may belong to the uncertain factors. It is necessary to extend the system dynamics to treat also the vague variables/parameters. The evaluation of fuzzy system dynamics may provide the decision-maker information regarding the system's behavior uncertainties. In this paper, the customer–producer–employment model is examined with the fuzzy system dynamics in two types of fuzzy arithmetic, α-cut fuzzy arithmetic and Tω weakest t-norm operator. Symmetrical and nonsymmetrical triangular fuzzy number (TFN), varied amount of fuzzy inputs’ fuzziness, and length of the system time delay are examined with useful results provided. Particularly, it is revealed that (1) both types of fuzzy arithmetic can provide the steady-state analysis of the system's variables as their counterpart, the crisp arithmetic analysis. (2) The α-cut arithmetic realizes the fu...	system dynamics	Ping-Teng Chang;Ping-Feng Pai;Kuo-Ping Lin;Ming-Shiu Wu	2006	Int. J. Systems Science	10.1080/00207720600774222	fuzzy logic;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;affine arithmetic;mathematics;fuzzy associative matrix;steady state;fuzzy set operations;algorithm;fuzzy control system	Logic	2.0036964431275397	-22.190552453215872	157197
967c7d0af0e96ebf726810e523dc3043ab4ffdbe	hesitant fuzzy translations and extensions of subalgebras and ideals in bck/bci-algebras			brain–computer interface	G. Muhiuddin;Hee Sik Kim;Seok-Zun Song;Young Bae Jun	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-151031		Robotics	1.6131701018164883	-23.4751779696317	157213
b9ce1d660f3a1fb1779017b69a62571fa196ecb3	prediction of vehicle activity for emissions estimation under oversaturated conditions along signalized arterials	signalization;vehicle emission;oversaturation traffic flow;exhaust gases;driving activity;traffic signal timing optimization;vehicle activity;traffic congestion;mathematical models;congestion control;arterial highways;timing optimization;loop detectors;vehicle emissions;analytical model;traffic signal timing;vehicle miles traveled	The traditional methodology for estimating vehicle emissions based on vehicle miles traveled and average speed is not reliable because it does not consider the effects of congestion, control devices, and driving mode (cruise, acceleration, deceleration, and idle). We developed an analytical model to predict vehicle activity on signalized arterials with emphasis on oversaturated traffic conditions. The model depends only on loop detector data and signal settings as inputs and provides estimates of the time spent in each driving mode, which consequently leads to more accurate vehicle emission estimates. The application of the proposed model on a real-world arterial shows that it accurately estimates the time spent and consequently the emissions per driving mode. We also applied the model to evaluate the effectiveness of signal timing optimization in reducing vehicle emissions.		Alexander Skabardonis;Nikolas Geroliminis;Eleni Christofa	2013	J. Intellig. Transport. Systems	10.1080/15472450.2012.704338	simulation;computer science;engineering;automotive engineering;mathematical model;transport engineering;network congestion;statistics;computer network	ML	9.795704283455482	-10.274650180027313	157227
580e23f56bfa85080e3a1ed0eba00a3857b9274d	commodity derivatives pricing with cointegration and stochastic covariances	stochastic covariance;option pricing;cointegration;stochastic convenience yield	Empirically, cointegration and stochastic covariances, including stochastic volatilities, are statistically significant for commodity prices and energy products. To capture such market phenomena, we develop a continuous-time dynamics of cointegrated assets with a stochastic covariance matrix and derive the joint characteristic function of asset returns in closed-form. The proposed model offers an endogenous explanation for the stochastic mean-reverting convenience yield. The time series of spot and futures prices of WTI crude oil and gasoline shows cointegration relationship under both physical and risk-neutral measures. The proposed model also allows us to fit the observed term structure of futures prices and calibrate the marketimplied cointegration relationship. We apply it to value options on a single commodity and on multiple commodities. © 2015 Elsevier B.V. and Association of European Operational Research Societies (EURO) within the International Federation of Operational Research Societies (IFORS). All rights reserved.	apply;characteristic function (convex analysis);futures and promises;international federation of operational research societies;operations research;time series	Mei Choi Chiu;Hoi Ying Wong;Jing Zhao	2015	European Journal of Operational Research	10.1016/j.ejor.2015.05.012	financial economics;economics;stochastic modelling;valuation of options;macroeconomics;mathematics;microeconomics;stochastic investment model;statistics;cointegration	AI	3.6602032158774467	-12.400141374308408	157276
ac15a1e700386ec2fbb72620e0110b433f892e8b	interval regression analysis by quadratic programming approach	quadratic programming;possibilistic regression analysis;function fitting;fuzzy regression;quadratic program;quadratic programming approach;necessity estimation models;interval regression analysis;regression model;least squares approximation;indexing terms;fuzzy set theory;polynomials;fuzzy sets;data analysis;trapezoidal fuzzy output interval regression analysis quadratic programming approach possibilistic regression analysis central tendency possibilistic property fuzzy regression necessity regression models possibility estimation models necessity estimation models function fitting;statistical analysis;regression analysis quadratic programming linear programming polynomials least squares methods fuzzy systems least squares approximation fuzzy sets data analysis analysis of variance;possibility estimation models;possibility theory statistical analysis quadratic programming polynomials fuzzy set theory;least square;linear programming;analysis of variance;linear program;possibility theory;regression analysis;possibilistic property;central tendency;least squares methods;fuzzy systems;necessity regression models;trapezoidal fuzzy output;polynomial approximation	When we use linear programming in possibilistic regression analysis, some coefficients tend to become crisp because of the characteristic of linear programming. On the other hand, a quadratic programming approach gives more diverse spread coefficients than a linear programming one. Therefore, to overcome the crisp characteristic of linear programming, we propose interval regression analysis based on a quadratic programming approach. Another advantage of adopting a quadratic programming approach in interval regression analysis is to be able to integrate both the property of central tendency in least squares and the possibilistic property in fuzzy regression. By changing the weights of the quadratic function, we can analyze the given data from different viewpoints. For data with crisp inputs and interval outputs, the possibility and necessity models can be considered. Therefore, the unified quadratic programming approach obtaining the possibility and necessity regression models simultaneously is proposed. Even though there always exist possibility estimation models, the existence of necessity estimation models is not guaranteed if we fail to assume a proper function fitting to the given data as a regression model. Thus, we consider polynomials as regression models since any curve can be represented by the polynomial approximation. Using polynomials, we discuss how to obtain approximation models which fit well to the given data where the measure of fitness is newly defined to gauge the similarity between the possibility and the necessity models. Furthermore, from the obtained possibility and necessity regression models, a trapezoidal fuzzy output can be constructed.	approximation algorithm;coefficient;curve fitting;fitness function;least squares;linear programming;polynomial;polynomial-time approximation scheme;quadratic function;quadratic programming	Hideo Tanaka;Haekwan Lee	1998	IEEE Trans. Fuzzy Systems	10.1109/91.728436	econometrics;mathematical optimization;proper linear model;discrete mathematics;computer science;linear programming;polynomial regression;mathematics;fuzzy set;least squares;quadratic programming;regression analysis;statistics	ML	3.992781533338366	-22.875688007603223	157304
32e3121dd42c6fcd7382c5ff5ff2266bac64450a	fuzzy random weighted weber problems in facility location		Abstract   This article considers facility location in a Weber problem with weights including both uncertainty and vagueness. By representing its weights as fuzzy random variables, it can be extended to a fuzzy random weighted Weber problem, and then formulated as a fuzzy random programming problem. By introducing possibility and necessity measures and chance constraints, the extended problem is reformulated to new two types of Weber problems. Based upon characteristics of facility location, theorem for solving the reformulated problems are shown.	facility location problem	Takeshi Uno;Kosuke Kato;Hideki Katagiri	2015		10.1016/j.procs.2015.08.257	mathematical optimization;artificial intelligence;mathematics;mathematical economics	Theory	-0.733155243937914	-18.549865808326093	157414
281f0ebf4e801147e515f7f40a2470aac266cf8b	daily load forecasting based on rough sets and relevance vector machine	support vector machines;bayes methods;rough set theory;support vector machines bayes methods load forecasting power engineering computing power system economics power system planning power system security rough set theory;small samples;load forecasting;power engineering computing;power system economics;relevance vector machine;power system planning;load forecasting rough sets relevance vector machine;kernel method;power system secure operation daily load forecasting model rough sets relevance vector machine rvm model bayesian frame support vector machine svm power system planning power system economic;support vector machine;rough set;forecast accuracy;power system security;load modeling predictive models load forecasting forecasting rough sets training meteorology	Relevance vector machine (RVM) is a novel kernel methods based on Bayesian frame. It is similar to support vector machine (SVM), but it needs much less learning parameters to adjust and has a better forecast accuracy. However, due to the fact that the RVM is not suitable for the forecast task with large sample dataset, the application of RVM in daily load forecasting is limited. This paper proposes a daily load forecasting model based on rough sets and RVM. This model takes load data and weather information as condition attributes, classifies the historical load dataset into 24 hourly sub-dataset, utilizes the theory of rough sets for each sub-dataset to obtain the reduced condition attribute which is used as the input of the RVM model, thus avoids the blindness in building the load forecast model. Therefore the issue of daily load forecasting is transformed into the issue of small sample forecast. In experiments, this model illustrates a high practicability and forecast accuracy.	experiment;kernel method;load profile;relevance vector machine;rough set;support vector machine	Xueming Yang	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6023665	support vector machine;rough set;computer science;machine learning;pattern recognition;data mining	Robotics	8.733940145514831	-17.462729163199413	157649
411d406666c780c21f15348903b47f1b940342f1	emd2fnn: a strategy combining empirical mode decomposition and factorization machine based neural network for stock market trend prediction		Abstract Stock market forecasting is a vital component of financial systems. However, the stock prices are highly noisy and non-stationary due to the fact that stock markets are affected by a variety of factors. Predicting stock market trend is usually subject to big challenges. The goal of this paper is to introduce a new hybrid, end-to-end approach containing two stages, the Empirical Mode Decomposition and Factorization Machine based Neural Network (EMD2FNN), to predict the stock market trend. To illustrate the method, we apply EMD2FNN to predict the daily closing prices from the Shanghai Stock Exchange Composite (SSEC) index, the National Association of Securities Dealers Automated Quotations (NASDAQ) index and the Standard u0026 Poor’s 500 Composite Stock Price Index (Su0026P 500), which respectively exhibit oscillatory, upward and downward patterns. The results are compared with predictions obtained by other methods, including the neural network (NN) model, the factorization machine based neural network (FNN) model, the empirical mode decomposition based neural network (EMD2NN) model and the wavelet de-noising-based back propagation (WDBP) neural network model. Under the same conditions, the experiments indicate that the proposed methods perform better than the other ones according to the metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). Furthermore, we compute the profitability with a simple long-short trading strategy to examine the trading performance of our models in the metrics of Average Annual Return (AAR), Maximum Drawdown (MD), Sharpe Ratio (SR) and AAR/MD. The performances in two different scenarios, when taking or not taking the transaction cost into consideration, are found economically significant.	artificial neural network;association for automated reasoning;backpropagation;closing (morphology);darknet market;experiment;hilbert–huang transform;ibm ssec;mean squared error;molecular dynamics;network model;performance;software propagation;stationary process;wavelet	Feng Zhou;Haomin Zhou;Zhihua Yang;Lihua Yang	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.07.065	trading strategy;machine learning;financial economics;artificial intelligence;stock market;artificial neural network;profitability index;sharpe ratio;mean absolute percentage error;stock exchange;mean squared error;computer science	ML	6.853550315281514	-18.994098180667294	157686
3664c8426c0942486bf6b6e90cbb221a5703107d	extension of inagaki general weighted operators and a new fusion rule class of proportional redistribution of intersection masses	fusion rules;smarandache codification;dsm cardinal;dsm classic rule;double weight;conflicting mass;fusion rule;information fusion;proportional redistribution rules;inagaki weighted operator rule	In this paper we extend Inagaki Weighted Operators fusion rule (WO) [see 1, 2] in information fusion by doing redistribution of not only the conflicting mass, but also of masses of non-empty intersections, that we call Double Weighted Operators (DWO). Then we propose a new fusion rule Class of Proportional Redistribution of Intersection Masses (CPRIM), which generates many interesting particular fusion rules in information fusion. Both formulas are presented for 2 and for n ≥ 3 sources. An application and comparison with other fusion rules are given in the last section.		Florentin Smarandache	2008	CoRR		artificial intelligence;data mining;algorithm	Vision	1.9642235919367652	-21.64931878754515	158051
86ff15a748660aff68d26ca9574561e2008f0f39	the application of grey correlation analysis in the big electrical customers from baoding city	cities and towns costs information analysis companies computer applications conference management energy management data analysis forward contracts sorting;training and education;power markets grey systems;baoding city data analysis consumer values grey correlative analysis method low value customers;presses;low value customers;companies;service;the grey correlative analysis;aluminum;incomplete information;data analysis;power markets;fertilizers;service customer s value the grey correlative analysis;baoding city;consumer values;steel;grey systems;cities and towns;correlation;point of view;grey correlative analysis method;customer s value;correlation analysis	By analyzing the data of 20 big electrical customers from Baoding City, Hebei province, calculating the consumer values, and adopting the grey correlative analysis method, this paper has found out the VIP customers from many electrical customers based on incomplete information, and defined a clear correlative degree between the important customer and the enterprise. After that, this paper puts forward a point of view based on sorting correlative degree, which asks us to devote plentiful time and energy to the loyal customers, and give them man-to-man service. The customers with low correlative degree are called low-value customers, their current and potential values are low, and enterprises should analyze the reason and give them certain training and education. Enterprise should also abandon those customers who are sensitive to price, less loyal, hard to persuade and have poor credit records.	sorting	Shuliang Liu;Yining Ma	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.827	marketing;operations management;advertising;business	ECom	-3.285785142920298	-13.53922496957169	158139
3136e3020de85ccac0698f3bbe145d86d8b20649	the optimal group consensus models for 2-tuple linguistic preference relations	2 tuple linguistic;combination matrix;convergence;journal;consensus measure;期刊论文;group decision making;article	We establish in this paper the optimization model of group consensus of 2-tuple linguistic preferential relations (LPRGCO Model), put forward three kinds of solutions to this model, and discover in it the convergence of group consensus. To detect the LPRGCO Model, we first build two kinds of optimal matrices as standards to measure the group consensus of 2-tuple linguistic preference relations (LPRs). And to analyze consensus deviations, we then adopt three types of measures, namely, the individual degree of consistency regarding alternative decision pairs, the deviational degree of the group consensus regarding alternative decision pairs, and the degree of group consensus regarding the original 2-tuple LPRs. On the basis of the previous analysis we not only construct an optimization model to probe into the deviation of the group consensus of 2-tuple LPRs by minimizing the weighted arithmetic average of deviation degrees of individual consistency, but also point out three feasible solutions to this optimization model: the optimal solution, satisfactory solutions, and non-inferior solutions. Accordingly, we discover different conditions in terms of the three solutions. And hence, we can from the aforementioned discussion draw a conclusion that the deviation of group consensus either decreases or stays invariant as the number of decision makers (DM) increases. To expatiate on the practical value of the model proposed, we will display in this paper numerical examples.	categorization;mathematical optimization;numerical analysis;optimal design	Zaiwu Gong;Jeffrey Forrest;Yingjie Yang	2013	Knowl.-Based Syst.	10.1016/j.knosys.2012.09.001	group decision-making;convergence;computer science;artificial intelligence;data mining	AI	-3.4894889904331863	-20.072960022968402	158182
20a43e09a1f7118b09123db0224f4c506b13f3a2	a modified genetic programming for behavior scoring problem	back propagation neural network bpn;financial data processing;genetic program;customer relationship management;conference;risk management;credit;backpropagation neural network genetic programming behavior scoring problem risk management financial institutions future credit performance forecasting real life credit data set chinese commercial bank;backpropagation;data mining;back propagation neural network;commercial banks;genetic programming data mining computational intelligence risk management neural networks statistical analysis artificial intelligence decision trees artificial neural networks computer science;risk;financial institutions;back propagation neural network bpn data mining genetic programming gp behavior scoring;genetic programming gp;genetic algorithms;classification accuracy;genetic algorithms backpropagation customer relationship management financial data processing;behavior scoring	Behavior scoring is an important part of risk management in financial institutions, which is used to help financial institutions make better decisions in managing existing customers by forecasting their future credit performance. In this paper, a modified genetic programming (MGP) is introduced to solve the behavior scoring problems. A real life credit data set in a Chinese commercial bank is selected as the experimental data to demonstrate the classification accuracy of this method. MGP is compared with back-propagation neural network (BPN), and another GP that uses normalized inputs (NGP), the experimental results show that the MGP has slight better classification accuracy rate than NGP, and outperforms BPN in dealing with behavior scoring problems because of less historical samples of credit data in Chinese commercial banks	artificial neural network;backpropagation;business process network;genetic programming;mouse genetics project;neo geo pocket;real life;risk management;software propagation	Qing-Shan Chen;Zhang De-Fu;Wei Li-Jun;Huo-Wang Chen	2007	2007 IEEE Symposium on Computational Intelligence and Data Mining	10.1109/CIDM.2007.368921	genetic algorithm;risk management;computer science;artificial intelligence;backpropagation;machine learning;data mining;risk	AI	6.618772295783452	-19.530846980971173	158448
6e21fe902cdc621f642576413204a757da82bca5	hybrid flexible neural tree for exchange rates forecasting	forecasting;international economic relations;exchange rates;hybrid flexible neural tree;exchange rate forecasting;artificial neural networks;computational modeling;particle swarm optimizer;forecasting theory;vectors;particle swarm optimisation exchange rates forecasting theory instruction sets international finance learning artificial intelligence;international finance;flexible neural tree;particle swarm optimization;probabilistic incremental program evolution hybrid flexible neural tree exchange rate forecasting international economic relations instruction sets particle swarm optimization;probabilistic incremental program evolution;exchange rate;predictive models;gene expression programming;neurons;particle swarm optimization exchange rate flexible neural tree forecasting;learning artificial intelligence;particle swarm optimization algorithm;particle swarm optimisation;exchange rates forecasting vectors neurons predictive models computational modeling artificial neural networks;artificial neural network;instruction sets	Exchange rate is an important link of international economic relations. In this paper, a novel method for improving flexible neural tree is proposed to forecasting exchange rate data. The hybrid flexible neural tree with pre-defined instruction sets can be created and evolved. The structure and parameters of hybrid flexible neural tree is optimized using probabilistic incremental program evolution and particle swarm optimization algorithm. Compared with the conventional artificial neural network and flexible neural tree based on gene expression programming, the experimental results indicate that the proposed method is feasible and efficient.	algorithm;artificial neural network;gene expression programming;mathematical optimization;particle swarm optimization;simulation	Yuehui Chen;Zhenxiang Chen	2012	2012 8th International Conference on Natural Computation	10.1109/ICNC.2012.6234577	mathematical optimization;computer science;artificial intelligence;machine learning	Robotics	7.468483384002457	-18.99612667587648	158451
244799df47d4ed54d58589625bd47fcfcb957209	a similarity measure with uncertainty for incompletely known fuzzy sets	uncertain systems;recommender systems uncertain information i fuzzy set theory atanassov intuitionistic fuzzy sets interval valued fuzzy sets interval valued similarity measure lower bound upper bound similarity degrees desirable property;fuzzy set theory;fuzzy sets uncertainty measurement uncertainty upper bound cognition recommender systems fuzzy set theory;uncertain systems fuzzy set theory	This paper is devoted to the problem of measuring similarity between pieces of uncertain (incomplete) information in the framework of I-fuzzy set theory (Atanassov's intuitionistic fuzzy sets and interval-valued fuzzy sets). We propose a way of determining an interval-valued similarity measure of I-fuzzy sets that preserves information about the operands' uncertainty by approximating lower and upper bounds of similarity degrees. We discuss some of the desirable properties of this measure and illustrate it with an example application in recommender systems.	approximation algorithm;fuzzy set;information;operand;recommender system;set theory;similarity measure	Anna Stachowiak;Krzysztof Dyczkowski	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608432	fuzzy logic;possibility theory;combinatorics;mathematical analysis;discrete mathematics;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy set operations	AI	-1.8630792723709966	-22.273163399278697	158484
64df25c89fc05bd2c6f72c5366c34ea0325f521c	a hybrid evolutionary probabilistic forecasting model applied for rainfall and wind power forecast	forecasting;wind forecasting;wind power plants evolutionary computation load forecasting regression analysis time series;time series analysis;forecasting predictive models wind forecasting probabilistic logic wind power generation adaptation models time series analysis;predictive models;hybrid forecasting model and metaheuristics rainfall forecast wind power forecast probabilistic forecast soft sensors;historical datasets hybrid evolutionary probabilistic forecasting model rainfall forecast wind power forecast deterministic point forecasts conditional distribution hybrid fuzzy forecasting model wind farm irish eirgrid institute real time series quantile regression probabilistic quantiles;probabilistic logic;wind power generation;adaptation models	Several works in the literature so far have been focused on deterministic point forecasts, which, usually, indicates the conditional mean of future observations. An increasing need for generating the entire conditional distribution of future observations has been required for the new generation of soft sensors. This study aims the probabilistic forecasts, reporting the use of a hybrid fuzzy forecasting model applied in two different forecasting problems. Our adapted model is applied to predict the rain of the city of Vitoria, in the state of Espírito Santo, Brazil. Real data from a wind farm, provided by the Irish EirGrid institute, was used for analyzing the proposal over a real time series with high fluctuations. Due to the stochasticity of the the hybrid model, which is calibrated through the use of an evolutionary metaheuristic, we adapted it in order to generate future using quantile regression. Computational experiments indicated the ability of the model in finding useful probabilistic quantiles, which were flexible enough in order to limit the lower and upper bounds of the historical datasets. While the probabilistic quantiles suggested the probability of rain and its magnitude, they were also able to predict expected ranges of the amount of energy generated from the wind farm.	algorithm;approximation;computation;experiment;feasible region;load profile;metaheuristic;quantum fluctuation;sensor;time series	Guilherme G. Netto;Alexandre C. Barbosa;Mateus N. Coelho;Arthur R. L. Miranda;Vitor Nazário Coelho;Marcone J. F. Souza;Frederico G. Guimarães;Agnaldo J. R. Reis	2016	2016 IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2016.7502494	meteorology;probabilistic forecasting;econometrics;technology forecasting;simulation;geography;consensus forecast	AI	9.179142291018943	-16.994476374810862	158626
9518f90d517d0a81a7589c0eab1e1a4cf4d1e9a9	"""review: """"generalized concavity in fuzzy optimization and decision analysis"""" by jaroslav ramík and milan vlach"""			concave function;decision analysis	Milan Mares	2002	Kybernetika		mathematical economics;decision analysis;fuzzy logic;mathematical optimization;mathematics	Logic	-0.260910594794971	-17.155190581019436	158643
c76158935040848bd527c01b3c2cc56ecb821b43	electricity market clearing price forecast based on adaptive kalman filter		In the competitive electricity market, in order to guarantee generators profits, they usually adopt bidding strategies to participate in the electricity market competition. The forecast of market clearing prices in the electricity market can provide a reference for the quotation behavior of generator companies. This paper develops a new day-ahead electricity price forecasting based on adaptive Kalman filter. Under the condition of unknown prediction model state transition matrix and the statistical characteristics of the observed noise, To estimate the unknown parameters of the prediction model according to the electricity market clearing electricity price data. According to the estimation, the generators will make a quotation with a slightly lower than the predicted market clearing price, ensuring their unit capacity can participate in the market bidding and achieve the goal of maximizing its own profit. The predicted price is applied to the PJM power market to verify its prediction accuracy.		Liang Ding;Quanbo Ge	2018	2018 International Conference on Control, Automation and Information Sciences (ICCAIS)	10.1109/ICCAIS.2018.8570534		Robotics	5.404667111912473	-13.486500169309195	158790
1cd0f8df5f97fca4d4d68bc0cb4a93d6feb60838	concept lattices under non-commutative conjunctors are generalized concept lattices	galois connections.;con- cept lattices;: formal concept analysis;formal concept analysis;fuzzy logic;incomplete information;concept analysis	Generalized concept lattices have been recently proposed to deal with uncertainty or incomplete information as a non-symmetric generalization of the theory of fuzzy formal concept analysis. On the other hand, concept lattices have been defined as well in the framework of fuzzy logics with noncommutative conjunctors. The contribution of this paper is to prove that any concept lattice for non-commutative fuzzy logic can be interpreted inside the framewok of generalized concept lattices, specifically, it is isomorphic to a sublattice of the cartesian product of two generalized concepts lattices.	formal concept analysis;fuzzy logic	Jesús Medina;Manuel Ojeda-Aciego;Jorge Ruiz-Calviño	2007			fuzzy logic;discrete mathematics;concept class;lattice miner;noncommutative geometry;galois connection;commutative property;formal concept analysis;cartesian product;mathematics	Theory	-1.4123829636007765	-23.775577609437534	158887
077e1f0ef2a23e6c0e24620b5194c94a0eb30dd1	multi-level subjective effects-based assessment	inference mechanisms;matrix algebra;sensor fusion;cross impact matrix;effects-based assessment;subjective assessments;dempster-shafer theory;effects-based approach to operations;effects-based assessment;subjective assessment;belief function;decomposition;information fission;inverse support function;pseudo belief function	In this paper we develop a multi-level subjective effects-based assessment method. This method takes subjective assessments regarding activities and effects of a plan as inputs. From these assessments and a cross impact matrix that represents the impact between all elements of the plan we calculate combined assessments for all plan elements. For each activity (and effect) we calculate how much additional assessment value is needed to reach the assessment of the higher-level effect without its local assessment. The discrepancy between assessments received and assessments needed is an indication of relative performance of the activities. The method is based on belief functions and their combination under a generalization of the discounting operation.	discrepancy function	Johan Schubert	2010	2010 13th International Conference on Information Fusion		reliability engineering;artificial intelligence;data mining;mathematics	SE	-4.166220928159976	-18.57296026974079	159021
b0549916129bdcd03e69523afcc24752f0ac421c	the treasury bill rate, the great recession, and neural networks estimates of real business sales	recession	This paper analyzes out-of-sample forecasts of real total business sales. We study monthly data from January 1970 to June 2012. The predictor variable, 3-month Treasury bill interest rate, was used with both the regression (used as a benchmark) and neural network models. The neural network models’, trained in supervised learning with the Levenberg-Marquardt backpropagation through time algorithm, prediction accuracy was confirmed with correlation coefficient and root mean square tests. The activation function used for the focused gamma models of the time-lag recurrent networks in both the hidden and output layers was tanh. The forecast period ranged from January 2006 to June 2012 thus encompassing the past recession. The real business sales variable is one of the indicators used as a coincident index of the U.S. business cycle, and is included among the variables studied by the Federal Reserve to formulate monetary policy. It is thus an important indicator surrogating for real GDP, which is reported quarterly and with a longer time delay. Our analysis shows that recent recessions have increased in duration, so that using a 36-month change to approximate an average cycle in estimating and forecasting is more relevant and accurate than past usage of a 24-month change.	neural networks	Anthony Joseph;Maurice Larrain;Claude Turner	2014		10.1016/j.procs.2014.09.084	actuarial science;artificial intelligence;data mining;statistics	ML	7.316211526218198	-18.353099570931548	159157
1b0c253864c02b7d29d72e1c377a500a584636ae	possibilistic linear regression with fuzzy data: tolerance approach with prior information		We introduce the tolerance approach to the construction of fuzzy regression coefficients of a possibilistic linear regression model with fuzzy data (both input and output). The method is very general: the only assumption is that the fuzzy data are unimodal and their α-cuts are efficiently computable. We take into account possible prior restrictions of the parameters space: we assume that the restrictions are given by linear and quadratic constraints. The method for construction of the possibilistic regression coefficients is in a reduction of the fuzzy-valued model to an interval-valued model on a given α-cut, which is further reduced to a linear-time method computing with endpoints of the intervals. The speed of computation makes the method applicable for huge datasets. Unlike various approaches based on mathematical programming formulations, the tolerance-based construction preserves central tendency of the resulting regression coefficients. In addition, we prove further properties: if inputs are crisp and outputs are fuzzy, then the construction preserves piecewise linearity and convex shape of fuzzy numbers. We derive an O(np)-algorithm for enumeration of breakpoints of the membership function of the estimated coefficients. (Here, n is the number of observations and p is the number of regression parameters). Similar results are also derived for the fuzzy input-and-output model. We illustrate the theory for the case of triangular and asymmetric Gaussian fuzzy inputs and outputs of an inflation-consumption model.	breakpoint;coefficient;computable function;computation;defuzzification;fuzzy logic;fuzzy number;input/output;mathematical optimization;piecewise linear continuation;time complexity	Michal Cerný;Milan Hladík	2018	Fuzzy Sets and Systems	10.1016/j.fss.2017.10.007	proper linear model;mathematics;machine learning;artificial intelligence;fuzzy logic;linear model;statistics;polynomial regression;defuzzification;mathematical optimization;fuzzy number;membership function;fuzzy classification	ML	3.532601954761932	-22.985972708024295	159206
4d41ed71ed6e00121d5086b2923514e2e8127c12	quantile forecasting of pm10 data in korea based on time series models		In this chapter, we analyze the particulate matter PM10 data in Korea using time series models. For this task, we use the log-transformed data of the daily averages of the PM10 values collected from Korea Meteorological Administration and obtain an optimal ARMA model. We then conduct the entropy-based goodness of fit test for the obtained residuals to check the departure from the normal and skew-t distributions. Based on the selected skew-t ARMA model, we obtain conditional quantile forecasts using the parametric and quantile regression methods. The obtained result has a potential usage as a guideline for the patients with some respiratory disease to pay more attention to health care when the conditional quantile forecast is beyond the limit values of severe health hazards.	time series	Yingshi Xu;Sangyeol Lee	2017		10.1007/978-3-319-50742-2_36	probabilistic forecasting;econometrics	ML	6.215723637325914	-14.966987223469667	159213
30371d55793a2f0d692d5f87b1331451960c15ad	a methodology for fuzzy modeling of engineering systems	finanza;logical relation;engineering;parametric model;systeme commande;sistema control;optimisation;linguistique;fuzzy set;control difusa;finance;fuzzy programming;optimizacion;algoritmo borroso;fuzzy control;fuzzy modelling;modelisation floue;logique floue;linguistic modeling;conjunto difuso;logica difusa;ensemble flou;circuito logico;engineering system;ingenierie;fuzzy logic;inference rule;control system;linguistica;circuit logique;mathematical programming;fonction appartenance;fuzzy algorithm;membership function;algorithme flou;financial analysis;ingenieria;optimization;sistema difuso;statistical techniques;systeme flou;funcion pertenencia;programmation floue;optimal algorithm;logic circuit;programmation mathematique;random search;programacion matematica;fuzzy system;programacion difusa;fuzzy model;commande floue;empirical research;linguistics	This paper describes a systematic approach to the modeling of engineering systems using a fuzzy formulation that is independent of human knowledge. The computer algorithm described here operates on a set of experimental observations of the system and constructs an optimum fuzzy model for these observations. The program automatically selects membership functions, deduces inference rules, constructs logical relations, and determines the formulae for conducting union and intersection operations. Membership functions, rules, and logical operations are de ned parametrically. Model parameters are optimized so that the model can, at least, re-produce with minimum error the data that were used in obtaining the membership functions and rules. Therefore, model parameters are optimized to minimize error or entropy of the back-inferences of the observations from which the model was constructed. To reach the global minimum and avoid entrapment in a local minimum, a random search is carried out, then followed by a systematic Hooke–Jeeves search optimization algorithm. It has been found that this technique is more successful, compared with other statistical techniques. c © 2001 Elsevier Science B.V. All rights reserved.	algorithm;control system;fuzzy concept;logical connective;logical relations;mit engineering systems division;mathematical optimization;maxima and minima;membership function (mathematics);random search;rule-based system	Y. M. Ali;Liangchi Zhang	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00272-3	fuzzy logic;parametric model;random search;financial analysis;membership function;logic gate;computer science;artificial intelligence;machine learning;mathematics;fuzzy set;empirical research;algorithm;fuzzy control system;rule of inference	AI	1.0351380904277205	-16.797185375275244	159333
33cc0de3eb4f359df0e97c0773a547f3e1a6a266	a fuzzy soft set based approximate reasoning method			approximation algorithm;fuzzy set	Keyun Qin;Jilin Yang;Zhicai Liu	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-16088	fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;fuzzy set operations	Robotics	0.09732130176164398	-23.40771280520358	159474
