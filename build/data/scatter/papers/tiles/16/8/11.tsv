id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
1b13da5095e930ea2e43a9832a614c33e39d1f59	an accurate evaluation of routing density for symmetrical fpgas	vlsi cad algorithm;symmetrical fpga;fpga routing	""""""" # $ % & ' ( *)% & + ), , """"-. $ )0/ (21 ( '3 $ 4 / """" 5 $ ), 6 798+:<; $= >@? -A B C C #D & A FEG H A 4 I & ( *)2E< # $ & J ), , """"798+:<; K""""E< L 5MONQP4RTS UWVYX MOZ9[&N\MO] X VY^ _%NQVY` R""""X aQ]bE c A & , )%B (d e 9 d & f )H """" ( 5 % g)2 & J 0 $> h! 5 # I & f L Ei & A 2 & A jE 0 . <( *)e A . ) (! k D -A) >el+m f # $ -L n QD ( (! I $E + k AD& # + f & ( ( *)5 A o & e Cp ' *) # J J # q 4 5 ( $ / g) > r s 4 ) Kb & e i & A t u D $ & cv w > x y ( #-* ) O Co """" D E Fz > v y{# / $ """"B, $D G / )2 %| ;%} l5| '3 8+| > Keywords 798+:<;~ K, G r  } ; j & A 4K, ), A -b798+:<; 1. INTRODUCTION L 4  4$E 4 , A  ! ( A KC 4798+:C; 6 1 ( 'Y & $ 4 / """" n $ )J= 9/f e 0 & ' -* d;  r }6 'Y f 1 H A $ ( # $= 2 ),  *), >9 G % ) 0 $ ) % % . % 0 & E ( #)F (4 -* L798+:C; """" #  w *>5  """" A 0 5 + *EC '3( 0 -96  '"""" =5 """" ' $ ) 1 & $ / / -A B 06 }oGJ = K, $E % (H b e Kf (2 $ 4 / 9 E A """" / B $>ol5 } <4 G, &B,'3 d $ """"/ -A 6 J? %= KE ! QE + 0 , )sJ F F $ (F/ )n / & $> |  """" #H n #( 1 (iEG 2 # & GD """" g """"/ -A >QEG """" A !/ , """"B, 4 I & $  """"/ -A I ! 4 B I ' /f *EC F EG , % (} Gs A > } $ ) 0 5 % /  o & -* T)& $K& s """" $ ( (F 0 (H 4 B,'3 $ (H I $ ), K FEG """" E A H , F (sD g """" s/ d( $ EG u n """" / A $ """" A) K G / #  & f ), A 798+:C; .D ) # ( / ) +A ( & #  6 / ( = E + # (u $ 4 / 4 EG >} p , ) K< $ ' ! """" q -* )  )I """" * 798+:<;~ >  ,) I """" < C ), A 798+:C;u D / # (! $D 5 5 D $ -L) $>   ( f & (s , """" . , 2 + # L f gEC 5 # K ( (0  A 0 A' j( $ C >b G < /J AD& C ( A / C <EG < """" -A . %  C 4 m  """" -W( A *) 0  ( >  v  #(HQ A  '3 # </ ( '  """" 6 (~ 2 D F # & # , d # ( = $D L + #p , * -& 9 W *EC '3 # -& $>CQl5:<;   o A ( d , """"-J 1 9 d 92 # ( n & k n @ ( -g )H 4 ' $ 4 / 5 E ><:e<8   b -* (I 5 0 / ' t A F d *EC '3( 0 -L , D -+ B H / 4> r # *EC % & # ( )! """"-A & 0 G $ D )d 0 """"Bd # , $ B > rgn   z""""o ,( ( #  & # d $ """" K+ (s # (s F & / )4 f ( A   Q A '3 C & # $ H """"-A & >   KW , (H . #E & A """" & G""""' 4798+:<;  798 r } 6 1 ( 'Y & $  """"/ -A F , = A {EG { I 4  H  A  s  ( *)u  $D s $ / g)5 C( >b |;%}ol5|'3  x b (% |;%}ol5|' 8+|  w   A * )I (4 #p , * ) ( n # A ( & $ / -A $>< D -* d e 9 2 D (d $ D )%/ ) 0 $) A A % -* (I D F/ """" (F  I """" Ap >. G ( ' /f gE< #  %|; } l5|'Y q s ( |;%}ol5|'3 8+| I G G -* G 0/f (F 0 # *  5 >? -b $E%K |;%}ol5|'3 8+|i 9 # (H / # ! i {( #-* ) $>uC F( $ EG/ """"B ! m # D s 0 > r 4 -A)4/f < 0 q -A' , """" A & 2 b 9 f & # < & QE 2E D m,  w w + 1 ( $m, 4 e & G !Q A % # K ( 2 #( m, *)! G A  0 ( $ H e & #'3 # {6 *> > KW # & / (H / -G (( # $ """" A( = & A $ ) > r L # $K EC # , C E f &  """" # 0 ( $ """"/ A*) ( D  { ¡ ) 0 -5 """" $ T)!/ """" (798@' :<; $>! I 4 i / A & i 9 & 0EC B 0 c & f  A / % $  """" A ( ' h ! / # e C $ """" B A I¢~K@ '3 s I $ ' )d A  9E D C m, I ,(I #p %¢£L A 9 (d f + d + @ *EC '3 46 &-A &/ """" (d( # $ """" A( = $ """" # &),> g)I L ), , """"-o798+:<; > ¤ E , """" 2 # 2 ( ' D (! ¥ 0 -* 1 c C & A F e c E / B d """" # ( I n 0 # & # c ) K """" (4 H """" ' ) 0 & 9 ! & # F  ! I ! $> h! I { f &  2 #E¦  §EG { 0 ( A *)2 # )! (F k D -A) > + m 9 & A C 2 ̈3 ( # g) ©0 """" 0 b 9 & $ / -A *)% !6 / -*=@ >oa QE< #D& # $K c """" #( *) #« F c( c # '3 A & d F A & / B d ) 0 798+:<; ) K """" ( ( + L $ B& , n + 'Y & # 5 + E / B n # ) >¬£­}o & p # ) K< n( I   &' %E A  d / '3 4 -< n & A c # 0  EG """" !/ -A B >}o) K@ 4 m  """"-A)4 -A  F & A H ' n  4 E / B s IEC n  4 A & / B d A c 2B& #) q 4 """" #D  """"   """"-G s  & O """" %( #-* )I (0 $ / g) >b7O .  K$EC e """"' ) # L + A # e # + ), , """"798+:C; """" ( ( D )I #* / # E A """"  )I $ B A . 'Y & # . G / B ./ 2 < EG %/ , """"B, >bh! < % e L Ei d e  + + k AD& #) ><C C """"-O C( < E I ® A + ( 9  ̄ # + < n J """" I( #-* ) (I & $ / -A *) E -A 5( , 5 f (u e-A & I m # ! > I # ( K@ & 5 & A ¡ % , * )c( $ E  ¡ n % ( D, ( ' ( '3 p # .E A """" ( f 9 °\'Y -W , L #   '3 A """"-, / 6 0 '3( $ + ,= K (s ¥ #p , * ) K@ & c n % >0a $EC D $KbEC Q)d + #( D LEe T) 1 ( 0  A % 0'3( $ """" # { # !/ ) A 4 i ! & A {( A *) D ) , '3 '3 f 2   F nE A ® F # & ( $ n E I & # >oh! D ( #(i 9 # 9 o m, A' , ( $ e e H  e & f (H & A 4> G % -A QEj & n A & D ) #k D C 1 ( d . &-A E e % % ( -* ), H6 ,> > Ko #( 4 2( -g )!/,)v w&>Ax&y ± D $ ( """" . & %|; } l5|'Y @ 8+|  w &gK$E A """" L & /f 9B, $E± F A $ = > 2. ROUTING DENSITY FOR SYMMETRICAL FPGA 7 w 6 =O( # L e """" A G . L *), ) 0 798+:<; > r % % & G n *) f % f & # 2F6 w&=G ' 1 $ """"/ -A I& A I/ , """"B, 6 } GJ = K 6  =9 # & 1 $ / r  ®/ B 6 r  J = K (¦6 v =% # ~ & # >; 4 QE  7 Iw&6 / = KW A #  9 .EG # d ( E A """"  , E A """" { """" """" A (4 2D ' -5 (  , $ """"-+ H """" #>2;¦E d # F6 *> > K $ """"Bb=e I (±/f *EC # 3} <® A  (± E u f ! u (! E A """" !/ , """"B, >%7 A & # F 6 =5 """" (6 / = $E $ 4 / 9 EG """" # C d E d , C & @  (F E F/ , """"B, K, # f D ),> ; ( %  w  *K < G/f I $E 2 , """"-A) e ' / 5« m / A*)d & # % """" D (IE F c/ -A B 2 """" % -A)« m, / H6 *> > Kf ́Gμ5¶3¢·E # 4¢ n , %/f % G $ """"B, % f % -*= """" ( 2« m A/ g)! EG """" 0/ B + L 06 g> > K ́  ̧C¶±v&= ><} p , ) K & '  """" A EC  # & ( I c ( ) 5 £#Q G7 wC . G 5 & # L 0 ), A G798+:<; > I1 I2"""	a* search algorithm;cray urika-gd;emoticon;eurographics;field-programmable gate array;introduction to algorithms;kilobyte;quadratic equation;quantum dot;query expansion;routing;shebang (unix)	Nak-Woong Eum;Taewhan Kim;Chong-Min Kyung	2001		10.1145/368122.368794	electronic engineering;real-time computing;theoretical computer science;fpga prototype	Theory	8.198082700466585	49.75039228421165	46653
0a0d492145a5c6d31085f344631b916a629c94d6	iem926: an energy efficient soc with dynamic voltage scaling	circuit optimisation;energy management systems;mobile handsets;power consumption;system-on-chip;soc devices;circuit optimization;dynamic voltage scaling;embedded devices;energy consumption;energy management;intelligent energy manager;mobile phone;multiple power domain;system-on-chip	One of today's most successful embedded devices, the mobile phone, embodies a set of challenging design requirements: long battery life, small size, high performance and low cost. The single parameter that complicates the simultaneous fulfilment of all of these design goals is energy efficiency of the system, since batteries only hold a finite amount of charge. To operate within the allotted energy budget, systems must be optimized for energy consumptionduring design and also at run-time. Increasingly it is not sufficient to statically optimize for worst-case conditions but designers must enable systems to adapt to conditions at runtime.The Intelligent Energy ManagerTM (IEM) technology provides an integrated solution for addressing energy management of SoC devices. In this paper we present data about the energy consumption characteristics of a multiple power-domain based SoC which includes PDA functionality built around an ARM926EJ-S core.	best, worst and average case;dynamic voltage scaling;embedded system;image scaling;integrated enterprise modeling;mobile phone;personal digital assistant;requirement;run time (program lifecycle phase);system on a chip	Krisztián Flautner;David Flynn;David J Roberts;Dipesh I. Patel	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		system on a chip;embedded system;electronic engineering;real-time computing;computer science;engineering;efficient energy use;energy budget;energy management	EDA	-1.7156818751999334	57.081443478071634	47094
6b2ffee91f4258854afa3a76b87aad1b128845ae	a highly compressed timing macro-modeling algorithm for hierarchical and incremental timing analysis		Large-scale hierarchical and incremental timing analysis has driven the need for highly compressed timing macro-models. A small timing macro-model for accelerating hierarchical timing is desired because the size of incremental changes dramatically increases as the macro-models are widely used in the large design process. In fact, it takes days for an incremental timing analysis on millions of gates with thousands of incremental changes. To date, the timing macro-models generated by timing macro-modeling algorithms from all the previous works are not compact enough. In this work, we provide four essential techniques in our timing macro-modeling algorithm, which are able to generate highly compressed timing macro-models for hierarchical and incremental timing analysis. In addition, our timing macro-model maintain high accuracy and the efficiency in generating our macro-models. Our algorithm generates timing macro-models where the model sizes are 9% better in the number of nodes and 19% better in the number of edges than the original circuit. Our work outperforms the state of arts significantly in both model size and the runtime in macro-model usage.	algorithm;static timing analysis	Tin-Yin Lai;Martin D. F. Wong	2018	2018 23rd Asia and South Pacific Design Automation Conference (ASP-DAC)	10.1109/ASPDAC.2018.8297300	real-time computing;adaptive voltage scaling;computer science;mean time between failures;algorithm;macro;design process;static timing analysis	EDA	0.6060516637530564	57.098313993610596	47185
1d9a6b3462a737de7d4e40d9bd02a167bc024f53	parallel algorithms for fpga placement	verification;distributed memory;cosimulation;parallel algorithm;mems;architecture exploration;hdls;shared memory multi processor;versatile place and route;simulated annealing;socs;design	Fast FPGA CAD tools that produce high quality results has been one of the most important research issues in the FPGA domain. Simulated annealing has been the method of choice for placement. However, simulated annealing is a very compute-intensive method. In our present work we investigate a range of parallelization strategies to speedup simulated annealing with application to placement for FPGA. We present experimental results obtained by applying the different parallelization strategies to the Versatile Place and Route (VPR) Tool, implemented on an SGI Origin shared memory multi-processor and an IBM-SP2 distributed memory multi-processor. The results show the tradeoff between execution time and quality of result for the different parallelization strategies.	computer-aided design;display resolution;distributed memory;field-programmable gate array;multiprocessing;parallel algorithm;parallel computing;place and route;run time (program lifecycle phase);shared memory;simulated annealing;speedup	Malay Haldar;Anshuman Nayak;Alok N. Choudhary;Prithviraj Banerjee	2000		10.1145/330855.330988	embedded system;design;computer architecture;parallel computing;real-time computing;verification;distributed memory;simulated annealing;computer science;operating system;parallel algorithm;microelectromechanical systems	HPC	0.6002834743293305	47.845198533058884	47320
54f62d4fdfc770a8c224ab130f9ce7403e3a91f0	exploiting bit-level delay calculations to soften read-after-write dependences in behavioral synthesis	design automation;behavioral synthesis;high level synthesis circuit optimisation data flow graphs digital arithmetic;bit level delay calculations;clocks;routing;arithmetic operations;preprocessor;data flow graphs;resource management;allocation;data flow graph;high level synthesis;scheduling algorithm;scheduling;critical path;presynthesis optimization algorithm;schedules;digital arithmetic;optimization;field programmable gate arrays;bit granularity;design automation bit level delay calculations read after write dependences behavioral synthesis high level synthesis algorithms arithmetic operations presynthesis optimization algorithm data flow graphs logic operations preprocessor bit granularity circuit synthesis;circuit optimisation;delay high level synthesis clocks scheduling algorithm arithmetic routing circuit optimization processor scheduling flow graphs logic;logic operations;optimal algorithm;high level synthesis algorithms;circuit synthesis;read after write dependences;scheduling allocation circuit synthesis design automation	Conventional high-level synthesis (HLS) algorithms are very conservative when dealing with read-after-write (RAW) dependences, the execution of one operation is allowed once all its predecessors have been calculated. However, in the execution of arithmetic operations, some bits are required later than others, and some bits are produced earlier than others. This paper proposes a presynthesis optimization algorithm that relaxes RAW dependences, taking advantage of this feature for a more efficient HLS of data flow graphs formed by additions, multiplications, and logic operations. The presented preprocessor analyzes the critical path at bit granularity and splits the arithmetic operations into subword fragments. These fragments become the input to any regular HLS tool to speed up circuit execution times through scheduling in different cycles of the fragments obtained from the same original operation. This way, the execution of one operation may begin before the calculus of its predecessors has been completed. This becomes feasible when the execution of the predecessor has begun in the selected cycle or in a previous one, and even if it will finish in a posterior cycle. The experimental results that were carried out show that implementations obtained from the optimized specification are, on the average, 70% faster, with only slight variations in the data path area.	algorithm;clock signal;critical path method;dataflow;direct read after write;high- and low-level;high-level synthesis;mathematical optimization;preprocessor;run time (program lifecycle phase);scheduling (computing);substring	Rafael Ruiz-Sautua;Maria Del Carmen Bisi Molina;Jose Manuel Mendias	2007	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2007.895570	embedded system;electronic engineering;parallel computing;real-time computing;computer science;resource management;theoretical computer science;operating system;programming language;scheduling;algorithm	EDA	-1.7469656456263818	51.939367244363694	47642
f022604f97a9b612d5906ae1e371bc1d1f39d7ee	delay management for programmable video signal processors	assignment problem;video algorithms;memory management;delay management problem;video signal processing;job shop scheduling;constructive approach;signal analysis;resource management;programmable video signal processors;benchmark set;np hard problem;delay signal processing signal processing algorithms job shop scheduling resource management memory management signal mapping video signal processing signal analysis streaming media;delay assignment problem;streaming media;computational complexity;signal processing;intermediate data;signal mapping;digital signal processing chips;computational complexity real time systems video signal processing digital signal processing chips timing;delay minimization problem;memory allocation;benchmark set delay management problem programmable video signal processors memory allocation intermediate data video algorithms np hard problem delay minimization problem delay assignment problem network flow techniques constructive approach;network flow;signal processing algorithms;network flow techniques;real time systems;timing	We consider the problem of memory allocation for intermediate data in the mapping of video algorithms onto programmable video signal processors. The corresponding delay management problem is proved to be NP-hard. We present a solution strategy that decomposes the delay management problem into a delay minimization problem followed by a delay assignment problem. The delay minimization problem is solved with network flow techniques. The delay assignment problem is handled by a constructive approach. The performance of the combined approach is analyzed by means of a benchmark set of industrially relevant video algorithms.	algorithm;assignment problem;benchmark (computing);central processing unit;feasible region;flow network;heuristic (computer science);np-hardness;scheduling (computing);systemic functional grammar	M. L. G. Smeets;Emile H. L. Aarts;Gerben Essink;Erwin A. de Kock	1997		10.1109/EDTC.1997.582345	parallel computing;real-time computing;computer science;processing delay;theoretical computer science;elmore delay;network delay	EDA	0.6220982949967645	53.118185430680676	47649
b679033a8bc303f900f227ad494a55d8c47f0de0	shared i/o-cell structures: a framework for extending the ieee 1149.1 boundary-scan standard	working group;design for testability;ieee standards;logic testing circuit testing integrated circuit interconnections standards publication guidelines digital integrated circuits integrated circuit testing pins system testing electronic design automation and methodology;ieee 1149 1 working group shared i o cell structures ieee 1149 1 boundary scan standard customized design for test features test logic functional logic;boundary scan testing;logic testing;integrated circuit testing;vlsi;design for test;vlsi ieee standards boundary scan testing design for testability integrated circuit testing logic testing	This paper proposes a framework to help designers understand how to integrate customized Design for Test features under the guidelines of the 1149.1 standard. The paper begins by discussing the reasoning behind some of the essential features of the IEEE 1149.1 Boundary Scan Standard. Following that, arguments are made for allowing greater flexibility within the standard in order to accommodate design features, that may follow the intent of the standard but not necessarily the letter of its specification. An example of such a feature is to allow I/O cells in which test logic is “shared” with functional logic. The proposed framework for compatibility with the IEEE 1149.1 Standard may result in some changes to the current standard. The framework is intended to provide a guideline for additional tools and recommendations to be developed by the IEEE 1149.1 working group and EDA companies in order to help produce testable designs. The framework also helps to explain and understand the ramifications of implementing testability features, which are not compliant with the Boundary-Scan Standard.	boundary scan;design for testing;input/output;jtag	Bulent I. Dervisoglu;Mike Ricchetti;Bill Eklow	1998		10.1109/TEST.1998.743294	reliability engineering;embedded system;electronic engineering;boundary scan;logic family;computer science;design for testing;system testing;engineering drawing;power optimization;computer engineering	Visualization	9.88651852299992	53.59774814848544	47797
1f64f6ffec9a2a2204af97843b851cf1ad9ac57f	software pipelining for coarse-grained reconfigurable instruction set processors	processing element;optimisation;application software;processor scheduling;reconfigurable architectures;operation assignment;spatial computation;reconfigurable logic;code generation;pipeline processing vliw application software hardware processor scheduling software algorithms registers energy consumption reconfigurable logic software performance;reconfigurable instruction generation;software performance;compilers;vliw;reconfigurable processors;loop optimisation;registers;energy consumption;scheduling;coarse grained reconfigurable instruction set processors;assignment algorithm;software algorithms;reconfigurable functional unit software pipelining coarse grained reconfigurable instruction set processors operation assignment reconfigurable instruction generation scheduling compilers reconfigurable processors assignment algorithm loop optimisation u assignment spatial computation;optimisation reconfigurable architectures pipeline processing program compilers;coarse grained logic;software pipelining;coarse grained;program compilers;instruction scheduling;u assignment;reconfigurable processor;pipeline processing;reconfigurable functional unit;hardware	This paper shows that software pipelining can be an effective technique for code generation for coarse-grained reconfigurable instruction set processors. The paper describes a technique, based on adding an operation assignment phase to software pipelining, that performs reconfigurable instruction generation and instruction scheduling on a combined algorithm. Although typical compilers for reconfigurable processors perform these steps separately, results show that the combination enables a successful usage of the reconfigurable resources. The assignment algorithm is the key for using software pipelining on the reconfigurable processor.The technique presented is also able to exploit spatial computation inside the reconfigurable functional unit by which the output of a processing element is directly connected to the input of another processing element without the need of an intermediate register. Results show that it is possible to reduce the cycle count by using this spatial computation.	algorithm;central processing unit;code generation (compiler);compiler;computation;cycle count;execution unit;instruction scheduling;pipeline (computing);reconfigurable computing;scheduling (computing);software pipelining	Francisco Barat;Murali Jayapala;Pieter Op de Beeck;Geert Deconinck	2002		10.1109/ASPDAC.2002.994945	software pipelining;computer architecture;compiler;application software;parallel computing;real-time computing;software performance testing;computer science;very long instruction word;operating system;processor register;instruction scheduling;scheduling;code generation	EDA	-0.12040817057332376	51.76463812634562	47984
c6ec6ef3c799740fdebfcab836c28d796b066418	dark silicon: from embedded to hpc systems (dagstuhl seminar 16052)	004;dark silicon embedded hpc parallel computing performance analysis and tuning power density power modelling programming tools resource manageme	"""Semiconductor industry is hitting the utilization wall and puts focus on parallel and heterogeneous many-core architectures. While continuous technological scaling enables the high integration of 100s-1000s of cores and, thus, enormous processing capabilities, the resulting power consumption per area (the power density) increases in an unsustainable way. #R##N#With this density, the problem of Dark Silicon will become prevalent in future technology nodes: It will be infeasible to operate all on-chip components at full performance at the same time due to the thermal constraints (peak temperature, spatial and temporal thermal gradients etc.). However, this is not only an emerging threat for SoC and MPSoC designers, HPC faces a similar problem as well: The power supplied by the energy companies as well as the cooling capacity does not allow to run the entire machine at highest performance anymore. The goal of Dagstuhl Seminar 16052 """"Dark Silicon: From Embedded to HPC Systems"""" was to increase the awareness of the research communities of those similarities and to work and explore common solutions based on more flexible thermal/power/resource management techniques both for runtime, design time as well as hybrid solutions."""	dark silicon	Michael Gerndt;Michael Glaß;Sri Parameswaran;Barry Rountree	2016	Dagstuhl Reports	10.4230/DagRep.6.1.224	embedded system;real-time computing;simulation;engineering	EDA	-3.1139831274062666	56.173215743546926	48099
348e3a5bf00616947c620cbe5bc0ab7999095312	efficient rapid prototyping of image and video processing algorithms	video processing algorithms;complex systems-on-a-chip;heterogenous parallel;efficient rapid prototyping;data structure;video processing;real time systems;hardware description languages;image processing;field programmable gate arrays;reconfigurable hardware;programming language;real time;field programmable gate array	Image and video processing tasks are often confined for real-time execution on large size workstations or expensively custom designed hardware. The current availability of mature reconfigurable hardware, like field programmable gate arrays (FPGAs), coupled with the usage of hardware programming languages offers a good path for porting such applications on portable devices. This paper explores the rapid prototyping of a real-time road sign recognition system on a FPGA, using an algorithmic-like hardware programming language: the Handel-C language. We investigate the relationship between efficient Handel-C data, structures, constructs and the related high level C data, structures, constructs. Programming guidelines are proposed for the development of real-time image and video processing, starting from a better organized high level C code that can be then easily ported in Handel-C. Results are illustrated showing the effectiveness of employing Handel-C to turn an entirely software based system into a fully functional field deployable device.	algorithm;field-programmable gate array;handel;handel-c;high-level programming language;personal digital assistant;rapid prototyping;real-time clock;real-time locating system;real-time transcription;reconfigurable computing;video processing;workstation	Salvatore Vitabile;Antonio Gentile;Sabato Marco Siniscalchi;Filippo Sorbello	2004	Euromicro Symposium on Digital System Design, 2004. DSD 2004.	10.1109/DSD.2004.1333310	embedded system;computer architecture;data structure;image processing;computer science;operating system;programming language;field-programmable gate array	Arch	3.111074868299702	48.775791668785786	48220
4f22775857bc7e59ad41a3754e5481256c922d69	constraint analysis for code generation: basic techniques and applications in facts	processor architecture;resource constraint;search space;constraint analysis;code generation;register binding;general solution;foreground memory;scheduling;digital signal processor;phase coupling;resource availability;dsp;time constraint	Code generation methods for digital signal processors are increasingly hampered by the combination of tight timing constraints imposed by signal p processing applications and resource constraints implied by the processor architecture. In particular, limited resource availability (e.g.registers) poses a problem for traditional methods that perform code generation in separate stages (e.g., scheduling followed by register binding). This separation often results in suboptimality (or even infeasibility) of the generated solutions because it ignores the problem of phase coupling (e.g., since value lifetimes are a result of scheduling, scheduling affects the solution space for register binding). As a result, traditional methods need an increasing amount of help from the programmer (or designer) to arrive at a feasible solution. Because this requires an excessive amount of design time and extensive knowledge of the processor architecture, there is a need for automated techniques that can cope with the different kinds of contraints during scheduling. By exploiting these constraints to prune the schedule search space, the scheduler is often prevented from making a decision that inevitably violates one or more constraints. FACTS is a research tool developed for this purpose. In this paper we will elucidate the philosophy and concepts of FACTS and demonstrate them on a number of examples.	central processing unit;code generation (compiler);digital signal processor;feasible region;microarchitecture;programmer;scheduling (computing)	Koen van Eijk;Bart Mesman;Carlos A. Alba Pinto;Qin Zhao;Marco Bekooij;Jef L. van Meerbergen;Jochen A. G. Jess	2000	ACM Trans. Design Autom. Electr. Syst.	10.1145/362652.362660	embedded system;mathematical optimization;digital signal processor;parallel computing;real-time computing;microarchitecture;computer science;theoretical computer science;operating system;digital signal processing;programming language;scheduling;algorithm;code generation	EDA	-0.6659669624471708	55.13966969663117	48670
83da98b0358717a195985fc40dad7ed9d4bb4682	the study of transient faults propagation in multithread applications		Whereas contemporary Error Correcting Codes (ECC) designs occupy a significant fraction of total die area in chipmultiprocessors (CMPs), approaches to deal with the vulnerability increase of CMP architecture against Single Event Upsets (SEUs) and Multi-Bit Upsets (MBUs) are sought. In this paper, we focus on reliability assessment of multithreaded applications running on CMPs to propose an adaptive application-relevant architecture design to accommodate the impact of both SEUs and MBUs in the entire CMP architecture. This work concentrates on leveraging the intrinsic softerror-immunity feature of Spin-Transfer Torque RAM (STTRAM) as an alternative for SRAM-based storage and operation components. We target a specific portion of working set for reallocation to improve the reliability level of the CMP architecture design. A selected portion of instructions in multithreaded program which experience high rate of referencing with the lowest memory modification are ideal candidate to be stored and executed in STT-RAM based components. We argue about why we cannot use STT-RAM for the global storage and operation counterparts and describe the obtained resiliency compared to the baseline setup. In addition, a detail study of the impact of SEUs and MBUs on multithreaded programs will be presented in the Appendix.	baseline (configuration management);mbus (sparc);reliability engineering;single event upset;software propagation;static random-access memory;thread (computing);working set	Navid Khoshavi;Armin Samiei	2016	CoRR		mbus;architecture;real-time computing;parallel computing;static random-access memory;computer science;working set	Arch	7.161601695683136	59.73741716397137	48688
1b93b8c30f3a32f4e505848b64844178ce774789	a layered architecture for noc design methodology	layered architecture;design methodology	Multiprocessor system on chip (MPSoC) platform is an innovative trend of System on Chip (SoC) that enhances system performance. Demanding quality of service parameters and performance metrics, especially in mobile applications, are leading to the exploration of even more innovative architectures for SoC. These will have to incorporate highly scalable, reusable, predictable, cost and energy efficient architectures. Network on Chip (NOC) is a key example of this trend. NOC separates computing and communication concerns in an elegant manner. We propose here a seven layered architecture for designing NOC-based systems. Such a platform can separate domain specific issues in separate layers, which will allow for more effective modeling of concurrency and synchronization issues, in an attempt to develop an optimized system. For such a layered architecture, models of computation (MOC) will provide a framework to model various algorithms and activities, while accounting for and exploiting concurrency and synchronization aspects. These MOCs may differ from one to another NOC region. Further, a combination of these MOCs may be needed to truly represent a given NOC region. We have analyzed various models of computation (MOC) suitable for NOC. MLDesigner provides a system level modeling platform which allows one to integrate such MOCs together. We present our efforts and experiences so far.	algorithm;concurrency (computer science);defense in depth (computing);mpsoc;mobile app;model of computation;multiprocessing;network on a chip;quality of service;scalability;simulation;synchronization (computer science);system on a chip	A. Agarwal;R. Shankar	2005			mpsoc;distributed computing;system on a chip;parallel computing;concurrency;scalability;multitier architecture;applications architecture;computer science;network on a chip;model of computation	EDA	-0.10507648335018649	55.5067020799679	48837
1447be3d899115a834874e585256360911036a4d	compiling packet programs to reconfigurable switches		Programmable switching chips are becoming more commonplace, along with new packet processing languages to configure the forwarding behavior. Our paper explores the design of a compiler for such switching chips, in particular how to map logical lookup tables to physical tables, while meeting data and control dependencies in the program. We study the interplay between Integer Linear Programming (ILP) and greedy algorithms to generate solutions optimized for latency, pipeline occupancy, or power consumption. ILP is slower but more likely to fit hard cases; further, ILP can be used to suggest the best greedy approach. We compile benchmarks from real production networks to two different programmable switch architectures: RMT and Intel’s FlexPipe. Greedy solutions can fail to fit and can require up to 38% more stages, 42% more cycles, or 45% more power for some benchmarks. Our analysis also identifies critical resources in chips. For a complicated use case, doubling the TCAM per stage reduces the minimum number of stages needed by 12.5%.	benchmark (computing);compiler;dependence analysis;greedy algorithm;hard coding;heuristic (computer science);integer programming;linear programming;lookup table;network packet;network switch;period-doubling bifurcation;telecommunications access method;virtual economy	Lavanya Jose;Lisa Yan;George Varghese;Nick McKeown	2015			parallel computing;real-time computing;computer science;theoretical computer science;operating system;database;distributed computing;computer network	Networks	0.21533357329585728	50.51155024053586	48873
286d9338ac5f3d2ca762208924fdde4a49a6537a	system level design with spade: an m-jpeg case study	application specific integrated circuits;circuit cad;data compression;image sequences;integrated circuit design;multiprocessing systems;video coding;m-jpeg case study;spade;y-chart paradigm;design space exploration;explicit design step;generic building blocks;heterogeneous signal processing systems;model construction;multi-processor architectures;system level design;system level performance analysis	In this paper we present and evaluate the Spade (System level Performance Analysis and Design space Exploration) methodology through an illustrative case study. Spade is a method and tool for architecture exploration of heterogeneous signal processing systems. In this case study we start from an M-JPEG application and use Spade to evaluate alternative multi-processor architectures for implementing this application. Spade follows the Y-chart paradigm for system level design; application and architecture are modeled separately and mapped onto each other in an explicit design step. Spade permits architectures to be modeled at an abstract level using a library of generic building blocks, thereby reducing the cost of model construction and simulation. The case study shows that Spade supports efficient exploration of candidate architectures; models can be easily constructed, modified and simulated in order to quickly evaluate alternative system implementations.	design space exploration;jpeg;level design;multiprocessing;programming paradigm;signal processing;simulation	Paul Lieverse;Todor Stefanov;Pieter van der Wolf;Ed F. Deprettere	2001	IEEE/ACM International Conference on Computer Aided Design. ICCAD 2001. IEEE/ACM Digest of Technical Papers (Cat. No.01CH37281)		data compression;embedded system;computer architecture;real-time computing;microarchitecture;computer science;operating system;signal processing;application-specific integrated circuit;electronic system-level design and verification;statistics;computer engineering;integrated circuit design	EDA	3.840508521587625	52.43646968290771	48939
b5d054b4aa252f2bc9a8fda8b641d7edf1769a9c	a broadcast-enabled sensing system for embedded multi-core processors	voltage droop;temperature sensors;system on chip;broadcasting resources broadcast enabled sensing system embedded multicore processors on chip sensing systems microarchitectural parameter signatures sensor data transfer rates distributed voltage droop sensor information;multicore processing;on chip sensing voltage droop multi core;on chip sensing;broadcasting;voltage measurement distributed sensors intelligent sensors multiprocessing systems;multicore processing program processors system on chip temperature sensors broadcasting benchmark testing;program processors;benchmark testing;multi core	Contemporary multi-core architectures deployed inembedded systems are expected to function near the operational limits of temperature, voltage, and device wear-out. To date, most on-chip sensing systems have been designed to collect and use sensor information for these parameters locally. In this paper, a new sensing system to enhance multi-core dependability which supports both the local and global distribution of sensing data in embedded processors is considered. The benefit of the new sensing architecture is verified using the broadcast of microarchitectural parameter signatures which can be used toidentify impending voltage droops. Low-latency broadcasts are supported for a range of sensor data transfer rates. Up to a 9% performance improvement for a 16-core system is determined via the use of the distributed voltage droop sensor information (5.4% on average). The entire sensing system, including broadcasting resources, requires about 2.6% of multi-core area.	antivirus software;best, worst and average case;central processing unit;core (optical fiber);dependability;embedded system;microarchitecture;multi-core processor;sensor;simulation;type signature	Jia Zhao;Shiting Lu;Wayne P. Burleson;Russell Tessier	2014	2014 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2014.18	embedded system;electronic engineering;real-time computing;engineering	Arch	4.737851234005494	57.95854321681022	49019
c4dfe04b8bdaa613d802628c571a6231f3922c85	analysis and characterization of data energy tradeoffs: for vlsi architectural agility in c-ran platforms	fft c ran vlsi lte hevc;very large scale integration;runtime;data energy tradeoffs traffic adaptation vlsi architecture adaptation c ran platforms dynamic architectural scaling technique interactive applications fft computations datapath scaling benefits applications fabric scaling cloud computing applications energy efficiency built in flexibility;computer architecture;application specific integrated circuits;fabrics;computer architecture very large scale integration fabrics runtime cloud computing throughput application specific integrated circuits;vlsi cloud computing fast fourier transforms radio access networks telecommunication power management;cloud computing;throughput	We investigate trade-offs between traffic adaptation and VLSI architecture adaptation in C-RAN platforms. We propose a dynamic architectural scaling technique applied to interactive applications that require FFT computations. Our implementation results suggest Datapath scaling benefits applications with up to 4.89x improvement in GOPS/mW, while Fabric scaling can benefit Cloud Computing applications with up to 181.97x improvement in GOPS/mW, when compared to published methods. Improvements in network performance and energy-efficiency was achieved at a cost of built-in flexibility in the proposed VLSI architectures.	built-in self-test;c-ran;cloud computing;computation;datapath;fast fourier transform;image scaling;network performance;very-large-scale integration	Pascal Nsame;Guy Bois;Yvon Savaria	2015	2015 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2015.7168921	embedded system;throughput;computer architecture;parallel computing;real-time computing;cloud computing;computer science;operating system;application-specific integrated circuit;very-large-scale integration	Arch	2.2580212139772	48.37083844786938	49040
9a37c3d5ddc1d7bc032df2e4f7c6a889c76d6450	hierarchical dataflow model for efficient programming of clustered manycore processors		Programming Multiprocessor Systems-on-Chips (MPSoCs) with hundreds of heterogeneous Processing Elements (PEs), complex memory architectures, and Networks-on-Chips (NoCs) remains a challenge for embedded system designers. Dataflow Models of Computation (MoCs) are increasingly used for developing parallel applications as their high-level of abstraction eases the automation of mapping, task scheduling and memory allocation onto MPSoCs. This paper introduces a technique for deploying hierarchical dataflow graphs efficiently onto MPSoC. The proposed technique exploits different granularity of dataflow parallelism to generate both NoC-based communications and nested OpenMP loops. Deployment of an image processing application on a many-core MPSoC results in speedups of up to 58.7 compared to the sequential execution.	central processing unit;code generation (compiler);computation;dataflow;distributed memory;embedded system;hardware acceleration;high- and low-level;image processing;locality of reference;mpsoc;manycore processor;map (parallel pattern);memory hierarchy;microprocessor;multiprocessing;network on a chip;openmp;parallel computing;performance;scheduling (computing);shared memory;speedup;system on a chip	Julien Hascoet;Karol Desnos;Jean-François Nezan;Benoît Dupont de Dinechin	2017	2017 IEEE 28th International Conference on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2017.7995270	memory architecture;computer science;computer architecture;parallel computing;real-time computing;memory management;mpsoc;scheduling (computing);dataflow architecture;multiprocessing;dataflow;model of computation	EDA	-0.502295578935407	50.540372100761545	49071
53980e92e224088b899f51183c5f5ad869a83cc5	hardware/software-based diagnosis of load-store queues using expandable activity logs	software;software testing;program diagnostics;long period;microarchitecture;microarchitectural activity;processor full speed;hybrid post silicon validation approach;memory ordering hardware software based diagnosis load store queue expandable activity logs bug diagnosis hybrid post silicon validation approach error detection mechanism expandable logging mechanism microarchitectural activity processor full speed;testing;system recovery computer architecture error detection fault diagnosis microprocessor chips program debugging program diagnostics program verification;program verification;expandable activity logs;chip;computer architecture;memory ordering;system recovery;hardware software based diagnosis;expandable logging mechanism;conference report;bug diagnosis;load store queue;optimization;latches;program debugging;error detection;computer bugs;error detection mechanism;optimization software testing microarchitecture computer bugs hardware latches;fault diagnosis;microprocessor chips;hardware	The increasing device count and design complexity are posing significant challenges to post-silicon validation. Bug diagnosis is the most difficult step during post-silicon validation. Limited reproducibility and low testing speeds are common limitations in current testing techniques. Moreover, low observability defies full-speed testing approaches. Modern solutions like on-chip trace buffers alleviate these issues, but are unable to store long activity traces. As a consequence, the cost of post-Si validation now represents a large fraction of the total design cost. This work describes a hybrid post-Si approach to validate a modern load-store queue. We use an effective error detection mechanism and an expandable logging mechanism to observe the microarchitectural activity for long periods of time, at processor full-speed. Validation is performed by analyzing the log activity by means of a diagnosis algorithm. Correct memory ordering is checked to root the cause of errors.	dspace;error detection and correction;medical algorithm;memory ordering;microarchitecture;operating system;overhead (computing);tracing (software)	Javier Carretero;Xavier Vera;Jaume Abella;Tanausú Ramírez;Matteo Monchiero;Antonio González	2011	2011 IEEE 17th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2011.5749740	embedded system;parallel computing;real-time computing;computer science;operating system;software testing	Arch	6.723804627703586	59.1120866070998	49151
5df45f34617ae86d3aa4cbe608d070d7e217284f	a fault injection tool for sram-based fpgas	logic simulation;circuit faults;fault simulation;quick fault injection;field programmable gate arrays circuit faults hardware emulation testing single event upset software tools space technology embedded software circuit simulation;fault emulation technique;standard synthesis tools;emulation;testing;configuration bitstream;circuit simulation;jbits;fault injection tool;logic testing;logic simulation field programmable gate arrays sram chips integrated circuit testing fault simulation logic testing;integrated circuit testing;device configuration cell;software tools;space technology;single event upset;field programmable gate arrays;fault injection;sram based fpga;embedded software;hardware;sram chips;device configuration cell fault injection tool sram based fpga fault emulation technique configuration bitstream standard synthesis tools jbits quick fault injection	A fault injection tool for SRAM-based FPGAs based on the fault emulation technique is presented. Faults are injected by modifying the configuration bitstream while this is loaded into the device, without using standard synthesis tools or available commercial software, such as Jbits or similar. This makes our tool independent of the system used for design development and allows a quick fault injection. Also, any device configuration cell can be accessed and this permits to study the effects of possible contentions or shorts, which cannot be analyzed using commercial tools. An example of the use of the tool is described.	bitstream;commercial software;emulator;fault injection;field-programmable gate array;programming tool;single event upset;static random-access memory	Monica Alderighi;Sergio D'Angelo;Marcello Mancini;Giacomo R. Sechi	2003		10.1109/OLT.2003.1214379	embedded system;emulation;electronic engineering;real-time computing;embedded software;computer science;engineering;stuck-at fault;logic simulation;software testing;space technology;field-programmable gate array	EDA	9.018759456584414	52.83627059035558	49218
09d2d311992e4ffa445b23ea3715a5bd1e14e700	design space exploration of hardware architectures for content based music classification	audio signal processing;feature extraction computer architecture hardware multiple signal classification signal processing space exploration computational efficiency;advanced audio signal processing application design space exploration hardware architectures content based music classification digital end consumer devices static consumer devices mobile consumer devices;signal classification;mobile computing;music;signal classification audio signal processing mobile computing music	The content based music classification is a very attractive feature for digital end consumer devices but incurs high computational costs. In this paper, a design space exploration of hardware architectures for content based music classification is performed in order to support the design of static and mobile consumer devices.	design space exploration;mobile device	Ingo Schmädecke;Holger Blume	2014	2014 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2014.6776116	embedded system;computer vision;audio signal processing;computer science;operating system;music;multimedia;mobile computing	EDA	2.14022703093739	55.253002176346335	49275
bb9b151bbda5f035a1c14c30e8da16c1fdac686c	keeping current with silicon and systems technology in the mid-90s	software tool;design process;formal specification;multimedia systems;failure analysis;embedded systems;integrated circuit design;formal verification;software requirements and specifications;embedded systems integrated circuit design electronic design automation formal specification formal verification multimedia systems failure analysis;silicon testing springs radiofrequency identification manufacturing built in self test microelectronics intellectual property business personal digital assistants;electronic design automation	"""IEEE Design & Test Magazine was originally conceived in late 1982 to cover the nascent field of electronic design automation (EDA). Software tools to automate many parts of the IC design process were becoming essential to support the rapid growth of IC densities and to leverage IC capabilities. The magazine was one of the first nontrade engineering publications dedicated to educating professional engineers about state-of-the-art practices in silicon and systems technology. More than a decade later, its mission statement had evolved to """"Make IEEE Design & Test the premiere professional magazine reporting on both innovative recent practice and promising future methods in design and test of electronics and electronics-based systems""""."""	electronic design automation;formal specification;integrated circuit design	Kenneth D. Wagner	2005	IEEE Design & Test of Computers	10.1109/MDT.2005.18	embedded system;failure analysis;electronic engineering;formal methods;design process;electronic design automation;formal verification;computer science;systems engineering;engineering;design flow;electrical engineering;computer-automated design;operating system;software engineering;formal specification;design for testing;programming language;functional verification;computer engineering;integrated circuit design;systems design	EDA	9.469722692186348	54.34724121501839	49404
ff906160a59f427737aeef17c2411a118d353718	stam: system level state-machine-based thermal behavior analysis for multicore processor	thermal analysis;system level design;state based modeling		finite-state machine;multi-core processor	Soongyu Kwon;Jong Kang Park;Jong Tae Kim	2014	IEICE Electronic Express	10.1587/elex.11.20140798	electronic engineering;real-time computing;simulation;computer science;electronic system-level design and verification;thermal analysis	EDA	4.964115783892669	50.77722505958388	49429
ba995a8c97ce53f9f873276bef09f2fbc5044d5d	identifying efficient combinations of error detection mechanisms based on results of fault injection experiments	raw readout data performance ratings error detection mechanisms fault injection experiments;fault diagnosis fault detection;performance evaluation;fault tolerant computing;performance evaluation fault tolerant computing error detection;optimization of fault tolerance;coverage estimation;error detection;fault injection	We introduce novel performance ratings for error detection mechanisms. Given a proper setup of the fault injection experiments, these ratings can be directly computed from raw readout data. They allow the evaluation of the overall performance of arbitrary combinations of mechanisms without the need for further experiments. With this means we can determine a minimal subset of mechanisms that still provides the required performance.	error detection and correction;experiment;fault injection	Andreas Steininger;Christoph Scherrer	2002	IEEE Trans. Computers	10.1109/12.980011	embedded system;real-time computing;error detection and correction;fault coverage;computer science;stuck-at fault;statistics	Embedded	7.617414552245232	58.561043773329835	49470
ba1625ea74a3a75e9b559cd14157aa061624c44a	superoptimizing memory subsystems for multiple objectives		We consider the automatic determination of application-specific memory subsystems via superoptimization, with the goals of reducing memory access time and of minimizing writes. The latter goal is of concern for memories with limited write endurance. Our subsystems outperform general-purpose memory subsystems in terms of performance, number of writes, or both.	access time;benchmark (computing);cas latency;cpu cache;general-purpose markup language;mathematical optimization;superoptimization	Joseph G. Wingbermuehle;Ron Cytron;Roger D. Chamberlain	2015		10.1007/978-3-319-27308-2_29	access time;design space exploration;superoptimization;computer science;distributed computing	Arch	-3.217874985466435	52.76931497260621	49477
6fd344c359faa7fcd15d1adf76da58ce328a4b45	practical performance prediction under dynamic voltage frequency scaling	dynamic voltage frequency scaling;leading loads cycles;performance evaluation;clocks;linear regression model;radiation detectors;large scale dvfs runtime algorithm;time frequency analysis benchmark testing clocks load modeling predictive models computational modeling radiation detectors;interval analysis model;spec cpu 2006 benchmarks dynamic voltage frequency scaling performance prediction linear regression model large scale dvfs runtime algorithm processor independent analytic framework performance counter interval analysis model leading loads architectural model leading loads cycles nas parallel benchmarks;power aware computing;computational modeling;leading loads architectural model;performance prediction;predictive models;regression analysis;processor independent analytic framework;performance counter;nas parallel benchmarks;load modeling;time frequency analysis;benchmark testing;regression analysis benchmark testing performance evaluation power aware computing;spec cpu 2006 benchmarks	Predicting performance under Dynamic Voltage Frequency Scaling (DVFS) remains an open problem. Current best practice explores available performance counters to serve as input to linear regression models that predict performance. However, the inaccuracies of these models require that large-scale DVFS runtime algorithms predict performance conservatively in order to avoid significant consequences of mispredictions. Recent theoretical work based on interval analysis advocates a more accurate and reliable solution based on a single new performance counter, Leading Loads. In this paper, we evaluate a processor-independent analytic framework for existing performance counters based on this interval analysis model. We begin with an analysis of the counters used in many published models. We then briefly describe the Leading Loads architectural model and describe how we can use Leading Loads Cycles to predict performance under DVFS. We validate this approach for the NAS Parallel Benchmarks and SPEC CPU 2006 benchmarks, demonstrating an order of magnitude improvement in both error and standard deviation compared to the best existing approaches.	algorithm;best practice;bus contention;central processing unit;clock rate;critical path method;dynamic voltage scaling;frequency scaling;hardware performance counter;high- and low-level;interval arithmetic;kerrison predictor;multi-core processor;nas parallel benchmarks;nehalem (microarchitecture);overclocking;performance prediction;runtime system;sandy bridge;scheduling (computing)	Barry Rountree;David K. Lowenthal;Martin Schulz;Bronis R. de Supinski	2011	2011 International Green Computing Conference and Workshops	10.1109/IGCC.2011.6008553	parallel computing;real-time computing;simulation;computer science	HPC	-4.2899178346396765	54.77354474134848	49683
d71011bd3ac93a8ea1beb569dd44ec744d9db461	demonstrating hw–sw transient error mitigation on the single-chip cloud computer data plane	kernel;reliability;transient errors dynamic frequency scaling dfs joint photographic experts group jpeg format motion jpeg mjpeg single chip cloud computer scc;decoding;transform coding;transient analysis;application timing constraints hw sw transient error mitigation single chip cloud computer data plane low level cache memories hybrid hardware software scheme video decoding many core chip intel labs on chip memories error detection dynamic frequency scaling;video coding cache storage cloud computing error detection microprocessor chips;tiles;transient analysis cache storage decoding transform coding error analysis;timing	Transient errors are a major concern for the correct operation of low-level cache memories. Aggressive integration requires effective mitigation of such errors, without extreme overheads in power, timing, or silicon area. We demonstrate a hybrid (hardware-software) scheme that mitigates bit flips in data that reside in low-level caches. The methodology is shown to be applicable in streaming applications and we illustrate that with a video decoding case study on a state-of-the-art many-core chip. The single-chip cloud computer is an experimental processor created by Intel Labs. Dedicated on-chip memories are utilized to keep safe copies for key application data, thus allowing rollbacks upon error detection. The experimental results illustrate the tradeoff between application delay, consumed energy, and output fidelity as the injected errors are corrected. When output fidelity is considered as a hard constraint, application slack used for mitigation can be reclaimed with dynamic frequency scaling. Output fidelity is guaranteed regardless of the error injection intensity and the application's timing constraints are respected up to a certain upper bound of error injection.	cpu cache;cloud computing;constrained optimization;data (computing);dynamic frequency scaling;error detection and correction;forwarding plane;high- and low-level;image scaling;manycore processor;shattered world;single-chip cloud computer;slack variable;streaming media;video decoder	Dimitrios Rodopoulos;Antonis Papanikolaou;Francky Catthoor;Dimitrios Soudris	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2309663	embedded system;electronic engineering;parallel computing;kernel;real-time computing;transform coding;computer hardware;telecommunications;computer science;operating system;reliability;statistics;computer network	EDA	-2.672554465799208	56.6650202219289	49815
a0e2c1ac25d9439401f18814f66cb1f74c10cdd5	time-constrained code compaction for dsps	parallelisme;traitement signal;digital signal processors;concepcion asistida;digital signal processing;compilacion;computer aided design;optimisation;synthese circuit;concepcion circuito;design automation;resource constraint;high level languages;programacion entera;real time constraint;optimizacion;embedded dsp s;very large scale integration;integer programming model;generation code;circuit design;generacion codigo;code generation;indexing terms;programmation en nombres entiers;retargetable compilation;compaction digital signal processing hardware design automation high level languages digital signal processors encoding very large scale integration embedded software optimizing compilers;algorithm;parallelism;paralelismo;integer programming;compaction;signal processing;conception assistee;digital signal processor;compilation;sintesis circuito;circuit layout cad;digital signal processing chips;optimization;code compaction;conception circuit;procesador;integer programming model retargetable compilation embedded dsp instruction level parallelism code generation digital signal processor algorithm real time constraint local code compaction;processeur;instruction level parallelism;optimizing compilers;encoding;procesamiento senal;embedded dsp;processor;local code compaction;circuit synthesis;embedded software;circuit layout cad digital signal processing chips integer programming real time systems;hardware;real time systems	This paper addresses instruction-level parallelism in code generation for digital signal processors (DSPs). In the presence of potential parallelism, the task of code generation includes code compaction, which parallelizes primitive processor operations under given dependency and resource constraints. Furthermore, DSP algorithms in most cases are required to guarantee real-time response. Since the exact execution speed of a DSP program is only known after compaction, real-time constraints should be taken into account during the compaction phase. While previous DSP code generators rely on rigid heuristics for compaction, we propose a novel approach to exact local code compaction based on an integer programming (IP) model, which handles time constraints. Due to a general problem formulation, the IP model also captures encoding restrictions and handles instructions having alternative encodings and side effects and therefore applies to a large class of instruction formats. Capabilities and limitations of our approach are discussed for different DSPs.	data compaction;digital signal processor	Rainer Leupers;Peter Marwedel	1997	IEEE Trans. VLSI Syst.	10.1109/92.555991	embedded system;digital signal processor;computer architecture;electronic engineering;parallel computing;real-time computing;integer programming;computer science;electrical engineering;operating system;computer aided design;signal processing;programming language;algorithm	Arch	0.37106381925322734	52.153322509190865	49955
c33ef37214f13547deb14d7ca7e3c77e49b2b3d0	a scheduling method by stepwise expansion in high-level synthesis	high-level synthesis;stepwise expansion;scheduling method;linear programming;integer programming;functional unit;scheduling problem;scheduling;high level synthesis	This paper proposes a fast heuristic method for the scheduling problem ,minimizing hardware costs of functional units, registers, and busses on the basis of an integer linear programming (ILP) model. In our method, the total computation time can be much reduced compared to the general ILP method, since we r-educe the number of the integer variables which appear in the ILP formulation by introducing a stepwise expansion approach. Results obtained for a practical scheduling problem indicate that the computation time of the proposed method ia linear to the number of the control steps, and we can find optimal or near-optimal solutions.	computation;heuristic;high- and low-level;high-level synthesis;integer programming;linear programming;scheduling (computing);stepwise regression;time complexity	Hironori Komi;Shoichiro Yamada;Kunio Fukunaga	1992		10.1145/304032.304100	fair-share scheduling;mathematical optimization;discrete mathematics;integer programming;linear-fractional programming;dynamic priority scheduling;computer science;linear programming;branch and price;theoretical computer science;mathematics;high-level synthesis;scheduling;branch and cut	EDA	0.37461052518294835	52.810483679873315	50076
cfa6bc981cff6bca60c7dba81242745fb5d7ab7e	multiple clock cycle real-time implementation of a system for time-frequency analysis	convolution;signal synthesis;time-frequency analysis;s-method;spec;spectrogram;tf distributions;tf signal analysis;tfd execution;wd;convolution window widths;flexible system;hardware complexity;multicycle design;multiple clock cycle real-time implementation;pseudo wigner distribution;time-frequency signal analysis	Multiple clock cycle implementation of a flexible system for time-frequency (TF) signal analysis is presented. It allows TF distributions (TFDs) to take different numbers of clock cycles and to share functional units within a TFD execution. These abilities represent the major advantages of multicycle design and they help reduce both hardware complexity and cost. The designed hardware is suitable for a wide range of applications, because it allows sharing in realization of some frequently used TFDs: Spectrogram (SPEC), S-method (SM) for various convolution window widths, and pseudo Wigner distribution (WD), as well as for the realization of the higher order TFDs.	algorithm;clock signal;convolution;execution unit;frequency analysis;real-time clock;signal processing;spectrogram;time–frequency analysis;wigner quasiprobability distribution	Veselin N. Ivanovic;Ljubisa Stankovic	2004	2004 12th European Signal Processing Conference		embedded system;real-time computing;computer hardware;computer science	EDA	4.715916980456843	46.79264622371594	50259
5d25559733c79712c234ab121d9b636ca112a759	analog boundary-scan description language (absdl) for mixed-signal board test	working group;ieee standards;automatic test pattern generation;boundary scan testing;integrated circuit interconnections;mixed analogue digital integrated circuits automatic test pattern generation boundary scan testing ieee standards integrated circuit interconnections;circuit testing pins automatic testing integrated circuit interconnections switches electronic equipment testing measurement standards graphics automation lan interconnection;mixed analogue digital integrated circuits;test generation;interconnect test analog boundary scan description language ieee standard 1149 4 mixed signal device ieee std 1149 1 bsdl automatic test generation	The IEEE Standard 1149.4 has been ratified and available for some time, now. However, describing the architectural content of an analog boundary-scan register has not yet been standardized. The work of the IEEE Std 1149.4 Working Group over the past several years has been to define and codify an extension to IEEE Std 1149.1 's BSDL. In this paper, a language to describe the boundary-scan implementation in a mixed-signal device is proposed. The language is called analog boundary-scan description language (ABSDL) and it is compatible with the existing IEEE Std 1149.1 BSDL. Using this language, test generation automation can proceed, and interconnect test on a mixed-signal board using analog boundary-scan cells can be performed to test both simple wires and discrete components between packaged devices with IEEE Std 1149.1 and 1149.4 infrastructure and access. Although not yet promulgated, the syntax definitions proposed in this paper are indicative of the current state of the Working Group's attempts at this effort. Along with the generation of the syntax for describing IEEE Std 1149.4 structures, the semantic checks for such an ABSDL file are presented	boundary scan description language;electronic component;jtag;mixed-signal integrated circuit	Bambang Suparjo;Adam W. Ley;Adam Cron;Heiko Ehrenberg	2006	2006 IEEE International Test Conference	10.1109/TEST.2006.297708	embedded system;electronic engineering;joint test action group;working group;boundary scan;vhdl;computer science;electrical engineering;automatic test pattern generation;operating system;boundary scan description language;ieee floating point;computer engineering	Visualization	9.878238060627707	53.500242781302646	50591
8b2ed7dbc26ec3361d77fe9d6cee36400260810c	system level adaptive framework for power and performance scaling on intel® pxa27x processor	adaptive systems frequency energy management power system management dynamic voltage scaling microarchitecture energy consumption application software operating systems technology management;voltage control;frequency control;adaptive control;optimal policy;phone idling system level adaptive framework power scaling performance scaling pxa27x processor mobile phones pda dynamic voltage management dynamic frequency management software driven adaptive power management methods executing workload dynamic characterization mp3 playback memory data transfer;power management;next generation;mobile handsets;frequency control adaptive control power control microcomputers mobile handsets notebook computers voltage control;notebook computers;microcomputers;data transfer;power control	Next generation phone and PDAs face stringent power and performance requirements. In order to take advantage of dynamic voltage and frequency management, software driven adaptive power management methods are emerging as the key to performance and power scaling. This paper demonstrates an adaptive power management framework for Intel XScale/spl trade/ microarchitecture based platforms, which dynamically characterizes executing workloads based on system level events and adapts frequency and voltage in order to save power. In this paper we discuss the overall framework and analysis behind the optimal policy to adapt processor frequency and voltage. The paper also illustrated benefits of using this framework for MP3 playback, memory data transfer, phone idling etc. real life case studies.	image scaling;xscale	Priya Vaidya;Moinul H. Khan;Bryan Morgan;Prem Sakarda	2005		10.1109/ICASSP.2005.1416389	embedded system;frequency scaling;real-time computing;adaptive control;computer hardware;telecommunications;power control;computer science;automatic frequency control;microcomputer	Arch	-4.262807309863725	57.19646644341653	50628
4987844ef4c2b105cd1428dc706a88c7a433a989	towards effective portability of packet handling applications across heterogeneous hardware platforms		This paper presents the Network Virtual Machine (NetVM), a virtual network processor optimized for implementation and execution of packet handling applications. As a Java Virtual Machine virtu alizes a CPU, the NetVM virtualizes a network processor. The NetVM is expec ted to provide a unified layer for networking tasks (e.g., packet filtering, packet counting, string matching) performed by various network applications (fir ewalls, network monitors, intrusion detectors) so that they can be executed o n any network device, ranging from high-end routers to small appliances. More over, the NetVM will provide efficient mapping of the elementary functional ities used to realize the above mentioned networking tasks onto specific hard w e functional units (e.g., ASICs, FPGAs, and network processing elements) incl uded in special purpose hardware systems possibly deployed to implement net work devices.	ahead-of-time compilation;application-specific integrated circuit;central processing unit;compiler;discrepancy function;field-programmable gate array;firewall (computing);handy board;heterogeneous computing;high-level programming language;integrated development environment;java virtual machine;just-in-time compilation;machine code;network packet;network processor;networking hardware;performance evaluation;reference architecture;reversing: secrets of reverse engineering;sensor;software deployment;software portability;stemming;string searching algorithm	Mario Baldi;Fulvio Risso	2005		10.1007/978-3-642-00972-3_3	embedded system;parallel computing;real-time computing;computer science;network processor	Networks	3.1061641870149113	48.750539802085	51135
5d65b0140435cfc40d71c16ee88f35ae781fd52d	integrated cpu cache power management in multiple clock domain processors	energy efficient;dynamic voltage scaling;chip;power management;multiple clock domain;energy delay product;embedded processor;clock skew;energy saving;energy management	Multiple clock domain (MCD) chip design addresses the problem of increasing clock skew in different chip units. Importantly, MCD design offers an opportunity for fine grain power/energy management of the components in each clock domain with dynamic voltage scaling (DVS). In this paper, we propose and evaluate a novel integrated DVS approach to synergistically manage the energy of chip components in different clock domains. We focus on embedded processors where core and L2 cache domains are the major energy consumers. We propose a policy that adapts clock speed and voltage in both domains based on each domain’s workload and the workload experienced by the other domain. In our approach, the DVS policy detects and accounts for the effect of inter-domain interactions. Based on the interaction between the two domains, we select an appropriate clock speed and voltage that optimizes the energy of the entire chip. For the Mibench benchmarks, our policy achieves an average improvement over nopower-management of 15.5% in energy-delay product and 19% in energy savings. In comparison to a traditional DVS policy for MCD design that manages domains independently, our policy achieves an 3.5% average improvement in energy-delay and 4% less energy, with a negligible 1% decrease in performance. We also show that an integrated DVS policy for MCD design with two domains is more energy efficient for simple embedded processors than high-end ones.	cpu cache;central processing unit;clock rate;clock signal;clock skew;dynamic voltage scaling;embedded system;image scaling;inter-domain;interaction;magnetic circular dichroism;power management;synergy	Nevine AbouGhazaleh;Bruce R. Childers;Daniel Mossé;Rami G. Melhem	2008		10.1007/978-3-540-77560-7_15	chip;embedded system;parallel computing;real-time computing;telecommunications;clock skew;computer science;operating system;efficient energy use;clock gating;digital clock manager;cpu multiplier;energy management	EDA	-4.533396033925215	55.7312396554011	51141
5731845dcc93aa42dd734cf00c10e1f130a3a950	rtl ip abstraction into optimized embedded software	embedded systems;system on chip embedded systems multiprocessing systems scheduling;system on chip;scheduling;register transfer level rtl ip abstraction optimized embedded software mpsoc multiprocessing system on chip parallelism level general purpose processors data processing entities dpe time to market requirements error risk reduction rough abstraction techniques software code hardware communication protocols data types protocol refinement data redefinitions software performance industrial designs;multiprocessing systems	Modern SoCs gain a high level of parallelism by using both general purpose processors and a number of data processing entities (DPE), dedicated to certain heavy functionalities. As a consequence, most systems devote DPEs to executing functions with high performance rather than using dedicated hardware. Reusing already existing and pre-verified IPs through abstraction methodologies is a key idea to meet time-to-market requirements and to reduce the error risk. Rough abstraction techniques lead to non efficient software code, that results in being very limited by hardware communication protocols and data types. This paper proposes an abstraction methodology that produces optimized code. Protocol refinement and data redefinitions are exploited to increase software performance. The effectiveness of the methodology has been proven by applying it to industrial designs.	central processing unit;definition;embedded software;embedded system;entity;high-level programming language;parallel computing;refinement (computing);requirement;software performance testing;verification and validation	Nicola Bombieri;Diego Forrini;Franco Fummi;Matteo Laurenzi;Sara Vinco	2013	East-West Design & Test Symposium (EWDTS 2013)	10.1109/EWDTS.2013.6673144	computer architecture;parallel computing;real-time computing;computer science	EDA	1.463118204302885	50.67402863122805	51146
313d3c02107425ac3c76598d40e2b1c0d071cbc5	dynamically reconfigurable on-chip communication architectures for multi use-case chip multiprocessor applications	multiple use case;traditional on-chip communication architecture;multiple use-case bandwidth;low power dissipation;lower power dissipation;design technique;on-chip communication architecture;low cost;lower cost;dynamically reconfigurable on-chip communication;design decision;multi use-case chip multiprocessor;optimal design;system on a chip;use case;switches;chip;integrated circuit design;power dissipation;bandwidth;layout	The phenomenon of digital convergence and increasing application complexity today is motivating the design of chip multiprocessor (CMP) applications with multiple use cases. Most traditional on-chip communication architecture design techniques perform synthesis and optimization only for a single use-case, which may lead to sub-optimal design decisions for multi-use case applications. In this paper we present a framework to generate a dynamically reconfigurable crossbar-based on-chip communication architecture that can support multiple use-case bandwidth and latency constraints. Our framework generates on-chip communication architectures with a low cost, low power dissipation, and with minimal reconfiguration overhead. Results of applying our framework on several networking CMP applications show that our approach is able to generate a crossbar solution with significantly lower cost (2.4x to 3.8x), and lower power dissipation (1.5x to 3.1x), compared to the best previously proposed approach.	cpu power dissipation;crossbar switch;cuecat;mathematical optimization;multi-core processor;multiprocessing;optimal design;overhead (computing);reconfigurability;reconfigurable computing	Sudeep Pasricha;Nikil D. Dutt;Fadi J. Kurdahi	2009	2009 Asia and South Pacific Design Automation Conference		chip;use case;system on a chip;layout;embedded system;electronic engineering;real-time computing;telecommunications;network switch;computer science;engineering;optimal design;dissipation;bandwidth;statistics;integrated circuit design	EDA	2.88712919121496	59.50576627944741	51449
400f112cc8405497f6032b04adb49fe179d8f70a	a low power synthesis flow for multi-rate systems		In this paper we develop a synthesis flow for multi-rate systems modelled by SDF graphs with the objective of minimizing power consumption while satisfying the given throughput constraint and using as few asynchronous FIFOs as possible. A novel hybrid synchronous/asynchronous buffering mechanism to optimize computation power using self-timed scheduling and Globally Asynchronous Locally Synchronous (GALS) architecture is proposed. This hybrid buffering mechanism employs a just-enough size of buffers for data synchronization in the computational components and then inserts the minimal size of asynchronous FIFOs for the Clock-Domain-Crossing (CDC) communication. Experimental results on a JPEG encoder show that 82.7% power reduction is achieved compared to the single clock domain design, and 53.9% power reduction compared to the generic GALS design without the proposed hybrid buffering mechanism.	canonical account;clock signal;computation;data synchronization;encoder;globally asynchronous locally synchronous;jpeg;scheduling (computing);throughput	Hsin-Pang Kuo;Alan P. Su;Kuen-Jong Lee	2017	2017 International Symposium on VLSI Design, Automation and Test (VLSI-DAT)	10.1109/VLSI-DAT.2017.7939677	real-time computing;architecture;electronic engineering;encoder;computer science;throughput;parallel computing;asynchronous communication;scheduling (computing);globally asynchronous locally synchronous;data synchronization;jpeg	EDA	5.7794859174773645	51.69879458356138	51806
69ed6239e01d65784c8c366b1aca5177081fdb1c	increasing pipelined ip core utilization in process networks using exploration	internet protocol;field programmable gate array;diseno circuito;protocolo internet;data path;reconfigurable architectures;circuit design;protocole internet;process network;red puerta programable;camino datos;reseau porte programmable;qr algorithm;chemin donnees;procesador oleoducto;conception circuit;processeur pipeline;architecture reconfigurable;hardware implementation;pipeline processor	At Leiden Embedded Research Center, we are building a tool chain called Compaan/Laura that allows us to do fast mapping of applications written in Matlab onto reconfigurable platforms, such as FPGAs, using IP cores to implement the data-path of the applications. A particular characteristic of the derived networks is the existence of selfloops. These selfloops have a large impact on the utilization of IP cores in the final hardware implementation of a Process Network (PN), especially if the IP cores are deeply pipelined. In this paper, we present an exploration methodology that uses feedback provided by the Laura tool to increase the utilization of IP cores embedded in our PN. Using this exploration, we go from 60MFlops to 1,7GFlops for the QR algorithm using the same number of resources except for memory.	compile time;compiler;embedded system;feedback;field-programmable gate array;iteration;matlab;paging;pipeline (computing);profiling (computer programming);qr algorithm;semiconductor intellectual property core;throughput;toolchain	Claudiu Zissulescu;Bart Kienhuis;Ed F. Deprettere	2004		10.1007/978-3-540-30117-2_70	internet protocol;embedded system;parallel computing;real-time computing;computer science;operating system;circuit design;field-programmable gate array	EDA	4.776204120443361	49.58549459553529	51808
bd550c8a94e6155c19ca68467c77429d5fc5f7c7	code compression as a variable in hardware/software co-design	cost based code motion;optimisation;instruction cache;bit level;hardware software codesign;hardware software co design;constraint optimization;application software;response time;software engineering;design optimization;embedded system;computer architecture;embedded systems;markov model;permission;levels of abstraction;post cache architecture;hardware computer architecture embedded system permission application software national electric code constraint optimization design optimization design methodology power system modeling;software engineering embedded systems optimisation hardware software codesign;national electric code;optimization;power system modeling;post cache architecture hardware software co design code compression bit level;code compression;hardware;design methodology	We present a new way to practice and view handware/software co-design: rather than raising the level of abstraction in order to exploit the highest possible degree of optimization, we use code compression i.e. we practice co-design at the bit-level. Through our novel architecture combined with our compression methodology this results in optimization of all major design goals/constraints. In particular, we present a compression methodology that deploys what we call a “post-cache architecture” (i.e. the detached decompression unit is located between the CPU and the instruction cache). We present a design methodology that allows the designer to control parameters like speed, power, and area through the choice of compression parameters. In addition we show that our compression methodology (using a Markov Model) is more efficient than the widely used Huffman compression scheme.	bit-level parallelism;cpu cache;central processing unit;data compression;huffman coding;markov chain;markov model;mathematical optimization	Haris Lekatsas;Jörg Henkel;Wayne H. Wolf	2000		10.1145/334012.334035	computer architecture;parallel computing;real-time computing;computer science;lossless compression	Arch	1.0169579665054325	53.88846379583471	51865
0435ebc799e5c2508c38ee9917a981890e6ab64f	link-time optimization of arm binaries	lenguaje programacion;compilacion;lenguaje ensamblador;execution time;programming language;program counter;real time;performance;compactacion;interface design;compactage;machine risc technologie avancee;technology and engineering;langage assembleur;compaction;energy consumption;linker;timing optimization;link time optimization;consommation energie;langage programmation;compilation;code size;temps execution;separate compilation;optimization;assembler;production cost;power consumption;tiempo ejecucion;consumo energia	The overhead in terms of code size, power consumption and execution time caused by the use of precompiled libraries and separate compilation is often unacceptable in the embedded world, where real-time constraints, battery life-time and production costs are of critical importance. In this paper we present our link-time optimizer for the ARM architecture. We discuss how we can deal with the peculiarities of the ARM architecture related to its visible program counter and how the introduced overhead can be eliminated to a large extent. Our link-time optimizer is evaluated in two tool chains. In the Arm Developer Suite tool chain, average code size reductions with 14.6% are achieved, while execution time is reduced with 8.3% on average, and energy consumption with 7.3%. On binaries from the GCC tool chain the average code size reduction is 16.6%, execution time is reduced with 12.3% and the energy consumption with 11.5% on average. Finally, we show how the incorporation of link-time optimization in tool chains may influence library interface design.	arm architecture;binary file;display resolution;embedded system;gnu compiler collection;heuristic (computer science);interprocedural optimization;library (computing);linker (computing);mathematical optimization;offset (computer science);overhead (computing);pc bruno;program counter;real-time clock;run time (program lifecycle phase);toolchain	Bruno De Bus;Bjorn De Sutter;Ludo Van Put;Dominique Chanet;Koen De Bosschere	2004		10.1145/997163.997194	program counter;compaction;embedded system;parallel computing;real-time computing;performance;computer science;interface design;operating system;programming language	EDA	-2.6666824089567163	54.32054094813188	52145
5f913178b5168e06854e869c77f1bcded73e2d76	efficient optimal design space characterization methodologies	bounding;clock length determination;design space;high level synthesis;scheduling;module selection;efficient searching;optimal design;design space exploration	One of the primary advantages of a high-level synthesis system is its ability to explore the design space. This paper presents several methodologies for design space exploration that compute all optimal tradeoff points for the combined problem of scheduling, clock-length determination, and module selection. We discuss how each methodology takes advantage of the structure within the design space itself as well as the structure of, and interactions among, each of the three subproblems. (CAD)	computer-aided design;design space exploration;high- and low-level;high-level synthesis;interaction;optimal design;scheduling (computing)	Stephen A. Blythe;Robert A. Walker	2000	ACM Trans. Design Autom. Electr. Syst.	10.1145/348019.348058	mathematical optimization;bounding overwatch;real-time computing;computer science;optimal design;theoretical computer science;operating system;mathematics;high-level synthesis;scheduling	EDA	1.3903635813722588	52.56688213985635	52157
bc1afb85e16144f47f3a5dc237594521de0602eb	atree-based topology synthesis for on-chip network	topology synthesis algorithm;hybrid network;existing noc synthesis algorithm;communication flow;on-chip network;atree-based topology synthesis methodology;synthesized topology;power-latency product;atree-based algorithm;network resource;experimental result	The Network-on-Chip (NoC) interconnect network of future multi-processor system-on-a-chip (MPSoC) needs to be efficient in terms of energy and delay. In this paper, we propose a topology synthesis algorithm based on shortest path Steiner arborescence (hereafter we call it ATree). The concept of temporal merging is applied to allow communication flows that are not temporal overlapping to share the same network resource. For scalability and power minimization, we build a hybrid network which consists of routers and buses. We evaluate our ATree-based topology synthesis methodology by applying it to several benchmarks and comparing the results with some existing NoC synthesis algorithms [1], [2]. The experimental results show a significant reduction in the power-latency product. The power-latency product of the synthesized topology using our ATree-based algorithm is 47% and 51% lower than [1], and 10% and 17% lower than [2] for the case without considering bus and the case with bus, respectively.	algorithm;bus (computing);mpsoc;multiprocessing;network on a chip;router (computing);scalability;shortest path problem;steiner tree problem;system on a chip	Jason Cong;Yuhui Huang;Bo Yuan	2011	2011 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)			EDA	2.272072311183943	60.4372022764327	52158
4c73aaf0a836e9dec29d8b8b1a5ddb7a8745475c	high-level system synthesis and optimization of dataflow programs for mpsocs	hardware design languages;software;complexity theory;computer architecture;datavetenskap datalogi;optimization;parallel processing;hardware	The growing complexity of digital signal processing applications make a compelling case the use of high-level design and synthesis methodologies for the implementation on reconfigurable and embedded devices. Past research has shown that raising the level of abstraction of design stages does not necessarily gives penalties in terms of performance or resources. Dataflow programs provide behavioral descriptions capable of expressing both sequential and parallel algorithms and enable natural design abstractions, modularity, and portability. In this paper, a tool implementing dataflow programs onto embedded heterogeneous platforms by means of high-level synthesis, software synthesis and interface synthesis is presented for MPSoCs platfroms.	dataflow programming;digital signal processing;embedded system;high- and low-level;high-level synthesis;level design;mathematical optimization;parallel algorithm;software portability	Endri Bezati;Simone Casale Brunet;Marco Mattavelli;Jörn W. Janneck	2016	2016 50th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2016.7869072	computer architecture;parallel computing;real-time computing;computer science	EDA	1.2717727695417718	50.31400770521669	52222
31655644dbc68c2bb21f8353d26024e43f4596df	faster-than-at-speed execution of functional programs: an experimental analysis	stress;random access memory;clocks;pipelines;switches;time frequency analysis;timing	Burn-In (BI) test is usually applied in manufacturing process to screen out chip early life failures, especially for safety critical applications. Unfortunately, this test method has elevated costs for companies. In recent days, Faster-than-at-Speed-Test (FAST) has become a useful technique to discover small delay defects. At the same time, overclocking methods to enhance system performances have been studied, which focus on temperature management to preserve system functionalities. In this paper, a FAST technique is approached with the aim of intentionally provoking a thermal overheating in the microprocessor by mean of the execution of functional test programs, partly regardless of system behavior preservation. The goal is to introduce an internal stress stronger than current procedures used during BI in order to speed up early detection of latent faults. The method illustrates how to avoid blocking configurations due to timing constraints violation and leads to a significant increase of the switching activity. Experimental results on a MIPS architecture show that, by using the described technique, the processor is not falling into an unpredictable state even at frequencies up to about 20 times higher than the nominal one and the switching activity is increasing up to 300% per nanoseconds.	blocking (computing);functional testing;microprocessor;overclocking;performance	Paolo Bernardi;Alberto Bosio;Giorgio Di Natale;Andrea Guerriero;Federico Venini	2016	2016 IFIP/IEEE International Conference on Very Large Scale Integration (VLSI-SoC)	10.1109/VLSI-SoC.2016.7753581	embedded system;parallel computing;real-time computing;engineering	EDA	5.969648470526562	57.70649983944026	52232
223e92edebeb2b09091716ead9f29b20adfcf89f	analog and mixed signal modelling with systemc-ams	hardware design languages;active methods;continuous time;systemc specification design methodology;digital software;design process;hardware software codesign;executable specification;tool support;and mixed signal;mixed signal;analog environment analog modelling mixed signal modelling systemc ams digital circuit design design specification rt level design analog components complex systems systemc specification design methodology telecommunication system digital hardware digital software analog filter;signal design;object oriented modeling mathematical model context modeling computational modeling signal processing hardware design languages signal design registers signal processing algorithms design methodology;communicating sequential process;specification;design flow;filters;software systems;systemc ams;discrete time;analog environment;analog filter;communication model;circuit simulation;analogue integrated circuits;computational modeling;complex system;integrated circuit modelling;object oriented;registers;levels of abstraction;system design;signal processing;object oriented programming languages;telecommunication system;digital systems;mathematical model;mixed analogue digital integrated circuits;mixed signal modelling;complex systems;rt level design;design specification;circuit cad;analog components;model of computation;kahn process network;digital circuits;analog modelling;signal processing algorithms;system simulation;modeling;register transfer level;context modeling;digital hardware;digital circuit design;object oriented modeling;hardware software codesign integrated circuit modelling analogue integrated circuits mixed analogue digital integrated circuits circuit simulation circuit cad filters;discrete event;discrete system;systemc;design methodology	SystemC will become more and more important for the design of digital circuits from the specification down to the RT-Level. However complex systems containing also analog components. This paper introduces concepts for the extension of the SystemC methodology for the specification and design of analog and mixed signal systems. The concepts will be illustrated on a telecommunication system including digital hardand software, analog filter and an analog environment.	analog signal;analogue filter;complex systems;digital electronics;mixed-signal integrated circuit;systemc;vhdl-ams;verilog-ams	Alain Vachoux;Christoph Grimm;Karsten Einwich	2003		10.1109/ISCAS.2003.1205169	complex systems;computer architecture;electronic engineering;real-time computing;computer science;electrical engineering;object-oriented programming;statistics;field-programmable analog array	EDA	5.244026514864027	52.64610167670019	52243
8057a15e6d5a76c4a0e04e2d5265d9abf27f25da	functional verification coverage vs. physical stuck-at fault coverage	verification;signature analysis;functional property;test application time;functional verification;uio;stuck at fault coverage;functional properties;verification coverage;software systems;integrated circuit testing fault diagnosis logic testing vlsi;fsm;logic testing;integrated circuit testing;vlsi;circuit faults automatic test pattern generation costs logic testing telephony automatic testing acoustic testing circuit testing sun hardware;fault coverage;validation;design verification;test application time functional verification coverage physical stuck at fault coverage functional property model nonredundant physical stuck at faults short verification;fault diagnosis;atpg	It is shown that a functional verification coverage model based on functional property model is a super set of nonredundant physical stuck-at faults in this paper. This paper overviews a methodology to validate and verify hardware or software systems where the specification is modeled as a finite functional property model. The methodology proposed can produce a short verification/test with short verification and test application time and high design verification/physical fault coverage.	fault coverage;stuck-at fault	Xiao Sun;Carmie Hull	1998		10.1109/DFTVS.1998.732157	reliability engineering;embedded system;verification and validation of computer simulation models;real-time computing;verification;fault coverage;software verification;physical verification;engineering;automatic test pattern generation;high-level verification;runtime verification;very-large-scale integration;intelligent verification;functional verification;software system	EDA	8.269112495457021	52.715899557437	52530
1a926ee4013bb0fc77a043474e0a881a2c2ba90b	application of bus emulation techniques to the design of a pci/mc68000 bridge	personal computer;direct memory access;bus emulation;embedded system;bridge;performance improvement;industrial robots;pci	Bridges easy the interconnection and communication of devices that operate using different buses. In fact, we can see a computer as a hierarchy of buses to which devices are connected. In this paper we design a PCI/MC68000 bridge in order to improve communications between a Personal Computer and a MC68000 based system. The previous interface between both devices was based on the old 16-bit ISA bus, which represented a bottleneck in their communication. However, the methodology described here is generic and can be applied to the design of PCI bridges to other buses. We finish this work with an analysis of the bridge performance improvement which can also be easily adapted to other situations. As an example our interface is used in an interesting situation, i.e., updating the obsolete control unit of a highly valuable system (an industrial robot). q 2002 Elsevier Science B.V. All rights reserved.	16-bit;bottleneck (engineering);bus (computing);control unit;conventional pci;emulator;industrial robot;industry standard architecture;interconnection;motorola 68000;personal computer	Jose Maria Rodríguez Corral;Antonio Abad Civit Balcells;Gabriel Jiménez-Moreno;Arturo Morgado Estevez;Alejandro Linares-Barranco	2002	Microprocessors and Microsystems	10.1016/S0141-9331(02)00063-7	bus;embedded system;parallel computing;real-time computing;pci configuration space;computer hardware;m.2;pc/104;computer science;local bus;electrical engineering;operating system;conventional pci;direct memory access;system bus;bridge;computer network	EDA	6.702689427328582	48.91806831363159	52909
bfb8e3d25d508ee5d1d0636e7735095d98c75118	achieving exascale capabilities through heterogeneous computing	energy efficiency;graphics processing units random access memory bandwidth memory management energy efficiency supercomputers computer programs;random access memory;memory management;heterogeneous computing;exascale computing;heterogeneous exascale system exascale capability heterogeneous computing amd exascale computing performance capability hardware optimization energy efficiency supercomputer high end high performance computing system high volume gpu technology energy efficient data parallel computing gpu capability accelerated processing units;computer programs;graphics processing units;hardware exascale computing heterogeneous computing energy efficiency data parallel computing;data parallel computing;bandwidth;supercomputers;parallel programming energy conservation graphics processing units parallel machines;hardware	This article provides an overview of AMD's vision for exascale computing, and in particular, how heterogeneity will play a central role in realizing this vision. Exascale computing requires high levels of performance capabilities while staying within stringent power budgets. Using hardware optimized for specific functions is much more energy efficient than implementing those functions with general-purpose cores. However, there is a strong desire for supercomputer customers not to have to pay for custom components designed only for high-end high-performance computing systems. Therefore, high-volume GPU technology becomes a natural choice for energy-efficient data-parallel computing. To fully realize the GPU's capabilities, the authors envision exascale computing nodes that compose integrated CPUs and GPUs (that is, accelerated processing units), along with the hardware and software support to enable scientists to effectively run their scientific experiments on an exascale system. The authors discuss the hardware and software challenges in building a heterogeneous exascale system and describe ongoing research efforts at AMD to realize their exascale vision.	central processing unit;experiment;general-purpose markup language;graphics processing unit;heterogeneous computing;parallel computing;supercomputer	Michael J. Schulte;Mike Ignatowski;Gabriel H. Loh;Bradford M. Beckmann;William C. Brantley;Sudhanva Gurumurthi;Nuwan Jayasena;Indrani Paul;Steven K. Reinhardt;Gregory Rodgers	2015	IEEE Micro	10.1109/MM.2015.71	computer architecture;parallel computing;real-time computing;computer science;operating system;efficient energy use;bandwidth;symmetric multiprocessor system;unconventional computing;memory management	HPC	-2.9510520390180197	47.13630893771823	53131
365772a9933ae5007d7be0c3a46b986ddb4d8945	run-time monitoring mechanism for efficient design of application-specific noc architectures in multi/manycore era	multiprocessor interconnection networks;execution time run time monitoring mechanism application specific noc architecture multicore era manycore era network on chip interconnect storage buffer power hungry components system performance power consumption design decision architectural parameter software simulation theoretical modeling optimal buffers size system behavior rmm traffic flow monitoring system resources run time traffic information optimal architecture hardware parameter pnoc;system monitoring buffer storage integrated circuit design multiprocessing systems multiprocessor interconnection networks network on chip;run time monitoring;buffer design;network on chip;system monitoring;buffer storage;integrated circuit design;design method;monitoring probes network on chip hardware algorithm design and analysis computer architecture registers;multiprocessing systems;power reduction;noc;noc buffer design power reduction run time monitoring design method	"""One of the major design challenges of Network-on-Chip interconnect is the storage buffers. They occupy a significant portion of the system's area and so they are considered as main """"power-hungry"""" components. Deciding the appropriate buffers size and implementation in these systems is the key technique for increasing system performance and also for reducing overall area and power consumption. However, this goal is very hard to achieve with traditional design approaches, where design decisions of the main architectural parameters are generally made with slow and inaccurate software simulation or theoretical modeling. In order to quickly capture and decide the optimal buffers size and the whole system behavior, we propose in this work an efficient design method for Network-on-Chip architecture based on a novel run-time monitoring mechanism (RMM). The system monitors the traffic flow at different system's resources and sends the monitored run-time traffic information to a specialized controller. In addition, our proposed design method allows to easily compute optimal architecture hardware parameters (i.e Buffer size) and allocate the appropriate values on demand to satisfy the requirements of any given application. The RMM mechanism was designed in hardware and integrated into our NoC system (PNoC). From the evaluation results, we conclude that the system performance in terms of execution time was about 27% better when compared with traditional design methods over several benchmark programs."""	benchmark (computing);bitwise operation;computer simulation;encoder;field-programmable gate array;jpeg;manycore processor;multi-core processor;network on a chip;requirement;run time (program lifecycle phase)	Akram Ben Ahmed;Takayuki Ochi;Shohei Miura;Ben A. Abderazek	2013	2013 Seventh International Conference on Complex, Intelligent, and Software Intensive Systems	10.1109/CISIS.2013.80	embedded system;parallel computing;real-time computing;engineering	EDA	-0.7495097720355417	54.05503827777798	53479
9e9fc91eb74cffccfe0a63cd1510dfb7ee58ecc1	data dependent energy modeling for worst case energy consumption analysis	cs pf	Safely meeting Worst Case Energy Consumption (WCEC) criteria requires accurate energy modeling of software. We investigate the impact of instruction operand values upon energy consumption in cacheless embedded processors. Existing instruction-level energy models typically use measurements from random input data, providing estimates unsuitable for safe WCEC analysis.  We examine probabilistic energy distributions of instructions and propose a model for composing instruction sequences using distributions, enabling WCEC analysis on program basic blocks. The worst case is predicted with statistical analysis. Further, we verify that the energy of embedded benchmarks can be characterised as a distribution, and compare our proposed technique with other methods of estimating energy consumption.	analysis of algorithms;basic block;benchmark (computing);best, worst and average case;biasing;central processing unit;control flow;convolution;data dependency;dynamic logic (digital electronics);embedded system;energy modeling;input/output;operand;profiling (computer programming);randomness;run time (program lifecycle phase);software release life cycle;spatial variability	James Pallister;Steve Kerrison;Jeremy Morse;Kerstin Eder	2017		10.1145/3078659.3078666	parallel computing;operand;energy modeling;computer science;real-time computing;energy consumption;probabilistic logic;software;dataflow programming	Embedded	-2.368915932633567	55.3437839508848	53485
1bbe797ba9bf3d4bd4109680f81e5f5cf6816c11	energy-guided exploration of on-chip network design for exa-scale computing	network design;energy efficient;hierarchical networks;broadcast;chip;scratch pad memory;high performance computer;networks on chip;design space exploration;system architecture;transmission line;memory model	Designing energy-efficient systems under tight performance and energy constraints becomes increasingly challenging for exascale computing. In particular, interconnecting hundreds of cores, caches, integrated memory and I/O controllers in energy efficient way stands out as a new challenge. This paper proposes hierarchical on-chip networks that take the proximity advantage between the cores in smaller clusters as a promising approach toward energy-efficient high performance computing. The design trade-offs of hierarchical interconnect architectures are studied using a fast and scalable design space exploration tool for exascale systems with number of cores in the order of thousands. In particular, we consider a system with 720 processing nodes and two-level network hierarchy. By supporting both traditional cache-based memory model and scratch pad memory (SPM) model, the target system architecture proves to be a good testbed for energy-guided exploration of hierarchical networks.	design space exploration;exa;input/output;memory controller;network on a chip;network planning and design;scalability;super paper mario;supercomputer;systems architecture;testbed;tree network	Ümit Y. Ogras;Yunus Emre;Jianping Xu;Timothy Kam;Michael Kishinevsky	2012		10.1145/2347655.2347669	chip;memory model;embedded system;network planning and design;parallel computing;real-time computing;telecommunications;computer science;engineering;operating system;transmission line;efficient energy use;computing with memory;systems architecture	HPC	-2.8776216698155825	47.99054797448273	53633
a3311c684e7284ce9d1ad1dcf3f43b3658e595b0	a dcfl e/d-mesfet gaas experimental risc machine	32 bits processor;gallium arsenides;microprocessor;risc machine;architecture systeme;performance evaluation;concepcion sistema;32 bit;32 bit rca dcfl e d mesfet risc machine gaas microprocessor software environment instruction set architecture instruction execution sequence;instruction execution sequence;transistor effet champ barriere schottky;reduced instruction set computing;gaas microprocessor;circuit vlsi;software environment;galio;rca;instruction set architecture;procesador 32 bits;vlsi circuit;reduced instruction set computing field effect integrated circuits gallium arsenide iii v semiconductors microprocessor chips;field effect integrated circuits;gallium arsenide reduced instruction set computing microprocessors computer architecture instruction sets;metal semiconductor field effect transistor;gallium arsenide;system design;processeur 32 bits;iii v semiconductors;arquitectura sistema;microprocesseur;circuito vlsi;system architecture;microprocesador;dcfl e d mesfet;gallium arseniure;conception systeme;microprocessor chips;transistor efecto campo barrera schottky	The design of RCA's 32-bit GaAs microprocessor is described. Technology limitations and influences of the software environment are discussed. The details of the instruction set architecture (ISA) and the instruction execution sequence (IES) are described. The essence of the original contributions of the research and the design is emphasized. The simulated performance evaluation data are presented. >	deterministic context-free language	Walter A. Helbig;Veljko M. Milutinovic	1989	IEEE Trans. Computers	10.1109/12.16503	embedded system;reduced instruction set computing;gallium arsenide;parallel computing;32-bit;computer science;electrical engineering;operating system;instruction set;systems architecture;systems design	Embedded	7.4631414111290075	50.90703490556683	53731
9b96e0801213c40856f951a7dd41de005a6be759	a self-reconfiguring ieee 1687 network for fault monitoring	self reconfiguration;ieee standards error detection fault location;time analysis;fault monitoring;fault localization fault monitoring self reconfiguring ieee 1687 network error detection;kommunikationssystem;ieee 1687;monitoring instruments circuit faults registers hardware standards integrated circuits;time analysis ieee 1687 fault monitoring self reconfiguration	Efficient handling of faults during operation is highly dependent on the interval (latency) from the time embedded instruments detect errors to the time when the fault manager localizes the errors. In this paper, we propose a self-reconfiguring IEEE 1687 network in which all instruments that have detected errors are automatically included in the scan path. To enable self-reconfiguration, we propose a modified segment insertion bit (SIB) compliant to IEEE 1687. We provide time analyses on error detection and fault localization for single and multiple faults, and we suggest how the self-reconfiguring IEEE 1687 network should be designed such that time for error detection and fault localization is kept low and deterministic. For validation, we implemented and performed post-layout simulations for one self-reconfiguring network. We show that compared to previous schemes, our proposed network significantly reduces the fault localization time.		Farrokh Ghani Zadegan;Dimitar Nikolov;Erik Larsson	2016	2016 21th IEEE European Test Symposium (ETS)	10.1109/ETS.2016.7519288	embedded system;electronic engineering;real-time computing;fault coverage;fault indicator;engineering;stuck-at fault;fault management	Embedded	7.749720880821817	58.22560103539029	53742
70f91bee53b6b01524a779b5d65b284c90bcea51	characterizing the branch misprediction penalty	parallel architectures pipeline processing performance evaluation;performance evaluation;superscalar processor performance modeling branch misprediction penalty pipelined superscalar processors frontend pipeline length last miss event functional unit latency interval analysis;parallel architectures;technology and engineering;pipelines delay performance analysis impedance length measurement clocks analytical models time measurement data analysis;functional unit;pipeline processing	Despite years of study, branch mispredictions remain as a significant performance impediment in pipelined superscalar processors. In general, the branch misprediction penalty can be substantially larger than the frontend pipeline length (which is often equated with the misprediction penalty). We identify and quantify five contributors to the branch misprediction penalty: (i) the frontend pipeline length, (ii) the number of instructions since the last miss event (branch misprediction, I-cache miss, long D-cache miss)-this is related to the burstiness of miss events, (iii) the inherent ILP of the program, (iv) the functional unit latencies, and (v) the number of short (LI) D-cache misses. The characterizations done in this paper are driven by 'interval analysis', an analytical approach that models superscalar processor performance as a sequence of inter-miss intervals.	benchmark (computing);branch misprediction;branch predictor;cpu cache;central processing unit;dcache;emoticon;execution unit;ibm notes;interval arithmetic;national fund for scientific research;superscalar processor	Stijn Eyerman;James E. Smith;Lieven Eeckhout	2006	2006 IEEE International Symposium on Performance Analysis of Systems and Software	10.1109/ISPASS.2006.1620789	computer architecture;parallel computing;real-time computing;execution unit;computer science;operating system;branch misprediction	Arch	-4.215314426583805	54.590467544893585	53787
6dbdc33e215965b58aaf1054cf2a432d9cb695ed	hierarchical clustered register file organization for vliw processors	vliw processor;local first level register file;hierarchical clustering;microprocessors;cycle time;cluster selection;modulo scheduling;microprocessor designs;register allocation;modulo scheduling technique;processor scheduling;storage management;vliw processors;space exploration;mirs_hc;very long instruction word architectures hierarchical clustered register file organization vliw processors wire delays microprocessor designs local first level register file intercluster communications mirs spl i bar hc modulo scheduling technique instruction scheduling cluster selection;microprocessor chips multiprocessing systems storage management file organisation instruction sets;wire;vliw;radio frequency;registers vliw wire delay processor scheduling space technology radio frequency microprocessors space exploration proposals;registers;hierarchical clustered register file organization;very long instruction word architectures;wire delays;register file;space technology;multiprocessing systems;functional unit;intercluster communications;instruction scheduling;proposals;microprocessor chips;instruction sets;file organisation	Technology projections indicate that wire delays will become one of the biggest constraints in future microprocessor designs. To avoid long wire delays and therefore long cycle times, processor cores must be partitioned into components so that most of the communication is done locally. In this paper, we propose a novel register file organization for VLIW cores that combines clustering with a hierarchical register file organization. Functional units are organized in clusters, each one with a local first level register file. The local register files are connected to a global second level register file, which provides access to memory. All inter– cluster communications are done through the second level register file. This paper also proposes MIRS HC, a novel modulo scheduling technique that simultaneously performs instruction scheduling, cluster selection, inserts communication operations, performs register allocation and spill insertion for the proposed organization. The results show that although more cycles are required to execute applications, the execution time is reduced due to a shorter cycle time. In addition, the combination of clustering and hierarchy provides a larger design exploration space that trades-off performance and technology requirements.	cluster analysis;computer cluster;instruction scheduling;microprocessor;modulo operation;register allocation;register file;requirement;run time (program lifecycle phase);scheduling (computing)	Javier Zalamea;Josep Llosa;Eduard Ayguadé;Mateo Valero	2003		10.1109/IPDPS.2003.1213178	computer architecture;parallel computing;real-time computing;register window;control register;cycle time variation;computer science;very long instruction word;memory buffer register;space exploration;operating system;register renaming;stack register;instruction set;hierarchical clustering;space technology;open;processor register;instruction scheduling;register allocation;radio frequency;register file;memory data register;memory address register	Arch	-2.6593519144530444	52.326287742216564	53919
5e903897b0ae65ac992b3e55d421ae8c5b08c2e9	low power reconfiguration technique for coarse-grained reconfigurable architecture	processing element array;traitement pipeline;cache storage;evaluation performance;haute performance;intellectual property core;performance evaluation;systeme embarque;integrated circuit;intellectual property;application software;flexibilidad;reconfigurable architectures;configuration cache;evaluacion prestacion;reconfigurable architectures cache storage industrial property pipeline arithmetic;configuration memory unit;cache memory;circuito integrado;reusable context pipelining architecture;endommagement;deterioracion;embedded system;antememoria;miniaturisation;embedded systems;antememoire;calculateur mimd;low power;energy consumption;application specific integrated circuits;loop pipelining;multiple instruction stream;low power electronics;multiple data stream style execution model;alto rendimiento;coeur propriete intellectuelle;low power coarse grained reconfigurable architecture configuration cache embedded system loop pipelining;flexibilite;intellectual property cores;procesador;miniaturization;industrial property;power system reliability;low power reconfiguration technique;power reduction;computer science;nucleo propiedad intelectual;miniaturizacion;power consumption;damaging;consommation energie electrique;processeur;multiple data stream style execution model low power reconfiguration technique coarse grained reconfigurable architecture configuration memory unit configuration cache processing element array power consumption intellectual property cores reusable context pipelining architecture power reduction loop pipelining multiple instruction stream;reconfigurable architectures energy consumption pipeline processing application software application specific integrated circuits power system reliability embedded system hardware computer science intellectual property;electronique faible puissance;high performance;architecture reconfigurable;pipeline arithmetic;processor;circuit integre;mimd computer;coarse grained reconfigurable architecture;flexibility	Coarse-grained reconfigurable architectures (CGRAs) require many processing elements (PEs) and a configuration memory unit (configuration cache) for reconfiguration of its PE array. Although this structure is meant for high performance and flexibility, it consumes significant power. Specially, power consumption by configuration cache is explicit overhead compared to other types of intellectual property (IP) cores. Reducing power is very crucial for CGRA to be more competitive and reliable processing core in embedded systems. In this paper, we propose a reusable context pipelining (RCP) architecture to reduce power-overhead caused by reconfiguration. It shows that the power reduction can be achieved by using the characteristics of loop pipelining, which is a multiple instruction stream, multiple data stream (MIMD)-style execution model. RCP efficiently reduces power consumption in configuration cache without performance degradation. Experimental results show that the proposed approach saves much power even with reduced configuration cache size. Power reduction ratio in the configuration cache and the entire architecture are up to 86.33% and 37.19%, respectively, compared to the base architecture.	elegant degradation;embedded system;experiment;mimd;microprocessor;overhead (computing);pipeline (computing);reconfigurable computing;requirement	Yoonjin Kim;Rabi N. Mahapatra;Ilhyun Park;Kiyoung Choi	2009	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2008.2006039	embedded system;electronic engineering;application software;parallel computing;real-time computing;cpu cache;computer science;operating system;integrated circuit;miniaturization;application-specific integrated circuit;cache algorithms;intellectual property;low-power electronics	EDA	-1.6570101260819976	54.00264526589362	53995
ca3d4ae8967b9334a25e979e9218f7c65bfa4509	pros and cons of public domain vlsic design suites	verification;computer aided design;multiplier;logic level simulation;routing;placement;compaction public domain cad tool suite microelectronics design octtools oasis vlsic chip multiplier specification language logic level simulation switch level simulation placement routing verification testability;public domain software vlsi integrated circuit design circuit cad;octtools;specification language;testability;public domain;chip;public domain software;integrated circuit design;microelectronics design;compaction;design and implementation;system design;switch level simulation;vlsi;vlsic chip;oasis;design verification;circuit cad;hierarchical design;public domain cad tool suite;routing microelectronics design automation circuits libraries logic design logic testing switches computational modeling fabrication	Computer-Aided Design (CAD) tools play an essential role in modern microelectronics. A comparison (between two of the available CAD tool suites, OCTTOOLS and OASIS, is presented along with detailed description of their features and advantages. These tools utilize full-custam or the standard cells semi-custom approaches. This paper is intended to facilitate proper choice of the tools for best VLSIC designs. At the Microelectronics System Design Lab (MSDL), Oakland University, several VLSIC chips have been designed and implemented based on: the ease of transfer among the various hierarchical desiign levels of the OCTTOOLS and the eficient compaction available in OASIS. An example of a six-bit multiplier is presented to illustrate the design and implementation. The pros and limitations of each tool address: specification language, logic and switch level simulations, placement and routing, design verification and testability, as well as compaction1 .	computer-aided design;data compaction;military scenario definition language;place and route;routing;semiconductor industry;simulation;software testability;specification language	Hoda S. Abdel-Aty-Zohdy	1997		10.1109/MSE.1997.612522	chip;testability;compaction;embedded system;routing;computer architecture;electronic engineering;public domain;verification;specification language;computer science;engineering;operating system;computer aided design;very-large-scale integration;multiplier;public domain software;placement;computer engineering;integrated circuit design;systems design	EDA	9.045936319927222	51.622912050443794	54145
0a80e3dce25d865e9fdf69da4d09cc8ac3398ff4	system level analysis of fast, per-core dvfs using on-chip switching regulators	voltage control;multi threading;chip multiprocessor;voltage regulators microprocessor chips multi threading power aware computing switching functions;embedded system;chip;power aware computing;low power;digital systems;nanosecond scale voltage switching system level analysis fast per core dvfs on chip switching regulators dynamic voltage and frequency scaling chip multiprocessors multi threaded workloads voltage regulators;next generation;regulators system on a chip voltage control program processors load modeling switches inductors;voltage regulators;switching functions;voltage regulator;high performance;energy saving;dynamic voltage and frequency scaling;microprocessor chips	Portable, embedded systems place ever-increasing demands on high-performance, low-power microprocessor design. Dynamic voltage and frequency scaling (DVFS) is a well-known technique to reduce energy in digital systems, but the effectiveness of DVFS is hampered by slow voltage transitions that occur on the order of tens of microseconds. In addition, the recent trend towards chip-multiprocessors (CMP) executing multi-threaded workloads with heterogeneous behavior motivates the need for per-core DVFS control mechanisms. Voltage regulators that are integrated onto the same chip as the microprocessor core provide the benefit of both nanosecond-scale voltage switching and per-core voltage control. We show that these characteristics provide significant energy-saving opportunities compared to traditional off-chip regulators. However, the implementation of on-chip regulators presents many challenges including regulator efficiency and output voltage transient characteristics, which are significantly impacted by the system-level application of the regulator. In this paper, we describe and model these costs, and perform a comprehensive analysis of a CMP system with on-chip integrated regulators. We conclude that on-chip regulators can significantly improve DVFS effectiveness and lead to overall system energy savings in a CMP, but architects must carefully account for overheads and costs when designing next-generation DVFS systems and algorithms.	algorithm;benchmark (computing);central processing unit;control system;digital electronics;dynamic frequency scaling;dynamic voltage scaling;embedded system;image scaling;low-power broadcasting;microprocessor;multi-core processor;online and offline;overhead (computing);power domains;processor design;resonance;thread (computing);voltage regulator module	Wonyoung Kim;Meeta Sharma Gupta;Gu-Yeon Wei;David M. Brooks	2008	2008 IEEE 14th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2008.4658633	embedded system;voltage regulator;parallel computing;real-time computing;computer hardware;telecommunications;computer science	Arch	-4.487537281945661	55.74597031604877	54221
dfe584d6c5ecc9883531b58e6e7ddc32cea861b2	an interface for coupling optimization algorithms with epanet in discrete event simulation platforms		The application of simulation optimization in water distribution network analysis and design is a promising method for generating solutions to existing challenges. The absence of a standard interface for coupling the open source EPANET software package to optimization algorithms increases the implementation effort and limits the comparison of results. This work presents a methodology for implementing an interface for coupling optimization algorithms with EPANET. The proposed technique uses the internal simulation clock events in a discrete event simulation platform to co-ordinate optimization loops and data exchange. The utilization of intermediate input/output files is avoided in order to increase the simulation speed. A water distribution network implemented in the EPANET solver is considered as a discrete event to be interfaced with optimization algorithms. The interface module is implemented as a C/C++ mex-file for EPANET in the MATLAB/Simulink platform. The methodology enables the user to evaluate the fitness of the design parameters with easy access to data logging and visualization tools at run-time. The proposed technique is used to implement the particle swarm optimization algorithm (PSO) and applied to design a benchmark water distribution network.	accessibility;algorithm;benchmark (computing);c++;data logger;epanet;input/output;matlab;mex file;mathematical optimization;network planning and design;open-source software;particle swarm optimization;run time (program lifecycle phase);simulation;simulink;solver	Lawrence K. Letting;Yskandar Hamam;Adnan M. Abu-Mahfouz	2017	2017 IEEE 15th International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2017.8104951	network analysis;discrete event simulation;real-time computing;matlab;software;algorithm design;algorithm;data exchange;particle swarm optimization;solver;computer science	EDA	3.145069010836435	56.70441120854644	54412
d9d5723abab069577f492a5fb25bd5850fc70d0c	sh-5: the 64-bit superh architecture	64 bit sh 5 64 bit superh architacture superh microprocessor series simd embedded core design set top cable boxes digital tv voice over ip personal digital assistants in car information systems game machines system on chip;multimedia systems;internet telephony personal digital assistants collaboration microprocessors registers cable tv communication cables digital tv ip networks home appliances;embedded systems;multimedia systems microprocessor chips embedded systems;microprocessor chips	A collaborative effort of Hitachi and STMicroelectronics, the SH-5 is the latest member of the SuperH microprocessor series. Its CPU core is the first implementation of a new instruction set architecture consisting of 32-bit instructions, 64-bit registers, SIMD (single-instruction, multiple-data) instructions for multimedia applications, and a compatibility mode supporting the 16-bit SuperH instruction set. Embodying an emerging philosophy of embedded-core design, the SH-5 provides a platform for a wide range of applications: set top cable boxes, digital TV, voice over IP (Internet telephony), network processing, PDAs (personal digital assistants), Internet appliances, in-car information systems, game machines, and so on. A single cost-effective, optimum design that will cater to the requirements of such a wide range of applications is not feasible. So the SH-5 core supports a carefully selected set of functions critical to meeting the performance, power, and code-size requirements of these applications. At the same time, it: provides features that ease integration into a system on chip (SOC) that uses application-specific hardware modules to cater to specific requirements.	superh	Prasenjit Biswas;Atsushi Hasegawa;Srinivas Mandaville;Mark Debbage;Andy Sturges;Fumio Arakawa;Yasuhiko Saito;Kunio Uchiyama	2000	IEEE Micro	10.1109/40.865864	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system	Arch	6.150347356773609	47.999446413422795	54471
1d7d523f6b05cb6fd2748f7d12938b7705d7c202	a rapid prototyping method to reduce the design time in commercial high-level synthesis tools	databases;software;hardware optimization databases field programmable gate arrays software c languages algorithm design and analysis;hls;eda high level synthesis hls rapid prototyping esl;rtl description rapid prototyping method design time reduction commercial high level synthesis tools hls tools abstraction level hardware design process high level programming languages domain specific languages graphs ansi c language c source code database creation optimization algorithm;source code software c language high level synthesis optimisation software prototyping;eda;high level synthesis;rapid prototyping;c languages;optimization;field programmable gate arrays;esl;algorithm design and analysis;hardware	High-Level Synthesis (HLS) tools have been developed to increase the abstraction level of hardware design process, by using models like high-level programming languages (e.g. C/C++), Domain Specific Languages and Graphs. However, despite their advances in the last decade, the available HLS tools still require from the designer a broad hardware knowledge, which prevents a bigger reduction in the design time. In this work, we propose a method to be used at the top of current high-level synthesis tools, allowing for a speed-up in the development process. The method starts with the application description in a subset of the ANSI-C language. Then we generate a Graph from the C source code. There is a finite number of possible nodes, and this fact allows the creation of a database of alternative hardware models for each possible node type. A simple optimization algorithm selects the combination of nodes which best fits under the constraints (power consumption, resource use, speed). If a node is not in the database or the constraints were not met, then the designer can use any commercial high-level synthesis tool (or a direct RTL description), to create a new hardware model and include it in the database. Some test cases were implemented using both the proposed methodology and a commercial HLS tool. The results obtained indicate that the method can reduce the design time while still providing fair results when compared to the commercial tool.	ansi c;abstraction layer;algorithm;benchmark (computing);c object processor;compatibility of c and c++;computer architecture;database;domain-specific language;fits;high- and low-level;high-level programming language;high-level synthesis;llvm;mathematical optimization;rapid prototyping;register-transfer level;static program analysis;test case	Jones Yudi Mori;André Werner;Florian Fricke;Michael Hübner	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.56	algorithm design;computer architecture;parallel computing;real-time computing;electronic design automation;computer science;operating system;high-level synthesis;programming language;field-programmable gate array	EDA	2.4478895069743163	51.55424665721767	54483
f25118e33ddeb1c7e8662ed2446aa844a14a61f7	design of a real-time co-operating system for multiprocessor workstations	personal computer;real time;real time systems workstations process control coprocessors software architecture hardware computer architecture microcomputers software prototyping sun;software engineering;hardware architecture;software architecture;operating system;systems analysis;signal processing;workstations;process control;operating systems computers real time systems workstations multiprocessing systems software engineering systems analysis;multiprocessing systems;real time application;operating systems computers;non real time;manufacturing real time co operating system design multiprocessor workstations non real time activities real time activities software architecture software prototype solaris 2 4 four processor sun sparcstation 20 real time applications sensor based control process control signal processing multimedia;real time systems	We have designed a Real-Time Co-operating System (RTCOS) for simultaneously supporting real-time and non-real-time activities on a workstation with two or more processors. The RTCOS is the software equivalent of a coprocessor, with a software architecture analogous to the hardware architecture that hlas been used in many workstations and personal computer& In this paper, we discuss our jirst software prototype of the RTCOS, which co-exists with Solaris 2.4 on a four-processor Sun SPARCstation 20. We summarize the feasibility of our approach through an experimental characterization of Solaris 2.4. We address the technical issues involved and present the details of our design. The RTCOS is targeted towards real-time applications in the sensor-based control, process control, signal processing, multimedia, and manufacturing domains.	central processing unit;coprocessor;information system;multiprocessing;personal computer;prototype;real-time clock;real-time data;real-time locating system;real-time transcription;signal processing;software architecture;software prototyping;unix signal;workstation	Gebran Krikor;Md. Touhidur Raza;David B. Stewart	1996		10.1109/HICSS.1996.495499	embedded system;software architecture;systems analysis;computer architecture;real-time computing;workstation;computer science;operating system;software engineering;signal processing;process control;hardware architecture	Embedded	4.451247077249269	48.037496068912205	54508
1460754fcfcb7c51fa7a50365a69f545b0e9db0f	rapid system-on-a-programmable-chip development and hardware acceleration of ansi c functions	hardware software codesign;system on chip c language electronic engineering computing embedded systems hardware software codesign;hardware system integration;automatic hardware acceleration;ansi c functions;embedded software development;system on a programmable chip;embedded systems;c language;system on chip;automatic hardware acceleration system on a programmable chip ansi c functions rapid development embedded solutions systems on chip altera hardware system integration embedded software development;altera;rapid development;electronic engineering computing;embedded solutions;systems on chip;acceleration master slave switches embedded system fabrics automatic logic units microprocessors hardware design languages registers embedded software	This tutorial explores the use of the system on a programmable chip as an ideal platform for rapid development of embedded solutions. Many sophisticated tools exist that facilitate rapid development of embedded systems on a chip. As an example, the authors discussed a suite of three tightly-coupled embedded systems development tools from Altera, including: (1) hardware system generation and integration, (2) embedded software development, and (3) automatic hardware acceleration of ANSI/ISO-spec C functions	ansi c;altera quartus;c standard library;compiler;embedded software;embedded system;field-programmable gate array;hardware acceleration;microprocessor;nios embedded processor;programming tool;software development process;spec#;system generation;system integration;system on a chip	David J. Lau;Orion Pritchard	2006	2006 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2006.311185	system on a chip;embedded system;computer architecture;parallel computing;computer science;rapid application development	EDA	4.423304833990446	50.779915053437904	54522
f7be03d3440e07a74cb807847afeb4bc95e5185b	ring-based sharing fabric for efficient pipelining of kernel-stream on cgra-based multi-core architecture	kernel level parallelism klp embedded systems coarse grained reconfigurable architecture cgra multi core;reconfigurable architectures;reconfigurable architectures multiprocessing systems parallel architectures pipeline processing;parallel architectures;computer architecture kernel pipeline processing fabrics system on chip resource management;multiprocessing systems;inter cgra reconfiguration technique ring based sharing fabric efficient pipelining kernel stream cgra based multicore architecture coarse grained reconfigurable architecture kernel level parallelism klp rsf resource utilization;pipeline processing	Coarse-grained reconfigurable architecture (CGRA)-based multi-core architecture aims at achieving high performance by kernel level parallelism (KLP). However, the existing CGRA-based multi-core architectures suffer from much energy and performance bottleneck when trying to exploit the KLP because of poor resource utilization caused by insufficient flexibility. In this work, we propose a new ring-based sharing fabric (RSF) to boost their flexibility level for the efficient resource utilization focusing on the kernel-stream type of the KLP. In addition, we introduce a novel inter-CGRA reconfiguration technique for the efficient pipelining of kernel-stream based on the RSF. Experimental results show that the proposed approaches improve performance by up to 50.62 times and reduce power by up to 44.03% when compared with the conventional CGRA-based multi-core architectures.	intel core (microarchitecture);kernel (operating system);multi-core processor;parallel computing;pipeline (computing);reasonable server faces	Heesun Kim;Seungyun Sohn;Yoonjin Kim	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783337	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-4.177398457263535	50.36260826107609	54528
3a3e6ab175a2ea23d2844f4a46eb840f2d879703	embedding of a real time image stabilization algorithm on sopc platform, a chip multi-processor approach	processor architecture;image processing;system on a programmable chip;image stabilization;community networks;hardware design;chip multi processor;parallel architecture;hardware implementation;reconfigurable hardware	Highly regular multi-processor architecture are suitable for inherently highly parallelizable applications such as most of the image processing domain. System on a programmable chip (SoPC) allows hardware designers to tailor every aspects of the architecture in order to match the specific application needs. These platforms are now large enough to embed an increasing number of core, allowing implementation of a multi-processor architecture with an embedded communication network.#R##N##R##N#In this paper we present the parallelization and the embedding of a real time image stabilization algorithm on SoPC platform. Our overall hardware implementation method is based upon meeting algorithm processing power requirement and communication needs with refinement of a generic parallel architecture model. Actual implementation is done by the choice and parameterization of readily available reconfigurable hardware modules and customizable commercially available IPs. We present both software and hardware implementation with performance results on a Xilinx SoPC target.	algorithm	Jean-Pierre Dérutin;Lionel Damez;Alexis Landrault	2008		10.1007/978-3-540-88458-3_15	embedded system;computer architecture;real-time computing;image processing;microarchitecture;reconfigurable computing;computer science;hardware architecture;image stabilization	Robotics	2.8969484783518538	47.15355601603864	54714
99b976f5e61a405dc3657db99d400557004e32e4	variability-tolerant high-reliability multicore platforms		motivations Variability and Aging Adaptive techniques Energy minimization under Real time constraints (optimal and on-line strategies) Improving Reliability and Performance (idleness distribution and TECs) Outline Overview smart camera smart phone Object recognition Video decoding Nowadays, electronic devices have to meet several requirements: • High performance (complex applications run even in small devices) • Low-power (e.g. long battery lifetime for portable devices) • High-reliable (miniaturization of components leads soft and hard errors) Small and portable devices: Performance demanding applications: Why multicore Due to the miniaturization of the components Pollack's Rule suggests to adopt multicore: monolithic core multicore      complexity power complexity e performanc      complexity power complexity e performanc	energy minimization;heart rate variability;mpsoc;mobile device;multi-core processor;online and offline;outline of object recognition;pollack's rule;real-time transcription;requirement;smart camera;smartphone;video decoder	Francesco Paterna	2012			embedded system;real-time computing;simulation;engineering	OS	-3.5770295076073406	58.66781525214605	54960
e9a746ffd9c953a7ce2f5deca85b835e30a0ebfe	issues on view switching for rf soc verification	hardware design languages;radio frequency circuit simulation analog circuits hardware design languages signal design analytical models systems engineering and theory baseband switches electronic design automation and methodology;baseband;impedance;functional verification;circuit partitioning rf soc verification systems on chip hierarchy editor;hardware description languages;formal verification;radio frequency;system on chip formal verification hardware description languages radiofrequency identification;system on chip;integrated circuit modeling;on the fly;switches;load modeling;radiofrequency identification	The main focus of this work is the functional verification of radio frequency systems on chip (RF SoCs). Different modeling approaches, like baseband modeling, analog modeling and event driven modeling, and their applications for verification are discussed. The possibilities and problems to use the Hierarchy Editor (HED) to switch between different modeling approaches on the fly in the top level schematic are discussed. Especially the cross domain connectivity issues between different model abstraction levels, like event driven modeling and analog modeling, are described in detail. Some suggestions for the circuit partitioning for verification purposes are given. This paper concludes with a suggestion for a possible extension of the Verilog HDL and the connect module insertion algorithm for the EDA industry.	algorithm;analogue electronics;baseband;carrier frequency;electronic design automation;event-driven programming;hardware description language;low if receiver;on the fly;operational amplifier;polyphase matrix;radio frequency;schematic;simulation;superword level parallelism;verilog	Yifan Wang;Stefan Joeres;Ralf Wunderlich;Stefan Heinen	2008	2008 IEEE International Behavioral Modeling and Simulation Workshop	10.1109/BMAS.2008.4751243	embedded system;electronic engineering;real-time computing;computer science	EDA	6.212607812003401	53.14613064053537	54979
c1c2a896f817d095f499a17e30efde4f7610ea51	decchip 21066: the alpha axp chip for cost-focused systems	cmos integrated circuits;digital arithmetic dec computers microprocessor chips computer architecture cmos integrated circuits pipeline processing buffer storage;buffer storage;system performance;chip;computer architecture;data cache;microprocessors clocks central processing unit microcomputers bandwidth phase locked loops personal communication networks electrical equipment industry cost function cmos technology;system integration;dec computers;0 675 micron decchip 21066 microprocessor alpha axp architecture cost focused system applications cmos based superscalar super pipelined processor dual instruction issue system integration system performance on chip fully pipelined processors integer processor floating point processor high bandwidth memory controller peripheral component interconnect pci i o controller graphics assisting hardware internal instruction cache internal data cache external cache controller design verification implementation;digital arithmetic;peripheral component interconnect;floating point;design verification;pipeline processing;microprocessor chips	The DECchip 21066 microprocessor is the first Alpha AXP architecture microprocessor to target cost-focused system applications and the second in a family of chips to implement the Alpha AXP architecture. The 21066 is a 0.675-micron, CMOS-based superscalar, super-pipelined processor using dual instruction issue. It incorporates a high level of system integration to provide best-in-class system performance for low-cost system applications. The 21066 integrates on-chip fully pipelined integer and floating-point processors, a high-bandwidth memory controller, an industry-standard Peripheral Component Interconnect (PCI) I/O controller, graphics-assisting hardware, internal instruction and data caches, and an external cache controller. This paper discusses the tradeoffs and results of the design, verification, and implementation of the 21066. >	alpha 21064;dec alpha	Dina L. McKinney;Daniel L. Leibholz;Mark B. Rosenbluth;James R. Mullens;Kwong-Tak A. Chui;Masooma Bhaiwala;Sanjay J. Patel;Christopher L. Houghton;Delvan A. Ramey	1994		10.1109/CMPCON.1994.282899	embedded system;computer architecture;parallel computing;computer science;dec alpha	EDA	6.808597315054231	49.925433151628376	55002
8f1361d56819439265cb21f575db7d11c1e43de3	efficient synchronization for embedded on-chip multiprocessors	sistema fila espera;energy economy;estensibilidad;economie energie;file attente;systeme attente;grapes;network on chip noc;economia energia;evaluation performance;synchronisation buffer storage embedded systems microprocessor chips network on chip shared memory systems;chemical mechanical polishing;optimisation;microelectronic fabrication;arquitectura red;fabricacion microelectrica;coherence schemes;shared memory;performance evaluation;optimizacion;systeme embarque;integrated circuit;multiprocessor;network on chip;implementation;memoria compartida;energy aware systems;evaluacion prestacion;chip multiprocessor;synchronization embedded systems energy aware systems multiprocessor system on chip mpsoc network on chip noc;buffer storage;embedded systems synchronization techniques shared memory on chip multiprocessors network on chip mobile systems synchronization operation buffer spin lock memory accesses grapes multiprocessor systems on chip coherence schemes;queue;cache memory;circuito integrado;indexing terms;architecture reseau;system on a chip;embedded system;interconnection network;synchronization operation buffer;antememoria;network on a chip system on a chip multiprocessing systems hardware protocols scalability costs pipelines;memory access;synchronisation;embedded systems;antememoire;performance improvement;shared memory systems;sistema sobre pastilla;synchronization;memory accesses;multi processor system on chip;queueing system;low power electronics;polissage mecanochimique;multiprocessor system on chip;experimental validation;optimization;network architecture;procesador;synchronization techniques;systeme sur puce;sincronizacion;extensibilite;scalability;power consumption;spin lock;consommation energie electrique;processeur;multiprocesador;mobile systems;implementacion;multiprocessor systems on chip;multiprocessor system on chip mpsoc;shared memory on chip multiprocessors;electronique faible puissance;fila espera;red interconexion;processor;circuit integre	This paper investigates optimized synchronization techniques for shared memory on-chip multiprocessors (CMPs) based on network-on-chip (NoC) and targeted at future mobile systems. The proposed solution is based on the idea of locally performing synchronization operations requiring continuous polling of a shared variable, thus, featuring large contentions (e.g., spin locks and barriers). A hardware (HW) module, the synchronization-operation buffer (SB), has been introduced to queue and to manage the requests issued by the processors. By using this mechanism, we propose a spin lock implementation requiring a constant number of network transactions and memory accesses per lock acquisition. The SB also supports an efficient implementation of barriers. Experimental validation has been carried out by using GRAPES, a cycle-accurate performance/power simulation platform for multiprocessor systems-on-chip (MPSoCs). Two different architectures have been explored to prove that the proposed approach is effective independently from caches and coherence schemes adopted. For an eight-processor target architecture, we show that the SB-based solution achieves up to 50% performance improvement and 30% energy saving with respect to synchronization based on the caching of the synchronization variables and directory-based coherence protocol. Furthermore, we prove the scalability of the proposed approach when the number of processors increases	barrier (computer science);busy waiting;cache coherence;central processing unit;computer hardware;lock (computer science);mpsoc;mathematical optimization;memory controller;memory module;modular programming;multiprocessing;network on a chip;overhead (computing);sandy bridge;scalability;shared variables;shared memory;simulation;spinlock;system on a chip	Matteo Monchiero;Gianluca Palermo;Cristina Silvano;Oreste Villa	2006	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2006.884147	embedded system;synchronization;electronic engineering;parallel computing;real-time computing;computer science;operating system;energy economics;network on a chip;synchronization;computer network	EDA	0.2510749253586809	58.19974144966007	55088
ddecd73edd0bcdfb6aa825cfaa37b73dc28fedf0	flexible and resource efficient fpga-based quad data rate memory interface design for high-speed data acquisition systems		Data acquisition is the process of collecting digital data produced by various types of electronic devices, such as sensors, analog to digital converters (ADCs), communication interfaces and digital I/O modules. FPGAs are the most common data acquisition platform for implementation, as they provide a generic environment for different types of interfaces and supplementary logic elements for further processing. This paper presents a flexible and resource efficient data acquisition system implementation on FPGAs especially targeted for high bandwidth interfaces. The implementation details are based on a QDR (Quad Data Rate) Memory interface design, as QDR interface reveals the common characteristics of high speed data channels. Also QDR memories have a wide application area such as the network processor, baseband processor and other high performance parallel processing network and communication. Proposed interface is fully tested and verified on a custom board between XILINX XC7V980T-1 FPGA and CYPRESS CY7C2665KV18-450BZI QDRII+ memory. The methodology described for the QDR memory interface design can be directly utilized for data acquisition systems requiring similar high speed DDR interfaces, such as high speed ADCs and I/O demultiplexer chips.	analog-to-digital converter;baseband processor;data acquisition;data rate units;digital data;field-programmable gate array;geforce 7 series;global variable;input/output;multiplexer;network processor;paging;parallel computing;quad data rate sram;routing;semiconductor intellectual property core;sensor	Nizam Ayyildiz	2018	2018 21st Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2018.00038	computer hardware;field-programmable gate array;real-time computing;digital data;network processor;quad data rate;data acquisition;baseband processor;computer science;bandwidth (signal processing);communication channel	HPC	7.080616030441459	47.73581544679032	55089
72424a2742879e89c9977ce6808d37435560b636	a user-level library for fault tolerance on shared memory multicore systems	libraries;multi threading;shared memory multicore systems;libraries instruction sets fault tolerance fault tolerant systems multicore processing benchmark testing memory management;shared memory;memory management;fault tolerant;user level library;checkpointing;fault tolerant computing;shared memory systems;redundancy;fault tolerant systems;multicore processing;fault tolerance;shared memory systems checkpointing fault tolerant computing libraries multi threading redundancy;redundant execution;checkpoint rollback user level library shared memory multicore systems reliability concerns redundant execution multicore processing multithreaded user level application fault tolerance error detection;error detection;multithreaded user level application fault tolerance;checkpoint rollback;benchmark testing;reliability concerns;instruction sets	The ever decreasing transistor size has made it possible to integrate multiple cores on a single die. On the downside, this has introduced reliability concerns as smaller transistors are more prone to both transient and permanent faults. However, the abundant extra processing resources of a multicore system can be exploited to provide fault tolerance by using redundant execution. We have designed a library for multicore processing, that can make a multithreaded user-level application fault tolerant by simple modifications to the code. It uses the abundant cores found in the system to perform redundant execution for error detection. Besides that, it also allows recovery through checkpoint/rollback. Our library is portable since it does not depend on any special hardware. Furthermore, the overhead (up to 46% for 4 threads), our library adds to the original application, is less than other existing approaches, such as Respec.	application checkpointing;benchmark (computing);die (integrated circuit);error detection and correction;fault tolerance;multi-core processor;multithreading (computer architecture);overhead (computing);programmer;rollback (data management);sensor;shared memory;speedup;symmetric multiprocessing;thread (computing);transaction processing system;transistor;user space	Hamid Mushtaq;Zaid Al-Ars;Koen Bertels	2012	2012 IEEE 15th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2012.6219071	fault tolerance;computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	6.760735080916187	59.914632398671664	55227
7a49b91ad8d5ceca647c2a0f908217e8458f51f8	design and validation of a connection network for many-processor multiprocessor systems	multiprocessor systems;aerodynamics;computer architecture;circuit simulation;computational modeling;centralized control;multiprocessing systems;nasa;multiprocessing systems aerodynamics delay numerical simulation nasa centralized control circuit simulation computational modeling computer architecture costs;numerical simulation	Distributed control and circuit switching are the keys to fast access in the many-processor to many-memory-module connection network proposed for the Numerical Aerodynamic Simulator.	circuit switching;distributed control system;multiprocessing	George H. Barnes;Stephen F. Lundstrom	1981	Computer	10.1109/C-M.1981.220293	computer architecture;parallel computing;simulation;aerodynamics;computer science;computational model	HPC	-3.57596889287228	48.239352464350404	55275
4c71a613bb2fd3aa678489e4c5f2dda2d8c20105	applications of performance benchmarking to the development of signal processing systems based on personal computer technology	benchmarking;personal computer technology;application software signal processing microcomputers signal processing algorithms software libraries finite impulse response filter software performance fast fourier transforms software algorithms software architecture;mathematics computing;software libraries;personal computer;application software;benchmark;finite impulse response filter;finite impulse response filter benchmarking signal processing system personal computer technology intel integrated performance primitive library fast fourier transform;software performance;single instruction multiple data;fast fourier transform;software architecture;finite impulse response;multirate filter;system design;signal processing;fir filter;fast fourier transforms;software algorithms;signal processing system;fir filters;signal processing algorithms;microcomputer applications;high performance;software libraries benchmark testing fast fourier transforms fir filters mathematics computing microcomputer applications signal processing;benchmark signal processing single instruction multiple data fast fourier transform multirate filter;microcomputers;benchmark testing;intel integrated performance primitive library	Highly optimized libraries of vector and matrix math functions, such as the Intel integrated performance primitive (IPP) libraries, can be used to quickly and economically construct high performance signal processing systems built up around personal computer technology. This paper describes a portable benchmarking software suite that has been developed for benchmarking the performance obtainable with IPP implementations of common signal processing algorithms, including the fast Fourier transform (FFT) and finite impulse response (FIR) filter. The results presented provide useful insights into system design choices concerning algorithms, software architecture and processors	algorithm;central processing unit;fast fourier transform;finite impulse response;library (computing);personal computer;signal processing;software architecture;software suite;systems design	Robert J. Inkol;Collin Wilson;Mathieu Eidus	2006	2006 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2006.277339	embedded system;parallel computing;computer hardware;computer science;operating system;finite impulse response;signal processing	HPC	3.8914672688868635	47.451850389343534	55285
0d974ce4fd7ff6284f46543d016f374b64dd06fb	synthesis of native mode self-test programs	native mode self test;design process;functional testing;test synthesis;functional test generation;test generation;time to market	Recent studies show that at-speed functional tests are better for nding realistic defects than tests executed at lower speeds. This advantage has led to growing interest in design for at-speed tests. In addition, time-to-market requirements dictate developments of tests early in the design process. In this paper, we present a new methodology for synthesis of at-speed self-test programs for microprocessors. Based on information about the instruction set, this high{level test generation methodology can generate instruction sequences that exercise all the functional capabilities of complex processors. Modern processors have large memory modules, register les and powerful ALUs with comprehensive operations, which can be used to generate and control built-in tests and to evaluate the response of the tests. Our method exploits the functional units to compress and check the test response at chip internal speeds. No hardware test pattern generators or signature analyzers are needed, and the method reduces area overhead and performance impact as compared to current BIST techniques. A novel test instruction insertion technique is introduced to activate the control/status inputs and internal modules related to them. The new methodology has been applied to an example processor much more complex than any benchmark circuit used in academia today. The results show that our approach is very eeective in achieving high fault coverage and automation in at-speed self-test generation for microprocessor-like circuits.	arithmetic logic unit;benchmark (computing);built-in self-test;central processing unit;dimm;fault coverage;microprocessor;native (computing);overhead (computing);requirement;test card	Jian Shen;Jacob A. Abraham	1998	J. Electronic Testing	10.1023/A:1008305820979	embedded system;electronic engineering;real-time computing;simulation;design process;computer science;electrical engineering;automatic test pattern generation;test compression;functional testing;algorithm;computer engineering	EDA	8.37876129321492	52.71633286260595	55313
8db750cede13239a4fb606edd2d232cf75be626d	asics, processors, and configurable computing	compilation techniques asics configurable computing computational middle ground conventional microprocessors fpgas end user downloading configuration data high performance application specific circuits microprocessor architecture;computer architecture;application specific integrated circuits field programmable gate arrays microprocessors amplitude shift keying performance gain parallel processing distributed computing computer architecture computer languages circuit synthesis;application specific integrated circuits;configurable computing;field programmable gate arrays;point of view;computer architecture configuration management application specific integrated circuits field programmable gate arrays;high performance;configuration management	represents a new computational middle ground that fills the existing void between conventional microprocessors and ASICs. This point of view is based upon the observation that FPGAs share some similarities with both processors and ASICs. FPGAs are seen as similar to processors because they are customized in the field by the end-user by downloading configuration data into the device. They can also be seen as similar to ASICs because they can implement high-performance, applicationspecific circuits. It is hoped that if configurable computing can be shown to be similar to conventional processors, it will be possible to borrow microprocessor architecture and compilation techniques for use in the configurable-computing community.	application-specific integrated circuit;central processing unit;compiler;download;field-programmable gate array;microprocessor;point of view (computer hardware company);reconfigurable computing	Brad L. Hutchings	1997		10.1109/HICSS.1997.667465	computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;configuration management;application-specific integrated circuit;field-programmable gate array	Arch	0.32822221910626687	48.335313989460865	55338
60d864f5f72078b8a1c733faa1eef7762657c4d8	the memory challenge in computing systems: a survey		It is well known that DRAM memory performance cannot keep pace with the performance of today's multicore compute systems. In addition to the memory bandwidth problem, there is another major challenge, namely, the power/energy challenge. DRAMs are largely contributing to the overall power consumption. Thus, there is a need for power and bandwidth optimization of the DRAM memory subsystems. Moreover, new memory architectures are emerging like HBM, HMC and Wide I/O DRAMs to cope with the increasing bandwidth requirements. In this talk, we will give an overview on these new architectures and present various optimization techniques to optimize bandwidth and energy consumption in DRAM based memory systems.	dynamic random-access memory;graph bandwidth;high bandwidth memory;hybrid memory cube;mathematical optimization;memory bandwidth;multi-core processor;requirement	Norbert Wehn	2017	2017 30th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2017.8226061	memory architecture;parallel computing;microelectronics;energy consumption;memory bandwidth;multi-core processor;bandwidth (signal processing);computer science	EDA	-2.9211457702673784	47.73354735927498	55434
19ac122c6b83a41b8bd157c24637466a60959a90	highly-reliable integrated circuits for gro		This paper deal with highly-reliable integrated circuits for ground and space applications especially strong against soft errors. In the terrestrial region, neutrons and alpha particles are main sources to cause soft errors. They flip contents of storage cells such as SRAMs and flip flops on a semiconductor chip. Soft errors must be considered for semiconductor chips for security, automotive, avionics, cloud servers, social infrastructure and so on that are critical for safety and serviceability. In outer space, heavy ions are main sources to cause soft errors.	artificial intelligence;autonomous robot;avionics;flops;flip-flop (electronics);integrated circuit;semiconductor;semiconductor device;social infrastructure;soft error;static random-access memory;terrestrial television	Kazutoshi Kobayash	2017	2017 IEEE 12th International Conference on ASIC (ASICON)	10.1109/ASICON.2017.8252558	real-time computing;avionics;outer space;electronic engineering;transistor;integrated circuit;silicon on insulator;serviceability (structure);computer science;flip;server	EDA	9.217400455747352	59.93116427980342	55607
db563cbb73f119bbb2c2e56f0e1654ae57627fcb	memory in processor-supercomputer on a chip: processor design and execution semantics for massive single-chip performance	libraries;process design parallel processing hardware logic devices computer science scheduling libraries bandwidth instruction sets scalability;massive node performance;programming model processor supercomputer system on chip processor design single chip performance 2 mb mip node library specification cos hlf unit interface massive node performance;processor design;process design;chip;programming model;parallel architectures;system on chip;memory architecture;2 mb mip node;scheduling;cos hlf unit interface;library specification;processor supercomputer;single chip performance;bandwidth;scalability;computer science;parallel processing;logic devices;instruction sets;hardware;parallel architectures memory architecture system on chip	The MIP S.C.O.C was designed to overcome the Von-Neumann bottleneck and develop massive on-chip parallelism to achieve Teraflop scale single chip performance. We case study here a specific 2 MB MIP node that has a 128 bit datapath. This paper also specifies the technological reqirements and discusses the implementation strategy to support the feasibility of the project. We develop the ISA format and preview the hardware compiler COS, that will map applications and schedule the instructions on the MIP S. C. O. C. We then discuss the library specifications and COS-HLF unit interface. We develop the scalable pyramid cluster to accomodate massive node performance. We discuss briefly the programming model and an application execution to demonstrate the scalability.	128-bit;cos;datapath;flops;lu reduction;locality of reference;megabyte;parallel computing;performance per watt;processor design;programming language;programming model;simd;scalability;silicon compiler;supercomputer;von neumann architecture	Nagarajan Venkateswaran;Arrvindh Shriraman;Niranjan Soundararajan	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.279	chip;system on a chip;process design;parallel processing;computer architecture;parallel computing;real-time computing;scalability;processor design;computer science;operating system;instruction set;distributed computing;programming paradigm;programming language;scheduling;bandwidth	Arch	1.3948646314415545	48.880548640362285	55930
2e239b24ea5942be838684421036bef4300c12cf	signal processing domain application mapping on the brick reconfigurable array	signal processing field programmable gate arrays reconfigurable architectures;reconfigurable computing;reconfigurable architectures;application mapping;signal processing reconfigurable computing expression grain application mapping;reconfigurable architecture;mips brick reconfigurable array expression grain reconfigurable architecture signal processing domain application mapping 2d convolution 16 tap fir filter 8 point fft brick reconfigurable array vhdl implementation sparc v8 simulators fpga performance simulation analysis;signal processing;fir filter;simulation analysis;array signal processing signal mapping computer architecture signal processing algorithms image processing field programmable gate arrays proposals convolution finite impulse response filter analytical models;field programmable gate arrays;expression grain	This paper introduces the proposal of an Expression Grain Reconfigurable Architecture called BRICK, its functionality and main components. A mapping for three signal processing applications such as a 3x3 2-D convolution, a 16-Tap FIR filter and an 8-point FFT is developed inside the 4x4 Reconfigurable Array. A performance simulation analysis study is developed comparing the BRICK reconfigurable array VHDL implementation to a MIPS and a SPARC V8 simulators in order to validate the Reconfigurable Array proposal. Considerable gains up to an order of magnitude are obtained and important design issues and challenges were discovered when developing this work.	convolution;fast fourier transform;finite impulse response;performance prediction;sparc;signal processing;simulation;vhdl	Juan Fernando Eusse Giraldo;Ricardo Pezzuol Jacobi	2009	2009 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2009.85	embedded system;parallel computing;reconfigurable computing;computer science;finite impulse response;signal processing;field-programmable gate array	EDA	3.9062942915062826	47.54090922807	56220
308072934e2466c8c5373ee4b53421de19893754	dise: a programmable macro engine for customizing applications	optimising compilers;engines production application software pattern matching information science computer architecture decoding control systems hardware communication system control;application program interfaces macros microprocessor chips instruction sets optimising compilers;control systems;static binary rewriting;acf;decoding;information science;application software;application customization function;isa;binary rewriting;dynamic code decompression;macros;computer architecture;memory fault isolation;software hardware scheme;server disks;dise;engines;dynamic instruction stream editing;pattern matching;programmable macro engine dynamic instruction stream editing dise software hardware scheme safety checking dynamic code decompression dynamic optimization application customization function acf macro expansion isa static binary rewriting memory fault isolation;application program interfaces;power management;production;macro expansion;communication system control;safety checking;fault isolation;hardware implementation;large classes;microprocessor chips;instruction sets;dynamic optimization;hardware;programmable macro engine	Dynamic Instruction Stream Editing (DISE) is a cooperative software-hardware scheme for efficiently adding customization functionality---e.g, safety/security checking, profiling, dynamic code decompression, and dynamic optimization---to an application. In DISE, application customization functions (ACFs) are formulated as rules for macro-expanding certain instructions into parameterized instruction sequences. The processor executes the rules on the fetched instructions, feeding the execution engine an instruction stream that contains ACF code. Dynamic instruction macro-expansion is widely used in many of today's processors to convert a complex ISA to an easier-to-execute, finer-grained internal form. DISE co-opts this technology and adds a programming interface to it.DISE unifies the implementation of a large class of ACFs that would otherwise require either special-purpose hardware widgets or static binary rewriting. We show DISE implementations of two ACFs---memory fault isolation and dynamic code decompression---and their composition. Simulation shows that DISE ACFs have better performance than their software counterparts, and more flexibility (which sometimes translates into performance) than hardware implementations.	acf;algorithm;application programming interface;binary recompiler;central processing unit;code word;data compression;dynamic programming;experiment;fault detection and isolation;general protection fault;general-purpose markup language;mathematical optimization;operating system;palmtop pc;profiling (computer programming);program optimization;requirement;rewriting;server (computing);simulation;static library;workstation	Marc L. Corliss;E. Christopher Lewis;Amir Roth	2003		10.1145/859618.859660	computer architecture;application software;parallel computing;real-time computing;industry standard architecture;information science;computer science;operating system;macro;pattern matching;instruction set;programming language;fault detection and isolation	Arch	0.8080437306169042	46.63227850031264	56383
a72a614770ce2e44b6c1cfe430e6fa752f1f1a4b	the new framework of applications: the aladdin system	vision system;cnn technology;cellular visual microprocessor	The first CNN technology-based, high performance industrial visual computer called Aladdin is reported. The revolutionary device is the world premier of the ACE4k Cellular Visual Microprocessor (CVM) chip powering an industrial visual computer. One of the most important features of the Aladdin system is the image processing library. The library reduces algorithm development time, provides efficient codes, error free operation in binary, and accurate operation in grayscale nodes. Moreover the library provides an easy way to use the Aladdin system for those who are not familiar with the CNN technology.	algorithm;code;computation;digital signal processor;grayscale;image processing;microprocessor	Ákos Zarándy;Csaba Rekeczky;Péter Földesy;István Szatmári	2003	Journal of Circuits, Systems, and Computers	10.1142/S0218126603001148	embedded system;computer vision;machine vision;computer hardware;computer science;operating system;computer graphics (images)	EDA	5.568833024178769	48.580492750166535	56482
6728cbcb5adf6550bf658d1444d3e04a4af24cf4	a verification methodology for reconfigurable systems	reconfigurable system;reconfigurable architectures;field programmable gate arrays formal verification silicon system recovery conferences microprocessors image processing system testing productivity design optimization;reconfigurable soc;integrated circuit design;integrated circuit design reconfigurable architectures formal verification system on chip field programmable gate arrays;fpga reconfigurable systems symbad project industrial design flow reconfigurable soc formal verification reconfigurability symbc tool hw virtualization phase;formal verification;system on chip;field programmable gate arrays;industrial design;systemc	In the frame of the Symbad project, an industrial design flow for reconfigurable SoC's is analyzed by a pool of experts in high-level formal proof tools. The goal of Symbad is to introduce formal verification along this flow. As a consequence, formal verification is applied to specific problems related to reconfigurability. The focus of the paper is on the SymbC tool, which is applied during the HW virtualization phase, when it is mandatory to assure that a task mapped on FPGA is available at the time of its call.	field-programmable gate array;formal proof;formal verification;high- and low-level;reconfigurability	Michele Borgatti;Andrea Fedeli;Umberto Rossi;Jean-Luc Lambert;Imed Moussa;Franco Fummi;Cristina Marconcini;Graziano Pravadelli	2004	Fifth International Workshop on Microprocessor Test and Verification (MTV'04)	10.1109/MTV.2004.2	system on a chip;embedded system;computer architecture;real-time computing;industrial design;formal verification;reconfigurable computing;computer science;programming language;field-programmable gate array;integrated circuit design	EDA	3.980436549760648	52.132040104018934	56504
18b06f317094aca2d8a75616e309dc37a8a15bb2	design-space exploration of fault-tolerant building blocks for large-scale quantum computing	quantum computing fault tolerance large scale systems design methodology buildings failure analysis algorithm design and analysis circuit faults microarchitecture logic gates;fault tolerant;logic design;data communication mechanisms space exploration logical fault tolerant building blocks large scale quantum computing sensitivity microarchitecture model logic gates memory mechanisms;building block;sensitivity computer architecture fault tolerant computing large scale systems logic design quantum gates;space exploration;data communication;quantum gates;computer architecture;sensitivity;large scale;fault tolerant computing;logical fault tolerant building blocks;logic gates;quantum computer;quantum;data communication mechanisms;failure rate;memory mechanisms;large scale quantum computing;sensitivity quantum fault tolerant;design space exploration;logic gate;microarchitecture model;large scale systems;design methodology	In this paper, we present a design methodology for quantifying the role each building component of a logical fault-tolerant building block for quantum computers plays in the performance of the logical block. A logical building block is the set of operations necessary to execute a fault-tolerant circuit structure in quantum programs, such as the network of operations implementing a logical quantum bit. By analyzing the interaction between the algorithmic structure of a building block and the number of lower-level elements where faults are likely to occur, we can quantify the sensitivity of logical building blocks to two things: (1) to changes in the failure rates of the lower level elements comprising a proposed microarchitecture model, which are defined as logic gates, memory mechanisms, and data communication mechanisms; and (2) to transformation of the program structure for each building block through compilation techniques. We further show how this information can be used to develop optimized building blocks by inserting the gathered design constraints in our compilation mechanisms.	compiler;computer;fault tolerance;logic gate;microarchitecture;quantum computing;qubit;structured programming	Tzvetan S. Metodi;Andrew W. Cross;Darshan D. Thaker;Isaac L. Chuang;Frederic T. Chong	2007	2007 IEEE International Symposium on Nanoscale Architectures	10.1109/NANOARCH.2007.4400851	computer architecture;parallel computing;computer science;theoretical computer science	Arch	7.729508979233916	56.873936248666126	56539
c3d8c0a1b435578eeb415fee00310482d0033214	dependability analysis of fault tolerant systems based on partial dynamic reconfiguration implemented into fpga	reconfiguration;reliability;reconfigurable architectures;controller;maintenance engineering;fpga;sram chips failure analysis fault tolerance field programmable gate arrays integrated circuit reliability integrated logic circuits markov processes reconfigurable architectures redundancy;failure analysis;computer architecture;fault tolerant system;redundancy;maintenance engineering field programmable gate arrays markov processes reliability computer architecture tunneling magnetoresistance mathematical model;fault tolerance;dependability;mathematical model;model;controller reliability dependability model fault tolerant system fpga reconfiguration architecture;reliability analysis fault tolerant systems dependability analysis partial dynamic reconfiguration sram based fpga fault tolerant architectures functional unit redundancy fault occurrence recovery mechanism seu injector partial bitstream failure rate repair rate markov dependability models;markov processes;integrated logic circuits;field programmable gate arrays;integrated circuit reliability;tunneling magnetoresistance;architecture;sram chips	In this paper, a dependability analysis of fault tolerant systems implemented into the SRAM-based FPGA is presented. The fault tolerant architectures are based on the redundancy of functional units associated with a concurrent error detection technique which uses the principles of partial dynamic reconfiguration as a recovery mechanism from a fault occurrence. Architectures are tested by injecting soft errors into partial bitstream in FPGA by an SEU injector and the faults coverage of this architecture is obtained. From faults coverage, the failure rate and repair rate are evaluated. Then, for fault tolerant architecture Markov dependability models are created and how the reliability and availability parameters derived from this model for different configurations of architectures and faulty modules is demonstrated. The reliability analysis results are then shown.	bitstream;complex systems;computation;dependability;error detection and correction;experiment;failure rate;fault tolerance;field-programmable gate array;markov chain;mathematical optimization;numerical analysis;programming tool;simulation;single event upset;soft error;software bug;solver;static random-access memory;triple modular redundancy	Jan Kastil;Martin Straka;Lukas Miculka;Zdenek Kotásek	2012	2012 15th Euromicro Conference on Digital System Design	10.1109/DSD.2012.40	maintenance engineering;embedded system;fault tolerance;real-time computing;fault coverage;control reconfiguration;general protection fault;field-programmable gate array	EDA	9.10325907514188	60.2273401678344	56790
174fe6b1b657bddb6193bc51543a1ffeecd95c42	power estimation of time variant socs with tapes	modern socs;design space;architecture exploration;power minimization strategy;power estimation;design tool;design process;power consumption;time variant socs;run time;tapes system simulator;low power electronics;system on chip;integrated circuit design;network processor	During the design process of modern SoCs (systems on chip), design tools and methods are required for the exploration of promising solutions. Evaluation criteria in this process are performance and often also power consumption. The design space is expanded by a trend towards time variant SoCs, which adapt their behaviour at run time to improve reliability or power consumption. This paper presents an extension to the TAPES system simulator in order to enable not only the exploration of architectures but also the investigation of power minimization strategies. The usefulness of the simulator is demonstrated in an architecture exploration of a network processor.	common criteria;mathematical optimization;network processor;power optimization (eda);run time (program lifecycle phase);simulation;system on a chip	Andreas Lankes;Thomas Wild;Johannes Zeppenfeld	2007	10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)	10.1109/DSD.2007.4341478	system on a chip;embedded system;real-time computing;design process;computer science;operating system;low-power electronics;network processor;integrated circuit design	EDA	3.239024494687771	54.72204189842046	56832
7db2a37633a42facb61e3d16e295cbb2cc70d682	heterogeneous multi-processor soc: an emerging paradigm of embedded system design and its challenges	processing element;mobile multimedia;wireless network;digital television;international technology roadmap for semiconductors;embedded system;chip;low power;embedded system design;design and implementation;multi processor system on chip;hardware design;software design;low power consumption	The recent years have witnessed a variety of new embedded applications. Typical examples include mobile multimedia gadgets, digital televisions, high-end cell phones, wireless network applications, etc. The salient features of these applications include more comprehensive functionalities, higher performance demand, and low-power consumption. These requirements render the traditional single processor-based embedded systems no longer an appropriate realization for such applications. On the other hand, the continual advance of VLSI technologies enables more and more transistors to be integrated on a single chip. The International Technology Roadmap for Semiconductors predicts that chips with a billion transistors are within reach. As a result, the push (application demands) and pull (VLSI technology) forces together give birth to the multi-processor system-on-chips (MPSoCs).#R##N##R##N#Heterogeneous MPSoCs are different from traditional embedded systems in many aspects and they ask for new design and implementation methodologies. Heterogeneous MPSoCs are not merely a hardware design. The complexity and heterogeneity of the system significantly increase the complexity of the HW/SW partitioning problem. Meanwhile, evaluating the performance and verifying its correctness is much more difficult compared to traditional single processor-based embedded systems. Constructing a simulator to simulate the system’s behavior and evaluate its performance takes more effort compared to conventional embedded systems. The verification of the system also becomes challenging.#R##N##R##N#Programming a heterogeneous MPSoC is another challenge to be faced. This problem arises simply because there are multiple programmable processing elements and since they are heterogeneous, software designer needs to have expertise on all of these processing elements and needs to take a lot of care on how to make the software running as a whole.#R##N##R##N#There are a lot more issues that do not appear or easier to tackle on traditional embedded systems, trade-offs between performance and low-power will dominate the design life time. However, the incoming challenges also brought us many opportunities either to industry and academic research.		Xu Cheng	2005		10.1007/11599555_3	chip;embedded system;real-time computing;simulation;digital television;telecommunications;computer science;software design;operating system;wireless network;software engineering;computer network	EDA	9.158820674130201	55.60902103160537	56836
1863e1597de4ada98b24d7920045a1a903d053b6	all the aes you need on cortex-m3 and m4		This paper describes highly-optimized AES-{128, 192, 256}CTR assembly implementations for the popular ARM Cortex-M3 and M4 embedded microprocessors. These implementations are about twice as fast as existing implementations. Additionally, we provide the fastest bitsliced constant-time and masked implementations of AES-128-CTR to protect against timing attacks, power analysis and other (first-order) sidechannel attacks. All implementations, including an architecture-specific instruction scheduler and register allocator, which we use to minimize expensive loads, are released into the public domain.	arm cortex-m;embedded system;fastest;first-order predicate;microprocessor;register allocation;scheduling (computing);smart-m3	Peter Schwabe;Ko Stoffelen	2016		10.1007/978-3-319-69453-5_10	parallel computing;timing attack;power analysis;arm cortex-m;allocator;computer science	Arch	-4.303072494142306	51.5206066277707	57097
659da9421d0f96ef5f596f8f589c6b1cf7ed5396	gals systems prototyping using multiclock fpgas and asynchronous network-on-chips	network on chip field programmable gate arrays asynchronous circuits logic design;asynchronous data encryption standard module;logic design;network on chip;building block;gals system;quasidelay insensitive logic;asynchronous circuit;serial communication links;memory banks;quasi delay insensitive;prototypes field programmable gate arrays network on a chip clocks asynchronous circuits integrated circuit interconnections network topology circuit topology synchronization cryptography;quasidelay insensitive logic multiclock fpga asynchronous network on chips globally asynchronous locally synchronous system gals system modularity property asynchronous circuits interconnect topology arbitration problem synchronization problem memory banks serial communication links parallel communication links asynchronous data encryption standard module;interconnect topology;multiclock fpga;data encryption standard;modularity property;communication cost;arbitration problem;asynchronous circuits;asynchronous network on chips;globally asynchronous locally synchronous system;field programmable gate arrays;globally asynchronous locally synchronous;parallel communication links;high performance;synchronization problem	This paper presents an innovating methodology for network-centric globally-asynchronous locally-synchronous (GALS) system prototyping. High-performance multiclock FPGAs are exploited for easy and fast prototyping of GALS systems based of an asynchronous network-on-chip (ANoC) interfacing synchronous standard IP cores. Modularity property of asynchronous circuits is fully exploited to design regular distributed interconnect topologies by the means of basic topology-free building blocks, with a focus and special design effort to solve arbitration and synchronization problems. A case-study is implemented on an up-to-date FPGA which includes two independently clocked processors, memory banks, serial and parallel communication links and an asynchronous DES (data encryption standard) module connected through an asynchronous 5/spl times/5 crossbar. The clock-less modules are implemented using a quasidelay insensitive logic on the FPGA by the means of a dedicated library. Performance figures are reported on the FPGA platform, especially for communication costs, speed and latency of the ANoC.	central processing unit;clock rate;crossbar switch;encryption;field-programmable gate array;globally asynchronous locally synchronous;memory bank;network on a chip;prototype	Jerome Quartana;Salim Renane;Arnaud Baixas;Laurent Fesquet;Marc Renaudin	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515738	embedded system;logic synthesis;real-time computing;memory bank;asynchronous circuit;computer science;distributed computing;network on a chip;field-programmable gate array	EDA	1.921417633735151	59.783167362805195	57149
696914462854aff6dc9e0978d7fc26e172cd83a9	predictive energy management techniques for pgas programming	power consumption computer power supplies parallel programming power aware computing;parallel programming;power optimization technique predictive energy management pgas programming power consumption sustainable large scale computing partitioned global address space parallel programming locality awareness scalable energy efficient computation high performance computing hpc cluster distributed cluster dynamic voltage frequency scaling dvfs unified parallel c upc codes cpu socket level;upc;power aware computing;synchronization;data locality awareness;clustering algorithms;pgas;optimization;power consumption;dvfs;electronics packaging;instruction sets optimization electronics packaging synchronization power measurement clustering algorithms programming;parallel programming pgas upc dvfs data locality awareness energy management;programming;computer power supplies;energy management;power measurement;instruction sets	Power consumption increasingly presents an upper bound on sustainable large scale computing performance and reliability. The Partitioned Global Address Space (PGAS) programming model is a family of parallel programming paradigms with a global address space for ease-of-use while providing locality awareness for efficient execution. Very little exploration has been done to determine the potential of PGAS programming models in improving scalable energy efficient computation for high performance computing (HPC) clusters. This paper examines features of the PGAS programming model that may support predictively reducing power consumption in distributed clusters via dynamic voltage frequency scaling (DVFS). These concepts are tested with Unified Parallel C (UPC) codes running on a cluster of commodity PCs which have been instrumented to measure power at the CPU socket level. We have also explored approaches to automating these power optimization techniques at compile time. Benchmarking results show a tangible reduction in power consumption without impacting the overall execution time of the program.	cpu socket;central processing unit;code;compile time;compiler;computation;dynamic voltage scaling;frequency scaling;image scaling;locality of reference;mathematical optimization;parallel computing;partitioned global address space;programming model;programming paradigm;run time (program lifecycle phase);scalability;supercomputer;unified parallel c (upc)	David K. Newsom;Sardar F. Azari;Ahmad Anbar;Tarek A. El-Ghazawi	2013	2013 ACS International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2013.6616462	computer architecture;parallel computing;real-time computing;computer science;partitioned global address space	HPC	-4.410182698803395	54.45909939535067	57298
74f7a9e6a11956ae5b3f3cf921e2cda661dda571	systematic design and evaluation of a scalable reconfigurable multiplier scheme for hls environments	computer architecture adders multiplexing optimization hardware timing microprocessors;multiplying circuits application specific integrated circuits logic design;high level synthesis systematic design scalable reconfigurable multiplier scheme hls environments digital design more moore integration density radical more than moore solutions reconfigurable computing coarse grain reconfigurable components asic domain systematic design methodology dsp applications nonreconfigurable component architectures;multiplying circuits;logic design;high level synthesis reconfigurable computing reconfigurable multiplier runtime reconfiguration design methodologies;application specific integrated circuits	Modern digital design has been greatly forced to offer More-Moore integration densities and very high operation frequencies for demanding applications. In this search-for-performance race, alternative and less radical More-than-Moore solutions are emerging, like reconfigurable computing. Reconfigurable computing stands between hardware and software and promises to offer the former's performance alongside with the latter's flexibility. Research in the field deals with fine or coarse grain reconfigurable components and efficient ways to map applications onto them. In this paper, a systematic design methodology and evaluation of a coarse grain reconfigurable component targeting the ASIC domain is presented. The specific component is a morphable architecture, that works in mutually exclusive modes, offering different functionality in each mode. The novelty presented in this paper is a systematic evaluation of the scalability of the morphable component. Continuously functionally improved modes are evaluated for performance, area and power, in order to choose the best architecture for a number of widely used DSP applications. Overall, a power* performance improvement of up to 24% is reported and a power* area of up to 13% compared to conventional, non-reconfigurable component architectures.	application-specific integrated circuit;digital signal processor;high-level synthesis;logic synthesis;reconfigurable computing;scalability	Dimitris Bekiaris;Efstathios Sotiriou-Xanthopoulos;George Economakos;Dimitrios Soudris	2012	7th International Workshop on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2012.6322890	embedded system;parallel computing;logic synthesis;real-time computing;reconfigurable computing;computer science;operating system;application-specific integrated circuit	Arch	1.6776228440122807	49.99493603762355	57300
1bc12519b34a6218d38054e7176e923c7d303a18	"""comments on """"universal logic modules and their applications"""""""	pins;variable structure;logic design;application software;universal logic modules;large scale integration learning systems variable structure automata universal logic modules;logic circuits;learning automata;learning systems;learning system;springs;large scale integration;adaptive systems;variable structure automata;production systems;hardware	Research on universal logic modules has taken place with two directed aims: 1) the production of universal microcircuits, and 2) the design of adaptive logic systems. This correspondence comments on the progress in these two fields.	logic gate	Igor Aleksander	1971	IEEE Trans. Computers	10.1109/T-C.1971.223299	application software;discrete mathematics;logic synthesis;logic gate;computer science;theoretical computer science;adaptive system;mathematics;signature;production system;institutional model theory;algorithm	Embedded	8.232395467774882	50.24862384022882	57389
4375cbcf3672b00900502c24dab268465b7d492c	reconfigurable pda for the visually impaired using fpgas	computers;microprocessor;keyboards;32 bit microblaze processor;personal digital assistant;reconfigurable pda;intellectual property;embedded system on a chip;curtin university brailler;print to braille translation system;system on chip embedded systems field programmable gate arrays handicapped aids keyboards notebook computers;fpga;system on a chip;embedded system;braille keyboard controller;chip;embedded systems;visually impaired people;handicapped aids;note taking function;design and implementation;system on chip;registers;system on a chip assistive technology fpgas embedded systems;fpgas;assistive technology;blenkhorn algorithm;visual impairment;notebook computers;ip networks;intellectual property cores;field programmable gate arrays;field programmable gate arrays personal digital assistants keyboards system on a chip control systems hardware microprocessors cost function intellectual property embedded system;embedded systems reconfigurable pda fpga curtin university brailler personal digital assistant visually impaired people print to braille translation system braille keyboard controller blenkhorn algorithm microprocessor low cost keyboard note taking function intellectual property cores 32 bit microblaze processor embedded system on a chip;low cost keyboard;hardware	Curtin University Brailler (CUB) is a Personal Digital Assistant (PDA) for visually impaired people. Its objective is to make information in different formats accessible to people with limited visual ability. This paper presents the design and implementation of two modules: a print-to-Braille translation system and a Braille keyboard controller. The translator implements Blenkhornpsilas algorithm in hardware, liberating the microprocessor to perform other functions. The Braille keyboard controller along with a low cost keyboard provides users with a note-taking function. These modules are used as intellectual property (IP) cores coupled to a 32-bit MicroBlaze processor in an embedded system-on-a-chip (SoC). The system is a potential platform for the development of embedded systems to assist the visually impaired.	32-bit;algorithm;embedded system;field-programmable gate array;keyboard controller (computing);machine translation;microprocessor;personal digital assistant;system on a chip	Xuan Zhang;Cesar Ortega-Sanchez;Iain Murray	2008	2008 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2008.62	keyboard controller;system on a chip;embedded system;computer hardware;computer science;operating system;field-programmable gate array	EDA	6.63952908448248	48.59236580831952	57577
fe441f7f3947e57b4c84af6281972cb75195d8f6	sdrm: simultaneous determination of regions and function-to-region mapping for scratchpad memories	bin packing problem;embedded system;compilers;chip;low power;energy consumption;static code analysis;code overlay;static analysis;scratchpad memory;integer linear program	Many programmable embedded systems feature low power processorscoupled with fast compiler controlled on-chip scratchpad memories (SPMs) toreduce their energy consumption. SPMs are more efficient than caches in termsof energy consumption, performance, area and timing predictability. However,unlike caches SPMs need explicit management by software, the quality ofwhich can impact the performance of SPM based systems. In this paper, wepresent a fully-automated, dynamic code overlaying technique for SPMs basedon pure static analysis. Static analysis is less restrictive than profiling and canbe easily extended to general compiler framework where the time consumingand expensive task of profiling may not be feasible. The SPM code mappingproblem is harder than bin packing problem, which is NP-complete. Therefore weformulate the SPMcode mapping as a binary integer linear programming problemand also propose a heuristic, determining simultaneously the region (bin) sizesas well as the function-to-region mapping. To the best of our knowledge, thisis the first heuristic which simultaneously solves the interdependent problemsof region size determination and the function-to-region mapping. We evaluateour approach for a set of MiBench applications on a horizontally split I-cache and SPM architecture (HSA). Compared to a cache-only architecture (COA),the HSA gives an average energy reduction of 35%, with minimal performancedegradation. For the HSA, we also compare the energy results from our proposedSDRM heuristic against a previous static analysis based mapping heuristic andobserve an average 27% energy reduction.	best, worst and average case;bin packing problem;central processing unit;elegant degradation;embedded system;heterogeneous system architecture;heuristic;integer programming;interdependence;linear programming;np-completeness;optimizing compiler;scratchpad memory;set packing;static program analysis;super paper mario	Amit Pabalkar;Aviral Shrivastava;Arun Kannan;Jongeun Lee	2008		10.1007/978-3-540-89894-8_49	chip;compiler;bin packing problem;parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing;programming language;static analysis;static program analysis	EDA	-1.8301760140769645	52.93588436061144	57615
419d20de0c5f7a02f8a36b53634ef8d395e40cd7	application-informed platform evaluation for commercial-off-the-shelf dynamic voltage scaling	power aware computing electronic engineering computing;cots application informed platform evaluation commercial off the shelf dynamic voltage scaling mobile electronic device duty cycling critical energy reduction worst case dvs break even time;voltage control clocks hardware mathematical model dynamic voltage scaling software	As form-factor and lifetime constraints placed on mobile electronic devices grow more restrictive, techniques such as dynamic voltage scaling (DVS) and duty cycling can provide critical energy reductions. This work presents a model-based analysis of worst-case DVS break-even time, and introduces an application-agnostic metric for rapid side-by-side comparison of commercial-off-the-shelf (COTS) products being considered for DVS applications. This metric is intended to reduce time spent profiling multiple devices by enabling more informed processing platform selection earlier in the design cycle. To validate this modeling effort, a COTS platform is selected, prototyped, and evaluated for actual versus predicted break-even time.	best, worst and average case;duty cycle;dynamic voltage scaling;form factor (design);image scaling;profiling (computer programming)	Benjamin Boudaoud;John Lach;Harry C. Powell	2014	2014 21st IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2014.7050020	embedded system;real-time computing;simulation;engineering	EDA	-3.250515836260179	56.96785381918267	58213
fedde4725d4fbc3bed273cc8b6de7b9453b3201c	simulated annealing based datapath synthesis	thesis or dissertation;kb thesis scanning project 2015	The behavioural synthesis procedure aims to produce optimised register-transfer level datapath descriptions from an algorithmic problem definition, normally expressed in a high-level programming language. The procedure can be partitioned into a number of subtasks linked by a serial synthesis flow. Graph theoretic algorithms can be used to provide solutions to these subtasks. Many of these techniques, however, belong to a class of algorithm for which there is no exact solution computable in polynomial time. To overcome this problem, heuristics are used to constrain the solution space. The introduction of heuristics can cause the algorithm to terminate in a local cost minimum. This thesis supports a global formulation of the behavioural synthesis problem. An algorithm which can avoid local minima, simulated annealing, forms the basis of the synthesis system reported. A modular software system is presented in support of this approach. A novel data structure enables multiple degrees of optimisation freedom within the datapath solution space. Synthesis primitives, tightly coupled to a solution costing mechanism directed towards the prevalent datapath implementation technologies, form the core of the system. The software is exercised over small and large-scale synthesis benchmarks. The synthesis paradigm is extended by the provision of optimisation routines capable of supporting the generation of functional pipelines.	algorithm;benchmark (computing);computable function;data structure;datapath;feasible region;heuristic (computer science);high- and low-level;high-level programming language;mathematical optimization;maxima and minima;modular programming;pipeline (computing);programming paradigm;register-transfer level;simulated annealing;software system;terminate (software);theory;time complexity	John Paul Neil	1994			parallel computing;real-time computing;computer science;algorithm	EDA	-0.6198681366895492	51.91259162973939	58315
2e2069d388cff28363389c39999e1ae27d5dcb0c	controlling energy demand in mobile computing systems	energy;battery power;wireless;energy demand;mobile computing system;operating system;power management;voltage scaling	This lecture provides an introduction to the problem of managing the energy demand of mobile devices. Reducing energy consumption, primarily with the goal of extending the lifetime of battery-powered devices, has emerged as a fundamental challenge in mobile computing and wireless communication. The focus of this lecture is on a systems approach where software techniques exploit state-of-the-art architectural features rather than relying only upon advances in lower-power circuitry or the slow improvements in battery technology to solve the problem. Fortunately, there are many opportunities to innovate on managing energy demand at the higher levels of a mobile system. Increasingly, device components offer low power modes that enable software to directly affect the energy consumption of the system. The challenge is to design resource management policies to effectively use these c apabilities. The lecture begins by providing the necessary foundations, including basic energy terminology and widely accepted metrics, system models of how power is consumed by a device, and measurement methods and tools available for experimental evaluation. For components that offer low power modes, management policies are considered that address the questions of when to power down to a lower power state and when to power back up to a higher power state. These policies rely on detecting periods when the device is idle as well as techniques for modifying the access patterns of a workload to increase opportunities for power state transitions. For processors with frequency and voltage scaling capabilities, dynamic scheduling policies are developed that determine points during execution when those settings can be changed without harming quality of service constraints. The interactions and tradeoffs among the power management policies of multiple devices are discussed. We explore how the effective power management on one component of a system may have either a positive or negative impact on overall energy consumption or on the design of policies for another component. The important role that application-level involvement may play in energy management is described, with several examples of cross-layer cooperation. Application program interfaces (APIs) that provide information flow across the application-OS boundary are valuable tools in encouraging development of energy-aware applications. Finally, we summarize the key lessons of this lecture and discuss future directions in managing energy demand.	application programming interface;backup;central processing unit;dynamic voltage scaling;electronic circuit;interaction;mobile computing;mobile device;operating system;power glove;power management;quality of service;scheduling (computing);sensor	Carla Schlatter Ellis	2007		10.2200/S00089ED1V01Y200704MPC002	real-time computing;simulation;energy;computer science;operating system;mobile computing;wireless	Arch	-3.796495031616129	56.66484394397556	58463
0a9eb4032f98f09a705d63698406d0eff94e6106	development of ppram-link interface (plif) ip core for high-speed inter-soc communication	high speed communication standard;protocols;application programming interface ppram link interface ip core high speed inter soc communication high speed communication standard merged dram logic soc architecture ppram link standard physical logical layers api upper software layer ppram link interface ip family logical protocols subaction level communications fpga based pci to ppram link board inter pc ws communications parallel processing ram;electronic mail;standards;ppram link standard;api;information science;physical layer;logic;subaction level communications;physical logical layers;inter pc ws communications;ppram link interface ip family;standards development;application specific integrated circuits;integrated circuit interconnections;communication standards;ppram link interface ip core;logical protocols;random access storage;high speed inter soc communication;merged dram logic soc architecture;software standards;computer science;upper software layer;parallel processing ram;protocols standards application specific integrated circuits random access storage parallel processing parallel memories computer interfaces microprocessor chips;computer interfaces;protocols communication standards logic standards development physical layer software standards integrated circuit interconnections computer science information science electronic mail;high speed;fpga based pci to ppram link board;parallel processing;parallel memories;application programming interface;microprocessor chips	"""We are proposing """"PPRAM-Link"""": a new high-speed communication standard for merged-DRAM/logic SoC architecture. PPRAM-Link standard is composed of physical/logical layers and an API for the upper software layer, which are standardized by PPRAM Consortium. We developed a PPRAM-Link Interface IP family, or """"PLIF Core"""" that realizes logical protocols necessary for subaction-level communications, and it can be applied to various applications. In addition, we designed an FPGA-based PCI-to-PPRAM-Link board for inter-PC/WS communications."""	application programming interface;consortium;dynamic random-access memory;field-programmable gate array;semiconductor intellectual property core;system on a chip	Takanori Okuma;Koji Hashimoto;Kazuaki Murakami	2001		10.1145/370155.370257	embedded system;parallel processing;electronic engineering;parallel computing;real-time computing;application programming interface;telecommunications;information science;computer science;operating system;computer network	EDA	7.077703187537091	49.55427951468146	58629
7bc76ab4668a76ba607243e89358a5a7c1c0bb10	software in silicon in the oracle sparc m7 processor	databases;silicon;software;contracts;hardware;timing	1 SW in Silicon is custom hardware targeted at specific higher level functions traditionally implemented in software 2 Cloud based applications, especially analytics, offer many opportunities for SW in Silicon features 3 SW in Silicon in SPARC M7 provides gains in Performance, Power, Security and Memory Capacity 4 Oracle database automatically uses these SW in Silicon features. Other applications access through public API 5 Oracle is researching tighter HW-SW codesign, targeting deeper gains on a wider set of applications	application programming interface;oracle database;sparc;shattered world	Kathirgamar Aingaran;Sumti Jairath;David Lutz	2016	2016 IEEE Hot Chips 28 Symposium (HCS)	10.1109/HOTCHIPS.2016.7936220	parallel computing;real-time computing;computer science;operating system	Arch	3.8090887258886443	48.87127136462769	58669
0beb7e2e0fc9ece1f6e15d7af415b81a0d9a0c8e	system design for dsp applications using the masic methodology	design flow;design cycle;top-down iteration loop;expensive top-down iteration;system design;complex dsp system;functional modeling phase;masic methodology;implementation level;amba on-chip architecture;dsp applications;sw component;architectural decision;integrated circuit design;grammar;computation;reconfigurable computing;communication;system modeling;functional model;place and route;chip;speech processing;top down;system on a chip;codesign;hardware description languages	Expensive top-down iterations are often required in the design cycle of complex DSP systems. In this paper, we introduce two levels of abstraction in the design flow by systematically categorizing the architectural decisions. As a result, the top-down iteration loop is broken. We also present a technique to capture and inject the architectural decisions such that the system models can be created and simulated efficiently. The concepts are illustrated by a realistic speech processing example, which is implemented using the AMBA on-chip architecture. Our methodology offers a smooth path from the functional modeling phase to the implementation level, facilitates the reuse of HW and SW components, and enjoys existing tool support at thebackend.	categorization;design flow (eda);digital signal processor;iteration;principle of abstraction;speech processing;top-down and bottom-up design	Abhijit K. Deb;Axel Jantsch;Johnny Öberg	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		chip;system on a chip;co-design;embedded system;computer architecture;electronic engineering;real-time computing;systems modeling;reconfigurable computing;computer science;design flow;function model;operating system;computation;place and route;top-down and bottom-up design;speech processing;grammar;hardware description language;programming language;integrated circuit design;systems design	EDA	5.1794027684966	53.62076727806782	58675
e8949dc75e03ba40d0f0cd38ca2fa0f3fa20c479	a generic pixel distribution architecture for parallel video processing	reconfigurable architecture pixel distribution parallel video processing;streaming media computer architecture hardware field programmable gate arrays parallel processing writing ip networks;video signal processing convolution field programmable gate arrays image filtering multimedia computing parallel architectures;computer architecture;streaming media;writing;ip networks;field programmable gate arrays;parallel processing;code generation tool generic pixel distribution architecture parallel video processing i o data distribution neighbourhood operations parallel computing multimedia video processing domain reconfigurable computing hardware partitioning parallel architecture xilinx zynq zc706 fpga evaluation board video downscaler convolution filter parallel ips;hardware	I/O data distribution for neighbourhood operations processed in parallel computing dominates the multimedia video processing domain. Hardware designers are confronted with the challenge of architecture obsolescence due to the lack of flexibility to adapt the I/O system while upgrading the parallelism level. The usage of reconfigurable computing solves the problem partially with the capability of hardware partitioning according to the application requirements. Taking this aspect into consideration, we propose a generic I/O data distribution model dedicated to parallel video processing. Several parameters can be configured according to the required size of macro-block with the possibility to control the sliding step in both horizontal and vertical directions. The generated model is used as a part of the parallel architecture processing multimedia applications. We implemented our architecture on the Xilinx Zynq ZC706 FPGA evaluation board for two applications: the video downscaler (1:16) and the convolution filter. The efficiency of our system for distributing pixels among parallel IPs is demonstrated through several experiments. The experimental results show the decrease in the design effort using the code generation tool, the low hardware cost of our solution and how flexible is the model to be configured for different distribution scenarios.	code generation (compiler);convolution;experiment;field-programmable gate array;io.sys;input/output;microprocessor development board;parallel computing;pixel;real-time clock;reconfigurable computing;requirement;run time (program lifecycle phase);spmd;streaming media;systems design;vhdl;video processing	Karim M. A. Ali;Rabie Ben Atitallah;Saïd Hanafi;Jean-Luc Dekeyser	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032547	embedded system;parallel processing;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;writing;field-programmable gate array	HPC	3.899866654412301	46.73836740038979	58911
512fafac3a7d45c17d93f428194524510a418e6f	a seamless virtualization approach for transparent dynamical function mapping targeting heterogeneous and reconfigurable systems	reconfigurable system;high performance computer;program development	Future systems are not only heading towards increased parallelism, but also embrace heterogeneity and reconfigurability. We therefore present an approach targeting comfortable program development and execution, enabling full exploitation of the underlying hardware without burdening the application programmer with the details of the underlying hardware infrastructure. The approach employs lightweight resource virtualization by means of on-demand function resolution. By carefully extending the existing system infrastructure, the approach comes at virtually no cost and with highest compatibility to existing legacy code. The approach is suitable for a wide range of architectures from embedded systems to high-performance computing platforms.	seamless3d	Rainer Buchty;David Kramer;Fabian Nowak;Wolfgang Karl	2009		10.1007/978-3-642-00641-8_42	embedded system;parallel computing;real-time computing;computer science;operating system	HPC	-2.11896384694019	49.62629449581386	58931
0abd8375f65e3a130570e18d35b17bae21540ae9	experience in designing a large-scale multiprocessor using field-programmable devices and advanced cad tools	multiple vendors;board level design;design automation;cadence logic workbench;cad tools;field programmable devices;logic design;logic circuits;altera max plusii;chip;logic circuitry;circuit simulation;large scale;computational modeling;logic modelling smart models large scale multiprocessor field programmable devices advanced cad tools multiple vendors logic circuitry cad tools cadence logic workbench board level design altera max plusii;permission;logic modelling smart models;advanced cad tools;development systems;software tools;large scale systems design automation logic circuits field programmable gate arrays permission logic devices logic design computational modeling circuit simulation computer simulation;multiprocessing systems;field programmable gate arrays;logic cad;large scale multiprocessor;computer simulation;high speed;logic devices;large scale systems	This paper provides a case study that shows how a demanding application stresses the capabilities of today’s CAD tools, especially in the integration of products from multiple vendors. We relate our experiences in the design of a large, high-speed multiprocessor computer, using state of the art CAD tools. All logic circuitry is targeted to field-programmable devices (FPDs). This choice amplifies the difficulties associated with achieving a highspeed design, and places extra requirements on the CAD tools. Two main CAD systems are discussed in the paper: Cadence Logic Workbench (LWB) is employed for board-level design, and Altera MAX+plusII is used for implementation of logic circuits in FPDs. Each of these products is of great value for our project, but the integration of the two is less than satisfactory. The paper describes a custom procedure that we developed for integrating sub-designs realized in FPDs (via MAX+plusII) into our board-level designs in LWB. We also discuss experiences with Logic Modelling Smart Models, for simulation of FPDs and other types of chips.	computer-aided design;electronic circuit;field-programmability;level design;logic gate;multiprocessing;requirement;simulation;workbench	Stephen Dean Brown;Naraig Manjikian;Zvonko G. Vranesic;S. Caranci;A. Grbic;R. Grindley;M. Gusat;K. Loveless;Zeljko Zilic;Sinisa Srbljic	1996		10.1145/240518.240600	computer simulation;chip;embedded system;computer architecture;electronic engineering;logic synthesis;electronic design automation;logic gate;computer science;electrical engineering;operating system;computational model;field-programmable gate array;computer engineering	EDA	8.261950744914714	51.3667605039472	59264
69edb3cb88ff6c5c640c082459bfef4f4f27b33b	rtl datapath optimization using system-level transformations	canonization form high level synthesis system level transformations register transfer level rtl polynomial datapath univariate functional decomposition;system level transformations;polynomials;polynomial datapath;high level synthesis;canonization form;register transfer level rtl;register transfer level rtl datapath optimization system level transformations datapath designs polynomial computations computer graphics digital signal processing domains multivariate polynomial systems arithmetic operations univariate functional decomposition polynomial expressions z 2 m gaut high level synthesis tool optimized polynomials rtl datapath architectures sequential datapath architectures speed optimization mode clock cycles area optimization;univariate functional decomposition;circuit optimisation;polynomials computer architecture optimization methods clocks registers complexity theory;polynomials circuit optimisation high level synthesis	This paper describe a system-level approach to improve the area and delay of datapath designs that perform polynomial computations over Z2m, which are used in many applications such as computer graphics and digital signal processing domains. This approach optimizes the implementation of multivariate polynomial systems in terms of the number of arithmetic operations by performing optimization on a system level prior to high-level synthesis. Univariate functional decomposition of polynomial expressions and canonization form over Z2m are used in this method. We use GAUT high-level synthesis tool to generate RTL datapath architectures for the optimized polynomials. Experimental results on a set of benchmark applications with polynomial expressions show that this method outperforms conventional methods in terms of the area of the sequential datapath architectures in speed optimization mode with an average improvement of 25.81 %, and the required clock cycles in two modes of speed optimization and area optimization, with an average improvement of 23.48% and 38.24%, respectively.	algorithm;benchmark (computing);clock signal;computation;computer graphics;datapath;digital signal processing;high- and low-level;high-level synthesis;mathematical optimization;polynomial	Samaneh Ghandali;Bijan Alizadeh;Masahiro Fujita;Zainalabedin Navabi	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783341	computer architecture;finite state machine with datapath;parallel computing;computer science;theoretical computer science;high-level synthesis;polynomial	EDA	2.690122867143067	51.527340864681534	59274
6b93a667dd6405e1e8d32b1f28b226076610e2ec	design space exploration for low-power reconfigurable fabrics	digital signal processing;field programmable gate array;architectural design;computer aided design;design automation;multiplexing equipment application specific integrated circuits field programmable gate arrays logic cad;signal processing application low power reconfigurable fabric field programmable gate array fpga like programmability computer aided design cad application specific integrated circuit asic like power characteristic digital signal processing data width interconnection flexibility multiplexer cardinality usage;energy efficient;cad;signal design;multiplexer cardinality usage;data width;space exploration;interconnection flexibility;multiplexing equipment;multiplexing;low power;application specific integrated circuits;application specific integrated circuit;signal processing;integrated circuit interconnections;integrated circuit modeling;power optimization;signal processing application;fabrics;asic like power characteristic;low power reconfigurable fabric;design space exploration;field programmable gate arrays;coarse grained;logic cad;fpga like programmability;space exploration fabrics multiplexing digital signal processing integrated circuit interconnections integrated circuit modeling field programmable gate arrays signal design design automation application specific integrated circuits	This paper presents a parameterizable, coarse grained, reconfigurable fabric model that attempts to maintain field programmable gate array (FPGA)-like programmability and computer aided design (CAD), with application specific integrated circuit (ASIC)-like power characteristics for digital signal processing (DSP) style applications. Using this model, architectural design space decisions are explored in order to define an energy-efficient fabric. The impact on energy and performance due to the variation of different parameters such as data width and interconnection flexibility has been studied. The multiplexer cardinality usage has also been studied by mapping some of the signal processing applications onto the fabric. The results point to the use of power optimized 32-bit width computational elements interconnected by low cardinality multiplexers like 4:1 multiplexers	32-bit;application-specific integrated circuit;computer-aided design;design space exploration;digital signal processing;field-programmable gate array;interconnection;low-power broadcasting;mapper;multiplexer;routing	Gayatri Mehta;Raymond R. Hoare;Justin Stander;Alex K. Jones	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639484	embedded system;computer architecture;parallel computing;computer science;signal processing;application-specific integrated circuit;field-programmable gate array	EDA	2.8093149378724043	50.82287994912518	59344
663a934b8ca2aa39a33d8b19161daa7c066f7147	a power reduction technique with object code merging for application specific embedded processors	on chip bus;cache storage;cache;intellectual property;system on a chip;embedded systems;low power;estimation;integer programming;application specific integrated circuits;instruction memory power reduction technique object code merging application specific embedded processors frequently executed sequences instruction decompressor energy reductions;low power electronics;merging read only memory energy consumption application software system on a chip computer science power engineering and energy information science image restoration decoding;power reduction;circuit optimisation;microprocessor chips low power electronics embedded systems application specific integrated circuits cache storage circuit optimisation integer programming;embedded processor;read only memory;microprocessor chips	In this paper, a power reduction technique which merges frequently executed sequences of object codes into a set of single instructions is proposed. The merged sequence of object codes is restored by an instruction decompressor before decoding the object codes. The decompressor is implemented by a ROM. In many programs, only a few sequences of object codes are frequently executed. Therefore, merging these frequently executed sequences into a single instructions leads to a significant energy reduction. Our experiments with actual read only memory(ROM) modules and some benchmark program demonstrate significant energy reductions up to more than 65% at best case over a instruction memory without the object code merging.	benchmark (computing);best, worst and average case;experiment;object code	Tohru Ishihara;Hiroto Yasuura	2000		10.1145/343647.343871	system on a chip;embedded system;estimation;computer architecture;electronic engineering;parallel computing;real-time computing;integer programming;object code;cache;computer science;operating system;application-specific integrated circuit;programming language;read-only memory;intellectual property;low-power electronics	PL	-1.9079246942260162	52.62935790333606	59993
cada10971a53df4c475ab226228d33dbddfbbff0	hardware and software tools for the development of a micro-programmed microprocessor	software tool;automated process;mc68000 microprocessor;integrated circuit industry;error detection;micro-programmed microprocessor;correct microcode;microcode verification;functional description;integrated circuit;microprogramming;error detection and correction	This paper discusses the development and implementation of a number of hardware and software tools used in the design, development, and debugging of the microcode and nanocode for the MC68000 microprocessor. A functional description of these tools is included as well as an analysis of how each worked with respect to solving the desired problem (ie., generating correct microcode and nanocode). Peculiarities in the integrated circuit industry for microcode verification is considered as well as the automated process of error detection and correction.	debugging;error detection and correction;integrated circuit;microcode;microprocessor;motorola 68000	James Nash;Mike Spak	1979			computer architecture;parallel computing;error detection and correction;computer science;operating system;integrated circuit;microcode;programming language	EDA	7.521944127594245	51.69645332280605	60227
82ddc2fa74747ca891e360645b3cea3219dc46cc	design and evaluation of an energy-efficient dynamically reconfigurable architecture for wireless sensor nodes	wireless sensor network node;processor architecture;small energy constraint embedded system;energy efficient dynamically reconfigurable architecture;low energy;processor architecture energy efficient dynamically reconfigurable architecture wireless sensor network node small energy constraint embedded system energy consumption asic architecture;dynamic reconfiguration;wireless sensor networks application specific integrated circuits embedded systems integrated circuit design microprocessor chips;energy efficient;high energy;data processing;heterogeneous data;asic architecture;runtime;design space;wireless sensor node;embedded system;wireless sensor network;computer architecture;embedded systems;integrated circuit design;reconfigurable architecture;energy consumption;application specific integrated circuits;energy efficiency reconfigurable architectures wireless sensor networks energy consumption runtime application specific integrated circuits computer architecture embedded system costs hardware;coarse grained;wireless sensor networks;energy saving;microprocessor chips;hardware	We explore the design of a coarse-grained reconfigurable architecture for wireless sensor network nodes, which combines high energy efficiency with programmability and hence meets the requirements of small energy-constraint embedded systems. Its energy consumption, area, and performance are evaluated and compared to processor and ASIC architectures. Our case study particularly focuses on the question if the architecture concept of frequent dynamic reconfiguration of a small heterogeneous data path can lead to suitable system solutions for the target domain. To answer this, the effect of the reconfiguration overhead on total system efficiency is examined closely. As important result, our experiments show the low energy consumption achieved, the low reconfiguration overhead, and the specific region of the architecture in the design space between processors and ASICs. In particular, large energy-savings of factor 2 to 6 and speed-ups of factor 6 to 14 compared to processors are obtained on average. Our work shows the high suitability of frequent runtime reconfiguration of small coarse-grain data paths for the design of very efficient but yet programmable embedded systems platforms.	application-specific integrated circuit;central processing unit;embedded system;experiment;overhead (computing);reconfigurability;requirement	Heiko Hinkelmann;Peter Zipf;Manfred Glesner	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272268	embedded system;parallel computing;real-time computing;wireless sensor network;data processing;computer science	EDA	-1.2491423391398206	53.65140540085135	60813
b9491f6398c963d1df22f8301d3fe2b10b18ffba	system-level metrics for hardware/software architectural mapping	software metrics;hw sw architectures;hardware application software embedded system computer architecture software performance performance analysis delay estimation design methodology system performance modeling;hardware software architectural mapping;communication performance estimation system level metrics hardware software architectural mapping embedded systems design hw sw architectures optimal partitioning software performance estimation hardware performance estimation;application software;software performance evaluation;system performance;embedded system;communication performance estimation;software performance;system level metrics;chip;embedded systems design;computer architecture;embedded systems;software architecture;hardware performance estimation;systems analysis;optimal partitioning;performance analysis;software performance estimation;computer architecture software metrics embedded systems software performance evaluation systems analysis software architecture;modeling;delay estimation;hardware;design methodology	The current trend in Embedded Systems (ES) design is moving towards the integration of increasingly complex applications on a single chip, while having to meet strict market demands which force to face always shortening design times. In general, the ideal design methodology shall support the exploration of the highest possible number of alternatives (in terms of HW-SW architectures) starting in the early design stages as this will prevent costly correction efforts in the deployment phase. The present paper will propose a new methodology for tackling the design exploration problem, with the aim of providing a solution in terms of optimal partitioning with respect of the overall system performance.	embedded system;exploration problem;shattered world;software deployment;transform, clipping, and lighting	Fabrizio Ferrandi;Pier Luca Lanzi;Donatella Sciuto;Mara Tanelli	2004	Proceedings. DELTA 2004. Second IEEE International Workshop on Electronic Design, Test and Applications	10.1109/DELTA.2004.10060	chip;embedded system;software architecture;systems analysis;computer architecture;application software;real-time computing;systems modeling;software performance testing;design methods;computer science;computer performance;software metric	EDA	2.7075769630103035	55.228636672019356	60855
6c60aa5fe3b0b165b424e4b737f2cd35579cc386	amba to socwire network on chip bridge as a backbone for a dynamic reconfigurable processing unit	software;resource utilization;field programmable gate array;random access memory;codecs;amba;dynamic reconfiguration;network on chip;direct memory access;reconfigurable architectures;bridges;fpga;dynamic reconfigurable processing unit;field programmable gate arrays codecs bridges process control switches random access memory software;public domain software;telecommand processing;system on chip;dynamic reconfigurable architectures;process control;sequential processor;socwire network on chip bridge;spacecraft instruments socwire network on chip bridge dynamic reconfigurable processing unit amba telecommand processing hardware design system on chip fpga dynamic reconfigurable architectures sequential processor leon processor open source design bus based communication architecture ahb2socw bridge direct memory access resource utilization;hardware design;reconfigurable architectures field programmable gate arrays microprocessor chips network on chip public domain software;open source design;field programmable gate arrays;ahb2socw bridge;bus based communication architecture;switches;spacecraft instruments;leon processor;microprocessor chips	Instruments on spacecrafts or even complete payload data handling units are typically controlled by a dedicated data processing unit. This data processing unit exercises control of subsystems and telecommand processing as well as processing of acquired science data. With increasing detector resolutions and measurement speeds the processing demands are rising rapidly. To fulfill these increasing demands under the constraints of limited power budgets, a dedicated hardware design as a System on Chip (SoC), e.g. in a FPGA, has been shown to be a viable solution. In previous papers [1], [2] we have presented our Network on Chip (NoC) solution SoCWire as a higly efficient and reliable approach for a dynamic reconfigurable architectures. However, the control task still requires a sequential processor which is able to execute software. The LEON processor is a processor that is available in a fault tolerant version suitable for space applications and is accessible as an open source design. This paper presents an efficient solution for a combined NoC and classic processor bus-based communication architecture, i.e. the AHB2SOCW bridge as an efficient connection between a SoCWire network and a LEON processor bus systems. Direct memory access enables the AHB2SOCW bridge to operate efficiently. The resource utilization and obtainable data rates are presented as well as an outlook for the intended target application, which is an efficient SoC controller for a reconfigurable processing platform based on FPGAs.	antifuse;application-specific integrated circuit;codec;data rate units;direct memory access;duplex (telecommunications);fault tolerance;field-programmable gate array;internet backbone;leon;megabit;microsoft outlook for mac;network on a chip;open design;open-source software;payload (computing);requirement;system on a chip	Holger Michel;Frank Bubenhagen;Björn Fiethe;Harald Michalik;Björn Osterloh;Wayne Sullivan;Alex Wishart;Jørgen Ilstad;Sandi Habinc	2011	2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2011.5963941	embedded system;parallel computing;real-time computing;computer science;operating system;process control;field-programmable gate array	Arch	2.0128397548627115	48.686137096089396	60882
13892f17b42d54e0b0cf873837a0b380a3f62ce7	a width expansion of mmx/simd processing architecture on an fpga			field-programmable gate array;simd	Raymond R. Hoare;D. Swope;S. Bailey	2002			mmx;field-programmable gate array;architecture;parallel computing;computer architecture;computer science;simd	Arch	4.927360769057911	48.90428724379475	60952
3198546e00928e3be7fc5431b3f2bf00dd27bd92	rs-fdra: a register sensitive software pipelining algorithm for embedded vliw processors	optimising compilers;vliw processor;registers embedded software pipeline processing software algorithms vliw signal processing algorithms optimizing compilers delay embedded system software performance;resource constraint;compilers optimisation;rs fdra;optimizing compiler;vliw processors;parallel programming optimising compilers pipeline processing embedded systems parallel architectures;parallel programming;embedded system;software performance;register sensitive force directed retiming algorithm;vliw;embedded systems;register sensitive software pipelining algorithm;parallel architectures;registers;embedded vliw processors;software algorithms;code size;latency;time critical segments rs fdra register sensitive software pipelining algorithm embedded vliw processors register sensitive force directed retiming algorithm compilers optimisation latency resource constraints;software pipelining;time critical segments;signal processing algorithms;optimizing compilers;pareto optimality;retiming;pipeline processing;embedded software;resource constraints	The paper proposes a novel software-pipelining algorithm, Register Sensitive Force Directed Retiming Algorithm (RS-FDRA), suitable for optimizing compilers targeting embedded VLIW processors. The key difference between RS-FDRA and previous approaches is that our algorithm can handle code size constraints along with latency and resource constraints. This capability enables the exploration of pareto “optimal” points with respect to code size and performance. RS-FDRA can also minimize the increase in “register pressure” typically incurred by software pipelining. This ability is critical since, the need to insert spill code may result in significant performance degradation. Extensive experimental results are presented demonstrating that the extended set of optimization goals and constraints supported by RS-FDRA enables a thorough compiler-assisted exploration of trade-offs among performance, code size, and register requirements, for time critical segments of embedded software components.	algorithm;central processing unit;component-based software engineering;elegant degradation;embedded software;embedded system;mathematical optimization;optimizing compiler;pareto efficiency;pipeline (computing);reed–solomon error correction;register allocation;requirement;retiming;software pipelining	Cagdas Akturan;Margarida F. Jacome	2001		10.1145/371636.371681	computer architecture;parallel computing;real-time computing;computer science	EDA	-3.6631230643366637	51.95338202805105	61050
ac924589f32d23c0eebe2173a77e1cc732f351b9	vst: a virtual stress testing framework for discovering bugs in ssd flash-translation layers		Flash translation layers (FTLs) are the core embedded software (also known as firmware) of NAND flash-based solid-state drives (SSDs). The relentless pursuit of high-performance SSDs renders FTLs increasingly complex and intricate. Therefore, testing and validating FTLs are crucial and challenging tasks. Directly testing and validating FTLs on SSD hardware are common practices though, they are time-consuming and cumbersome because 1) the testing speed is limited by the hardware speed of SSDs and 2) just reproducing bugs can be challenging, let alone locating and root causing the bugs. This work presents virtual stress testing (VST), a simulation framework to enable executing SSD FTLs on PCs or servers against virtual SRAM, DRAM, and flash emulated by host-side main memory. FTL function calls, such as moving data from flash to DRAM, are served by the VST framework. Therefore, VST can test FTLs without SSD hardware requirements nor SSD speed limitations, and root causing bugs becomes manageable tasks. We apply VST to representative SSD design, OpenSSD, which is actively utilized and maintained by SSD and FTL communities. Experimental results show that VST can test FTLs at a speed up to 375 GB/s, which is several hundred times faster than directly testing FTLs on SSD hardware. Moreover, we successfully discover seven new FTL bugs in the OpenSSD design using VST, which is a solid evidence of VST's bug-discovering effectiveness.	adobe flash;central processing unit;computer data storage;dynamic random-access memory;embedded software;embedded system;emulator;ftl: faster than light;firmware;flash memory;gigabyte;little big adventure;multi-core processor;rendering (computer graphics);requirement;simulation;software bug;solid-state drive;speedup;static random-access memory;stress testing	Ren-Shuo Liu;Yun-Sheng Chang;Chih-Wen Hung	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203790	real-time computing;firmware;computer science;parallel computing;stress testing;static random-access memory;speedup;embedded software;systems simulation;nand gate;server	EDA	-3.8849541159127727	46.60135606527919	61164
9c4e025853225baa5ccf9398ffb593374db4a0d4	parallelizing fpga technology mapping using graphics processing units (gpus)	data parallel;kernel;graph based algorithm;paper;graph based algorithm fpga technology graphics processing unit gpu;hardware description languages coprocessors field programmable gate arrays;hardware description languages;gpu;fpga;coprocessors;logic gates;table lookup graphics processing unit kernel instruction sets logic gates delay field programmable gate arrays;fpga technology;graphic processing unit;algorithms;place and route;computer science;field programmable gate arrays;technology mapping;graphics processing unit;table lookup;instruction sets	GPUs are becoming an increasingly attractive option for obtaining performance speedups for data-parallel applications. FPGA technology mapping is an algorithm that is heavily data parallel; however, it has many features that make it unattractive to implement on a GPU. The algorithm uses data in irregular ways since it is a graph-based algorithm. In addition, it makes heavy use of constructs like recursion which is not supported by GPU hardware. In this paper, we take a state-of-the-art FPGA technology mapping algorithm within Berkeley’s ABC package and attempt to parallelize it on a GPU. We show that runtime gains of 3.1x are achievable while maintaining identical quality as demonstrated by running these netlists through Altera’s Quartus II place-and-route tool.	algorithm;altera quartus;central processing unit;graphics processing unit;parallel computing;place and route;recursion	Doris Chen;Deshanand P. Singh	2010	2010 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2010.33	embedded system;computer architecture;parallel computing;computer science;theoretical computer science;operating system;field-programmable gate array	EDA	-0.6745621716158515	46.46347473798668	61213
ee3e3443d771a6912c82e5fcd08b91d44f7fa4ab	microarchitectural optimization by means of reconfigurable and evolvable cache mappings	software;nature inspired optimization microarchitectural optimization reconfigurable cache mapping evolvable cache mapping multicore architecture many core architecture reconfigurable computing parallelization degrees processor fpga memory to cache address mapping function dynamic reconfiguration;tuning;tuning field programmable gate arrays software;field programmable gate arrays;reconfigurable architectures cache storage field programmable gate arrays multiprocessing systems parallel architectures	Physical limits are pushing chip manufacturer towards multi- and many-core architectures to maintain the progress of computing power. This trend has also emphasized reconfigurable computing, which enables for even higher parallelization degrees. Reconfigurable computing is often used together with a conventional processor to accelerate highly specific applications. However, exploiting dynamically reconfigurable systems for microarchitectural optimization is a novel research area. This paper presents for the first time an FPGA-based implementation of a processor that can reconfigure and adapt its own memory-to-cache address mapping function at runtime by means of dynamic reconfiguration and nature-inspired optimization. In experiments we can achieve up to 7.8% better execution times compared to a processor with a conventional cache mapping function.	experiment;field-programmable gate array;manycore processor;mathematical optimization;microarchitecture;parallel computing;reconfigurability;reconfigurable computing;run time (program lifecycle phase)	Nam Ho;Abdullah Fathi Ahmed;Paul Kaufmann;Marco Platzner	2015	2015 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2015.7231178	embedded system;pipeline burst cache;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;cache algorithms;field-programmable gate array	EDA	-0.2496258223640336	49.840443057633934	61330
c631f922298466d1d9c2ae8be10527718e005c99	an optimal lower-bound algorithm for the high-level synthesis scheduling problem	lower bound algorithm;resource constraint;scheduling algorithm high level synthesis optimal scheduling delay processor scheduling algorithm design and analysis hardware flow graphs helium software engineering;search space;branch and bound search;exact solution;search method;resource constraints lower bound algorithm high level synthesis scheduling problem minimum time scheduling branch and bound search;high level synthesis;optimal scheduling;scheduling;scheduling problem;tree searching high level synthesis scheduling;tree searching;branch and bound;lower bound;minimum time scheduling;resource constraints	An optimal scheduling algorithm in high-level synthesis is described, which searches for the minimum time scheduling under given resource constraints. The algorithm can substantially reduce the computational effort required to obtain the exact solution using a branch-and-bound search method and some novel bounding techniques to prune the unfruitful search space. Some time properties are explored to enlighten these bounding techniques. Experimental results on several benchmarks with varying resource constraints show our algorithm is both efficient and effective	algorithm;benchmark (computing);branch and bound;computation;high- and low-level;high-level synthesis;scheduling (computing)	Geguang Pu;Jifeng He;Zongyan Qiu	2006	2006 IEEE Design and Diagnostics of Electronic Circuits and systems	10.1109/DDECS.2006.1649599	fair-share scheduling;job shop scheduling;mathematical optimization;real-time computing;dynamic priority scheduling;computer science;distributed computing;upper and lower bounds;high-level synthesis;scheduling;branch and bound	EDA	0.23634612951329464	56.921709255111104	61388
41b92afa31dc00d9708b70242d78e6e3ed4ebfa8	matisse: a system-on-chip design methodology emphasizing dynamic memory management	protocols;dynamic memory management;object oriented methods;object oriented language;programming language;real time;hardware synthesis;telecommunication computing;embedded system;chip;atm system on chip design methodology dynamic memory management matisse design environment data flow behavior intensive data storage intensive data transfer real time requirements embedded single chip hardware software implementation memory architecture exploration timing constraints embedded system specifications high level programming language data abstraction telecom protocol processing systems;integrated circuit design;data storage;system on chip;design environment;memory architecture;data structures;data abstraction;vlsi;circuit cad;vlsi virtual storage real time systems data structures object oriented methods circuit cad asynchronous transfer mode telecommunication computing protocols integrated circuit design;power consumption;data flow;virtual storage;communication synthesis;asynchronous transfer mode;software implementation;real time systems;design methodology;system on a chip design methodology hardware control systems real time systems bridges embedded software memory management memory architecture timing;time constraint	is a design environment intended for developing systems characterized by a tight interaction between control and data-flow behavior, intensive data storage and transfer, and stringent real-time requirements. Matisse bridges the gap from a system specification, using a concurrent object-oriented language, to an optimized embedded single-chip hardware/software implementation. Matisse supports stepwise exploration and refinement of dynamic memory management, memory architecture exploration, and gradual incorporation of timing constraints before going to traditional tools for hardware synthesis, software compilation, and inter-processor communication synthesis. With this approach, specifications of embedded systems can be written in a high-level programming language using data abstraction. Application of Matisse on telecom protocol processing systems in the ATM area shows significant improvements in area usage and power consumption.	atm turbo;abstraction (software engineering);compiler;computer data storage;dataflow;embedded system;high- and low-level;high-level programming language;memory management;real-time transcription;refinement (computing);requirement;stepwise regression	Diederik Verkest;Julio Leao da Silva;Chantal Ykman-Couvreur;Kris Croes;Miguel Corbalan;Sven Wuytack;Francky Catthoor;Gjalt G. de Jong;Hugo De Man	1999	VLSI Signal Processing	10.1023/A:1008002332109	chip;system on a chip;embedded system;data flow diagram;communications protocol;computer architecture;parallel computing;real-time computing;data structure;design methods;computer science;operating system;asynchronous transfer mode;computer data storage;very-large-scale integration;programming language;object-oriented programming;integrated circuit design	EDA	4.56829947909821	53.47913456593113	61389
bd57879748b9b5380e632985fccb6a28b204bc0e	a scheduling method for synchronous communication in the bach hardware compiler	throughput of synthesized circuits;yarn;clocks;sequential circuits;hardware description languages;communication complexity;large system lsi;scheduling method;high level synthesis;large scale integration;control system synthesis;system design;scheduling;large scale integration high level synthesis circuit layout cad logic partitioning hardware description languages communication complexity sequential circuits;high speed circuit bach hardware compiler synchronous communication between threads scheduling method behavioral bach c description prescheduling large system lsi throughput of synthesized circuits;bach hardware compiler;circuit layout cad;prescheduling;high speed circuit;synchronous communication;logic partitioning;synchronous communication between threads;algorithm design and analysis;circuit synthesis;behavioral bach c description;large scale systems;hardware circuit synthesis scheduling algorithm design and analysis yarn costs large scale systems control system synthesis timing clocks;hardware;timing	Abstract − In this paper, we propose a scheduling method for synchronous communication between threads in the Bach hardware compiler. In this method, all communications are extracted from a behavioral Bach-C description and statically prescheduled to synchronize communications between threads if possible. Then all the operations and communications of each thread are synthesized independently according to the prescheduling result. Consequently, we can synthesize large system LSIs efficiently, because we do not need to synthesize the whole system descriptions at once to synchronize communications. Experimental results show that our method improves throughput of synthesized circuits and is applicable to large systems designed with the Bach hardware compiler.	clock signal;electronic circuit;high-level synthesis;integrated circuit;interconnection;overhead (computing);scheduling (computing);silicon compiler;systems design;throughput	Ryoji Sakurai;Mizuki Takahashi;Andrew Kay;Akihisa Yamada;Tetsuya Fujimoto;Takashi Kambe	1999		10.1109/ASPDAC.1999.759993	embedded system;algorithm design;computer architecture;electronic engineering;parallel computing;real-time computing;computer science;operating system;asynchronous communication;communication complexity;sequential logic;hardware description language;high-level synthesis;programming language;scheduling;systems design	EDA	1.5269295545134784	53.478390727469694	61479
273a704dbdf769d3b104991dc1f2b30694ff1677	a novel physical defects recovery technique for fpga-ip cores	performance evaluation;multiplexing equipment;fpga;network routing;faulty fpga performance degradation physical defects recovery technique fpga ip cores fpga fault detection replacement rerouting fault point avoidance recovery time test time fault detection method routing tools placement tools fault source avoidance tile level avoidance multiplexer level avoidance faulty muxes test configurations;fault tolerant computing;logic testing;circuit faults tiles multiplexing routing field programmable gate arrays fault detection testing;performance evaluation fault tolerant computing field programmable gate arrays logic testing microprocessor chips multiplexing equipment network routing;fault recovery fpga;field programmable gate arrays;fault recovery;microprocessor chips	FPGA fault detection consumes a great deal of test time compared with ASICs because FPGAs have complex structures. Re-placement and re-routing must be performed to avoid fault points, which causes an increase in recovery time and degrades performance. Therefore, we propose a fault detection method and develop placement and routing tools to avoid fault sources in tile and multiplexer level avoidance, respectively. In the evaluation, the detection method diagnosed faulty MUXes with six test configurations. We found that the performance of a faulty FPGA slightly decreased by 2% compared with a normal FPGA in multiplexer level avoidance.	application-specific integrated circuit;elegant degradation;fault detection and isolation;field-programmable gate array;multiplexer;place and route;routing;semiconductor intellectual property core	Yuki Nishitani;Kazuki Inoue;Motoki Amagasaki;Masahiro Iida;Morihiro Kuga;Toshinori Sueyoshi	2012	2012 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2012.6416766	embedded system;real-time computing;fault coverage;computer science;stuck-at fault;field-programmable gate array	EDA	7.3947086398064386	59.143109531664415	61548
579825009b6b3e657043c5cb05657deec9db7399	scheduling reconfiguration activities of run-time reconfigurable rtos using an aperiodic task server	systeme temps reel;field programmable gate array;haute performance;systeme embarque;integrated circuit;execution time;reconfigurable computing;flexibilidad;reconfigurable architectures;real time operating system;circuito integrado;red puerta programable;reseau porte programmable;embedded system;general purpose processor;embedded systems;run time reconfigurable;scheduling;real time scheduling;alto rendimiento;hybrid architecture;temps execution;flexibilite;real time system;procesador;sistema tiempo real;processeur;tiempo ejecucion;high performance;architecture reconfigurable;processor;ordonnancement;circuit integre;flexibility;reglamento	Reconfigurable computing based on hybrid architectures, comprising general purpose processor (CPU) and Field Programmable Gate Array (FPGA), is very attractive because it can provide high computational performance as well as flexibility to support today's embedded systems requirements. However, the relative high reconfiguration costs often represent an obstacle when using such architectures for run-time reconfigurable systems. In order to overcome this barrier the used real-time operating system must explicitly respect the reconfiguration time. In such systems, the reconfiguration activities need to be carried out during run-time without causing the application tasks to miss their dead-lines. In this paper, we show how we model these activities as aperiodic jobs. Therefore, we apply the server-based method from the real-time schedule theory to the scheduling of aperiodic activities.	real-time operating system;scheduling (computing)	Marcelo Götz;Florian Dittmann	2006		10.1007/11802839_33	embedded system;parallel computing;real-time computing;real-time operating system;reconfigurable computing;computer science;operating system;integrated circuit;scheduling;field-programmable gate array	Robotics	-0.5365459761221489	54.28196819868533	61556
b9591f747bc41da175b010baeee7edc0f9f0cca5	low power dsp's for wireless communications (embedded tutorial session)	processor architecture;digital signal processing;communication system;programmable processors;building block;wireless communication;low power;wireless communications;memory architecture;viterbi algorithm;fir filter;digital signal processor;architectures;turbo code	Wireless communications and more specifically, the fast growing penetration of cellular phones and cellular infrastructure are the major drivers for the development of new programmable Digital Signal Processors (DSPs). In this tutorial, an overview will be given of recent developments in DSP processor architectures, that makes them well suited to execute computationally intensive algorithms typically found in communications systems. DSP processors have adapted instruction sets, memory architectures and data paths to execute compute intensive communications algorithms efficiently and in a low power fashion. Basic building blocks include convolutional decoders (mainly the Viterbi algorithm), turbo coding algorithms, FIR filters, speech coders, etc. This is illustrated with examples of different commercial and research processors. Please note that the authors do not endorse the processors used in this tutorial. These processors are used to illustrate how different solutions are proposed for the same problem.	central processing unit;digital signal processor;embedded system;finite impulse response;microprocessor;mobile phone;turbo code;viterbi algorithm	Ingrid Verbauwhede;Chris Nicol	2000		10.1145/344166.344647	embedded system;electronic engineering;parallel computing;real-time computing;telecommunications;computer science;operating system;wireless	EDA	2.9605941864420258	48.006185927960146	61914
37e374471f3dc47c630b40075a13df14d5583a70	exploiting performance counters to predict and improve energy performance of hpc systems	systeme d exploitation;architectures materielles;high performance computing;systemes embarques;reseaux et telecommunications;energy performance;hardware performance counters;power consumption;green it	Hardware monitoring through performance counters is available on almost all modern processors. Although these counters are originally designed for performance tuning, they have also been used for evaluating power consumption. We propose two approaches for modelling and understanding the behaviour of high performance computing (HPC) systems relying on hardware monitoring counters. We evaluate the effectiveness of our systemmodelling approach considering both optimizing the energy usage of HPC systems and predicting HPC applications’ energy consumption as target objectives. Although hardware monitoring counters are used for modelling the system, other methods – including partial phase recognition and cross platformenergy prediction – are used for energy optimization and prediction. Experimental results for energy prediction demonstrate that we can accurately predict the peak energy consumption of an application on a target platform; whereas, results for energy optimization indicate that with no a priori knowledge of workloads sharing the platform we can save up to 24% of the overall HPC system’s energy consumption under benchmarks and real-life workloads.		Ghislain Landry Tsafack Chetsa;Laurent Lefèvre;Jean-Marc Pierson;Patricia Stolf;Georges Da Costa	2014	Future Generation Comp. Syst.	10.1016/j.future.2013.07.010	green computing;embedded system;supercomputer;parallel computing;real-time computing;simulation;computer science;operating system	HPC	-3.301147788264947	55.88249900460104	62031
257fbb0d992e473a289a622a5ce150ff66491cde	embedded floating-point units in fpgas	scientific application;floating point unit;fpga;fpga architecture;floating point;floating point arithmetic;fpu	Due to their generic and highly programmable nature, FPGAs provide the ability to implement a wide range of applications. However, it is this nonspecific nature that has limited the use of FPGAs in scientific applications that require floating-point arithmetic. Even simple floating-point operations consume a large amount of computational resources. In this paper, we introduce embedding floating-point multiply-add units in an island style FPGA. This has shown to have an average area savings of 55.0% and an average increase of 40.7% in clock rate over existing architectures.	clock rate;computational resource;embedded system;field-programmable gate array;multiply–accumulate operation	Michael J. Beauchamp;Scott Hauck;Keith D. Underwood;Karl S. Hemmert	2006		10.1145/1117201.1117204	floating-point unit;embedded system;double-precision floating-point format;parallel computing;computer hardware;computer science;floating point;operating system	Arch	1.3801028404792168	47.20787419696384	62219
ad4f269300d9a79a28c6fcdcdab23c976d09e0b9	timing driven c-slow retiming on rtl for multicores on fpgas		In this paper C-Slow Retiming (CSR) on RTL is discussed. CSR multiplies the functionality of cores by adding the same number of registers into each path. The technique is ideal for FPGAs with their already existing registers. Previously publications are limited to adding registers on netlist level, which generates a lot of system verification problems and which is assumed to be the major drawback to use this technology in the modern multicore times. The paper shows how CSR can efficiently be done with timing driven automatic RTL modification. The methodology provided with this paper can be used as guidance for using CSR in high level synthesis (HLS). The paper shows the results of a CSR-ed complex RISC core on RTL implemented on FPGAs.	field-programmable gate array;high-level programming language;high-level synthesis;multi-core processor;netlist;processor register;retiming;semiconductor intellectual property core	Tobias Strauch	2013		10.3233/978-1-61499-381-0-515	computer architecture;parallel computing;real-time computing	EDA	2.366510162093752	50.09486645041586	62375
ba71c8099bb5d081206f3cf677256951d0d35af8	modula-2 based multitasking environment for a flexible biprocessor controller		This paper presents the most important issues of a realtime software package developed for the industrial multiloop controller MMC-90. MMC-90 is a &processor system based on the concept of a two-level controller. On the lower level of complexity, the base control processor performs the basic control functions. On the higher level of complexity, the flexible control coprocessor executes complex functions on request from the control tasks on the base control processor. All the software on the control coprocessor is written in Modula-2. The corresponding multitasking environment was designed as a set of Modula-2 libraries at various horizontal and vertical decomposition /eve/s. On the control coprocessor a preemptive, priority based scheduler as well as a high level interprocess communication and synchronization mechanism has been developed. Communication between the processors is realized by means of a fast bi-directional communication channel, which is implemented via dualport RAM. The developed system opens the door to applications of modern control algorithms in industrial practice.	algorithm;central processing unit;channel (communications);computer multitasking;control function (econometrics);coprocessor;high-level programming language;inter-process communication;library (computing);modula-2;preemption (computing);random-access memory;scheduling (computing)	Giovanni Godena;Janko Petrovcic	1993	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(93)90075-I	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;programming language	Embedded	-2.4854896877000203	48.533945433397896	62511
2186f09631cb90ceae1f8311ca53d61bc30d09c2	formal verification of superscale microprocessors with multicycle functional units, exception, and branch prediction	microprocessors;design automation;branch prediction;computer aided instruction;logic;interconnect testing;field programmable gate arrray;formal verification;permission;equality with uninterpreted functions;formal verification microprocessors delay logic permission computer science predictive models modems design automation computer aided instruction;superscalar processor;predictive models;modems;computer science;functional unit;hierarchical test	We extend the Burch and Dill flushing technique [6] for formal verification of microprocessors to be applicable to designs where the functional units and memories have multicycle and possibly arbitrary latency. We also show ways to incorporate exceptions and branch prediction by exploiting the properties of the logic of Positive Equality with Uninterpreted Functions [4][5]. We study the modeling of the above features in different versions of dual-issue superscalar processors.	arm architecture;approximation;boolean satisfiability problem;branch predictor;central processing unit;computational complexity theory;dlx;embedded system;exception handling;formal verification;hardware description language;heuristic (computer science);high- and low-level;indeterminacy in concurrent computation;itanium;microprocessor;mobile phone;pipeline (computing);speculative execution;superscalar processor;uninterpreted function;verilog;vertex-transitive graph	Miroslav N. Velev;Randal E. Bryant	2000		10.1145/337292.337331	embedded system;computer architecture;electronic engineering;parallel computing;electronic design automation;formal verification;computer science;theoretical computer science;operating system;programming language;logic;algorithm	EDA	7.927643218574175	52.49437705449627	62572
92e1c78a12b2f5ca52f881d6b7ae28eeebc3beca	hardware software partitioning using genetic algorithm	hardware software co design;cad;search algorithm;constraint satisfaction problem hardware software partitioning genetic algorithm hardware software co design cad embedded systems specification partitioning;genetic algorithms software engineering high level synthesis;software engineering;embedded system;embedded systems;high level synthesis;hardware software partitioning;genetic algorithm;genetic algorithms;constraint satisfaction problem;specification partitioning;hardware genetic algorithms costs software algorithms embedded software design automation embedded system partitioning algorithms algorithm design and analysis computer science;software implementation	Hardware Software Co-Design is gaining importance with the advent of CAD for embedded systems. A key phase in such designs is partitioning the specification into hardware and software implementation sets. The problem being combinatorically explosive, seveml greedy search algorithms have been proposed for Hardware Software Partitioning. In this paper, we model the Hardware Software Partitioning problem as a Constraint Satisfaction Problem (CSP), and present a Genetic algorithm based approach to solve the CSP in order to obtain the partitioning solution.	computer hardware;computer-aided design;constraint satisfaction problem;embedded system;genetic algorithm;greedy algorithm;search algorithm	Debanjan Saha;Anupam Basu;Raj S. Mitra	1997		10.1109/ICVD.1997.568069	embedded system;computer architecture;real-time computing;genetic algorithm;computer science;theoretical computer science;hardware architecture	EDA	9.722951267098708	49.20477150766883	62621
1027c8b45154c5a8336a7a6de7bccc7ac12d72e7	dsp address optimization using evolutionary algorithms	assignment problem;code optimization;search space;performance improvement;memory controller;digital signal processor;evolutionary algorithm;sdram;address mapping	Offset assignment has been studied as a highly effective approach to code optimization in modern digital signal processors (DSPs). In this paper, we propose two evolutionary algorithms to solve the general offset assignment problem with k address registers and an arbitrary auto-modify range. These algorithms differ from previous algorithms by having the capability of visiting the entire search space. We implement and analyze a variety of existing general offset assignment algorithms and test them on a set of standard benchmarks. The algorithms we propose can achieve a performance improvement of up to 31% over the best existing algorithm. We also achieve an average of 14% improvement over the union of recently proposed algorithms.	assignment problem;benchmark (computing);central processing unit;digital signal processor;evolutionary algorithm;mathematical optimization;program optimization	Sean Leventhal;Lin Yuan;Neal K. Bambha;Shuvra S. Bhattacharyya;Gang Qu	2005		10.1145/1140389.1140399	digital signal processor;parallel computing;real-time computing;computer science;theoretical computer science;operating system;evolutionary algorithm;program optimization;assignment problem;memory controller;weapon target assignment problem;memetic algorithm	EDA	-0.6324862579305128	52.175837017386826	62708
c34b5cf268985ffc22d6b998fc5820f499ec0142	an instruction throughput model of superscalar processors	analytical models;modeling of computer architecture;architectural design;design tool;modeling technique;semiconductor technology;application software;analytical modeling;cycle accurate simulators;design space;modeling techniques;out of order;process design;pipeline processors;program processors analytical models computer architecture biological system modeling mathematical model computational modeling parallel processing;computer architecture;computational modeling;modeling techniques modeling of computer architecture pipeline processors;complex system;superscalar processors;architecture evaluation;performance model;architectural evaluation methods;simplescalar out of order simulator;superscalar processor;performance prediction;predictive models;space technology;time to market;simplescalar out of order simulator instruction throughput model superscalar processors semiconductor technology cycle accurate simulators architectural evaluation methods;model of computation;superscalar architectures;microprocessor chips;throughput;instruction throughput model	Advances in semiconductor technology enable larger processor design spaces, leading to increasingly complex systems. At an initial stage, designers must evaluate many architecture design points to achieve a suitable design. Currently, most architecture exploration is performed using cycle accurate simulators. Although accurate, these tools are slow, thus limiting a comprehensive design search. The vast design space of today's complex processors and time to market economic pressures motivate the need for faster architectural evaluation methods. This paper presents a superscalar processor performance model that enables rapid exploration of the architecture design space for superscalar processors. It supplements current design tools by quickly identifying promising areas for more thorough and time consuming exploration with traditional tools. The model estimates the instruction throughput of a superscalar processor based on early architectural design parameters and application properties. It has been validated with the SimpleScalar out-of-order simulator. The core of the model, which executes 1.6 million times faster, produces instruction throughput estimates that are with within 5.5 percent of the corresponding SimpleScalar values.	superscalar processor;throughput	Tarek M. Taha;D. Scott Wills	2008	IEEE Trans. Computers	10.1109/TC.2007.70817	model of computation;process design;embedded system;complex systems;throughput;computer architecture;application software;parallel computing;real-time computing;computer science;out-of-order execution;operating system;predictive modelling;space technology;object-modeling technique;computational model	Arch	1.942234282727204	56.03534212976814	62769
0bd2684b49068ca684fad48f01e1b727bacc1ce5	optimizing power consumption in multicore smartphones	power aware scheduler;multi core processor;android smartphone	This paper addresses the issue of managing power consumption in multicore smartphones via a middleware layer that schedules optimal number of cores for currently running applications taking into account the tradeoff between power consumption, performance and user experience. The paper first describes a simple and accurate method to measure the overall power consumption and then studies the impact of scheduling seven different popular applications over one to four cores on the overall power consumption. Based on this study, the paper proposes three new power-aware scheduling algorithms that dynamically schedule optimal number of cores as well as dynamically adjust the voltage frequency of each online core to achieve the best tradeoff between power consumption, application performance and user experience under the current context. Evaluation from a prototype implementation of the middleware on a quad-core HTC One shows that these algorithms result in significant reduction in power consumption while ensuring good performance and user experience. Holistic view of power management in multi-core smartphones.Tradeoffs between power, performance and user experience.Dynamic scheduling of optimal number of cores and their frequencies.Significant reduction in overall power consumption.		Shaosong Li;Shivakant Mishra	2016	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2016.02.004	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;computer network	HPC	-4.49019407968417	58.28000157985788	62780
b12d094e958b6e3212e3dfca10230ff99327bdeb	delphi: a framework for rtl-based architecture design evaluation using dsent models	libraries;hardware design languages;standards;clocks;logic gates;technology independent dsent design model delphi rtl based architecture design evaluation register transfer level rtl synthesis process computer architecture dsent modeling engine rtl hardware designs verilog vhdl rtl design industry standard synopsys design compiler tool;high level synthesis computer architecture hardware description languages;standards libraries hardware clocks logic gates hardware design languages timing;hardware;timing	Computer architects are increasingly interested in evaluating their ideas at the register-transfer level (RTL) to gain more precise insights on the key characteristics (frequency, area, power) of a micro/architectural design proposal. However, the RTL synthesis process is notoriously tedious, slow, and errorprone and is often outside the area of expertise of a typical computer architect, as it requires familiarity with complex CAD flows, hard-to-get tools and standard cell libraries. The effort is further multiplied when targeting multiple technology nodes and standard cell variants to study technology dependence. This paper presents DELPHI, a flexible, open framework that leverages the DSENT modeling engine for faster, easier, and more efficient characterization of RTL hardware designs. DELPHI first synthesizes a Verilog or VHDL RTL design (either using the industry-standard Synopsys Design Compiler tool or a combination of open-source tools) to an intermediate structural netlist. It then processes the resulting synthesized netlist to generate a technology-independent DSENT design model. This model can then be used within a modified version of the DSENT flow to perform very fast-one to two orders of magnitude faster than full RTL synthesis-estimation of hardware performance characteristics, such as frequency, area, and power across a variety of DSENT technology models (e.g., 65nm Bulk, 32nm SOI, 11nm Tri-Gate, etc.). In our evaluation using 26 RTL design examples, DELPHI and DSENT were consistently able to closely track and capture design trends of conventional RTL synthesis results without the associated delay and complexity. We are releasing the full DELPHI framework (including a fully open-source flow) at http://www.ece.cmu.edu/CALCM/delphi/.	compiler;computer architecture;computer-aided design;design space exploration;embarcadero delphi;first-order predicate;library (computing);netlist;open-source software;register-transfer level;standard cell;vhdl;verilog	Michael Papamichael;Cagla Cakir;Chen Sun;Chia-Hsin Owen Chen;James C. Hoe;Ken Mai;Li-Shiuan Peh;Vladimir Stojanovic	2015	2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2015.7095780	computer architecture;parallel computing;real-time computing;logic gate;computer science;design flow;operating system;high-level synthesis	Arch	5.612887732972952	53.28059687169467	62905
55897c2cdb63b70a42c9fecf6652b4ea388ba365	an hla-based formal co-simulation approach for rapid prototyping of heterogeneous mixed-signal socs			co-simulation;mixed-signal integrated circuit;rapid prototyping;simulation;system on a chip	Moon Gi Seok;Tag Gon Kim;Daejin Park	2017	IEICE Transactions		theoretical computer science;mixed-signal integrated circuit;mathematics;co-simulation;computer architecture;rapid prototyping	EDA	5.404283303188846	51.15714446497609	62909
536dee1630b16d2fcfc748e343cc04ba91860db8	microprogrammed control and reliable design of small computers: g d kraft and w n toy, prentice-hall (1981) pp 428, £11.00	microprogrammed control;reliable design;small computers		computer;microcode	Joe Gallacher	1981	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(81)90590-1	real-time computing;computer hardware;computer science	Arch	5.281586335840732	48.89939150507352	63074
075e9dcfb52bc9c216825e0e6c92981bbfdd0414	reconfigurable computing cluster (rcc) project: investigating the feasibility of fpga-based petascale computing	field programmable gate arrays dynamic execution dynamic hardware dynamic dataflow execution model aggregated hierarchical abstract hardware architecture fpga high level language handshaking control path solution;dynamic hardware;fpga;hardware architecture;dynamic dataflow execution model;field programmable gate arrays data flow computing;hardware fires field programmable gate arrays timing clocks logic protocols sorting computer science computer architecture;aggregated hierarchical abstract hardware architecture;control path solution;data flow computing;dynamic execution;handshaking;field programmable gate arrays;data flow;high level language	While medium- and large-sized computing centers have increasingly relied on clusters of commodity PC hardware to provide cost-effective capacity and capability, it is not clear that this technology will scale to the PetaFLOP range. It is expected that semiconductor technology will continue its exponential advancements over next fifteen years; however, new issues are rapidly emerging and the relative importance of current performance metrics are shifting. Future PetaFLOP architectures will require system designers to solve computer architecture problems ranging from how to house, power, and cool the machine, all the while remaining sensitive to cost. The reconfigurable computing cluster (RCC) project is a multi-institution, multi-disciplinary project investigating the use of Platform FPGAs to build cost-effective petascale computers. This paper describes the nascent project's objectives and a 64-node prototype cluster. Specifically, the aim is to provide an detailed motivation for the project, describe the design principles guiding development, and present a preliminary performance assessment. Microbenchmark results are reported to answer several pragmatic questions about key subsystems, including the system software, network performance, memory bandwidth, power consumption of nodes in the cluster. Results suggest that the approach is sound.	benchmark (computing);computer architecture;computer cluster;computer engineering;dynamic random-access memory;flops;field-programmable gate array;memory bandwidth;memory controller;network performance;petascale computing;prototype;reconfigurable computing;semiconductor;time complexity	Ron Sass;William V. Kritikos;Andrew G. Schmidt;Srinivas Beeravolu;Parag Beeraka	2007	15th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM 2007)	10.1109/FCCM.2007.62	dataflow architecture;embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;hardware architecture;field-programmable gate array	Arch	-2.850167431648664	47.25806675003786	63100
7314338a99092a0f15df931e8b451bc34f975b50	enabling efficient system configurations for dynamic wireless applications using system scenarios		Next generation mobile wireless systems (4G) support a wide range of communication protocols and services, thus opening new design challenges. The desired flexibility requires an effective utilization of system resources. In this article, we introduce the concept of system scenarios in wireless baseband engine signal processing optimization and in digital front-end power optimization. The scenario methodology classifies the system behavior from a cost perspective and provides the necessary information for an effective system tuning. We propose improvements for the clustering of the system executions into scenarios and the detection of scenarios at run time achieving a better trade-off between cost estimation accuracy and detection overhead. The first case study of the paper, using the WLAN communication protocol, demonstrates the accurate prediction of the execution time of each block of bits, which on average is 92 % shorter than the worst case allowing us to use the remaining time for the optimization of specifications like power consumption. In the second case study, we concentrate on the efficient signal power management during a WLAN transmission reducing the total energy consumption 50–94 % based on the throughput utilization.		Nikolaos Zompakis;Antonis Papanikolaou;Praveen Raghavan;Dimitrios Soudris;Francky Catthoor	2013	IJWIN	10.1007/s10776-012-0197-x	embedded system;real-time computing;simulation;operating system;computer network	Mobile	3.3715062900450223	55.531878045556546	63168
9d912bdbff0cb41b9cc435ac214467224448cc75	resiliency-aware scheduling for reconfigurable vliw processors	reconfigurable architectures multiprocessing systems processor scheduling;processor scheduling;reconfigurable architectures;fpga resiliency reconfigurability vliw softcore;tunneling magnetoresistance vliw computer architecture hardware field programmable gate arrays processor scheduling program processors;multiprocessing systems;over source level software approaches resiliency aware scheduling reconfigurable vliw processors hostile environments softcore vliw architectures runtime customization vliw datapath safety critical applications failure rates error rates software duplication physics code kernel source level code replication traditional hardware tmr	VLIW architectures are seeing increased deployment in a number of hostile environments. In addition, softcore VLIW architectures, which allow for run-time customization of the VLIW datapath, are becoming viable for a number of safety-critical applications. As error and failure rates rise, these applications elicit a need for automated and resilient architecture configuration tools. To mitigate these issues, this paper presents a Resiliency-aware Scheduling approach to the configuration of a custom VLIW architecture, providing computational resilience via software duplication. The automated RaS tool determines the optimal set of resources needed to provide a given level of resilience for a reconfigurable softcore VLIW architecture. For a sample case study, based on a common physics code kernel, the RaS approach is compared to traditional hardware (TMR) and software (source-level code replication) approaches. Results show a Resiliency-aware Scheduling-generated architecture configuration can potentially require up to 50% fewer functional units when compared to a TMR-hardened machine of similar performance, and can potentially improve performance by up to 40% over source-level software approaches.	central processing unit;datapath;kernel (operating system);scheduling (computing);software deployment;triple modular redundancy;very long instruction word	Jeremy Abramson;Pedro C. Diniz	2012	2012 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2012.6416784	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	6.129523523920295	58.91911314906927	63172
c39e5451755d5bdf7eb1e1f0f10af8deed0f4caf	a single chip fpga-based solution for controlling of multi-unit pmsm motor with time-division multiplexing scheme	space vector pulse width modulation svpwm;field programmable gate array fpga;vector control;time division multiplexing;permanent magnet synchronous motor pmsm	The use of multiple unit controllers for parallel processing of multi-unit motor drive systems can significantly reduce the execution time of the control algorithm. However, this approach does not only increase the system cost but also incurs in additional cost of hardware and software interconnections. This paper presents a fully integrated single chip field programmable gate array (FPGA) based solution for controlling of an independent multi-unit permanent magnet synchronous motor (PMSM) drive system with space vector pulse width modulation (SVPWM) based vector control. For multi-unit motor systems, the complexity of control algorithms often exceeds the resource availability of low-cost FPGA devices. Thus, a system-level time-division multiplexing scheme applicable for multimotor control systems is proposed. Using the proposed method, large identical complex control algorithms can be simplified into a single compact algorithm, which is fitted and configured into a low-cost FPGA. Simulation modeling and experimental results are shown, confirming the effectiveness of a multi-unit PMSM motor drive system using an inexpensive controller based on system-level timedivision multiplexing scheme, which can operate simultaneously with robustness under different operating conditions. Keywords—Field Programmable Gate Array (FPGA), Permanent Magnet Synchronous Motor (PMSM), Space Vector Pulse Width Modulation (SVPWM), Time-Division Multiplexing, Vector Control.	algorithm;control system;field-programmable gate array;multiplexing;parallel computing;pulse-width modulation;run time (program lifecycle phase);simulation	Sarayut Amornwongpeeti;Mongkol Ekpanyapong;Nattapon Chayopitak;João L. Monteiro;Júlio S. Martins;João Luiz Afonso	2015	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2015.08.011	embedded system;vector control;time-division multiplexing	EDA	3.0562535159080735	57.00133301699545	63199
66baf60155f208fbe0e0d389b310d804898ea684	hardware support for performance measurements and energy estimation of openrisc processor	physical measurements hardware support performance measurements energy estimation openrisc processor energy consumption openrisc or1200 hardware configurations tool chain side hardware design access core architecture configurations compiler parameters;radiation detectors;monitoring;radiation detectors power demand benchmark testing field programmable gate arrays hardware monitoring power measurement;energy estimation performance counters openrisc processor processor profiling design;field programmable gate arrays;power demand;reduced instruction set computing computer architecture energy consumption performance evaluation power aware computing program compilers;benchmark testing;power measurement;hardware	This paper addresses the problem of providing support for energy consumption accounting and performance evaluation by means of performance counters in an open source processor core - OpenRISC OR1200. The OpenRISC processing core is flexible in that it allows different hardware configurations, and provides full support on the tool-chain side. In addition to this, it gives full hardware design access, and it is used by a well-established community. This paper has taken advantage of these features in order to study how different processing core's architecture configurations and compiler parameters influence the processing core's performance. Furthermore, an energy consumption model based on performance counters values correlated by physical measurements has been proposed.	compiler;field-programmable gate array;microprocessor development board;multi-core processor;open-source software;openrisc 1200;performance evaluation;toolchain	Lucian Bara;Oana Boncalo;Marius Marcu	2015	2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2015.7208237	embedded system;benchmark;parallel computing;real-time computing;computer science;operating system;particle detector;field-programmable gate array	Arch	-2.587485366858065	55.2303641506155	63213
acff3969458e2fd3e4a178ba7e06833c5e67f2a9	automated high-level verification against clocked algorithmic specifications	automated high-level verification;clocked algorithmic specifications	Abstract We present a new method for automated verification of a circuit against an algorithmic specification where clock statements are used to indicate the scheduling of operations. We use a representation of the circuit and its specification where each data path register is treated as a unit, as in high-level synthesis. Therefore, in contrast with BDD-based methods, the time it takes to verify a circuit is independent of the width of the data path. We have been able to verify the Tamarack-3 microprocessor without any user guidance in 34 seconds, including parsing and compilation, on an IBM RS/6000 workstation.	clock rate;high-level verification	Francisco Corella	1993			computer architecture;parallel computing;real-time computing;computer science	Logic	7.547780503297537	51.74488035723543	63356
ea47b712d835815e0457297139f072f911e181d3	exploiting dynamic thermal energy harvesting for reusing in smartphone with mobile applications		Recently, mobile applications have gradually become performance- and resource- intensive, which results in a massive battery power drain and high surface temperature, and further degrades the user experience. Thus, high power consumption and surface over-heating have been considered as a severe challenge to smartphone design. In this paper, we propose DTEHR, a mobile Dynamic Thermal Energy Harvesting Reusing framework to tackle this challenge. The approach is sustainable in that it generates energy using dynamic Thermoelectric Generators (TEGs). The generated energy not only powers Thermoelectric Coolers (TECs) for cooling down hot-spots, but also recharges micro-supercapacitors (MSCs) for extended smartphone usage. To analyze thermal characteristics and evaluate DTEHR across real-world applications, we build MPPTAT (Multi-comPonent Power and Thermal Analysis Tool), a power and thermal analyzing tool for Android. The result shows that DTEHR reduces the temperature differences between hot areas and cold areas up to 15.4°C (internal) and 7°C (surface). With TEC-based hot-spots cooling, DTEHR reduces the temperature of the surface and internal hot-spots by an average of 8° and 12.8mW respectively. With dynamic TEGs, DTEHR generates 2.7-15mW power, more than hundreds of times of power that TECs need to cool down hot-spots. Thus, extra-generated power can be stored into MSCs to prolong battery life.	android;computer cooling;event generator;mobile app;smartphone;thermal management (electronics);thermal management of high-power leds;user experience	Yuting Dai;Tao Li;Benyong Liu;Mingcong Song;Huixiang Chen	2018		10.1145/3173162.3173188	real-time computing;battery (electricity);thermal energy;embedded system;thermoelectric generator;thermoelectric cooling;reuse;android (operating system);computer science	Mobile	-2.8558728590698936	59.32032645442471	63566
feb4247f1a0d2f643c88aabcbd2fdfa5eeb67bc4	heterogeneity and interconnect	processing element;wide dynamic range;energy efficient;smart phone;time synchronization;software architecture;broadcast optimization;sensor networks;energy consumption	The current usage of client devices - smart phones, tracking devices and wearable electronics - force us to take a deep look at energy consumption, they demand long battery life. Energy needs also get exacerbated by the demands of high level of connectivity, the Always On Always Connected usage model expected of such devices. We are unable to predict the demands on these devices or the usable models two to three years down the future. But few things are certain, namely the demand for higher power, energy efficiency and a wide dynamic range of operation. The talk will explore the trends and show how these affect the hardware and software architecture and its directions. In particular, the heterogeneity of mixing various types of processing elements, accelerators with a flexible interconnect fabric will be explored and some architectural findings on flexible interconnect and 3DIC will be shared.	dynamic range;high-level programming language;smartphone;software architecture;switched fabric;three-dimensional integrated circuit;wearable computer	Ganapati Srinivasa	2012		10.1145/2347655.2347656	embedded system;software architecture;electronic engineering;real-time computing;wireless sensor network;telecommunications;computer science;engineering;electrical engineering;operating system;efficient energy use;computer network	Arch	-1.5646956663226028	58.09530535091548	63661
4066ccaccd4fb16856d8aa534955775a3d681423	automatic generation of globally asynchronous locally synchronous wrapper circuits			globally asynchronous locally synchronous	Esmail Amini;Mehrdad Najibi;Hossein Pedram	2008	I. J. Comput. Appl.		synchronizer;parallel computing;electronic circuit;globally asynchronous locally synchronous;computer science;distributed computing	Logic	5.934390593835219	50.639265352611694	63774
4ab91b8dddb4383c40b9593ea809cb93cb997bd6	resource-aware architectures for particle filter based visual target tracking	bandwidth compression energy management target tracking particle filtering;computing capabilities resource aware architectures particle filter visual target tracking mobile devices smart phones smart cameras computer vision algorithms;target tracking computer vision mobile computing mobile handsets object tracking particle filtering numerical methods;tracking system;mobile device;smart phone;smart phones;computer vision;bandwidth compression;particle filter;computer vision algorithms;object tracking;smart cameras;visual target tracking;particle filtering;mobile handsets;computing capabilities;target tracking;visual tracking;mobile computing;resource aware architectures;mobile devices;strontium complexity theory erbium;particle filtering numerical methods;energy management	There are a growing number of visual tracking applications for mobile devices such as smart phones and smart cameras. However, existing computer vision algorithms are demanding and mobile devices possess limited computational capabilities, energy, and bandwidth to support them. Conventional approaches to distributed target tracking with a camera node and a receiver node are either sender based or receiver based. Both approaches are highly suited for certain scenarios, but have limited applicability outside of their scope. In this paper, we propose two new approaches for a particle filter based tracking system. The first proposed approach reduces the energy and bandwidth typically required for the receiver based setup. The second proposed approach partitions tracking workload between sender and receiver and adapts to the frame-to-frame demands of particle filtering. In doing so, this scheme promotes better balance of computing capabilities, energy, and bandwidth among sender and receiver. Results show that the proposed solutions require low additional overhead, can improve on tracking system lifetime, and may be more effective than conventional architectures for many tracking instances.	algorithm;bandwidth (signal processing);computer vision;data compression;embedded system;mobile device;overhead (computing);particle filter;smartphone;tracking system;video tracking	Domenic J. Forte;Ankur Srivastava	2011	2011 International Green Computing Conference and Workshops	10.1109/IGCC.2011.6008586	embedded system;computer vision;simulation;computer science	Mobile	-1.1052972312959288	59.565383230267464	63945
ac9b272168c566f9a3e2553a491a773c8e77ca6b	multi-objective optimal fsm state assignment	fpga targeted optimal multiobjective fsm state assignment method;system on chip field programmable gate arrays finite state machines high level synthesis state assignment;comparative analysis;state assignment;multi objective optimization;circuit synthesis integrated circuit interconnections power system interconnection field programmable gate arrays logic circuits microelectronics energy consumption electronic design automation and methodology circuit testing embedded system;embedded system;chip;embedded systems;high level synthesis;finite state machines;complex system;system on chip;system development;soc;microelectronics;field programmable gate arrays;soc microelectronics embedded systems fpga targeted optimal multiobjective fsm state assignment method	The recent spectacular progress in modern microelectronics made possible implementation of a complex system on a single chip, and created a big stimulus towards development of embedded systems for the existing and new applications. Unfortunately, it also introduced unusual complexity that results in many serious issues that cannot be resolved without new more adequate multi-objective circuit and system development methods and EDA-tools. As a part of our research that aims at development of such more adequate methods, we performed a comparative analysis of several representative commercial and academic synthesis methods and tools for the FPGA-targeted FSM synthesis, and developed a new FPGA-targeted multi-objective FSM state assignment method. In this paper, a part of results and conclusions from our research is discussed	complex system;electronic design automation;embedded system;field-programmable gate array;qualitative comparative analysis	Lech Józwiak;Aleksander Slusarczyk;Dominik Gawlowski	2006	9th EUROMICRO Conference on Digital System Design (DSD'06)	10.1109/DSD.2006.69	system on a chip;embedded system;complex systems;real-time computing;computer science;finite-state machine	EDA	9.10746900691074	52.81499859693034	64151
6fe77ae1869958c2b27fd3ef0cfdc3651a65b083	resource conflict detection in simulation of function unit pipelines	conflict detection;artikkeli article;simulation technique;finite state automata;functional unit	Processor simulators are important parts of processor design toolsets in which they are used to verify and evaluate the properties of the designed processors. While simulating architectures with independent function unit pipelines using simulation techniques that avoid the overhead of instruction bitstring interpretation, such as compiled simulation, the simulation of function unit pipelines can become one of the new bottlenecks for simulation speed.#R##N##R##N#This paper evaluates commonly used models for function unit pipeline resource conflict detection in processor simulation: a resource vector based-model, and an finite state automata (FSA) based model. In addition, an improvement to the simulation initialization time by means of lazy initialization of states in the FSA-based approach is proposed. The resulting model is faster to initialize and provides equal simulation speed when compared to the actively initialized FSA. Our benchmarks show at best 23 percent improvement to the initialization time.	pipeline (software);simulation	Pekka Jääskeläinen;Vladimír Guzma;Jarmo Takala	2007		10.1007/978-3-540-73625-7_25	embedded system;computer architecture;parallel computing;real-time computing;simulation;execution unit;computer science;operating system;finite-state machine;algorithm	EDA	-3.691078813072705	50.47459383165698	64163
c5a3e91e25dc7f00db67ed719dcf0f3252bd2dad	simd assisted fault detection and fault attack mitigation		Author(s): Chen, Zhi | Advisor(s): Nicolau, Alex | Abstract: Modern processors continue to aggressively scale down the feature size and reduce voltage levels to run faster and be more energy efficient. However, this trend also poses significant reliability concern as it makes transistors more susceptible to soft errors. Soft errors are transient. Although they donu0027t impair the computing systems permanently, these errors can corrupt the output of a program or even crash the entire system. Hardware or software redundant techniques could be used to detect errors during the execution of a program. However, hardware redundancy, e.g. DMR (dual-modular redundancy) and TMR (triple-modular redundancy), leads to significant area overhead and very high energy cost. Software redundancy, e.g. instruction duplication, has lower performance and energy penalty and virtually no hardware cost by sacrificing a small degree of error coverage. Yet commodity processors generally donu0027t require ``five-ninesu0027u0027 reliability as they are not mission-critical. Instead, performance and energy consumption have more priority. This dissertation proposes a novel approach to instruction duplication, which exploits the redundancy within SIMD instructions. The key idea is to pack the original data and its duplicate in the different lanes of the same vector register instead of executing two scalar instructions separately as these registers are underutilized on most applications. The proposed solution is implemented in the LLVM compiler as a stand-alone pass. Evaluation on a host of benchmarks reveal that proposed SIMD-based error detection technique causes much less performance, code size, and energy overheads.This dissertation further extends the proposed approach as a countermeasure to protect cryptographic algorithms. These algorithms are widely adopted in modern processors and embedded systems to protect information. A number of popular cryptographic algorithms in the Libgcrypt library are protected using the SIMD-based instruction duplication technique. A large amount of errors are injected to these algorithms. The results show that almost all injected faults can be detected with reasonable performance and code size cost.	differential fault analysis;simd	Zhi Chen	2018			compiler;redundancy (engineering);real-time computing;error detection and correction;software;exploit;fault detection and isolation;crash;computer science;simd	Logic	6.610169720494164	59.92589548558392	64243
0c25cd0053f52c46dc67e88521ffa99d40253758	evaluation of compact high-throughput reconfigurable architecture based on bit-serial computation	high availability;performance evaluation compact high throughput reconfigurable architecture bit serial computation ds hie reconfigurable processor benes network routing resource transistor count mep risc processor;performance evaluation;routing;reconfigurable architectures;reconfigurable architectures routing computer architecture pipeline processing computer networks reduced instruction set computing delay availability high performance computing hardware;reduced instruction set computing;data mining;chip;computer architecture;reconfigurable architecture;transistors;reduced instruction set computing microprocessor chips performance evaluation reconfigurable architectures;high throughput;reconfigurable processor;pipeline processing;microprocessor chips;throughput	In this paper, aiming toward a compact high-throughput reconfigurable architecture, we propose the reconfigurable processor DS-HIE. In order to achieve the characteristics of compactness and high-throughput, the DS-HIE architecture executes operations following a bit-serial computation scheme and adopts a Benes network as its routing resource. Implementing bit-serial computation brings the advantage of small chip area and high throughput to the DS-HIE architecture, and the Benes network ensures the high availability of the routing paths within a compact chip area. In this paper, we evaluated its transistor count and performance, compared with the RISC processor MeP. From this evaluation, the DS-HIE processor required 9.2 times the transistor count of the MeP processor, it achieved 13 to 33 times higher performance as compared with the MeP processor.	clos network;computation;discrete cosine transform;fast fourier transform;finite impulse response;high availability;high- and low-level;high-level programming language;high-throughput computing;integrated development environment;low-density parity-check code;media-embedded processor;reconfigurable computing;routing;serial communication;throughput;transistor count	Kazuya Tanigawa;Tetsuo Hironaka	2008	2008 International Conference on Field-Programmable Technology	10.1109/FPT.2008.4762396	chip;high-throughput screening;embedded system;reduced instruction set computing;routing;throughput;computer architecture;parallel computing;computer science;operating system;high availability;transistor	Arch	7.792837453193783	47.041401580186786	64447
0f83ca6c5cece3c81fba4e44fc4d21e815e4fba0	hierarchical power management of a system with autonomously power-managed components using reinforcement learning	reinforcement learning;semi markov decision process;power management;temporal difference learning	1. Introduction Power consumption in battery operated portable devices is nowadays a major concern. Such systems generally contain many I/O device components, ranging from digital and analog to electro-mechanical and electro-chemical. For these systems, the major energy dissipation is coming from these I/O devices. Dynamic power management (DPM) refers to a set of strategies that achieves efficient power consumption by the selective shut-off or slowdown of I/O components that are idle or underutilized [1]. Such technique has proven to be a particularly effective way of reducing power dissipation at the system level [2]. An effective DPM policy should minimize power consumption while maintaining performance degradation within an acceptable level. The DPM methods proposed in the literature can be broadly classified into three categories: heuristic, stochastic, and learning based methods. Heuristic methods attempt to predict the length of the next idle time based on the computation history, and then shut the device down if the predicted idle period length justifies the cost. More precisely, a decision to sleep will be made if the prediction indicates that the idle period is longer than the break-even time T be. Among these methods, Srivastava et al. [3] use a regression function to predict the idle period length, while Hwang et al. [4] propose an exponential-weighting average function for predicting the idle period length. Such techniques are simple and easy to implement, and have been adopted in many commercial products. However, they perform well only when the requests are highly correlated and do not take performance constraints into account, and thus, can hardly achieve a desirable trade-off between performance and energy dissipation. The stochastic approaches can take into account both power and performance and are able to derive provably optimal DPM policies, by modeling the request arrival times and device service times as stationary stochastic processes such as Markov Decision Processes (MDP) [5], [6], [7]. The essential shortcoming of these methods is the need of exact knowledge of the MDP state transition probability function. However, the workload of a complex system is usually changing with time and hard for accurate prediction [8]. The workload variation has a significant impact on the system performance and power consumption. Thus, a robust power management technique must consider the uncertainty and variability that emanate from the environment, hardware and application characteristics [9] and must be able to interact with the environment to obtain information which can be processed to …	analog signal;cpu power dissipation;complex system;computation history;digital data;elegant degradation;heuristic;input/output;markov chain;markov decision process;mobile device;power management;reinforcement learning;spatial variability;state transition table;stationary process;stochastic process;time complexity	Maryam Triki;Yanzhi Wang;Ahmed C. Ammari;Massoud Pedram	2015	Integration	10.1016/j.vlsi.2014.06.001	temporal difference learning;embedded system;real-time computing;simulation;computer science;operating system;reinforcement learning	AI	-3.5001706092072933	58.986961854878594	64663
daddacf1f050cc780cb75508664a5e15dfe7fff2	design of a rs485-to-1553b bus bridge	protocols;reliability;assembly integration and test processes rs485 to 1553b bus bridge design 1553b protocol rs485 transceivers spacecrafts space missions fpga hardware design logic design high reliability methods;transceivers protocols field programmable gate arrays bridges bridge circuits reliability hardware;1553b bus;logic design;bridges;aerospace application;fpga;system buses;bridge;rs485;artificial satellites;transceivers;field programmable gate arrays;aerospace application rs485 1553b bus fpga bridge;bridge circuits;transceivers artificial satellites bridge circuits field programmable gate arrays logic design protocols system buses;hardware	Based on the analysis on the characteristics of RS485 and 1553B bus, a bus system of 1553B protocol over RS485 transceivers is introduced, which can reduce the mass, size and power of spacecrafts when applied in space missions. Design of a RS485-to-1553B bus bridge is proposed here to realize the interoperation between 1553B bus and RS485 bus, which is easy to implement over FPGA and take up little hardware resources. Hardware design and logic design in FPGA of the bus bridge are described in detail, and the high reliability methods adopted are introduced also. The test results prove the validity and reliability of the bridge design. The RS485-to-1553B bus bridge can be used in the Assembly, Integration and Test processes of satellites.	advanced intelligent tape;field-programmable gate array;interoperation;logic synthesis;processor design;rs-485;real-time clock;reliability engineering;transceiver	Li Zhou;Junshe An	2013	2013 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery	10.1109/CyberC.2013.50	bus;std bus;embedded system;memory bus;real-time computing;can bus;three-state logic;iebus;computer science;local bus;system bus;control bus;back-side bus;bus network;field-programmable gate array	EDA	5.88204694538681	55.62821755289119	64687
72985a66784998bf91d69e9a62de848207f517f6	a highly parallel fpga-based evolvable hardware architecture	inf		evolvable hardware;field-programmable gate array	Fabio Cancare;Marco Castagna;Matteo Renesto;Donatella Sciuto	2009		10.3233/978-1-60750-530-3-608	embedded system;computer architecture;parallel computing;computer science	Arch	4.6851625408825335	49.30339678547661	64703
2a7bd6d4281e9d63097c047f1fd8348c6807a30d	implementing an ofdm receiver on the rapid reconfigurable architecture	field programmable gate array;receiver;diseno circuito;reconfigurable architectures;circuit design;multiplexage frequence orthogonal;receptor;red puerta programable;reseau porte programmable;multiplaje frecuencia ortogonal;fpga implementation;reconfigurable architecture;design and implementation;orthogonal frequency division multiplexing;fpga architecture;recepteur;conception circuit;c programming language;coarse grained;high performance;architecture reconfigurable	Field-programmable gate arrays (FPGAs) have become an extremely popular implementation technology for custom hardware because they offer a combination of low cost and very fast turnaround. Because of their in-system reconfigurability, FPGAs have also been suggested as an efficient replacement for application-specific integrated circuits (ASICs) and digital signal processors (DSPs) for applications that require a combination of high performance, low cost, and flexibility. Unfortunately, the use of FPGAs in mobile embedded systems platforms is hampered by the very large overhead of FPGA-based architectures. Coarse-grained configurable architectures can reduce this overhead substantially by taking advantage of the application domain to specialize the reconfigurable architecture via coarse-grained components and interconnects. This paper presents the design and implementation of an OFDM receiver in the RaPiD reconfigurable architecture as a case study for comparing the relative cost and performance of ASIC, DSP, FPGA, and coarse-grained reconfigurable architectures. RaPiD is a coarse-grained reconfigurable architecture specialized to the domain of signal and image processing. The RaPiD architecture provides a reconfigurable pipelined datapath controlled by efficient reconfigurable control logic: We have implemented the computationally intensive parts of an OFDM receiver on the RaPiD architecture and have developed careful estimates of corresponding implementations in representative ASIC, DSP and FPGA technology. Our results show that, for this application, RaPiD fills the cost/performance gap between programmable DSP and ASIC architectures, achieving a factor of 6 better than a DSP implementation but a factor of 6 less than an ASIC implementation.	algorithm;application domain;application-specific integrated circuit;central processing unit;centralized computing;compiler;computation;control reconfiguration;datapath;digital signal processing;digital signal processor;electrical connection;electronic circuit;embedded system;field-programmable gate array;image processing;image scaling;overhead (computing);problem domain;reconfigurability;system on a chip	Carl Ebeling;Chris Fisher;Guanbin Xing;Manyuan Shen;Hui Liu	2003	IEEE Transactions on Computers	10.1007/978-3-540-45234-8_3	receiver;embedded system;computer architecture;orthogonal frequency-division multiplexing;receptor;computer science;circuit design;field-programmable gate array	EDA	2.222485359707413	48.88927122973427	64797
8384bfad2d237f7ba96c683e1007f8d15d3a6c33	a hybrid fixed-function and microprocessor solution for high-throughput broad-phase collision detection	signal image and speech processing;circuits and systems;control structures and microprogramming;journal article;electronic circuits and devices	We present a hybrid system spanning a fixed-function microarchitecture and a general-purpose microprocessor, designed to amplify the throughput and decrease the power dissipation of collision detection relative to what can be achieved using CPUs or GPUs alone. The primary component is one of the two novel microarchitectures designed to perform the principal elements of broad-phase collision detection. Both microarchitectures consist of pipelines comprising a plurality of memories, which rearrange the input into a format that maximises parallelism and bandwidth. The two microarchitectures are combined with the remainder of the system through an original method for sharing data between a ray tracer and the collision-detection microarchitectures to minimise data structure construction costs. We effectively demonstrate our system using several benchmarks of varying object counts. These benchmarks reveal that, for over one million objects, our design achieves an acceleration of 812× relative to a CPU and an acceleration of 161× relative to a GPU. We also achieve energy efficiencies that enable the mitigation of silicon power-density challenges, while making the design amenable to both mobile and wearable computing devices.	benchmark (computing);central processing unit;collision detection;data structure;file spanning;fixed-function;general-purpose modeling;graphics processing unit;high-throughput computing;hybrid system;microarchitecture;microprocessor;parallel computing;pipeline (computing);ray tracing (graphics);throughput;wearable computer	Muiris Woulfe;Michael Manzke	2017	EURASIP J. Emb. Sys.	10.1186/s13639-016-0037-7	throughput;parallel computing;computer science;microprocessor;hybrid system;real-time computing;remainder;collision detection;ray tracing (graphics);data structure;microarchitecture	Arch	-1.8431452472413354	53.36969609749872	64817
319daee803df5df23b4dc160266b293d84a87933	a 32-bit cmos microprocessor with six-stage pipeline structure				Y. Miki;S. Nohara;K. Koya;M. Araki	1986			microprocessor;electronic engineering;32-bit;computer science;cmos	Arch	6.726151262933448	50.093256007490446	64881
d3c00cc122b0b3f64dece16eebb3b48e99e609ee	the universal configurable block/machine— an approach for a configurable soc-architecture	reconfigurable computing;real time;space time;development process;block based architecture;programming model;space time mapping;operating system;system on chip;configurable computing	The universal configurable block/machine is a block-based approach for a configurable system-on-chip-(CSoC-) architecture. The programming model of the blocks is similar to microprocessor models, while the execution model supports configurable computing including reconfiguration. This is achieved by the microarchitecture of the blocks and an additional translation phase, resulting in three phases of overall program execution: fetching, translation and execution. These phases may act without strict coupling, simplifying the duplication of the executing part. The resulting hardware model is classified by four parameter: number of blocks, hyperblock sequencer, hyperblock scheduler and a set of block interconnections. The scheduler indicates that the model is capable of executing operating system work by scheduling hardware resources to threads or processes. This homogeneous CSoC may be used as compile-time defined inhomogeneous application-specific architecture. In this case the development process defines threads to run completely in one or more blocks solving partial problems and communicating to others. This enhances the flexibility and the optimization capabilities towards performance and/or real-time behavior.	compile time;compiler;concurrent computing;coupling (computer programming);mathematical optimization;microarchitecture;microprocessor;microsequencer;microsoft outlook for mac;multithreading (computer architecture);operating system;programming model;real-time clock;real-time computing;real-time transcription;reconfigurable computing;requirement;scheduling (computing);simulation;system on a chip;system requirements;thread (computing);transistor	Christian Siemers;Volker Winterstein	2003	The Journal of Supercomputing	10.1023/A:1025651132236	system on a chip;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;space time;distributed computing;programming paradigm;software development process	Arch	-1.3438295080003582	49.73952660943568	64969
4015e9a2d82e8b3fea88ef01b2042010b3118147	fast fourier transformation algorithm for single-chip cloud computers using rcce			algorithm;fast fourier transform	Wasuwee Sodsong;Bernd Burgstaller	2011			fast fourier transform;chip;cloud computing;theoretical computer science;computer science	Robotics	4.785373150976696	48.495580298236796	65048
44e5d6f8caaff7fa4c4017a1c20de22e684a1ecf	automated mapping of the mapreduce pattern onto parallel computing platforms	parallel computing;mapreduce;pipelining;geometric programming	The MapReduce pattern can be found in many important applications, and can be exploited to significantly improve system parallelism. Unlike previous work, in which designers explicitly specify how to exploit the pattern, we develop a compilation approach for mapping applications with the MapReduce pattern automatically onto Field-Programmable Gate Array (FPGA) based parallel computing platforms. We formulate the problem of mapping the MapReduce pattern to hardware as a geometric programming model; this model exploits loop-level parallelism and pipelining to give an optimal implementation on given hardware resources. The approach is capable of handling single and multiple nested MapReduce patterns. Furthermore, we explore important variations of MapReduce, such as using a linear structure rather than a tree structure for merging intermediate results generated in parallel. Results for six benchmarks show that our approach can find performance-optimal designs in the design space, improving system performance by up to 170 times compared to the initial designs on the target platform. Q. Liu · T. Todman (B) · W. Luk Department of Computing, Imperial College London, London SW7 2AZ, UK e-mail: tjt97@doc.ic.ac.uk Q. Liu e-mail: qiang.liu2@imperial.ac.uk W. Luk e-mail: w.luk@imperial.ac.uk G. A. Constantinides Department of Electrical Engineering, Imperial College London, London SW7 2AZ, UK e-mail: g.constantinides@imperial.ac.uk	compiler;electrical engineering;email;field-programmable gate array;geometric programming;loop-level parallelism;mapreduce;optimal design;parallel computing;pipeline (computing);programming model;speedup;tree structure	Qiang Liu;Tim Todman;Wayne Luk;George A. Constantinides	2012	Signal Processing Systems	10.1007/s11265-010-0563-9	parallel computing;geometric programming;computer science;theoretical computer science;operating system;data-intensive computing;distributed computing;pipeline	HPC	-0.4879132755764627	50.99176541011587	65296
1550428e3dff89ac608eb2f96b255250de44f647	influence of caching and encoding on power dissipation of system-level buses for embedded systems	cache storage;system bus architecture caching encoding power dissipation system level buses embedded systems power consumption cache configuration associativity power oriented encoding techniques data buses address buses memory hierarchy;cache memory;embedded system;system buses;virtual storage cache storage system buses embedded systems;embedded systems;power dissipation;memory hierarchy;power consumption;encoding power dissipation embedded system energy consumption electronic switching systems microprocessors frequency system buses power system modeling power generation;point of view;virtual storage	This paper proposes a methodology to evaluate the effects of encodings on the power consumption of system-level buses in the presence of multi-level cache memories. The proposed model can consider any cache configuration in terms of size, associativity and block. It includes also the most widely adopted power oriented encoding techniques for data and address buses. Experimental results show how the proposed model can be effectively adopted to configure the memory hierarchy and the system bus architecture from the power point of view.	cpu cache;cache (computing);embedded system;memory hierarchy;point of view (computer hardware company);system bus	William Fornaciari;Donatella Sciuto;Cristina Silvano	1999	Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)	10.1145/307418.307458	embedded system;parallel computing;real-time computing;cache coloring;128-bit;cpu cache;computer science;dissipation;256-bit;cache pollution	EDA	1.553671254364117	55.019023481474804	65307
379426e7dce58df5b5b9ea580482d59f7cff4266	design space exploration in application-specific hardware synthesis for multiple communicating nested loops	hardware synthesis;nested loops;parallel mpsoc computing resources design space exploration application specific hardware synthesis multiple communicating nested loops high performance data intensive applications mpsoc design hardware architecture data distribution data architecture;clocks abstracts parallel processing tiles arrays hardware;integrated circuit design;parallel architectures;system on chip;system on chip integrated circuit design multiprocessing systems parallel architectures;multiprocessing systems;design space exploration	Application specific MPSoCs are often used to implement high-performance data-intensive applications. MPSoC design requires a rapid and efficient exploration of the hardware architecture possibilities to adequately orchestrate the data distribution and architecture of parallel MPSoC computing resources. Behavioral specifications of data-intensive applications are usually given in the form of a loop-based sequential code, which requires parallelization and task scheduling for an efficient MPSoC implementation. Existing approaches in application specific hardware synthesis, use loop transformations to efficiently parallelize single nested loops and use Synchronous Data Flows to statically schedule and balance the data production and consumption of multiple communicating loops. This creates a separation between data and task parallelism analyses, which can reduce the possibilities for throughput optimization in high-performance data-intensive applications. This paper proposes a method for a concurrent exploration of data and task parallelism when using loop transformations to optimize data transfer and storage mechanisms for both single and multiple communicating nested loops. This method provides orchestrated application specific decisions on communication architecture, memory hierarchy and computing resource parallelism. It is computationally efficient and produces high-performance architectures.	algorithmic efficiency;communicating sequential processes;control flow;data-intensive computing;design space exploration;field-programmable gate array;mpsoc;mathematical optimization;memory hierarchy;overhead (computing);parallel computing;scheduling (computing);task parallelism;throughput;vii	Rosilde Corvino;Abdoulaye Gamatié;Marc Geilen;Lech Józwiak	2012	2012 International Conference on Embedded Computer Systems (SAMOS)	10.1109/SAMOS.2012.6404166	computer architecture;parallel computing;real-time computing;computer science	HPC	-0.22499031797530353	51.148091977736264	65327
ece89f7e4288d36a460cfc9e88716064674ff02c	vhsic hardware description (vhdl) development program	general structure;vhsic hardware description language;development program;paper briefly;vhsic technology;vhsic program;hierarchical simulator;back-ground information;vhsic hardware description;future military digital system;standard hardware description language;complexity;statistical mechanics;hardware description language;design automation;thermal equilibrium;algorithms;documentation	The VHSIC Program has realized the importance of a standard hardware description language to facilitate the design and documentation of future military digital systems incorporating VHSIC technology. To that end, the VHSIC Hardware Description Language Development Program has been organized to generate a language and an associated hierarchical simulator. The paper briefly covers some back-ground information and then outlines the general structure of the program. The latter is explained with respect to the requirements of the VHSIC Hardware Description Language effort and how the tasks have been designed to meet the requirements.	digital electronics;documentation;hardware description language;requirement;vhdl;vhsic	Al Dewey	1983	20th Design Automation Conference Proceedings		embedded system;electronic engineering;complexity;electronic design automation;vhdl;documentation;statistical mechanics;computer science;electrical engineering;thermal equilibrium;theoretical computer science;operating system;hardware description language;programming language;algorithm;computer engineering	EDA	9.019273422846279	51.60881667181166	65361
dd2f8eb1cc7c3f131afd1df18e81b75f763710d6	a bayesian network approach for compiler auto-tuning for embedded processors	gcc transformation space bayesian network compiler auto tuning embedded processors application code porting application code tuning standard optimization options machine learning approach cost reduction embedded architectures microarchitecture compiler transformations complex probability distribution function arm platform;optimization bayes methods probability distribution training vectors program processors network topology;statistical distributions belief networks computer architecture electronic engineering computing embedded systems learning artificial intelligence microprocessor chips optimisation program compilers	The complexity and diversity of today's architectures require an additional effort from the programmers in porting and tuning the application code across different platforms. The problem is even more complex when considering that also the compiler requires some tuning, since standard optimization options have been customized for specific architectures or designed for the average case. This paper proposes a machine-learning approach for reducing the cost of the compiler auto-tuning phase and to speedup the application performance in embedded architectures. The proposed framework is based on an application characterization done dynamically with microarchitecture independent features and based on the usage of Bayesian Networks. The main characteristic of the Bayesian Network approach consists of not describing the solution as a strict set of compiler transformations to be applied, but as a complex probability distribution function to be sampled. Experimental results, carried out on an ARM platform and GCC transformation space, proved the effectiveness of the proposed methodology for the selected benchmarks. The selected set of solutions (less than 10% of the search space) demonstrated to be very close to the optimal sequence of transformations, showing also an applications performance speedup up to 2.8 (1.5 on average) with respect to -O2 and -O3 for the cBench suite. Additionally, the proposed method demonstrated a 3× speedup in terms of search time with respect to an iterative compilation approach, given the same quality of the solutions1.	arm architecture;bayesian network;best, worst and average case;central processing unit;embedded system;gnu compiler collection;iterative method;machine learning;mathematical optimization;microarchitecture;optimizing compiler;performance tuning;programmer;self-tuning;speedup	Amir Hossein Ashouri;Giovanni Mariani;Gianluca Palermo;Cristina Silvano	2014	2014 IEEE 12th Symposium on Embedded Systems for Real-time Multimedia (ESTIMedia)	10.1109/ESTIMedia.2014.6962349	embedded system;computer architecture;compiler;parallel computing;real-time computing;compiler correctness;computer science;theoretical computer science;operating system;programming language	Embedded	-1.4054295729595885	50.97064579938573	65408
892b0f5138905ed35bf00153e8842cb217b3d25b	run-time support for heterogeneous multitasking on reconfigurable socs	sistema operativo;communication process;reconfigurable system;integrated circuit;concepcion sistema;reconfigurable computing;dynamic reconfiguration;network on chip;reconfigurable architectures;implementation;multiprocessing system;circuit vlsi;circuito integrado;time management;system on a chip;chip;proceso comunicacion;processus communication;reconfigurable architecture;vlsi circuit;operating system;sistema sobre pastilla;systeme multitraitement;system design;scheduling;reseau sur puce;sistema multitratamiento;multithread;systeme exploitation;systeme sur puce;multitâche;circuito vlsi;implementacion;architecture reconfigurable;multitarea;conception systeme;ordonnancement;circuit integre;reglamento	In complex reconfigurable systems on chip (SoC), the dynamism of applications requires an efficient management of the platform. To allow run-time management of heterogeneous resources, operating systems (OS) and reconfigurable SoC platforms should be developed together. For run-time support of reconfigurable architectures, the OS must abstract the reconfigurable computing resources and provide an efficient communication layer. This paper presents our efforts to simultaneously develop the run-time support and the communication layer of reconfigurable SoCs. We show that networks-on-chip (NoC) are an ideal communication layer for dynamically reconfigurable SoCs, explain how our OS provides run-time support for dynamic task relocation and detail how hardware parts of the OS are integrated into the higher layers of the NoC. An implementation of the OS and of the dedicated communication layer on our reconfigurable architecture supports the concepts we describe.	computer multitasking;system on a chip	Théodore Marescaux;Vincent Nollet;Jean-Yves Mignolet;Andrei Bartic;W. Moffat;Prabhat Avasare;Paul Coene;Diederik Verkest;Serge Vernalde;Rudy Lauwereins	2004	Integration	10.1016/j.vlsi.2004.03.002	chip;system on a chip;embedded system;parallel computing;real-time computing;time management;reconfigurable computing;computer science;engineering;operating system;integrated circuit;network on a chip;implementation;scheduling;systems design	EDA	-0.4074994440855934	54.35326459126037	65522
5d689feb8b10eaea981e8d547840b6777aa16437	composite confidence estimators for enhanced speculation control	ipc speculation control techniques multiple confidence estimators composite confidence estimator statistical methodology microarchitectural simulator energy reduction technique pipeline gating;confidence estimation;estimation theory;multiple confidence estimators;pipelines microarchitecture statistical analysis probability computer architecture high performance computing computer science hardware statistics yield estimation;microarchitecture;statistical methodology;history;branch prediction;radiation detectors;data mining;ipc;composite confidence estimator;accuracy;statistical analysis;estimation;pipeline gating;pipelines;microarchitectural simulator;energy reduction technique;statistical analysis estimation theory pipeline processing;microarchitecture branch prediction confidence estimation;speculation control techniques;pipeline processing;value prediction	This paper proposes a way to allow more effective use of speculation control techniques by combining multiple confidence estimators into a {em composite confidence estimator}. This new class of confidence estimators provides improved performance and finer speculation control. This paper makes three contributions. First, we describe techniques for building efficient composite confidence estimators. Second, we present an improved statistical methodology for evaluating confidence estimators. Finally, we use a detailed microarchitectural simulator to evaluate the ability of our estimator to support an energy reduction technique called pipeline gating. Using previous confidence estimators, pipeline gating reduces the amount of extra work due to mis-speculated instructions by 22%, with a reduction in IPC of 5%. With the same impact on IPC, our confidence estimators reduce extra work by 31%.	branch misprediction;confidentiality;graphics pipeline;microarchitecture;pipeline (computing);simulation	Daniel A. Jiménez	2009	2009 21st International Symposium on Computer Architecture and High Performance Computing	10.1109/SBAC-PAD.2009.17	estimation;parallel computing;real-time computing;microarchitecture;computer science;pipeline transport;accuracy and precision;estimation theory;particle detector;branch predictor;inter-process communication	Arch	-2.1365415345668133	56.4294439234922	65570
664b8e74f204461edb9b0feeaed248449b1dcb01	exploring many-core design templates for fpgas and asics	paper;nvidia geforce gtx 580;performance;fpga;bayesian;nvidia;computer science;asic;opencl	We present a highly productive approach to hardware design based on a many-core microarchitectural template used to implement compute-bound applications expressed in a high-level data-parallel language such as OpenCL. The template is customized on a per-application basis via a range of high-level parameters such as the interconnect topology or processing element architecture. The key benefits of this approach are that it (i) allows programmers to express parallelism through an API defined in a high-level programming language, (ii) supports coarse-grained multithreading and fine-grained threading while permitting bit-level resource control, and (iii) reduces the effort required to repurpose the system for different algorithms or different applications. We compare template-driven design to both full-custom and programmable approaches by studying implementations of a compute-bound data-parallel Bayesian graph inference algorithm across several candidate platforms. Specifically, we examine a range of templatebased implementations on both FPGA and ASIC platforms and compare each against full custom designs. Throughout this study, we use a general-purpose graphics processing unit (GPGPU) implementation as a performance and area baseline. We show that our approach, similar in productivity to programmable approaches such as GPGPU applications, yields implementations with performance approaching that of full-custom designs on both FPGA and ASIC platforms.	abstraction layer;algorithm;application domain;application programming interface;application-specific integrated circuit;baseline (configuration management);benchmark (computing);bit-level parallelism;compiler;computer graphics;field-programmable gate array;full custom;general-purpose computing on graphics processing units;general-purpose markup language;general-purpose modeling;graphics processing unit;high- and low-level;high-level programming language;high-level synthesis;imperative programming;intel core (microarchitecture);manycore processor;microarchitecture;multithreading (computer architecture);opencl api;parallel computing;parallel language;programmer;programming model;rapid prototyping;reconfigurable computing;thread (computing)	Ilia A. Lebedev;Christopher W. Fletcher;Shaoyi Cheng;James Martin;Austin Doupnik;Daniel Burke;Mingjie Lin;John Wawrzynek	2012	Int. J. Reconfig. Comp.	10.1155/2012/439141	embedded system;computer architecture;parallel computing;real-time computing;performance;bayesian probability;computer science;operating system;application-specific integrated circuit;field-programmable gate array	Arch	-0.7302915994511239	47.44706706883911	65627
e61105e3f4f2841ea9b5a1f1752f1ea5c490a8b2	a control-based methodology for power-performance optimization in nocs exploiting dvfs	network on chip;power performance tradeoff;multiprocessors;dfs;dvfs	Networks-on-Chip (NoCs) are considered a viable solution to fully exploit the computational power of multi- and many-cores, but their non negligible power consumption requires ad hoc power-performance design methodologies. In this perspective, several proposals exploited the possibility to dynamically tune voltage and frequency for the interconnect, taking steps from traditional CPU-based power management solutions. However, the impact of the actuators, i.e. the limited range of frequencies for a PLL (Phase Locked Loop) or the time to increase voltage and frequency for a Dynamic Voltage and Frequency Scaling (DVFS) modules, are often not carefully accounted for, thus overestimating the benefits. This paper presents a control-based methodology for the NoC power-performance optimization exploiting the Dynamic Frequency Scaling (DFS). Both timing and power overheads of the actuators are considered, thanks to an ad hoc simulation framework. Moreover the proposed methodology eventually allows for user and/or OS interactions to change between different high level power-performance modes, i.e. to trigger performance oriented or power saving system behaviors. Experimental validation considered a 16-core architecture comparing our proposal with different settings of threshold-based policies. We achieved a speedup up to 3 for the timing and a reduction up to 33.17% of the power ∗ time product against the best threshold-based policy. Moreover, our best control-based scheme provides an averaged power-performance product improvement of 16.50% and 34.79% against the best and the second considered threshold-based policy setting.	dynamic voltage scaling;mathematical optimization	Davide Zoni;Federico Terraneo;William Fornaciari	2015	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2015.04.004	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;distributed file system;network on a chip;computer network	EDA	-3.9315074553305407	55.911023877324034	65985
ba70f3975ba248e01b7c3dc2d0efcbb6a6662857	picos, a hardware task-dependence manager for task-based dataflow programming models		Task-based programming Task-based programming models such as OpenMP, Intel TBB and OmpSs are widely used to extract high level of parallelism of applications executed on multi-core and manycore platforms. These programming models allow applications to be expressed as a set of tasks with dependences to drive their execution at runtime. While managing these dependences for task with coarse granularity proves to be highly beneficial, it introduces noticeable overheads when targeting fine-grained tasks, diminishing the potential speedups or even introducing performance losses. To overcome this drawback, we propose a hardware/software co-design Picos that manages inter-task dependences efficiently. In this paper we describe the main ideas of our proposal and a prototype implementation. This prototype is integrated with a parallel task- based programming model and evaluated with real executions in Linux embedded system with two ARM Cortex-A9 and a FPGA. When compared with a software runtime, our solution results in more than 1.8x speedup and 40% of energy savings with only 2 threads.	arm cortex-a9;arm architecture;dataflow programming;embedded system;field-programmable gate array;high-level programming language;linux on embedded systems;manycore processor;multi-core processor;openmp;parallel computing;programming model;prototype;run time (program lifecycle phase);speedup;threading building blocks	Xubin Tan;Jaume Bosch;Miquel Vidal;Carlos Álvarez;Daniel Jiménez-González;Eduard Ayguadé;Mateo Valero	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.134	field-programmable gate array;parallel computing;thread (computing);dataflow programming;software;computer hardware;granularity;speedup;programming paradigm;supercomputer;computer science	HPC	-2.887272309689006	49.291317331307695	66066
62b886e7342fcee1561784bddb73b258935dbda8	data-driven and demand-driven computer architecture	spectrum;computer architecture;next generation;model of computation;data flow	Novel data-driven and demand-driven computer architectures are under development in a large number of laboratories in the United States, Japan, and Europe. These computers are not based on the tradlUonal von Neumann organization; instead, they are attempts to identify the next generation of computer. Basmally, m data-driven (e.g., data-flow) computers the availability of operands triggers the execution of the operation to be performed on them, whereas in demand-driven (e.g, reduction) computers the reqmrement for a result triggers the operation that will generate it. Although there are these two distinct areas of research, each laboratory has developed its own mdlvxdual model of computation, stored program representation, and machine organization. Across this spectrum of designs there m, however, a significant sharing of concepts. The aim of this palaer m to identify the concepts and relationships that exist both within and between the two areas of research. It does thin by examlmng data-driven and demand-driven architecture at three levels, computation organizatmn, (stored) program organization, and machine organLzation. Finally, a survey of various novel computer architectures under development is given.	computer architecture;dataflow architecture;model of computation;next-generation network;operand;stored-program computer;von neumann architecture	Philip C. Treleaven;David R. Brownbridge;Richard P. Hopkins	1982	ACM Comput. Surv.	10.1145/356869.356873	model of computation;spectrum;data flow diagram;computer science;theoretical computer science;operating system;database;programming language	Arch	-3.0275167662938363	47.13985783248844	66174
10acfeccca30877a9fdd20832261d0aec0e9df5d	customizing the branch predictor to reduce complexity and energy consumption	instruction level parallel;performance evaluation parallel architectures computational complexity;performance evaluation;performance instruction level parallelism complexity branch predictors high end processors;energy consumption clocks switching circuits computer industry shipbuilding industry personal communication networks energy efficiency microprocessors batteries resistance heating;parallel architectures;energy consumption;computational complexity;high power	Computer systems’ prevalence in almost every aspect of society has profound implications for computer system designers. With the number of computers in use (the industry is shipping about a quarter billion CPUs a year just for PCs and servers), their energy efficiency has a significant impact on the economy and the environment. On a smaller scale, high energy consumption in modern microprocessors complicates many system design issues, such as battery life, heat dissipation, and electricity delivery. High-end processors typically incorporate powerful branch predictors consisting of many large structures that together consume a large portion of total chip power. Depending on the application, some of these structures are underutilized for long periods or contribute nothing to the prediction. This results in energy waste and high power consumption. Here, we propose to gauge branch prediction demand and dynamically adjust prediction resources accordingly. Specifically, we customize the size of the branch target buffer (BTB) and the composition of the hybrid direction predictor for each code section. To gauge branch prediction demand, we use a profile-based approach that incurs little runtime overhead and results in simple and straightforward architectural support. The approach is also very accurate because of program behavior repetition. We focus on reducing branch prediction energy consumption in high-performance processors by dynamically reducing the branch predictor’s complexity. To clarify our concept of complexity, we informally classify it into two Michael C. Huang	branch predictor;branch target predictor;central processing unit;complexity;michael sipser;microprocessor;overhead (computing);personal computer;systems design;thermal management (electronics)	Michael C. Huang;Daniel Chaver;Luis Piñuel;Manuel Prieto;Francisco Tirado	2003	IEEE Micro	10.1109/MM.2003.1240209	embedded system;parallel computing;real-time computing;computer science;operating system;computational complexity theory	Arch	-4.321095822361454	54.07761970657941	66214
ca447374fcbf42d2dc7ddb1e627e771b92d47ef2	a generic network interface architecture for a networked processor array (nepa)	network on chip noc;network on chip;networked processor array nepa;design flow;network processor;multiprocessor systemon chip mpsoc;interconnection network;chip;network interface;turbo decoding	Recently Network-on-Chip (NoC) technique has been proposed as a promising solution for on-chip interconnection network. However, different interface specification of integrated components raises a considerable difficulty for adopting NoC techniques. In this paper, we present a generic architecture for network interface (NI) and associated wrappers for a networked processor array (NoC based multiprocessor SoC) in order to allow systematic design flow for accelerating the design cycle. Case studies for memory and turbo decoder IPs show the feasibility and efficiency of our approach.	design flow (eda);interconnection;multiprocessing;network interface controller;network on a chip;processor array	Seung Eun Lee;Jun Ho Bahn;Yoon Seok Yang;Nader Bagherzadeh	2008		10.1007/978-3-540-78153-0_19	chip;embedded system;computer architecture;parallel computing;computer science;design flow;network interface;operating system;network on a chip;network processor;computer network	Arch	1.2639298136731156	49.885305322056425	66296
00b6e7a12db57bd8d4aed223e2ef5bc1a3ec506b	speculation techniques for high level synthesis of control intensive designs	verification;monitors;high level synthesis;control flow;code motion;biased random simulation;testbenches;finite state machine	The quality of synthesis results for most high level synthesis approaches is strongly affected by the choice of control flow (through conditions and loops) in the input description. In this paper, we explore the effectiveness of various types of code motions, such as moving operations across conditionals, out of conditionals (speculation) and into conditionals (reverse speculation), and how they can be effectively directed by heuristics so as to lead to improved synthesis results in terms of fewer execution cycles and fewer number of states in the finite state machine controller. We also study the effects of the code motions on the area and latency of the final synthesized netlist. Based on speculative code motions, we present a novel way to perform early condition execution that leads to significant improvements in highly control-intensive designs. Overall, reductions of up to 38 \% in execution cycles are obtained with all the code motions enabled.	control flow;finite-state machine;heuristic (computer science);high-level programming language;high-level synthesis;netlist;speculative execution	Sumit Gupta;Nicolae Savoiu;Sunwoo Kim;Nikil D. Dutt;Rajesh K. Gupta;Alexandru Nicolau	2001		10.1145/378239.378481	embedded system;parallel computing;real-time computing;verification;computer science;theoretical computer science;operating system;finite-state machine;high-level synthesis;programming language;control flow;algorithm	EDA	-2.6598438658368893	51.689609848885745	66304
06baccdff90673e8b70c9246e4b9b00dc9192786	instruction set design and optimizations for address computation in dsp architectures	address computation;virtual ars;physical ars;loop induction variable;benchmark dsp program;address register;final code quality;dsp architectures;loop construct;code generation;dsp processor;instruction set design;indexing;embedded computing;digital signal processing;software development;instruction sets;graph coloring;hardware;registers;instructional design;embedded systems;real time systems;computer architecture	In this paper we investigate the problem of code generation for address computation for DSP processors. This work is divided into four parts. First, we propose a branch instruction design which can guarantee minimum overhead for programs that make use of implicit indirect addressing. Second, we give a formulation and propose a solution for the problem of allocating address registers (ARs) for array accesses within loop constructs. Third, we describe retargetable approaches for auto-increment (decrement) optimizations of pointer variables, and loop induction variables. Finally, we use a graph coloring technique to allocate physical ARs to the virtual ARs used in the previous phases. The results show that the combination of the above techniques considerably improves the final code quality for benchmark DSP programs.	addressing mode;benchmark (computing);branch (computer science);central processing unit;code generation (compiler);computation;graph coloring;increment and decrement operators;memory address register;overhead (computing);pointer (computer programming);software quality	Guido Araujo;Ashok Sudarsanam;Sharad Malik	1996			computer architecture;parallel computing;real-time computing;computer science	Arch	-0.4879569752078879	51.72523188895361	66427
1f9c20d7344272a2f945448a96f32d8298052228	scheduling of parallelized synchronous dataflow actors for multicore signal processing	digital signal processors;multicore processors;synchronous dataflow;software synthesis;dataflow modeling	Parallelization of Digital Signal Processing (DSP) software is an important trend in Multiprocessor System-on-Chip (MPSoC) implementation. The performance of DSP systems composed of parallelized computations depends on the scheduling technique, which must in general allocate computation and communication resources for competing tasks, and ensure that data dependencies are satisfied. In this paper, we formulate a new type of parallel task scheduling problem called Parallel Actor Scheduling (PAS) for MPSoC mapping of DSP systems that are Z. Zhou ( ) Texas Instruments, Germantown, MD, USA e-mail: z-zhou@ti.com W. Plishker · S.S. Bhattacharyya Department of ECE and UMIACS, University of Maryland, College Park, USA W. Plishker e-mail: plishker@umd.edu S.S. Bhattacharyya e-mail: ssb@umd.edu K. Desnos · M. Pelcat · J.-F. Nezan IETR, INSA Rennes, CNRS UMR 6164, UEB, Rennes, France W. Plishker e-mail: karol.desnos@insa-rennes.fr M. Pelcat e-mail: Maxime.Pelcat@insa-rennes.fr J.-F. Nezan e-mail: jean-francois.nezan@insa-rennes.fr represented as Synchronous Dataflow (SDF) graphs. In contrast to traditional SDF-based scheduling techniques, which focus on exploiting graph level (inter-actor) parallelism, the PAS problem targets the integrated exploitation of both intraand inter-actor parallelism for platforms in which individual actors can be parallelized across multiple processing units. We first address a special case of the PAS problem in which all of the actors in the DSP application or subsystem being optimized are parallel actors (i.e., they can be parallelized to exploit multiple cores). For this special case, we develop and experimentally evaluate a two-phase scheduling framework with three work flows that involve particle swarm optimization (PSO) — PSO with a mixed integer programming formulation, PSO with simulated annealing, and PSO with a fast heuristic based on list scheduling. Then, we extend our scheduling framework to support the general PAS problem, which considers both parallel actors and sequential actors (actors that cannot be parallelized) in an integrated manner. We demonstrate that our PAS-targeted scheduling framework provides a useful range of trade-offs between synthesis time requirements and the quality of the derived solutions. We also demonstrate the performance of our scheduling framework from two aspects: simulations on a diverse set of randomly generated SDF graphs, and implementations of an image processing application and a software defined radio benchmark on a state-of-the-art multicore DSP	actor model;automatic parallelization;benchmark (computing);computation;data dependency;dataflow architecture;digital signal processing;electrical engineering;email;experiment;gnu radio;heuristic;image processing;integer programming;linear programming;list scheduling;mpsoc;mathematical optimization;multi-core processor;multiprocessing;parallel computing;parallel programming model;particle swarm optimization;procedural generation;randomness;requirement;scheduling (computing);simulated annealing;simulation;synthetic intelligence;two-phase locking	Zheng Zhou;William Plishker;Shuvra S. Bhattacharyya;Karol Desnos;Maxime Pelcat;Jean-François Nezan	2016	Signal Processing Systems	10.1007/s11265-014-0956-2	multi-core processor;fair-share scheduling;embedded system;digital signal processor;parallel computing;real-time computing;dynamic priority scheduling;computer science;operating system;distributed computing;programming language	Embedded	-0.5622848666381379	53.31919430707984	66520
22c58b980f613d4d7d91d7bfb72e1e1ba400952f	top-down system level design methodology using specc, vcc and systemc	systemc;top-down system level design;specc;specification languages;c language;vcc;top-down system level design methodology;jpeg encoder;mpeg encoding architecture;mpeg decoding architecture;design methodology;decoding;encoding;design flow;top down;silicon;system level design;convergence;modeling;computer architecture	There appears to be an increasing trend towards the use of the C/C++ language as a basis for the next generation modeling tools and platform methodology to encompass design reuse. However, even with this convergence, industry is suffering the pain that there is no one tool or a complete tool flow methodology that can implement a topdown design methodology from C to silicon . In this paper we suggest a top-down methodology from C to silicon. In our methodology, we focus on methods to make the design flow smooth, efficient, and easy. The proposed methodology is a pure top-down methodology. We developed our design methodology by using SpecC [1], VCC[2], and SystemC[3]. We choose SpecC, VCC and SystemC because they are all C-related and each have strong support in at least one field of design. Our proposal for a methodology is based on our experiences of attempting to model the JPEG encoder with SpecC, SystemC and VCC, and one internal project, attempting to implement architecture exploration for MPEG encoding and decoding using VCC.	c++;design flow (eda);encoder;jpeg;level design;specc;systemc;top-down and bottom-up design;virtual collective consciousness	Lukai Cai;Daniel Gajski;Paul Kritzinger;Mike Olivarez	2002			computer architecture;parallel computing;real-time computing;systems modeling;convergence;design methods;computer science;design flow;top-down and bottom-up design;electronic system-level design and verification;silicon;encoding;computer network	EDA	5.665322100026957	52.9036581295324	66950
26f9d308a2ee997eaf22fbd9f342df9f9f6b38d0	reducing energy consumption in microcontroller-based platforms with low design margin co-processors	coprocessors;parallel processing;personalized medicine;resource management;scalability;microcontrollers;multicore processing;cyber physical systems	Advanced energy minimization techniques (i.e. DVFS, Thermal Management, etc) and their high-level HW/SW requirements are well established in high-throughput multi-core systems. These techniques would have an intolerable overhead in low-cost, performance-constrained microcontroller units (MCU's). These devices can further reduce power by operating at a lower voltage, at the cost of increased sensitivity to PVT variation and increased design margins. In this paper, we propose an runtime environment for next-generation dual-core MCU platforms. These platforms complement a single-core with a low area overhead, reduced design margin shadow-processor. The runtime decreases the overall energy consumption by exploiting design corner heterogeneity between the two cores, rather than increasing the throughput. This allows the platform's power envelope to be dynamically adjusted to application-specific requirements. Our simulations show that, depending on the ratio of core to platform energy, total energy savings can be up to 20%.	central processing unit;dynamic voltage scaling;energy minimization;high- and low-level;high-throughput computing;microcontroller;multi-core processor;overhead (computing);requirement;runtime system;simulation;single-core;thermal management of high-power leds;throughput	Andres Gomez;Christian Pinto;Andrea Bartolini;Davide Rossi;Luca Benini;Hamed Fatemi;José Pineda de Gyvez	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;personalized medicine;parallel computing;real-time computing;scalability;computer science;engineering;operating system;cyber-physical system	EDA	-4.452749745641542	55.45250806428329	66977
ac2f35431384fe6f9e76c7bdd4c1fdb74dfa6442	os-level ipc implementation in complementary multi-processor systems	multiprocessor interconnection networks;dsp os level ipc implementation complementary multiprocessor systems embedded multiprocessor platforms operating systems interprocessor communication hpi single board embedded multimedia system arm;tms320dm642;digital signal processing;instruments;µc os ii ipc interface hpi cmps tms320dm642;µc os ii;complementary multiprocessor systems;os level ipc implementation;cmps;operating systems computers embedded systems multimedia computing multiprocessing systems multiprocessor interconnection networks;usa councils;single board embedded multimedia system;multimedia systems;multimedia computing;embedded systems;ipc interface;operating system;registers;interprocessor communication;data transformation;embedded multiprocessor platforms;arm;linux;multiprocessing systems;digital signal processing hardware registers educational institutions parallel processing embedded system coprocessors algorithm design and analysis logic devices multicore processing;hpi;operating systems computers;parallel processing;dsp;operating systems;hardware;real time systems	Most of embedded multiprocessor platforms are ideal for running diverse operating systems and implementing different applications. Inter-processor communication interface makes it possible for an embedded multi-processor system to easily support multiple subsystems parallel processing. This paper propose a kind of OS-level communication interface implementing method based-on HPI in the Complementary Multi-processor System and present the primary principles and corresponding DSP side code structure for HPI. Take a single-board embedded multimedia system which was integrated three embedded microprocessors (PXA255, TMS320DM642 and SM501) as hardware platform, detailed description of how implement tasks communication and data transform by HPI between different embedded OS(ARM-Linux and µC/OS-II) running on ARM and DSP separately.	arm architecture;bios;care-of address;common object request broker architecture;coprocessor;dsp/bios link;digital signal processor;embedded system;hardware platform interface;linux;linux;memory management;microprocessor;middleware;multiprocessing;operating system;parallel computing;scheduling (computing);single-board computer;software portability;system call	Ke Pei;Gang Zhang;Chang Qing	2010	2010 Asia-Pacific Conference on Wearable Computing Systems	10.1109/APWCS.2010.26	embedded system;parallel processing;embedded operating system;parallel computing;real-time computing;computer science;operating system;digital signal processing	Embedded	3.617345457726314	48.16724289964424	67038
c5c65b789ff6792520ddfb20f155918914424198	accurate and fast system-level power modeling: an xscale-based case study	power estimation;system modeling;simulation framework;system performance;embedded system;embedded systems;embedded system design;system design;design space exploration;power consumption;high speed;power modeling;systemc	Accurate and fast system modeling is central to the rapid design space exploration needed for embedded-system design. With fast, complex SoCs playing a central role in such systems, system designers have come to require MIPS-range simulation speeds and near-cycle accuracy. The sophisticated simulation frameworks that have been developed for high-speed system performance modeling do not address power consumption, although it is a key design constraint. In this paper, we define a simulation-based methodology for extending system performance-modeling frameworks to also include power modeling. We demonstrate the use of this methodology with a case study of a real, complex embedded system, comprising the Intel XScale®g embedded microprocessor, its WMMX#8482; SIMD coprocessor, L1 caches, SDRAM and the on-board address and data buses. We describe detailed power models for each of these components and validate them against physical measurements from hardware, demonstrating that such frameworks enable designers to model both power and performance at high speeds without sacrificing accuracy. Our results indicate that the power estimates obtained are accurate within 5% of physical measurements from hardware, while simulation speeds consistently exceed a million instructions per second (MIPS).	bus (computing);cpu cache;computer hardware;coprocessor;design closure;design space exploration;embedded system;microprocessor;on-board data handling;performance prediction;simd;simulation;system on a chip;systems design;systems modeling;xscale	Ankush Varma;Bruce Jacob;Eric Debes;Igor Kozintsev;Paul Klein	2007	ACM Trans. Embedded Comput. Syst.	10.1145/1274858.1274864	embedded system;computer architecture;real-time computing;systems modeling;computer science;operating system;computer performance;systems design	EDA	-2.615610215191629	55.55202515063798	67075
8b86dffce5bc8cc9d415f13dd831c51cc2cd37c9	an integrated high-level hardware/software partitioning methodology	field programmable gate array;control data flow graphs;modeling technique;hardware software co design;embedded system;design representation;particle swarm optimizer;control data flow graph;embedded system design;hardware software partitioning;particle swarm optimization;fpgas;time to market;power consumption;high level design;software implementation	Embedded systems are widely used in many sophisticated applications. To speed the time-to-market cycle, the hardware and software co-design has become one of the main methodologies in modern embedded systems. The most important challenge in the embedded system design is partitioning; i.e. deciding which modules of the system should be implemented in hardware and which ones in software. Finding an optimal partition is hard because of the large number and different characteristics of the modules that have to be considered.#R##N##R##N#In this article, we develop a new high-level hardware/software partitioning methodology. Two novel features characterize this methodology. Firstly, the Particle Swarm Optimization (PSO) technique is introduced to the Hardware/Software partitioning field. Secondly, the hardware is modeled using two extreme implementations that bound different hardware scheduling alternatives. Our methodology further partitions the design into hardware and software modules at the early Control-Data Flow Graph (CDFG) level of the design; thanks to improved modeling techniques using intermediate-granularity functional modules. A new restarting technique is applied to PSO to avoid quick convergence. This technique is called Re-Excited PSO. Our numerical results prove the usefulness of the proposed technique.#R##N##R##N#The target technology is Field Programmable Gate Arrays (FPGAs). We developed FPGA-based estimation techniques to evaluate the costs of implementing the design components. These costs are the area, delay, latency, and power consumption for both the hardware and software implementations. Hardware/software communication is also taken into consideration.#R##N##R##N#The aforementioned methodology is embodied in an integrated CAD tool for hardware/software co-design. This tool accepts behavioral, un-timed, algorithmic-level, VHDL, design representation, and outputs a valid hardware/software partition and schedule for the design subject to a set of area/power/delay constraints. This tool is code named CUPSHOP for (Cairo University PSo-based Hardware/sOftware Partitioning tool). Finally, a JPEG-encoder case study is used to validate and contrast our partitioning methodology against the prior-art methodologies.	high- and low-level	Mohamed B. Abdelhalim;Serag El-Din Habib	2011	Design Autom. for Emb. Sys.	10.1007/s10617-010-9068-9	hardware compatibility list;embedded system;computer architecture;parallel computing;real-time computing;hardware acceleration;computer science;operating system;software construction;hardware architecture;field-programmable gate array	EDA	0.832719912937112	55.522062711685464	67100
55602b1c980cf93776b682926a5a707593524097	c compiler design for an industrial network processor	c compiler design;compiler intrinsics;industrial asip;application specific architecture;industrial network processor;efficient compiler;network processors;c-level programming;application specific feature;target asip;code generation;c compiler;compiler backend;compilers;embedded processors;register allocation;communication protocol;embedded processor;network processor	One important problem in code generation for embedded processors is the design of efficient compilers for ASIPs with application specific architectures. This paper outlines the design of a C compiler for an industrial ASIP for telecom applications. The target ASIP is a network processor with special instructions for bit-level access to data registers, which is required for packet-oriented communication protocol processing. From a practical viewpoint, we describe the main challenges in exploiting these application specific features in a C compiler, and we show how a compiler backend has been designed that accomodates these features by means of compiler intrinsics and a dedicated register allocator. The compiler is fully operational, and first experimental results indicate that C-level programming of the ASIP leads to good code quality without the need for time-consuming assembly programming.	application-specific instruction set processor;assembly language;bit-level parallelism;central processing unit;code generation (compiler);communications protocol;compiler;embedded system;intrinsic function;network packet;network processor;packet switching;register allocation;software quality	Jens Wagner;Rainer Leupers	2001		10.1145/384197.384218	computer architecture;compiler;parallel computing;real-time computing;compiler correctness;interprocedural optimization;computer science;loop optimization;compiler construction;dead code elimination;optimizing compiler;programming language;inline expansion;intrinsic function;functional compiler;network processor	EDA	1.3826457085909218	50.93869172533462	67164
583ef9ce48a5cf8a0b147e35c03eb702d09af70c	dynamic power management for nonstationary service requests	power management;power consumption;dynamic power management;sliding window;design methodology	Dynamic power management is a design methodology aiming at reducing power consumption of electronic systems, by performing selective shutdown of the idle system resources. The effectiveness of a power management scheme depends critically on an accurate modeling of the environment, and on the computation of the control policy. This paper presents two methods for characterizing nonstationary service requests by means of a prediction scheme based on sliding windows. Moreover; it describes how control policies for nonstationary models can be derived.	computation;interpolation;microsoft windows;power management;shutdown (computing);simulation;stationary process	Eui-Young Chung;Luca Benini;Alessandro Bogliolo;Giovanni De Micheli	1999	Design, Automation and Test in Europe Conference and Exhibition, 1999. Proceedings (Cat. No. PR00078)	10.1145/307418.307456	control engineering;sliding window protocol;embedded system;real-time computing;design methods;computer science;computer network	EDA	-4.415369961286516	59.79274067371436	67199
0b1a093aaad4e3329be4faa9184b34fdebb56753	efficient performance evaluation of multi-core simt processors with hot redundancy	manufacturing yield and manufacturing cost single instruction multiple thread architecture defect tolerance;multicore processing redundancy performance evaluation program processors space exploration manufacturing	"""Redundancy is a well-known technique for improving yield and consequently reducing cost. Prior work has shown that at the architectural level, hot spare components not only increase yield (and reduce costs) when components are defective, but also improve performance. Hot spares, however, complicate system evaluation: the presence of defects affects what resources are available. Accurate performance evaluation thus requires the simulation of the entire population of resulting dice in order to determine the <italic>expected performance</italic> <inline-formula> <tex-math notation=""""LaTeX"""">$E[P]$</tex-math><alternatives><inline-graphic xlink:href=""""mozafari-ieq1-2594957.gif""""/> </alternatives></inline-formula> of the system. While simply expensive for single system evaluation, it is intractable for design space exploration. We therefore introduce two <inline-formula><tex-math notation=""""LaTeX"""">$E[P]$</tex-math> <alternatives><inline-graphic xlink:href=""""mozafari-ieq2-2594957.gif""""/></alternatives></inline-formula> estimation techniques, <inline-formula><tex-math notation=""""LaTeX"""">$\hat{E}_{m}[P]$</tex-math><alternatives> <inline-graphic xlink:href=""""mozafari-ieq3-2594957.gif""""/></alternatives></inline-formula> and <inline-formula> <tex-math notation=""""LaTeX"""">$\hat{E}_{s}[P]$</tex-math><alternatives> <inline-graphic xlink:href=""""mozafari-ieq4-2594957.gif""""/></alternatives></inline-formula>. <inline-formula> <tex-math notation=""""LaTeX"""">$\hat{E}_{m}[P]$</tex-math><alternatives> <inline-graphic xlink:href=""""mozafari-ieq5-2594957.gif""""/></alternatives></inline-formula> evaluates the <inline-formula> <tex-math notation=""""LaTeX"""">$m$</tex-math><alternatives><inline-graphic xlink:href=""""mozafari-ieq6-2594957.gif""""/> </alternatives></inline-formula> most likely configurations, and assumes the performance of all others is zero, reducing simulation by 93 percent. This remains computationally expensive for design space exploration when individual, detailed, simulations require hours. <inline-formula><tex-math notation=""""LaTeX"""">$\hat{E}_{s}[P]$</tex-math> <alternatives><inline-graphic xlink:href=""""mozafari-ieq7-2594957.gif""""/></alternatives></inline-formula> evaluates only the most likely configuration, and assumes its performance for all other configurations, reducing simulation by 98 percent, with no more than 2.6 percent error in <inline-formula><tex-math notation=""""LaTeX"""">$E[P]$</tex-math> <alternatives><inline-graphic xlink:href=""""mozafari-ieq8-2594957.gif""""/></alternatives></inline-formula>, sufficient for differentiating designs along the Pareto-optimal front during design space exploration. Consequently, designers may add redundancy, and evaluate system performance and cost, with no greater design effort than performance evaluation alone."""		Seyyed Hasan Mozafari;Brett H. Meyer	2018	IEEE Transactions on Emerging Topics in Computing	10.1109/TETC.2016.2594957	parallel computing;real-time computing;redundancy (engineering);computer science;very-large-scale integration;dice;design space exploration;multi-core processor;hot spare;approximation error;population	Arch	6.2294577659593395	59.669485970827665	67205
99b95b1563b3e4afdc8f94f8f1702b4785968e49	instruction scheduler generation for retargetable compilation	availability;processor scheduling;formal processor description;generation time;automatic instruction scheduler generation;processor scheduling computer architecture context modeling delay hardware software tools vliw computational modeling availability embedded system;automatic generation;embedded system;retargetable compilation;vliw;computer architecture;embedded systems;computational modeling;processor scheduling embedded systems program compilers;retargetable compilation automatic instruction scheduler generation c compilers embedded systems formal processor description;software tools;c compilers;program compilers;instruction scheduling;context modeling;hardware	The availability of C compilers is crucial to the efficient design of embedded systems. Using virtual resources to automatically generate parts of a compiler's instruction scheduler from a formal processor description significantly reduces the overall scheduler generation time.	compiler;embedded system;scheduling (computing)	Oliver Wahlen;Manuel Hohenauer;Rainer Leupers;Heinrich Meyr	2003	IEEE Design & Test of Computers	10.1109/MDT.2003.1173051	embedded system;availability;scheduler activations;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;context model;instruction scheduling;generation time;computational model;scheduling	EDA	1.8014934631744357	52.68200727364624	67395
c0cc14bdfb6055135f378074d16dd241bca8a02b	technologies for the development of dependable and secure component-based embedded systems: tecnosec project.	fault tolerant;separation of concern;embedded system;error propagation;electromagnetic interference;time to market;point of view	A byte-swapping circuit is provides for interfacing PCI systems to VMEbus systems. The circuit allows the PCI bus to selectively enable or disable byte swapping whether the VMEbus system is in master mode or slave mode. Also, the circuit delays the assertion of the DS0/DS1 VMEbus signals until after the byte swapping circuit has completed its operation, thus avoiding violations of the VMEbus timing specification. Finally, the circuit optimizes its performance during block transfer operations by maintaining its byte-swapping configuration constant during each bus cycle in a given block transfer. Thus, the circuit configures itself for the first bus cycle, but eliminates the reconfiguration delay for each bus cycle afterwards. The circuit can also byte-swap data bytes being transferred on the address bus during multiplexed block transfer cycles.	embedded system	Juan Pardo;David de Andrés;Juan Carlos Ruiz;José Carlos Campelo	2009			electromagnetic interference;embedded system;fault tolerance;real-time computing;separation of concerns;computer science;propagation of uncertainty;distributed computing	Embedded	6.940701204713683	57.705027398661976	67537
0388ded741807aabf65326d57e05399c248fd37d	mom: a matrix simd instruction set architecture for multimedia applications	message oriented middleware registers instruction sets parallel processing kernel computer architecture electronic mail delay embedded computing graphics	MOM is a novel matrix-oriented ISA paradigm for multimedia applications, based on fusing conventional vector ISAs with SIMD ISAs such as MMX. This paper justifies why MOM is a suitable alternative for the multimedia domain due to its efficiency handling the small matrix structures typically found in most multimedia kernels. MOM leverages a performance boost between 1.3x and 4x over more conventional multimedia extensions (such as MMX and MDMX), which already achieve performance benefits ranging from 1.3x to 15x over conventional Alpha code. Moreover, MOM exhibit a high relative performance for low-issue rates and a high tolerance to memory latency. Both advantages present MOM as an attractive alternative for the embedded domain.		Jesús Corbal;Roger Espasa;Mateo Valero	1999		10.1109/SC.1999.10055	reduced instruction set computing;computer architecture;parallel computing;real-time computing;computer science;instruction set	HPC	-4.096376802472357	50.22766631623466	67610
0125e7b13d13871351572e10f5312630336a4b8a	communication-aware module placement for power reduction in large fpga designs		Modern multi-million logic FPGAs allow hardware designers to map increasingly large designs into FPGAs. However, traditional FPGA CAD flows scale poorly for large designs, often producing low quality solutions in terms of performance and power in such cases. To improve the design productivity, modular design methodology partitions a large design into subsystems, compiles them individually and finally collates the individual solutions to complete the mapping process. Existing work has attempted to partition large designs into smaller subsystems, based on the intra-subsystem communication frequencies, to reduce routing power dissipation. However, inter-subsystem communication has not been considered, especially, during the placement stage. In this paper, we first show the adverse effect of ignoring the inter-subsystem communication during the placement stage. Next, we propose an inter-subsystem communication-aware placement technique using a Simulated Annealing based approach to achieve significant power savings. Experimental results show over 7% reduction in routing power when compared to the existing state-of-the-art partitioning flow that does not consider inter-subsystem communication, while the routing power reduction is over 11% when compared to a commercial CAD tool such as Altera Quartus.	altera quartus;computer-aided design;display resolution;field-programmable gate array;inter-process communication;modular design;routing;simulated annealing	Kalindu Herath;Alok Prakash;Udaree Kanewala;Thambipillai Srikanthan	2018	2018 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)	10.1109/ISVLSI.2018.00047	field-programmable gate array;real-time computing;simulated annealing;altera quartus;modular design;computer science	EDA	2.9728491971696616	58.84841349474751	67650
847e1628a3f987d48b62e4f6fd08c2ef8b3a9028	optimized scheduling and mapping of logarithm and arctangent functions on ti tms320c67x processor	communication system;clocks;optimal method;performance improvement;digital communication;efficient implementation;optimal scheduling;modems radar clocks;modems;functional unit;radar	DSP processors have gained more importance and popularity in implementing communication systems. Efficient implementation of logarithm and arctangent functions on DSP processors is necessary for applications such as digital receiver used in modern radar systems and digital communication systems. This paper presents a general scheduling and mapping optimization method based on grain packing to implement the two functions on TI TMS320C67X architecture with multiple parallel function units. Experimental results of our optimized implementation on TMS320C67x have achieved up to 79.5% performance improvement over TI C67x library functions. Our optimization method and techniques can also be applied to other DSP processors with parallel execution units.	central processing unit;digital signal processor;execution unit;mathematical optimization;parallel computing;radar;scheduling (computing);set packing;ti-nspire series	Mei Yang;Yuke Wang;Jinchu Wang;Si-Qing Zheng	2002	2002 IEEE International Conference on Acoustics, Speech, and Signal Processing	10.1109/ICASSP.2002.5745319	embedded system;parallel computing;real-time computing;computer science;electrical engineering;radar;communications system	EDA	3.573510075404191	47.85560842733214	67713
2337e6263c5554f6a669f848c7ed26568fe599de	resource-constrained high-level datapath optimization in asip design	speculation;benchmark testing;dynamic programming;space exploration;high level synthesis;greedy algorithms	In this work, we study the problem of optimizing the datapath under resource constraint in the high-level synthesis of Application-Specific Instruction Processor (ASIP). We propose a two-level dynamic programming (DP) based heuristic algorithm. At the inner level of the proposed algorithm, the instructions are sorted in topological order, and then a DP algorithm is applied to optimize the topological order of the datapath. At the outer level, the space of the topological order of each instruction is explored to iteratively improve the solution. Compared with an optimal brutal-force algorithm, the proposed algorithm achieves near-optimal solution, with only 3% more performance overhead on average but significant reduction in runtime. Compared with a greedy algorithm which replaces the DP inner level with a greedy heuristic approach, the proposed algorithm achieves 48% reduction in performance overhead.	application-specific instruction set processor;datapath;davis–putnam algorithm;dynamic programming;greedy algorithm;heuristic (computer science);high- and low-level;high-level synthesis;iterative method;mathematical optimization;overhead (computing)	Yuankai Chen;Hai Zhou	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		mathematical optimization;greedy algorithm;speculation;parallel computing;computer science;theoretical computer science;high-level synthesis;algorithm	EDA	-0.7809454286303733	51.94388001428149	67895
c9718de24ab20847ed1fe24e81ecc68394db990f	fpga-based hardware acceleration: a cpu/accelerator interface exploration	acceleration power demand finite impulse response filter hardware computer architecture embedded systems;finite impulse response filter;design flow;hardware accelerator;embedded system;acceleration;computer architecture;embedded systems;dynamic power consumption fpga based hardware acceleration cpu accelerator interface exploration;finite impulse response;embedded system design;power consumption;field programmable gate arrays;power demand;power consumption field programmable gate arrays;hardware	One of the main challenges for embedded system designers is to find a tradeoff between performance and power consumption. In order to reach this goal, hardware accelerators have been used to offload specific tasks from the CPU, improving the global performance of the system and reducing its dynamic power consumption. Enabling the use of accelerators could become a tricky task for embedded system designers. This paper presents a complete acceleration design flow for embedded systems with an exploration of different interfaces between CPU and accelerator, analyzing their performances, resources overhead, power consumption, and implementation methods.	central processing unit;design flow (eda);embedded system;field-programmable gate array;hardware acceleration;overhead (computing);performance;requirement	Paulo Da Cunha Possa;David Schaillie;Carlos Valderrama	2011	2011 18th IEEE International Conference on Electronics, Circuits, and Systems	10.1109/ICECS.2011.6122291	embedded system;real-time computing;hardware acceleration;computer hardware;computer science;cpu shielding	EDA	2.158015860746708	53.18697295905743	67917
62ca3aca4fb64984d288c521d563b79aeb7ed6e2	a clustering-based mpsoc design flow for data flow-oriented applications			dataflow architecture;design flow (eda);mpsoc	Joachim Falk	2015				EDA	5.039320372825977	50.41719891448922	67939
4b1bce3a4b4c4aa9a68ab2bd79515650ef12a8bc	comparing the bitstreams of applications specified in hardware join java and handelc	multiplication operator;hardware description languages;hardware java application software field programmable gate arrays high level languages software development management australia lakes computer applications yarn;hardware description languages field programmable gate arrays java c language;c language;place and route;field programmable gate arrays;addition operation synthesis hardware join java fpga bitstreams handelc xilinx foundation tools multiplication operation synthesis;java	In this paper, we investigate the FPGA bitstreams of applications that have been specified in HandelC and Hardware Join Java. We focus mainly on the FPGA area used. To achieve this we specifj, an application consisting of a mixfure of additions and multiplication operations and synthesise them. The resulting Netlists are place and routed using XiIim Foundation Tools to determine the number of slices used by both solutions.	field-programmable gate array;join java;routing	John Hopf	2003		10.1109/FPT.2003.1275786	embedded system;multiplication operator;parallel computing;real-time computing;computer science;theoretical computer science;operating system;strictfp;place and route;real time java;hardware description language;java;field-programmable gate array	EDA	5.6917774010510165	49.274272331980356	68170
f64c58e0ada0be67c3f9f1a98bbe957bc4128dac	gpgpus ecc efficiency and efficacy	error correction codes instruction sets benchmark testing reliability graphics processing units neutrons interrupters;reliability;error correction codes;interrupters;functional interruption;ecc;gpgpu algorithm based fault tolerance functional interruption rate radiation induced silent data corruption safety critical application high performance computing error correcting code mechanism ecc efficiency;abft;silent data corruption;neutrons;gpgpu;graphics processing units;ecc gpgpu silent data corruption functional interruption abft duplication with comparison;fault tolerant computing electronic engineering computing error correction codes;duplication with comparison;benchmark testing;instruction sets	In this paper we assess and discuss the efficiency and overhead of the Error-Correcting Code (ECC) mechanism available on modern GPGPUs, which are increasingly used for both High Performance Computing and safety-critical applications. Both the resilience to radiation-induced silent data corruption and functional interruption are experimentally and analytically addressed. The provided experimental analysis demonstrates that the ECC significantly reduces the occurrence of silent data corruption but may not be sufficient to guarantee high reliability. Moreover, the ECC increases the GPGPU functional interruption rate. Finally, the ECC performances and reliability are compared to Algorithm-Based Fault Tolerance and Duplication With Comparison strategies.	algorithm;computation;ecc memory;experiment;fault tolerance;forward error correction;general-purpose computing on graphics processing units;interrupt;overhead (computing);performance;semiconductor device fabrication	Daniel A. G. de Oliveira;Paolo Rech;Laércio Lima Pilla;Philippe Olivier Alexandre Navaux;Luigi Carro	2014	2014 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)	10.1109/DFT.2014.6962085	embedded system;benchmark;parallel computing;real-time computing;computer science;theoretical computer science;operating system;instruction set;reliability;neutron;general-purpose computing on graphics processing units;algorithm	Embedded	6.953632520188537	60.33479421221462	68215
bfe145907923cc81e806a03a8300b88fdab65789	simulation pattern capturing system for design verification using a dynamic high speed functional tester (dhsft)			simulation	David Florcik;David Low	1983			computer science;computer architecture	EDA	8.22071876224748	52.06947639281901	68315
8b7ccca65ada504810ca803d6276d82d3bfa6667	swsl: software synthesis for network lookup	high level synthesis;design;data structures;software architecture	Data structure lookups are among the most expensive operations on routers' critical path in terms of latency and power. Therefore, efficient lookup engines are crucial. Several approaches have been proposed,based on either custom ASICs, general-purpose processors,or specialized engines. ASICs enable high performance but have long design cycle and scarce flexibility, while general-purpose processors present the opposite trade-off. Specialized programmable engines achieve some of the benefits of both approaches, but are still hard to program and limited either in terms of flexibility or performance.  In this paper we investigate a different design point. Our solution,SWSL (SoftWare Synthesis for network Lookup) generates hardware logic directly from lookup applications written in C++. Therefore, it retains a simple programming model yet leads to significant performance and power gains. Moreover, compiled application can be deployed on either FPGA or ASIC, enabling a further trade-off between flexibility and performance. While most high-level synthesis compilers focus on loop acceleration, SWSL generates entire lookup chains performing aggressive pipelining to achieve high throughput.  Initial results are promising: compared with a previously proposed solution, SWSL gives 2 - 4x lower latency and 3 - 4x reduced chip area with reasonable power consumption.	application-specific integrated circuit;c++;central processing unit;compiler;critical path method;data structure;electronic hardware;field-programmable gate array;general-purpose markup language;hardware acceleration;high- and low-level;high-level synthesis;lookup table;pipeline (computing);programming model;router (computing);throughput	Sung Jin Kim;Lorenzo De Carli;Karthikeyan Sankaralingam;Cristian Estan	2013	Architectures for Networking and Communications Systems		software architecture;programming;design;throughput;computer architecture;parallel computing;real-time computing;data structure;computer science;operating system	Arch	0.4279155766324993	49.875060645656745	68397
b42acda0a523243ec72dac7131136edad501ac4a	evaluation of dvfs techniques on modern hpc processors and accelerators for energy-aware applications	gpu;hpc;energy aware;user;dvfs;application	Energy efficiency is becoming increasingly important for computing systems, in particular for large scale HPC facilities. In this work we evaluate, from an user perspective, the use of Dynamic Voltage and Frequency Scaling (DVFS) techniques, assisted by the power and energy monitoring capabilities of modern processors in order to tune applications for energy efficiency. We run selected kernels and a full HPC application on two high-end processors widely used in the HPC context, namely an NVIDIA K80 GPU and an Intel Haswell CPU. We evaluate the available trade-offs between energy-to-solution and time-to-solution, attempting a function-by-function frequency tuning. We finally estimate the benefits obtainable running the full code on a HPC multi-GPU node, with respect to default clock frequency governors. We instrument our code to accurately monitor power consumption and execution time without the need of any additional hardware, and we enable it to change CPUs and GPUs clock frequencies while running. We analyze our results on the different architectures using a simple energy-performance model, and derive a number of energy saving strategies which can be easily adopted on recent high-end HPC systems for generic applications. Copyright c © 0000 John Wiley & Sons, Ltd.	2.5d;best practice;central processing unit;clock rate;code;computer cooling;dynamic voltage scaling;experiment;frequency scaling;graphics processing unit;haswell (microarchitecture);john d. wiley;library (computing);mpeg transport stream;mathematical optimization;megabyte;memory bound function;performance application programming interface;point of view (computer hardware company);real life;run time (program lifecycle phase);year 10,000 problem	Enrico Calore;Alessandro Gabbana;Sebastiano Fabio Schifano;Raffaele Tripiccione	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4143	parallel computing;computer science;efficient energy use;frequency scaling;clock rate;real-time computing	HPC	-3.262907466971553	55.36901677685641	68713
6e4a4140e76264fbe9512a38fcd0783d37241796	energy efficient parameterized fft architecture	n point fft energy efficient parameterized fft architecture classic fast fourier transform energy efficient designs fpga design trade offs design space exploration algorithm mapping parameters vertical parallelism horizontal parallelism decomposition based fft algorithms architecture parameters memory elements interconnection network pipeline stages;logic design fast fourier transforms field programmable gate arrays;logic design;computer architecture random access memory algorithm design and analysis pipelines field programmable gate arrays delays;fast fourier transforms;field programmable gate arrays	In this paper, we revisit the classic Fast Fourier Transform (FFT) for energy efficient designs on FPGAs. A parameterized FFT architecture is proposed to identify the design trade-offs in achieving energy efficiency. We first perform design space exploration by varying the algorithm mapping parameters, such as the degree of vertical and horizontal parallelism, that characterize decomposition based FFT algorithms. Then we explore an energy efficient design by empirical selection on the values of the chosen architecture parameters, including the type of memory elements, the type of interconnection network and the number of pipeline stages. The trade offs between energy, area, and time are analyzed using two performance metrics: the energy efficiency (defined as the number of operations per Joule) and the Energy×Area×Time (EAT) composite metric. From the experimental results, a design space is generated to demonstrate the effect of these parameters on the various performance metrics. For N-point FFT (16 ≤ N ≤ 1024), our designs achieve up to 28% and 38% improvement in the energy efficiency and EAT, respectively, compared with a state-of-the-art design.	algorithm;design space exploration;fast fourier transform;field-programmable gate array;interconnection;joule;parallel computing;pipeline (computing)	Ren Chen;Hoang Le;Viktor K. Prasanna	2013	2013 23rd International Conference on Field programmable Logic and Applications	10.1109/FPL.2013.6645545	embedded system;fast fourier transform;parallel computing;logic synthesis;split-radix fft algorithm;computer science;theoretical computer science;operating system;prime-factor fft algorithm;field-programmable gate array	EDA	0.48582713328155774	49.96701810457299	69199
484d6f50d71c0a8c1b57855cff7f032dd23c508d	a performance evaluation method for optimizing embedded applications	performance evaluation;low energy;optimization methods hardware application software energy consumption throughput embedded software engines design optimization software performance detectors;transmission error;memory access;embedded systems;integrated circuit design;system on chip;energy consumption;memory architecture;system on chip performance evaluation memory architecture error detection microprocessor chips integrated circuit design embedded systems;source code;interconnection network performance evaluation method embedded application optimization simulation based method memory access energy consumption execution time embedded software dedicated processing engine soc design pseudo random number generator transmission error detector chip area processor based engine;pseudo random number generator;error detection;process engineering;high throughput;hardware implementation;embedded software;microprocessor chips	Performance evaluation is an important step for designing embedded applications that require small footprints, low energy consumption and high throughput. We present a simulation-based method to characterize several resource properties (memory accesses, energy consumption, execution time) of embedded software that runs on dedicated processing engines targeted for SoC designs. The results of the characterization process are back-annotated to the source code to aid the designer in optimizing the implementation. Our approach allows the replacement of software parts by hardware units to speed up processing. We have performed case studies with software and hardware implementations of a pseudo-random number generator and a transmission error detector. The results show that computation speed-ups and energy reductions up to a factor of 15 can be obtained with implementations that exploit hardware extensions.	arbitrary code execution;computation;coprocessor;embedded software;embedded system;error detection and correction;hardware acceleration;interconnection;lazy evaluation;performance evaluation;pseudorandom number generator;pseudorandomness;random number generation;run time (program lifecycle phase);simulation;throughput	Matthias Grünewald;Jörg-Christian Niemann;Ulrich Rückert	2003		10.1109/IWSOC.2003.1212997	embedded system;parallel computing;real-time computing;computer science	Embedded	3.129357513197905	52.663299835387846	69200
52b67699115f726b82a7112b890057a3a0f2efbd	analyzing performance of multi-cores and applications with cache-aware roofline model		To satisfy growing computational demands of modern applications, significant enhancements have been introduced in the contemporary processor architectures with the aim to increase their attainable performance, such as increased number of cores, improved capability of memory subsystem and enhancements in the processor pipeline [1]. Therefore, the performance improvements are usually coupled with an increased complexity at the architecture level, which imposes additional challenges when designing, prototyping and optimizing the execution of real-world applications on a given compute platform. Since the application performance depends on multiple factors, e.g., multi-threading, vectorization efficiency and memory accesses, achieving the most efficient execution is not a trivial task, especially when aiming at fully exploiting the capabilities of modern multi-core processors.	automatic vectorization;central processing unit;computation;multi-core processor;multithreading (computer architecture);thread (computing)	Diogo Marques;Helder Duarte;Leonel Sousa;Aleksandar Ilic	2017	2017 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCS.2017.158	instruction pipeline;parallel computing;architecture;vectorization (mathematics);cache;computer science	HPC	-3.9241420081579674	49.95368801931938	69522
fef6267263c4ca25f68d281bed66fa5c89ee7733	32-bit risc cpu based on mips instruction fetch module design	synchronous control module;mips instruction format;random access memory;reduced instruction set computing instruction fetch module design mips instruction format instruction data path decoder module function risc cpu instruction set fetch instruction latch module arithmetic module synchronous control module quartus ll;instruction fetch module design;decoding;design engineering;clocks;data path;radiation detectors;reduced instruction set computing;data engineering;latch module;data mining;fetch instruction;decoder module function;mips;registers;pipelines;risc cpu instruction set;pipeline mips data flow data path;arithmetic;quartus ll;artificial intelligence;computer science;design theory;data flow;arithmetic module;instruction data path;reduced instruction set computing registers pipelines computer science data engineering decoding arithmetic artificial intelligence design engineering information analysis;reduced instruction set computing instruction sets microcomputers;information analysis;microcomputers;pipeline;instruction sets	In this paper, we analyze MIPS instruction format¿ instruction data path¿decoder module function and design theory basend on RISC CPUT instruction set. Furthermore, we design instruction fetch(IF) module of 32-bit CPU based on RISC CPU instruction set. Function of IF module mainly includes fetch instruction and latch module¿address arithmetic module¿ check validity of instruction module¿synchronous control module. Function of IF modules are implemented by pipeline and simulated successfully on QuartusII¿	32-bit;altera quartus;central processing unit;control unit;debugging;pipeline (computing);prospective search;top-down and bottom-up design;vhdl	Kui Yi;YueHua Ding	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.158	classic risc pipeline;data flow diagram;reduced instruction set computing;computer architecture;parallel computing;information engineering;computer hardware;computer science;instructions per second;instruction set;instruction register;microcomputer;pipeline transport;cycles per instruction;designtheory;processor register;data analysis;particle detector;pipeline	EDA	7.744657134259306	47.58205042094817	69617
7c70820015bb02a93448554a93523c220ec94a22	a scheduling approach for distributed resource architectures with scarce communication resources	communication resources;interconnect;compiler;compilers;distributed resource;data structures;scheduling;scarce resources;distributed resource architectures;instruction level parallelism;distributed resources;multi core	Advances in semiconductor fabrication technology will continue to enable exponential increase in the number of transistors available. However, conventional architectures, such as superscalars or VLIWs, will not be able to use the abundant on-chip resources efficiently to achieve high performance because of their inherent lack of scalability. In order to overcome the deficiencies of conventional architectures, architects have come up with a new breed of processors that have distributed resources interconnected via sophisticated networks. Although these processors have the potential of achieving higher performance and being more power efficient than conventional processors, the distributed resources make it difficult to write a compiler that generates a high quality schedule for an application. In this paper, we propose a scheduling approach targeted for such distributed resource architectures. Our approach simultaneously places and schedules an operation and routes communications between consumer and producer operations. We introduce techniques to use the scarce interconnect resources efficiently and empirically to show that they contribute to speeding up applications. In addition, we present a simple yet flexible way to describe the target architecture and generate data structures used for scheduling that represents the interconnect structure of the target architecture.	scheduling (computing)	Akira Hatanaka;Nader Bagherzadeh	2011	IJHPSA	10.1504/IJHPSA.2011.038054	compiler;parallel computing;real-time computing;data structure;computer science;operating system;distributed computing;programming language	HPC	-0.4221732785530715	48.71504197342283	69888
40944f1195c5e737e9bd33b77973974456fc84cf	exploiting dead value information	processor architecture;registers;simultaneous multithreading;performance;switches;compiler optimizations;cache size;loop tiling;remote procedure calls;hardware;cycle time;scheduling;context switches;parallel programming;register file;instruction sets	We describe Dead Value Information (DVI) and introduce three new optimizations which exploit it. DVI provides assertions that certain register values are dead, meaning they will not be read before being overwritten. The processor can use DVI to track dead registers and dynamically eliminate unnecessary save and restore instructions from the execution stream at procedure calls and context switches. Our results indicate that dynamic saves and restore instances can be reduced by 46% for procedure calls and by 51% for context switches. In addition, save/restore elimination for procedure calls can improve overall performance by up to 5%. DVI also allows the processor manage physical registers to efficiently, reducing the size requirements of the physical register file. When the system clock rate is proportional to the register file cycle time, this optimization can improve performance. All of these optimizations can be supported with only a few new instructions and minimal additional hardware structures.	assertion (software development);clock rate;context switch;digital visual interface;mathematical optimization;network switch;register file;requirement	Milo M. K. Martin;Amir Roth;Charles N. Fischer	1997			loop tiling;computer architecture;parallel computing;real-time computing;cpu cache;performance;microarchitecture;cycle time variation;computer science;operating system;instruction set;optimizing compiler;programming language;remote procedure call;scheduling;simultaneous multithreading;register file	Arch	-4.5064767873938605	52.44962618387881	70008
4c94721441550b92082476c9a8038b8670fe0a74	hw/sw co-detection of transient and permanent faults with fast recovery in statically scheduled data paths	vliw processor;half scale residue transfer characteristic rtc;triple modular redundant;reliability;hardware software codesign;circuit faults;fault tolerant;statically scheduled data paths;clocks;processor scheduling;triple modular redundancy;embedded system;vliw;large scale;statistical analysis;fault tolerant systems;registers;circuit faults fault detection processor scheduling hardware clocks vliw power system reliability embedded system production application specific processors;scheduling;fault detection;fault tolerance;statistical analysis fault diagnosis fault tolerance hardware software codesign reliability scheduling;differential nonlinearity dnl;super scalar processor fault tolerance;dithering;residue amplifier ra;application specific processors;integral nonlinearity inl;production;hardware overhead;reliability analysis;cyclic analog to digital converter adc;hardware software based technique;power system reliability;permanent faults;triple modular redundancy hw sw codetection permanent faults statically scheduled data paths hardware software based technique super scalar processor fault tolerance vliw processor reliability analysis hardware overhead;hw sw codetection;fault diagnosis;hardware	This paper describes a hardware-/software-based technique to make the data path of a statically scheduled super scalar processor fault tolerant. The results of concurrently executed operations can be compared with little hardware overhead in order to detect a transient or permanent fault. Furthermore, the hardware extension allows to recover from a fault within one to two clock cycles and to distinguish between transient and permanent faults. If a permanent fault was detected, this fault is masked for the rest of the program execution such that no further time is needed for recovering from that fault. The proposed extensions were implemented in the data path of a simple VLIW processor in order to prove the feasibility and to determine the hardware overhead. Finally a reliability analysis is presented. It shows that for medium and large scaled data paths our extension provides an up to 98% better reliability than triple modular redundancy.	central processing unit;clock signal;fault tolerance;overhead (computing);scalar processor;shattered world;superscalar processor;triple modular redundancy	Mario Schölzel	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)	10.1109/DATE.2010.5456957	embedded system;fault tolerance;electronic engineering;parallel computing;real-time computing;computer science;stuck-at fault;operating system;fault model;statistics	EDA	7.574634597392079	58.80104245633782	70384
56de17b145c2757be322c763d4a27b799f98167a	design space exploration of real-time multi-media mpsocs with heterogeneous scheduling policies	space exploration real time systems multimedia systems system on a chip intellectual property libraries dynamic scheduling energy management power system management costs;dynamic power management real time multimedia application multiprocessor system on chip mpsoc heterogeneous scheduling policy hardware software intellectual property software library cosynthesis framework design space exploration energy reduction technique;energy;software library;hardware software codesign;software libraries;real time;real time multimedia application;real time scheduling mpsoc cosynthesis energy;heterogeneous scheduling policy;system on chip hardware software codesign multimedia computing multiprocessing systems real time systems scheduling software libraries;multimedia computing;hardware software intellectual property;system on chip;scheduling;real time scheduling;multi processor system on chip;multiprocessor system on chip;energy reduction technique;multiprocessing systems;design space exploration;cosynthesis framework;dynamic power management;mpsoc;cosynthesis;real time systems	Real-time multi-media applications are increasingly being mapped onto MPSoC (multi-processor system-on-chip) platforms containing hardware-software IPs (intellectual property) along with a library of common scheduling policies such as EDF, RM. The choice of a scheduling policy for each IP is a key decision that greatly affects the design's ability to meet real-time constraints, and also directly affects the energy consumed by the design. We present a cosynthesis framework for design space exploration that considers heterogenous scheduling while mapping multimedia applications onto such MPSoCs. In our approach, we select a suitable scheduling policy for each IP such that system energy is minimized - our framework also includes energy reduction techniques utilizing dynamic power management. Experimental results on a realistic multi-mode multi-media terminal application demonstrate that our approach enables us to select design points with up to 60.5% reduced energy for a given area constraint, while meeting all real-time requirements. More importantly, our approach generates a tradeoff space between energy and cost allowing designers to comparatively evaluate multiple system level mappings.	design space exploration;earliest deadline first scheduling;mpsoc;multiprocessing;power management;real-time clock;real-time transcription;requirement;scheduling (computing);system on a chip	Minyoung Kim;Sudarshan Banerjee;Nikil D. Dutt;Nalini Venkatasubramanian	2006	Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS '06)	10.1145/1176254.1176261	fair-share scheduling;embedded system;parallel computing;real-time computing;dynamic priority scheduling;computer science	EDA	1.4836513796452253	54.406531596733785	70575
e47b6a3da335297951982cd662c61bb25272684c	a fully-synthesizable single-cycle interconnection network for shared-l1 processor clusters	multiprocessor interconnection networks;routing clocks delay multiprocessor interconnection bandwidth;shared memory systems multiprocessor interconnection networks;clocks;routing;combinational mesh of trees interconnection network;synthesizable rtl;physical optimization;interconnection network;design constraints;shared memory systems;l1 coupled processor clusters;design constraints fully synthesizable single cycle interconnection network shared l1 processor clusters shared l1 memory tightly coupled multicore processor clusters combinational mesh of trees interconnection network high performance single cycle communication l1 coupled processor clusters interconnect ip synthesizable rtl design automation strategy mixing advanced synthesis physical optimization;interconnect ip;design automation strategy mixing advanced synthesis;bandwidth;shared l1 memory;fully synthesizable single cycle interconnection network;shared l1 processor clusters;high performance single cycle communication;multiprocessor interconnection;tightly coupled multicore processor clusters	Shared L1 memory is an interesting architectural option for building tightly-coupled multi-core processor clusters. We designed a parametric, fully combinational Mesh-of-Trees (MoT) interconnection network to support high-performance, single-cycle communication between processors and memories in L1-coupled processor clusters. Our interconnect IP is described in synthesizable RTL and it is coupled with a design automation strategy mixing advanced synthesis and physical optimization to achieve optimal delay, power, area (DPA) under a wide range of design constraints. We explore DPA for a large set of network configurations in 65nm technology. Post placement&routing delay is 38FO4 for a configuration with 8 processors and 16 32-bit memories (8×16); when the number of both processors and memories is increased by a factor of 4, the delay increases almost logarithmically, to 84FO4, confirming scalability across a significant range of configurations. DPA tradeoff flexibility is also promising: in comparison to the maxperformance 16×32 configuration, there is potential to save power and area by 45% and 12 % respectively, at the expense of 30% performance degradation.	32-bit;central processing unit;combinational logic;elegant degradation;interconnection;logic synthesis;mathematical optimization;multi-core processor;scalability	Abbas Rahimi;Igor Loi;Mohammad Reza Kakoee;Luca Benini	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763085	routing;computer architecture;parallel computing;real-time computing;computer science;bandwidth;computer network	EDA	2.9090996714632933	58.84639206654235	70649
b6cfc771d58107fbd2d8c96f2247ea90253c1656	supporting runtime reconfigurable vliws cores through dynamic binary translation		Single ISA-Heterogeneous multi-cores such as the ARM big.LITTLE have proven to be an attractive solution to explore different energy/performance trade-offs. Such architectures combine Out of Order cores with smaller in-order ones to offer different power/energy profiles. They however do not really exploit the characteristics of workloads (compute-intensive vs. control dominated). In this work, we propose to enrich these architectures with runtime configurable VLIW cores, which are very efficient at compute-intensive kernels. To preserve the single ISA programming model, we resort to Dynamic Binary Translation, and use this technique to enable dynamic code specialization for Runtime Reconfigurable VLIWs cores. Our proposed DBT framework targets the RISC-V ISA, for which both OoO and in-order implementations exist. Our experimental results show that our approach can lead to best-case performance and energy efficiency when compared against static VLIW configurations.	best, worst and average case;binary translation;mathematical optimization;multi-core processor;open-source software;partial template specialization;programming model;prototype;risc-v;scheduling (computing);very long instruction word	Simon Rokicki;Erven Rohou;Steven Derrien	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342160	implementation;parallel computing;computer science;programming paradigm;out-of-order execution;multi-core processor;very long instruction word;efficient energy use;exploit;binary translation	EDA	-2.8391887545105545	49.734274972526144	70905
13345c01ce58c2b7e06cb34ef90cdd8717a98b81	computing approximately, and efficiently	fpga;approximation algorithms;median filter;computer architecture;resilience;hardware;algorithm design and analysis	Recent years have witnessed significant interest in the area of approximate computing. Much of this interest stems from the quest for new sources of computing efficiency in the face of diminishing benefits from technology scaling. We argue that trends in computing workloads will greatly increase the opportunities for approximate computing, describe the vision and key principles that have guided our work in this area, and outline a range of approximate computing techniques that we have developed at all layers of the computing stack, spanning circuits, architecture, and software.	approximate computing;approximation algorithm;file spanning;image scaling;principle of abstraction	Swagath Venkataramani;Srimat T. Chakradhar;Kaushik Roy;Anand Raghunathan	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		median filter;embedded system;parallel computing;reconfigurable computing;computer science;theoretical computer science;operating system;end-user computing;data-intensive computing;distributed computing;soft computing;utility computing;field-programmable gate array;grid computing;computing with memory;unconventional computing;autonomic computing	EDA	-2.587660724883481	47.46300786402374	70920
5ba369de27330ce1befc339e16aec923d0a63c76	smart camera based on embedded hw/sw coprocessor	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices	This paper describes an image acquisition and a processing system based on a new coprocessor architecture designed for CMOS sensor imaging. The system exploits the full potential CMOS selective access imaging technology because the coprocessor unit is integrated into the image acquisition loop. The acquisition and coprocessing architecture are compatible with the majority of CMOS sensors. It enables the dynamic selection of a wide variety of acquisition modes as well as the reconfiguration and implementation of high-performance image preprocessing algorithms (calibration, filtering, denoising, binarization, pattern recognition). Furthermore, the processing and data transfer, from the CMOS sensor to the processor, can be operated simultaneously to increase achievable performances. The coprocessor architecture has been designed so as to obtain a unit that can be configured on the fly, in terms of type and number of chained processing stages (up to 8 successive predefined preprocessing stages), during the image acquisition process that can be defined by the user according to each specific application requirement. Examples of acquisition and processing performances are reported and compared to classical image acquisition systems based on standard modular PC platforms. The experimental results show a considerable increase of the achievable performances.	algorithm;cmos;central processing unit;coprocessor;embedded system;image sensor;imaging technology;noise reduction;on the fly;pattern recognition;performance;preprocessor;smart camera;test case	Romuald Mosqueron;Julien Dubois;Marco Mattavelli;David Mauvilet	2008	EURASIP J. Emb. Sys.	10.1155/2008/597872	embedded system;real-time computing;computer hardware;computer science;operating system	Robotics	5.344921225181668	46.607915579116494	71074
990a2cbed13b129d6cbf82399dfef0399e1aa071	a reduced high-level-language instruction set	computer aided instruction;computer architecture;computer aided software engineering;computational modeling;transportation;computer aided instruction hardware computer architecture transportation computational modeling encoding computer aided software engineering instruction sets;encoding;high level language;instruction sets;hardware	The Object Pascal Architecture provides 22 simple stack instructions which enable straightforward compilation of Pascal-like languages.	compiler;object pascal	Peter Schulthess	1984	IEEE Micro	10.1109/MM.1984.291227	one instruction set computer;transport;reduced instruction set computing;computer architecture;computing;addressing mode;computer science;theoretical computer science;minimal instruction set computer;operating system;instruction set;computational model;computer-aided software engineering;high-level programming language;encoding;orthogonal instruction set	Arch	0.5619362406330439	47.11834779130009	71136
05376abbc42e914e792bf54dc895fde582f43096	high-level data-access analysis for characterisation of (sub)task-level parallelism on java	high level data access analysis;instruction level parallel;program instrumentation;automated design;instruments;constraint optimization;java programming;multiprocessor systems;perforation;platform independent data access analysis;java applications high level data access analysis subtask level parallelism embedded systems multiprocessor systems instruction level parallelism compiler tools sequential object oriented programs parallel performance analysis platform independent data access analysis java programs automated design time analysis program instrumentation program profiling support post processing analysis;parallel programming;subtask level parallelism;object oriented programming;program compilers java multiprocessing systems software tools parallel programming object oriented programming data analysis;embedded system;program profiling support;embedded systems;data analysis;compiler tools;automated design time analysis;parallel performance analysis;performance analysis;data access;timing analysis;data analysis java performance analysis object oriented modeling embedded system instruments constraint optimization program processors usability performance loss;post processing analysis;sequential object oriented programs;java applications;java programs;software tools;multiprocessing systems;program compilers;instruction level parallelism;usability;performance loss;program processors;object oriented modeling;java	In the era of future embedded systems the designer is confronted with multi-processor systems both for performance and energy reasons. Exploiting (sub)task-level parallelism is becoming crucial because the instruction-level parallelism alone is insufficient. The challenge is to build compiler tools that support the exploration of the task-level parallelism in the programs. To achieve this goal, we have designed an analysis framework to evaluate the potential parallelism from sequential object-oriented programs. Parallel-performance and data-access analysis are the crucial techniques for estimation of the transformation effects. We have implemented support for platform-independent data-access analysis and profiling of Java programs, which is an extension to our earlier parallel-performance analysis framework. The toolkit comprises automated design-time analysis for performance and data-access characterisation, program instrumentation, program-profiling support and post-processing analysis. We demonstrate the usability of our approach on a number of realistic Java applications.	compiler;embedded system;instruction-level parallelism;java;multiprocessing;parallel computing;profiling (computer programming);task parallelism;usability;video post-processing	Richard Stahl;Robert Pasko;Francky Catthoor;Rudy Lauwereins;Diederik Verkest	2004	Ninth International Workshop on High-Level Parallel Programming Models and Supportive Environments, 2004. Proceedings.	10.1109/HIPS.2004.1299188	data access;constrained optimization;computer architecture;parallel computing;usability;computer science;operating system;data parallelism;data analysis;programming language;object-oriented programming;java;instruction-level parallelism;static timing analysis;implicit parallelism;task parallelism	PL	-3.0638439172557654	51.80987059526298	71329
5ce26774cc087067b5578d321b24caed072137b7	on the performance and energy efficiency of fpgas and gpus for polyphase channelization	memory management;clocks;finite impulse response filters;graphics processing units;fixed width gpu architecture energy efficiency fpga wideband channelization front end subsystem software defined radio sdr low power mobile gpu optimized polyphase channelization mega samples per second msps watt hardware platform flexible datapath width;field programmable gate arrays;software radio energy conservation field programmable gate arrays graphics processing units low power electronics;field programmable gate arrays graphics processing units finite impulse response filters hardware memory management clocks;hardware	Wideband channelization is an important and computationally demanding task in the front-end subsystem of several software-defined radios (SDRs). The hardware that supports this task should provide high performance, consume low power, and allow flexible implementations. Several classes of devices have been explored in the past, with the FPGA proving to be the most popular as it reasonably satisfies all three requirements. However, the growing presence of low-power mobile GPUs holds much promise with improved flexibility for instant adaptation to different standards. Thus, in this paper, we present optimized polyphase channelizations for the FPGA and GPU, respectively, that must consider power and accuracy requirements in the context of a military application. The performance in mega-samples per second (MSPS) and energy efficiency in MSPS/watt are compared between the two classes of hardware platforms: FPGA and GPU. The results show that by exploiting the flexible datapath width of FPGAs, FPGA implementations generally deliver an order-of-magnitude better performance and energy efficiency over fixed-width GPU architectures.	channelization (telecommunications);datapath;field-programmable gate array;filter bank;graphics processing unit;ibm notes;low-power broadcasting;performance per watt;polyphase matrix;polyphase quadrature filter;reconfigurable computing;requirement	Vignesh Adhinarayanan;Thaddeus Koehn;Krzysztof Kepa;Wu-chun Feng;Peter M. Athanas	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032542	embedded system;computer hardware;reconfigurable computing;computer science;operating system;field-programmable gate array;memory management	HPC	2.371303545915512	47.81659594681486	71396
045784f3157e728a296e8d982d7c804b5121fcb0	code generation for custom architectures using constraint programming	datavetenskap datalogi;inbaddad systemteknik	As custom multicore architectures become more and more common for DSP applications, instruction selection and scheduling for such applications and architectures become important topics. In this paper, we explore the effects of defining the problem of finding an optimal instruction selection and scheduling as a constraint satisfaction problem (CSP). We incorporate methods based on sub-graph isomorphism and global constraints designed for scheduling. We experiment using several media applications on a custom architecture, a generic VLIW architecture and a RISC architecture, all three with several cores. Our results show that defining the problem with constraints gives flexibility in modeling, while state-of-the-art constraint solvers enable optimal solutions for large problems, hinting a new method for code generation.	code generation (compiler);constraint programming;constraint satisfaction problem;digital signal processor;graph isomorphism;instruction selection;multi-core processor;scheduling (computing)	Mehmet Ali Arslan	2016			real-time computing;simulation;computer science;theoretical computer science	EDA	-0.16367428456891397	51.788499255148146	71514
e0c06af3fd0616ebd489350437fa240b551fa30f	power-aware resource management techniques for low-power embedded systems		Energy consumption has become one of the most important design constraints for modern embedded systems, especially for mobile embedded systems that operate with a limited energy source such as batteries. For these systems, the design process is characterized by a tradeoff between a need for high performance and low power consumption, emphasizing high performance to meeting the performance constraints while minimizing the power consumption. Decreasing the power consumption not only helps with extending the battery lifetime in portable devices, but is also a critical factor in lowering the packaging and the cooling costs of embedded systems. While better low-power circuit design techniques have helped to lower the power consumption [9, 41, 34], managing power dissipation at higher abstraction levels can considerably decrease energy requirements [14, 3]. Since we will be focusing on dynamic power P dynamic in this chapter, the total power dissipation P CMOS can be approximated as P CMOS ≈ P dynamic. The dynamic power of CMOS circuits is dissipated when the output capacitance is charged or discharged, and is given by P dynamic = α · C L · V 2 dd · f clk where α is the switching activity (the average number of high-to-low transitions per cycle), C L is the load capacitance, V dd is the supply voltage, and f clk is the clock frequency. The energy consumption during the time interval [0, T ] is given by E = T 0 P (t)dt ∝ V 2 dd · f clk · T = V 2 dd · N cycle where P (t) is the power dissipation at t and N cycle is the number of clock cycles during the interval [0, T ]. These equations indicate that a significant energy saving can be achieved by reducing the supply voltage V dd ; a decrease in the supply voltage by a factor of two yields a decrease in the energy consumption by a factor of four. In this chapter, we focus on the system-level power-aware resource management techniques. System-level power management technqiues can be roughly classified into two categories, dynamic voltage scaling (DVS) and dynamic power management (DPM). Dynamic voltage scaling (DVS) [7], which can be applied in both hardware and software desgin abstractions, is one of most effective design techniques in minimizing the energy consumption of VLSI systems. Since the energy consumption E of CMOS circuits has a quadratic dependency on the supply …	advanced configuration and power interface;approximation algorithm;cmos;cpu power dissipation;circuit design;clock rate;clock signal;computer cooling;dynamic voltage scaling;embedded system;finite-state machine;heuristic (computer science);image scaling;low-power broadcasting;markov chain;markov decision process;mobile device;power management;quadratic function;requirement;semiconductor industry;slack variable;timeout (computing);very-large-scale integration	Jihong Kim;Tajana Simunic	2007		10.1201/9781420011746.ch6	systems engineering;human resource management system;resource management;computer science	EDA	-3.9792102653745016	58.48115508551255	71548
35cd1050598cf95cb44af92baf601300f02889f2	code generation of data dominated dsp applications for fpga targets	automatic control;automatic code generation;digital signal processing;vhdl code generator;identity based encryption;software prototyping;digital signal processing field programmable gate arrays pipelines application software hardware flow graphs automatic control computer architecture prototypes identity based encryption;application software;implementation choices;prototypes;hardware description languages;code generation;fpga targets;grape rapid prototyping;application generators;flow graphs;csdf applications;computer architecture;rapid prototyping;data dominated dsp applications;design environment;scheduling;signal processing;pipelines;implementation choices code generation data dominated dsp applications fpga targets vhdl code generator grape rapid prototyping design environment csdf applications automatic code generation task communication scheduling dsp processors;field programmable gate arrays;dsp processors;field programmable gate arrays application generators hardware description languages software prototyping signal processing;task communication;hardware	The VHDL code generator of the GRAPE rapid prototyping and design environment has been extended to support a much wider range of data dominated applications. We describe the approach taken to implement CSDF applications on FPGAs, including the automatic code generation for task communication and scheduling on FPGAs alone or in conjunction with DSP processors. The implementation choices are discussed, and a comparison to manual code generation is made.	field-programmable gate array	J. Dalcolmo;Rudy Lauwereins;Marleen Adé	1998		10.1109/IWRSP.1998.676686	embedded system;computer architecture;application software;parallel computing;computer science;operating system;digital signal processing;signal processing;automatic control;pipeline transport;prototype;hardware description language;programming language;scheduling;code generation;field-programmable gate array	EDA	4.020439739635498	47.73786217499218	71684
5f9b0347fbed1d192a787ca1cec553b6e0ba2ac0	formulating mitf for a multicore processor with seu tolerance	energy efficiency;process variation;microarchitecture;error correction codes;homogeneous multicore processor;natural radiation interference;multicore processing redundancy energy consumption breakdown voltage single event upset digital systems design methodology performance gain geometry large scale integration;radio frequency;redundancy;performance trade off;instruction level redundancy;soft error problem;undependable redundancy;intelligent system;mean instructions to failure;multicore processors;multiprocessing systems;undependable redundancy portable intelligent systems natural radiation interference process variation power consumption performance trade off multiple clustered core processor homogeneous multicore processor mean instructions to failure instruction level redundancy modes thread level redundancy;power consumption;portable intelligent systems;power demand;programming;soft error;instruction level redundancy soft error problem thread level redundancy;instruction level redundancy modes;thread level redundancy;embedded device;multiple clustered core processor	While shrinking geometries of embedded LSI devices is beneficial for portable intelligent systems, it is increasingly susceptible to influences from electrical noise, process variation, and natural radiation interference. Even in consumer applications, modern embedded devices should be protected by dependable technologies. The challenging issue is there is a severe constraint in power consumption. As a platform to investigate the dependability, power, and performance trade-off, multiple clustered core processor (MCCP) is being investigated. It is a homogeneous multicore processor and has configurability in scale, which is beneficial for considering the trade-off. This paper focuses on how to explore the trade-off, and proposes to use mean instructions to failure (MITF) as a metric. To the best of our knowledge, this is the first study that formulates MITF for multicore processors. We compare three redundancy modes; undependable, thread-level redundancy, and instruction-level redundancy modes based on detailed simulations. As expected, thread-level redundancy shows largest MITF.	central processing unit;comparison of mud clients;dependability;embedded system;interference (communication);memory protection;multi-core processor;noise (electronics);simulation;single event upset	Toshimasa Funaki;Toshinori Sato	2008	2008 11th EUROMICRO Conference on Digital System Design Architectures, Methods and Tools	10.1109/DSD.2008.48	multi-core processor;embedded system;programming;parallel computing;real-time computing;soft error;microarchitecture;computer science;operating system;efficient energy use;redundancy;process variation;radio frequency	EDA	-4.099858098015325	55.80174723943592	71791
975effcca770a7be4f68daf2cfe9af7bfb9d7726	constraint-driven automatic generation of interconnect for partially reconfigurable architectures (abstract only)	online algorithm;reconfigurable system;partial reconfiguration;dynamic reconfiguration;interconnect;simulated annealing;automatic generation;crossbar;partial dynamic reconfiguration;constraint based design;port assignment	Dynamic partial reconfiguration allows the exchange of hardware configurations on FPGAs at run-time. Within a reconfigurable system that supports several different modules, resource requirements for interconnect between these modules may be considerably high. Enabling communication via a crossbar may require too many resources. State-of-the-art modelling methods for partial dynamic reconfiguration already support the fine-grained description of interaction between the partial modules. We propose both an online and an offline method for automatically generating interconnect according to such communication constraints, aiming at a low resource usage. The online algorithm determines an appropriate port assignment for the partial modules by means of a greedy approach and exploits port overlaps. The offline algorithm employs simulated annealing in order to find a proper port assignment and also incorporates the scheme for exploiting port overlaps. Constraint-generated interconnect requires significantly less resources than a crossbar, even if only a random port assignment is used. Proper port assignment by the online method reduces these requirements by an additional 10%, and using the offline method reduces them by an additional 30% on average. Online port assignment is faster than the offline method by several orders of magnitude. The interconnect generation tool introduced in this work takes textual input of communication constraints and automatically generates a corresponding hardware description in VHDL.	crossbar switch;exploit (computer security);field-programmable gate array;greedy algorithm;online algorithm;online and offline;reconfigurable computing;requirement;simulated annealing;vhdl	André Seffrin;Sorin A. Huss	2012		10.1145/2145694.2145748	embedded system;online algorithm;parallel computing;real-time computing;simulated annealing;computer science;operating system;interconnection;crossbar switch;distributed computing	EDA	0.25444918104679537	52.14150070128035	71798
6c7dc682b18ade6b9342014c51834f07a5a5bb22	efficient implementation of the row-column 8×8 idct on vliw architectures	discrete cosine transforms;microprocessor chips;parallel architectures;program compilers;vliw architectures;advanced compilers;inverse discrete cosine transform;processors;row-column idct;special-purpose hardware;time-consuming hand-tuning code;very long instruction word architectures;schedules;vliw;computer architecture;parallel processing	This paper experiments with a methodology for mapping the 8×8 row-column Inverse Discrete Cosine Transform on general-purpose Very Long Instruction Word architectures. By exploiting the parallelism inherent in the algorithm, the results obtained indicate that such processors, using sufficiently advanced compilers, can provide satisfactory performance at low cost without need to resort to special-purpose hardware or time-consuming hand-tuning of codes.	algorithm;central processing unit;code;compiler;discrete cosine transform;experiment;general-purpose markup language;parallel computing;very long instruction word	Rizos Sakellariou;Christine Eisenbeis;Peter M. W. Knijnenburg	1998	9th European Signal Processing Conference (EUSIPCO 1998)		computer architecture;parallel computing;real-time computing;computer science	HPC	1.9334347041886142	49.42826441300981	71933
96fc4389002fb90af1d2c94e390aa0e1dcc496bc	evaluating variable-grain logic cells using heterogeneous technology mapping	reconfigurable logic;efficient implementation;critical path;technology mapping;coarse grained	Generally, reconfigurable logic devices have been classified as fine-grained or coarse-grained devices depending on the input size of their logic cells. These architectures have conflicting characteristics, which limits their application domain for an efficient implementation. In order to solve this constraint, we propose a variable grain logic cell (VGLC) architecture that exhibits the characteristics of both fine-grained and coarse-grained cells. In this study, we investigate a VGLC structure and its mapping technique. We evaluate the capability of VGLC with respect to its critical path delay, implementation area, and configuration data bits; we propose a maximum improvement of 49.7%, 54.6%, and 48.5% in these parameters, respectively.		Kazunori Matsuyama;Motoki Amagasaki;Hideaki Nakayama;Ryoichi Yamaguchi;Masahiro Iida;Toshinori Sueyoshi	2007		10.1007/978-3-540-71431-6_14	embedded system;parallel computing;real-time computing;logic optimization;computer science;theoretical computer science;critical path method;distributed computing	EDA	0.4193890497994574	50.743395114980515	72216
11d54aa6b7972c66083863596529171d1e7070f4	automated programming of a ring-structured multiprocessor digital filter ic	timing schedules automated programming ring structured multiprocessor digital filter ic computer algorithm parallel processors dsp chip programming ring type topology;parallel programming;digital filter;chip;digital filters;digital signal processing chips;timing digital filters parallel programming parallel algorithms digital signal processing chips;possibility distribution;parallel algorithms;timing;automatic programming digital filters digital integrated circuits concurrent computing signal processing algorithms digital signal processing chips partitioning algorithms topology flow graphs computer architecture	A computer algorithm is described that automatically writes optimal programs for the implementation of arbitrary digital filter structures on parallel processors. The algorithm has been adapted particularly for programming a DSP chip with multiple processors arranged in a ring-type topology. T h e algorithm star ts from a netlist describing a desired digital filter structure. I t can check all possible timing schedules and all possible distributions of the operations over the parallel processors, taking into account the constraints imposed by the multiprocessor topology and the processors’ architecture. T h e algorithm’s output is a set of programs for the parallel processors which causes them to implement the given digital filter.	algorithm;central processing unit;digital filter;digital signal processor;multiprocessing;netlist;parallel computing	Michael J. Werter;Alan N. Willson	1995		10.1109/ISCAS.1995.520402	computer architecture;parallel computing;real-time computing;digital filter;digital signal;computer science;electrical engineering;digital signal processing;digital image processing;digital delay line	EDA	5.358057522730418	47.01332696550012	72228
cc4c44413ea27291a56d2c9e835d5472be1b0b57	configurable logic: a dynamically programmable cellular architecture and its vlsi implementation	thesis or dissertation		cellular architecture;very-large-scale integration	Thomas Andrew Kean	1988			computer architecture;parallel computing;macrocell array;computer science;simple programmable logic device;computer engineering	EDA	5.370688998330975	49.78959904344883	72285
dbd1e209da4a000b4562294da51421c95ed9b701	vlsi algorithms and architectures for dsp arithemetic computations			algorithm;computation;digital signal processor;very-large-scale integration	Robert Hamill	1995				EDA	5.949089740971197	49.779244597545045	72424
a320fd8824b5122b144296bce91b5f28c7b803e2	controller design for matrix multiplication on fpgas	arithmetic operators controller design matrix multiplication fpga technology floating point components system architecture design space fpga based accelerators control logic design operation scheduling;field programmable gate array;matrices reconfigurable logic accelerator architectures parallel architectures floating point arithmetic;frequency modulation;logic design;reconfigurable logic;arithmetic operators;system performance;design space;system on a chip;accelerator architectures;matrices;parallel architectures;frequency modulated;adders;scheduling;pipelines;controller design;schedules;fpga technology;system architecture design space;floating point;matrix multiplication;scheduling field programmable gate arrays logic design;adders system on a chip schedules field programmable gate arrays pipelines parallel processing frequency modulation;control logic design;parallel architecture;floating point arithmetic;field programmable gate arrays;system architecture;operation scheduling;high performance;parallel processing;fpga based accelerators;floating point components	FPGA technology constitutes an attractive platform for high-performance accelerators of parallel workloads in general-purpose computers. Matrix multiplication is a computationally intensive application that is highly parallelizable. Previous work has typically described custom floating-point components and reported on specific designs or implementations using these components for FPGA-based matrix multiplication. We seek to utilize vendor-supplied or other available floating-point components to explore the system-architecture design space for flexible, high-performance, FPGA-based accelerators. In this paper, we focus on the design of control logic that accommodates the configuration of as many implementation aspects as possible (e.g., scheduling of operations, levels of parallelism, and choice of arithmetic operators) for inclusion in an experimental infrastructure to assess the effects of these parameters on overall system performance.	computer;field-programmable gate array;general-purpose modeling;matrix multiplication;merge sort;parallel computing;scheduling (computing)	Ahmad Khayyat;Naraig Manjikian	2011	2011 24th Canadian Conference on Electrical and Computer Engineering(CCECE)	10.1109/CCECE.2011.6030678	parallel processing;computer architecture;electronic engineering;parallel computing;computer science;floating point;operating system;field-programmable gate array	HPC	3.6624795874120344	46.656908725805465	72582
5f1e27963793825490de806b3b45778bf8d2e08e	optimization of quantum computer architecture using a resource-performance simulator	computer architecture logic gates hardware optical switches performance evaluation quantum computing;shor algorithm quantum computer architecture optimisation resource performance simulator hardware technology device parameters dp architectural optimization computer design reconfigurable architecture trapped ions quantum circuit circuit execution qubit communication failure probability;resource performance trade off;quantum computing computer architecture optimisation performance evaluation probability;quantum architecture;device parameter;performance simulation quantum architecture device parameter resource performance trade off;performance simulation	The hardware technology characterized by the device parameters (DPs) often drives the architectural optimization in a novel computer design such as the quantum computer. We highlight the role of DPs by quantifying the performance of a fully error-corrected 1024-bit quantum carry look-ahead adder on a modular, reconfigurable architecture based on trapped ions. We develop a simulation tool that estimates the performance and resource requirements for running a quantum circuit on various quantum architectures as a function of the underlying DPs. Using this tool, we found that (1) the latency of the adder circuit execution due to slow entanglement generation process for qubit communication can be adequately eliminated with a small increase in entangling qubits, and (2) the failure probability of the circuit is ultimately determined by the qubit coherence time, which needs to be improved in order to reliably execute the adders comprising core of the Shor's algorithm.	adder (electronics);carry-lookahead adder;computer architecture;mathematical optimization;quantum circuit;quantum computing;quantum entanglement;qubit;requirement;self-reconfiguring modular robot;shor's algorithm;simulation	Muhammad Ahsan;Jungsang Kim	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.7873/DATE.2015.0318	embedded system;electronic engineering;quantum information;computer science;theoretical computer science;operating system;computer engineering	EDA	7.619968885207647	56.7609105291124	72716
5a443c07c52da0cb0e84577ba7e4b9efc6fe0729	scbxp: an efficient cam-based xml parsing technique in hardware environments	cam based xml parsing technique;xml computer aided manufacturing skeleton loading clocks writing field programmable gate arrays;field programmable gate array;hardware based environments;clocks;multiple state machines;xml processing;fpga scbxp cam based xml parsing technique software based xml parsing techniques hardware based environments cam based architecture content addressable memory xml document finite state machine fifo multiple state machines dual port memory modules;loading;fpga;dual port memory modules;skeleton;xml parsing;finite state machines;computer aided manufacturing;xml content addressable storage field programmable gate arrays finite state machines;content addressable memory xml processing xml parsing field programmable gate arrays;xml;content addressable memory;writing;xml document;scbxp;cam based architecture;field programmable gate arrays;fifo;software based xml parsing techniques;content addressable storage;finite state machine	The underlying technologies of web information and distributed systems often require efficient XML parsing. Even though new software-based XML parsing techniques improve XML processing, the verbose nature of XML does not help to achieve the substantial improvements that are desired. In some systems, such as mobile devices, the restricted memory resources exacerbate the problems associated with XML processing. In this paper, we present a novel XML parsing technique-titled SCBXP-that is designed to achieve high performance in hardware-based environments. In addition, the parsing technique provides a natural way of checking for full well formedness and partial validation, thereby taking advantage of our CAM-based architecture and the inherent parallel features of the hardware. Furthermore, the efficiency of XML parsing is maintained even when memory resources are limited. The SCBXP technique architecture makes use of 1) a content-addressable memory that must be configured with a skeleton of the XML document being parsed, 2) a finite state machine that controls FIFOs, in order to align XML data properly, 3) multiple state machines acting on the multilevel nature of XML, and 4) dual-port memory modules. The results of testing the SCBXP technique, implemented on an FPGA, demonstrate that a processing rate of at least 2 bytes of XML data can be performed during each clock cycle.	algorithm;align (company);byte;clock signal;content-addressable memory;dimm;data rate units;distributed computing;field-programmable gate array;finite-state machine;hardware interface design;mobile device;parsing;stratix;throughput;xml	Fadi T. El-Hassan;Dan Ionescu	2011	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2011.51	xml validation;binary xml;xml encryption;simple api for xml;parallel computing;xml;streaming xml;computer science;theoretical computer science;operating system;xml framework;xml database;xml schema;database;distributed computing;finite-state machine;xml signature;programming language;efficient xml interchange;field-programmable gate array	Web+IR	5.612229393455108	54.98710081805884	72823
f1f2f395c7f310f140677f54344b95eed2e6d952	software optimization for embedded communication system	software optimization;optimization algorithm design and analysis software algorithms embedded systems partitioning algorithms memory management;optimisation;software performance evaluation;military communication;embedded system;embedded systems;energy consumption;message passing;software optimization embedded system;program compilers;military computing;memory usage embedded communication system software optimization techniques software performance energy consumption embedded systems source code level tactical message processing software message encoder decoder tactical communication weapon systems optimized software memory access;software performance evaluation embedded systems energy consumption message passing military communication military computing optimisation program compilers	This paper presents general software optimization techniques to improve software performance and energy consumption in embedded systems. Software optimization can be categorized into three levels: algorithmic, source code-level. Then these techniques are applied to optimize our tactical message processing software, which is a message encoder-decoder for tactical communication equipped on weapon systems. The optimized software achieved performance increase of about 12%, memory access decrease of 72% and memory usage decrease of 35% over the original software.	algorithm;categorization;embedded system;encoder;experiment;mathematical optimization;profiling (computer programming);program optimization;software performance testing	Inhye Park;Hyungkeun Lee;Hyukjoon Lee	2013	The International Conference on Information Networking 2013 (ICOIN)	10.1109/ICOIN.2013.6496708	computer architecture;parallel computing;message passing;real-time computing;software sizing;embedded software;search-based software engineering;computer science;backporting;software framework;component-based software engineering;software development;software design description;operating system;software construction;resource-oriented architecture;software deployment;software system;avionics software	EDA	-2.7262894841929888	51.68822897446877	72916
6dbbfe80a104584e669b98515747f6895cab3e06	ras-nano: a reliability-aware synthesis framework for reconfigurable nanofabrics	high defect;ras-nano;alternative implementation;complex probabilistic design space;nanotechnology;reconfigurable nanofabrics;configuration time;current design methodology;traditional high level synthesis;high level synthesis problem;reliability;new design paradigm;benchmark kernels;high level synthesis;reliability-aware synthesis framework;alternative solution;synthesis framework;design methodology	Entering the nanometer era, a major challenge to current design methodologies and tools is to effectively address the high defect densities projected for nanotechnologies. To this end, we proposed a reconfiguration-based defect-avoidance methodology for defect-prone nanofabrics. It judiciously architects the nanofabric, using probabilistic considerations, such that a very large number of alternative implementations can be mapped into it, enabling defects to be circumvented at configuration time in a scalable way. Building on this foundation, in this paper we propose a synthesis framework aimed at implementing this new design paradigm. A key novelty of our approach with respect to traditional high level synthesis is that, rather than carefully optimizing a single (`deterministic') solution, our goal is to simultaneously synthesize a large family of alternative solutions, so as to meet the required probability of successful configuration, or yield, while maximizing the family's average performance. Experimental results generated for a set of representative benchmark kernels, assuming different defect regimes and target yields, empirically show that our proposed algorithms can effectively explore the complex probabilistic design space associated with this new class of high level synthesis problems	algorithm;benchmark (computing);best, worst and average case;high- and low-level;high-level programming language;high-level synthesis;programming paradigm;scalability;software bug	Chen He;Margarida F. Jacome	2006	Proceedings of the Design Automation & Test in Europe Conference		embedded system;algorithm design;electronic engineering;kernel;real-time computing;scalability;simulation;design methods;computer science;engineering;theoretical computer science;space exploration;operating system;reliability;high-level synthesis;computer-aided manufacturing	EDA	9.875642748634393	57.985364857968584	73016
245cea2e04f23dbb685c10b2e3359fdc664dd715	an opencl software compilation framework targeting an soc-fpga vliw chip multiprocessor	heterogeneous computing;fpga;compilation;opencl;article;multi core	Modern systems-on-chip augment their baseline CPU with coprocessors and accelerators to increase overall computational capability and power efficiency, and thus have evolved into heterogeneous multi-core systems. Several languages have been developed to enable this paradigm shift, including CUDA and OpenCL. This paper discusses a unified compilation environment to enable heterogeneous system design through the use of OpenCL and a highly configurable VLIW Chip Multiprocessor architecture known as the LE1. An LLVM compilation framework was researched and a prototype developed to enable the execution of OpenCL applications on a number of hardware configurations of the LE1 CMP. The presented OpenCL framework fully automates the compilation flow and supports work-item coalescing which better maps onto the ILP processor cores of the LE1 architecture. This paper discusses in detail both the software stack and target hardware architecture and evaluates the scalability of the proposed framework by running 12 industry-standard OpenCL benchmarks drawn from the AMD SDK and the Rodinia suites. The benchmarks are executed on 40 LE1 configurations with 10 implemented on an SoC-FPGA and the remaining on a cycle-accurate simulator. Across 12 OpenCL benchmarks results demonstrate near-linear wall-clock performance improvement of 1.8x (using 2 dual-issue cores), up to 5.2x (using 8 dual-issue cores) and on one case, super-linear improvement of 8.4x (FixOffset kernel, 8 dual-issue cores). The number of OpenCL benchmarks evaluated makes this study one of the most complete in the literature.		Samuel J. Parker;Vassilios A. Chouliaras	2016	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2016.06.003	multi-core processor;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;symmetric multiprocessor system;field-programmable gate array	Arch	-1.0791923454040764	48.53444817280377	73211
85b8b133186964e2422abaac8616297d8359f342	evaluation of cpu utilization under a hardware-software partitioned enviroment (migrating software to hardware)	operating system;embedded system;embedded processor;adaptive filter;chip;real time operating system	The embedded systems application space is growing at a fast pace and has a very wide range that encompasses minute sensor nodes through large FPGA based systems with multiple embedded processors within a single chip. The use of real-time operating systems (RTOS) has become pivotal in embedded systems design because RTOSes foster multi-tasking flexibility through the use of the operating system’s services. Having a number of tasks running concurrently leads to increased CPU utilization, even with the existence of a number of task scheduling techniques it is still essential to mitigate CPU utilization. The art of embedding a processor or processors in an FPGA offers the system additional computational resources that could alleviate the processor loading by migrating some of the computational needs onto the hardware. In this paper hardware/software partitioning is explored with hardware oriented computations performed in the logic while those suitable for execution by the CPU are performed in software. Digital adaptive filtering is considered to demonstrate the benefits and flexibility offered by the integration of processors and logic on the same die. A well partitioned finite impulse response filter is shown to outperform a software based filter; executing 3.6	adaptive filter;central processing unit;computation;computational resource;computer multitasking;die (integrated circuit);embedded system;field-programmable gate array;finite impulse response;real-time clock;real-time operating system;scheduling (computing);systems design	Hsiang-Ling Jamie Lin;Jabulani Nyathi;Clint Cole	2007			chip;cpu time;real-time operating system;embedded system;computer science;computer hardware;software;hardware compatibility list;computer-on-module;embedded operating system	Embedded	2.394537365331805	47.98116339337254	73248
5c4ce8d05692572e096b60e20cf02d39aa23724e	reconfigurable fault tolerance: a framework for environmentally adaptive fault mitigation in space	dynamic change;system engineering;field programmable gate array;sram chips aerospace computing fault tolerant computing field programmable gate arrays markov processes reconfigurable architectures;fault tolerant;reconfigurable architectures;reconfigurable fault tolerance;tmr techniques;aerospace application;triple modular redundancy;phased mission markov modeling;orbits;fault tolerant computing;markov model;aerospace computing;fault tolerant systems;field programmable gate array reconfigurable fault tolerance sram based fpga aerospace application next generation mission requirements tmr techniques phased mission markov modeling triple modular redundancy static ram;fault tolerance;next generation;single event upset;markov processes;field programmable gate arrays;tunneling magnetoresistance;sram based fpga;next generation mission requirements;fault tolerance field programmable gate arrays fault tolerant systems reliability engineering design engineering power engineering and energy systems engineering and theory redundancy power system reliability environmental factors;static ram;environmental factor;sram chips	Commercial SRAM-based FPGAs have the potential to provide aerospace applications with the necessary performance to meet next-generation mission requirements. However, the susceptibility of these devices to radiation in the form of single-event upsets is a significant drawback. TMR techniques are traditionally used to mitigate these effects, but with an overwhelming amount of extra area and power. We propose a framework for reconfigurable fault tolerance which enables systems engineers to dynamically change the amount of redundancy and fault mitigation that is used in an FPGA design. This approach leverages the reconfigurable nature of the FPGA to allow significant processing to be performed safely and reliably when environmental factors permit. Phased-mission Markov modeling is used to estimate performability gains that can be achieved using the framework for two case-study orbits.	fault tolerance;field-programmable gate array;markov chain;requirement;single event upset;static random-access memory;systems engineering;triple modular redundancy	Adam Jacobs;Alan D. George;Grzegorz Cieslewski	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272313	embedded system;fault tolerance;real-time computing;computer science;field-programmable gate array	EDA	8.815176988758163	59.22916001741785	73251
4b76b9773557e5e8804e2bc1058225f004730bc3	memory-efficient pipelined architecture for large-scale string matching	vlsi field programmable gate arrays memory architecture pipeline processing string matching;clocks;very large scale integration;state machine;memory architecture large scale systems pipelines field programmable gate arrays pattern matching impedance matching table lookup very large scale integration clocks frequency;fpga;memory efficient pipelined architecture;chip;auxiliary table;computer architecture;large scale;bit field input;memory architecture;pattern matching;pipelines;impedance matching;dictionaries;partial state machine;vlsi;large scale string matching;field programmable gate arrays;string matching;high throughput;frequency;table lookup;field merge architecture;on chip memory memory efficient pipelined architecture large scale string matching field merge architecture bit field input partial state machine auxiliary table fpga vlsi;doped fiber amplifiers;on chip memory;pipeline processing;large scale systems	We propose a pipelined field-merge architecture for memory-efficient and high-throughput large-scale string matching (LSSM). Our proposed architecture partitions the (8-bit) character input into several bit-field inputs of smaller (usually 2-bit) widths. Each bit-field input is matched in a partial state machine (PSM) pipeline constructed from the respective bit-field patterns. The matching results from all the bit-fields in every pipeline stage are then merged with the help of an auxiliary table (ATB). This novel architecture essentially divides the LSSM problem with a continuous stream of input characters into two disjoint and simpler sub-problems: 1) O\left(\mathrm{character\_bitwidth}\right) number of pipeline traversals, and 2) O\left(\mathrm{pattern\_length}\right) number of table lookups. It is naturally suitable for implementation on FPGA or VLSI with on-chip memory. Compared to the bit-split approach, our field-merge implementation on FPGA requires 1/5 to 1/13 the total memory while achieving 25% to 54% higher clock frequencies.	8-bit;aho–corasick algorithm;bit field;clock rate;clock signal;color depth;dictionary;field-programmable gate array;finite-state machine;high-throughput computing;lookup table;pipeline (computing);prototype;snort;string searching algorithm;throughput;tree traversal;very-large-scale integration;virtex (fpga)	Yi-Hua Edward Yang;Viktor K. Prasanna	2009	2009 17th IEEE Symposium on Field Programmable Custom Computing Machines	10.1109/FCCM.2009.17	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;very-large-scale integration;finite-state machine;programming language;field-programmable gate array	Arch	8.813444614396493	47.40592504741783	73274
b2e0041d2c9069d39055a14fa586a8ef646855b7	dynamic run-time hardware/software scheduling for 3d reconfigurable soc	software;system on chip dynamic run time hardware software scheduling online hardware software scheduling algorithm hw sw scheduling algorithm 3d reconfigurable soc platform multiprocessors layer heterogeneous reconfigurable layer sw execution hw execution software execution prediction hssp;scheduling algorithms;system on chip hardware software codesign multiprocessing systems processor scheduling reconfigurable architectures;three dimensional displays;scheduling;software algorithms;scheduling three dimensional displays scheduling algorithms hardware software software algorithms;hardware	In this paper, we present a new online hardware/software (HW/SW) scheduling algorithm for a 3D Reconfigurable SoC platform comprising a multiprocessors layer and a heterogeneous reconfigurable layer. The proposed algorithm decides on the fly whether the tasks will run in SW or HW, at which time, on which processor or in which region of the reconfigurable layer in order to minimize the overall execution time of the application. It evaluates, during runtime, the interest to continue the SW execution of a task or to cancel it for starting a new HW execution of this task from the initial state. By using our algorithm called Hardware/Software algorithm with Software execution Prediction (HSSP), the overall execution time can be reduced by 26 % compared with other existing HW/SW scheduling methods.	algorithm;homology-derived secondary structure of proteins;on the fly;run time (program lifecycle phase);scheduling (computing);shattered world;software quality assurance	Quang-Hai Khuat;Daniel Chillet;Michael Hübner	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032512	fair-share scheduling;computer architecture;parallel computing;real-time computing;computer science;rate-monotonic scheduling;operating system;two-level scheduling;scheduling	EDA	-1.5841646653061083	51.844745411705816	73311
8593bca4c6d28d777024f6dc5077c0006087f521	application-specific architectures for field-programmable vlsi technologies	vlsi;digital signal processing chips;integer programming;logic arrays;application-specific dsp architectures;application-specific architectures;fast prototyping;field-programmable vlsi technologies;field-programmable gate array;high level design optimization;integer programming models;multichip implementation;multiple fpga chips;multiple busses;multiple register files;optimization methodology;optimized register file architectures;performance constraints;user-programmable ram blocks	FPT is the premier conference in the Asian region on field-programmable technologies including reconfigurable computing devices and systems containing such components. Fieldprogrammable devices promise the flexibility of software with the performance of hardware. The development and application of field-programmable technology have become important topics of research and development. Field-programmable components are widely applied, such as in high-performance computing systems, embedded and low-power control instruments, mobile communications, rapid prototyping and product emulation.	embedded system;emulator;field-programmability;low-power broadcasting;parameterized complexity;rapid prototyping;reconfigurable computing;supercomputer;very-large-scale integration	Catherine H. Gebotys;Robert J. Gebotys	1994			computer architecture;parallel computing;integer programming;computer science;operating system;very-large-scale integration;field-programmable gate array	EDA	5.710144774598278	49.84562061025093	73826
a8df3bfeae0ed48d535791d7dd3f5ef985f61ffe	nocout : noc topology generation with mixed packet-switched and point-to-point networks	greedy algorithms;logic design;network topology;network-on-chip;packet switching;noc topology generation;nocout;greedy iterative improvement strategy;networks-on-chip;packet-switched networks;point-to-point networks;system-level floorplanner	"""Networks-on-Chip (NoC) have been widely proposed as the future communication paradigm for use in next-generation System-on-Chip. In this paper, we present NoCOUT, a methodology for generating an energy optimized application specific NoC topology which supports both point-to-point and packet-switched networks. The algorithm uses a prohibitive greedy iterative improvement strategy to explore the design space efficiently. A system-level floorplanner is used to evaluate the iterative design improvements and provide feedback on the effects of the topology on wire length.  The algorithm is integrated within a NoC synthesis framework with characterized NoC power and area models to allow accurate exploration for a NoC router library. We apply the topology generation algorithm to several test cases including real-world and synthetic communication graphs with both regular and irregular traffic patterns, and varying core sizes. Since the method is iterative, it is possible to start with a known design to search for improvements. Experimental results show that many different applications benefit from a mix of """"on chip networks"""" and """"point-to-point networks"""". With such a hybrid network, we achieve approximately 25% lower energy consumption (with a maximum of 37%) than a state of the art min-cut partition based topology generator for a variety of benchmarks. In addition, the average hop count is reduced by 0.75 hops, which would significantly reduce the network latency."""	apollonian network;approximation algorithm;benchmark (computing);fibre channel point-to-point;greedy algorithm;iterative design;iterative method;local search (optimization);mpsoc;maxima and minima;minimum cut;np-hardness;network on a chip;network packet;packet switching;point-to-point protocol;point-to-point (telecommunications);programming paradigm;router (computing);synthetic intelligence;system on a chip;test case	Jeremy Chan;Sri Parameswaran	2008	2008 Asia and South Pacific Design Automation Conference			EDA	2.820538911343201	60.23241630038437	73830
f8f40f3d0cb7e87876d2f23c20720cfd96960720	microcode support for operating system functions: issues and examples	operating system		microcode;operating system	John L. Cuadrado;Michael Hinkey;Mark Gaertner	1983			computer architecture;computer engineering;embedded operating system;microcode;computer science	EDA	4.6514868685655095	49.5001716371791	73990
18867e8cf55922f7eaf98b31f0021e560870017b	bio-inspired self-testing and self-organizing processing arrays	image processing array;organisms;image processing biocomputing boolean algebra;boolean operations;biocomputing;image processing;circuit faults;self repair mechanisms;bioinspired self testing;maintenance engineering;multiplexing;boolean algebra;arrays;boolean operation;built in self test;registers;molecular biology;self organization;molecular biology process;error detection;registers organisms arrays multiplexing parallel processing circuit faults maintenance engineering;boolean operations bioinspired self testing self organizing processing arrays molecular biology process self repair mechanisms image processing array;self organizing processing arrays;parallel processing	Inspired by the basic processes of molecular biology, our previous studies resulted in defining a configurable molecule implementing self-replication and self-repair mechanisms made up of simple processes. The goal of our paper is to add error detection features to the molecule in order to make it able to perform also built-in self-test mechanisms. The hardware description of the molecule with all its self-organizing mechanisms leads to the simulation of an image processing array dedicated to thresholding and boolean operations.	artificial cell;british informatics olympiad;built-in self-test;error detection and correction;image processing;integrated circuit;organizing (structure);register-transfer level;self-organization;self-replication;semiconductor intellectual property core;simulation;thresholding (image processing);vhdl	André Stauffer;Joël Rossier	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586225	maintenance engineering;boolean algebra;organism;parallel processing;self-organization;error detection and correction;image processing;computer science;artificial intelligence;theoretical computer science;processor register;algorithm;multiplexing	Embedded	8.969105104313908	48.89178812255268	74001
fc1dc79fbd8fdfc765730aa341649af9ba2fd391	implementation-independent model of an instruction set architecture in vhdl	performance measure;processor architecture;specification languages computer architecture instruction sets;instruction set architecture;computer architecture;specification languages;architectural specification implementation independent model instruction set architecture vhdl vhsic hardware description language computer architectures processor architecture wm performance measurements;vhsic hardware description language;model of computation;computer architecture computational modeling integrated circuit modeling clocks synchronization parallel processing hardware design languages high speed integrated circuits very high speed integrated circuits process design;instruction sets	A methodology using a VHDL (VHSIC hardware description language) to create executable models of computer architectures independent of implementation attributes is described. The authors present such a model of a processor architecture known as the WM as the first step in developing an implementation. Simulations using the model can provide performance measurements such as potential parallelism. The model can also serve as an architectural specification for the computer.<<ETX>>	computer architecture;computer simulation;executable;hardware description language;parallel computing;vhdl;vhsic	Maximo H. Salinas;Barry W. Johnson;James H. Aylor	1993	IEEE Design & Test of Computers	10.1109/54.232471	enterprise architecture framework;reference architecture;reduced instruction set computing;space-based architecture;computer architecture;parallel computing;vhdl;computer science;applications architecture;cellular architecture;operating system;instruction set;transport triggered architecture;software architecture description;programming language;data architecture;systems architecture;computer engineering	Arch	5.244649207562799	52.33820087909187	74009
6e1ebde6e11e25d8ed004efc7a2c3e1be5dde846	methodology for fault tolerant system design based on fpga into limited redundant area	resource allocation fault tolerant computing field programmable gate arrays reconfigurable architectures redundancy;relocation technique fault tolerant system design methodology fpga limited redundant area transient fault ability permanent fault mitigation transient fault mitigation partial dynamic reconfiguration fault tolerant architecture external memory partial bitstream;resource allocation;reconfigurable architectures;fault tolerant computing;redundancy;field programmable gate arrays;field programmable gate arrays synchronization circuit faults transient analysis tunneling magnetoresistance fault tolerant systems hardware	The paper presents a methodology of fault tolerant system design into an FPGA with the ability of the transient fault and the permanent fault mitigation. The transient fault mitigation is done by the partial dynamic reconfiguration. The mitigation of a certain number of permanent faults is based on using a specific fault tolerant architecture occupiing less resources than the previosly used one and excluding the faulty part of the FPGA. This inovative technique is based on the precompiled configurations stored in an external memory. To reduce the required space for a partial bit stream the relocation technique is used.	bitstream;complex system;design review (u.s. government);experiment;fault tolerance;field-programmable gate array;relocation (computing);software architecture;statistical relational learning;systems design	Lukas Miculka;Martin Straka;Zdenek Kotásek	2013	2013 Euromicro Conference on Digital System Design	10.1109/DSD.2013.33	embedded system;real-time computing;fault coverage;fault indicator;resource allocation;computer science;stuck-at fault;redundancy;general protection fault;software fault tolerance;field-programmable gate array	EDA	8.69451795393585	58.85270770165171	74017
c9b0aa52c411c01876197fbe90f5964ba8492b30	an introduction to reconfigurable systems	field programmable gate arrays programmable logic arrays reconfigurable architectures software radio logic gates;reconfigurable architectures;software radio field programmable gate arrays reconfigurable architectures;programmable logic arrays;software radio;software definable radio reconfigurable systems software defined functionality bit pattern specification field programmable gate arrays;logic gates;field programmable gate arrays	Reconfigurability can be thought of as software-defined functionality, where flexibility is controlled predominately through the specification of bit patterns. Reconfigurable systems can be as simple as a single switch, or as abstract and powerful as programmable matter. This paper considers the generalization of reconfigurable systems as an important evolving discipline, bolstered by real-world archetypes such as field programmable gate arrays and software-definable radio (platform and application, respectively). It considers what reconfigurable systems actually are, their motivation, their taxonomy, the fundamental mechanisms and architectural considerations underlying them, designing them and using them in applications. With well-known real-world instances, such as the field programmable gate array, the paper attempts to motivate an understanding of the many possible directions and implications of a new class of system which is fundamentally based on the ability to change.	field-programmable gate array;programmable matter;reconfigurability;taxonomy (general)	James Lyke;Christos G. Christodoulou;G. Alonzo Vera;Arthur H. Edwards	2015	Proceedings of the IEEE	10.1109/JPROC.2015.2397832	erasable programmable logic device;computer architecture;electronic engineering;macrocell array;logic gate;reconfigurable computing;programmable logic array;computer science;engineering;programmable logic device;software-defined radio;simple programmable logic device;programmable array logic;field-programmable gate array;computer engineering	EDA	8.903692200317913	49.93757223708723	74138
f05ac917103efe751f8eeaab1c272dc5c63d0769	exploiting system-level parallelism in the application development on a reconfigurable computer	application development;microcomputers reconfigurable architectures computation theory microprocessor chips;reconfigurable computing;computation theory;reconfigurable machine system level parallelism reconfigurable computer fpga field programmable gate arrays hardware functionality i o time computation time dma transfer microprocessor memory onboard memory state of the art reconfigurable platform;reconfigurable architectures;parallel processing application software concurrent computing microprocessors hardware tiles field programmable gate arrays computer architecture computer hacking logic programming;large classes;microcomputers;microprocessor chips	Reconfigurable Computers (RCs) can leverage the synergism between conventional processors and FPGAs to provide low-level hardware finctionalify of the same level ofprogrommabilify as general-purpose computers. In a large class of opplicatims, the tom1 I/O time is comparable or even grenter than the camputotiom time. As a result, the rate of the DMA transfer between the microprocessor memory and the m-baard memory becomes the performance bottleneck even on RCs. In this paper, we perform a theoretical and experimental study of this specific performance limitation far the stnte-of-the art reconfgivable pla@rm, SRC-6E. We demomtrate and quantify the possible solution to this problem that exploits the system-level parallelism within the reconfigurable machine.	central processing unit;computer;direct memory access;experiment;field-programmable gate array;general-purpose markup language;high- and low-level;input/output;microprocessor;parallel computing;reconfigurable computing	Esam El-Araby;Mohamed Taher;Kris Gaj;Tarek A. El-Ghazawi;David Caliga;Nikitas A. Alexandridis	2003		10.1109/FPT.2003.1275798	embedded system;computer architecture;parallel computing;theory of computation;reconfigurable computing;computer science;operating system;microcomputer;rapid application development	Arch	1.3718806583381578	47.83544161017891	74548
8ef43bb17969b31626a18bf0923dc73939048404	unsync: a soft error resilient redundant multicore architecture	system reliability soft error resilient redundant multicore architecture device dimension reduction transistor density timing window processor vulnerability charge carrying particles processor technology general purpose chip multiprocessors hardware resources redundancy based technique soft error failure cmp systems redundant cmp architecture hardware based detection mechanism overhead reduction error free execution always forward execution enabled recovery mechanism system resilience rtl model hardware synthesis power overhead area overhead reunion technique cycle accurate simulation spec2000 mibench benchmark performance efficiency unsync architecture power consumption reduction;hardware detection;error resilient;hardware detection multi core architecture soft error error resilient core level redundancy redundant architecture low power;system recovery computer architecture failure analysis fault tolerant computing microprocessor chips power aware computing;failure analysis;redundant architecture;computer architecture;power aware computing;fault tolerant computing;system recovery;low power;core level redundancy;hardware computer architecture pipelines redundancy instruction sets;error resilience;multi core architecture;soft error;microprocessor chips	"""Reducing device dimensions, increasing transistor densities, and smaller timing windows, expose the vulnerability of processors to soft errors induced by charge carrying particles. Since these factors are only consequences of the inevitable advancement in processor technology, the industry has been forced to improve reliability on general purpose Chip Multiprocessors (CMPs). With the availability of increased hardware resources, redundancy based techniques are the most promising methods to eradicate soft error failures in CMP systems. In this work, we propose a novel redundant CMP architecture (UnSync) that utilizes hardware based detection mechanisms (most of which are readily available in the processor), to reduce overheads during error free executions. In the presence of errors (which are infrequent), the """"always forward execution"""" enabled recovery mechanism provides for resilience in the system. We design a detailed RTL model of our UnSync architecture and perform hardware synthesis to compare the hardware (power/area) overheads incurred. We compare the same with those of the Reunion technique, a state-of-the-art redundant multi-core architecture. We also perform cycle-accurate simulations over a wide range of SPEC2000, and MiBench benchmarks to evaluate the performance efficiency achieved over that of the Reunion architecture. Experimental results show that, our UnSync architecture reduces power consumption by 34.5% and improves performance by up to 20% with 13.3% less area overhead, when compared to Reunion architecture for the same level of reliability achieved."""	architecture framework;central processing unit;compiler;harsh realm;image scaling;intel core (microarchitecture);microsoft windows;mike lesser;multi-core processor;overhead (computing);parallel computing;pipeline (computing);processor technology;register file;simulation;soft error;transistor	Reiley Jeyapaul;Fei Hong;Abhishek Rhisheekesan;Aviral Shrivastava;Kyoungwoo Lee	2011	2011 International Conference on Parallel Processing	10.1109/ICPP.2011.76	embedded system;failure analysis;parallel computing;real-time computing;soft error;computer science;operating system	Arch	6.631392315996116	59.54803883169966	74557
f47ce64c832488efd330e6ded9c20ecf33f2e82c	operand-value-based modeling of dynamic energy consumption of soft processors in fpga		This paper presents a novel method for estimating the dynamic energy consumption of soft processors in FPGA, using an operand-value-based model at the instruction level. Our energy model contains three components: the instruction base energy, the maximum variation in the instruction energy due to input data, and the impact of one’s density of the operand values during software execution. Using multiple benchmarks, we demonstrate that our model has only 4.7% average error and 12% worst case error compared to the reference post-place-and-route simulations, and is more than twice as accurate as existing instruction-level models.	field-programmable gate array;operand	Zaid Al-Khatib;Samar Abdi	2015		10.1007/978-3-319-16214-0_6	operand;energy modeling;parallel computing;real-time computing;field-programmable gate array;computer science;software;energy consumption	EDA	-2.552651918151829	55.438540023547574	74814
f58d3363fb73a8b9a58fe3f34f5e3115121427ff	static mapping with dynamic switching of multiple data-parallel applications on embedded many-core socs		This paper studies mapping techniques of multiple applications on embedded many-core SoCs. The mapping techniques proposed in this paper are static which means the mapping is decided at design time. The mapping techniques take into account both inter-application and intraapplication parallelism in order to fully utilize the potential parallelism of the many-core architecture. Additionally, the proposed static mapping supports dynamic application switching, which means the applications mapped onto the same cores are switched to each other at runtime. Two approaches are proposed for static mapping: one approach is based on integer linear programming and the other is based on a greedy algorithm. Experimental results show the effectiveness of the proposed techniques. key words: many-core SoCs, application mapping, system-level design, embedded systems	electronic system-level design and verification;embedded system;greedy algorithm;integer programming;intel core (microarchitecture);level design;linear programming;manycore processor;parallel computing;run time (program lifecycle phase);system on a chip	Ittetsu Taniguchi;Junya Kaida;Takuji Hieda;Yuko Hara-Azumi;Hiroyuki Tomiyama	2014	IEICE Transactions		real-time computing;computer science;electronic system-level design and verification	EDA	-0.28338659720548487	51.97902798556237	74865
3f5b63c9aa434c51509ab215129661b10e59c10d	a low-power heterogeneous multiprocessor architecture for audio signal processing	audio signal processing;multiprocessor;low power;heterogeneous;multiprocessor architecture;scalable architecture;message passing;application specific instruction set processor;asip application specific instruction set processor	This paper describes a low-power programmable DSP architecture that targets audio signal processing. The architecture can be characterized as a heterogeneous multiprocessor consisting of small instruction set processors called mini-cores as well as standard DSP and CPU cores that communicate using message passing. The minicores are tailored for different classes of filtering algorithms (FIR, IIR, N-LMS etc.), and in a typical system the communication among processors occur at the sampling rate only. The mini-cores are intended as soft-macros to be used in the implementation of system-on-chip solutions using a synthesis-based design flow targeting a standard-cell implementation. They are parameterized in word-size, memory-size, etc. and can be instantiated according to the needs of the application. To give an impression of the size of a mini-core we mention that one of the FIR mini-cores in a prototype design has 16 instructions, a 32-word × 16-bit program memory, a 64-word × 16-bit data memory and a 25-word × 16-bit coefficient memory. Results obtained from the design of a prototype chip containing mini-cores for a hearing aid application, demonstrate a power consumption that is only 1.5–1.6 times larger than a hardwired ASIC and more than 6–21 times lower than current state of the art low-power DSP processors. This is due to: (1) the small size of the processors and (2) a smaller instruction count for a given task.	16-bit;algorithm;application domain;application-specific integrated circuit;audio signal processing;central processing unit;coefficient;design flow (eda);digital signal processor;electronic circuit;finite impulse response;hardware acceleration;infinite impulse response;intel core (microarchitecture);low-power broadcasting;message passing;multiprocessing;pleiades (supercomputer);prototype;read-only memory;reconfigurability;reconfigurable computing;sampling (signal processing);standard cell;system on a chip	Özgün Paker;Jens Sparsø;Niels Haandbæk;Mogens Isager;Lars Skovby Nielsen	2004	VLSI Signal Processing	10.1023/B:VLSI.0000017005.01462.d5	embedded system;computer architecture;parallel computing;message passing;real-time computing;multiprocessing;distributed memory;application-specific instruction-set processor;audio signal processing;computer science;operating system;programming language;symmetric multiprocessor system	Arch	3.314149632944064	48.37873252792607	74942
2b2a61ad7d1de2ae0f2f5fd47334085e7949b746	dynamic power management in environmentally powered systems	embedded systems;energy harvesting;solar cells;application rate control;dynamic power management;energy harvesting embedded systems;energy management;environmentally powered systems;real-time scheduling;reward maximization;solar cells;system performance optimization;wireless sensor nodes;power management;embedded systems;energy harvesting;model predictive control;real-time scheduling;reward maximization	In this paper a framework for energy management in energy harvesting embedded systems is presented. As a possible example scenario, we focus on wireless sensor nodes which are powered by solar cells. We demonstrate that classical power management solutions have to be reconceived and/or new problems arise if perpetual operation of the system is required. In particular, we provide a set of algorithms and methods for different application scenarios, including real-time scheduling, application rate control as well as reward maximization. The goal is to optimize the performance of the application subject to given energy constraints. Our methods optimize the system performance which allows the usage of, e.g., smaller solar cells and smaller batteries. Our theoretical results are supported by simulations using long-term measurements of solar energy in an outdoor environment. Furthermore, to demonstrate the practical relevance of our approaches, we measured the implementation overhead of our algorithms on real sensor nodes.	embedded system;expectation–maximization algorithm;overhead (computing);power management;real-time clock;relevance;scheduling (computing);simulation;solar cell	Clemens Moser;Jian-Jia Chen;Lothar Thiele	2010	2010 15th Asia and South Pacific Design Automation Conference (ASP-DAC)		control engineering;embedded system;real-time computing;computer science;engineering;computer performance;solar energy;electric power system;computational model;energy harvesting;model predictive control;energy management	EDA	-3.697505501917455	59.49064275748244	75124
bf1975efe12710fb3aa7349a85406d44cd1f1d75	a re-configurable processor for petri net simulation	computerised control;reconfigurable architectures;real time systems reconfigurable architectures computerised control digital simulation petri nets field programmable gate arrays parallel architectures;traffic flow;scaling up;parallel architectures;pci bus speeds re configurable processor petri net simulation system simulation traffic flows network message traffic cpu intensive sequential processors achilles reconfigurable processor innovative 3 dimensional stack fpgas 3d arrangement host achilles bandwidth individual stacks computing power pc host data transfer;3 dimensional;field programmable gate arrays;petri nets;petri net;reconfigurable processor;digital simulation;real time systems;computational modeling field programmable gate arrays bandwidth prototypes petri nets concurrent computing parallel processing intelligent systems intelligent networks information processing	Simulation of systems for the control of large numbers of objects such as traffic flows, network message traffic, etc is CPU intensive and may require inordinately long runs on conventional sequential processors. This work describes the Achilles reconfigurable processor and techniques for programming it to carry out Petri Net simulations. Achilles is an innovative 3-dimensional stack of FPGAs. The 3-D arrangement allows (a) a large number of FPGAs to fit in a small volume, (b) a large degree of flexibility in the way individual devices are interconnected, (c) interconnection with one or more hosts with host-Achilles bandwidth being scaled up to meet requirements and (d) individual stacks to be connected together in a wide variety of patterns so that the computing power of the stack may be scaled as neces sary. Bandwidths between the stack and a PC host have been measured at over 30Mbytes/second in the first prototype of the stack: the interconnection is capable of transferring data at PCI bus speeds with the newer, faster FPGAs used in the second prototype currently under construction. This architecture is particularly suitable for Petri Net simulations as hundreds of places in a net can be simultaneously active reducing by orders of magnitude the time necessary for simulations.	accumulator (computing);central processing unit;emoticon;field-programmable gate array;interconnection;petri net;prototype;real-time clock;real-time computing;reconfigurable computing;requirement;shift register;simulation;vhdl	John Morris;Gary A. Bundell;Sonny Tham	2000		10.1109/HICSS.2000.926968	embedded system;parallel computing;real-time computing;computer science;operating system;petri net	Arch	4.113399921328945	46.52150686777815	75207
3fa1de1de182279c8567e99a18d8b0d80b46020e	secure-by-construction composable componentry for network processing	100 gbps;hardware software co design;embedded hardware;network processor;line speed processor;secure by construction;stream processing	Techniques commonly used for analyzing streaming video, audio, SIGINT, and network transmissions, at less-than-streaming rates, such as data decimation and ad-hoc sampling, can miss underlying structure, trends and specific events held in the data[3]. This work presents a secure-by-construction approach [7] for the upper-end data streams with rates from 10- to 100 Gigabits per second. The secure-by-construction approach strives to produce system security through the composition of individually secure hardware and software components. The proposed network processor can be used not only at data centers but also within networks and onboard embedded systems at the network periphery for a wide range of tasks, including preprocessing and data cleansing, signal encoding and compression, complex event processing, flow analysis, and other tasks related to collecting and analyzing streaming data. Our design employs a four-layer scalable hardware/software stack that can lead to inherently secure, easily constructed specialized high-speed stream processing.  This work addresses the following contemporary problems:  (1) There is a lack of hardware/software systems providing stream processing and data stream analysis operating at the target data rates; for high-rate streams the implementation options are limited: all-software solutions can't attain the target rates[1]. GPUs and GPGPUs are also infeasible: they were not designed for I/O at 10-100Gbps; they also have asymmetric resources for input and output and thus cannot be pipelined[4, 2], whereas custom chip-based solutions are costly and inflexible to changes, and FPGA-based solutions are historically hard to program[6];  (2) There is a distinct advantage to utilizing high-bandwidth or line-speed analytics to reduce time-to-discovery of information, particularly ones that can be pipelined together to conduct a series of processing tasks or data tests without impeding data rates;  (3) There is potentially significant network infrastructure cost savings possible from compact and power-efficient analytic support deployed at the network periphery on the data source or one hop away;  (4) There is a need for agile deployment in response to changing objectives;  (5) There is an opportunity to constrain designs to use only secure components to achieve their specific objectives. We address these five problems in our stream processor design to provide secure, easily specified processing for low-latency, low-power 10-100Gbps in-line processing on top of a commodity high-end FPGA-based hardware accelerator network processor. With a standard interface a user can snap together various filter blocks, like Legos™, to form a custom processing chain.  The overall design is a four-layer solution in which the structurally lowest layer provides the vast computational power to process line-speed streaming packets, and the uppermost layer provides the agility to easily shape the system to the properties of a given application. Current work has focused on design of the two lowest layers, highlighted in the design detail in Figure 1.  The two layers shown in Figure 1 are the embeddable portion of the design; these layers, operating at up to 100Gbps, capture both the low- and high frequency components of a signal or stream, analyze them directly, and pass the lower frequency components, residues to the all-software upper layers, Layers 3 and 4; they also optionally supply the data-reduced output up to Layers 3 and 4 for additional processing.  Layer 1 is analogous to a systolic array of processors on which simple low-level functions or actions are chained in series[5]. Examples of tasks accomplished at the lowest layer are: (a) check to see if Field 3 of the packet is greater than 5, or (b) count the number of X.75 packets, or (c) select individual fields from data packets. Layer 1 provides the lowest latency, highest throughput processing, analysis and data reduction, formulating raw facts from the stream;  Layer 2, also accelerated in hardware and running at full network line rate, combines selected facts from Layer 1, forming a first level of information kernels. Layer 2 is comprised of a number of combiners intended to integrate facts extracted from Layer 1 for presentation to Layer 3. Still resident in FPGA hardware and hardware-accelerated, a Layer 2 combiner is comprised of state logic and soft-core microprocessors.  Layer 3 runs in software on a host machine, and is essentially the bridge to the embeddable hardware; this layer exposes an API for the consumption of information kernels to create events and manage state. The generated events and state are also made available to an additional software Layer 4, supplying an interface to traditional software-based systems.  As shown in the design detail, network data transitions systolically through Layer 1, through a series of light-weight processing filters that extract and/or modify packet contents. All filters have a similar interface: streams enter from the left, exit the right, and relevant facts are passed upward to Layer 2. The output of the end of the chain in Layer 1 shown in the Figure 1 can be (a) left unconnected (for purely monitoring activities), (b) redirected into the network (for bent pipe operations), or (c) passed to another identical processor, for extended processing on a given stream (scalability).	agile software development;algorithm;application programming interface;central processing unit;complex event processing;component-based software engineering;data center;data rate units;data-flow analysis;decimation (signal processing);diplexer;embedded system;emoticon;field-programmable gate array;general-purpose computing on graphics processing units;gigabit;graphics processing unit;hardware acceleration;high- and low-level;hoc (programming language);hypervisor;input/output;low-power broadcasting;microprocessor;network packet;network processor;preprocessor;processor design;sampling (signal processing);scalability;signals intelligence;software deployment;software system;state logic;stream (computing);stream processing;streaming media;systolic array;throughput;transponder (satellite communications)	Lisa J. K. Durbeck;Peter M. Athanas;Nicholas J. Macias	2014		10.1145/2600176.2600203	embedded system;parallel computing;real-time computing;computer science;network layer	Arch	3.0656315804391157	48.60143264072874	75286
98242fae8f66ca9aab16a0cbb54da7a880960451	seamless hardware-software integration in reconfigurable computing systems	virtual memory;master slave hardware application software memory management programming profession parallel programming concurrent computing multithreading operating systems;multi threading;reconfigurable system;hardware software codesign;software integration;reconfigurable computing;programming paradigm;reconfigurable architectures;storage management;virtual reality reconfigurable architectures hardware software codesign multi threading storage management;virtual reality;hardware accelerator;system level virtualization layer seamless hardware software integration reconfigurable computing system hardware agnostic programming paradigm platform specific task;programming model	Ideally, reconfigurable-system programmers and designers should code algorithms and write hardware accelerators independently of the underlying platform. To realize this scenario, the authors propose a portable, hardware-agnostic programming paradigm, which delegates platform-specific tasks to a system-level virtualization layer. This layer supports a chosen programming model and hides platform details from users much as general-purpose computers do. We introduce multithreaded programming model for reconfigurable computing based on a unified virtual-memory image for both software and hardware application parts. We also address the challenge of achieving seamless hardware-software interfacing and portability with minimal performance penalties.	algorithm;computer;general-purpose modeling;hardware acceleration;platform-specific model;programmer;programming model;programming paradigm;reconfigurable computing;seamless3d;software portability;system integration;thread (computing)	Miljan Vuletic;Laura Pozzi;Paolo Ienne	2005	IEEE Design & Test of Computers	10.1109/MDT.2005.44	embedded system;computer architecture;computing;parallel computing;real-time computing;reactive programming;reconfigurable computing;computer science;software development;operating system;hardware architecture;virtual reality;programming paradigm;programming language;system programming;fpgac	EDA	-2.1992175916136856	49.10828487020544	75683
8977f8be1b59a8084dd6ac17f47e01c21a6d2761	high-level software synthesis for the design of communication systems	systeme temps reel;software;traitement signal;telecommunications computing digital communication systems digital signal processing chips high level languages optimisation;optimisation;communication system;bloc diagramme;high level languages;simulation systeme;synthese;capacidad canal;analisis sistema;optimizacion;logiciel;concepcion sistema;optimization technique;phase synchronization;real time;flot donnee;capacite canal;conception;floating point dsp high level software synthesis computer simulation design communication systems software programmable architectures digital signal processors real time signal processing systems optimization techniques data flow oriented block diagrams phase synchronizer automatic gain control;flujo datos;sistema complejo;design space;synthesis;sintesis;telecommunications computing;systeme complexe;telecomunicacion;channel capacity;complex system;system design;senal numerica;signal processing;temps reel;telecommunication;diseno;signal numerique;system analysis;digital signal processor;tiempo real;logicial;design;digital signal processing chips;real time system;optimization;floating point;analyse systeme;procesador;sistema tiempo real;signal synthesis digital signal processing computer architecture digital signal processors real time systems signal processing throughput constraint optimization memory management sampling methods;digital signal;software synthesis;processeur;data flow;diagrama conjunto;system simulation;procesamiento senal;simulacion sistema;digital communication systems;conception systeme;processor;block diagram;automatic gain control	bstracr-Design and analysis of complex communication syss increasingly rely on sophisticated simulation and synthesis techbiques. In this paper, we present a synthesis environment which targets software pmgrammable architectures such as digital signal processors (DSP’s). These processors are well suited for implementation of real-time signal processing systems with medium throughput requirements. We present techniques which tightly couple the synthesis environment to an existing communibtion system simulator. This enables a seamless transition n the simulation and implementation design level of communication systems. Special focus is on optimization techniques for mapping data flow oriented block diagrams ontq DSP’s. The tombination of different mapping and optimization strategies dloWs comfortable synthesis of real-time code which is highly adapted to application-specific needs imposed by constraints on membry space, sampling rate, or latency. Thus, tradeoff analysis is supported by efficient interactive or automatic exploration of the design space. Finally, all presented concepts are illustrated by the ,design of a phase synchronizer with automatic gain control on a floating-point DSP.	automatic gain control;central processing unit;dataflow;diagram;digital signal processor;mathematical optimization;real-time clock;requirement;sampling (signal processing);seamless3d;signal processing;simulation;synchronizer (algorithm);throughput;unix signal	Sebastian Ritz;Matthias Pankert;Vojin Zivojnovic;Heinrich Meyr	1993	IEEE Journal on Selected Areas in Communications	10.1109/49.219550	block diagram;embedded system;design;digital signal processor;automatic gain control;real-time computing;telecommunications;phase synchronization;digital signal;computer science;floating point;signal processing;system analysis;channel capacity	EDA	1.6608803134842185	53.76696761734251	75734
2e2dd06e0caefa2b2689a1e4be60508a158ad563	hardware implementation of ram neural networks	logica booleana;software tool;concepcion sistema;implementation;boolean neural networks;ram neural network;image classification;estrategia;fast prototyping;classification;strategy;prototipo;ejecucion;reconnaissance caractere;grande vitesse;system design;vhdl;logique booleenne;gran velocidad;neural network hardware implementation;computer hardware;reseau neuronal;boolean logic;logic gate;strategie;prototype;character recognition;materiel informatique;high speed;material informatica;clasificacion;hardware implementation;red neuronal;conception systeme;reconocimiento caracter;software implementation;neural network	This work describes an alternative technique for hardware and software implementation of RAM based Boolean neural networks, which describes neurons using the VHDL language. An example of application consisting of the classification problem of the British mail scanned address is attended with a RAM architecture presenting 340 x 12-input neurons. The weights of each neuron are represented by its truth table and described using simple logic gates (AND, OR, and NOT), aiming to make possible the network logic minimisation and its hardware implementation by the ALTERA MAX+PLUS II fast prototyping package (Altera, 1992). The developed software tool allows the specification and training of the network. Then, its VHDL description is generated to be interpreted and minimised by the ALTERA EPLD design system. If it is not necessary to have high speed processing or if pre-processing phases are needed, the ANN can be implemented in software. The software strategy makes use of the direct translation of the VHDL description into a simplified C language code. Once the user has specified and taught the network, this approach makes possible automatic prototyping of RAM neural networks in software and hardware.	artificial neural network;erasable programmable logic device;language code;logic gate;neuron;preprocessor;programming tool;random-access memory;vhdl	Eduardo do Valle Simões;Luís Felipe Uebel;Dante Augusto Couto Barone	1996	Pattern Recognition Letters	10.1016/0167-8655(95)00137-9	boolean algebra;embedded system;contextual image classification;logic gate;biological classification;vhdl;strategy;computer science;artificial intelligence;machine learning;prototype;implementation;artificial neural network;algorithm;systems design	ML	8.662921947612128	48.022725843359616	75738
0d81597723c868d355d7fa781c3dd6451d6acd61	data flow cad tool for firniware development and power consumption estimation in multi-core hearing aids	digital signal processing;design automation;auditory system;estimation;microprogramming;power demand;power measurement	This paper depicts highlights of data flow CAD (Computer-aided design) tool used for firmware development at the highest level of abstraction, with the ability of source code profiling and power consumption estimation for heterogeneous multi-core hearing aids.	computer-aided design;dataflow architecture;firmware;multi-core processor;profiling (computer programming)	Momcilo Krunic;Ivan Povazan;Miroslav Popovic;Jelena Kovacevic	2016	2016 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2016.7430736	embedded system;estimation;electronic engineering;electronic design automation;computer hardware;computer science;engineering;digital signal processing;microcode;statistics	EDA	2.5814670765432837	53.61420409990543	75816
27cf9dc5e4289497855cb1e593667c18e4f1be2c	leveraging semi-formal and sequential equivalence techniques for multimedia soc performance validation	verification;silicon;analytical models;data intensive application;logic design;video signal processing;application software;performance analysis silicon hardware analytical models field programmable gate arrays computational modeling runtime application software software performance bandwidth;real time;sequential circuits;system on chip logic design sequential circuits;multimedia soc;runtime;sequential equivalence checking verification experimentation performance validation bandwidth analysis semi formal methods;software performance;formal method;sequential equivalence checking semiformal equivalence multimedia soc performance validation memory subsystems rtl implementation;computational modeling;system on chip;sequential equivalence checking;semiformal equivalence;performance analysis;bandwidth analysis;rtl implementation;performance validation;bandwidth;memory subsystems;field programmable gate arrays;high throughput;abstract interpretation;experimentation;equivalence checking;semi formal methods;hardware	For multimedia SOCs supporting real time, high throughput and data intensive applications, performance validation of memory subsystems is needed to uncover the bottlenecks in the RTL implementation. Traditional validation techniques are either too slow, non-exhaustive (like performance simulations or running pseudo applications on FPGA platforms), or are not accurate enough to guarantee conformance (like abstract interpretation and analysis). In this paper we present an approach for performance validation which uses (a) semi-formal techniques rather than pure simulation for providing a wider coverage, (b) actual RTL implementations wherever available for more accurate analysis, and (c) sequential equivalence checking for validating the abstract models for IP's whose RTL is either not present or from which datapath has been abstracted out. We illustrate this approach using two case studies from video signal processing platforms. In the first study, performance Issues found in silicon were detected using the proposed approach, and in the second study a number of performance bottlenecks were detected much before the RTL was frozen.	abstract interpretation;bottleneck (software);conformance testing;data-intensive computing;datapath;emoticon;field-programmable gate array;formal equivalence checking;formal methods;semiconductor industry;signal processing;simulation;system on a chip;throughput;turing completeness	Lovleen Bhatia;Jayesh Gaur;Praveen Tiwari;Raj S. Mitra;Sunil H. Matange	2007	2007 44th ACM/IEEE Design Automation Conference	10.1145/1278480.1278499	system on a chip;high-throughput screening;embedded system;computer architecture;application software;parallel computing;logic synthesis;real-time computing;verification;formal methods;software performance testing;computer science;formal equivalence checking;sequential logic;silicon;computational model;bandwidth;field-programmable gate array	EDA	-0.5880240259725453	56.758209558321155	75837
a3d10567097fae9bf61b1db21addd1303366c91b	development of flexible inserter for ic chip testing			integrated circuit	Toshihiro Taguchi	2001	JRM	10.20965/jrm.2001.p0289	computer hardware;chip;embedded system;computer science	EDA	6.29540259132084	50.12004782865651	75838
531727ded1095480c9c3e89affd782dda649c73c	the multi-context reconfigurable processing unit for fine-grain computing	field programmable gate array;data parallel;evaluation performance;reseau logique;architecture systeme;estimation mouvement;performance evaluation;reconfigurable computing;routing;reconfigurable architectures;evaluacion prestacion;estimacion movimiento;reconfigurable logic;routage;software systems;motion estimation;sistema n niveles;circuito logico;fpga;red puerta programable;reseau porte programmable;computer architecture;software architecture;reconfigurable architecture;architecture ordinateur;circuit logique;systeme n niveaux;contexto;multimedia communication;fine grain reconfigurable architecture;multilevel system;contexte;arquitectura sistema;arquitectura ordenador;reconfigurable processing unit;multi context;high throughput;system architecture;communication multimedia;logic circuit;architecture reconfigurable;context;architecture logiciel;red logica;logic array;enrutamiento	Due to the fast development of multimedia and communication applications, reconfigurable computing which has the great potential to accelerate a wide variety of applications is getting more and more important in computer architecture and software system. By mapping the computationally intense portions of an application into hardware, the application could be greatly accelerated. Reconfigurable computing incorporates the benefits of implementations of software and hardware. In this paper, we propose a novel reconfigurable processing unit, FMRPU, which is a fine-grain with multi-context reconfigurable processing unit targeting at high-throughput and data-parallel applications. It contains 64 reconfigurable logic arrays, 16 switch boxes, and connects with each other via three hierarchical-level connectivities. According to the simulation results, the longest routing path of FMRPU only takes 6.5 ns at 0.35 processes, which is able to construct the required logic circuit efficiently. To compare with same kind devices in dealing with Motion Estimation operations, the performance is raise to 17% and has excellent performance in executing DSP algorithms.	algorithm;bit-level parallelism;computation;computer architecture;high-throughput computing;interconnection;logic gate;motion estimation;parallel computing;reconfigurable computing;routing;signal processing;simulation;software system;stream processing;throughput	Jih-Ching Chiu;Yu-Liang Chou;Ren-Bang Lin	2008	J. Inf. Sci. Eng.		embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;field-programmable gate array;systems architecture	Arch	1.9131514504329494	56.853807114986346	75991
16ff5a76ded09e64b407a68b3d00b0fcf1b828fd	minimizing communication cost for reconfigurable slot modules	integer linear programming;degradation;cost function;building block;routing;runtime;field programmable gate arrays runtime space technology routing cost function degradation delay integer linear programming mathematical model context;chip;mathematical model;communication cost;space technology;field programmable gate arrays;context;integer linear program	We discuss the problem of communication-aware module placement in array-like reconfigurable environments, such as the Erlangen Slot Machine (ESM). Bad placement of modules may degrade performance due to increased signal delays and wastes chip space for the reconfigurable multiple bus. We present integer linear programming (ILP) formulations that address both of these problems; both ILPs can be used stand-alone or as building blocks for more involved mathematical models. We validate our models by demonstrating their usefulness for a set of realistic benchmarks.	benchmark (computing);integer programming;linear programming;mathematical model	Sándor P. Fekete;Jan van der Veen;Mateusz Majer;Jürgen Teich	2006	2006 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2006.311263	chip;embedded system;routing;real-time computing;degradation;integer programming;computer science;theoretical computer science;operating system;mathematical model;space technology;field-programmable gate array	EDA	0.2173372071704174	53.646390520167145	76111
538503d85a4dc09c31c394db12472011b9614584	a time-triggered network-on-chip	pseudo static communication schedule;time triggered;real time systems field programmable gate arrays logic design microprocessor chips network on chip;logic design;network on chip;chip multiprocessor;network interlace easiest certification;cell processor;fpga;network on a chip clocks bandwidth scheduling real time systems synchronization field programmable gate arrays network interfaces certification digital signal processing;chip;on chip multiprocessor designs;time triggered architecture;network on chip time triggered architecture on chip real time systems pseudo static communication schedule fpga on chip multiprocessor designs cell processor network interlace easiest certification safety critical applications;field programmable gate arrays;network interface;safety critical applications;on chip real time systems;microprocessor chips;real time systems	In this paper we propose a time-triggered network-on-chip (NoC) for on-chip real-time systems. The NoC provides time predictable on-and off-chip communication, a mandatory feature for dependable real-time systems. A regular structured NoC with a pseudo-static communication schedule allows for a high bandwidth. In this paper we argue for a simple, time-triggered NoC structure to achieve maximum bandwidth. We have implemented the proposed TT-NoC in a low-cost FPGA. The base bandwidth is 29 Gbit/s and the peak bandwidth 230 Gbit/s for eight nodes. The idea is in line with current on-chip multiprocessor designs, such as the cell processor. The simple design of the network and the network interlace easiest certification of the proposed NoC for safety critical applications.	cell (microprocessor);data synchronization;dependability;field-programmable gate array;gigabit;global positioning system;interconnection;interlaced video;multi-core processor;multiplexer;multiprocessing;network on a chip;real-time clock;real-time computing;real-time transcription;system on a chip;systems design;transponder timing	Martin Schoeberl	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380675	embedded system;parallel computing;real-time computing;computer science;operating system;network on a chip;field-programmable gate array	EDA	1.9209562737970989	60.13689393369345	76142
093820168bd9be6f4d335c538decb9abcf35deba	towards a standard hardware description language	unary-to-binary conversion;comparator circuit;hardware description language zeus;standard hardware description language;cad tool;close integration;music generation;design example;hardware description language;example circuit;general requirement;recent development;layout semantics;music generation circuit;good result;design automation;computer languages;adders;security;software systems;satisfiability;standardization	General requirements for hardware description languages are defined. The hardware description language Zeus is claimed to satisfy most of these requirements. Zeus is summarized and briefly compared to other hardware description languages. The expressiveness of Zeus is demonstrated on designs for music generation, comparison and unary-to-binary conversion.	hardware description language;requirement;unary operation	Karl J. Lieberherr	1984	21st Design Automation Conference Proceedings		embedded system;computer architecture;electronic engineering;electronic design automation;computer science;information security;theoretical computer science;operating system;hardware description language;programming language;standardization;algorithm;adder;software system;satisfiability	EDA	8.84393248645153	51.931298094906055	76218
21bc2a2630c9105893ebbf66697d6029a70e7a2c	ercbench: an open-source benchmark suite for embedded and reconfigurable computing	software;software based control flow;reconfigurable computing benchmarks open source embedded computing;open source benchmark suite;decoding;encryption;reconfigurable computing;hardware benchmark testing software field programmable gate arrays decoding wireless communication encryption;reconfigurable architectures;hardware description languages;hardware accelerator;ercbench open source benchmark suite reconfigurable computing embedded computing software based control flow hardware based computation verilog model hardware accelerator;verilog model;wireless communication;public domain software;control flow;benchmarks;reconfigurable architectures benchmark testing hardware description languages public domain software;field programmable gate arrays;benchmark testing;ercbench;embedded computing;hardware based computation;hardware;open source	Researchers in embedded and reconfigurable computing are often hindered by a lack of suitable benchmarks with which to accurately evaluate their work. Without a suitable benchmark suite, researchers use either outdated, unrealistic benchmarks or spend valuable time creating their own. In this paper, we present ERCBench—a freely-available, open-source benchmark suite geared towards embedded and reconfigurable computing research. ERCBench benchmarks represent a variety of application areas, including multimedia processing, wireless communications, and cryptography. They consist of synthesizable Verilog models for hardware accelerators and hybrid hardware/software applications that combine software-based control flow with hardware-based computation tasks.	algorithm;benchmark (computing);computation;control flow;cryptography;embedded system;hardware acceleration;open-source software;reconfigurable computing;verilog	Daniel W. Chang;Christipher D. Jenkins;Philip C. Garcia;Syed Zohaib Gilani;Paula Aguilera;Aishwarya Nagarajan;Michael J. Anderson;Matthew A. Kenny;Sean M. Bauer;Michael J. Schulte;Katherine Compton	2010	2010 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2010.85	embedded system;benchmark;computer architecture;parallel computing;real-time computing;hardware acceleration;reconfigurable computing;computer science;operating system;hardware description language;control flow;public domain software;encryption;wireless;field-programmable gate array	EDA	3.6870089365043937	51.24082288569445	76561
013d76fefe82f6f9f2733fd463e195e943c2d2b7	energy-driven integrated hardware-software optimizations using simplepower	hardware software interaction;architectural experimentation;energy simulator;hardware software codesign;application software;system energy;signal analysis;energy consumption multidimensional signal processing application software energy dissipation throughput design optimization circuits computer architecture hardware signal analysis;hardware software optimizations;energy dissipation;spectrum;compiler;design optimization;computer architecture;design constraint;low power;computer architecture hardware software codesign microprocessor chips;energy optimization;energy hotspots;energy consumption;signal processing;low power architectures;compiler optimization;multidimensional signal processing;energy optimization and estimation;simplepower;circuits;it evaluation;energy simulation;compiler optimizations;energy estimate;hardware software interaction hardware software optimizations simplepower energy dissipation design constraint compiler architectural experimentation energy hotspots system energy low power architectures compiler optimizations energy simulator;microprocessor chips;throughput;hardware	With the emergence of a plethora of embedded and portable applications, energy dissipation has joined throughput, area, and accuracy/precision as a major design constraint. Thus, designers must be concerned with both optimizing and estimating the energy consumption of circuits, architectures, and software. Most of the research in energy optimization and/or estimation has focused on single components of the system and has not looked across the interacting spectrum of the hardware and software. The novelty of our new energy estimation framework, SimplePower, is that it evaluates the energy considering the system as a whole rather than just as a sum of parts, and that it concurrently supports both compiler and architectural experimentation. We present the design and use of the SimplePower framework that includes a transition-sensitive, cycle-accurate datapath energy model that interfaces with analytical and transition sensitive energy models for the memory and bus subsystems, respectively. We analyzed the energy consumption of ten codes from the multidimensional array domain, a domain that is important for embedded video and signal processing systems, after applying different compiler and architectural optimizations. Our experiments demonstrate that early estimates from the SimplePower energy estimation framework can help identify the system energy hotspots and enable architects and compiler designers to focus their efforts on these areas.	code;compiler;datapath;design closure;embedded system;emergence;experiment;interaction;mathematical optimization;signal processing;throughput	Narayanan Vijaykrishnan;Mahmut T. Kandemir;Mary Jane Irwin;Hyun Suk Kim;Wu Ye	2000		10.1145/339647.339659	computer architecture;parallel computing;real-time computing;computer science;operating system;signal processing;optimizing compiler;programming language	Arch	2.3593681376301245	53.83232053204205	76682
75d192c2b549ce10326952bd718e87a72ffbe8d7	investigation and optimization of pin multiplexing in high-level synthesis		This paper investigates the effect of pin multiplexing on the resultant micro-architecture of synthesizable behavioral descriptions for High-Level Synthesis (HLS). A method is presented to find the most efficient pin assignments by assigning multiple logic inputs and outputs to the same physical ports such that the performance degradation and area overhead is minimized. The proposed method is a fast heuristic based on the scheduling results of HLS seen as a black box and hence is flexible enough to work with any HLS tool. Experimental results show that our proposed method is very efficient compared to an exhaustive search and a simulated annealing method at a fraction of the time and much better than randomly selecting the pins to be multiplexed.	black box;brute-force search;elegant degradation;heuristic;high-level synthesis;input/output;microarchitecture;multiplexing;overhead (computing);program optimization;randomness;resultant;scheduling (computing)	Shuangnan Liu;Francis Lau;Benjamin Carrión Schäfer	2018		10.1145/3194554.3194629	black box (phreaking);real-time computing;computer science;scheduling (computing);simulated annealing;high-level synthesis;brute-force search;heuristic;microarchitecture;multiplexing	EDA	-0.5670891904788108	52.008159805638506	76753
4046fa422ca480a857736ed9251ed136614c4f45	why to redesign pdes framework for smart devices: an empirical study		Parallel and distributed simulation system requires time management algorithms to ensure the executions are synchronized. Without the synchronization procedures it is difficult to utilize the hardware platform efficiently. Similarly, traditional synchronization algorithms are not developed for mobile devices. The execution of state-of-the-art algorithms over mobile devices consumed energy significantly. At the same time, before applying energy optimization on traditional algorithms, it is more beneficial to analyze function level energy and time consumption, that can be used to reduce overall energy consumption. Moreover, as an insight, energy profiling at function level can help researchers to modify the implementation for energy constraint devices. In this paper, we have analyzed Rensselaers Optimistic Simulation System (ROSS) and benchmark the energy and time consumption at function level. The experimental section shows the function level energy and time consumption of conservative and optimistic synchronization algorithms during PHOLD benchmark is being executed.	smart device	Fahad Maqbool;Syed Meesam Raza Naqvi;Asad W. Malik	2017			empirical research;time management;energy consumption;profiling (computer programming);synchronization;real-time computing;energy minimization;mobile device;engineering	SE	-3.5005114356451985	56.9314250848929	76848
29d66177d2172fd3597e75d4073335c82deedbd6	a robust protocol for concurrent on-line test (colt) of noc-based systems-on-a-chip	verification;on chip components;reliability;systems on a chip;on line testing;network on chip;test access mechanism;robust protocol management performance design reliability verification concurrent on line testing network on chip;performance;concurrent on line test;networks on a chip;concurrent on line testing;system on a chip;health monitoring;chip;system health;condition monitoring;complex system;test infrastructure ip;integrated circuit testing;communication protocol;network on chip condition monitoring integrated circuit testing;robust protocol;design;soc;networks on a chip concurrent on line test noc systems on a chip on chip components system health test infrastructure ip soc health monitoring;noc;network on a chip;management;protocols system testing network on a chip system on a chip monitoring noise robustness life testing degradation disaster management hazards	Concurrent on-line testing (COLT) of complex systems-on-a-chip (SoC) designs under lowering noise margins and degrading lifetimes of on-chip components, provides the ideal solution for the monitoring of system health while managing intrusion into executing applications. Deploying Test Infrastructure-IPs (TI-IPs) into designs has demonstrated the feasibility of using COLT in SoCs. Identifying potential hazards and ensuring correct operation of COLT is critical to providing reliable health monitoring. With the emergence of networks-on-a-chip (NoC) as communication infrastructures not only suitable for application related on-chip communication, but also test access mechanisms to on-chip cores, the experimental setup in this research, deploys TI-IP in a NoC environment and demonstrates TI-IP operation, its communication protocol specification and other related costs.	colt;communications protocol;complex systems;emergence;network on a chip;noise margin;online and offline;system on a chip	Praveen Bhojwani;Rabi N. Mahapatra	2007	2007 44th ACM/IEEE Design Automation Conference	10.1145/1278480.1278650	system on a chip;embedded system;electronic engineering;real-time computing;computer science;engineering;network on a chip	EDA	6.4935395229763975	57.992050638884855	77038
8b9a880af0d0c0a2bc7469417dc836d7adadb44d	improving offset assignment for embedded processors	graph theory;traitement signal;digital signal processing;teoria grafo;calculateur embarque;code optimization;virgule fixe;code generation;theorie graphe;embedded system;coma fija;program optimization;fixed point;chip;tratamiento numerico;signal processing;boarded computer;code size;digital processing;optimisation programme;procesamiento senal;high level language;embedded processor;calculador embarque;traitement numerique;optimizacion programa	Embedded systems consisting of the application program ROM, RAM, the embedded processor core, and any custom hardware on a single wafer are becoming increasingly common in application domains such as signal processing. Given the rapid deployment of these systems, programming on such systems has shifted from assembly language to high-level languages such as C, C++, and Java. The processors used in such systems are usually targeted toward specific application domains, e.g., digital signal processing (DSP). As a result, these embedded processors include application-specific instruction sets, complex and irregular data paths, etc., thereby rendering code generation for these processors difficult. In this paper, we present new code optimization techniques for embedded fixed point DSP processors which have limited on-chip program ROM and include indirect addressing modes using post-increment and decrement operations. We present a heuristic to reduce code size by taking advantage of these addressing modes. Our solution aims at improving the offset assignment produced by Liao et al.’s solution. It finds a layout of variables in RAM, so that it is possible to subsume explicit address register manipulation instructions into other instructions as a post-increment or post-decrement operation. Experimental results show the effectiveness of our solution.	addressing mode;algorithm;assembly language;assignment (computer science);c++;central processing unit;code generation (compiler);compiler;computer;digital signal processing;embedded system;fixed point (mathematics);heuristic;high- and low-level;increment and decrement operators;java;mathematical optimization;memory address register;multi-core processor;program optimization;random-access memory;read-only memory;software deployment;static single assignment form;time complexity	Sunil Atri;J. Ramanujam;Mahmut T. Kandemir	2000		10.1007/3-540-45574-4_11	embedded system;parallel computing;real-time computing;computer science;graph theory;operating system;program optimization;signal processing;distributed computing;programming language;algorithm	EDA	-0.21331203841053134	51.91335945057252	77056
267ff19679cc989988f33fee1d7c96dab33edd69	variability-aware duty cycle scheduling in long running embedded sensing systems	semiconductor technology;sensors;quality of service variability aware duty cycle scheduling long running embedded sensing systems temperature dependent leakage power variability embedded processors semiconductor technology;temperature dependence;batteries schedules temperature measurement sensors temperature distribution power demand program processors;embedded systems;long running embedded sensing systems;leakage power;semiconductor technology embedded systems microprocessor chips;batteries;duty cycle;embedded processors;schedules;temperature dependent leakage power variability;temperature measurement;power consumption;quality of service;variability aware duty cycle scheduling;power demand;program processors;temperature distribution;microprocessor chips	Instance and temperature-dependent leakage power variability is already a significant issue in contemporary embedded processors, and one which is expected to increase in importance with scaling of semiconductor technology. We measure and characterize this leakage power variability in current microprocessors, and show that variability aware duty cycle scheduling produces 7.1× improvement in sensing quality for a desired lifetime. In contrast, pessimistic estimations of power consumption leave 61% of the energy untapped, and datasheet power specifications fail to meet required lifetimes by 14%. Finally, we introduce a duty cycle abstraction for TinyOS that allows applications to explicitly specify lifetime and minimum duty cycle requirements for individual tasks, and dynamically adjusts duty cycle rates so that overall quality of service is maximized in the presence of power variability.	central processing unit;datasheet;duty cycle;embedded system;heart rate variability;image scaling;microprocessor;overhead (computing);quality of service;requirement;scheduling (computing);semiconductor;sensor;spatial variability;spectral leakage;tinyos	Lucas Francisco Wanner;Rahul Balani;Sadaf Zahedi;Charwak Apte;Puneet Gupta;Mani B. Srivastava	2011	2011 Design, Automation & Test in Europe	10.1109/DATE.2011.5763031	embedded system;electronic engineering;real-time computing;quality of service;temperature measurement;schedule;computer science;engineering;sensor;duty cycle	EDA	-3.6002773474733076	57.58126414941625	77077
be2d06c90bbebbbc24af8f6138189e15fe606741	priority-driven area optimization in high-level synthesis	resource management;simulated annealing;high level synthesis;scheduling	"""One of the major enhancements that can be made to the high-level synthesis (HLS) process is reducing the overall area of a design in order to either decrease the manufacturing costs or to introduce more functionality to the circuit. Optimizing the area of the datapath is considered a primary field of research in HLS. This work proposes an approach to reduce the area in field programmable gate array (FPGA) by simultaneously tackling the three central tasks of HLS. Scheduling, allocation, and binding are performed and the optimal solution based on area reduction is obtained by using simulated annealing with a priority function. The aim of the priority function is to guide the simulated annealing process into finding the best solution while at the same time incurring the least possible execution time. In order to achieve better results than the initial solution, rescheduling, swapping operations between functional units, swapping variables between registers, and swapping inputs to functional units are considered in the annealing process. A cost function is devised to evaluate a potential move's success or failure. The simulation environment """"Eridanus"""" has been developed in order to support implementation and testing. Several benchmarks were tested and the numerical results consisting of the execution time along with the best solution were recorded to illustrate the performance of the proposed technique. Area reduction was obtained compared to the conventional HLS flow; furthermore, an average substantial reduction in design space exploration time was obtained compared to non-priority based area optimization techniques."""	high-level synthesis;program optimization	Maria Abi Saad;Iyad Ouaiss	2011	Journal of Circuits, Systems, and Computers	10.1142/S0218126611007803	embedded system;real-time computing;simulation;simulated annealing;computer science;engineering;resource management;operating system;high-level synthesis;scheduling	EDA	0.18708058149135565	52.62999365161872	77107
bf73af783e944dc6e4dacf61a965652588b61f31	run-time reconfigurable systems for digital signal processing applications: a survey	digital signal processing;run time reconfiguration rtr;fpga;adaptive algorithm;run time reconfigurable;power consumption;hardware implementation;dsp	Today's digital signal processing (DSP) applications use computationally complex and/or adaptive algorithms and have stringent requirements in terms of speed, size, cost, power consumption, and throughput. Efficient hardware implementation techniques should be employed to meet the requirements of these applications. Run-Time Reconfiguration (RTR) is a promising technique for reducing the hardware required for implementing DSP systems as well as improving the performance, speed and power consumption of these systems. In this survey, we explain different issues in run-time reconfigurable systems and list the implemented systems which support run-time reconfiguration. We also describe different applications of run-time reconfiguration and discuss the improvements achieved by applying run-time reconfiguration.	digital signal processing	Alireza Shoa;Shahram Shirani	2005	VLSI Signal Processing	10.1007/s11265-005-4841-x	embedded system;computer vision;parallel computing;real-time computing;computer science;electrical engineering;digital signal processing	EDA	5.351438538024256	47.569305004469285	77172
810009918e690eddbf31c599abd0d95540f0a0b1	a system level hw/sw partitioning and optimization tool	circuit layout cad;integer programming;linear programming;logic cad;optimisation;signal processing;systems analysis;cad tool;area expense;heterogeneous multiprocessor systems;mathematical framework;mixed integer linear programming;numerical optimization;optimization tool;processor resources;signal processing scheme;system level hw/sw partitioning;throughput rate	This paper presents a system level HW/SW partitoning methodology and its implementation as CAD tool for the optimization of heterogeneous multiprocessor systems. Starting from modelling of the signal processing scheme and of the available processor resources, performance and expense measures are estimated for a nite set of processor modules. Based on these measurements, a numerical optimization can be carried out by using mixed integer linear programming as mathematical framework, leading to a heterogeneous system, which is optimal in terms of area expense and throughput rate.	computer-aided design;heterogeneous computing;integer programming;linear programming;mathematical optimization;multiprocessing;numerical analysis;shattered world;signal processing;throughput;transform, clipping, and lighting	Markus Schwiegershausen;Holger Kropp;Peter Pirsch	1996			embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;signal processing	EDA	1.1247335376191234	52.81164052253853	77284
86b0d3a662db376c4cb315e135764cd6b0ee06a4	reducing the mean latency of floating-point addition	distributed system;arithmetique ordinateur;systeme reparti;algorithm performance;integrated circuit;routing;utility function;circuito integrado;system performance;algorithme;algorithm;computer arithmetic;sistema repartido;resultado algoritmo;performance algorithme;addition;aritmetica ordenador;floating point;coma flotante;encaminamiento;data flow;variable latency;circuit integre;acheminement;virgule flottante;adiccion;algoritmo;dynamically scheduled processors	Addition is the most frequent floating-point operation in modem microprocessors. Due to its complex shift-add-shift-round data flow, floating-point addition can have a long latency. To achieve maximum system performance, it is necessary to design the floating-point adder to have minimum latency, while still providing maximum throughput. This paper proposes a new floating-point addition algorithm which exploits the ability of dynamically scheduled processors to utilize functional units which complete in variable time. By recognizing that certain operand combinations do not require all of the steps in the complex addition data flow, the mean latency is reduced. Simulation on SPECfp92 applications demonstrates that a speedup in mean addition latency of 1.33 can be achieved using this algorithm, while maintaining single-cycle throughput. @ 1998 Published by Elsevier Science B.V. All rights reserved	adder (electronics);algorithm;central processing unit;clock rate;dataflow;flops;instruction scheduling;interrupt latency;maximum throughput scheduling;microprocessor;modem;operand;scheduling (computing);simulation;speedup	Stuart F. Oberman;Michael J. Flynn	1998	Theor. Comput. Sci.	10.1016/S0304-3975(97)00201-6	embedded system;data flow diagram;routing;parallel computing;real-time computing;computer science;floating point;operating system;integrated circuit;computer performance;addition;algorithm	Arch	0.24158993438997017	57.851508356780904	77292
bcc542f3877dd4c8f38067b7a743187e97be2aa2	towards evolvable ip cores for fpgas	image filtering;field programmable gate array;image processing;intellectual property;logic design;reconfigurable architectures;evolvable hardware;genetics;programmable block evolvable ip core fpga adaptive hardware field programmable gate array intellectual property core core reuse autonomous evolution internal circuit hdl source code synthesizable core reconfigurable device virtual reconfigurable circuit genetic unit adaptive image filter design adaptive image filter implementation adaptive image filter synthesis business model definition evolvable hardware system reliability configuration memory;business model;digital filters;genetic algorithms;source code;logic design field programmable gate arrays genetic algorithms image processing digital filters reconfigurable architectures;field programmable gate arrays;field programmable gate arrays hardware application software design methodology circuit synthesis genetics libraries information technology paper technology adaptive arrays	The paper deals with a new approach to the design of adaptive hardware using common Field Programmable Gate Arrays (FPGA). The ultimate aim is to develop evolvable IP (Intellectual Property) cores. The cores should be reused in the same way as ordinary IP cores are reused. In contrast to the conventional cores, the evolvable cores are able to perform autonomous evolution of their internal circuits. The cores should be available in the form of HDL source code, i.e. they should be synthesizable into any reconfigurable device of a sufficient capacity. The approach is based on implementation of a virtual reconfigurable circuit and a genetic unit in an ordinary FPGA. In the presented case study an adaptive image filter is designed, implemented and synthesized. The proposed idea of evolvable IP core could open the way towards defining a business model for evolvable hardware.	autonomous robot;bitstream;composite image filter;computer vision;computer-aided design;continuous design;digital electronics;evolvable hardware;field-programmable gate array;hardware description language;ip address blocking;image compression;international standard book number;logic synthesis;real-time clock;real-time computing;semiconductor intellectual property core;software release life cycle	Lukás Sekanina	2003		10.1109/EH.2003.1217659	computer architecture;electronic engineering;real-time computing;computer science	EDA	8.995792799789443	48.81449392625189	77381
9ac428bf8236001626fd456504e1cd08cd034075	reverse engineering of microprocessor program code		This paper has an experimental character. Theoretical backgrounds presented here allow creating a research method. The research focus on analysis of microprocessor voltage supply changes. Such analysis based on the presented research assumptions allowed for a rather high efficiency of decoding program without interference in the internal structure of microprocessor. The obtained results show, that there is a possibility of uncontrolled access to program codes. Thus, it is necessary to search for and develop appropriate methods used for protecting program.	microprocessor;reverse engineering	Andrzej Kwiecien;Michal Mackowski;Krzysztof Skoroniak	2012		10.1007/978-3-642-31217-5_21	computer architecture	SE	9.668537140972802	51.07970937258923	77437
e732d4e470f9872c9734ba79948e83ad4ad428f4	worst case analysis for reducing algorithms on instruction systolic arrays with simple instruction sets	systolic array;worst case analysis	In this paper we investigate a technique to transform algorithms for Instruction Systolic Arrays (ISA's) to ones with very simple instruction sets. ISA's are a systolic mesh-connected architecture where besides data also instructions and binary selectors are shifted through the array. Many algorithms for different applications using complex instructions sets have been proposed for the ISA. To allow the combination and composition of algorithms on a single generic ISA they have to be reduced to ones with simple instructions. This paper shows that in the worst case on a m×n-Array a slowdown of factor m has to be accepted.	algorithm;best, worst and average case	Thomas Tensi	1988		10.1007/3-540-50647-0_128	systolic array;architecture;instruction set;case analysis;binary number;algorithm;computer science	Arch	0.8789032288792862	50.44979901093346	77454
5da50cc58fbe6d25e46fe31c44599bb2de064794	hardware synthesis for reconfigurable heterogeneous pipelined accelerators	re configurable computing;performance evaluation;image processing;reconfigurable architectures;hardware synthesis;information technology;design optimization;eda tool re configurable computing heterogeneous pipelined accelerators hardware synthesis;acceleration;high level synthesis;electronic design automation and methodology;eda tool;signal processing;reconfigurable architectures electronic design automation high level synthesis pipeline processing;signal processing hardware synthesis reconfigurable heterogeneous pipelined accelerator eda tool image processing;humans;signal synthesis;reconfigurable heterogeneous pipelined accelerator;configurable computing;hardware acceleration signal synthesis circuit synthesis signal processing performance evaluation humans electronic design automation and methodology information technology design optimization;heterogeneous pipelined accelerators;circuit synthesis;pipeline processing;hardware;electronic design automation	This paper discusses a method of hardware synthesis for re-configurable heterogeneous pipelined accelerators and corresponding EDA-tool that we developed. To evaluate the method and tool, we performed experiments using several representative image and signal processing cases. The experiments showed that our tool is able to automatically construct an optimized hardware that favorably compares to the hardware constructed by skilled human designers, but the tool does it several orders of magnitude faster than a human designer.	computer hardware;experiment;signal processing	Lech Józwiak;Alexander D Douglas	2008	Fifth International Conference on Information Technology: New Generations (itng 2008)	10.1109/ITNG.2008.65	acceleration;computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;electronic design automation;image processing;computer science;signal processing;high-level synthesis;information technology	EDA	2.732599202299474	50.48011317484113	77460
ac5789148b9c06b2a849404cdd929a607f1aa9b0	performance and power trade-offs for cryptographic applications in embedded processors	cryptographic algorithms;cryptography computational modeling clocks abstracts embedded systems iso standards program processors;core parallelism;clocks;iso standards;power consumption cryptographic applications cryptographic algorithms des aes rsa decryption operations encryption operations general purpose embedded processors core parallelism core frequency switching last level cache llc;embedded systems;power aware computing;computational modeling;voltage and frequency scaling cryptographic algorithms data encryption decryption embedded processors core parallelism;voltage and frequency scaling;abstracts;cryptography;embedded processors;multiprocessing systems;program processors;power aware computing cryptography embedded systems multiprocessing systems;data encryption decryption	Cryptographic operations are resource-intensive in terms of computational power and energy consumption. Typical approaches towards secure embedded systems employ dedicated modules, such as ASICs, co-processors, and accelerators, to implement these functions and optimize these hardware modules for the adopted algorithms. In our work, we analyze performance and power trade-offs of typical cryptographic algorithms (DES, AES, and RSA) when executed in processing elements that constitute typical embedded processors. Our goal is to characterize and optimize, performance-wise and power-wise, the sources of inefficiency when the encryption/decryption operations are executed in general purpose embedded processors with different processing and caching capabilities. Our analysis focuses on three major parameters: the parallelism of the core (issue width and size of execution window), voltage and frequency switching in the core, and size of the last-level cache (LLC). Those parameters constitute the major power-consumption contributors in all modern embedded general purpose processors. Our results demonstrate that cryptographic operations can be performed efficiently, in terms of both performance and power consumption, for specific values of the analyzed parameters, indicating that reconfigurable approaches can dynamically optimize processor organization and ameliorate the reported performance and power figures in the context of general purpose embedded processors.	algorithm;application-specific integrated circuit;cpu cache;central processing unit;compile time;compiler;computation;cryptographic hash function;embedded system;encryption;instruction-level parallelism;lunar lander challenge;parallel computing;requirement;smartphone	Chrysovalantis Datsios;Georgios Keramidas;Dimitrios N. Serpanos;P. Soufrilas	2013	IEEE International Symposium on Signal Processing and Information Technology	10.1109/ISSPIT.2013.6781860	computer architecture;parallel computing;real-time computing;computer science;cryptography;statistics	Arch	-3.9908499025501136	53.040047069213244	77496
50ed52f9ff6ffd94834e0c344c6c25f845d471d8	efficient procedure mapping using cache line coloring	instruction cache;call graph;optimization technique;cache memory;cache performance;hardware design;replacement policy	As the gap between memory and processor performance continues to widen, it becomes increasingly important to exploit cache memory eflectively. Both hardware and aoftware approaches can be explored to optimize cache performance. Hardware designers focus on cache organization issues, including replacement policy, associativity, line size and the resulting cache access time. Software writers use various optimization techniques, including software prefetching, data scheduling and code reordering. Our focus is on improving memory usage through code reordering compiler techniques.In this paper we present a link-time procedure mapping algorithm which can significantly improve the eflectiveness of the instruction cache. Our algorithm produces an improved program layout by performing a color mapping of procedures to cache lines, taking into consideration the procedure size, cache size, cache line size, and call graph. We use cache line coloring to guide the procedure mapping, indicating which cache lines to avoid when placing a procedure in the program layout. Our algorithm reduces on average the instruction cache miss rate by 40% over the original mapping and by 17% over the mapping algorithm of Pettis and Hansen [12].	access time;algorithm;cpu cache;cache (computing);call graph;color mapping;compiler;computational complexity theory;graph coloring;linker (computing);mathematical optimization;scheduling (computing)	Amir H. Hashemi;David R. Kaeli;Brad Calder	1997		10.1145/258915.258931	bus sniffing;call graph;least frequently used;pipeline burst cache;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;cache;computer science;write-once;theoretical computer science;cache invalidation;adaptive replacement cache;smart cache;programming language;mesi protocol;cache algorithms;cache pollution;mesif protocol;global assembly cache	PL	-3.2385330516042403	52.599142067066396	77540
a0cdde8b552ccee511a8e8dbe818e686d399c38c	high level design for wearables and iot	internet of things;high level synthesis;software prototyping;wearable computers;internet of things;iot;actuators;communications modules;complex software;compute modules;hardware components;high level design techniques;intelligence;model-based design methodology;next generation systems;rapid prototyping;recognition;refinement based approach;sensing;sensors;system design;ultra-low power envelope;user experience;wearables	Next generation systems in such domains as wearables and Internet of Things will have capabilities for sensing, recognition and intelligence. Such systems integrate multiple hardware components including sensors, actuators, compute and communications modules together with complex software in an ultra-low power envelope. Constraints from the target user experience must also be factored in. This talk will present a top down, model-based design methodology to quickly design such systems. It follows a refinement based approach and heavily leverages high level design techniques and rapid prototyping.	high-level programming language;internet of things;level design;rapid prototyping;refinement (computing);sensor;top-down and bottom-up design;user experience;wearable computer	Yatin Hoskote;Ilya Klotchkov	2014	2014 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)		embedded system;electronic engineering;simulation;design methods;computer science;engineering;operating system;reinforcement learning;internet of things;intelligent sensor;computer engineering	EDA	4.067234699918332	55.20718866239137	77778
aa0312b6bed57d8f09f3db2954c913fec256be20	automated bug detection for pointers and memory accesses in high-level synthesis compilers	silicon;hardware design languages;debugging;resource management;optimization;computer bugs;algorithm design and analysis	Modern High-Level Synthesis (HLS) compilers aggressively optimize memory architectures. Bugs involving memory accesses are hard to detect, especially if they are inserted in the compilation process. We present an approach to isolate automatically memory bugs introduced by HLS tools, without user interaction, using only the original high-level specification. This is possible by tracing memory accesses in software (SW) and hardware (HW) executions on a given input dataset. The execution traces are compared performing a context-aware HW/SW address translation, leveraging alias-analysis, HLS memory allocation information and SW memory debugging practices. No restrictions are imposed on memory optimizations. We show results on the relevance of the problem, the coverage, the detected bugs. We also show that the approach can be adapted to different commercial and academic HLS tools.	alias analysis;code coverage;compiler;computer hardware;debugging;high- and low-level;high-level synthesis;memory debugger;relevance;shattered world;software bug;tracing (software)	Pietro Fezzardi;Fabrizio Ferrandi	2016	2016 26th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2016.7577369	embedded system;algorithm design;memory safety;parallel computing;real-time computing;software bug;computer science;resource management;operating system;flat memory model;silicon;debugging;memory management	EDA	-4.321381443753659	49.9351836343049	77779
11923307372657daf02e5aa46ab6df599368d700	models for bit-true simulation and high-level synthesis of dsp applications	libraries;digital signal processing;quantization;kernel;digital signal processing chips application specific integrated circuits circuit layout cad;real time;simulation;bit true synthesis;high level synthesis;application specific integrated circuits;asics;bit true simulation;high level synthesis digital signal processing signal synthesis libraries signal processing algorithms application specific integrated circuits kernel real time systems timing quantization;circuit layout cad;digital signal processing chips;signal synthesis;cathedral ii compiler digital signal processing bit true simulation high level synthesis dsp applications bit true synthesis asics simulation;signal processing algorithms;dsp applications;cathedral ii compiler;real time systems;timing	Real-time DSP applications require a bit-true synthesis system to generate correct and efficient ASICs. This requires concise simulation and synthesis models, which are presented in this paper and ezemplijied for a non-restoring division operation. Such models are used in the synthesis library of our bit-true CATHCDBAL2ND compiler, by which industrial size applications have been synthesised.	application-specific integrated circuit;compiler;digital signal processor;division algorithm;high- and low-level;high-level synthesis;real-time transcription;simulation	Marc Pauwels;Dirk Lanneer;Francky Catthoor;Gert Goossens;Hugo De Man	1992		10.1109/GLSV.1992.218365	computer architecture;electronic engineering;parallel computing;kernel;quantization;computer science;digital signal processing;application-specific integrated circuit;high-level synthesis	EDA	5.387430624132171	47.439328530657974	77828
a9363bed88d03f045d21f896d2e0d43c9ca049c1	design of a low-power embedded processor architecture using asynchronous function units	processor architecture;real time;asynchronous circuit;embedded system;general purpose processor;low power;design environment;power dissipation;very long instruction word;cost effectiveness;special functions;transport triggered architecture;functional unit;embedded processor;low power consumption	Efficiency and flexibility are crucial features of processors in the embedded systems. The embedded processors need to be efficient in order to achieve real-time requirements with low power consumption for specific algorithms. And the flexibility allows design modifications in order to respond to different applications. As the superset of traditional very long instruction word (VLIW) architecture, Transport Triggered Architecture (TTA) offers a cost-effective trade-off between the size and performance of ASICs and the programmability of generalpurpose processors. The main advantages of TTA are its simplicity and flexibility. In TTA processors, the special function units can be utilized to increase performance or reduce power dissipation. In this paper, we design a low-power processor architecture using asynchronous function units based on TTA. The processor core is globally synchronous and locally asynchronous implementation using not only synchronous function units but also asynchronous function units. We solve the problem that use asynchronous circuits in TTA that is only synchronous design environment. The test result shows that this processor has lower power dissipation and higher performance than its pure synchronous version that only uses synchronous function units.	microarchitecture	Yong Li;Zhiying Wang;Xue-mi Zhao;Jian Ruan;Kui Dai	2007		10.1007/978-3-540-74309-5_33	computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;asynchronous circuit;microarchitecture;computer science;very long instruction word;dissipation;operating system;transport triggered architecture;special functions	EDA	1.6506552092653242	49.06170231694727	78040
10f227df02eb07fdd9c84fb34e2d1845b1c82571	parallélisme des nids de boucles pour l'optimisation du temps d'exécution et de la taille du code. (nested loop parallelism to optimize execution time and code size)		The real time implementation algorithms always incl ude nested loops which require important execution times. Thus, several nested loo p parallelism techniques have been proposed with the aim of decreasing their execution times. These techniques can be classified in terms of granularity, which are the iteration le vel parallelism and the instruction level parallelism. In the case of the instruction level p arallelism, the techniques aim to achieve a full parallelism. However, the loop carried depende ci s implies shifting instructions in both side of nested loops. Consequently, these technique s provide implementations with nonoptimal execution times and important code sizes, w hich represent limiting factors when implemented on embedded real-time systems. In this work, we are interested on enhancing the pa rallelism strategies of nested loops. The first contribution consists of purposing a nove l instruction level parallelism technique, called “delayed multidimensional retiming”. It aims to scheduling the nested loops with the minimal cycle period, without achieving a full para llelism. The second contribution consists of employing the “delayed multidimensional retiming ” when providing nested loop implementations on real time embedded systems. The aim is to respect an execution time constraint while using minimal code size. In this c ontext, we proposed a first approach that selects the minimal instruction parallelism level a llowing the execution time constraint respect. The second approach employs both instructi on level parallelism and iteration level parallelism, by using the “delayed multidimensional retiming” and the “loop striping”.	algorithm;bibliothèque des ecoles françaises d'athènes et de rome;control flow;data striping;embedded system;instruction-level parallelism;iteration;linear algebra;loop invariant;pa-risc;parallel computing;real-time clock;real-time computing;retiming;run time (program lifecycle phase);scheduling (computing)	Yaroub Elloumi	2013				EDA	-0.456065756333026	53.82456975307375	78054
060bd7ae0b9f7d9a659d7101735ffff944c933e1	dynamically parameterized algorithms and architectures to exploit signal variations	digital signal processing;computational mechanics;power aware;motion estimation;parameterized algorithm;signal processing;parameter space;vlsi;mpeg;power reduction	Signal processing algorithms and architectures can use dynamic reconfiguration to exploit variations in signal statistics with the objectives of improved performance and reduced power. Parameters provide a simple and formal way to characterize incremental changes to a computation and its computing mechanism. This paper develops a framework for dynamic parameterization and applies it to MPEG-4 motion estimation. A novel motion estimation architecture facilitates the dynamic variation of parameters to achieve power-compression tradeoffs. Our work shows that parameter variation in motion estimation helps achieve power reduction by an order of magnitude, trading off higher compression for lower power. The magnitude of the tradeoffs depends on the input signal variation. The monitoring of input and output signal statistics and subsequent variation of parameters is accomplished by a hardware controller. To provide the controller with a model of the parameter space and corresponding measures in terms of power and performance, a configuration sample space graph is developed. This graph identifies the parameters which	algorithm;computation;controller (computing);high-level programming language;ibm notes;input/output;motion estimation;signal processing	Prashant Jain;Andrew Laffely;Wayne P. Burleson;Russell Tessier;Dennis Goeckel	2004	VLSI Signal Processing	10.1023/B:VLSI.0000008068.26922.0b	embedded system;computer vision;parallel computing;real-time computing;computer science;computational mechanics;theoretical computer science;digital signal processing;machine learning;signal processing;motion estimation;very-large-scale integration;parameter space	Arch	2.6125073803951357	53.06328978468741	78148
eb243096f2128119d7c75d10998c6f25d3afb6f9	power analysis and optimization techniques for energy efficient computer systems	power analysis;selected works;energy efficient;optimization technique;interconnection network;monitoring system;operating system;software component;bepress;power consumption;system architecture;dynamic power management	Reducing power consumption has become a major challenge in the design and operation of today’s computer systems. This chapter describes different technique s addressing this challenge at different levels of system hardware, such as CPU, memory, a nd internal interconnection network, as well as at different levels of software components, such as compi ler, o erating system and user applications. These techniques can be broadly categorized into two types: Design time power analysis versus run-time dynamic power management. Mechanisms in the first category use analytical energy models that are integrated into existing simulators to measure the system’s power consumption and thus help engineers to test power-conscious hardware and software during design time. On the other hand, dynamic power management techniques are applie d during runtime, and are used to monitor system workload and adapt the system’s b havior dynamically to save energy.	categorization;central processing unit;component-based software engineering;computer;interconnection;power management;power optimization (eda);simulation	Wissam Chedid;Chansu Yu;Ben Lee	2005	Advances in Computers	10.1016/S0065-2458(04)63004-X	embedded system;power-flow study;real-time computing;power analysis;computer science;component-based software engineering;efficient energy use;power optimization;systems architecture;software system	EDA	-1.8217597683621376	55.88562850185234	78258
bfe980827ecf8a622cd19aff4adbde3d6179ed68	multi-core technology -- next evolution step in safety critical systems for industrial applications?	safety multicore processing hardware operating systems iec standards;redundant architectures;semiconductor technology;semiconductor technology embedded systems multiprocessing systems occupational safety production engineering computing;redundant architectures safety critical systems embedded systems functional safety multi core;embedded system;production engineering computing;embedded systems;multicore technology;iec standards;multicore based architectures;multicore processing;occupational safety;safety;safety critical system;industrial application;multiprocessing systems;industrial applications;multicore based architectures multicore technology safety critical systems industrial applications embedded systems;functional safety;multi core;safety critical systems;operating systems;hardware	Multi-core technology can provide valuable benefits for improving safety critical embedded systems. Examples range from multiple core architectures, introducing system redundancy, asymmetric multiprocessing allowing high software diversity, to hyper visors reducing system complexity. Can these benefits be taken for granted without considering the drawbacks and effects that come with them? The move to multi-core based architectures is already underway. Sooner, rather than later, we are forced to discover and resolve its issues for safety related applications. This paper is an attempt to evaluate the value of multi-core for safety critical systems on a broader level.	asymmetric multiprocessing;cost efficiency;embedded system;emoticon;failure analysis;hypervisor;multi-core processor;operating system;random-access memory;reduction (complexity);reliability engineering;requirement;scalability;single point of failure	Frank Reichenbach;Alexander Wold	2010	2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools	10.1109/DSD.2010.50	multi-core processor;embedded system;real-time computing;computer science;operating system;occupational safety and health	EDA	5.2577169046708185	56.34265072648292	78272
ea30b245110d802aaa684f7ad27d1f91eca830b8	analysis of cache tuner architectural layouts for multicore embedded systems	tuners;tuners multicore processing layout embedded systems energy consumption registers;configurable hardware cache tuning low power design cache memories multicore embedded systems;layout;optimization goals cache tuner architectural layout analysis multicore embedded systems memory hierarchy microprocessor power overall power consumption optimization power constrained embedded system area constrained embedded system cache tuning orchestration power overhead area overhead intercore data sharing processor communication synchronization cross core cache tuning coordination cache tuner overhead minimization multicore system scalability low overhead scalable cache tuner constrained multicore embedded systems;embedded systems;registers;energy consumption;multicore processing;power aware computing cache storage embedded systems memory architecture multiprocessing systems	Due to the memory hierarchy's large contribution to a microprocessor's total power, cache tuning is an ideal method for optimizing overall power consumption in embedded systems. Since most embedded systems are power and area constrained, the hardware and/or software that orchestrate cache tuning - the cache tuner - must not impose significant power and area overhead. Furthermore, as embedded systems increasingly trend towards multicore, inter-core data sharing, communication, and synchronization impose additional cache tuner design complexity, necessitating cross-core cache tuning coordination. In order to minimize cache tuner overhead, cache tuner design must consider these overheads and scalability. Whereas prior work proposes low-overhead cache tuners, scalability to multicore systems requires additional considerations. In this work, we present a low-overhead, scalable cache tuner and extensively evaluate various cache tuner design tradeoffs with respect to power and area for constrained multicore embedded systems. Based on our analysis, we formulate valuable insights and designer-assisted guidelines for modeling scalable and efficient cache tuners that best achieve optimization goals while maintaining power and area constraints.	cpu cache;cache (computing);cache-oblivious algorithm;core data;embedded system;mathematical optimization;memory hierarchy;microprocessor;multi-core processor;overhead (computing);resource contention;scalability;tv tuner card;telecommunications network	Tosiron Adegbija;Ann Gordon-Ross;Marisha Rawlins	2014	2014 IEEE 33rd International Performance Computing and Communications Conference (IPCCC)	10.1109/PCCC.2014.7017091	bus sniffing;multi-core processor;layout;embedded system;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cache;computer science;write-once;cache invalidation;operating system;smart cache;processor register;cache algorithms;cache pollution;mesif protocol	EDA	-1.1857839737496925	55.02062857822201	78306
7180069feb37eff24a0a493aeb64ce7a04c2339a	instruction-based energy estimation methodology for asymmetric manycore processor simulations	cycle-level microarchitectural simulation approach;dynamic energy;scalable energy simulation;average simulation time speedup;main simulation speed bottleneck;instruction-based energy estimation methodology;detailed microarchitecture model;asymmetric manycore processor simulation;instruction-based energy estimation model;detailed core model;proposed instruction-based energy model;manycore simulation	Processor power is a complex function of device, packaging, microarchitecture, and application. Typical approaches to power simulation require detailed microarchitecture models to collect the statistical switching activity counts of processor components. In manycore simulations, the detailed core models are the main simulation speed bottleneck. In this paper, we propose an instruction-based energy estimation model for fast and scalable energy simulation. Importantly, in this approach the dynamic energy is modeled as a combination of three contributing factors: physical, microarchitectural, and workload properties. The model easily incorporates variations in physical parameters such as clock frequencies and supply voltages. When compared to commonly used cycle-level microarchitectural simulation approach with SPEC2006 benchmarks, the proposed instruction-based energy model incurred a 2.94% average error rate while achieving an average simulation time speedup of 74X for a 16-core asymmetric x86 ISA processor model with multiple clock domains operating at different frequencies.	clock rate;manycore processor;microarchitecture;scalability;simulation;speedup;x86	William J. Song;Sudhakar Yalamanchili;Arun Rodrigues;Saibal Mukhopadhyay	2012			computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-2.991839270420252	55.63172992460341	78424
64896a53ce64c3e392fd8fdf01a3f3c50dbfeb5b	octavo: an fpga-centric processor family	processor architecture;microarchitecture;fpga;soft processor;multithreading	Overlay processor architectures allow FPGAs to be programmed by non-experts using software, but prior designs have mainly been based on the architecture of their ASIC predecessors. In this paper we develop a new processor architecture that from the beginning accounts for and exploits the predefined widths, depths, maximum operating frequencies, and other discretizations and limits of the underlying FPGA components. The result is Octavo, a ten-pipeline-stage eight-threaded processor that operates at the block RAM maximum of 550MHz on a Stratix IV FPGA. Octavo is highly parameterized, allowing us to explore trade-offs in datapath and memory width, memory depth, and number of supported thread contexts.	application-specific integrated circuit;datapath;discretization;field-programmable gate array;random-access memory;stratix	Charles Eric LaForest;J. Gregory Steffan	2012		10.1145/2145694.2145731	embedded system;computer architecture;parallel computing;real-time computing;application-specific instruction-set processor;microarchitecture;computer science;operating system	Arch	0.16314778022952645	49.33376792935496	78431
1ae012765bdc2d134bd3e471b666ec615e3ec279	on-line routing of reconfigurable functions for future self-adaptive systems - investigations within the æther project	cad tools;online routing;reconfigurable architectures;aether project;design space;embedded system;aether project online routing reconfigurable functions self adaptive systems embedded systems cad tools;emerging technology;routing hardware space technology consumer products costs embedded system design automation power system management system testing computer networks;embedded systems;reconfigurable architectures embedded systems field programmable gate arrays logic cad;low power;adaptive system;self adaptive systems;field programmable gate arrays;logic cad;reconfigurable functions	The progress in hardware technologies for implementing portable, low power and low cost electronic systems for consumer products has been major the last years. The complexity of embedded systems will further increase at a rate which is not met by the development of advanced CAD tools for managing the large design space. This will likely lead to increased design problems regarding system implementation, test and verification. In the next 15-20 years, it is likely that the consumer products are based on computing devices which are grouped together in networks including thousands or even millions of nodes. The /ETHER project deals with managing the complexity of such systems based on emerging technologies for future applications. This paper presents how the design complexity can be managed at the hardware level by integrating self-adaptive characteristics, and how the trade-off in performance and flexibility can be optimized to fulfill all application requirements while reducing the design complexity.	computer-aided design;embedded system;requirement;routing	Katarina Paulsson;Michael Hübner;Jürgen Becker;Jean-Marc Philippe;Christian Gamrat	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380682	embedded system;real-time computing;computer science;adaptive system;operating system;emerging technologies;field-programmable gate array	EDA	9.108465650711311	55.44819115264241	78596
c86d9376f61ca7e4e589ac6f54c7681cfda53dca	microarchitecture description techniques	horizontal processor;microprogram compiler;microarchitecture description;hardware element;behavioral rule;large extent;instruction set interpretation mechanism;real processor;microarchitecture description technique;description procedure;microarchitecture description methodology	A procedure is outlined for describing the microarchitecture of a horizontal processor such that a retargetable Microprogram Compiler System can incorporate the description to generate microcode for that processor. The microarchitecture description methodology is an organized approach to defining a machine's microinstruction formats, fields, and microorders; its hardware elements; its microoperation usage rules; and its behavioral rules. To a large extent, the description procedure can be performed interactively. The link between the microarchitecture description and the microprogram compiler, termed the instruction set interpretation mechanism, is also described. Preliminary application of the microarchitecture description methodology to several real processors has shown that, despite some problems, the procedure shows promise for significantly reducing the time required to retarget a microprogram compiler.	central processing unit;compiler;interactivity;microarchitecture;microcode	John L. Gieser;Robert J. Sheraga	1982			computer architecture;parallel computing;computer science;operating system;programming language	Arch	1.7577048252221545	49.53669807381388	78834
ec5eb8a983f9ac6ee5330e897fc0db22a5731bd1	performance-driven event-based synchronization for multi-fpga simulation accelerator with event time-multiplexing bus	field programmable gate array;functional verification;multi fpga system;interconnect;circuit optimisation field programmable gate arrays system on chip integrated circuit interconnections circuit simulation synchronisation timing formal verification;primary nets multi fpga simulation accelerator event time multiplexing bus functional verification system on chip soc multi field programmable gate array emulator interconnection wires multiplexing pin limitation problem gate utilization synchronization time software simulator multi fpga system signal synchronization mechanism circuit partitioning signal probability event probability;time multiplexing event based synchronization fpga interconnect multi fpga system simulation acceleration synchronization;fpga;time multiplexing;discrete event simulation field programmable gate arrays integrated circuit interconnections wires circuit simulation pins acceleration system on a chip programmable logic arrays integrated circuit technology;synchronisation;simulation acceleration;circuit simulation;formal verification;system on chip;synchronization;integrated circuit interconnections;field programmable gate arrays;circuit optimisation;system simulation;event based synchronization;timing	Simulation is the most viable solution for the functional verification of system-on-chip (SoC). The acceleration of simulation with multi-field programmable gate array (multi-FPGA) emulator is a promising method to comply with the increasing complexity and large gate capacity of SoC. Time multiplexing of interconnection wires is the inevitable solution to solve the pin limitation problem that limits the gate utilization of FPGAs and speed of multi-FPGA simulation accelerators. The most time-consuming factor of multi-FPGA simulation acceleration is the synchronization time between a software simulator and a multi-FPGA system and the inter-FPGA synchronization time. This paper proposes a performance-driven signal synchronization mechanism for a simulation accelerator with multiple FPGAs using time-multiplexed interconnection. The event-based signal synchronization optimizes the synchronization time between a software simulator and the multi-FPGA system as well as the synchronization time among FPGAs. The synchronization time among FPGAs is optimized by circuit partitioning considering the signal probability, net dependency reduction, and efficient net clustering to reduce addressing overhead. The synchronization time between the software simulator and the multi-FPGA system is also optimized by exploiting the event probability of primary nets. Experiments show that the synchronization time is reduced to 6.2-9.8% of traditional mechanisms.	cluster analysis;emulator;field-programmable gate array;interconnection;multiplexing;overhead (computing);simulation;system on a chip	Young-Su Kwon;Chong-Min Kyung	2005	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2005.852035	embedded system;synchronization;electronic engineering;real-time computing;computer science;self-clocking signal;field-programmable gate array	EDA	2.3572044205762728	59.56055229082888	79020
d8aeb62fa28513d290d1ac6c00051ecf25089987	testability measures: a review	testability measure			Chantal Robach;S. Guibert	1988	Comput. Syst. Sci. Eng.		testability;database;computer science;computer architecture	DB	9.161078193994937	53.40265565594444	79484
7ac1dcdcf69e2863ff857b45cc6140d4ca75cfab	hardware/software co-design of an fpga-based embedded tracking system	performance measure;image motion analysis;tracking system;hardware software co design;real time;embedded system;fabrics;area measurement;optical flow;power consumption;field programmable gate arrays;hardware embedded software field programmable gate arrays image motion analysis optical design fabrics design methodology embedded system optical devices area measurement;optical design;software implementation;embedded software;optical devices;hardware;design methodology	This paper discusses a practical design experience pertaining to a tracking system employing optical flow. The system was previously extracted from an existing software implementation and modified for FPGA deployment. Details are provided regarding transference of the resulting high-level design to a usable form for FPGA fabrics. Furthermore, discussion is given for obstacles made manifest in embedded vision design and the methods employed for overcoming them. This is attempted with the intent of maintaining a consistent level of vision algorithm performance as well as meeting real-time requirements. The system discussed differs from previous embedded systems employing optical flow in that it consists strictly of fully disclosed nonproprietary transferable components while providing performance measures for power consumption, latency, and area. The system was synthesized onto a Xilinx Virtex-II Pro XC2VP30 FPGA utilizing less than 25% of system resources, performing with a maximum operating frequency of 67MHz without pipelining, and consuming 497mW of power.	algorithm;clock rate;computer vision;connected component (graph theory);data dependency;embedded system;field-programmable gate array;high- and low-level;level design;optical flow;parallel computing;pipeline (computing);real-time clock;real-time computing;requirement;software deployment;tracking system;virtex (fpga)	Jason Schlessman;Cheng-Yao Chen;Wayne H. Wolf;I. Burak Özer;Kenji Fujino;Kazurou Itoh	2006	2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW'06)	10.1109/CVPRW.2006.92	embedded system;real-time computing;embedded software;design methods;tracking system;computer hardware;computer science;optical flow;field-programmable gate array	Robotics	2.978329853965604	52.105995980445435	79697
be34386a9f865c5e87a63c4fd046a211ad915e8c	algorithm partitioning including optimized data-reuse for processor arrays	different partitioning scheme;design flow;former design flow;parameterized processor array;fir filter algorithm;processor arrays;algorithm partitioning;partitioning methodology;spacetime transformation;optimized data-reuse;information technology;hardware;parallel processing;computer architecture;indexation;finite impulse response filter;algorithm design and analysis;fir filter;fir filters	This paper describes a method for algorithm partitioning through which affine indexed algorithms are transformed to Processor Arrays. Former design flows start with a spacetime transformation which we omit completely. Therefore, we are able to consider the constraints of a target architecture at the beginning of our design flow. We show our method for three different partitioning schemes and emphasize on the derivation of a schedule. The principle of an optimized data-reuse is introduced for our partitioning methodology. Under this aspect, we give a parameterized Processor Array for the 2D FIR filter algorithm.	design flow (eda);digital signal processor;finite impulse response;peterson's algorithm;processor array;schedule (computer science);scheduling (computing);tiling window manager	Sebastian Siegel;Renate Merker	2004	Parallel Computing in Electrical Engineering, 2004. International Conference on	10.1109/PCEE.2004.10	parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;finite impulse response;information technology	EDA	0.3535266069278609	51.56393702442593	79924
d3e203b7b86e2fb1b05fe0a3511623564f6fdb4e	verilog and other standards			verilog		2002	IEEE Design & Test of Computers			EDA	6.24304422869178	50.118949169632955	80035
89d67bb0b6aedd236e1b37334861fdbd413d015c	csl: configurable fault tolerant serial links for inter-die communication in 3d systems	serialization based repair;fault tolerance;thru silicon via;interconnect yield	Three dimensional (3D) integrated systems become a reality nowadays, as Thru-Silicon-Via (TSV) technologies mature. 3D integration promises significant performance and energy efficiency improvements by reducing the signal travel distances and integrating more capabilities on a single chip. High integration costs, thermal management, and poor reliability and yield are major challenges of TSV based 3D chips. High structural and parametric fault rates due to manufacturing defects makes it difficult to achieve high interconnect yield using only spare-based repair solutions. In this paper we address the TSV yield issue by implementing the inter-die links of 3D chips as Configurable fault-tolerant Serial Links (CSLs). When there are not enough available functional TSVs, faults are tolerated by performing data serialization. CSLs help reduce chip costs by improving the TSV yield with very few or no spares at all. For 3D Networks-on-Chip (3D NoCs) we show that the CSL yield improvement comes with moderate area overheads (~12---26%) and small performance penalties (less than 5% average latency overhead).	fault-tolerant computer system	Vladimir Pasca;Lorena Anghel;Michael Nicolaidis;Mounir Benabdenbi	2012	J. Electronic Testing	10.1007/s10836-011-5260-5	reliability engineering;embedded system;fault tolerance;parallel computing;real-time computing;computer science;engineering	Theory	6.233588670214166	59.71349209614076	80186
42c92f2cdfa8992fa2f56d4aa68989075bb1002b	the effect of number of virtual channels on noc edp	performance;noc network on chip;energy model;power	Low scalability and power efficiency of the shared bus in SoCs is a motivation to use on chip networks instead of traditional buses. In this paper we have modified the Orion power model to reach an analytical model to estimate the average message energy in K-Ary n-Cubes with focus on the number of virtual channels. Afterward by using the power model and also the performance model proposed in [11] the effect of number of virtual channels on Energy-Delay product have been analyzed. In addition a cycle accurate power and performance simulator have been implemented in VHDL to verify the results.		Mahdieh Nadi Senejani;Mahdiar Hosein Ghadiry	2008			embedded system;parallel computing;real-time computing;engineering	PL	1.235899849852416	59.014417447819646	80390
f655702a8ae685cfa0f2a0d3283be4fb9ff2dca0	multiprocessor system-on-chip - hardware design and tool integration		System-on-chip (SOC) design gets increasingly complex, as a growing number of applications are integrated in modern systems. Some of these applications have real-time requirements, such as a minimum throughput or a maximum latency. To reduce cost, system resources are shared between applications, making their timing behavior inter-dependent. Real-time requirements must hence be verified for all possible combinations of concurrently executing applications, which is not feasible with commonly used simulation-based techniques. This chapter addresses this problem using two complexity-reducing concepts: composability and predictability. Applications in a composable system are completely isolated and cannot affect each other’s behaviors, enabling them to be independently verified. Predictable systems, on the other hand, provide lower bounds on performance, allowing applications to be verified using formal performance analysis. Five techniques to achieve composability and/or predictability in SOC resources are presented and we explain their implementation for processors, interconnect, and memories in our platform.	central processing unit;composability;computer performance;mpsoc;multiprocessing;processor design;real-time computing;requirement;simulation;system on a chip;throughput	Michael Hübner;Jürgen Becker	2011		10.1007/978-1-4419-6460-1		Embedded	-0.4429830505334085	55.88610302933309	80675
9198597d5a6516ca7314808f74e071dd68d05580	automatic hardware synthesis from specifications: a case study	formal language;protocols;psl;buffer;high level synthesis;hardware description language;formal specification;hardware description languages;automata;polynomials;formal languages;formal specifications	We propose to use a formal specification language as a high-level hardware description language. Formal languages allow for compact, unambiguous representations and yield designs that are correct by construction. The idea of automatic synthesis from specifications is old, but used to be completely impractical. Recently, great strides towards efficient synthesis from specifications have been made. In this paper we extend these recent methods to generate compact circuits and we show their practicality by synthesizing an arbiter for ARM's AMBA AHB bus and a generalized-buffer from specifications given in PSL. These are the first industrial examples that have been synthesized automatically from their specifications.	advanced microcontroller bus architecture;arbiter (electronics);formal specification;hardware description language;high- and low-level;specification language	Roderick Bloem;Stefan J. Galler;Barbara Jobstmann;Nir Piterman;Amir Pnueli;Martin Weiglhofer	2007	2007 Design, Automation & Test in Europe Conference & Exhibition	10.1145/1266366.1266622	computer architecture;formal language;computer science;theoretical computer science;hardware description language;programming language	EDA	9.828136702998151	50.43124334519704	80744
00c60b032d7c76bcfc83b94afa8260e29298695c	synthesis and implementation of ram-based finite state machines in fpgas	configurable logic block;field programmable gate array;architecture systeme;memoria acceso directo;maquina estado finito;red puerta programable;reseau porte programmable;codificacion;design and implementation;memoire acces direct;coding;random access memory ram;arquitectura sistema;combinational circuit;system architecture;machine etat fini;finite state machine;codage	This paper discusses the design and implementation of finite state machines (FSM) with combinational circuits that are built primarily from RAM blocks. It suggests a novel state assignment technique, based on fuzzy codes, that is combined with the replacement (encoding) of the FSM input vectors. It also shows how FSMs with dynamically modifiable functionality can be constructed and then implemented in commercially available FPGAs. The results of experiments have shown that FSMs with the proposed architecture can be implemented using less hardware resources, such as the number of FPGA configurable logic blocks (CLB), while at the same time extending their functional capabilities.	code;combinational logic;experiment;field-programmable gate array;finite-state machine;random-access memory	Valery Sklyarov	2000		10.1007/3-540-44614-1_76	embedded system;parallel computing;computer science;coding;combinational logic;finite-state machine;field-programmable gate array;systems architecture	EDA	8.599474415895424	48.05818550817863	80838
1108af609469e420aeae551ba8a893c3200e07fa	prediction models for multi-dimensional power-performance optimization on many cores	software tool;power saving;concurrent computing;concurrency throttling;high performance computing;perforation;system dynamics;runtime;heuristic search;multi dimensional;power aware computing;performance improvement;thesis;multicore processing;runtime adaptation;high performance computer;mathematical model;performance prediction;predictive models;multicore processors;prediction model;power consumption;parallel programs;adaptation models;dynamic concurrency throttling;performance optimization;energy saving;constrained system;hardware	Power has become a primary concern for HPC systems. Dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT) are two software tools (or knobs) for reducing the dynamic power consumption of HPC systems. To date, few works have considered the synergistic integration of DVFS and DCT in performance-constrained systems, and, to the best of our knowledge, no prior research has developed application-aware simultaneous DVFS and DCT controllers in real systems and parallel programming frameworks. We present a multi-dimensional, online performance predictor, which we deploy to address the problem of simultaneous runtime optimization of DVFS and DCT on multi-core systems. We present results from an implementation of the predictor in a runtime library linked to the Intel OpenMP environment and running on an actual dual-processor quad-core system. We show that our predictor derives near-optimal settings of the power-aware program adaptation knobs that we consider. Our overall framework achieves significant reductions in energy (19% mean) and ED2 (40% mean), through simultaneous power savings (6% mean) and performance improvements (14% mean). We also find that our framework outperforms earlier solutions that adapt only DVFS or DCT, as well as one that sequentially applies DCT then DVFS. Further, our results indicate that prediction-based schemes for runtime adaptation compare favorably and typically improve upon heuristic search-based approaches in both performance and energy savings.	concurrency (computer science);discrete cosine transform;dynamic frequency scaling;dynamic voltage scaling;heuristic;kerrison predictor;mathematical optimization;multi-core processor;openmp;parallel computing;runtime library;synergy	Matthew Curtis-Maury;Ankur Shah;Filip Blagojevic;Dimitrios S. Nikolopoulos;Bronis R. de Supinski;Martin Schulz	2008	2008 International Conference on Parallel Architectures and Compilation Techniques (PACT)	10.1145/1454115.1454151	multi-core processor;computer architecture;parallel computing;real-time computing;concurrent computing;computer science;operating system;predictive modelling	HPC	-3.587978897527873	53.81686396312197	80868
b6f4965021f3c621321e2760d691f2fcace9f358	3dfar: a three-dimensional fabric for reliable multi-core processors		In the past decade, silicon technology trends into the nanometer regime have led to significantly higher transistor failure rates. Moreover, these trends are expected to exacerbate with future devices. To enhance reliability, several approaches leverage the inherent core-level and processor-level redundancy present in large chip multiprocessors. However, all of these methods incur high overheads, making them impractical. In this paper, we propose 3DFAR, a novel architecture leveraging 3-dimensional fabrics layouts to efficiently enhance reliability in the presence of faults. Our key idea is based on a finegrained reconfigurable pipeline for multicore processors, which minimizes routing delay among spare units of the same type by using physical layout locality and efficient interconnect switches, distributed over multiple vertical layers. Our evaluation shows that 3DFAR outperforms state-of-the-art reliable 2D solutions, at a minimal area cost of only 7% over an unprotected design.	central processing unit;integrated circuit layout;locality of reference;multi-core processor;network switch;principle of locality;propagation delay;routing;semiconductor research corporation;software propagation;transistor	Javad Bagherzadeh;Valeria Bertacco	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		multi-core processor;embedded system;electronic engineering;parallel computing;real-time computing;computer science;engineering;operating system;reliability;pipeline transport;silicon;statistics	EDA	6.305383806525272	59.86349705090281	81119
d1be29a5541c33e1e5c91c4d8bda7127d8c6515a	mmv: a metamodeling based microprocessor validation environment	modelizacion;validation collaterals architectural description language metamodel metamodeling microprocessor model driven design and validation refinement;software;microprocessors;microprocessor;systematic validation approach;architecture systeme;procesador risc;methode essai;architectural description language;integrated circuit;logiciel;registro rtl;architecture description language;multiple microprocessing core;automatic test pattern generation;computer processor system;circuito integrado;testing;instruction set architecture;refinement;metamodeling microprocessors face hardware refining production registers design methodology computational modeling testing;generacion automatica prueba;simulator;procesador 32 bits;chip;modelisation;jeu instruction;computational modeling;lenguaje descripcion;metamodel;simulador;automatic test generation multiple microprocessing core systematic validation approach microprocessor modeling and validation environment computer processor system;model driven design and validation;registers;microprocessor modeling and validation environment;levels of abstraction;processeur 32 bits;software component;risc processor;niveau transfert registre;simulateur;generation automatique test;automatic test generation;production;test generation;logicial;arquitectura sistema;face;microprocesseur;refining;test method;processeur risc;system architecture;metamodeling;modeling;register transfer level;microprocesador;validation collaterals;langage description;circuit integre;microprocessor chips;microprocessor chips automatic test pattern generation;instruction sets;32 bit processor;hardware;design methodology;description language;metodo ensayo	With increasing levels of integration of multiple processing cores and new features to support software functionality, recent generations of microprocessors face difficult validation challenges. The systematic validation approach starts with defining the correct behaviors of the hardware and software components and their interactions. This requires new modeling paradigms that support multiple levels of abstraction. Mutual consistency of models at adjacent levels of abstraction is crucial for manual refinement of models from the full chip level to production register transfer level, which is likely to remain the dominant design methodology of complex microprocessors in the near future. In this paper, we present microprocessor modeling and validation environment (MMV), a validation environment based on metamodeling, that can be used to create models at various abstraction levels and to generate most of the important validation collaterals, viz., simulators, checkers, coverage, and test generation tools. We illustrate the functionalities in MMV by modeling a 32-bit reduced instruction set computer processor at the system, instruction set architecture, and microarchitecture levels. We show by examples how consistency across levels is enforced during modeling and also how to generate constraints for automatic test generation.	32-bit;central processing unit;component-based software engineering;computer simulation;emulator;interaction;metamodeling;microarchitecture;microprocessor;multiuse model view;principle of abstraction;refinement (computing);register-transfer level;requirements analysis;viz: the computer game	Deepak Mathaikutty;Sreekumar V. Kodakara;Ajit Dingankar;Sandeep K. Shukla;David. J. Lilja	2008	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2008.917419	metamodeling;embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;computer science;operating system;instruction set;programming language;algorithm;systems architecture	EDA	7.250777674824069	52.76836573578546	81328
da7a95382f8ba2dd62f31b2bdde27c1b557ed6e0	optimized synthesis of dedicated controllers with concurrent checking capabilities	signature analysis;control systems;detection of faults;logic arrays;optimisation;programmable controllers;justifying signature;state assignment;automatic testing;external checker optimised synthesis ate logic testing dedicated controllers concurrent checking detection of faults state code flow polynomial division justifying signature control flow graph state assignment signature analysis built in monitor;automatic test equipment;control flow graph;control system cad;flow graphs;external checker;ate;concurrent checking;built in self test;polynomial division;dedicated controllers;monitoring;fault tolerant systems;error correction;flow graphs hardware flowcharts circuit testing fault detection error correction monitoring control systems fault tolerant systems built in self test;fault detection;logic testing;built in monitor;reference data;state assignment automatic test equipment automatic testing control system cad fault location logic arrays logic cad logic testing optimisation programmable controllers;state code flow;circuit testing;optimised synthesis;logic cad;flowcharts;hardware implementation;path following;software implementation;hardware;fault location	"""Dedicated controllers (or FSMs) with concurrent checking capabilities are of prime importance in highly dependable applications. This paper presents a new synthesis method of dedicated controllers which aims at the detection of faults which cause errors in the state sequences. The state code flow is compacted through polynomial division. An implicit """"justifying signature"""" method is applied at the state code level and ensures identical signatures before each join node of the control flow graph. The signatures are then independent of the path followed previously in the graph and the comparison with reference data is greatly facilitated. This property is obtained by a clever state assignment, nearly without area overhead. The controllers can therefore be checked by signature analysis, either by a built-in monitor or by an external checker."""	antivirus software;canonical account;control flow graph;overhead (computing);polynomial long division;type signature	Régis Leveugle;Gabriele Saucier	1989		10.1109/TEST.1989.82319	reliability engineering;embedded system;automatic test equipment;electronic engineering;real-time computing;error detection and correction;flowchart;reference data;computer science;control system;theoretical computer science;operating system;programmable logic controller;polynomial long division;fault detection and isolation;algorithm;control flow graph	EDA	7.979602737194436	58.658324136046	81439
30c4cb3cd6feb4d459b447cb7f3b13cc78563493	digital system design and microprocessors: john p hayes mcgraw-hill, new york, usa (1984) £30.50 pp786	digital systems	Generally this is an excellent text and provides a wealth of material which is very well structured. The book should prove to be ideal for undergraduates and postgraduates on various courses involved in the design and/or application of digital technology. Similarly the book will also be valuable reference material to practising designers of digital systems and for engineering managers wishing to become familiar with emerging technologies. The book achieves admirablythe majority of the objectives stated by the author and is well presented and easy to read. A wide range of design problems is presented for the reader to solve. The book provides an interesting summary of developments in integrated circuit technology presenting from first principles the methods used in fabricating TTL and MOS families, giving useful information of a practical nature such as a simple but very effective description of noise suppression. The text provides a fairly comprehensive treatment of the design of both combinational and sequential logic. It presents a range of design problems and solutions which highlight the major fundamental elements of circuit design. Similarly a very good treatment of MSI and LSI circuits is given with design problems and solutions in the use of encoders, decoders, arithmetic circuits, counters, memories and character generation. An excellent and particularly well structured treatment of computer architecture is presented. This covers a number of popular processor families based on Motorola, Intel and Zilog processors. This is very much the centre point of the book and makes interesting and enlightening reading. A rather superficial but very useful treatment of computer programming is included in which PASCAL and assembler programming (for selected processors) is presented. This draws out some of the relative merits of high-level and low-level languages by giving examples of problem-solving software. The text provides interesting examples of interface hardware for a variety of peripherals and outl ines the fundamental approaches used when interfacing to computers. However, this section could have been expanded, particularly with respect to defining and illustrating the use of internationally accepted I/O standards. This could have centred on common peripherals such as VDUs and outl ined methods used in achieving both 'closely coupled' and 'loosely coupled' interprocessor communications. The book concludes by looking at advanced computer architectures and considers further developments. This is probably the weakest chapter in the book and could have indicated ways in which the industrial world and consumers should be uti l izing increased processing power as this becomes available. However, the rapid developments in digital technology are clearly diff icult to cater for and overall the book is an excellent text which should find widespread use over the next few years.	assembly language;binary decoder;central processing unit;circuit design;combinational logic;computer architecture;computer monitor;computer programming;diff utility;digital electronics;encoder;hayes microcomputer products;high- and low-level;input/output;integrated circuit;loose coupling;microprocessor;pascal;peripheral;problem solving;sequential logic;systems design;the superficial;transistor–transistor logic;zero suppression	Richard H. Weston	1985	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(85)90214-5	computer science	EDA	9.514726000125902	51.60408976486445	81466
0568c568401a88fc03ff93bc1deb483b7e5294bb	register allocation for software pipelined loops	instruction level parallel;vliw processor;modulo scheduling;multiplication operator;register allocation;code generation;very long instruction word;superscalar processor;software pipelining;instruction scheduling	Software pipelining is an important instruction scheduling technique for efficiently overlapping successive iterations of loops and executing them in parallel. This paper studies the task of register allocation for software pipelined loops, both with and without hardware features that are specifically aimed at supporting software pipelines. Register allocation for software pipelines presents certain novel problems leading to unconventional solutions, especially in the presence of hardware support. This paper formulates these novel problems and presents a number of alternative solution strategies. These alternatives are comprehensively tested against over one thousand loops to determine the best register allocation strategy, both with and without the hardware support for software pipelining.	algorithm;cpu cache;christian ristow;compiler;computational complexity theory;curve fitting;heuristic (computer science);instruction scheduling;iteration;memory management;modulo operation;pipeline (computing);pipeline (software);preconditioner;processor register;register allocation;register file;register window;scheduling (computing);software pipelining;warren abstract machine	B. Ramakrishna Rau;Meng Lee;Parthasarathy P. Tirumalai;Mike Schlansker	1992		10.1145/143095.143141	software pipelining;multiplication operator;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;instruction register;instruction scheduling;programming language;register allocation;code generation	PL	-1.6191043077682	51.50845748637782	81620
613d57643da1286902a23ed72cce626eebe84275	understanding the role of gpgpu-accelerated soc-based arm clusters		The last few years saw the emergence of 64-bit ARM SoCs targeted for mobile systems and servers. Mobile-class SoCs rely on the heterogeneous integration of a mix of CPU cores, GPGPU cores, and accelerators, whereas server-class SoCs instead rely on integrating a larger number of CPU cores with no GPGPU support and a number of network accelerators. Previous works, such as the Mont-Blanc project, built their prototype ARM cluster out of mobile-class SoCs and compared their work against x86 solutions. These works mainly focused on the CPU performance. In this paper, we propose a novel ARM-based cluster organization that exploits faster network connectivity and GPGPU acceleration to improve the performance and energy efficiency of the cluster. Our custom cluster, based on Nvidia Jetson TX1 boards, is equipped with 10Gb network interface cards and enables us to study the characteristics, scalability challenges, and programming models of GPGPU-accelerated workloads. We also develop an extension to the Roofline model to establish a visually intuitive performance model for the proposed cluster organization. We compare the GPGPU performance of our cluster with discrete GPGPUs. We demonstrate that our cluster improves both the performance and energy efficiency of workloads that scale well and can leverage the better CPU-GPGPU balance of our cluster. We contrast the CPU performance of our cluster with ARM-based servers that use many CPU cores. Our results show the poor performance of the branch predictor and L2 cache are the bottleneck of server-class ARM SoCs. Furthermore, we elucidate the impact of using 10Gb connectivity with mobile systems instead of traditional, 1Gb connectivity.	64-bit computing;arm architecture;artificial neural network;benchmark (computing);branch predictor;cpu cache;cuda;central processing unit;computer vision;deep learning;emergence;general-purpose computing on graphics processing units;gigabyte;kerrison predictor;linpack benchmarks;lunpack;manycore processor;network interface controller;prototype;scalability;server (computing);system on a chip;tegra;x86	Reza Azimi;Tyler Fox;Sherief Reda	2017	2017 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2017.86	scalability;real-time computing;parallel computing;cpu cache;computer science;multi-core processor;general-purpose computing on graphics processing units;network interface;central processing unit;server;branch predictor	HPC	-4.146636644282177	47.191669350972475	82031
ba87b30835184745f19f2729af26dd4a048d2ed3	routine based os-aware microprocessor resource adaptation for run-time operating system power saving	logic simulation;power saving;logic simulation adaptive systems microcomputers resource allocation operating systems computers low power electronics;resource allocation;microprocessors operating systems modems permission memory management thermal management parallel processing runtime pipelines adaptive systems;low power;operating system;adaptive systems;adaptive processor;adaptive processor routine based resource adaptation os aware microprocessor resource adaptation run time operating system power saving power consumption reduction power performance trade off;low power electronics;operating systems computers;microcomputers	The increasingly constrained power budget of today's microprocessor has resulted in a situation where power savings of all components in a system have to be taken into consideration. Operating System (OS) is a major power consumer in many modern applications execution. This paper advocates a routine based OS-aware microprocessor resource adaptation mechanism targeting run-time OS power savings. Simulation results show that compared with the existing sampling-based adaptation schemes, this novel methodology yields more attractive power and performance trade-off on the OS execution. To our knowledge, this paper is the first step to address the power saving issue of the OS itself, an increasingly important area that has been largely overlooked in the previous studies.	microprocessor;operating system;sampling (signal processing);simulation	Tao Li;Lizy Kurian John	2003		10.1145/871506.871565	embedded system;electronic engineering;parallel computing;real-time computing;resource allocation;computer science;adaptive system;operating system;logic simulation;microcomputer;low-power electronics	Arch	-3.646022435618009	55.6474601739274	82872
6cab8dbfc35438780b97ab31166fe0b7c62c63c7	powerpctm array verification methodology using formal techniques	atpg test vectors;microprocessors;design for testability;logic arrays;array verification methodology;trademarks;search space;automatic test pattern generation;automatic testing;reduced instruction set computing;symbolic trajectory array verification methodology formal techniques powerpc risc microprocessors embedded array blocks functional behavior functional simulators equivalence checking atpg test vectors;symbolic trajectory evaluation;powerpc risc microprocessors;testing;functional simulators;design representation;symbolic trajectory;formal method;functional behavior;formal verification;computer testing;logic testing reduced instruction set computing automatic testing computer testing integrated circuit testing real time systems design for testability logic arrays;embedded array blocks;logic arrays microprocessors timing reduced instruction set computing testing automatic test pattern generation design methodology formal verification trademarks power generation;formal techniques;logic testing;integrated circuit testing;power generation;equivalence checking;real time systems;design methodology;timing	In this paper we discuss the methodology used on PowerPC RISC microprocessors to verify the correctness of embedded array blocks. The ficnctional behavior of these blocks cannot be verifid using traditwnal functional simulators since the search space is too large. Our methodology combines the use of equivalence checking f o d methoh, simulation using ATPG test vectors, and symbolic trajectory evaluation We discuss how these techniques are applied to verifi the operatwn of an array in dffering design representation fonnats. We also discuss how these techniques can be used for checking the consistency of different design representations.	correctness (computer science);embedded system;emulator;formal equivalence checking;microprocessor;powerpc;simulation;symbolic trajectory evaluation;turing completeness	Neeta Ganguly;Magdy S. Abadir;Manish Pandey	1996		10.1109/TEST.1996.557147	electricity generation;reduced instruction set computing;computer architecture;parallel computing;formal methods;design methods;formal verification;computer science;theoretical computer science;automatic test pattern generation;design for testing;formal equivalence checking;software testing;symbolic trajectory evaluation	EDA	7.953330009338411	52.41625714699945	83094
79a00b0f932feb85b30997a80c1adeb0d0fed002	chstone: a benchmark program suite for practical c-based high-level synthesis	hardware design languages;microprocessors;digital signal processing;kernel;article publisher;c based high level synthesis;information science;information technology;benchmark program suite;high level synthesis;c language;chstone;high level synthesis c language;function level transformation;function level transformation chstone benchmark program suite c based high level synthesis;arithmetic;quantitative evaluation;circuit synthesis;embedded software;high level synthesis hardware design languages digital signal processing kernel embedded software microprocessors circuit synthesis arithmetic information science information technology	In general, standard benchmark suites are critically important for researchers to quantitatively evaluate their new ideas and algorithms. This paper presents CHStone, a suite of benchmark programs for C-based high-level synthesis. CHStone consists of a dozen of large, easy-to-use programs written in C, which are selected from various application domains. This paper also presents synthesis results which will be served as a baseline for researchers to compare their new techniques with. In addition, we present a case study on function-level transformation using a program in the CHStone suite.	algorithm;baseline (configuration management);benchmark (computing);function-level programming;high- and low-level;high-level synthesis	Yuko Hara-Azumi;Hiroyuki Tomiyama;Shinya Honda;Hiroaki Takada;Katsuya Ishii	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541637	embedded system;computer architecture;kernel;embedded software;information science;computer science;engineering;electrical engineering;theoretical computer science;operating system;digital signal processing;high-level synthesis;information technology;algorithm;computer engineering	Arch	2.527365685811824	51.10139351499443	83113
6cf509bdf38a57c6f2d8a1e838f97ac816074547	evaluating the model accuracy in automated design space exploration	abstracted model;low-level cycle-accurate simulation;communication cost;simulation speed-up;multi-level communication cost;multi-level communication;automated design space exploration;different communication cost;inter-task communication event;simulation speed;model accuracy;simulation engine;system on chip;data compression;integrated circuit design;probability;probabilistic model;frames per second;motion jpeg;system modeling;uml;soc;unified modeling language	Design space exploration is used to shorten the design time of System-on-Chips (SoCs). The models used in the exploration need to be both accurate and fast to simulate. This paper introduces a multi-level communication cost to improve the accuracy of the abstracted system models. During the simulation, one of three different communication costs is applied for each inter-task communication event based on the mapping of the communicating tasks. The accuracy of three system abstraction models including the presented communication cost is evaluated using a Motion-JPEG (M-JPEG) application described in Unified Modeling Language (UML). According to the results, the average error in frames per second (FPS) is 3.8% for the trace model, 4.3% for the modulo model, and 12.8% for the probabilistic model compared to FPGA execution. The results show that with the multi-level communication cost the accuracy is increased significantly, and accurate results can be achieved with arbitrary mappings.	design space exploration;field-programmable gate array;floating point systems;hardware acceleration;inter-process communication;jpeg;modulo operation;run time (program lifecycle phase);simulation;speedup;statistical model;system on a chip;unified modeling language	Kalle Holma;Mikko Setälä;Erno Salminen;Timo Hämäläinen	2007	10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)	10.1109/DSD.2007.4341466	system on a chip;unified modeling language;statistical model;embedded system;parallel computing;real-time computing;simulation;systems modeling;computer science;theoretical computer science;operating system;accuracy and precision;frame rate;motion jpeg;statistics	EDA	2.9566243985784393	52.97057458091471	83149
9af3d70c258678967ed7f7c81943ea50f239bef2	temporal partitioning of data flow graphs for reconfigurable architectures	computer aided design;temporal partitioning;fpga engineering;data flow graph;reconfigurable architecture;vlsi applications	In this paper, we present the famous temporal partitioning algorithms that temporally partition a data flow graph on reconfigurable systems. We have classified these algorithms into four classes: 1) whole latency optimisation algorithms; 2) whole communication cost optimisation algorithms; 3) whole area optimisation algorithms; 4) whole latency-communication cost optimisation algorithms. These algorithms can be used to solve the temporal partitioning problem at the behaviour level.	algorithm;dataflow;mathematical optimization;partition problem;space partitioning	Bouraoui Ouni;Abdellatif Mtibaa	2014	IJCSE	10.1504/IJCSE.2014.058694	computer architecture;parallel computing;computer science;theoretical computer science;computer aided design;data-flow analysis;programming language	EDA	0.3968034075883753	52.67069135179943	83181
404170ce7d757df9095aad6741b7023c43d24488	a software approach to protecting embedded system memory from single event upsets		Radiation from radioactive environments, such as those encountered during space flight, can cause damage to embedded systems. One of the most common examples is the single event upset (SEU), which occurs when a high-energy ionizing particle passes through an integrated circuit, changing the value of a single bit by releasing its charge. The SEU could cause damage and potentially fatal failures to spacecraft and satellites. In this paper, we present an approach that extends the AVR-GCC compiler to protect the system stack from SEUs through duplication, validation, and recovery. Three applications are used to verify our approach, and the time and space overhead characteristics are evaluated.		Jiannan Zhai;Yangyang He;Fred S. Switzer;Jason O. Hallstrom	2015		10.1007/978-3-319-15582-1_20	embedded system;parallel computing;real-time computing	Embedded	8.42385635610405	59.874595591829326	83217
5bdb6dd01121197fb0bfac8ed568321f9fbb03f5	a regular expression matching circuit: decomposed non-deterministic realization with prefix sharing and multi-character transition	mnfau;fpga;ids;nfa;regular expression	This paper shows a compact realization of regular expression matching circuits on FPGAs. First, the given regular expression is converted into a non-deterministic finite automaton (NFA) by the modified McNaughton-Yamada method. Second, to reduce the number of the states in the NFA, prefixes for the NFA are shared. Also, the NFA is converted into the NFA with multicharacter transition (MNFAU: Modular non-deterministic finite automaton with unbounded string transition). Third, the MNFAU is decomposed into the transition string part and the state transition part. The transition string part is represented by the Aho-Corasic deterministic finite automaton (AC-DFA), and it is implemented by an o ff-chip memory and a register. On the other hand, the state transition part is implemented by a cascade of logic cells (LCs) and the the interconnection on the FPGA. We implemented the regular expressions for SNORT (an open source intrusion detection system) on a Xilinx FPGA. Experimental results showed that, the embedded memory size per a character of the MNFAU is reduced to 0.2% of the pipelined DFA; 4.2% of the bit-partitioned DFA; 41.0% of the MNFAU(3); and 71.4% of the MNFAU without prefix sharing. Also, the number of LCs per a character of the MNFAU is reduced to 0.9% of the pipelined DFA; 15.6% of the NFA; and 80.0% of MNFAU without prefix sharing.	deterministic finite automaton;embedded system;field-programmable gate array;finite-state machine;interconnection;intrusion detection system;nondeterministic finite automaton;open-source software;regular expression;snort;state transition table	Hiroki Nakahara;Tsutomu Sasao;Munehiro Matsuura	2012	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2012.05.009	intrusion detection system;embedded system;parallel computing;computer science;theoretical computer science;programming language;generalized nondeterministic finite automaton;regular expression;algorithm;field-programmable gate array	Logic	8.682660000849566	47.164872577796395	83376
995907f9e0cb147aaaadc13ba94a2ba557172a35	high performance connected components labeling on fpga	field programmable gate array;fpga connected components labeling system on chip cmos image sensor;ddr sdram;random access memory;decision tree;image processing;high performance image processing tasks;system on chip cmos image sensors decision tables decision trees embedded systems field programmable gate arrays image processing;connected components labeling algorithm;fpga;embedded system;spi flash;cmos image sensors;computer architecture;embedded systems;cmos image sensor;data cache;system on chip;data cache sizes;pixel;usb controller;universal serial bus;high performance image processing tasks connected components labeling algorithm field programmable gate array soft core soc architecture system on chip decision tables decision trees embedded system cmos image sensor usb controller spi flash ddr sdram data cache sizes;field programmable gate arrays;connected components labeling;connected component;decision trees;decision tables;high performance;decision table;pixel labeling field programmable gate arrays universal serial bus random access memory computer architecture decision trees;labeling;soft core soc architecture	This paper proposes a comparison of the two most advanced algorithms for connected components labeling, highlighting how they perform on a soft core SoC architecture based on FPGA. In particular we test our block based connected components labeling algorithm, optimized with decision tables and decision trees. The embedded system is composed of the CMOS image sensor, FPGA, DDR SDRAM, USB controller and SPI Flash. Results highlight the importance of caching and instructions and data cache sizes for high performance image processing tasks.	active pixel sensor;algorithm;cmos;cache (computing);connected-component labeling;decision table;decision tree;embedded system;field-programmable gate array;high-level programming language;image processing;image sensor;soft core (synthesis);speedup;system on a chip;usb	Costantino Grana;Daniele Borghesani;Paolo Santinelli;Rita Cucchiara	2010	2010 Workshops on Database and Expert Systems Applications	10.1109/DEXA.2010.57	decision table;embedded system;parallel computing;computer hardware;image processing;computer science;decision tree;field-programmable gate array	EDA	2.9161548665165573	46.5933792804599	83677
9d616ae4a953c729ba00ac42aa66ae72bfbe6639	processor load analysis for mobile multimedia streaming: the implication of power reduction	energy efficiency;voltage control;mobile multimedia;codecs;mobile device;time series analysis processor load analysis mobile multimedia streaming power reduction software codec power consumption energy efficiency dynamical voltage scaling dvs control internet statistical analysis;streaming media voltage control energy consumption energy efficiency algorithm design and analysis time series analysis codecs hardware application specific integrated circuits accuracy;energy efficient;dynamical voltage scaling;dynamic voltage scaling;time series;accuracy;mobile multimedia streaming;software codec;internet;statistical analysis;time series analysis;streaming media;energy consumption;application specific integrated circuits;linear time;mobile communication;prediction accuracy;dvs control;voltage control media streaming microprocessor chips mobile communication time series;media streaming;power reduction;power consumption;algorithm design and analysis;processor load analysis;microprocessor chips;hardware	The software codec on mobile device introduces significant power consumption because the energy efficiency of general processor based system is much lower than that of the dedicated hardware such as ASIC based accelerator. Dynamical voltage scaling (DVS) is one of the most efficient techniques to promote the energy efficiency. Most existing papers on this topic use simple heuristics to predict processor load, and poor prediction accuracy is observed in experiments. We advocate intensive analysis on processor load before designing DVS framework and algorithm. Hence, we conduct load analysis on more than 600 processor load trace files for 57 test sequences and 98 representative clips from Internet. Basic statistical analysis and time series analysis are applied intensively to identify major characteristics of the processor load. The analysis shows that it is feasible to predict processor load using low order linear time series model if the load is sampled using feature period. Moreover, there is indeed significant potential to reduce the energy consumption. Based on the analysis results, we develop a fully adaptive DVS technique to adjust supply voltage online with controllable penalty	algorithm;application-specific integrated circuit;central processing unit;codec;dynamic voltage scaling;experiment;heuristic (computer science);mobile device;time complexity;time series	Min Li;Xiaobo Wu;Zihua Guo;Richard Yao;Xiaolang Yan	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521712	embedded system;real-time computing;computer hardware;computer science;operating system;time series;efficient energy use;statistics	EDA	-3.9754108884870276	57.831379425000485	83689
e2057bbeb37680ff17053bed674e360cebe6930a	an event-driven fir filter: design and implementation	altera fpga board;protocols;network synthesis;non uniform sampling;fpga asynchronous logic non uniform sampling fir filter;convolution;finite impulse response filter;nonuniform sampling;fpga;asynchronous design;digital filter;computer architecture;low power;logic gates;design and implementation;micropipeline asynchronous fir filter;synchronization;fir filter;finite impulse response filter computer architecture protocols convolution logic gates synchronization power demand;asynchronous circuits;asynchronous logic;design of asynchronous circuits and systems;fir filters;power consumption;field programmable gate arrays;power demand;altera fpga board event driven fir filter asynchronous logic asynchronous design micropipeline asynchronous fir filter nonuniform sampling;event driven fir filter;network synthesis asynchronous circuits field programmable gate arrays fir filters	Non-uniform sampling has proven through different works, to be a better scheme than the uniform sampling to sample low activity signals. With such signals, it generates fewer samples, which means less data to process and lower power consumption. In addition, it is well-known that asynchronous logic is a low power technology. This paper deals with the coupling between a non-uniform sampling scheme and an asynchronous design in order to implement a digital Filter. This paper presents the first design of a micropipeline asynchronous FIR filter architecture coupled to a non-uniform sampling scheme. The implementation has been done on an Altera FPGA board.	asynchronous circuit;digital filter;event-driven programming;field-programmable gate array;finite impulse response;nonuniform sampling;sampling (signal processing)	Taha Beyrouthy;Laurent Fesquet	2011	2011 22nd IEEE International Symposium on Rapid System Prototyping	10.1109/RSP.2011.5929976	embedded system;nonuniform sampling;electronic engineering;real-time computing;computer science;finite impulse response;control theory	EDA	7.785429433026473	48.68445898205356	83722
c32de2441c152be3d9146c50c85b343a4935f09c	asynchronous transfer of control in the rtsj-compliant java processor	computer languages;yarn;application software;optimal method;virtual machining;real time specification for java;runtime;worst case execution time;guidelines;computer science;real time application;algorithm design and analysis;java yarn application software optimization methods computer science computer languages virtual machining guidelines runtime algorithm design and analysis;java;optimization methods	Asynchronous Transfer of Control (ATC) is a crucial mechanism for real-time applications, and is currently provided in the Real-Time Specification for Java (RTSJ). This paper proposes a framework to implement ATC in the RTSJ-compliant Java processor based on the instruction optimization method proposed in our previous work [1]. Because most of the processing is done before bytecode execution in this method, the implementation using our framework is straightforward. Moreover, its Worst Case Execution Time (WCET) is more predictable.	advanced transportation controller;java processor;mathematical optimization;real time java;real-time transcription;worst-case execution time	ZhiLei Chai;Wenjie Chen;ZhiQiang Tang;Zhang-long Chen;Shi-liang Tu	2005	The Fifth International Conference on Computer and Information Technology (CIT'05)	10.1109/CIT.2005.78	algorithm design;computer architecture;application software;real-time computing;jsr 94;java concurrency;computer science;operating system;strictfp;database;real time java;programming language;java;scala;worst-case execution time	Embedded	-3.1493664694510217	52.97876874477182	83805
2bd3f1a20ad88d9408b74cb8c656e84524c52f33	single event upset detection and correction in virtex-4 and virtex-5 fpgas		A design for the detection and correction of single event upsets (SEUs) in the configuration memory of field programmable gate arrays (FPGAs) is presented. Larger configuration memories and shrinking design rules have caused concerns to rise about SEUs in highreliability high-availability systems using FPGAs. We describe the operation and architecture of the proposed design as well as its implementation in Xilinx Virtex-4 and Virtex-5 FPGAs. 1	field-programmable gate array;high availability;single event upset	Bradley F. Dutton;Charles E. Stroud	2009			parallel computing;field-programmable gate array;fishing line;computer science;virtex;computer hardware;single event upset	EDA	8.67247924190591	59.53398908923208	83975
cd4dacff48d7523149aab1d444b70fc2507610ab	aggressive loop pipelining for reconfigurable architectures	pipeline processing reconfigurable architectures field programmable gate arrays hardware delay computer architecture program processors scheduling algorithm counting circuits clocks;reconfigurable architectures;field programmable gate arrays aggressive loop pipelining reconfigurable architecture software loop mapping hardware synergy;chip;reconfigurable architectures field programmable gate arrays pipeline processing;reconfigurable architecture;software loop mapping;field programmable gate arrays;hardware synergy;high performance;pipeline processing;aggressive loop pipelining	In this work aims new techniques for mapping software loops to FPGAs. Extensive and aggressive use of pipelining techniques for achieving high performance solutions is the main goal. Those techniques are foreseen to effectively take advantage of the hardware synergies available in the current FPGA devices, especially the DSP blocks and the on-chip configurable memories.	digital signal processor;field-programmable gate array;pipeline (computing);synergy	Ricardo Menotti;Eduardo Marques;João M. P. Cardoso	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380699	chip;software pipelining;embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;field-programmable gate array	EDA	1.0460070896011786	49.96310345495365	84238
0120f121009f215d3a96b4c22e61925804738e75	computing with stochastic processors: revisiting the correctness contract between software and hardware	computers;power saving;reliability;power density;technological innovation;error tolerance;chip;design technique;hardware program processors stochastic processes computers software reliability manufacturing;stochastic processes;system design;environmental variation;manufacturing;stochastic processor;error resilience;context dependent;software reliability;program processors;hardware	Moore's Law, the primary driver for the astonishing advances in computing, is being threatened due to manufacturing and environmental variations and the resulting non-determinism in the circuit behavior. The non-determinism is exacerbated by the dynamic voltage and timing variations caused by the unprecedented increase in the power density of the chips and the time and context-dependent variation in temperature and utilization across the chip. The most immediate impact of such non-determinism is on chip yields and manufacturing costs. Recent predictions suggest that unless chip yields improve or manufacturing costs are tamed, process shrinks beyond 18nm may become infeasible. Clearly the status quo cannot continue, and we must find a solution to the non-determinism problem' if the semiconductor industry has to remain a viable driver of technological innovation and capabilities for the future.  In this talk, I will argue that the problem is not non-determinism per se, but the way computer system designers treat it. The chip components have become stochastic, yet, the basic approach to designing and operating computing machines has remained unchanged. Software continues to expect hardware to behave flawlessly for all inputs under all conditions, while hardware is overdesigned to meet this software mindset. I will argue that the cost of maintaining the abstraction of flawless hardware will soon become prohibitive and that we need to fundamentally rethink the correctness contract between hardware and software. Instead of computing machines where the hardware variations are hidden from the software through over-design, I will present a vision of computing machines where a) these variations are fully exposed to the highest layers of software in form of hardware errors, and b) errors are managed through architectural and design techniques to maximize power savings afforded by relaxed correctness. The hardware would be deliberately under-designed with relaxed constraints to allow errors, especially for rare computation, and produce stochastically correct results even under nominal conditions. The software would be aware of hardware errors and proactively self-adapt. We call such under-designed processors that produce only stochastically correct results even under nominal conditions and rely on software adaptability and architectural resilience for tolerating errors, stochastic processors. We call the applications that have been implemented to be adaptively error-tolerant, stochastic applications. I will discuss approaches to architect and design stochastic processors and stochastic applications.	central processing unit;computation;computer;context-sensitive language;correctness (computer science);emoticon;error-tolerant design;moore's law;nondeterministic algorithm;semiconductor industry;stochastic process	Rakesh Kumar	2010	2010 ACM/IEEE International Symposium on Low-Power Electronics and Design (ISLPED)	10.1145/1840845.1840920	chip;stochastic process;embedded system;electronic engineering;real-time computing;simulation;telecommunications;computer science;engineering;electrical engineering;theoretical computer science;operating system;context-dependent memory;power density;reliability;hardware architecture;manufacturing;software quality;statistics;systems design	Arch	5.367524993212495	59.54271965488675	84279
0e24f25897cb07d7fc3cdca871c3d49dd97383af	conformance testing of vmebus and multibus ii products	standards;bus interfaces vmebus multibus ii testing;electronic equipment testing;conformance testing;original equipment manufacturer;system testing logic testing control systems electronic equipment testing automatic testing logic devices information analysis programmable logic arrays backplanes pattern analysis;standards computer interfaces electronic equipment testing;computer interfaces	A system for testing VMEbus and Multibus II products is described. The system is mainly automated to reduce costs and ensure impartiality. A test campaign is explained in the sense of a walk through the test system, in the way of a customer-such as a manufacturer, reseller, or original equipment manufacturer-would see it. The focus is on testing bus interfaces because it is more complex than testing backplanes.<<ETX>>	conformance testing;multibus;vmebus	Marcus Adams;Yi Qian;Jacek Tomaszunas;Josef Burtscheidt;Edgar Kaiser;Csaba Juhász	1992	IEEE Micro	10.1109/40.124382	embedded system;black-box testing;electronic test equipment;white-box testing;integration testing;computer science;original equipment manufacturer;operating system;conformance testing;software testing;real-time testing;system testing	SE	9.891965896869753	53.8009515578566	84566
4bcdc9bc45243500a60660e48a3848347bc7766b	efficient self-reconfigurable implementations using on-chip memory	camino mas corto;reconfiguration;field programmable gate array;memoire;shortest path;reconfiguracion;implementation autoreconfiguration;plus court chemin;red puerta programable;pulga electronica;reseau porte programmable;chip;etat actuel;memoria;state of the art;on the fly;estado actual;puce electronique;memory	The limited I/O bandwidth in reconfigurable devices results in a prohibitively high reconfiguration overhead for dynamically reconfigured FPGA-based platforms. Thus, the full potential of dynamic reconfiguration can not be exploited. Usually, any attainable speed-up by executing an application on hardware is diminished by the reconfiguration overhead. The self-reconfiguration concept aims at drastically reducing the reconfiguration overhead by performing dynamic reconfiguration on-chip without the intervention of an external host. Thus, using self-reconfiguration, a configurable device can alter its functionality autonomously. Implementations based on self-reconfiguration promise significant speed-up compared with conventional approaches [7, 8]. Self-reconfiguration was first introduced in [4, 5]. In [7, 8] self-reconfiguration was proposed to be realized by altering the configuration bit-stream, that is, on-chip logic accesses and alters the configuration bit-stream to reconfigure the device. Compared with conventional implementations, significant speed-up was achieved for string matching and genetic programming problems [7, 8]. However, the proposed approach in [7, 8] can be realized only using multi-context configurable devices that allow on-chip manipulation of the configuration bit-stream. In state-of-the-art FPGAs, direct manipulation of the configuration bit-stream can only be performed by an external host. Moreover, the complexity depends on the structure of the configuration bit-stream and the on-chip configuration mechanism, and has not been analyzed thus far.	bitstream;direct manipulation interface;field-programmable gate array;genetic programming;input/output;overhead (computing);self-reconfiguring modular robot;string searching algorithm	Sameer Wadhwa;Andreas Dandalis	2000		10.1007/3-540-44614-1_47	chip;shared memory;embedded system;telecommunications;computer science;control reconfiguration;flat memory model;shortest path problem;memory;field-programmable gate array;cache-only memory architecture	Arch	1.9181963646464077	56.735374725500094	84574
60a19051d1e82a2ab48d81346a0fb100d77adb6a	finding near-perfect parameters for hardware and code optimizations with automatic multi-objective design space explorations	hardware complexity estimation;code optimization;multi objective optimization;automatic design space exploration	In the design process of computer systems or processor architectures, typically many different parameters are exposed to configure, tune, and optimize every component of a system. For evaluations and before production, it is desirable to know the best setting for all parameters. Processing speed is no longer the only objective that needs to be optimized; power consumption, area, and so on have become very important. Thus, the best configurations have to be found in respect to multiple objectives. In this article, we use a multi-objective design space exploration tool called Framework for Automatic Design Space Exploration (FADSE) to automatically find near-optimal configurations in the vast design space of a processor architecture together with a tool for code optimizations and hence evaluate both automatically. As example, we use the Grid ALU Processor (GAP) and its postlink optimizer called GAPtimize, which can apply feedback-directed and platformspecific code optimizations. Our results show that FADSE is able to cope with both design spaces. Less than 25% of the maximal reasonable hardware effort for the scalable elements of the GAP is enough to achieve the processor’s performance maximum. With a performance reduction tolerance of 10%, the necessary hardware complexity can be further reduced by about two-thirds. The found high-quality configurations are analyzed, exhibiting strong relationships between the parameters of the GAP, the distribution of complexity, and the total performance. These performance numbers can be improved by applying code optimizations concurrently to optimizing the hardware parameters. FADSE can find near-optimal configurations by effectively combining and selecting parameters for hardware and code optimizations in a short time. The maximum observed speedup is 15%. With the use of code optimizations, the maximum possible reduction of the hardware resources, while sustaining the same performance level, is 50%. Copyright © 2012 John Wiley & Sons, Ltd.	approximation;arithmetic logic unit;british informatics olympiad;cpu cache;complexity;computer;concurrency control;design space exploration;display resolution;embedded system;gap buffer;heuristic (computer science);john d. wiley;list of cpu architectures;mathematical optimization;maximal set;optimizing compiler;pareto efficiency;run time (program lifecycle phase);scalability;search algorithm;simulation;speedup;supercomputer;x86	Ralf Jahr;Horia Calborean;Lucian Vintan;Theo Ungerer	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.2975	mathematical optimization;computer architecture;computer science;theoretical computer science;multi-objective optimization;program optimization;programming language	Arch	-1.2097005331739412	54.79837828342112	84636
21cc65fddaabd6adc20417721cbd74b19b1f5035	automatic transformations for communication-minimized parallelization and locality optimization in the polyhedral model	cost function;nested loops;multi dimensional;polyhedral model;loop transformation;affine transformation;coarse grained;reuse distance;integer linear program	The polyhedral model provides powerful abstractions to optimize loop nests with regular accesses. Affine transformations in this model capture a complex sequence of execution-reordering loop transformations that can improve performance by parallelization as well as locality enhancement. Although a significant body of research has addressed affine scheduling and partitioning, the problem of automatically finding good affine transforms for communication-optimized coarse-grained parallelization together with locality optimization for the general case of arbitrarily-nested loop sequences remains a challenging problem. We propose an automatic transformation framework to optimize arbitrarilynested loop sequences with affine dependences for parallelism and locality simultaneously. The approach finds good tiling hyperplanes by embedding a powerful and versatile cost function into an Integer Linear Programming formulation. These tiling hyperplanes are used for communication-minimized coarse-grained parallelization as well as for locality optimization. The approach enables the minimization of inter-tile communication volume in the processor space, and minimization of reuse distances for local execution at each node. Programs requiring one-dimensional versus multi-dimensional time schedules (with schedulingbased approaches) are all handled with the same algorithm. Synchronization-free parallelism, permutable loops or pipelined parallelism at various levels can be detected. Preliminary studies of the framework show promising results.	algorithm;automatic parallelization;control flow;inter-process communication;linear programming formulation;locality of reference;loss function;mathematical optimization;parallel computing;polyhedral;polytope model;scheduling (computing);tiling window manager	Uday Bondhugula;Muthu Manikandan Baskaran;Sriram Krishnamoorthy;J. Ramanujam;Atanas Rountev;P. Sadayappan	2008		10.1007/978-3-540-78791-4_9	loop tiling;mathematical optimization;combinatorics;discrete mathematics;loop fission;frameworks supporting the polyhedral model;nested loop join;computer science;loop nest optimization;affine transformation;mathematics;polytope model;algorithm	PL	-1.251450648198577	51.70625157413693	84674
55e75fc5a7bfac921ae968f1eb996edfe3799b54	loop program mapping and compact code generation for programmable hardware accelerators	multiprocessor system on chip loop program mapping compact code generation programmable hardware accelerators nested loops functional programming language polyhedron model underlying accelerator architectures loop level parallelism instruction level parallelism zero overhead looping trimaran compilation framework generated processor codes mpsoc;registers parallel processing accelerator architectures hardware schedules vliw;system on chip multiprocessing systems program compilers programming languages;accelerator architectures;vliw;system on chip;registers;schedules;multiprocessing systems;program compilers;parallel processing;programming languages;hardware	We present a novel design methodology for the mapping of nested loops onto programmable hardware accelerators. Key features of our approach are: (1) Design entry in form of a functional programming language and loop parallelization in the polyhedron model, (2) the underlying accelerator architectures consist of lightweight, tightly-coupled, and programmable processor arrays, which can exploit both loop-level parallelism and instruction-level parallelism, (3) support of zero-overhead looping not only for inner most loops but also for arbitrarily nested loops. We implemented the proposed methodology in a prototype design tool and evaluated selected benchmarks by comparing our code generator with the Trimaran compilation framework. As the results show, our approach can reduce the size of the generated processor codes up to 64 % while at the same time achieving a significant higher throughput.	central processing unit;code generation (compiler);compiler;data compaction;dataflow;design tool;functional programming;goodyear mpp;hardware acceleration;image processing;instruction-level parallelism;linear algebra;loop-level parallelism;mpsoc;multiprocessing;overhead (computing);parallel computing;polyhedron model;programming language;prototype;requirement;streaming algorithm;throughput;very long instruction word	Srinivas Boppu;Frank Hannig;Jürgen Teich	2013	2013 IEEE 24th International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2013.6567544	system on a chip;embedded system;parallel processing;computer architecture;parallel computing;real-time computing;schedule;computer science;very long instruction word;operating system;processor register;programming language	EDA	1.0794896481785652	50.41824678802469	84705
e1a8c62e8cd6e57ebe4cdc81e74fdf81acc4f771	runtime adaptation of embedded tasks with a-priori known timing behavior utilizing on-line partner-core monitoring and recovery	online monitoring;on line partner core recovery runtime embedded task adaptation timing behavior on line partner core monitoring coprocessor task execution reliability embedded system anomalous software behavior identification online reconfiguration debugging software algorithms embedded multicore system;multicore systems;system recovery coprocessors embedded systems fault tolerant computing multiprocessing systems program debugging system monitoring;fault tolerant embedded soc;multicore systems online monitoring runtime recovery fault tolerant embedded soc supervised embedded computing;monitoring;multicore processing;program processors monitoring hardware software reliability multicore processing;software reliability;program processors;runtime recovery;hardware;supervised embedded computing	As the development of heterogeneous embedded Systems-on-Chip with a multitude of hardware accelerator coprocessors creates new possibilities for evolution in aerospace, medicine, communications and consumer eras, improving reliable performance of systems is therefore increasingly important and challenging. Our contributions pertaining to this context are two-fold. We focus on enhancing reliability in the execution of coprocessor tasks with a priori known execution times by allowing an embedded system to identify anomalous software behaviors and additionally to provide rapid online reconfiguration and re-execution in run-time. We present an innovative methodology that combines hardware and software techniques for flexibility, through essentially employing low-cost on-line monitoring, debugging and real-time replacement of the failing sections of software algorithms in an embedded multi-core system. The proposed mechanisms introduce negligible performance degradation, reduced hardware cost and require minimum code instrumentation.	algorithm;coprocessor;data access;debugging;dependability;elegant degradation;embedded system;failure;hardware acceleration;malware;mathematical optimization;memory management (operating systems);multi-core processor;online and offline;overhead (computing);real-time transcription;run time (program lifecycle phase);system on a chip	Ioannis Christoforakis;Othon Tomoutzoglou;Dimitrios Bakoyiannis;George Kornaros	2014	2014 12th IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/EUC.2014.10	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;software quality	Embedded	6.286788469183673	58.62838277351794	84718
4ad3898e05c28170cb831366235161f32f265f0a	distributed simulation of asynchronous hardware: the program driven synchronization protocol	proceso secuencial comunicante;occam;critical point;programa simulacion;asynchronous hardware;point critique;communicating sequential process;circuit vlsi;semantics;vlsi design;semantica;semantique;synchronous system;asynchronisation;programme simulation;synchronisation;clock distribution;vlsi circuit;processus sequentiel communicant;asynchronism;algebra proceso;causalite;simulation program;synchronization;digital design;algebre processus;punto critico;csp;sincronizacion;computer hardware;temporal coherence;power consumption;circuito vlsi;modelling and simulation;distributed simulation;process algebra;concurrent process;materiel informatique;material informatica;asincronia;causality;causalidad	Synchronous VLSI design is approaching a critical point, with clock distribution becoming an increasingly costly and complicated issue and power consumption rapidly emerging as a major concern. Hence, recently, there has been a resurgence of interest in asynchronous digital design techniques which promise to liberate digital design from the inherent problems of synchronous systems. This activity has revealed a need for modelling and simulation techniques suitable for the asynchronous design style. The concurrent process algebra Communication Sequential Processes (CSP) and its executable counterpart, occam, are increasingly advocated as particularly suitable for this purpose. However, the parallel distributed semantics of CSP and occam introduce synchronization problems in the model. This paper presents the Program Driven Synchronization Protocol, which seeks to address causality and synchronization problems and enforce temporal coherency in distributed CSP/ occam models of asynchronous hardware systems. © 2002 Elsevier Science (USA)	asynchronous circuit;causality;communicating sequential processes;critical point (network science);executable;independence day: resurgence;logic synthesis;parallel computing;process calculus;simulation;occam	Georgios K. Theodoropoulos	2002	J. Parallel Distrib. Comput.	10.1006/jpdc.2001.1806	synchronization;parallel computing;real-time computing;simulation;telecommunications;computer science;theoretical computer science;operating system;mathematics;distributed computing;semantics;programming language;algorithm	EDA	6.421769638203177	53.76938791640571	85177
9525f2810277a8048dece17a991b4794d6e1eebd	xf-board: a prototyping platform for reconfigurable hardware operating systems	reconfigurable hardware;operating system		field-programmable gate array	Herbert Walder;Samuel Nobs;Marco Platzner	2004			embedded system;hardware compatibility list;reconfigurable computing;computer science	Arch	4.4519224289926225	49.48329821409297	85608
2eb173fa8a8fcbe2af7195f65813e7f5c1a18b50	what your dram power models are not telling you: lessons from a detailed experimental study		Main memory (DRAM) consumes as much as half of the total system power in a computer today, due to the increasing demand for memory capacity and bandwidth. There is a growing need to understand and analyze DRAM power consumption, which can be used to research new DRAM architectures and systems that consume less power. A major obstacle against such research is the lack of detailed and accurate information on the power consumption behavior of modern DRAM devices. Researchers have long relied on DRAM power models that are predominantly based off of a set of standardized current measurements provided by DRAM vendors, called IDD values. Unfortunately, we find that state-of-the-art DRAM power models are often highly inaccurate when compared with the real power consumed by DRAM. This is because existing DRAM power models (1) are based off of the worst-case power consumption of devices, as vendor specifications list the current consumed by the most power-hungry device sold; (2) do not capture variations in DRAM power consumption due to different data value patterns; and (3) do not account for any variation across different devices or within a device.	approximation algorithm;bendix g-15;best, worst and average case;column (database);computer data storage;dimm;data dependency;datasheet;dynamic random-access memory;open-source software;simulation;total system power;tracing (software)	Saugata Ghose;Abdullah Giray Yaglikçi;Raghav Gupta;Donghyuk Lee;Kais Kudrolli;William X. Liu;Hasan Hassan;Kevin K. Chang;Niladrish Chatterjee;Aditya Agrawal;Mike O'Connor;Onur Mutlu	2018	POMACS	10.1145/3219617.3219661	total system power;embedded system;distributed computing;vendor;obstacle;bandwidth (signal processing);dram;computer science	Metrics	-0.8713112880675826	60.12501276803139	85620
a03b0b1a2fa205b527fb70570a04a4bead1875c7	a versatile vlsi fast fourier transform processor	real time;fast fourier transform;performance analysis;adaptive architecture	A versatile special-purpose VLSI fast Fourier transform (FFT) processor is presented. It can process variant data sizes of FFT and cooperate with other identical FFT processors to accomplish cascade and parallel FFT processing schemes. The operations of the single processor FFT processing scheme, the multiprocessor cascade FFT processing scheme, and the multiprocessor parallel FFT processing scheme are described. The results of performance analysis show that the combination of adaptive architecture capability and VLSI technology can provide a practical solution for meeting the goal of advanced real-time FFT processing.	adaptive architecture;central processing unit;fast fourier transform;multiprocessing;real-time clock;very-large-scale integration	Kuang-Cheng Ting;Chuan-lin Wu	1984		10.1145/1499310.1499330	fast fourier transform;computer architecture;parallel computing;real-time computing;split-radix fft algorithm;computer science;bruun's fft algorithm;prime-factor fft algorithm	HPC	4.760127357233087	46.653027022862034	85848
08390d519e63cbeda763e77d9a374cdfa9f5a92b	architectural simulation in the context of behavioral synthesis	circuit analysis computing;high level synthesis;interactive programming;program debugging;amis;architectural simulation;behavioral specification debugging;behavioral synthesis;concurrent synthesis/simulation;cycle based simulation;interactive simulator;simulation-synthesis model	This paper deals with integrating an interactive simulator within a behavioral synthesis tool, thereby allowing concurrent synthesis and simulation. The resulting environment provides a cycle based simulation of a behavioral module under synthesis. The simulator and the behavioral synthesis are based on a single model that allows to link the behavioral description and the architecture produced by synthesis. The basic simulation-synthesis model is extended in order to allow for concurrent architectural simulation of several modules under synthesis. This paper also discusses an implementation of this concept resulting in a simulator, called AMIS. This tool assists the designer for understanding the results of behavioral synthesis and for architecture exploration. It may also be used to debug the behavioral specification.	debugging;high-level synthesis;simulation	Abderrazek Jemai;Polen Kission;Ahmed Amine Jerraya	1998			computer architecture;real-time computing;computer science;high-level synthesis;computer engineering	EDA	5.248411507122657	52.349393213350325	85871
3ca4a0dddf6708bef1614997bad09cd9c2ff355e	closed-loop power-control governor for multimedia mobile devices	batteries;power demand;multimedia communication;mobile handsets;decoding;control systems;streaming media	The micro-electronics industry has been boosting the capabilities of multimedia mobile devices, but the battery, which is the only power source of most mobile devices, is experiencing relatively slow development. Therefore, determining how to optimize the energy consumption of mobile devices under a predefined performance requirement has become a critical issue. The video decoder, as one of the main energyconsuming multimedia applications, is the target application of this manuscript. This paper presents control algorithms for power regulation under the limited battery capacities of multimedia hand-held devices while executing a decoder application and maintaining a reasonable quality of user experience. A control system, which includes a real-time closedloop control subsystem and a power-control governor (PCG), has been implemented in the operating system of a low-cost development board. Instead of using any specific power sensor, a PMC (performance monitoring counter)-based estimator is used as the feedback signal in the closed-loop subsystem. After a theoretical system model has been obtained, classic controllers have been implemented in the development board. The control system is able to regulate the power consumption and the battery discharge rate in the presence of fluctuations in the decoder power-consumption demand. The proposed PCG can extend the battery lifetime by 15.5% and 12.8% in comparison with the conservative and ondemand governors of Linux, respectively.	algorithm;control system;discharger;feedback;linux;microprocessor development board;mobile device;operating system;real-time transcription;user experience;video decoder	Qiong Tang;Angel M. Groba;Eduardo Juárez;César Sanz	2017	IEEE Transactions on Consumer Electronics	10.1109/TCE.2017.014824	battery (electricity);power density;computer science;power control;real-time computing;embedded system;energy consumption;mobile device;system model;control system;multimedia;video decoder	Mobile	-2.9754921625691324	59.797528338910894	85964
1218467ee14a43485719008c040b7f6e1ddd3899	propan: a retargetable system for postpass optimisations and analyses	machine language;lenguaje maquina;programacion entera;resource allocation;instruction;instruccion;registre;programmation en nombres entiers;codificacion;lenguaje descripcion;programacion lineal;integer programming;scheduling;coding;linear programming;programmation lineaire;ordonamiento;procesador;asignacion recurso;computer hardware;processeur;allocation ressource;langage machine;materiel informatique;material informatica;registro;processor;ordonnancement;register;langage description;codage;description language	PROPAN is a system that allows for the generation of machine-dependent postpass optimisations and analyses on assembly level. It has been especially designed to perform high-quality optimisations for irregular architectures. All information about the target architecture is specified in the machine description language TDL. For each target architecture a phase-coupled code optimiser is generated which can perform integrated global instruction scheduling, register reassignment, and resource allocation by integer linear programming (ILP). All relevant hardware characteristics of the target processor are precisely incorporated in the generated integer linear programs. Two different ILP models are available so that the most appropriate modelling can be selected individually for each target architecture. The integer linear programs can be solved either exactly or by the use of ILP-based approximations. This allows for high quality solutions to be calculated in acceptable time. A set of practical experiments shows the feasibility of this approach.		Daniel Kästner	2000		10.1007/3-540-45245-1_5	real-time computing;integer programming;machine code;resource allocation;computer science;linear programming;artificial intelligence;theoretical computer science;operating system;coding;programming language;scheduling;algorithm	Logic	0.2433468285834845	52.24804259786819	86200
cf1ca40807eb5069b11084f7d0cea4eb92a0b018	research on communication technology of complex system based on fpga dual-port rams	digital signal processing;protocols;random access memory;arrays;complex systems;field programmable gate arrays;real time systems	The dual-port RAM based on FPGA plays an important role in the design of complex system. This paper firstly introduces the basic characteristics of reset, enable, read and write dual-port RAM, and their effects for the system's reliability, real-time and power consumption. And then focus on the dual-port RAM to achieve a large-capacity shift caches and the dual-port RAM array of communications chain in the application of complex communication system, which includes the implement of real-time communication among DSP, FPGA and other peripheral interfaces with applying the link, the interconnection communication between multi-CPU, and the related design of three-port RAM. The complex communication data and buses monitor and other applications show the configurable dual-port RAM can enrich the design of complex systems, improve the efficiency of system communication, and reduce the difficulty of system design.		Qingzhong Jia;Xingdou Wang	2016		10.1109/IConAC.2016.7604890	embedded system;communications protocol;complex systems;real-time computing;tag ram;computer hardware;computer science;operating system;digital signal processing;distributed computing;field-programmable gate array	EDA	2.6408714356423157	56.67876430658692	86476
2bb3527ed6aa66344886c5ebf8b37016713f06d3	powerpc 601 and alpha 21064: a tale of two riscs	pipelined implementations powerpc 601 alpha 21064 risc implementations digital equipment corporation ibm motorola apple superscalar implementations high performance processing order very fast clock streamlined implementation structure risc microprocessors implementation philosophies instruction sets;reduced instruction set computing;pipeline processing reduced instruction set computing instruction sets;reduced instruction set computing computer architecture clocks microprocessors pipelines instruction sets registers process design power generation computer aided instruction;instructions per clock;high performance;pipeline processing;instruction sets	A discussion is given on two RISC implementations: from Digital Equipment Corporation, the Alpha 21064, and from IBM/Motorola/Apple, the PowerPC 601. Both are superscalar implementations, that is, they can sustain execution of two or more instructions per clock cycle. Otherwise, these two implementations present vastly different philosophies for achieving high performance. The PowerPC 601 focuses on powerful instructions and great flexibility in processing order, while the Alpha 21064 depends on a very fast clock, with simpler instructions and a more streamlined implementation structure. These two RISC microprocessors exemplify contrasting, but equally valid, implementation philosophies. An overview is given of the instruction sets and the authors emphasize the differences in design: PowerPC uses powerful instructions so that fewer are needed to get the job done; Alpha uses simple instructions so that the hardware can be kept simpler and faster. The authors also discuss the pipelined implementations of the two architectures; again, the contrast is between powerful and simple.<<ETX>>	alpha 21064;clock signal;exemplification;instructions per cycle;microprocessor;pipeline (computing);powerpc 600;superscalar processor	James E. Smith;Shlomo Weiss	1994	Computer	10.1109/2.294853	reduced instruction set computing;computer architecture;parallel computing;powerpc;computer hardware;computer science;out-of-order execution;complex instruction set computing;operating system;central processing unit;instruction set;delay slot;megahertz myth;cycles per instruction;ibm power microprocessors;power architecture;instructions per cycle	HPC	6.145788141383976	48.87626771658479	86487
3f92ab3d4cb93ff4cc6eb7bbf6cc711426441a0a	system-level issues for software thread integration: guest triggering and host selection	hardware to software migration;thread integration;high resolution;timing real time systems systems analysis integrated software instruction sets;mpeg video;mpeg video player system level issues software thread integration guest triggering host selection low cost concurrency general purpose processors embedded system designers design constraints timing measurement temporally deterministic code design decisions system efficiency;compilers;software systems yarn application software concurrent computing interleaved codes automatic control hardware embedded software embedded system timing;general purpose processor;embedded systems;embedded system design;systems analysis;integrated software;host selection;software thread integration;embedded processor;instruction sets;multithreading;real time systems;timing	Software thread integration provides low-cost con-currency on general-purpose processors by automatically interleaving multiple threads of control into one. This simplifies hardware to software migration and can help embedded system designers meet design constraints. Previous work describes how to efficiently integrate threads. In this paper we demonstrate how to link trigger events with guest thread execution and how to analyze an application to determine which threads to integrate. The analysis involves timing measurement and prediction to identify the amount of easily accessible temporally deter-ministic code within each function. This information is used to predict quantitatively the impact of design decisions on system efficiency and help guide integration. To illustrate this process we evaluate an application predicted for the year 2005, when $20 buys a 2000 MIPS embedded processor-a software-based high-resolution MPEG video player.	algorithm;central processing unit;embedded system;forward error correction;general-purpose markup language;image resolution;moving picture experts group;naruto shippuden: clash of ninja revolution 3;software modernization;temporal logic;thread (computing)	Alexander G. Dean;John Paul Shen	1999		10.1109/REAL.1999.818849	embedded system;systems analysis;computer architecture;compiler;parallel computing;real-time computing;multithreading;image resolution;computer science;operating system;instruction set;programming language	EDA	3.1193557995281	52.908902478524936	86522
8f4ba57d26a34d80b289f52a5d2f7b8e345fa3f6	automatic compilation flow for a coarse-grained reconfigurable processor	reconfigurable architectures data flow graphs logic design microprocessor chips program compilers;front end;logic design;reconfigurable computing;reconfigurable architectures;data flow graphs;multimedia application;compiler;general purpose processor;data flow graph;dfg;general purpose processor automatic compilation flow coarse grained reconfigurable processor multimedia applications hardware design remus code sections data flow graph reconfigurable computing array target application domain code samples mpeg 2 c code;compiler coarse grained reconfigurable dfg;hardware design;coarse grained;program compilers;reconfigurable processor;microprocessor chips;reconfigurable	Reconfigurable processor is widely used in multimedia applications. Its performance depends not only on good hardware design but also on compilers that can quickly create efficient configurations. This paper describes an automatic compilation flow for REmus—a coarse-grained reconfigurable processor. The front-end of the compiler extracts code sections with high parallelism degree from the application and generates corresponding DFGs (data flow graph). The back-end of the compiler maps the DFGs onto the reconfigurable computing array. The suitability of the compiler for the target application domain is illustrated with code samples of MPEG-2. Experimental results indicate that the compilation flow can map the C code onto REmus automatically and 4.49× to 6.23× speed-up is achieved in comparison with implementation of general-purpose processor.	application domain;compiler;dataflow;general-purpose macro processor;mpeg-2;parallel computing;reconfigurable computing	Hao Wang;Weiguang Sheng;Weifeng He	2011	2011 9th IEEE International Conference on ASIC	10.1109/ASICON.2011.6157298	computer architecture;parallel computing;real-time computing;computer science	EDA	1.7269707826120884	49.65953917168704	86662
e16c6e27607b2072bcc5a62d51b96e340da3c100	a real-time executive for multiple microprocessor systems	real time		microprocessor;real-time transcription	Richard A. Light	1982			microprocessor;real-time computing;computer science	Arch	4.788797826169233	48.57791747776738	86933
22e82e7b0fc0068adb4e68b6071c1c585e9f50d2	compilers for low power with design patterns on embedded multicore systems	optimising compilers;program processors multicore processing optimization equations mathematical model finite impulse response filters;power reductions embedded multicore systems power dissipation minimization circuit levels multicore programming models parallel design patterns software layer mapreduce power optimization schemes compiler optimizations filter pattern pipe pattern iterator pattern heterogeneous multicore system sid simulation framework finite impulse response program fir program image recognition;pattern;compiler;finite impulse response filters;embedded systems;power aware computing;low power;power aware computing embedded systems multiprocessing systems optimising compilers parallel processing;multicore system;multicore processing;compiler low power multicore system pattern;mathematical model;optimization;multiprocessing systems;program processors;parallel processing	Minimization of power dissipation can be considered at algorithmic, compilers, architectural, logic, and circuit levels. Recent research trends for multicore programming models have come to the direction that parallel design patterns can be a solution to develop multicore applications. As parallel design patterns are with regularity, we view this as a great opportunity to exploit power optimizations in the software layer. In this paper, we present case studies to investigate compilers for low power with parallel design patterns on embedded multicore systems. We evaluate two major parallel design patterns, Pipe and Filter and MapReduce with Iterator. Our work, attempts to devise power optimization schemes in compilers by exploiting the opportunities of the recurring patterns of embedded multicore programs. In all two cases of the patterns investigated, the common recurring patterns of programs are exploited to seek the opportunity for compiler optimizations for low power. Proposed optimization schemes are rate-based optimization for Pipe and Filter pattern and early-exit power optimization for MapReduce with Iterator pattern. Our experiment is based on a power simulator simulating a heterogeneous multicore system under SID simulation framework. In our experiments, a finite impulse response (FIR) program with Pipe and Filter pattern and an image recognition application applied MapReduce with Iterator pattern are evaluated by incorporating our proposed power optimization schemes for each pattern. Significant power reductions are observed in all two cases. With the case study, we present a direction for power optimizations that one can further identify additional key design patterns for embedded multicore systems to explore power optimization opportunities via compilers.	multi-core processor	Cheng-Yen Lin;Chi-Bang Kuan;Jenq Kuen Lee	2013		10.1109/ICPP.2013.125	multi-core processor;parallel processing;computer architecture;compiler;parallel computing;real-time computing;computer science;operating system;mathematical model;pattern;programming language	EDA	-1.850521183813689	51.26070134930659	86983
290215c24d9657d6542ec767377a68d5ecbb0dfe	session abstract	conference proceedings;design;vlsi	The common global on-chip bus is becoming a bottleneck in communication bandwidth and power dissipation. Multi-bus approaches provide temporary alleviation, but the longer-term scalable solution is a Network-on-Chip (NOC). A NOC consists of a network of shared communication links and routers, which connect to the various IP cores through Network Interfaces. NOC-based SOC design seems to be moving from the research phase to first industrial prototypes and finally high-volume products. What does this paradigm shift imply for manufacturing test? How should NOCs be tested, and how can the NOC be leveraged as part of the on-chip test infrastructure? In this session, we explore these and other issues related to the combination of NOCs and test.	cpu power dissipation;network on a chip;programming paradigm;router (computing);scalability;system on a chip	André Ivanov	2006	25th IEEE VLSI Test Symposium (VTS'07)	10.1109/VTS.2006.71		EDA	3.397046994471956	60.209942803745825	87052
7ed105539a853ffab2b6eac63374352e7b23cb18	jpure - a purified java execution environment for controller networks	controller networks;purified java execution environment	About 98 % of the over eight billi ons processors produced in year 2000 will be used in the embedded systems market [11]. From these about 57 % will be 8-bit processors. Many of these microcontrollers will be interconnected using a networking technology that has littl e in common with the Internet. Rather special purpose technologies such as CAN, FireWire or BlueTooth are used to establish a controller network. Interesting	8-bit;bluetooth;central processing unit;embedded system;ieee 1394;internet;java;microcontroller	Danilo Beuche;Lars Büttner;Daniel Mahrenholz;Wolfgang Schröder-Preikschat;Friedrich Schön	2000			embedded system;real-time computing;operating system	EDA	4.609719967966158	49.4191683913023	87202
3bf3744793d1d27cf80fd5e228ec9d4ad5c996e5	dpf: fast, flexible message demultiplexing using dynamic code generation	carefully-designed declarative packet-filter language;flexible message demultiplexing;dpf design;dpf filter;performance impact;dynamic code generation;hand-crafted demultiplexing routine;high performance;quantitative result;fastest packet filter;public domain	Fast and flexible message demultiplexing are well-established goals in the networking community [1, 18, 22]. Currently, however, network architects have had to sacrifice one for the other. We present a new packet-filter system, DPF (Dynamic Packet Filters), that provides both the traditional flexibility of packet filters [18] and the speed of hand-crafted demultiplexing routines [3]. DPF filters run 10-50 times faster than the fastest packet filters reported in the literature [1, 17, 18, 27]. DPF's performance is either equivalent to or, when it can exploit runtime information, superior to hand-coded demultiplexors. DPF achieves high performance by using a carefully-designed declarative packet-filter language that is aggressively optimized using dynamic code generation. The contributions of this work are: (1) a detailed description of the DPF design, (2) discussion of the use of dynamic code generation and quantitative results on its performance impact, (3) quantitative results on how DPF is used in the Aegis kernel to export network devices safely and securely to user space so that UDP and TCP can be implemented efficiently as user-level libraries, and (4) the unrestricted release of DPF into the public domain.	code generation (compiler);declarative programming;digital photo frame;fastest;firewall (computing);library (computing);message authentication code;multiplexing;network packet;run time (program lifecycle phase);self-modifying code;user space	Dawson R. Engler;M. Frans Kaashoek	1996		10.1145/248156.248162	embedded system;public domain;real-time computing;simulation;operating system	Networks	-4.276663383092684	46.69558273745781	87827
5d9877171802e685145caf77b30cdacbac69cf2d	fast and resource aware image processing operators utilizing highly configurable ip blocks	ip block library;hdl design frameworks;generic architectures;architectures for image processing	"""Due to raising system complexity and higher """"time to market"""" demands in industry, hardware development for fast image processing applications is becoming more and more important. In order to ease and accelerate the design flow, special frameworks aim to hide the HDL code from the developer. On the one hand, many frameworks generate HDL code from a programming language like C++ to synthesize hardware from a higher abstraction level. On the other hand, HDL libraries, which instantiate predefined hardware components, are utilized. In contrast to high level synthesis, hardware designs, resulting from such a library, will lead to resource utilizations close to hand written implementations. Therefore, we propose a library of highly configurable IP blocks and demonstrate how they can be used on different Altera and Xilinx FPGAs. Our blocks are designed in a generic way, which makes the design very flexible in several functional parameters. At the current stage of our block library, it is possible to synthesize hardware for common local operations like Sobel, Laplacian or Median filter, but also complex operations like stereo matching and Canny edge detector. Moreover, we designed an XML based language interface, that gives users, who have only low specific hardware knowledge, access to predefined filter operations. With these features a rapid implementation of image processing operators for FPGA designs becomes possible."""	image processing	Konrad Häublein;Christian Hartmann;Marc Reichenbach;Dietmar Fey	2016		10.1007/978-3-319-30481-6_24	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language	EDA	3.1817028272719505	49.88704063589584	87847
f0477824427c8612f619e0dec63885e2ae9f2870	microprocessors for roots-of-trust	embedded systems;sensitivity;software algorithms;security;algorithm design and analysis;timing	Presents a collection of slides covering the following topics: roots-of-trust; microprocessors; encoding; ISA attack methodology; and TrustGUARD secure processor.	industry standard architecture;microprocessor	Kristopher Carver	2013	2013 IEEE Hot Chips 25 Symposium (HCS)	10.1109/HOTCHIPS.2013.7478322	software security assurance;computer architecture;parallel computing;real-time computing;computer science	Theory	4.935607244708977	50.14323126565321	87868
89a0d3584af8a992e2b8b3d55a2e45371447d37f	zycap: efficient partial reconfiguration management on the xilinx zynq	qa76 electronic computers computer science computer software;reconfigurable computing accelerator architectures field programmable gate arrays fpgas;program processors throughput hardware field programmable gate arrays random access memory computer architecture;dma controller zycap efficient partial reconfiguration management xilinx zynq hybrid fpga platform reconfigurable computing software applications hardware resource leveraging accelerator reconfiguration reconfiguration overheads process management architecture partial reconfiguration open source controller high throughput configuration high level software interface;reconfigurable architectures field programmable gate arrays file organisation microprocessor chips;tk electrical engineering electronics nuclear engineering	New hybrid FPGA platforms that couple processors with a reconfigurable fabric, such as the Xilinx Zynq, offer an alternative view of reconfigurable computing where software applications leverage hardware resources through the use of often reconfigured accelerators. For this to be feasible, reconfiguration overheads must be reduced so that the processor is not burdened with managing the process. We discuss partial reconfiguration (PR) on these architectures, and present an open source controller, ZyCAP, that overcomes the limitations of existing methods, offering more effective use of hardware resources in such architectures. ZyCAP combines high-throughput configuration with a high-level software interface that frees the processor from detailed PR management, making PR on the Zynq easy and efficient.	central processing unit;device driver;field-programmable gate array;high- and low-level;high-throughput computing;linux;open-source software;operating system;reconfigurable computing;throughput	Kizheppatt Vipin;Suhaib A. Fahmy	2014	IEEE Embedded Systems Letters	10.1109/LES.2014.2314390	embedded system;computer architecture;parallel computing;reconfigurable computing;computer science;operating system	Arch	0.9466064886074739	49.19500829159673	87885
3ed86a8d1fce6d319f2b38a3198a2e3b6a883f73	system architectural design of a hardware engine for moving target ipv6 defense over ieee 802.3 ethernet		The Department of Homeland Security Cyber Security Division (CSD) chose Moving Target Defense as one of the fourteen primary Technical Topic Areas pertinent to securing federal networks and the larger Internet. Moving Target Defense over IPv6 (MT6D) employs an obscuration technique offering keyed access to hosts at a network level without altering existing network infrastructure. This is accomplished through cryptographic dynamic addressing, whereby a new network address is bound to an interface every few seconds in a coordinated manner. The goal of this research is to produce a Register Transfer Level (RTL) network security processor implementation to enable the production of an Application Specific Integrated Circuit (ASIC) variant of MT6D processor for wide deployment. RTL development is challenging in that it must provide system level functions that are normally provided by the Operating System's kernel and supported libraries. This paper presents the architectural design of a hardware engine for MT6D (HE-MT6D) and is complete in simulation. Unique contributions are an inline stream-based network packet processor with a Complex Instruction Set Computer (CISC) architecture, Network Time Protocol listener, and theoretical increased performance over previous software implementations.	academy;application-specific integrated circuit;cambridge structural database;cryptography;internet;library (computing);man-in-the-middle attack;mathematical optimization;network address;network packet;network performance;network processor;network security;operating system;processor design;register-transfer level;relevance;simulation;software deployment	Joseph Sagisi;Joseph G. Tront;Randy C. Marchany	2017	MILCOM 2017 - 2017 IEEE Military Communications Conference (MILCOM)	10.1109/MILCOM.2017.8170846	computer network;architecture;network processor;ipv6;register-transfer level;network security;computer science;complex instruction set computing;network packet;network address;computer hardware	Arch	7.637640319073001	46.476065298933605	87912
c707d42e613f17c09078a9b948e4be01ed4284ad	rxv2 processor core for low-power microcontrollers	dual issue core mstruction fetch umt microcontrollcr low power;microcontrollers;microcontrollers registers flash memories digital signal processing pipelines process control power demand;prefetching unit rxv2 processor core architecture low power microcontroller high capacity flash memory peripheral functional module energy consumption instruction fetch set mechanism pipeline structure code density power consumption;microcontrollers flash memories instruction sets low power electronics;low power electronics;flash memories;instruction sets	We have developed a new processor architecture for microcontrollers which integrate high-capacity FLASH memory and many peripheral functional modules. This paper describes processor core architecture for low-power microcontrollers and our approach for reducing energy consumption with instruction fetch mechanisms. A large fraction of the total power budget of the microcontroller is the energy consumption in the path from the FLASH memory to the processor. An enhanced instruction set and pipeline structure provide an effective balance between high code density, power consumption performance and high processing performance with an novel prefetching unit to reduce the number of memory accesses.	cpu cache;flash memory;intel core (microarchitecture);low-power broadcasting;microcontroller;multi-core processor;peripheral	Sugako Otani;Naoshi Ishikawa;Hiroyuki Kondo	2013	2013 IEEE COOL Chips XVI	10.1109/CoolChips.2013.6547914	embedded system;parallel computing;computer hardware;computer science	Arch	-2.914285311562357	53.67869349037993	88012
dee41bf3c96b76c34fbd65f6236b99687d26a2c8	an evaluation of energy efficient microcontrollers	microcontrollers benchmark testing energy consumption clocks power demand oscillators energy measurement;energy consumption energy efficient microcontroller evaluation internet of things current low power off the shelf microcontrollers data sheets power consumption execution time;power consumption microcontrollers	The latest technological trends in many application fields e.g. the Internet of Things call for highly power efficient solutions. These solutions can be realised with current low power off the shelf microcontrollers. However the task of selecting the appropriate microcontroller is more difficult than it seems. The datasheets provide inadequate information which hinders fair comparisons among the different microcontrollers. This work attempts to give a solution to this issue using a set of low power microcontrollers available in our laboratory, together with a set of benchmarks which implement common tasks. We present the results of a comparison among these microcontrollers in terms of execution time, power consumption and energy consumption for the execution of the set of benchmarks.	benchmark (computing);datasheet;internet of things;microcontroller;run time (program lifecycle phase);shutdown (computing)	Ioanna Tsekoura;Gregor Rebel;Peter Gloesekoetter;Mladen Berekovic	2014	2014 9th International Symposium on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2014.6861368	embedded system;real-time computing;computer hardware	Arch	-1.9356245163295254	56.86124442232392	88050
9f643c707fb215ed72b79fcc931d6f4a9d8c10cb	asynchronous design for programmable digital signal processors	data transmission;processor architecture;logic design;very large scale integrated;signal design digital signal processors clocks circuit synthesis timing computer architecture signal processing synchronization circuit simulation cmos logic circuits;system performance;input output;parallel architectures;pipeline processing circuit cad digital signal processing chips logic cad parallel architectures;signal processing;digital systems;digital signal processor;digital signal processing chips;circuit cad;processor architecture asynchronous design input output interface cad programmable digital signal processors pipelining interconnection circuit specifications data flow control program flow control feedback initialization;data flow;flow control;logic cad;central processing unit;pipeline processing	A systematic procedure for designing fully asynchronous programmable processors from an architectural description is described. Design issues such as pipelining, interconnection circuit specifications, data flow control, program flow control, feedback and initialization, I/O (input/output) interface, and processor architecture are discussed. The system-level tradeoffs of using synchronous design versus asynchronous design are addressed. Simulation results of an asynchronous version of a commercial digital signal processor are given. >	asynchronous circuit;central processing unit;digital signal processor	Teresa H. Y. Meng;Robert W. Brodersen;David G. Messerschmitt	1991	IEEE Trans. Signal Processing	10.1109/78.80917	asynchronous system;input/output;data flow diagram;digital signal processor;computer architecture;logic synthesis;real-time computing;asynchronous circuit;microarchitecture;computer science;central processing unit;asynchronous communication;signal processing;flow control;asynchronous array of simple processors;data transmission;synchronizer	EDA	6.402087748595476	50.6931114903707	88131
78857eb33ef1df9b046673354741f49a91cf7c1c	gabind: a ga approach to allocation and binding for the high-level synthesis of data paths	force directed completion algorithm;concepcion circuito;architecture systeme;interconnection;cost function;integrated circuit;canal bus;data path;processor scheduling;bus interconnection;circuit design;multiple solution;government;canal colector;crossover;circuito integrado;design parameters;synthese haut niveau;indexing terms;multicycling;allocation;algoritmo genetico;flow graphs;data path synthesis;interconexion;data flow graph;high level synthesis;scheduling algorithm;registers;optimal scheduling;interconnexion;force measurement;algorithme genetique;bus channel;high level synthesis genetic algorithms scheduling algorithm processor scheduling cost function force measurement pipeline processing flow graphs registers government;chemin donnees;genetic algorithm;arquitectura sistema;genetic algorithms;gabind;conception circuit;pipelining;crossover gabind high level synthesis data path synthesis genetic algorithm force directed completion algorithm bus interconnection design parameters multiport memory multicycling pipelining allocation binding;functional unit;genetic algorithms high level synthesis;system architecture;binding;multiport memory;circuit integre;pipeline processing	We present here a technique for allocation and binding for data path synthesis (DPS) using a Genetic Algorithm (GA) approach. This GA uses an unconventional crossover mechanism relying on a force directed data path binding completion algorithm. The data path is synthesized using some supplied design parameters. A bus-based interconnection scheme, use of multi-port memories, and provision for multicycling and pipelining are the main features of this system. The method presented here has been applied to standard benchmark examples and the results obtained are promising.	benchmark (computing);genetic algorithm;high- and low-level;high-level synthesis;interconnection;pictbridge;pipeline (computing);software release life cycle	Chittaranjan A. Mandal;P. P. Chakrabarti;Sujoy Ghose	2000	IEEE Trans. VLSI Syst.	10.1109/92.902270	embedded system;electronic engineering;parallel computing;real-time computing;genetic algorithm;computer science;electrical engineering;theoretical computer science;operating system;algorithm;systems architecture	EDA	1.1068527193966147	52.96273616344204	88227
37a20839268657b9b34117daa28f1c15997eb980	testing and verification of hdl-models for soc components	observability;analytical models;software;controllability observability software testing registers engines analytical models;soc components;controllability;hardware description languages;testing;testable analysis soc components testing technology verification technology digital systems logical structure hdl program assertion engine time to market;system on chip hardware description languages;chip;engines;testing technology;system on chip;registers;digital systems;verification technology;assertion engine;logical structure hdl program;time to market;testable analysis;development time;simulation environment	The testing and verification technology for system HDL models, focused to the significant improvement of the quality of design components for digital systems on chips and reduction the development time (time-to-market) by using the simulation environment, testable analysis of the logical structure HDL-program and the optimal placement of assertion engine is proposed.	digital electronics;hardware description language;simulation;system on a chip	Vladimir Hahanov;Irina V. Hahanova;Christopher Umerah Ngene;Tiecoura Yves	2010	2010 East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2010.5742112	electronic engineering;verification and validation of computer simulation models;real-time computing;verification;computer science;theoretical computer science;functional verification	EDA	8.172788906991824	52.48983045197095	88377
2ee31d72f003eaa10fb8b1413241830de22d40d3	design of small reconfigurable embedded numerical control system	computers;numerical control embedded system s3c2410;microprocessors;machining;reconfigurable embedded numerical control system;control systems;s3c2410;keyboards;interpolation;motion control;wince4 2 reconfigurable embedded numerical control system resources waste reconfigurable numerical control machining center embedded microprocessor s3c2410 g codes;embedded microprocessor s3c2410;reconfigurable numerical control machining center;embedded system;production engineering computing;resources waste;computer architecture;embedded systems;computerised numerical control;production engineering computing computerised numerical control embedded systems microprocessor chips;registers;servomechanisms;milling machines;universal serial bus;circuits;wince4 2;numerical control;field programmable gate arrays;g codes;microprocessor chips;computer numerical control;machining interpolation keyboards registers computer numerical control computers field programmable gate arrays	The traditional numerical control system is independent and close. Different manufacturers are not compatible with each other which lead to much time and resources waste. A small reconfigurable numerical control machining center based on embedded microprocessor S3C2410 is implemented in the paper. Each module in the system is connected by standard interfaces which make it easy to reconstruct or expand. A general architecture of G codes interpreting under winCE4.2 and the method of embedded system to access external devices are discussed.	code;control system;embedded system;list of samsung system on chips;memory-mapped i/o;microprocessor;nc (complexity);numerical analysis;operating system;usb	Dianhong Wang;Xiaoyong Ni	2008	2008 International Conference on Embedded Software and Systems	10.1109/ICESS.2008.37	embedded system;computer hardware;computer science;control system;operating system;numerical control	Embedded	7.084795405421115	48.82709955771768	88467
4294164fef7928e01b2065a3a6b35df113f508d5	bridging pre-silicon verification and post-silicon validation	verification;silicon;observability;full system functional verification pre silicon verification post silicon validation simulation technique emulation technique;functional verification;full system functional verification;logic design;controllability;pre silicon;emulation technique;testing;pre silicon verification;system on a chip;post silicon validation;formal verification;simulation technique;post silicon;validation pre silicon;validation;computer bugs;post silicon verification validation pre silicon;logic design formal verification;testing silicon hardware observability computer bugs controllability costs emulation engines signal analysis;hardware	Post-silicon validation is a necessary step in a design's verification process. Pre-silicon techniques such as simulation and emulation are limited in scope and volume as compared to what can be achieved on the silicon itself. Some parts of the verification, such as full-system functional verification, cannot be practically covered with current pre-silicon technologies. This panel brings together experts from industry, academia, and EDA to review the differences and similarities between pre- and post-silicon, discuss how the fundamental aspects of verification are affected by these differences, and explore how the gaps between the two worlds can be bridged.	bridging (networking);data validation;electronic design automation;emulator;simulation	Amir Nahir;Avi Ziv;Rajesh Galivanche;Alan J. Hu;Miron Abramovici;Albert Camilleri;Bob Bentley;Harry Foster;Valeria Bertacco;Shakti Kapoor	2010	Design Automation Conference	10.1145/1837274.1837300	system on a chip;embedded system;electronic engineering;logic synthesis;real-time computing;verification;observability;software bug;controllability;formal verification;computer science;software testing;silicon;post-silicon validation;functional verification;computer engineering	EDA	9.145786074747475	53.83461251526062	88508
2fa55a467fef4c202305031f24c7ec5724ffa62d	automated estimation of power consumption for rapid system level design	space exploration;transform coding;estimation;monitoring;energy consumption;power dissipation;power demand	This paper describes an early power estimation method for Electronic System Level(ESL) design, which provides a scalable API to support automated power profiling and analysis at the early stages of the design process. The proposed framework utilizes a high-level power modeling mechanism along with an automated profiler to extract energy activity from the simulated system model. These two features are integrated into PowerMeter, a framework that automatically annotates power meters as well as energy and performance functions into the executable model. This integrated profiling helps the designer to rapidly explore the design space, trading off performance against power cost in order to make best design decisions. Our approach also provides the designer with the ability to quantify the effect of revisions in the ESL design models, in terms of both power and performance. Despite the high abstraction level, our results show that the PowerMeter delivers rapid estimates with high fidelity and at minimal cost.	level design	Yasaman Samei Syahkal;Rainer Dömer	2014		10.1109/PCCC.2014.7017085	embedded system;estimation;real-time computing;transform coding;simulation;telecommunications;dissipation;space exploration;operating system;computer security;statistics	EDA	2.354732785334685	54.78636115538027	88644
7301318bded183ef44133fa9c7c108d8d44d1607	a case study: synthesis and exploration of instruction set design for application-specific symbolic computing	architectural analysis;application specific instruction set processor;instruction set architecture synthesis;design exploration	The design of computer instruction sets has been mostly considered as being a manual process, due to complications between hardware and software, and the lack of suitable design tools. The manual process limits understanding of the hardware/ software interface and tradeoffs. Motivated by this limitation, the design automation system ASIA (Automatic Synthesis for Instruction-set Architecture) was developed to systematize the design process for instruction sets. This paper presents a case study of using ASIA in synthesis and design exploration for application specific Symbolic (Prolog) computing. Instruction sets are optimized for given applications while maintaining their support for general Prolog execution. The results are compared with the VLSI-BAM processor, a manually-designed, general purpose instruction set processor for Prolog. The experiments show that: (1) the systematic and quantitative approach to instruction set design is feasible; (2) the design space of application-specific instruction sets consists of multiple dimensions across hardware and software, and ASIA is capable of exploiting such a complex design space and managing the hardware/software interactions; (3) the architectural properties of software applications vary significantly; therefore, (4) application specific instruction sets can provide better performance/cost tradeoff than can the general purpose instruction set processor in the context of specific application domains.	assembly language;central processing unit;computer architecture;computer hardware;concatenation;cycle count;experiment;functional programming;graphical user interface;imperative programming;interaction;logic programming;programming language;prolog;romp;relational operator;symbolic computation;very-large-scale integration	Ing-Jer Huang	1998	J. Inf. Sci. Eng.		theoretical computer science;orthogonal instruction set;design tool;application-specific instruction-set processor;computer architecture;minimal instruction set computer;instruction set;one instruction set computer;computer science;design process;addressing mode	EDA	1.8875253852742522	50.16047495298446	88748
d9dca9a2c510e89a617a815e7fac8d386b259a7d	microprogramming in a multiprocessor data acquisition system	multiprocessor data acquisition system;data acquisition system	multiprocessor data acquisition system;data acquisition system	data acquisition;microcode;multiprocessing	Sergio D'Angelo;L. Lisca;A. Proserpio;Giacomo R. Sechi	1988	[1988] Proceedings of the 21st Annual Workshop on Microprogramming and Microarchitecture - MICRO '21	10.1145/62504.62513	embedded system;computer science;automatic control;data acquisition	Arch	4.581048879807521	48.11131684565456	88803
524bee75fc8deeb1504d1d906a2ee7429ae27246	from opencl to high-performance hardware on fpgas	peripheral interfaces;logic design;program compilers field programmable gate arrays logic design peripheral interfaces;kernel field programmable gate arrays instruction sets hardware hardware design languages computer architecture computational modeling;field programmable gate arrays;program compilers;opencl computing paradigm high performance hardware opencl compilation framework fpga verilog hdl altera complete design suite 12 0 windows based machine pcie interface	We present an OpenCL compilation framework to generate high-performance hardware for FPGAs. For an OpenCL application comprising a host program and a set of kernels, it compiles the host program, generates Verilog HDL for each kernel, compiles the circuit using Altera Complete Design Suite 12.0, and downloads the compiled design onto an FPGA.We can then run the application by executing the host program on a Windows(tm)-based machine, which communicates with kernels on an FPGA using a PCIe interface. We implement four applications on an Altera Stratix IV and present the throughput and area results for each application. We show that we can achieve a clock frequency in excess of 160MHz on our benchmarks, and that OpenCL computing paradigm is a viable design entry method for high-performance computing applications on FPGAs.	clock rate;compiler;field-programmable gate array;hardware description language;offset binary;opencl api;pci express;programming paradigm;stratix;supercomputer;throughput;verilog	Tomasz S. Czajkowski;Utku Aydonat;Dmitry Denisenko;John Freeman;Michael Kinsner;David Neto;Jason Wong;Peter Yiannacouras;Deshanand P. Singh	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339272	embedded system;computer architecture;parallel computing;logic synthesis;computer science;operating system;field-programmable gate array	EDA	0.1322935114885192	47.32979751662411	89126
7e6cc80dd4def5a88abc0ea34dc220a81078f7a3	a c-to-rtl flow as an energy efficient alternative to embedded processors in digital systems	energy efficiency;energy conservation;linear algebra;linear algebra application;c to rtl flow;vlsi application specific integrated circuits embedded systems energy conservation high level synthesis microcomputers power aware computing;low power processor;energy efficient;embedded processors high level synthesis energy efficiency correctness scalability;correctness;embedded systems;high level synthesis;power aware computing;hardware program processors delay labeling optimization algorithm design and analysis digital systems;digital system;low power;stream cipher;energy consumption;application specific integrated circuits;asic tool chain;digital systems;power dissipation;energy efficient alternative;resource sharing;embedded processors;vlsi;place and route;optimization;high level synthesis flow;scalability;digital vlsi system;algorithm description;register transfer level;embedded processor;program processors;algorithm design and analysis;microcomputers;labeling;digital vlsi system c to rtl flow energy efficient alternative embedded processor digital system high level synthesis flow algorithm description provably equivalent register transfer level description stream cipher linear algebra application asic tool chain power dissipation energy consumption low power processor;provably equivalent register transfer level description;hardware;intermediate representation	We present a high-level synthesis flow for mapping an algorithm description (in C) to a provably equivalent register transfer level (RTL) description of hardware. This flow uses an intermediate representation which is an orthogonal factorization of the program behavior into control, data and memory aspects, and is suitable for the description of large systems. We show that optimizations such as arbiter-less resource sharing can be efficiently computed on this representation. We apply the flow to a wide range of examples ranging from stream ciphers to database and linear algebra applications. The resulting RTL is then put through a standard ASIC tool chain (synthesis followed by automatic place-and-route), and the performance and power dissipation of the resulting layout is computed. We observe that the energy consumption (per completed task) of each resulting circuit is considerably lower than that of an equivalent executable running on a low-power processor, indicating that this C-to-RTL flow offers an energy efficient alternative to the use of embedded processors in mapping algorithms to digital VLSI systems.	algorithm;arbiter (electronics);c to hdl;central processing unit;embedded system;executable;experiment;high- and low-level;high-level synthesis;intermediate representation;linear algebra;low-power broadcasting;microprocessor;performance per watt;place and route;stream cipher;toolchain;vhdl;very-large-scale integration	Sameer D. Sahasrabuddhe;Sreenivas Subramanian;Kunal P. Ghosh;Kavi Arya;Madhav P. Desai	2010	2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools	10.1109/DSD.2010.52	embedded system;computer architecture;parallel computing;real-time computing;computer science;linear algebra;operating system;efficient energy use	EDA	2.1288313307943185	51.685507313941464	89165
5a93e6bdd9daafc1bbe2121cc81e1c0fc5a348d5	a pervasive smart camera network architecture applied for multi-camera object classification	application development;software;smart cameras intelligent sensors hardware joining processes computer architecture read write memory energy consumption software architecture software standards standards development;shared memory;performance evaluation;wireless sensor networks cameras data flow computing image classification image sensors linux software architecture;wireless network;image classification;image sensors;data mining;function block;sensor network;software architecture;visualization;ieee 802 11 standards;smart cameras;software component;data flow computing;middleware;linux;object classification;shared memory pervasive smart camera network architecture multicamera object classification visual sensor networks off the shelf hardware software components omap 3530 processor ram dual radio wireless network software architecture linux dataflow oriented application development performance evaluation;power consumption;high throughput;camera network;off the shelf;wireless sensor networks;cameras	Visual sensor networks are an emerging research area with the goal of using cameras as pervasive and affordable sensing and processing devices. This paper presents a pervasive smart camera platform which is built from off-the-shelf hardware and software components. The hardware platform is comprised of an OMAP 3530 processor, 128 MB RAM and various interfaces for connecting sensors and peripherals. A dual-radio wireless network allows to trade communication performance for power consumption. The software architecture is built upon standard Linux and supports dataflow oriented application development by dynamically instantiating and connecting functions blocks. Data is transferred between blocks via shared memory for high throughput. We present a performance evaluation of our smart camera platform as well as a multi-camera object classification system to demonstrate the capabilities and applicability of our approach.	arm cortex-a8;arm architecture;asynchronous circuit;autonomy;blocking (computing);component-based software engineering;computation;dataflow;image processing;inter-process communication;linux;neon (light synthesizer);network architecture;omap;online and offline;online machine learning;opencv;performance evaluation;peripheral;random-access memory;requirement;run time (program lifecycle phase);sensor;shared memory;smart camera;software architecture;software framework;systems architecture;throughput	Wolfgang Schriebl;Thomas Winkler;Andreas Starzacher;Bernhard Rinner	2009	2009 Third ACM/IEEE International Conference on Distributed Smart Cameras (ICDSC)	10.1109/ICDSC.2009.5289377	smart camera;embedded system;real-time computing;wireless sensor network;computer science;operating system	Mobile	3.094024184017749	49.05673577823214	89252
72a035220dc2420af39feb7d169d267f1b56797d	extending amdahl’s law and gustafson’s law by evaluating interconnections on multi-core processors	multi core processor;interconnection;chip area;gustafson s law;model;amdahl s law	Multicore chips are emerging as the mainstream solution for high performance computing. Generally, communication overheads cause large performance degradation in multi-core collaboration. Interconnects in large scale are needed to deal with these overheads. Amdahl’s and Gustafson’s law have been applied to multi-core chips but inter-core communication has not been taken into account. In this paper, we introduce interconnection into Amdahl’s and Gustafson’s law so that these laws work more precisely in the multi-core era. We further propose an area cost model and analyse our speedup models under area constraints. We find optimized parameters according to our speedup model. These parameters provide useful feedbacks to architects at an initial phase of their designs. We also present a case study to show the necessity of incorporating interconnection into Amdahl’s and Gustafson’s law.	amdahl's law;analysis of algorithms;central processing unit;computation;electrical connection;elegant degradation;feedback;gustafson's law;intel core (microarchitecture);interconnection;multi-core processor;speaker wire;speedup;supercomputer	Tian Huang;Yongxin Zhu;Meikang Qiu;Xiaojing Yin;Xu Wang	2013	The Journal of Supercomputing	10.1007/s11227-013-0908-9	multi-core processor;amdahl's law;parallel computing;speedup;computer science;theoretical computer science;interconnection;distributed computing;concurrent data structure;karp–flatt metric;gustafson's law;algorithm	HPC	-3.935261931179586	54.424500092672496	89255
a977d95eb4397f76eddaa38b228c72ddf29b0236	fpga-based reconfigurable computing iii			field-programmable gate array;reconfigurable computing	Dan Chia-Tien Lo;J. Morris Chang	2007	Microprocessors and Microsystems	10.1016/j.micpro.2007.02.002	piperench;parallel computing;reconfigurable computing;computer science;fpgac	EDA	4.423458045891888	49.17798886560854	89384
13dc8f81f96843a0c7200cf3b386de4035efcbf4	headroom and legroom in the 80960 architecture	satisfiability;computer architecture application software costs hardware microprocessors silicon investments product design image processing motor drives;32 bit 32 bit processor code compatible performance needs;microprocessor chips	Designing with a 32-bit processor requires a major investment of both hardware and software resources. Companies today rarely introduce successful 'one size fits all' products, but rather must design several products with varying degrees of features, performance and cost, to satisfy the marketplace. Once a decision is made on a 32-bit processor, it is highly desirable that this processor fit the needs of these end products across the price/feature/cost range. The Intel 80960 architecture satisfies this need by providing products which are code compatible and span a wide range of performance needs.<<ETX>>	32-bit;fits;intel i960	Thar Baker	1990	Digest of Papers Compcon Spring '90. Thirty-Fifth IEEE Computer Society International Conference on Intellectual Leverage	10.1109/CMPCON.1990.63691	embedded system;real-time computing;computer hardware;computer science	Arch	9.062874025751864	55.54550419459774	89385
3358925759400edc00b35034654990e775465c94	vlsi processor system for robotics	intelligent integrated systems;robot electronics;vlsi processor;small delay time;hardware subroutine		robotics;very-large-scale integration	Michitaka Kameyama;Yoshichika Fujioka	1996	JRM	10.20965/jrm.1996.p0496	embedded system;computer architecture;computer science;computer engineering	Robotics	5.672255327435929	49.58002603450748	89421
95208faf84b6846d8f359b55c9ca1b269735eb90	modeling, analysis and exploration of layers: a 3d computing architecture	computer architecture three dimensional displays kernel solid modeling silicon program processors hardware;coarse grained reconfigurable architecture cgra;3d silicon technology 3d computing architecture layered reconfigurable architecture high energy efficiency memory bandwidth functional reconfiguration theoretical concepts functional programming theory high level design methodology application mapping flow system integration 3d structure;functional architecture high level modeling adl modeling coarse grained reconfigurable architecture cgra 3d architecture functional reconfigurability;three dimensional integrated circuits circuit simulation energy conservation functional programming high level synthesis integrated circuit design integrated circuit modelling integrated circuit testing reconfigurable architectures;functional architecture;adl modeling;3d architecture;functional reconfigurability;high level modeling	A new layered reconfigurable architecture is proposed which exploits modularity, scalability and flexibility to achieve high energy efficiency and memory bandwidth. Functional Reconfiguration theoretical concepts are proposed to describe this kind of architectures, based on concepts from functional programming theory. A high-level design methodology is adapted and modified to allow easy design, testing and simulation. The architectural concepts are tested on an application domain and several tools are created to partially automate application mapping flow and system integration. Moreover, due to its inherent 3D structure of the proposed architecture, physical implementation into 3D silicon is attempted.	3d printing;application domain;computer architecture;functional programming;high- and low-level;level design;memory bandwidth;scalability;simulation;system integration	Zoltán Endre Rákossy	2014	2014 22nd International Conference on Very Large Scale Integration (VLSI-SoC)	10.1109/VLSI-SoC.2014.7004167	dataflow architecture;reference architecture;space-based architecture;computer architecture;parallel computing;computer science;applications architecture;cellular architecture;data architecture;computer engineering	EDA	4.776423702660619	51.75949663558977	89499
eb5c6f9e9cb57b7cca27ba26acb5c46439c73d2b	a model for matrix multiplication performance on fpgas	conference_paper;technology trend;performance model fpga matrix multiplication technology trend;future technologies;dense matrices;computational resources;spectrum;fpga;sparse matrices field programmable gate arrays matrix multiplication;sparse matrices field programmable gate arrays bandwidth system on a chip computational modeling matrix decomposition schedules;performance model;matrix multiplication;dense matrix multiplication fpga accelerator architectural parameter system parameter computational resource memory i o bandwidth sparse matrix vector matrix matrix multiplication;analytic models;field programmable gate arrays;sparse matrices;i o bandwidth;analytical model	Computations involving matrices form the kernel of a large spectrum of computationally demanding applications for which FPGAs have been utilized as accelerators. Their performance is related to their underlying architectural and system parameters such as computational resources, memory and I/O bandwidth. A simple analytic model that gives an estimate of the performance of FPGA-based sparse matrix-vector and matrix-matrix multiplication is presented, dense matrix multiplication being a special case. The efficiency of existing implementations are compared to the model and performance trends for future technologies examined.	computation;computational resource;field-programmable gate array;glossary of computer graphics;input/output;matrix multiplication;sparse matrix	Colin Yu Lin;Hayden Kwok-Hay So;Philip Heng Wai Leong	2011	2011 21st International Conference on Field Programmable Logic and Applications	10.1109/FPL.2011.62	embedded system;parallel computing;computer science;theoretical computer science;field-programmable gate array	HPC	-2.7234496540145385	46.509477760413624	89699
4371ad0b1b87e58fff3b83b93017033cfebe0d9c	transaction-level power analysis of vlsi digital systems	power estimation;transaction level modeling;systemc language;vlsi digital systems	The increasing complexity of VLSI digital systems has dramatically supported system-level representations in modeling and design activities. This evolution makes often necessary a compliant rearrangement of the modalities followed in validation and analysis tasks, as in the case of power performances estimation. Nowadays, transaction-level paradigms are having a wider and wider consideration in the research on electronic system-level design techniques. With regard to the available modeling resources, the most relevant framework is probably the transaction-level extension of the SystemC language (SystemC/TLM), which therefore represents the best platform for defining transaction-level design techniques. In this paper we present a macro-modeling power estimation methodology valid for SystemC/TLM prototypes and of general applicability. The present discussion illustrates the implementation modalities of the proposed approach, verifying its effectiveness through a comparison with RTL estimation techniques.	digital electronics;electronic system-level design and verification;finite impulse response;kilobyte;level design;partial template specialization;performance;random-access memory;simulation;systemc;transform, clipping, and lighting;verification and validation;very-large-scale integration	Giovanni B. Vece;Massimo Conti;Simone Orcioni	2015	Integration	10.1016/j.vlsi.2015.02.003	embedded system;computer architecture;transaction-level modeling;electronic engineering;real-time computing;computer science	EDA	6.239541441780604	52.98673794064674	89844
2c26f588e3502028c72dc81770c2af680bb4745e	exploiting dynamic timing margins in microprocessors for frequency-over-scaling with instruction-based clock adjustment	benchmark testing;reduced instruction set computing;optimization;energy efficiency;microarchitecture;resistive memory;register file;instruction sets;pipelines;reliability;soft error;static timing analysis;cmos technology;gpgpu;synchronisation	Static timing analysis provides the basis for setting the clock period of a microprocessor core, based on its worst-case critical path. However, depending on the design, this critical path is not always excited and therefore dynamic timing margins exist that can theoretically be exploited for the benefit of better speed or lower power consumption (through voltage scaling). This paper introduces predictive instruction-based dynamic clock adjustment as a technique to trim dynamic timing margins in pipelined microprocessors. To this end, we exploit the different timing requirements for individual instructions during the dynamically varying program execution flow without the need for complex circuit-level measures to detect and correct timing violations. We provide a design flow to extract the dynamic timing information for the design using post-layout dynamic timing analysis and we integrate the results into a custom cycle-accurate simulator. This simulator allows annotation of individual instructions with their impact on timing (in each pipeline stage) and rapidly derives the overall code execution time for complex benchmarks. The design methodology is illustrated at the microarchitecture level, demonstrating the performance and power gains possible on a 6-stage OpenRISC in-order general purpose processor core in a 28nm CMOS technology. We show that employing instruction-dependent dynamic clock adjustment leads on average to an increase in operating speed by 38% or to a reduction in power consumption by 24%, compared to traditional synchronous clocking, which at all times has to respect the worst-case timing identified through static timing analysis.	best, worst and average case;cmos;clock rate;computer architecture simulator;critical path method;dynamic voltage scaling;embedded system;error detection and correction;image scaling;microarchitecture;microprocessor;multi-core processor;online algorithm;openrisc;requirement;run time (program lifecycle phase);static timing analysis	Jeremy Constantin;Lai Wang;Georgios Karakonstantis;Anupam Chattopadhyay;Andreas Peter Burg	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;computer architecture;parallel computing;real-time computing;soft error;resistive random-access memory;microarchitecture;computer science;operating system;reliability;timing failure;efficient energy use;static timing analysis;register file;general-purpose computing on graphics processing units	EDA	-2.748482111972173	55.564877494129185	89858
0ddfd1af1bc1e63870f9a36c4ca07281f7688cc7	a visual simulation environment for mips based on vhdl	key words:	An application to perform a visual simulation of a machine based on MIPS is presented in this paper. The advantage of this system in relation to conventional simulators is that the simulation engine is the result of a real simulation under a VHDL development environment, so that hardware description can be modified and simulated in several ways to probe and study	augmented reality;simulation;vhdl	J. M. Álvarez;Nieves Pavón;J. Ballesteros	2000		10.1007/0-306-47532-4_6	embedded system;real-time computing;computer hardware;vhdl;computer science	EDA	5.164829653723196	51.582050780062396	89907
598a95e17737fbeff354b81ccdf3f89ff8850bd5	design decisions and design rationale in software architecture	article letter to editor;software architecture;design rationale	A circuit and method for limiting voltage swing on the complementary bit lines of a memory device. Complementary bit lines of the memory device are coupled to a sense amplifier through first and second p-channel isolation devices. A low voltage is applied to a gate of the p-channel isolation devices to activate the p-channel isolation devices such that one of the first and second p-channel isolation devices establishes the low logic level on one of the complementary bit lines at a voltage that limits the swing on the complementary bit lines.	design rationale;software architecture	Muhammad Ali Babar;Patricia Lago	2009	Journal of Systems and Software	10.1016/j.jss.2009.05.053	software architecture;idef6;computer science;systems engineering;software design;software design description;software engineering;software architecture description;design rationale	SE	9.925275157969331	53.800968417374754	90358
0cc45a292221002902d79820b7c5b551a0b528bb	realization of intelligent optimization algorithm on ip cores partition for noc testing	parallel algorithms benchmark testing embedded systems integrated circuit testing minimisation network on chip neural nets;ip networks testing hopfield neural networks partitioning algorithms optimization algorithm design and analysis artificial neural networks;embedded ip core testing matlab itc 02 benchmark circuits distributed parallel information processing algorithm mathematical model artificial neural network method noc testing intelligent optimization algorithm parallel testing time minimization ip core partition problem noc based soc	According to the principles of nature or the biosphere, people imitate its rules to create the intelligent algorithm for solving problems. Nowadays there are simulated annealing algorithm, genetic algorithms, artificial neural networks, swarm intelligence algorithms and so on. With the specific testing access architecture, testing embedded IP cores of NoC-based SoC can be altered to IP cores partition problem. The optimization object is to minimize the parallel testing time. The paper proposed the realization of intelligent optimization algorithm to group IP cores for NoC testing by artificial neural network method. The artificial neural network is an algorithm mathematical model of distributed parallel information processing. The experiment results of ITC'02 benchmark circuits on MATLAB showed the effectiveness of the proposed intelligent optimization algorithm. The parallel testing time of embedded IP cores on NoCs decrease by averagely 5.857% (W=16) and 5.65% (W=32).	artificial neural network;benchmark (computing);biosphere;embedded system;genetic algorithm;hopfield network;information processing;limit cycle;matlab;mathematical model;mathematical optimization;network on a chip;partition problem;semiconductor intellectual property core;simulated annealing;simulation;swarm intelligence	Yunhui Ling;Fang Liu	2015	2015 IEEE 11th International Conference on ASIC (ASICON)	10.1109/ASICON.2015.7517183	parallel computing;computer science;theoretical computer science;distributed computing	EDA	9.880905714317274	49.04874086904428	90473
8670a71c1670b8fb8ae74e7720404edc2d2a516b	bluetooth security implementation based on software oriented hardware-software partition	system on chip bluetooth hardware software codesign telecommunication security;bluetooth hardware partitioning algorithms authentication timing frequency information security algorithm design and analysis resource management communication system security;hardware software codesign;information security;authentication;resource management;system on chip;hardware software partitioning;resource sharing scheme bluetooth security algorithm hardware software codesign;bluetooth security algorithm;resource sharing;telecommunication security;bluetooth;frequency;algorithm design and analysis;partitioning algorithms;communication system security;hardware;resource sharing scheme;timing	In this paper, we analyze the Bluetooth security algorithm and implement the design in hardware and software. Software-oriented hardware-software partitioning method is applied to this design and resource-sharing scheme is also adopted. The partitioning and resource sharing reduce hardware resources though the timing requirement is also fulfilled.	algorithm;bluetooth	Gyongsu Lee;Sin-Chong Park	2005	IEEE International Conference on Communications, 2005. ICC 2005. 2005	10.1109/ICC.2005.1494702	software security assurance;system on a chip;shared resource;embedded system;algorithm design;real-time computing;computer science;information security;resource management;frequency;authentication;bluetooth;computer network	EDA	4.1179834677562175	56.23163916498649	90722
33640b7d297681c9cc4bef1029f20a746af0fc5d	diverse double modular redundancy: a new direction for soft-error detection and correction	reliability soft error modular redundancy error correction;digital signal processing;reliability;finite impulse response filter redundancy digital signal processing error correction integrated circuit reliability tunneling magnetoresistance error analylsis software reliability;finite impulse response filter;diverse double modular redundancy;journal article;integrated circuit design;redundancy;error correction;soft error detection;modular redundancy;soft error correction diverse double modular redundancy soft error detection;radiation hardening electronics;integrated circuit reliability;tunneling magnetoresistance;radiation hardening electronics integrated circuit design;error analylsis;software reliability;soft error correction;soft error	By introducing diversity between original and redundant modules, diverse double modular redundancy is a promising solution to mitigate the impact of soft errors.	digital signal processor;error detection and correction;error-tolerant design;integrated circuit;period-doubling bifurcation;soft error	Pedro Reviriego;Chris J. Bleakley;Juan Antonio Maestro	2013	IEEE Design & Test	10.1109/MDT.2012.2232964	triple modular redundancy;dual modular redundancy;electronic engineering;real-time computing;error detection and correction;soft error;computer science;electrical engineering;digital signal processing;finite impulse response;reliability;redundancy;software quality;statistics;integrated circuit design	Embedded	9.326283046353096	59.89863034102126	90844
9911714bbf86cbdc76de29ca9b4175979cfdef82	artificial intelligence of blokus duo on fpga using cyber work bench	game theory;logic design;frequency 100 mhz artificial intelligence fpga based blokus duo solver minimax algorithm game tree alpha beta pruning move ordering hls tool cyberworkbench cwb parallel fully pipelined design xilinx spartan 6 xc6slx45 fpga digilent atlys board;trees mathematics artificial intelligence field programmable gate arrays game theory logic design minimax techniques;trees mathematics;minimax techniques;artificial intelligence;field programmable gate arrays;games registers tiles field programmable gate arrays algorithm design and analysis hardware clocks	This paper presents a design of an FPGA-based Blokus Duo solver. It searches a game tree by using the miniMax algorithm with alpha-beta pruning and move ordering. In addition, HLS tool called CyberWorkBench (CWB) is used to implement hardware. By making the use of functions in CWB, parallel fully pipelined design is generated. The implemented solver works at 100MHz with Xilinx Spartan-6 XC6SLX45 FPGA on the Digilent Atlys board. It can search states after three moves in most cases.	algorithm;alpha–beta pruning;artificial intelligence;field-programmable gate array;high-level synthesis;minimax;pipeline (computing);solver;traverse;technical standard;tree (data structure)	Naru Sugimoto;Takaaki Miyajima;Takuya Kuhara;Yuki Katuta;Takushi Mitsuichi;Hideharu Amano	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718427	embedded system;game theory;logic synthesis;simulation;computer science;theoretical computer science;operating system;field-programmable gate array	EDA	9.688366317463018	47.69425801530629	90891
7112cd52a71d7a2158675c8a75f7d46301152b1b	a transformation-based approach for storage optimization	digital signal processing;constraint optimization;multiplication operator;processor scheduling;data management;code generation;software systems;high level synthesis;digital signal processing registers application specific processors software systems processor scheduling integrated circuit synthesis signal synthesis application specific integrated circuits high level synthesis constraint optimization;registers;application specific integrated circuits;application specific integrated circuit;application specific processors;integrated circuit synthesis;signal synthesis;functional unit;application specific instruction set processor	High-level synthesis (HLS) has been successfully targeted towards the digital signal processing (DSP) domain. Both application-specific integrated circuits (ASICs) and application-specific instruction-set processor (ASIPs) have been frequently designed using the HLS approach. Since most ASIP and DSP processors provide multiple addressing modes, and, in addition to classical constraint on the number of function units, registers, and buses, there are many resource usage rules, special considerations need to be paid to the optimizing code generation problem. In this paper we propose three transformation techniques, data management, data ordering, and transformational retiming, for storage optimization during code generation. With these transformations, some scheduling bottlenecks are eliminated, redundant instructions removed, and multiple operations mapped onto a single one. The proposed transformations have been implemented in a software system called Theda:MS. A set of benchmark programs has been used to evaluate the effectiveness of Theda:MS. Measurement on the synthesized codes targeted towards the TI-TMS320C40 DSP processor shows that the proposed approach is indeed very effective.	addressing mode;application-specific instruction set processor;application-specific integrated circuit;benchmark (computing);central processing unit;code generation (compiler);digital signal processing;geforce 900 series;high-level synthesis;mathematical optimization;microsoft windows;program optimization;retiming;scheduling (computing);software system	Wei-Kai Cheng;Youn-Long Lin	1995	32nd Design Automation Conference	10.1145/217474.217523	embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;data management;computer science;operating system;application-specific integrated circuit	EDA	0.19661177953207332	51.813544240404624	90893
a605add81a8027c7baa4673934f9608ba9e38850	an operating system for a time-predictable computing node	time triggered;software complexity;real time;embedded real time systems;real time operating system;temporal predictability;software structure;computer architecture;control structure;operating system;time triggered architecture;control flow;real time operating systems;timing analysis;determinism;real time computing	The increasing complexity of speed-up mechanisms found in modern computer architectures makes it difficult to predict the timing of the software that runs on this hardware, especially when the software itself has many different execution paths. To fight this combined hardware-software complexity that makes an accurate timing analysis infeasible, we have conceived a very simple software structure for real-time tasks: We do not allow that decisions about the control flow are made at runtime, i.e., all decisions are resolved in an off-line analysis before runtime. In this paper we show that simple control structures generated before runtime can as well be used within the operating system of an embedded real-time system. In this way we make not only task timing but also the timing of the operating system and thus the timing of the entire real-time computer system fully deterministic, thus time-predictable. We explain the principles and mechanisms we use to achieve this predictability and show the results of an experiment that demonstrates the feasibility of	analysis of algorithms;central processing unit;clock signal;computer architecture;control flow;embedded system;interrupt;online and offline;operating system;programming complexity;prototype;real-time clock;real-time computing;real-time transcription;run time (program lifecycle phase);static timing analysis	Guenter Khyo;Peter P. Puschner;Martin Delvai	2008		10.1007/978-3-540-87785-1_14	embedded system;real-time computing;real-time operating system;computer science;operating system;distributed computing;programming language;control flow	Embedded	-3.7886116669040115	51.52300106032881	90990
ee128e5039c117904e69624b5563a20140c49dbc	ma2tg: a functional test program generator for microprocessor verification	dlx processor functional test program generator microprocessor verification microprocessor architectural automatic test program generator user constraint file architecture description language specification constraint solving techniques;functional testing;constraint handling microprocessor chips automatic test pattern generation formal verification;architecture description language;automatic test pattern generation;automatic testing;random testing;automatic generation;program generation;formal verification;constraint handling;constraint solving;automatic programming microprocessors automatic testing power generation space exploration libraries prototypes architecture description languages hardware computer aided manufacturing;microprocessor chips	A novel specification driven and constraints solving based method to automatically generate test programs from simple to complex ones for advanced microprocessors is presented in this paper. Our microprocessor architectural automatic test program generator (MA/sup 2/TG) can produce not only random test programs but also a sequence of instructions for a specific constraint by specifying a user constraints file. The proposed methodology makes three important contributions. First, it simplifies the microprocessor architecture modeling and eases adoption of architecture modification via architecture description language (ADL) specification. Second, it generates test programs for specific constraints utilizing the power of state-to-art constraints solving techniques. Finally, the number of test program for microprocessor verification and the verification time are dramatically reduced. We applied this method on DLX processor to illustrate the usefulness of our approach.	functional testing;microprocessor	Tun Li;Dan Zhu;Yang Guo;GongJie Liu;Sikun Li	2005		10.1109/DSD.2005.54	random testing;embedded system;architecture description language;computer architecture;parallel computing;formal verification;computer science;automatic test pattern generation;operating system;functional testing;programming language;random test generator	Logic	7.172767860625172	52.70235889787293	91001
6f81271740a6d448835d25318eee79a10a3bf1f7	on bit-serial nocs for fpgas		We can build lightweight bit-serial FPGA NoC routers thatcost 20 LUT, 17 FF per router and operate at 800–900 MHzspeeds. Each bit-serial router implements deflection-routing on aunidirectional torus topology requiring 1b-wide connection perport. The key ideas that enable this implementation are (1)reformulation of the dimension-ordered routing (DOR) functionusing compact 1 LUT, 1 FF streaming pattern matchers, (2)compact retiming of the datapath signals into SRL16 blocks, and(3) careful FPGA layout to efficiently pack the router logic intosmall rectangular regions 2×4 SLICEs on the chip. We anticipatethese bit-serial NoCs can be used in a variety of scenariosincluding overlay support for triggered debug, lightweight controlsignal dissemination, massively-parallel bit-serial processing.	datapath;field-programmable gate array;nx bit;network on a chip;retiming;router (computing);routing;serial communication	Nachiket Kapre	2017	2017 IEEE 25th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)	10.1109/FCCM.2017.14	retiming;real-time computing;chip;field-programmable gate array;parallel computing;computer science;datapath;router;overlay;lookup table;core router	Arch	6.198688935645041	55.351102396205135	91076
1dfa40473206fe2ca94ba9074fb4b6ab446546ee	automatic retargeting of binary utilities for embedded code generation	libraries;program assemblers;front end;embedded coding;binary utilities;reduced instruction set computing;hardware description languages;binary codes;instruction set architecture;spectrum;system on chip hardware description languages program assemblers program compilers reduced instruction set computing;packaging;automatic generation;embedded code generation;assembly;risc system on chip binary utilities binary code generation embedded code generation instruction set architectures automatic library modification application specific instruction set processors asip;computer architecture;application specific instruction set processors;binary codes embedded computing computer architecture reduced instruction set computing application specific processors packaging time to market libraries automation assembly;system on chip;binary code generation;instruction set architectures;risc;application specific processors;time to market;program compilers;disa;automatic library modification;asip;embedded computing;automation	Contemporary SoC design involves the proper selection of cores from a reference platform. Such selection implies the design exploration of alternative CPUs, which requires the generation of binary code for each possible target. However, the embedded computing market shows a broad spectrum of instruction-set architectures, ranging from micro-controllers to RISCs and ASIPs. As a consequence, binary utilities cannot always rely on pre-existent tools within standard packages. Besides, the task of manually retargeting every binary utility is not acceptable under time-to-market pressure. This paper describes a technique for the automatic generation of binary utilities from an abstract model of the target CPU, which can be synthesized from an arbitrary ADL. The technique is based upon two key mechanisms: model provision for tool generation (at the front-end) and automatic library modification (at the back-end). To illustrate the technique's automation effectiveness, the authors describe the generation of assemblers, linkers and disassemblers. The authors successfully compared the files produced by the generated tools to those produced by conventional tools. Moreover, to give proper evidence of retargetability, the authors present results for MIPS, SPARC, PowerPC and i8051.	assembly language;benchmark (computing);binary code;central processing unit;debugger;disassembler;embedded system;intel mcs-51;linker (computing);machine-dependent software;powerpc;relocation (computing);retargeting;sparc	Alexandro Baldassin;Paulo Centoducatte;Sandro Rigo;Daniel C. Casarotto;Luiz Cláudio Villar dos Santos;Max R. de O. Schultz;Olinto J. V. Furtado	2007	IEEE Computer Society Annual Symposium on VLSI (ISVLSI '07)	10.1109/ISVLSI.2007.29	computer architecture;parallel computing;real-time computing;computer science	Embedded	2.1275705656470203	50.79537576715303	91535
5183c4c90983420307cf92117257e3b30a178bb6	a 4.9 mw neural network task scheduler for congestion-minimized network-on-chip in multi-core systems	piecewise linear approximation;processor scheduling augmented reality multiprocessing systems network on chip neural nets power aware computing;accuracy;artificial neural networks;system on chip;power 4 9 mw neural network task scheduler congestion minimized network on chip multicore systems nnts near optimal task assignment algorithm reconfigurable precision neural network accelerator nota network congestion rp nna noc based multicore soc augmented reality applications noc communication pattern energy efficiency;neurons;neural network nn network on chip noc network congestion task assignment;artificial neural networks accuracy system on chip throughput piecewise linear approximation neurons;throughput	A neural network task scheduler (NNTS) is proposed for the congestion-minimized network-on-chip in multi-core systems. The NNTS is composed of a near-optimal task assignment (NOTA) algorithm and a reconfigurable precision neural network accelerator (RP-NNA). The NOTA adopting a neural network is proposed to predict and avoid the network congestion intelligently. And the RP-NNA is implemented to improve the throughput of NOTA with dynamically adjustable precision. In the case that the NNTS is integrated into a NoC-based multi-core SoC for the augmented reality applications, 79.2% prediction accuracy of NoC communication pattern is achieved and the overall latency is reduced by 24.4%. As a result, the RP-NNA consumes only 4.9 mW and improves the energy efficiency of system by 22.7%.	algorithm;ar (unix);artificial neural network;augmented reality;multi-core processor;multiprocessing;network congestion;network on a chip;programmable logic array;rp (complexity);real-time clock;scheduling (computing);system on a chip;throughput;windows task scheduler	Youchang Kim;Gyeonghoon Kim;Injoon Hong;Donghyun Kim;Hoi-Jun Yoo	2014	2014 IEEE Asian Solid-State Circuits Conference (A-SSCC)	10.1109/ASSCC.2014.7008898	embedded system;parallel computing;network traffic control;real-time computing;computer science	EDA	-2.366465355570131	58.98734638541707	91584
ea0846e3a0a1ece53ec4295eed0b9150bc6d8fc7	a component-based software architecture for realtime audio processing systems		This paper describes a new software architecture for audio signal processing. The architecture was specifically designed low-latency, low-delay realtime applications in mind. Additionally, the frequently used paradigm of dividing the functionality into components all sharing the same interface, was adopted. The paper presents a systematic approach into structuring the processing inside the components by dividing the functionality into two groups of functions: realtime and control functions. The implementation options are also outlined with short descriptions of two existing implementations of the architecture. An algorithm example highlighting the benefits of the architecture concludes the paper.	algorithm;audio signal processing;best, worst and average case;control function (econometrics);digital signal processor;filter design;online and offline;programming paradigm;real-time computing;software architecture;streaming media	Jarmo Hiipakka	2006			embedded system;real-time computing;operating system;resource-oriented architecture	Embedded	4.909007737708696	46.991595310708476	91720
ac3114f69d0d4046649f732dac715c1b5840afb3	netlist-level ip protection by watermarking for lut-based fpgas	detectors;watermarking digital signatures field programmable gate arrays industrial property logic design optimisation table lookup;watermarking;optimisation;intellectual property;logic design;digital signatures;development environment;shift registers;lookup table;ip networks;protection watermarking field programmable gate arrays table lookup hardware intellectual property security computer science logic shift registers;industrial property;field programmable gate arrays;signature bit lut based fpga ip protection watermark fpga design netlist level lookup table shift register optimization algorithm intellectual property field programmable gate arrays;magnetic cores;table lookup;optimal algorithm	This paper presents a novel approach to watermark FPGA designs on the netlist level. We restrict the dynamically addressable part of the logic table, thus freeing space for insertion of signature bits into lookup tables (LUTs). In this way, we tightly integrate the watermark with the design so that simply removing mark carrying components would damage the intellectual property core. Converting functional LUTs to LUT-based RAMs or shift registers prevents deletion due to optimization. With this technique, we take watermark carrying components out of the scope of optimization algorithms to achieve complete transparency towards development environments. We can extract the marks from the bitfile of an FPGA. The method was tested on a Xilinx Virtex-II Pro FPGA and showed low overhead in terms of timing and resources at a reasonable number of water-marked cells.	algorithm;field-programmable gate array;lookup table;mathematical optimization;netlist;overhead (computing);shift register;virtex (fpga)	Moritz Schmid;Daniel Ziener;Jürgen Teich	2008	2008 International Conference on Field-Programmable Technology	10.1109/FPT.2008.4762385	embedded system;digital signature;detector;parallel computing;logic synthesis;lookup table;digital watermarking;computer science;theoretical computer science;shift register;development environment;intellectual property;field-programmable gate array	EDA	7.021373686941341	55.625880321115154	91761
b7e033561f964c657491b8f25fecc8480764bf80	blueshift: designing processors for timing speculation from the ground up.	static critical path;design optimization algorithm;microarchitecture;logic design;clocks;error correcting support;timing speculation;processor design;logic gates;lead;critical path;error correction;microprocessor chips circuit optimisation clocks logic design;blueshift;circuit optimisation;selection biases;voltage scaling;high performance;program processors;clock frequency;process design timing design optimization clocks frequency error correction proposals design methodology logic voltage;path constraint tuning;on demand selective biasing;voltage scaling blueshift processor design timing speculation clock frequency error correcting support static critical path design optimization algorithm on demand selective biasing path constraint tuning;microprocessor chips;design methodology;timing	Several recent processor designs have proposed to enhance performance by increasing the clock frequency to the point where timing faults occur, and by adding error-correcting support to guarantee correctness. However, such Timing Speculation (TS) proposals are limited in that they assume traditional design methodologies that are suboptimal under TS. In this paper, we present a new approach where the processor itself is designed from the ground up for TS. The idea is to identify and optimize the most frequently-exercised critical paths in the design, at the expense of the majority of the static critical paths, which are allowed to suffer timing errors. Our approach and design optimization algorithm are called BlueShift. We also introduce two techniques that, when applied under BlueShift, improve processor performance: On-demand Selective Biasing (OSB) and Path Constraint Tuning (PCT). Our evaluation with modules from the OpenSPARC T1 processor shows that, compared to conventional TS, BlueShift with OSB speeds up applications by an average of 8% while increasing the processor power by an average of 12%. Moreover, compared to a high-performance TS design, BlueShift with PCT speeds up applications by an average of 6% with an average processor power overhead of 23% . providing a way to speed up logic modules that is orthogonal to voltage scaling.	algorithm;biasing;central processing unit;clock rate;correctness (computer science);dynamic voltage scaling;error detection and correction;image scaling;mathematical optimization;opensparc;oracle service bus;overhead (computing)	Brian Greskamp;Lu Wan;Ulya R. Karpuzcu;Jeffrey J. Cook;Josep Torrellas;Deming Chen;Craig B. Zilles	2009	2009 IEEE 15th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2009.4798256	lead;parallel computing;logic synthesis;real-time computing;error detection and correction;design methods;logic gate;microarchitecture;processor design;computer science;operating system;clock rate;critical path method;blueshift	Arch	7.798998213772127	59.83692952938625	91763
1df81ecc73a45008e2a552343fcfce4916cc4e10	open-people: an open platform for estimation and optimizations of energy consumption	software;system on chip embedded systems integrated circuit design low power electronics microprocessor chips;computer architecture;embedded systems;integrated circuit design;estimation;usb key energy consumption optimizations low power complex embedded systems design electronic domains multiple motivations battery longevity temperature constraints power energy estimation abstraction levels open people project global framework heterogeneous multiprocessor system on chip mpsoc power modeling methodology;system on chip;energy consumption;low power electronics;hardware estimation power measurement software computer architecture energy consumption optimization;optimization;power measurement;microprocessor chips;hardware	Designing low power complex embedded systems is a main challenge for corporations in a large number of electronic domains. There are multiple motivations which lead designers to consider low power design such as increasing lifetime, improving battery longevity, limited battery capacity, and temperature constraints, etc. Unfortunately, there is a lack of efficient methodology and accurate tool to obtain power/energy estimation of a complete system at different abstraction levels. The Open People project addresses this topic and proposes a global framework for power/energy estimation and optimization of heterogeneous MultiProcessor System on Chip (MPSoC). Within this framework, a power modeling methodology is defined. This methodology supports all the embedded system relevant aspects; the software, the hardware, and the operating system. Since this year, the Open People platform is available for designers and we propose to present how it can be used to help the designer during the different design steps. During the evening event of the conference, we propose to explain the functionalities of the platform to elaborate power consumption measurements, to estimate power consumption and to explore the different design choices. Furthermore, to help the designers to start with this platform, we will distribute an USB key containing all the necessary tools to start evaluation on some designs.	embedded system;mpsoc;mathematical optimization;multiprocessing;open platform;operating system;system on a chip	Eric Senn;Daniel Chillet;Olivier Zendra;Cécile Belleudy;Rabie Ben Atitallah;A. Fritsch;Christian Samoyeau	2012	Proceedings of the 2012 Conference on Design and Architectures for Signal and Image Processing		system on a chip;embedded system;electronic engineering;real-time computing;engineering;power optimization	EDA	-1.6387591268074033	56.88730788263011	92023
f02b5b10f8b3441237934bba71d5e4714e5ba28a	a methodology to predict the power consumption of servers in data centres	design process;generic model;data centre;it resources;error rate;power consumption	Until recently, there have been relatively few studies exploring the power consumption of ICT resources in data centres. In this paper, we propose a methodology to capture the behaviour of most relevant energy-related ICT resources in data centres and present a generic model for them. This is achieved by decomposing the design process into four modelling phases. Furthermore, unlike the state-of-the-art approaches, we provide detailed power consumption models at server and storage levels. We evaluate our model for different types of servers and show that it suffers from an error rate of 2% in the best case, and less than 10% in the worst case.	best, worst and average case;bit error rate;computer cooling;data center;iteration;server (computing)	Robert Basmadjian;Nasir Ali;Florian Niedermeier;Hermann de Meer;Giovanni Giuliani	2011		10.1145/2318716.2318718	real-time computing;simulation;engineering;operations management	Mobile	-2.7235713535926407	58.64756685818675	92058
bfe87613b297158b13ea0a09b24b53a1137ae83a	minimizing address arithmetic instructions in embedded applications on dsps	minimizing address arithmetic instruction;address arithmetic instruction;address register;assignment problem;dedicated address generation unit;address generation;multiple address register;digital signal processor;embedded application;next address;memory location;efficient code	minimizing address arithmetic instruction;address arithmetic instruction;address register;assignment problem;dedicated address generation unit;address generation;multiple address register;digital signal processor;embedded application;next address;memory location;efficient code	digital signal processor;embedded system	Hassan Salamy	2012	Computers & Electrical Engineering	10.1016/j.compeleceng.2012.06.003	parallel computing;real-time computing;computer science;physical address;theoretical computer science;operating system;logical address;algorithm;address bus;memory address register	Embedded	-0.09495989352592556	51.86699243974258	92155
37d47be55855461aff259e800f46a41e13a63bff	a strategy for determining a jacobi specific dataflow processor	array processing;application specific integrated circuits adaptive signal processing parallel processing digital arithmetic data flow computing jacobian matrices;iterative algorithms;mapping efficiency;hierarchical exploration method;processor template;dependence graph;hierarchical control;lookahead techniques jacobi specific dataflow processor jacobi algorithms application domain array processing real lime adaptive signal processing applications quasi regularity property dependence graph representations exploration iteration processor template mapper hierarchical exploration method mapping efficiency retiming pipelining;array signal processing;vliw;application domain;jacobi algorithms;adaptive signal processing;streaming media;application specific integrated circuits;critical path;jacobi specific dataflow processor;performance analysis;mapper;data flow computing;digital arithmetic;exploration iteration;pipelining;jacobian matrices signal processing algorithms iterative algorithms parallel processing array signal processing pipeline processing vliw streaming media adaptive signal processing performance analysis;signal processing algorithms;jacobian matrices;lookahead techniques;real lime adaptive signal processing applications;dependence graph representations;retiming;parallel processing;pipeline processing;quasi regularity property	In this paper; we present a strategy fo r determining a datajow processor which is intended for the execution of Jacobi algorithms which are found in the application domain of array processing and other real-time adaptive signal processing applications. Our strategy to determine a processor fo r their execution is to exploit the quasi regularity property in their dependence graph representations in search fo r what we call the Jacobi processor. This processor emerges from an exploration iteration which takes off from a processor template and a set of Jacobi algorithms. Based on qualitative and quantitative performance analysis, both the algorithms and the processor template are restructured towards improved execution performance. To ensure the mapper is part of the emerging processor speci)cation, the algorithmto-processor mapping method is included in the iterative and hierarchical exploration method. Processor 5 hierarchy exploits properties related to regularity in the algorithm's structure, allows gentle transitions from regular to irregular levels in the algorithm hierarchy and offers different control models f o r the irregular structures that appear at deeper levels of the hierarchy. Transformations aiming at reducing critical paths, increasing throughput, improving mapping efJiciency and minimizing control a n d j a w overheads are essential. They include retiming, pipelining and lookahead techniques. keywords: Dataflow processors, application specific processors, Jacobi algorithms, hierarchical control, algorithm transformations, Cordic pipelines.	algorithm;application domain;array processing;cordic;central processing unit;dataflow;iteration;jacobi method;parsing;pipeline (computing);profiling (computer programming);real-time clock;retiming;signal processing;throughput	Edwin Rijpkema;Gerben J. Hekstra;Ed F. Deprettere;Jun Ma	1997		10.1109/ASAP.1997.606812	adaptive filter;embedded system;pipeline burst cache;parallel processing;computer architecture;parallel computing;application domain;real-time computing;computer science;very long instruction word;retiming;theoretical computer science;operating system;critical path method;information processor;application-specific integrated circuit;pipeline	Arch	-0.22681878749667214	53.48720779677555	92240
e421f937e66aa657f068fec656b6d79ef89f7045	analytical results for design space exploration of multi-core processors employing thread migration	throughput analytical leakage dependence on temperature thermal management thermal model thread migration throttling;power aware computing coprocessors multiprocessing systems multi threading;system level thermal simulator design space exploration multicore processors thermal management thread migration on chip system level power simulator;multi core processor;multi threading;thermal analysis;throttling;thread migration on chip;system level thermal simulator;analytical;system level power simulator;coprocessors;chip;thread migration;power aware computing;computational modeling;leakage dependence on temperature;multicore processing;thermal model;mathematical model;multicore processors;multiprocessing systems;design space exploration;space exploration multicore processing yarn thermal management temperature throughput frequency embedded system power system management clocks;thermal management;throughput;steady state	Migrating threads away from the hot cores in a multicore processor allows them to operate at up to higher speeds. While this technique has already attracted a lot of research effort, the majority of thread migration studies are simulation-based. Although they are valuable for micro-architectural level optimization, they require prohibitively long simulation times, and hence have limited value for early design space exploration. We derive closed form expressions for the steady-state throughput of a multicore processor that employs thread migration and throttling for thermal management. These expressions can be evaluated under a millisecond (vs days for cycle-accurate simulation), and allow designers greater flexibility in evaluating the trade-offs involved in implementing thread migration on-chip. We also developed a system-level power/thermal simulator that we used to validate the analytical results.	central processing unit;design space exploration;mathematical optimization;multi-core processor;process migration;simulation;steady state;thermal management of high-power leds;throughput	Ravishankar Rao;Sarma B. K. Vrudhula;Krzysztof S. Berezowski	2008	Proceeding of the 13th international symposium on Low power electronics and design (ISLPED '08)	10.1145/1393921.1393981	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system	Arch	-3.3017765946125306	55.61457128448148	92346
986a3fa19ae2ba4fa6f1f455b35c69c0475abd79	operating systems should manage accelerators	power efficiency;shared accelerator;higher performance;accelerator resource;general-purpose cpu;current operating system;performance policy;operating system;cpu time;heterogeneous system	The inexorable demand for computing power has lead to increasing interest in accelerator-based designs. An accelerator is specialized hardware unit that can perform a set of tasks with much higher performance or power efficiency than a general-purpose CPU. They may be embedded in the pipeline as a functional unit, as in SIMD instructions, or attached to the system as a separate device, as in a cryptographic co-processor.  Current operating systems provide little support for accelerators: whether integrated into a processor or attached as a device, they are treated as CPU or a device and given no additional consideration. However, future processors may have designs that require more management by the operating system. For example, heterogeneous processors may only provision some cores with accelerators, and IBM's wire-speed processor allows user-mode code to launch computations on a shared accelerator without kernel involvement. In such systems, the OS can improve performance by allocating accelerator resources and scheduling access to the accelerator as it does for memory and CPU time.  In this paper, we discuss the challenges presented by adopting accelerators as an execution resource managed by the operating system. We also present the initial design of our system, which provides flexible control over where and when code executes and can apply power and performance policies. It presents a simple software interface that can leverage new hardware interfaces as well as sharing of specialized units in a heterogeneous system.	central processing unit;computation;coprocessor;cryptography;electrical connector;embedded system;execution unit;general-purpose macro processor;heterogeneous computing;operating system;performance per watt;protection ring;simd;scheduling (computing);user space	Sankaralingam Panneerselvam;Michael M. Swift	2012			embedded operating system;parallel computing;real-time computing;computer hardware;computer science	Arch	-2.0816109502285696	48.335866563994394	92560
4517a29048172f6335267d35ecbe10709c11bac6	impact of die-to-die and within-die parameter variations on the throughput distribution of multi-core processors	throughput distribution;analytical models;microprocessors;multi core processor;multi threading;fmax distribution;throughput distribution fmax distribution multi core parameter fluctuations parameter variations;salient contributions;maximum clock frequency;standard deviation;parameter variations;throughput mean degradation;throughput multicore processing analytical models microprocessors clocks frequency delay bandwidth degradation circuit simulation;chip;statistical analysis multiprocessing systems multi threading;within die parameter variation;statistical analysis;critical path;product level variation analysis;multicore processing;parameter fluctuations;compact analytical throughput model;variational analysis;die to die parameter variation;bandwidth;statistical performance simulator;single threaded application;multicore processors;multiprocessing systems;bandwidth constraints;multithreaded application;program processors;multi core;memory latency;throughput;single threaded application die to die parameter variation within die parameter variation throughput distribution multicore processors statistical performance simulator maximum clock frequency compact analytical throughput model salient contributions product level variation analysis memory latency multithreaded application bandwidth constraints throughput mean degradation standard deviation	A statistical performance simulator is developed to explore the impact of die-to-die (D2D) and within-die (WID) parameter variations on the distributions of maximum clock frequency (FMAX) and throughput for multi-core processors in a future 22nm technology.allThe simulator integrates a compact analytical throughput model, which captures the key dependencies of multi-core processors, into a statistical simulation framework that models the effects of D2D and WID parameter variations on critical path delays across a die. The salient contributions from this paper are: (1) Product-level variation analysis for multi-core processors must focus on throughput, rather than just FMAX, and (2) Multi-core processors are inherently more variation tolerant than single-core processors due to the larger impact of memory latency and bandwidth on overall throughput. To elucidate these two points, multi-core and single-core processors have a similar chip-level FMAX distribution (mean degradation of 9% and standard deviation of 5%) for multi-threaded applications. In contrast to single-core processors, memory latency and bandwidth constraints significantly limit the throughput dependency on FMAX in multi-core processors, thus reducing the throughput mean degradation and standard deviation by 50%. Since single-threaded applications running on a multi-core processor can execute on the fastest core, mean FMAX and throughput gains of 4% are achieved from the nominal design target.	cas latency;central processing unit;clock rate;critical path method;elegant degradation;fastest;internet access;multi-core processor;simulation;single-core;thread (computing);throughput;variable rules analysis	Keith A. Bowman;Alaa R. Alameldeen;Srikanth T. Srinivasan;Chris Wilkerson	2007	Proceedings of the 2007 international symposium on Low power electronics and design (ISLPED '07)	10.1145/1283780.1283792	multi-core processor;embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system	Arch	-3.6408720934721104	55.16956631925965	92567
29bf9434504f2fbad504b22a90e4a22cb191c478	adaptive execution assistance for multiplexed fault-tolerant chip multiprocessors	cmos integrated circuits;multi threading circuit simulation cmos integrated circuits computer architecture fault tolerant computing integrated circuit reliability microprocessor chips;multi threading;reliability;elektroteknik och elektronik;multiplexing;computer architecture;circuit simulation;engineering and technology;fault tolerant computing;teknik och teknologier;lead;lead multiplexing throughput reliability;integrated circuit reliability;low overhead hardware mechanism adaptive execution assistance technique multiplexed fault tolerant chip multiprocessors cmos fabrication technology integrated circuit transient fault wearout related permanent fault intermittent fault throughput efficient architecture cmp priority based thread scheduling algorithm simulation based evaluation;microprocessor chips;throughput	Relentless scaling of CMOS fabrication technology has made contemporary integrated circuits increasingly susceptible to transient faults, wearout-related permanent faults, intermittent faults and process variations. Therefore, mechanisms to mitigate the effects of decreased reliability are expected to become essential components of future general-purpose microprocessors.	algorithm;cmos;dual modular redundancy;fault tolerance;futures studies;general-purpose markup language;image scaling;integrated circuit;little big adventure;microprocessor;multiplexing;network on a chip;scheduling (computing);semiconductor device fabrication;throughput	Pramod Subramanyan;Virendra Singh;Kewal K. Saluja;Erik Larsson	2011	2011 IEEE 29th International Conference on Computer Design (ICCD)	10.1109/ICCD.2011.6081432	embedded system;throughput;lead;electronic engineering;parallel computing;real-time computing;multithreading;computer science;operating system;reliability;cmos;multiplexing	EDA	7.174816610502077	59.544062763621255	92701
68a41995bd5f54a892fe18f1a458b68ce5895439	predicting hardware acceleration through object caching in amidar processors	functional unit;hardware accelerator	Dynamically reconfigurable architectures offer the opportunity to migrate software into hardware functional units at runtime. Architectures derived from the AMIDAR model exhibit such possibilities. In previous work we have shown how to identify heavily used code sequences and have also shown that it might be interesting to synthesize hardware for a set of methods of one class and also cache the state of particular objects in the synthesized hardware. In this paper we discuss a method to identify such objects at runtime and present a heuristics to select caching candidates in order to make optimal use of the limited storage resources.	cache (computing);central processing unit;hardware acceleration;heuristic (computer science);profiling (computer programming);reconfigurable computing;run time (program lifecycle phase);web cache	Stefan Döbrich;Christian Hochberger	2006			parallel computing;computer architecture;hardware register;hardware acceleration;computer science	Arch	-0.9460504640661684	50.13154789367868	92868
4759772f621bc07161ddd1f654d7b35a36cf91b8	design and implementation of digital linear control systems on reconfigurable hardware	signal image and speech processing;digital linear control;quantum information technology spintronics;linear control system;design and implementation;mechatronic systems;reconfigurable hardware	The implementation of large linear control systems requires a high amount of digital signal processing. Here, we show that reconfigurable hardware allows the design of fast yet flexible control systems. After discussing the basic concepts for the design and implementation of digital controllers for mechatronic systems, a new general and automated design flow starting from a system of differential equations to application-specific hardware implementation is presented. The advances of reconfigurable hardware as a target technology for linear controllers is discussed. In a case study, we compare the new hardware approach for implementing linear controllers with a software implementation.	computational complexity theory;control system;design flow (eda);digital signal processing;emoticon;field-programmable gate array;mechatronics;real life;reconfigurable computing;requirement	Marcus Bednara;Klaus Danne;Markus Deppe;Oliver Oberschelp;Frank Slomka;Jürgen Teich	2003	EURASIP J. Adv. Sig. Proc.	10.1155/S1110865703301040	hardware compatibility list;real-time computing;reconfigurable computing;computer science	EDA	5.827288948242561	47.629882669756554	93206
4f0dedf1129ced07282afa319bab7bdc459c20ae	dynamic memory management design methodology for reduced memory footprint in multimedia and wireless network applications	embedded systems;integrated circuit design;microprocessor chips;multimedia systems;storage management;wireless lan;dm behaviour;dm management;dynamic memory management design;multimedia application;portable consumer embedded devices;reduced memory footprint;system behaviour;wireless network applications	New portable consumer embedded devices must execute multimedia and wireless network applications that demand extensive memory footprint. Moreover, they must heavily rely on Dynamic Memory (DM) due to the unpredictability of the input data (e.g. 3D streams features) and system behaviour (e.g. number of applications running concurrently defined by the user). Within this context, consistent design methodologies that can tackle ef.ciently the complex DM behaviour of these multimedia and network applications are in great need. In this paper, we present a new methodology that allows to design custom DM management mechanismswith a reduced memory footprint for such kind of dynamic applications. The experimental results in real case studies show that our methodology improves memory footprint 60% on average over current state-of-the-art DM managers.	embedded system;fastest;general-purpose markup language;memory footprint;memory management;overhead (computing)	David Atienza;Stylianos Mamagkakis;Francky Catthoor;Jose Manuel Mendias;Dimitrios Soudris	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		system on a chip;memory footprint;co-design;embedded system;electronic engineering;parallel computing;real-time computing;design methods;reconfigurable computing;computer science;operating system;wireless network;place and route;integrated circuit design;memory management	EDA	0.8286356719125577	54.63421247431227	93474
16a54ebd77a75417e9b2e48fa9fe3f2c71ed6e82	implementation of the communication protocols spi and i2c using a fpga by the hdl-verilog language		Currently, the most used serial communication protocols to exchange information between different electronic embedded devices are the SPI and I2C. This paper describes the development and implementation of these protocols using a FPGA card. For the implementation of each protocol, it was taken into account different modes of operation, such as master/slave mode sending or pending data mode. For the implementation of the I2C protocol was necessary to perform a tri-state buffer, which makes a bidirectional data line for a successful communication between devices, allowing to take advantage of these sources provided by the FPGA. Verilog is a hardware description language better known as HDL and it was used in the work to implement and simulate these communication protocols with the software version 14.7 of Xilinx ISE Design Suite.	american and british english spelling differences;block cipher mode of operation;characteristic impedance;clock signal;communications protocol;concurrency (computer science);embedded system;field-programmable gate array;hardware description language;high impedance;high-level programming language;input/output;master/slave (technology);microcontroller;same-day affirmation;serial peripheral interface bus;serial communication;signal edge;simulation;software versioning;structured text;symbolic data analysis;three-state logic;triangular function;verilog;xilinx ise	Tatiana Mileydy Leal del Río;Luz Noé Oliva-Moreno;Antonio Gustavo Juárez Gracia	2014	Research in Computing Science		embedded system;parallel computing;real-time computing;computer science	EDA	6.817431931737014	49.20945622611947	93525
0fe7efa768356f1869730ff241c3338b199e6576	instruction-level test methodology for cpu core self-testing	bist;cpu core testing;test instruction set;software based self test;instruction level testing;fault coverage;pipelined processor;online testing;software based self testing	TIS is an instruction-level methodology for processor core self-testing that enhances instruction set of a CPU with test instructions. Since the functionality of test instructions is the same as the NOP instruction, NOP instructions can be replaced with test instructions. Online testing can be accomplished without any performance penalty. TIS tests different parts of the processor and detects stuck-at faults. This method can be employed in offline and online testing of single-cycle, multicycle and pipelined processors. But, TIS is more appropriate for online testing of pipelined architectures in which NOP instructions are frequently executed because of data, control and structural hazards. Running test instructions instead of these NOP instructions, TIS utilizes the time that is otherwise wasted by NOPs. In this article, two different implementations of TIS are presented. One implementation employs a dedicated hardware modules for test vector generation, while the other is a software-based approach that reads test vectors from memory. These two approaches are implemented on a pipelined processor core and their area overheads are compared. To demonstrate the appropriateness of the TIS test technique, several programs are executed and fault coverage results are presented.	algorithm;central processing unit;fault coverage;instruction pipelining;multi-core processor;nop;norm (social);online and offline;pipeline (computing);test vector	Saeed Shamshiri;Hadi Esmaeilzadeh;Zainalabedin Navabi	2005	ACM Trans. Design Autom. Electr. Syst.	10.1145/1109118.1109124	embedded system;computer architecture;parallel computing;real-time computing;fault coverage;computer science;operating system;instructions per cycle	EDA	6.23972857351326	57.42631616565457	93636
cdf99ff249378fcbf70571ed49c90ee55813f7f9	commerical considerations for designing with asics	asics economics of system design;microsystems;product life cycle;semicustom ics;commerical consideration	Abstract In implementing products within commercial environments, the choice between the various standard, semicustom and full custom devices and technologies available can be crucial to the success of the product. The paper reviews the many nontechnical factors that must be taken into account by engineering managers in specifying the implementation of a product, and how these factors change at different times within the product life cycle. IC product costs, nonrecurring IC design costs, system overheads, product design lead times and revenue losses due to design delays are compared for example systems made up of SSI/MSI, PLD, gate array, standard cell and full custom devices. Factors to be considered when choosing an IC vendor are discussed.		Jay P. Kamdar	1988	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(88)90126-3	embedded system;real-time computing;telecommunications;electrical engineering;operating system;product engineering	EDA	9.80999710938179	55.18040693473402	93994
0ec0efcac10313785d98414ec6d19e8e25b46a62	stagenetslice: a reconfigurable microarchitecture building block for resilient cmp systems	reliability;power density;building block;end of life;semiconductor devices;chip;multicore;communication delay;failure rate;coarse grained;architecture;pipeline	Although CMOS feature size scaling has been the source of dramatic performance gains, it has lead to mounting reliability concerns due to increasing power densities and on-chip temperatures. Given that most wearout mechanisms that plague semiconductor devices are highly dependent on these parameters, significantly higher failure rates are projected for future technology generations. Traditional techniques for dealing with device failures have relied on coarse-grained redundancy to maintain service in the face of failed components. In this work, we challenge this practice by identifying its inability to scale to high failure rate scenarios and investigate the advantages of finer-grained configurations. We use this study to motivate the design of StageNet, an embedded CMP architecture designed from its inception with reliability as a first class design constraint. StageNet relies on a reconfigurable network of replicated processor pipeline stages to maximize the useful lifetime of the chip, gracefully degrading performance toward end of life. This paper addresses the microarchitecture of the basic building block of StageNet, named StageNetSlice, which is a processor core comprised of networked pipeline stages. A naive slice design results in approximately 4X slowdown verses a traditional processor due to longer communication delays in the pipeline. However, several small design changes that eliminate inter-stage communication paths and minimize communication bandwidth reduce this overhead to 11% on average while providing high levels of fine-grain adaptability.	cmos;capacitor plague;design closure;embedded system;failure cause;failure rate;first-class function;futures studies;image scaling;inter-process communication;microarchitecture;multi-core processor;overhead (computing);performance;redundancy (engineering);semiconductor device	Shantanu Gupta;Shuguang Feng;Amin Ansari;Jason A. Blome;Scott A. Mahlke	2008		10.1145/1450095.1450099	chip;embedded system;parallel computing;real-time computing;telecommunications;computer science;architecture;operating system;failure rate;power density;reliability;pipeline	Arch	6.193895424979807	59.601597018577166	94044
295e0b42371128e050bdeff796bd373fc8e32bbc	crinkle: a heuristic mapping algorithm for network on chip	mapping algorithm;network on chip;task graph;energy consumption;communication cost	In this paper, a heuristic mapping algorithm which maps tasks, using priority lists and the crinkle moving pattern is proposed. To evaluate this algorithm, a set of real (i.e. Video Object Plan Decoder) and random applications have been used and the results have been compared. By reducing the number of hops between IP cores, the energy consumption and the completion time of the application (time which all tasks in the task graph execute wholly) have been optimized. Compared to other mapping algorithms, the algorithm execution time (due to its low complexity) is considerably lower.	algorithm;heuristic;network on a chip	Samira Saeidi;Ahmad Khademzadeh;Fatemeh Vardi	2009	IEICE Electronic Express	10.1587/elex.6.1737	embedded system;real-time computing;computer science;theoretical computer science;distributed computing;network on a chip	Logic	-2.301412813735564	59.12189722390073	94118
dd72b6d76d8428881d423af5c72f9d2689c00feb	combining height reduction and scheduling for vliw machines enhanced with three-argument arithmetic operations		In here we consider a technique to automatically extract three-argument instructions from sequential arithmetic code. The instructions include: multiply and add, three argument additions and three argument multiplications (MUL3). The proposed solution combines a height reduction technique that generates three-argument instructions and a VLIW scheduling that can benefit from these instructions. The proposed height reduction technique is based on a known theoretical algorithm (MRK) that in some cases can evaluate an algebraic circuit faster than its depth. We modified the MRK algorithm to generate less instructions and emit VLIW instructions. The modified MRK algorithm was implemented in the LLVM compiler and the potential usefulness was measured. Our results show that for arithmetic benchmarks the proposed technique can improve the VLIW scheduling while emitting three-argument instructions. The contribution of this work includes: the modified MRK algorithm as a new technique for height reduction optimizations and the study of the potential usefulness of three-argument instructions. Though our results are for a non existing hardware they show the usefulness of adding such instructions to VLIW CPUs. Note that a previous research showed that MUL3 can be executed as fast as MUL2.	algorithm;arithmetic coding;central processing unit;compiler;computation;digital signal processing;elbrus 2000;experiment;ia-64;llvm;level of detail;mad;scheduling (computing);very long instruction word	Fadi Abboud;Yosi Ben-Asher;Yousef Shajrawi;Esti Stein	2012	International Journal of Parallel Programming	10.1007/s10766-012-0196-7	parallel computing;real-time computing;computer hardware;computer science;operating system;instructions per cycle	EDA	-2.3983920193351893	51.48837257716388	94234
7083d01a1a7e7fd05e34f26a768828bcaa55234d	a configuration memory hierarchy for fast reconfiguration with reduced energy consumption overhead	run time reconfigurable hardware;random access memory;degradation;reconfigurable architectures;reduced energy consumption;configuration mapping algorithm;runtime;embedded system;reconfigurable architectures configuration management embedded systems;computer architecture;embedded systems;energy consumption field programmable gate arrays runtime embedded system performance analysis switches random access memory computer architecture hardware degradation;run time reconfigurable;energy consumption;configuration memory hierarchy;performance analysis;memory hierarchy;field programmable gate arrays;switches;high performance;low power consumption;configuration management;energy saving;configuration mapping algorithm configuration memory hierarchy reduced energy consumption run time reconfigurable hardware embedded systems;hardware	Currently run-time reconfigurable hardware offers really attractive features for embedded systems, such as flexibility, reusability, high performance and, in some cases, low-power consumption. However, the reconfiguration process often introduces significant overheads in performance and energy consumption. In our previous work we have developed a reconfiguration manager that minimizes the execution time overhead. Nevertheless, since the energy overhead is equally important, in this paper we propose a configuration memory hierarchy that provides fast reconfiguration while achieving energy savings. To take advantages of this hierarchy we have developed a configuration mapping algorithm and we have integrated it in our reconfiguration manager. In our experiments we have reduced the energy consumption 22.5% without introducing any performance degradation	algorithm;elegant degradation;embedded system;experiment;field-programmable gate array;low-power broadcasting;memory hierarchy;overhead (computing);run time (program lifecycle phase)	Elena Perez Ramo;Javier Resano;Daniel Mozos;Francky Catthoor	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639435	embedded system;parallel computing;real-time computing;degradation;network switch;computer science;configuration management;field-programmable gate array	Embedded	-1.7554319733639738	54.21085068269521	94272
7f2572d6d823cb98a3c5f98f65a9f7f783461121	modeling dynamically reconfigurable systems for simulation-based functional verification	libraries;hardware design languages;field programmable gate array;kernel;reconfigurable system;formal specification;functional verification;partial reconfiguration;dynamic reconfiguration;top down;reconfigurable architectures;application specification;computer model;high performance system;fpga;embedded system;data model;embedded systems;top down modeling methodology dynamically reconfigurable systems fpga functional verification;computational modeling;formal verification;dynamically reconfigurable systems;computer bugs field programmable gate arrays data models hardware design languages kernel libraries computational modeling;fpga architecture;hardware design;register transfer level dynamically reconfigurable systems simulation based functional verification embedded system high performance system fpga architecture application specification verification driven top down modeling methodology;verification driven top down modeling methodology;field programmable gate arrays;modeling methodology;top down modeling methodology;reconfigurable architectures embedded systems formal specification formal verification;computer bugs;register transfer level;high performance;simulation based functional verification;data models	Dynamically Reconfigurable Systems (DRS), which allow logic to be partially reconfigured during run-time, are promising candidates for embedded and high-performance systems. However, their architectural flexibility introduces a new dimension to the functional verification problem. Dynamic reconfiguration requires the designer to consider new issues such as synchronizing, isolating and initializing reconfigurable modules. Furthermore, by exposing the FPGA architecture to the application specification, it has made functional verification dependent on the physical implementation. This paper studies simulation as the most fundamental approach to the functional verification of DRS. The main contribution of this paper is in proposing a verification-driven top-down modeling methodology that guides designers in refining their reconfigurable system design from the behavioral level to the register transfer level. We assess the feasibility of our methodology via a case study involving the design of a generic partial reconfiguration platform.	embedded system;field-programmable gate array;reconfigurability;reconfigurable computing;register-transfer level;run time (program lifecycle phase);simulation;systems design;top-down and bottom-up design	Lingkan Gong;Oliver Diessel	2011	2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2011.18	computer simulation;embedded system;computer architecture;parallel computing;real-time computing;computer science;high-level verification;functional verification;field-programmable gate array	EDA	4.229779183831472	52.93371064897128	94533
2053ba2a82a0b32337428f72c027bd5f1ea7f159	multiple-wordlength resource binding	traitement signal;field programmable gate array;enlace;longueur mot;red puerta programable;simulated annealing;partage ressource;reseau porte programmable;liaison;recuit simule;word length;senal numerica;signal processing;longitud palabra;resource sharing;particion recursos;signal numerique;binding problem;recocido simulado;digital signal;procesamiento senal;binding	This paper describes a novel resource binding technique for use in multiple-wordlength systems implemented in FPGAs. It is demonstrated that the multiple-wordlength binding problem is significantly different for addition and multiplication, and techniques to share resources between several operations are examined for FPGA architectures. A novel formulation of the resource binding problem is presented as an optimal colouring problem on a resource conflict graph, and several algorithms are developed to solve this problem. Results collected from many sequencing graphs illustrate the effectiveness of the heuristics developed in this paper, demonstrating significant area reductions over more traditional approaches.	algorithm;binding problem;field-programmable gate array;graph coloring;heuristic (computer science);serializability	George A. Constantinides;Peter Y. K. Cheung;Wayne Luk	2000		10.1007/3-540-44614-1_69	binding problem;shared resource;embedded system;computer vision;simulated annealing;digital signal;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;signal processing;distributed computing;algorithm;field-programmable gate array	EDA	0.18820604333804686	52.45049026527372	94569
6cfe6ac82a8775622b037891574e9bd256331785	instruction scheduling for variation-originated variable latencies	variation-free processor;processor performance;variable latency adder;performance degradation;variation-originated variable latencies;instruction scheduling;long latency adder;proposed scheduling technique;conventional scheduling;instruction criticality;instruction scheduling technique;long latency;adders;threshold voltage;degradation;transistors;hardware;profitability;arithmetic logic unit;circuits	The advance in semiconductor technologies presents the serious problem of parameter variations. They affect threshold voltage of transistors and thus circuit delay also has variations. Recently, variable latency adders and long latency adders are proposed to manage the variation problem. Unfortunately, replacing a variation-affected adder with the long latency one has severe impact on processor performance. In order to maintain performance, the present paper proposes an instruction scheduling technique considering instruction criticality. By issuing and executing only uncritical instructions in the long latency ALU, we can maintain processor performance. From detailed simulations, we find that the proposed scheduling technique improves processor performance by 12.5% on average over the conventional scheduling and that performance degradation from a variation-free processor is only 4.0% on average, when 2 of 4 ALU's are affected by variations.	adder (electronics);arithmetic logic unit;criticality matrix;elegant degradation;instruction scheduling;schedule (project management);scheduling (computing);semiconductor;simulation;transistor	Toshinori Sato;Shingo Watanabe	2008	9th International Symposium on Quality Electronic Design (isqed 2008)	10.1109/ISQED.2008.61	electronic circuit;electronic engineering;parallel computing;real-time computing;degradation;computer science;engineering;electrical engineering;arithmetic logic unit;semiconductor;instruction scheduling;threshold voltage;transistor;adder;profitability index	Arch	7.097415557414875	59.65299620172233	94921
85d77fd78a3d8119bf5dbc10006e0b2408dc8d07	localizing globals and statics to make c programs thread-safe	program compilers c language embedded systems multiprocessing systems parallel programming;globals;thread safe;mobile device;instruction sets runtime libraries writing programming multicore processing;programming language;compiler based semiautomatic technique global localization static localization thread safe c program cpu power dissipation chip hot spots multicore processors embedded mobile devices parallel programs parallel code development embedded system application development partial mitigation techniques compiler based interactive technique;parallel programming;globals thread safe;embedded system;hot spot;chip;embedded systems;c language;low power;power dissipation;multicore processors;exponential growth;runtime system;multiprocessing systems;program compilers;parallel programs;interaction technique	Challenges emerging from the exponential growth in CPU power dissipation and chip hot spots with increasing clock frequencies have forced manufacturers to employ multicore processors as the ubiquitous platform in all computing domains. Embedded mobile devices are increasingly adopting multicore processors to improve program performance and responsiveness at low power levels. However, harnessing these performance and power benefits requires the construction of parallel programs, a task significantly more complex than writing sequential code. Parallel code development is also made more difficult by differences in the use of several programming language constructs. Therefore, it is critical to provide programmers with tools to ease the formidable task of parallelizing existing sequential code or developing new parallel code for multicore processors.  In this work we focus on the use of static and global variables that are commonly employed in C/C++ programs, the languages of choice for developing embedded systems applications. Unprotected use of such variables produces functions that are not thread-safe, thereby preventing the program from being parallelized. Manually eliminating global and static variables from existing sequential code is tedious, time-consuming and highly error-prone. While no good solution to this problem currently exists, researchers have proposed partial mitigation techniques that require massive changes to linkers and runtime systems. In this work we study the characteristics and effects of static and global variables in traditional benchmark programs, and propose, construct, and explore a compiler-based, semi-automatic and interactive technique to handle such variables and generate thread-safe code for parallel programs.	benchmark (computing);c++;cpu power dissipation;central processing unit;clock rate;cognitive dimensions of notations;compiler;embedded system;global variable;linker (computing);mobile device;moore's law;multi-core processor;parallel computing;programmer;programming language;responsiveness;runtime system;semiconductor industry;static variable;thread safety;time complexity	Adam R. Smith;Prasad A. Kulkarni	2011	2011 Proceedings of the 14th International Conference on Compilers, Architectures and Synthesis for Embedded Systems (CASES)	10.1145/2038698.2038730	chip;multi-core processor;embedded system;exponential growth;computer architecture;parallel computing;real-time computing;computer science;dissipation;operating system;mobile device;thread safety;programming language;hot spot;interaction technique	EDA	-4.073022806104731	48.98872943453918	95025
fdbb09c1d913caa9bc24dbe72745cd037afc5d88	towards a type 0 hypervisor for dynamic reconfigurable systems		The usage of application-specific hardware based on Field-Programmable Gate Arrays (FPGA) has proven its benefits. Current system-on-chips, which contain FPGA fabric, supporting dynamic partial reconfiguration, enable a dynamic hardware acceleration for hardware/software co-designs. With the trend to consolidate multiple computing systems into a single system, applications with mixed criticalities can come into conflict. With our approach, we are exploring the possibility to utilize dedicated hardware for the system management and benefit from possible parallelization of applications and system management tasks.	field-programmable gate array;hardware acceleration;hypervisor;parallel computing;systems management	Benedikt Janßen;Fatih Korkmaz;Halil Derya;Michael Hübner;Mário Lopes Ferreira;João Canas Ferreira	2017	2017 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/RECONFIG.2017.8279825	field-programmable gate array;real-time computing;computer science;hypervisor;systems management;memory management;control reconfiguration;software;hardware acceleration	EDA	-1.9953236041208802	49.802842808784106	95078
e94d266a11b052a6e03f69bb9bd2346c2623b2b1	acceleration of real-time proximity query for dynamic active constraints	concave programming;conference_paper;reconfigurable architectures;data stream;real time;gpu real time proximity query dynamic active constraints pq formulation nonconvex objects meshes cloud points reconfigurable hardware function transformation data structure memory architecture data streaming runtime reconfiguration dynamic precision optimisation optimised pq implementation reconfigurable platform fpga optimised cpu implementation;run time reconfigurable;memory architecture;data structures;robot motion planning;graphics processing units;field programmable gate arrays;field programmable gate arrays mathematical model equations hardware accuracy optimization robots;mesh generation;data structure;real time application;reconfigurable hardware;reconfigurable architectures concave programming data structures field programmable gate arrays graphics processing units memory architecture mesh generation;cloud point	Proximity Query (PQ) is a process to calculate the relative placement of objects. It is a critical task for many applications such as robot motion planning, but it is often too computationally demanding for real-time applications, particularly those involving human-robot collaborative control. This paper derives a PQ formulation which can support non-convex objects represented by meshes or cloud points. We optimise the proposed PQ for reconfigurable hardware by function transformation and reduced precision, resulting in a novel data structure and memory architecture for data streaming while maintaining the accuracy of results. Run-time reconfiguration is adopted for dynamic precision optimisation. Experimental results show that our optimised PQ implementation on a reconfigurable platform with four FPGAs is 58 times faster than an optimised CPU implementation with 12 cores, 9 times faster than a GPU, and 3 times faster than a double precision implementation with four FPGAs.	active set method;algorithm;central processing unit;computation;data structure;double-precision floating-point format;field-programmable gate array;graphics processing unit;mathematical optimization;motion planning;parallel computing;pipeline (computing);real-time clock;real-time locating system;real-time transcription;reconfigurable computing;robot;run time (program lifecycle phase);single-core;speedup	Thomas C. P. Chau;Ka-Wai Kwok;Gary C. T. Chow;Kuen Hung Tsoi;Kit-Hang Lee;Zion Tse;Peter Y. K. Cheung;Wayne Luk	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718355	embedded system;mesh generation;parallel computing;real-time computing;data structure;reconfigurable computing;computer science;operating system;cloud point;field-programmable gate array	Robotics	0.056867157980461235	47.36072074065205	95151
612d922ee80bcf283f67081b9c5e5bdcc464673f	safe limits on voltage reduction efficiency in gpus: a direct measurement approach	kernel;radiation detectors;semiconductor device measurement;gpgpu;critical path;graphics processing units;temperature measurement;dvfs;voltage measurement;power measurement	"""Energy efficiency of GPU architectures has emerged as an important aspect of computer system design. In this paper, we explore the energy benefits of reducing the GPU chip's voltage to the safe limit, i.e. Vmin point. We perform such a study on several commercial off-the-shelf GPU cards. We find that there exists about 20% voltage guardband on those GPUs spanning two architectural generations, which, if """"eliminated"""" completely, can result in up to 25% energy savings on one of the studied GPU cards. The exact improvement magnitude depends on the program's available guardband, because our measurement results unveil a program dependent Vmin behavior across the studied programs. We make fundamental observations about the program-dependent Vmin behavior. We experimentally determine that the voltage noise has a larger impact on Vmin compared to the process and temperature variation, and the activities during the kernel execution cause large voltage droops. From these findings, we show how to use a kernel's microarchitectural performance counters to predict its Vmin value accurately. The average and maximum prediction errors are 0.5% and 3%, respectively. The accurate Vmin prediction opens up new possibilities of a cross-layer dynamic guardbanding scheme for GPUs, in which software predicts and manages the voltage guardband, while the functional correctness is ensured by a hardware safety net mechanism."""	computer;correctness (computer science);experiment;file spanning;graphics processing unit;kernel (operating system);microarchitecture;systems design	Jingwen Leng;Alper Buyuktosunoglu;Ramon Bertran Monfort;Pradip Bose;Vijay Janapa Reddi	2015	2015 48th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/2830772.2830811	embedded system;parallel computing;kernel;real-time computing;computer hardware;temperature measurement;computer science;operating system;critical path method;particle detector;general-purpose computing on graphics processing units	Arch	-3.1619557208829008	56.59457464086922	95183
08d0e97257e3d7c4ef1914479f6579e1b5443428	a modular simulation framework for spatial and temporal task mapping onto multi-processor soc platforms	optimisation;spc;multi threading;signal processing timing computer architecture delay resource management space exploration data processing hardware operating systems resource virtualization;psc;energy efficient;data retention fault;simulation framework;design space;optimization modular simulation framework spatial task mapping temporal task mapping multiprocessor soc platforms signal processing networking applications systemc platform resources application to platform mappings executable performance model configurable event driven virtual processing unit multi threaded soc platforms xml design space exploration;memory diagnosis;integrated circuit design;integrated circuit design system on chip multiprocessing systems optimisation digital signal processing chips multi threading;system on chip;signal processing;performance model;digital signal processing chips;diagnosis time;multiprocessing systems;design space exploration;distributed small embedded srams;quantitative evaluation	Heterogeneous Multi-Processor SoC platforms bear the potential to optimize conflicting performance, flexibility and energy efficiency constraints as imposed by demanding signal processing and networking applications. However, in order to take advantage of the available processing and communication resources, an optimal mapping of the application tasks onto the platform resources is of crucial importance. In this paper, we propose a SystemC-based simulation framework, which enables the quantitative evaluation of application-to-platform mappings by means of an executable performance model. Key element of our approach is a configurable event-driven Virtual Processing Unit to capture the timing behavior of multi-processor/multi-threaded MP-SoC platforms. The framework features an XML-based declarative construction mechanism of the performance model to significantly accelerate the navigation in large design spaces. The capabilities of the proposed framework in terms of design space exploration is presented by a case study of a commercially available MP-SoC platform for networking applications. Focussing on the application to architecture mapping, our introduced framework highlights the potential for optimization of an efficient design space exploration environment.	design space exploration;event-driven programming;executable;mathematical optimization;multiprocessing;signal processing;simulation;system on a chip;systemc;thread (computing);virtual machine;xml	Torsten Kempf;Malte Doerper;Rainer Leupers;Gerd Ascheid;Heinrich Meyr;Tim Kogel;Bart Vanthournout	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.21	system on a chip;embedded system;electronic engineering;parallel computing;real-time computing;multithreading;computer science;operating system;signal processing;efficient energy use;statistical process control;integrated circuit design	EDA	2.027950369101582	52.090676659970796	95352
5c91c3a592b4a7a9880a8f82c38ee879f0a4810e	desyre: on-demand adaptive and reconfigurable fault-tolerant socs	publikationer;konferensbidrag;qa75 electronic computers computer science;artiklar;rapporter;r medicine general	The DeSyRe project builds on-demand adaptive, reliable Systems-on-Chips. In response to the current semiconductor technology trends that make chips becoming less reliable, DeSyRe describes a new gen- eration of by design reliable systems, at a reduced power and performance cost. This is achieved through the following main contributions. DeSyRe defines a fault-tolerant system architecture built out of unreliable compo- nents, rather than aiming at totally fault-free and hence more costly chips. In addition, DeSyRe systems are on-demand adaptive to various types and densities of faults, as well as to other system constraints and application requirements. For leveraging on-demand adaptation/customization and reliability at reduced cost, a new dynamically reconfigurable substrate is designed and combined with runtime system software support. The above define a generic and repeatable design framework, which is applied to two medical SoCs with high reliability constraints and diverse performance and power requirements. One of the main goals of the DeSyRe project is to increase the availability of SoC components in the presence of perma- nents faults, caused at manufacturing time or due to device aging. A mix of coarse- and fine-grain reconfigurable hardware substrate is designed to isolate and bypass faulty component parts. The flexibility provided by the DeSyRe reconfigurable substrate is exploited at runtime by system opti- mization heuristics, which decide to modify component configuration when a permanent fault is detected, providing graceful degradation.	system on a chip	Ioannis Sourdis;Christos Strydis;Antonino Armato;Christos-Savvas Bouganis;Babak Falsafi;Georgi Gaydadjiev;Sebastián Isaza;Alirad Malek;R. Mariani;Samuel Nascimento Pagliarini;Dionisios N. Pnevmatikatos;Dhiraj K. Pradhan;Gerard K. Rauwerda;Robert Mark Seepers;Rishad A. Shafik;Georgios Smaragdos	2014		10.1007/978-3-319-05960-0_34	embedded system;parallel computing;real-time computing;simulation;electrical engineering;operating system	Vision	6.780582290101963	58.28989870264159	95445
40f9b5efb8a723997091ae135ffb6bc233842d75	stageweb: interweaving pipeline stages into a wearout and variation tolerant cmp fabric	multiprocessing systems fault tolerant computing microprocessor chips;life time failure projections;process variation;reliability;pipelines fabrics;fault tolerant;manycore chips stageweb solution wearout tolerant cmp fabric variation tolerant cmp fabric manufacture time process variation life time failure projections fault tolerance chip multiprocessors coarse granularity;chip multiprocessor;manycore chips;argon;chip;fault tolerant computing;multicore;reliability permanent faults process variation multicore architecture;critical system;chip multiprocessors;fault tolerance;fabrics;failure rate;multiprocessing systems;permanent faults;wearout tolerant cmp fabric;cumulant;manufacture time process variation;architecture;variation tolerant cmp fabric;stageweb solution;coarse granularity;energy saving;microprocessor chips	Manufacture-time process variation and life-time failure projections have become a major industry concern. Consequently, fault tolerance, historically of interest only for mission-critical systems, is now gaining attention in the mainstream computing space. Traditionally reliability issues have been addressed at a coarse granularity, e.g., by disabling faulty cores in chip multiprocessors. However, this is not scalable to higher failure rates. In this paper, we propose StageWeb, a fine-grained wearout and variation tolerance solution, that employs a reconfigurable web of replicated processor pipeline stages to construct dependable many-core chips. The interconnection flexibility of StageWeb simultaneously tackles wearout failures (by isolating broken stages) and process variation (by selectively disabling slower stages). Our experiments show that through its wearout tolerance, a StageWeb chip performs up to 70% more cumulative work than a comparable chip multiprocessor. Further, variation mitigation in StageWeb enables it to scale supply voltage more aggressively, resulting in up to 16% energy savings.	algorithm;data recovery;experiment;failure cause;fault tolerance;futures studies;graphics pipeline;image scaling;interconnection;manycore processor;mission critical;multi-core processor;multiprocessing;pipeline (computing);scalability;semiconductor research corporation;shattered world	Shantanu Gupta;Amin Ansari;Shuguang Feng;Scott A. Mahlke	2010	2010 IEEE/IFIP International Conference on Dependable Systems & Networks (DSN)	10.1109/DSN.2010.5544915	reliability engineering;embedded system;fault tolerance;parallel computing;real-time computing;computer science;statistics	HPC	5.890507564579921	59.69336010661658	95485
69e79a248fbffdfb52e068b90b665ca6fd03e4ee	microelectronic device electrical test implementation problems on automated test equipment.	automated test equipment			Willis J. Horth;Frederick G. Hall;Robert G. Hillman	1982			reliability engineering;embedded system;automatic test equipment;5dx;electronic test equipment;device under test;computer science;software engineering;computer engineering	SE	9.665963117999182	53.58755755738488	95543
725c2a06e4b3b56565ce5d77bee139a020cd0052	the i486 cpu: executing instructions in one clock cycle	chip;clocks application software pipelines hardware computer architecture microcomputers multitasking logic protection microprocessors;instruction pipeline i486 cpu executing instructions binary compatibility math coprocessor cache minicomputer performance levels;microprocessor chips	The author discusses the design goals of the i486 development program, which were to ensure binary compatibility with the 386 microprocessor and the 387 math coprocessor, increase performance by two to three times over a 386/387 processor system at the same clock rate, and extend the IBM PC standard architecture of the 386 CPU with features suitable for minicomputers. A cache integrated into the instruction pipeline lets this 386-compatible processor achieve minicomputer performance levels. The design and performance of the on-chip cache and the instruction pipeline are examined in detail. >	central processing unit;clock signal	Stephen C. Johnson	1990	IEEE Micro	10.1109/40.46766	chip;pipeline burst cache;computer architecture;parallel computing;computer hardware;computer science;operating system;central processing unit;cache pollution;instructions per cycle	Embedded	6.244940465151874	49.209514234182706	95621
fef60ddb8ad35f4a3663537501b1e086b81fe339	self-adaptive architecture for multi-sensor embedded vision system		Architectural optimization for heterogeneous multi-sensor processing is a real technological challenge. Most of the vision systems involve only one single color sensor and they do not address the heterogeneous sensors challenge. However, more and more applications require other types of sensor in addition, such as infrared or low-light sensor, so that the vision system could face various luminosity conditions. These heterogeneous sensors could differ in the spectral band, the resolution or even the frame rate. Such sensor variety needs huge computing performance, but embedded systems have stringent area and power constraints. Reconfigurable architecture makes possible flexible computing while respecting the latter constraints. Many reconfigurable architectures for vision application have been proposed in the past. Yet, few of them propose a real dynamic adaptation capability to manage sensor heterogeneity. In this paper, a self-adaptive architecture is proposed to deal with heterogeneous sensors dynamically. This architecture supports on-the-fly sensor switch. Architecture of the system is self-adapted thanks to a system monitor and an adaptation controller. A stream header concept is used to convey sensor information to the self-adaptive architecture. The proposed architecture was implemented in Altera Cyclone V FPGA. In this implementation, adaptation of the architecture consists in Dynamic and Partial Reconfiguration of FPGA. The self-adaptive ability of the architecture has been proved with low resource overhead and an average global adaptation time of 75 ms.		Ali Isavudeen;Eva Dokladalova;Nicolas Ngan;Mohamed Akil	2015		10.1007/978-3-319-29817-7_7	reference architecture;embedded system;space-based architecture;real-time computing;simulation;database-centric architecture;engineering;applications architecture;cellular architecture;data architecture;systems architecture	Robotics	-0.5501931664668263	59.07961307103417	95674
dd9bb03f49e088262bc1c58c7e68bd3043201acd	dynamic configuration of application-specific implicit instructions for embedded pipelined processors	reconfiguration;vliw processor;code size;development time;implicit issue;pipelined architecture;dynamic configuration	In this paper, we propose the dynamic configuration of application specific implicit instructions for pipelined processors to better exploit the available parallelism at instruction level. Given the target application, the compiler selects a set of candidate instructions to be implicitly executed - i.e. their execution is controlled through a data-driven model, which avoids explicit instruction fetch. Consequently, the clock cycles usually required for the explicit issues are saved, thus improving the performance and reducing the code size. The compiler generates the reconfiguration operations to properly setup the data-path. The processor pipeline has been optimized to support the parallel execution of implicitly issued instructions, requiring a limited hardware overhead. The proposed technique has a negligible impact on the processor ISA - only reconfiguration instructions are added - which also benefits the compiler development times, since the optimization can be almost seamlessly added to an existing compilation tool-chain. The proposed approach has been applied to DSP and multimedia kernel loops, comparing its performance with those of two different baseline architectures: a scalar MIPS processor and a 4-issue VLIW processor of the LX family provided by STMicroelectronics [5]. Experimental results show a speedup ranging from 10 to 35%, and an average code size reduction of 19%.	baseline (configuration management);central processing unit;clock signal;compiler;digital signal processor;embedded system;mathematical optimization;overhead (computing);parallel computing;pipeline (computing);speedup;toolchain	Martino Sykora;Giovanni Agosta;Cristina Silvano	2008		10.1145/1363686.1364040	computer architecture;parallel computing;real-time computing;computer science;control reconfiguration;operating system;instructions per cycle	Arch	-1.0871226743545808	49.75213927060547	95860
bbcc78a36b31124f043413939c96bd364703758d	insulin: an instruction set simulation environment	instruction set simulation environment	Abstract We present Insulin , a new environment for the simulation of user-defined, application-specific programmable processors (ASPP). This environment is based on a reconfigurable VHDL model of a generic instruction set processor. The environment supports parallel instruction streams, cycle true simulation, arbitrary addressing modes, multi-cycle instructions, pipelining and user specified memory, register file and stack sizes. In our approach, the behavior of a user-defined instruction set (I/S) is expressed in terms of the generic instructions supported by the VHDL model. From this I/S definition, the tool automatically generates a cross-assembler which converts application-specific assembly code to the generic assembly code, which can be simulated. A graphic interface allows the user to interact with the model at the suitable level during the simulation. Insulin is part of an overall ASPP design environment at BNR which includes a retargetable code generator.	instruction set simulator;simulation	Shailesh Sutarwala;Pierre G. Paulin;Yatish Kumar	1993			computer architecture;parallel computing;computer architecture simulator;computer engineering	EDA	4.696958983092468	51.615296117386336	96137
cbce5cc1f2fc75b01d6c9ee06b5bd7a3eefac5af	the search for energy-efficient building blocks for the data center	computers;trace recording;building block;energy efficient;deterministic finite automaton;presentation;data center;single machine;low power;energy consumption;energy and power management;dynamic binary translation;data intensive computing;trace replaying;embedded processor	This paper conducts a survey of several small clusters of machines in search of the most energy-efficient data center building block targeting data-intensive computing. We first evaluate the performance and power of single machines from the embedded, mobile, desktop, and server spaces. From this group, we narrow our choices to three system types. We build five-node homogeneous clusters of each type and run Dryad, a distributed execution engine, with a collection of data-intensive workloads to measure the energy consumption per task on each cluster. For this collection of data-intensive workloads, our high-end mobile-class system was, on average, 80% more energy-efficient than a cluster with embedded processors and at least 300% more energy-efficient than a cluster with low-power server processors.	benchmark (computing);central processing unit;chipset;cobham's thesis;computer cluster;computer hardware;data center;data-intensive computing;desktop computer;dryad;embedded system;input/output;low-power broadcasting;operating system;peripheral;provisioning;requirement;server (computing);thread (computing);total system power	Laura Keys;Suzanne Rivoire;John D. Davis	2010		10.1007/978-3-642-24322-6_15	data center;computer architecture;parallel computing;real-time computing;computer science;operating system;deterministic finite automaton;data-intensive computing;efficient energy use	OS	-2.0496636481067814	57.62204857503289	96172
8ba9409984f6a2bef40377587b2785e0ede51c32	study and evaluation of an irregular graph algorithm on multicore and gpu processor architectures	graph theory;paper;performance;tesla c1060;cuda;thesis;nvidia;algorithms;computer science	To improve performance of an application over the years, software industry assumed that the application would automatically run faster on new upcoming processors. This assumption mainly relied on ability of the microprocessor industry to extract more ILP(Instruction level parallelism) in single-threaded programs through technology improvements and processor architecture innovations ( higher clock rates, greater transistor density due to transistor scaling, deeper pipelines etc.). Consequently, software programmers have traditionally focused on writing correct and efficient sequential programs and have rarely needed to understand hardware or processor details. In the last few years however, this reliance of the software industry on the processor industry to automatically extract and scale application performance with new generations processors has hit a wall due to processor industry moving towards Chip Multiprocessors. This sudden change in trend was motivated mainly by factors of technology limitations( exploding power, ILP flattening ), costs and changing market trends ( increased popularity of portable devices ). As a result, most upcoming processors in desktop, server and even embedded platforms these days are Multicore or Chip Multiprocessors(CMP). Technology scaling continues to date thanks to Moore’s law, although now in the form of more processor cores per CMP. Consequently, the focus of programming is slowly moving towards multi-threading or parallel computing. Writing scalable applications with increasing processor cores cannot be done without knowing architectural details of the underlying Chip Multiprocessor. One area of applications which poses significant challenges of performance scalability on CMP’s are Irregular applications. One main reason for this is that irregular applications have very little computation and unpredictable memory access patterns making them memory-bound in contrast to compute-bound applications. Since the gap between processor & memory performance continues to exist, difficulty to hide and decrease this gap is one of the important factors which results in poor performance acceleration of these applications on chip multiprocessors. The goal of this thesis is to overcome many such challenges posed during performance acceleration of an irregular graph algorithm called Triad Census. We started from a state of the art background in Social Network Analysis, Parallel graph algorithms and Triad Census graph algorithm. We accelerated the Triad Census algorithm on two significantly different Chip Multiprocessors: Dual-socket Intel Xeon Multicore (8 hardware threads/socket) and 240-processor core NVIDIA Tesla C1060 GPGPU(128 hardware threads/core). Intel Multicores are designed mainly for general purpose and irregular applications, while GPGPU’s are designed for data parallel applications with predictable memory access. We then looked at techniques for ef-	algorithm;central processing unit;computation;data parallelism;desktop computer;embedded system;general-purpose computing on graphics processing units;graph theory;graphics processing unit;image scaling;instruction-level parallelism;list of algorithms;microarchitecture;microprocessor;moore's law;multi-core processor;multiprocessing;multithreading (computer architecture);nvidia tesla;parallel computing;personal digital assistant;pipeline (software);programmer;scalability;server (computing);social network analysis;software industry;thread (computing);transistor	Varun Nagpal	2016	CoRR		computer architecture;parallel computing;real-time computing;performance;computer science;graph theory;operating system;distributed computing	Arch	-3.58879051477573	47.08807888801074	96267
02385e05bd17e663cd06cfdb5d7dcfd37c68e5da	translating affine nested-loop programs to process networks	distributed memory;integer linear programming;nested loops;process network;process networks;functional equivalence;embedded system;heterogeneous embedded systems;model of computation;integer linear program	New heterogeneous multiprocessor platforms are emerging that are typically composed of loosely coupled components that exchange data using programmable interconnections. The components can be CPUs or DSPs, specialized IP cores, reconfigurable units, or memories. To program such platform, we use the Process Network (PN) model of computation. The localized control and distributed memory are the two key ingredients of a PN allowing us to program the platforms. The localized control matches the loosely coupled components and the distributed memory matches the style of interaction between the components. To obtain applications in a PN format, we have built the Compaan compiler that translates affine nested-loop programs into functionally equivalent PNs. In this paper, we describe a novel analytical translation procedure we use in our compiler that is based on integer linear programming. The translation procedure consists of four main steps and we will present each step by describing the main idea involved, followed by a representative example.	central processing unit;compiler;digital signal processor;distributed memory;integer programming;kahn process networks;linear programming;loose coupling;model of computation;multiprocessing	Alexandru Turjan;Bart Kienhuis;Ed F. Deprettere	2004		10.1145/1023833.1023864	model of computation;embedded system;parallel computing;real-time computing;integer programming;distributed memory;nested loop join;computer science;theoretical computer science;operating system;distributed computing;programming language	PL	0.675989860043122	50.08787923716059	96307
cd15eef24d1ad25201a4369682dfb51ef8714873	automated instruction-set extension of embedded processors with application to mpeg-4 video encoding	maxmiso automated instruction set extension embedded processor mpeg 4 video encoding extensible processor application specific functional unit automated workload characterization instruction generation;maxmiso;mpeg 4 standard encoding kernel space exploration instruction sets motion estimation delay estimation design optimization optimizing compilers application software;kernel;workload characterization;platform based design;application software;embedded systems video coding microcomputers instruction sets;extensible processor;space exploration;motion estimation;mpeg 4 video encoding;design optimization;video coding;embedded systems;instruction set extension;performance improvement;mpeg 4 standard;automated instruction set extension;design space exploration;functional unit;automated workload characterization;optimizing compilers;encoding;embedded processor;instruction generation;application specific functional unit;microcomputers;delay estimation;domain specificity;instruction sets	A recent approach to platform-based design involves the use of extensible processors, offering architecture customization possibilities. Part of the designer responsibilities is the domain-specific extension of the baseline processor to fit customer requirements. Key issues of this process are the automated application analysis and candidate instruction identification/selection for implementation as application-specific functional units (AFUs). In this paper, a design approach that encapsulates automated workload characterization and instruction generation is utilized for extending processors to efficiently support embedded application sets. The method used for instruction generation is a highly parameterized adaptation of the MaxMISO technique, which allows for fast design space exploration. It is proven that only a small number of AFUs are needed in order to support the algorithms of interest (MPEG-4 encoding kernels) and that it is possible to achieve 2/spl times/ to 3.5/spl times/ performance improvements although further possibilities such as subword parallelization are not currently regarded.	algorithm;automatic parallelization;baseline (configuration management);central processing unit;data compression;design space exploration;embedded system;motion estimation;operand;parallel computing;platform-based design;prototype;requirement;simd;substring	Nikolaos Kavvadias;Spiridon Nikolaidis	2005	2005 IEEE International Conference on Application-Specific Systems, Architecture Processors (ASAP'05)	10.1109/ASAP.2005.20	embedded system;computer architecture;application software;parallel computing;kernel;real-time computing;multidisciplinary design optimization;computer science;space exploration;operating system;instruction set;motion estimation;microcomputer;programming language;encoding	EDA	2.336007308018548	51.746838264829016	96412
8055c2e51a758c7032e8a9b841097feb5617daaa	memory optimizations for fast power-aware sparse computations	energy efficiency;multigrid solver;kernel;mathematics computing;energy efficient;sparse matrix vector multiplication kernel;storage management;prefetching;power aware sparse computation;power saving mode;power engineering and energy;sparse matrices kernel hardware computational modeling power engineering computing computer science power engineering and energy prefetching energy efficiency computational efficiency;power aware computing;vectors mathematics computing matrix multiplication power aware computing sparse matrices storage management;power engineering computing;computational modeling;sparse scientific computation;vectors;memory optimization;memory subsystem optimization;function representation;scientific computing;multiobjective optimization;matrix multiplication;multiobjective optimization memory subsystem optimization power aware sparse computation sparse scientific computation sparse matrix vector multiplication kernel prefetching multigrid solver;computer science;sparse matrix;cross layer;computational efficiency;sparse matrices;hardware	We consider memory subsystem optimizations for improving the performance of sparse scientific computation while reducing the power consumed by the CPU and memory. We first consider a sparse matrix vector multiplication kernel that is at the core of most sparse scientific codes, to evaluate the impact of prefetchers and power-saving modes of the CPU and caches. We show that performance can be improved at significantly lower power levels, leading to over a factor of five improvement in the operations/Joule metric of energy efficiency. We then indicate that these results extend to more complex codes such as a multigrid solver. We also determine a functional representation of the impacts of such optimizations and we indicate how it can be used toward further tuning. Our results thus indicate the potential for cross-layer tuning for multiobjective optimizations by considering both features of the application and the architecture.	central processing unit;code;computation;computational science;function representation;joule;matrix multiplication;multigrid method;solver;sparse matrix	Konrad Malkowski;Padma Raghavan;Mary Jane Irwin	2007	2007 IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2007.370501	mathematical optimization;parallel computing;sparse matrix;computer science;theoretical computer science;efficient energy use	HPC	-2.6128747006904924	46.6372649104105	96491
5eed7b0b2e29c47f6fac87a7842ffaefa0be7796	exploring the energy efficiency of cache coherence protocols in single-chip multi-processors	multiprocessor;energy efficient;programming model;energy performance;low power;cache coherence protocol;system on chip;multi processor system on chip;cache coherence;multiprocessor system on chip;chip multi processor;high performance;power modeling	The performance of the various cache coherence protocols proposed in the literature have been extensively analyzed in the context of high-performance multi-processor systems.A similar analysis for Multi-Processor Systems-on-Chips (MP-SoCs), where energy is at least as important as performace, and for which strict constraints on hardware and software resources do exist, has not been done yet.This work provides an effort in that sense, showing energy/performance tradeoffs for different snoop-based protocols on a realistic MPSoC architecture. The analysis leverage a multi-processor simulation platform, augmented with accurate power models, that allows cycle-accurate simulations.Our analysis show that (i) cache write policy is actually more important than the actual cache coherence protocol, and (ii) matching the programming model and style to the architecture may have dramatic effects on the energy and performance of the system.	cache coherence;central processing unit;mpsoc;multiprocessing;programming model;simulation;system on a chip;snoop	Mirko Loghi;Martin Letis;Luca Benini;Massimo Poncino	2005		10.1145/1057661.1057728	bus sniffing;system on a chip;embedded system;pipeline burst cache;cache coherence;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;multiprocessing;cache coloring;cache;computer science;write-once;cache invalidation;operating system;efficient energy use;programming paradigm;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol	Arch	-3.5609380916562565	55.13886377781521	96533
7ac5130adeda98e052ac6193ff0406131b3afebb	coverage-directed management and optimization of random functional verification	distributed power generation;convergence;functional verification;system testing microelectronics convergence distributed power generation power generation logic testing inference mechanisms power system management power system modeling costs;inference mechanisms;random testing;power system management;random function;logic testing;system testing;test generation;power generation;system development;microelectronics;power system modeling;north carolina	This paper describes a functional verification methodology based on a system developed at the IBM Microelectronics Embedded PowerPC Design Center, in order to improve the coverage and convergence of random test generators in general and model-based random test generators in particular. It outlines specific tasks and methods devised for qualifying the test generators at various stages of the functional verification process to ensure the integrity of generated tests. It describes methods for calibrating the test generation process to improve functional coverage. In addition, it outlines a strategy for improved management and control of the test generation for faster convergence across corner cases, complex scenarios, and deep interdependencies. The described methodology and its associated verification platform are deployed at the IBM Embedded PowerPC Design Center in Research Triangle Park, North Carolina and has been used in the verification of 4XX and 4XXFPU family of PowerPC Processors.	central processing unit;corner case;embedded system;interdependence;mathematical optimization;powerpc	Amir Hekmatpour;James Coulter	2003	International Test Conference, 2003. Proceedings. ITC 2003.	10.1109/TEST.2003.1270835	random testing;reliability engineering;electricity generation;electronic engineering;real-time computing;convergence;computer science;engineering;electrical engineering;stochastic optimization;random function;system testing;microelectronics;functional verification;statistics	EDA	7.895550790677697	54.64687986900812	96706
2dfbedab66c3fdde78d1a5da591fd85b8929b552	a microprocessor-controlled message display system	microprocessors;control systems;temperature control;hardware computer displays microprocessors control systems eprom temperature control logic programming;logic programming;eprom;computer displays;hardware	Microprocessors replace electromechanical components to make this message display system both more efficient and less costly.	microprocessor	Paul D. Stigall;Brian E. Lenharth	1984	IEEE Micro	10.1109/MM.1984.291315	embedded system;computer architecture;parallel computing;real-time computing;computer science;control system;temperature control;eprom;programming language;logic programming	Visualization	7.394328948661443	49.698305038973636	96728
35774bf30ba67c82009099d47bf0bf47c1ca5d53	on the optimization of sbst test program compaction		Due to the increasing adoption of SBST solutions for both the end-of-manufacturing and the in-field test of SoC devices, the need for effective techniques able to reduce the duration of existing test programs became more pressing. Previous works demonstrated that this task is highly computational intensive and it is beneficial to partition it, e.g., by addressing the test program for one hardware module at a time. However, existing compaction techniques may become completely ineffective when dealing with faults which relate to memory addresses. This paper clarifies this issue and proposes possible solutions. Their effectiveness is experimentally demonstrated on a OR1200 pipelined processor.	central processing unit;computation;data compaction;experiment;fault coverage;instruction pipelining;mathematical optimization;memory address;nop;openrisc 1200;run time (program lifecycle phase)	R. Cantora;E. Sanchez;Matteo Sonza Reorda;Giovanni Squillero;E. Valea	2017	2017 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)	10.1109/DFT.2017.8244444	real-time computing;memory address;built-in self-test;compaction;computer science	Arch	6.08750593944575	57.50377944820516	96770
1a77dd2c0fa2bbdafe3316f4ed7651f0ad85a5a4	towards a low power virtual machine forwireless sensor network motes	hardware acceleration;virtual machine;processor architecture;conference publication;low power virtual machine;mate virtual machine;wireless sensor networks power aware computing virtual machines;virtual machining wireless sensor networks virtual manufacturing hardware energy consumption voice mail batteries power system modeling application software computer science;hardware accelerator;hardware acceleration low power virtual machine wireless sensor network motes generalised processor architecture;sensor network;wireless sensor network;programming model;power aware computing;low power;generalised processor architecture;virtual machines;wsn motes;power consumption;wireless sensor networks;wireless sensor network motes	Virtual machines (VMs) have been proposed as an efficient programming model for wireless sensor network (WSN) devices. However, the processing overhead required for VM execution has a significant impact on the power consumption and battery lifetime of these devices. This paper analyses the sources of power consumption in the Mate VM for WSNs. The paper proposes a generalised processor architecture allowing for hardware acceleration of VM execution. The paper proposes a number of hardware accelerators for Mate VM execution and assesses their effectiveness	hardware acceleration;overhead (computing);programming model;sensor node;speedup;tinyos;virtual machine;z/vm	Hitoshi Oi;Chris J. Bleakley	2006	2006 Japan-China Joint Workshop on Frontier of Computer Science and Technology	10.1109/FCST.2006.32	embedded system;real-time computing;wireless sensor network;computer science;operating system;computer network	Arch	-2.9425517615608885	57.968020377603466	96828
6e5e038ed4ed82fe06cf55977db330169285a7b8	an adaptive low-power transmission scheme for on-chip networks	adaptive design;low energy;network on chip;signal integrity;network on a chip crosstalk signal design voltage circuit noise magnetic noise encoding noise reduction noise robustness integrated circuit interconnections;multiprocessing systems system on chip vlsi error detection;chip;low power;complex system;system on chip;interconnected system;vlsi;worst case correct by design paradigm adaptive low power transmission scheme on chip networks systems on chip complex heterogeneous multiprocessors macrocells application specific interconnections intra chip interconnects soc design signal integrity low swing signalling error detection codes technology quality;networks on chip;multiprocessing systems;error detection;deep sub micron;systems on chip;robust design	Systems-on-Chip (SoC) are evolving toward complex heterogeneous multiprocessors made of many predesigned macrocells or subsystems with application-specific interconnections. Intra-chip interconnects are thus becoming one of the central elements of SoC design and pose conflicting goals in terms of low energy per transmitted bit, guaranteed signal integrity, and ease of design. This work introduces and shows first results on a novel interconnect system which uses low-swing signalling, error detection codes, and a retransmission scheme; it minimises the interconnect voltage swing and frequency subject to workload requirements and S/N conditions. Simulation results show that tangible savings in energy can be attained while achieving at the same time more robustness to large variations in actual workload, noise, and technology quality (all quantities easily mispredicted in very complex systems and advanced technologies). It can be argued that traditional worst-case correct-by-design paradigm will be less and less applicable in future multibillion transistor SoC and deep sub-micron technologies; this work represents a first example towards robust adaptive designs.	assistive technology;best, worst and average case;code;complex systems;electrical connection;error detection and correction;low-power broadcasting;programming paradigm;requirement;retransmission (data networks);signal integrity;simulation;system on a chip;transistor	Paolo Ienne;Patrick Thiran;Giovanni De Micheli;Frederic Worm	2002		10.1145/581199.581221	embedded system;electronic engineering;real-time computing;engineering	EDA	3.939820914606822	56.55941197415896	96966
25a74e4a74063624e1f9cdea81dbb3e27501ce5e	incorporating large-scale fpaas in analog design courses	analog ic;field programmable analog arrays;network synthesis;large scale systems field programmable analog arrays circuit testing laboratories programming profession system testing matlab voltage circuit synthesis network synthesis;field programmable analogue arrays;test cycle;integrated circuit design;large scale;analogue integrated circuits;field programmable analog array;large scale field programmable analog arrays;programming profession;voltage;educational courses;electronic engineering education;integrated circuit testing;system testing;integrated circuit testing analogue integrated circuits educational courses electronic engineering education field programmable analogue arrays integrated circuit design;circuit testing;large scale fpaa;reconfigurable analog signal processor large scale fpaa analog design courses large scale field programmable analog arrays test cycle analog ic;matlab;circuit synthesis;analog design courses;large scale systems;reconfigurable analog signal processor	The development of large-scale field-programmable analog arrays (TPAAs) provides a great opportunity for expanding the capabilities of analog design courses and their laboratories. These devices allow the complete design and test cycle of an analog IC to be explored within the span of a single course term. A brief discussion of our experience incorporating the reconfigurable analog signal processor (RASP) into courses and workshops is presented.	analog signal processing;field-programmability;random-access stored-program machine	Christopher M. Twigg;Paul E. Hasler	2007	2007 IEEE International Conference on Microelectronic Systems Education (MSE'07)	10.1109/MSE.2007.51	network synthesis filters;embedded system;electronic engineering;voltage;computer science;engineering;electrical engineering;system testing;computer engineering;integrated circuit design;field-programmable analog array	EDA	9.503612456022157	51.510773083834444	96983
cf448ab09833760895a911bfc8be9bd73e893b75	quantitative evaluation of formal based synthesis in asic design	design process;design flow;formal reasoning;logic synthesis;levels of abstraction;time to market;design space exploration;quantitative evaluation	Formal based synthesis allows design space exploration to identify optimized implementations conforming to the initial abstract specification. Wepropose to exploit the synergies between formal synthesis (at high level of abstraction) and logic synthesis (at lower levels of abstraction). In this way a two-fold goal is reached: quantitative figures are provided as a measureof the applicability of formal reasoning in the design process, and the good integration of the two phases in a unified design flow is demonstrated. Users' benefits include both improved quality of the design process (reduced time-to-market) and improved reliability of the final products (increased competitive profile).	application-specific integrated circuit	G. Bezzi;Massimo Bombana;Patrizia Cavalloro;Salvatore Conigliaro;Giuseppe Zaza	1994		10.1007/3-540-59047-1_55	computer architecture;computer science;systems engineering;formal equivalence checking;computer engineering	EDA	4.700998649556618	52.921478083722704	97182
aa7f023ffc71f357e5bf3cf837bc4d20d4b3c9ac	16-bit operating systems	operating system	16-bit microprocessors have only recently begun to be considered for applications. Now the advantages they can offer for colour graphics and input without keyboards make them competitive with the already established 8-bit devices. This review o f 16-bit operating systems covers design features, functions, resources and various problems that arise when several processes occur in one operating system.	16-bit;8-bit;graphics;microprocessor;operating system	Joe Gallacher	1983	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(83)90477-5	embedded system;embedded operating system;real-time computing;computer hardware;computer science;operating system	Arch	4.982802400011986	49.25352943210751	97274
05ef74814557a479d2df0f617a405078d0867ab9	character recognition using optical fibres and hardwired logic			optical fiber	P. L. O'Donovan	1978				Vision	7.623883138178479	49.7433843460761	97293
d4525be31a6a17f52a64c2997833faf64a3ffb94	an optimized tta-like vertex shader datapath for embedded 3d graphics processing unit	vectors vliw instruction sets multiprocessor interconnection parallel processing registers;instruction sets computer architecture computer graphic equipment coprocessors;multithreaded expanded vliw architecture optimized tta like vertex shader datapath embedded 3d graphics processing unit vliw architecture transport triggered architecture instruction level parallelism fine grained data transport user optimized interconnection network code density;computer graphic equipment;coprocessors;vliw;computer architecture;vectors;registers;parallel processing;multiprocessor interconnection;instruction sets	An alternative VLIW architecture of vertex shader datapath based on transport triggered architecture (TTA) is proposed in details. This architecture can exploit more instruction level parallelism (ILP) than traditional VLIW architecture by the fine-grained data transport. The proposed vertex shader architecture can also provide a simple and user-optimized inter-connection network which can efficiently reduce the complexity of interconnections design. The evaluation results show that the proposed architecture can achieve almost 18% reduction in interconnection number and 1.4 times improvement in code density compared with the multi-threaded expanded VLIW architecture (MT-eVLIW).	3d computer graphics;datapath;embedded system;graphics processing unit;instruction-level parallelism;interconnection;medium attachment unit;overhead (computing);parallel computing;shader;thread (computing);transport triggered architecture;very long instruction word	Jizeng Wei;Yisong Chang;Wei Guo;Jizhou Sun	2011	2011 IEEE/IFIP 19th International Conference on VLSI and System-on-Chip	10.1109/VLSISoC.2011.6081673	computer architecture;parallel computing;computer hardware;computer science;instruction set;transport triggered architecture	Arch	-0.7480425747161696	46.54876051082521	97950
e65f7991ff5127dac7b1b77ed55ae9fe6f777e1e	idsm: an improved disjoint signature monitoring scheme for processor behavioral checking	leona based system improved disjoint signature monitoring idsm scheme processor behavioral checking soft error embedded system flexible control flow error detection approach processor based system;watchdog processor vlsi dependability on line test soft error detection control flow checking;decision support systems indexes context fault diagnosis pipelines libraries;radiation hardening electronics electronic engineering computing embedded systems fault diagnosis fault tolerant computing multiprocessing systems	Soft errors with multiple erroneous bits have become a significant threat in embedded systems. New approaches must therefore be proposed to detect errors in a system without assumptions on the error multiplicity. Behavioral checking is in that case appealing. This paper presents a new extended and flexible control flow error detection approach, able to also cover errors in the critical variables of processor-based systems. The approach does not modify the initial system and is compatible with standards such as IEC 61508. Results on a Leon 3-based system are presented.	central processing unit;control flow;embedded system;emulator;entry point;error detection and correction;fault injection;leon;overhead (computing);program counter;prototype;systems design;watchdog timer	Salma Bergaoui;Pierre Vanhauwaert;Régis Leveugle	2014	2014 15th Latin American Test Workshop - LATW	10.1109/LATW.2014.6841915	embedded system;parallel computing;real-time computing;computer science	EDA	7.850418482072793	58.83738644179712	98073
6c7f8d6af815ff20ed109b9216fe333efb008cc3	run-time partial reconfiguration speed investigation and architectural design space exploration	cache storage;architectural design;random access memory;elektroteknik och elektronik;master burst cache;partial reconfiguration;logic design;direct memory access;reconfigurable architectures;electrical engineering electronic engineering information engineering;resource management;space exploration;xps hwicap design;icap;mst hwicap design;xilinx pr technology;data mining;runtime space exploration field programmable gate arrays read write memory random access memory fabrics hardware energy consumption throughput writing;reconfigurable architectures cache storage field programmable gate arrays logic design random access storage;run time partial reconfiguration speed investigation;bram hwicap design;internal configuration access port;fast ip core switching;random access storage;partial bitstream size;ip networks;opb hwicap design;field programmable gate arrays;architectural design space exploration;direct memory access cache;fpga fabric;switches;dma hwicap design;dedicated block ram cache;bram hwicap design run time partial reconfiguration speed investigation architectural design space exploration fast ip core switching direct memory access cache master burst cache dedicated block ram cache xilinx pr technology internal configuration access port icap fpga fabric partial bitstream size opb hwicap design xps hwicap design dma hwicap design mst hwicap design	Run-time Partial Reconfiguration (PR) speed is significant in applications especially when fast IP core switching is required. In this paper, we propose to use Direct Memory Access (DMA), Master (MST) burst, and a dedicated Block RAM (BRAM) cache respectively to reduce the reconfiguration time. Based on the Xilinx PR technology and the Internal Configuration Access Port (ICAP) primitive in the FPGA fabric, we discuss multiple design architectures and thoroughly investigate their performance with measurements for different partial bitstream sizes. Compared to the reference OPB HWICAP and XPS HWICAP designs, experimental results showthatDMA HWICAP and MST HWICAP reduce the reconfiguration time by one order of magnitude, with little resource consumption overhead. The BRAM HWICAP design can even approach the reconfiguration speed limit of the ICAP primitive at the cost of large Block RAM utilization.	bitstream;design space exploration;direct memory access;field-programmable gate array;open xml paper specification;overhead (computing);random-access memory;semiconductor intellectual property core	Ming Liu;Wolfgang Kuehn;Zhonghai Lu;Axel Jantsch	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272463	embedded system;parallel computing;logic synthesis;real-time computing;network switch;computer science;resource management;space exploration;operating system;direct memory access;field-programmable gate array	EDA	-1.408185351451393	52.03784542774167	98111
96ceba9075765c94cd635812398799084d4b3779	novel memory bus driver/receiver architecture for higher throughput	random access memory;design automation;commodity prices;logic design;multi valued logic;integrated memory circuits;adaptive multi level simultaneous bi directional transceiver;multivalued logic circuits;high speed memory bus interface;cache memory;current mode;process design;chip;system buses;receiver architecture;integrated memory circuits memory architecture system buses driver circuits cmos logic circuits logic design multivalued logic circuits impedance matching transceivers;logic synthesis;memory architecture;directional data;integrated circuit interconnections;cmos logic circuits;impedance matching;bidirectional control;driver circuits;bandwidth;transceivers;multivalued logic;switches;bi directional data bus;throughput multivalued logic bandwidth bidirectional control process design integrated circuit interconnections cmos logic circuits transceivers random access memory switches;current mode cmos logic synthesis;high speed;driver architecture;reading and writing;throughput;bandwidth receiver architecture throughput high speed memory bus interface current mode cmos logic synthesis multi valued logic driver architecture bi directional data bus design automation impedance matching adaptive multi level simultaneous bi directional transceiver cache memory	A high-speed memory bus interface which enables greater throughput for data reads and writes is described in this paper. Current mode CMOS logic synthesis methods are used to implement multi-valued logic (MVL) functions to create a high bandwidth bus. First, a fundamental bi-directional data bus for multiple logic levels is presented. Then a bi-directional data bus with impedance matching terminators is presented. Finally a novel Adaptive Multi-Level Simultaneous bi-directional Transceiver (AMLST) bus structure for cache or main memory is proposed. The proposed bus can balance the memory channel bandwidth with the instruction execution rate of modern processors. Despite the problems encountered in implementing complete systems with MVL circuits, among which are circuit speed and design automation support, there is great potential in the future for this approach.	cmos;cpu cache;central processing unit;computer data storage;directional statistics;impedance matching;logic synthesis;memory bus;throughput;transceiver	Gregory E. Beers;Lizy Kurian John	1998		10.1109/ICVD.1998.646614	bus sniffing;bus;std bus;embedded system;bus error;external bus interface;memory bus;electronic engineering;parallel computing;logic synthesis;electronic design automation;three-state logic;telecommunications;iebus;computer science;local bus;physical address;conventional pci;system bus;control bus;back-side bus;bus network;address bus	EDA	7.470341210986356	53.84208359727838	98296
054a93f5fade479b939b484eabaf39463f2bc936	cached-code compression for energy minimization in embedded processors	ip design;cached code compression;cache storage;instruction fetch energy;digital signal processing;control systems;low power electronics microprocessor chips embedded systems data compression cache storage;self clocked;data compression;reduced instruction set computing;static code size;active pixel sensor;system on a chip;computer architecture;embedded systems;decompression unit;low voltage;low power;engines;permission;ip design cached code compression energy minimization decompression unit embedded processor static code size instruction fetch energy;low power electronics;energy minimization;embedded computing costs reduced instruction set computing computer architecture system on a chip instruction sets permission engines digital signal processing control systems;embedded processor;cmos;image sensor;embedded computing;code compression;microprocessor chips;instruction sets	This paper contributes a novel approach for reducing static code size and instruction fetch energy for cache-based core processors running embedded applications. Our implementation of the decompression unit guarantees fast and lowenergy, on-they instruction decompression at each cache lookup. The decompressor is placed outside the core boundaries; therefore, processor architecture does not need any modi cation, making the proposed compression approach suitable to IP-based designs. Viability of our solution is assessed through extensive benchmarking performed on a number of typical embedded programs.	cpu cache;central processing unit;data compression;embedded system;energy minimization;lookup table;overhead (computing)	Luca Benini;Alberto Macii;Alberto Nannarelli	2001		10.1145/383082.383177	data compression;system on a chip;embedded system;reduced instruction set computing;computer architecture;parallel computing;computer science;control system;operating system;digital signal processing;instruction set;image sensor;cmos sensor;low voltage;cmos;energy minimization;low-power electronics	EDA	2.196419897985428	53.87033118856326	98760
ebb26c8ac1d648a7c35bc0c308ab83b1b064f123	data transmission with the battery utilization maximization	data transmission;bandwidth scheduling;3g devices;energy efficient;real time;embedded system;utility maximization;task scheduling;battery utilization;time constraint	With the growing popularity of 3G-powered devices, there are growing demands on energy-efficient data transmission strategies for various embedded systems. Different from the past work in energy-efficient real-time task scheduling, we explore strategies to maximize the amount of data transmitted by a 3G module under a given battery capacity. In particular, we present algorithms under different workload configurations with and without timing constraint considerations. Experiments were then conducted to verify the validity of the strategies and develop insights in energy-efficient data transmission.	download;embedded system;expectation–maximization algorithm;experiment;mobile device;real-time clock;scheduling (computing)	Che-Wei Chang;Tiefei Zhang;Chuan-Yue Yang;Ying-Jheng Chen;Shih-Hao Hung;Tei-Wei Kuo;Tianzhou Chen	2011	Journal of Computer Science and Technology	10.1007/s11390-011-1142-7	real-time computing;simulation;computer science;efficient energy use;data transmission	Embedded	-3.6787594610247316	59.62052615697522	98880
7006c9d8eb5f111970907d37c1cdd0729ce36b61	flexcore: utilizing exposed datapath control for efficient computing	microprocessors;fine grained control;publikationer;decoding;dynamic reconfiguration;software maintenance;decoding bandwidth hardware pipelines costs instruction sets microprocessors software maintenance size control computer architecture;code generation;konferensbidrag;size control;general purpose processor;computer architecture;embedded systems;system on chip;embedded application;pipelines;instruction bandwidth;artiklar;rapporter;vlsi;bandwidth;vlsi implementation;flexcore architecture;flexcore architecture wide control word fine grained control instruction decoding instruction bandwidth vlsi implementation embedded application flexsoc framework exposed datapath control;vlsi decoding embedded systems instruction sets microprocessor chips pipeline processing system on chip;instruction decoding;wide control word;flexsoc framework;pipeline processing;energy saving;microprocessor chips;exposed datapath control;instruction sets;hardware	rdquoWe introduce FlexCore, the first exemplar of an architecture based on the FlexSoC framework. Comprising the same datapath units found in a conventional five-stage pipeline, the FlexCore has an exposed datapath control and a flexible interconnect to allow the datapath to be dynamically reconfigured as a consequence of code generation. Additionally, the FlexCore allows specialized datapath units to be inserted and utilized within the same architecture and compilation framework. This study shows that, in comparison to a conventional five-stage general-purpose processor, the FlexCore is up to 40% more efficient in terms of cycle count on a set of benchmarks from the embedded application domain. We show that both the fine grained control and the flexible interconnect contribute to the speedup. Furthermore, according to our VLSI implementation study, the FlexCore architecture offers both time and energy savings. The exposed FlexCore datapath requires a wide control word. The conducted evaluation confirms that this increases the instruction bandwidth and memory footprint. This calls for efficient instruction decoding as proposed in the FlexSoC	datapath	Martin Thuresson;Magnus Själander;Magnus Björk;Lars J. Svensson;Per Larsson-Edefors;Per Stenström	2007		10.1109/ICSAMOS.2007.4285729	computer architecture;parallel computing;real-time computing;computer science	EDA	0.38403618753183405	50.007792404081094	99403
d3ef9722cd4cdedd0bdce5dc6a271c9629241e18	practical design of a computation and energy efficient hardware task scheduler in embedded reconfigurable computing systems	energy efficiency;concurrent computing;conference_paper;logic design;reconfigurable computing;energy efficient;processor scheduling;computer aided instruction;circuit design;parallel circuit design;design flow;energy efficient hardware task scheduler embedded reconfigurable computing system parallel circuit design fpga computation intensive tasks instruction set processor scheduling algorithm energy consumption design flow computation efficient hardware task scheduler;fpga;instruction set processor;embedded systems;scheduling algorithm;computer instructions;energy consumption;scheduling;computation efficient hardware task scheduler;energy efficient hardware task scheduler;energy eciency;fabrics;scheduling embedded systems field programmable gate arrays instruction sets logic design power consumption;embedded reconfigurable computing system;computation intensive tasks;power consumption;field programmable gate arrays;task scheduling;power demand;embedded computing energy efficiency hardware processor scheduling field programmable gate arrays computer aided instruction concurrent computing energy consumption circuit synthesis fabrics;circuit synthesis;embedded computing;instruction sets;hardware	By utilizing massively parallel circuit design in FPGAs, the overall system efficiency, in terms of computation efficiency and energy efficiency, can be greatly enhanced by offloading some computation-intensive tasks which are originally executed in the instruction set processor to the FPGA fabric. In essence, a hardware task scheduler is needed. However, most of the work in the literature considers scheduling algorithms which are unable or difficult to be implemented using the design flows in current development platform. Moreover, little of the work takes energy consumption into consideration. In this paper, we present the design of a hardware task scheduler which takes energy consumption into consideration, and can be readily implemented using current design flows	algorithm;circuit design;computation;embedded system;field-programmable gate array;reconfigurable computing;run time (program lifecycle phase);scheduling (computing);series and parallel circuits;windows task scheduler	Tyrone Tai-On Kwok;Yu-Kwong Kwok	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639488	computer architecture;parallel computing;real-time computing;concurrent computing;computer science;operating system;efficient energy use;scheduling;field-programmable gate array	Embedded	-1.1759269873086338	54.68155848559565	99625
421a296de90723292373bba1514840de676fce92	hardware/software co-compilation with the nymble system	kernel;microarchitecture;hardware registers kernel microarchitecture switches computer architecture;memory management;library service;nymble compiler system;fpga based reconfigurable computer;hardware software cocompilation;interface logic;hardware accelerator;fpga local memory;computer architecture;shared memory systems;operating system;registers;partitioning directive;c code;cache flush;field programmable gate arrays;program compilers;shared memory systems field programmable gate arrays program compilers;switches;shared main memory;memory management hardware software cocompilation nymble compiler system c code partitioning directive hardware accelerator fpga based reconfigurable computer interface logic cache flush fpga local memory shared main memory operating system library service;hardware	The Nymble compiler system accepts C code, annotated by the user with partitioning directives, and translates the indicated parts into hardware accelerators for execution on FPGA-based reconfigurable computers. The interface logic between the remaining software parts and the accelerators is automatically created, taking into account details such as cache flushes and copying of FPGA-local memories to the shared main memory. The system also supports calls from hardware back into software, both for infrequent operations that do not merit hardware area, as well as for using operating system / library services such as memory management and I/O.	benchmark (computing);cpu cache;compiler;computation;computer data storage;cryptography;field-programmable gate array;gnu c library;hardware acceleration;mathematical optimization;memory management;microprocessor;operating system;optimal control;reconfigurable computing	Jens Huthmann;Björn Liebig;Julian Oppermann;Andreas Koch	2013	2013 8th International Workshop on Reconfigurable and Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2013.6581538	embedded system;computer architecture;parallel computing;kernel;real-time computing;cpu cache;hardware acceleration;microarchitecture;network switch;computer science;operating system;processor register;field-programmable gate array;memory management	Arch	-2.300667182859699	49.0443922608331	99635
ec932ca074c90a60c07991ca33e8979f7744f7d6	invited paper: system-wide fault management based on ieee p1687 ijtag	system wide fault management mechanism;instruments;ieee standards;fault tolerant;instruments registers fault tolerant systems fault tolerance system on a chip resilience;ijtag;semiconductor device reliability;semiconductor device reliability fault tolerance ieee standards semiconductor device manufacture;system on a chip;chip;electronic devices;embedded instrumentation;fault tolerant system;resilience;semiconductor products;fault tolerant systems;registers;fault detection;fault tolerance;dft standard ieee p1687 ijtag;semiconductor device manufacture;ieee 1687;embedded instrumentation failure resilience fault management ieee 1687 ijtag;soft error;fault management;fault detection information system wide fault management mechanism fault tolerance semiconductor products electronic devices dft standard ieee p1687 ijtag;failure resilience;fault detection information	Fault tolerance and fault management mechanisms are necessary means to reduce the impact of soft errors and wear out in electronic devices. The semiconductor products manufactured with latest and emerging processes are increasingly affected by these effects. The paper describes a new general scalable fault management architecture based on the latest upcoming DFT standard IEEE P1687 IJTAG. The standard allows to create an efficient and regular network for handling fault detection information as well as to manage test and system resources as a system-wide background process during the system operation.	background process;fault detection and isolation;fault tolerance;scalability;semiconductor	Artur Jutman;Sergei Devadze;Igor Aleksejev	2011	6th International Workshop on Reconfigurable Communication-Centric Systems-on-Chip (ReCoSoC)	10.1109/ReCoSoC.2011.5981520	embedded system;fault tolerance;real-time computing;computer science;software fault tolerance;psychological resilience	EDA	9.699157453672735	58.253475384652965	99902
18d8fe55dd954f070318230f08ffe7b46cb201a4	system level modelling of rf ic in systemc-wms	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices	This paper proposes a methodology for modelling and simulation of RF systems in SystemC-WMS. Analog RF modules have been described at system level only by using their specifications. A complete Bluetooth transceiver, consisting of digital and analog blocks, has been modelled and simulated using the proposed design methodology. The developed transceiver modules have been connected to the higher levels of the Bluetooth stack described in SystemC, allowing the analysis of the performance of the Bluetooth protocol at all the different layers of the protocol stack.	bluetooth;c file input/output;care-of address;high- and low-level;interconnection;nonlinear system;protocol stack;radio frequency;simulation;standard streams;systemc;transceiver;web map service	Simone Orcioni;Mauro Ballicchia;Giorgio Biagetti;Rocco D. d'Aparo;Massimo Conti	2008	EURASIP J. Emb. Sys.	10.1155/2008/371768	embedded system;real-time computing;computer science	EDA	5.250057804532627	52.46290598761959	99926
cdbdedd49bdbe4cbf8e838adf88ce0711a9ef534	bitwidth-aware high-level synthesis for designing low-power dsp applications	digital signal processing;resource management;hardware accelerator;automatic generation;wireless communication;high level synthesis;low power;signal processing;field programmable gate array bitwidth aware high level synthesis low power dsp application digital signal processing hardware accelerators bit width information xilinx virtex 5 fpga;signal processing field programmable gate arrays high level synthesis;power consumption;field programmable gate arrays	Digital Signal Processing (DSP) applications are widely used from wireless communications to automotive. Their ever growing complexity and throughput still require significant parts to be implemented as dedicated hardware accelerators. A High-Level Synthesis (HLS) flow to automatically generate hardware accelerators for DSP applications is proposed in this paper. By considering bit-width information during all the synthesis process both area and power consumption are optimized. Experimental results show that the proposed approach allows to generate architectures that offer better computation accuracy for a given area and/or power consumption. Effectiveness of the approach is shown through several design experiments in the DSP domain realized on a Xilinx Virtex-5 FPGA.	computation;digital signal processing;experiment;field-programmable gate array;hardware acceleration;high- and low-level;high-level synthesis;low-power broadcasting;throughput	Ghizlane Lhairech-Lebreton;Philippe Coussy;Dominique Heller;Eric Martin	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724566	embedded system;electronic engineering;real-time computing;computer science	EDA	3.0854859758417765	51.8302183787944	99980
4218832b2d8d8f76d303ad95ec0227d853af63ed	an fpga-based specific processor for blokus duo	trees mathematics computer games field programmable gate arrays logic design;logic design;trees mathematics;games software field programmable gate arrays hardware registers random access memory measurement;software player fpga based specific processor blokus duo game low cost spartan 6 fpga search tree pentobi software application;field programmable gate arrays;computer games	In this article, we present a design of a specific processor for Blokus Duo game. This design has been submitted to the ICFPT'13 Design Competition and implemented on a low-cost Spartan-6 FPGA. Our player applies several techniques to identify which movements are potentially interesting, and applies a search-tree in order to evaluate the consequences of each of those options. To achieve an efficient implementation we have developed custom modules to manage the board and to identify whether a block can be placed or not in a given vertex. The results demonstrate that our design is competitive, even against advanced Blokus Duo players, such as the Pentobi software application considered as the best available software player.	accessibility;clock rate;combinational logic;computation;emoticon;field-programmable gate array;hardware acceleration;monte carlo tree search;static timing analysis;vertex (graph theory)	Javier Olivito;Carlos González;Javier Resano	2013	2013 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2013.6718428	embedded system;logic synthesis;real-time computing;computer science;theoretical computer science;operating system;field-programmable gate array	EDA	9.651601158705244	47.7410353559929	100125
1b0aae310295e155a017033fa01df24009f091b5	the sand neurochip and its embedding in the mind system	red sistolica;arquitectura circuito;circuit vlsi;circuit architecture;microelectronique;microelectronica;chip;vlsi circuit;systolic network;radial basis function network;architecture circuit;software component;reseau systolique;microelectronics;circuito vlsi;reseau neuronal;red neuronal;artificial neural network;neural network	"""The system MiND (Multipurpose integrated Neural Device) is a tool for the development of artiicial neural network applications which integrates hardware and software components. It includes a PCI neuro{board with up to four SAND (Simple Applicable Neural Device) neuro{chips. The neuro{board accelerates feedforward networks, Radial{ Basis-Function networks, and Kohonen feature maps. There are several simple to use software layers for exploiting the neuro{board. At the bottom , there is the driver's C interface. Secondly, a number of C++ network classes are built on the C{drivers. Thirdly, comfortable simulators with graphical interfaces base on the C++ classes. These stem from a pool of \predeened"""" simulators provided by the MiND system. Each simulator is constituted by a network deenition written in the neural network description language CONNECT, and by an interface deenition script. The interface deenition is based on a C++ network class generated from the CONNECT deenition, and on abstract graphical user interface classes. A user can develop own network or interface deenitions. Also, the C++ network classes can be exported for integration into custom applications."""	artificial neural network;c++ classes;classful network;component-based software engineering;computer hardware;feedforward neural network;graphical user interface;mind;network description language;neurochip;radial basis function;self-organizing map;simulation	Thomas Fischer;Wolfgang Eppler;Hartmut Gemmeke;Gerd Kock;Thomas Becher	1997		10.1007/BFb0020320	chip;embedded system;interface description language;computer science;artificial intelligence;theoretical computer science;component-based software engineering;operating system;machine learning;distributed computing;radial basis function network;microelectronics;artificial neural network;algorithm	ML	6.815011896994869	47.605276944143746	100455
7fbb6b0dc20ea3be81843e0aa0bbd78bc706e36a	a layered adaptive verification platform for simulation, test, and emulation	design model;logic simulation;platform based design;automatic test pattern generation;hardware description languages;computer architecture hardware description languages logic testing logic simulation automatic test pattern generation;satisfiability;computer architecture;logic testing;switch level simulator layered adaptive verification platform silicon transaction adapter based architecture design model hardware emulator modular layered testbench mltb approach generic testbench kernel high level simulator rtl simulator gate level simulator;emulation hardware design languages automatic testing switches acceleration ieee press silicon design engineering communication system control tree data structures;adaptive architecture	This adaptive architecture for structuring testbenches accommodates various models of a design, from transaction to silicon. Moreover, the adapter-based architecture supports the execution of design models on different simulators (high level, RTL, gate level, and switch level), hardware emulators (the testbench runs entirely on the emulator), and even testers. Here, we present a modular, layered testbench (MLTB) approach to building a testbench. This approach is similar to platform-based design. It consists of a generic testbench kernel (TBK), connected through a bus to testbench elements. Our verification platform also satisfies another meaning of platform: a set of connected tools or a powerful tool environment, normally with an attached database, that acts as a platform for verification.	adaptive architecture;database;emulator;high-level programming language;platform-based design;simulation;test bench	Martin Zambaldi;Wolfgang Ecker;Renate Henftling;Matthias Bauer	2004	IEEE Design & Test of Computers	10.1109/MDT.2004.73	embedded system;computer architecture;real-time computing;computer science;automatic test pattern generation;operating system;logic simulation;hardware description language;intelligent verification;semulation;satisfiability	EDA	6.361716381451986	52.74259535932364	100611
1b1403ccdcdb947b291e4f97cf975a5777a6502e	flushing-enabled loop pipelining for high-level synthesis	integer linear programming loop pipelining high level synthesis pipelined execution loop iterations pipeline flushing pipeline synthesis;spin torque oscillators;pipeline processing throughput schedules pipelines dynamic scheduling delays;synchronization;associative memory;processor scheduling high level synthesis integer programming linear programming pipeline processing	Loop pipelining is a widely-accepted technique in high-level synthesis to enable pipelined execution of successive loop iterations to achieve high performance. Existing loop pipelining methods provide inadequate support for pipeline flushing. In this paper, we study the problem of enabling flushing in pipeline synthesis and examine its implications in scheduling and binding. We propose novel techniques for synthesizing a conflict-aware flushing-enabled pipeline that is robust against potential resource collisions. Experiments with real-life benchmarks show that our methods significantly reduce the possibility of resource collisions compared to conventional approaches while conserving hardware resources and achieving near-optimal performance.	benchmark (computing);experiment;heuristic;high- and low-level;high-level synthesis;iteration;name binding;pipeline (computing);real life;scheduling (computing);throughput	Steve Dai;Mingxing Tan;Kecheng Hao;Zhiru Zhang	2014	2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2593069.2593143	software pipelining;synchronization;loop inversion;computer architecture;parallel computing;real-time computing;telecommunications;computer science;pipeline	EDA	-1.665289204822463	51.69584159079858	100630
67c92b58d1a527aa6dfd8c7388e9a09885b7c4b4	a study of multi-core processor design with asynchronous interconnect using synchronous design tools	system level asynchronous design;multi core processor;design tool		multi-core processor;processor design;synchronous circuit	Katsunori Tanaka;Yuichi Nakamura;Atsushi Atarashi	2008	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.1.58	asynchronous system;multi-core processor;computer architecture;parallel computing;real-time computing;computer science;operating system;synchronizer	EDA	5.395936727847063	51.06626497589643	100668
8a7195b3fd94743f3adef1dffac9d062bf82117d	model-driven design and generation of new multi-facet arbiters: from the design model to the hardware synthesis	model design;design model;generic model;multi facet arbiter;multivariable systems;hardware synthesis;design flow;multi facet arbiter architecture model architecture template decentralized parallel tree design model design space generator model driven design flow;indexing terms;model driven design flow;design space;system on a chip;automatic generation;round robin arbiter model driven design scalable multifacet arbiter systematic model driven flow template based modular design design model derivation phase architecture model design phase arbiter hardware implementation phase arbiter hardware generation phase scalable decentralized parallel tree structure modular algorithms;chip;generator;architecture template;round robin;tree structure;decentralized parallel tree;on the fly;modular design;hardware design;design;design space exploration;multivariable systems computer interfaces design;architecture model;computer interfaces;hardware bandwidth system on a chip systematics space exploration integrated circuit modeling design methodology;high speed;hardware implementation	Designing of arbiters has become increasingly important due to their wide use in the areas such as multi-processor systems-on-a-chip and on-chip or off-chip high-speed cross-bar switches and networks. In this paper, we proposed a new systematic model-driven flow for designing the new scalable multi-facet arbiters through a 3-phase process combined with the template-based modular design approach that includes the design model derivation phase, the architecture model (or template) design phase as well as the arbiter hardware implementation and generation phase. First, we described the phase 1 of the design flow of how to induce an arbiter design model in detail by careful analysis of arbiter design issues and systematic design space exploring the construction of the model. Then, we continue to discuss the phase 2 of how to derive an architecture model or template using the reusability, modularity and expansibility techniques. With both the design and the architecture models, designers can easily design or at least understand and thus choose many kinds of different but better arbiters efficiently. Finally, we have developed a scalable decentralized parallel tree structure and corresponding modular algorithms for efficient arbiter hardware implementation, which also is the final phase of our proposed 3-phase model-driven design flow. Moreover, based on this modular and reusable hardware implementation structure as well as the algorithms, we have designed a parametric arbiter generator that automatically generates various multi-facet arbiters. Using this hardware implementation design and the generator, not only a fast and small round-robin arbiter but also other type arbiters were designed and generated on the fly. The hardware implementation algorithms, the generator, and the experiment results all were given to verify their performances. To our knowledge, this is the first time that such a systematic model-driven design approach is proposed in the practical hardware design and such a multi-facet arbiter is designed.	algorithm;arbiter (electronics);model-driven architecture;model-driven engineering;model-driven integration;modular design;multiprocessing;network switch;on the fly;performance;round-robin scheduling;scalability;system on a chip;tree structure	Jer-Min Jou;Yun-Lung Lee;Sih-Sian Wu	2011	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2011.2139211	chip;system on a chip;embedded system;design;computer architecture;parallel computing;real-time computing;computer science;design flow;operating system;tree structure;modular design	EDA	6.141868899712144	53.202325468915156	100712
28c2c12d868a54578904e95bd30a27aa1edfce5f	"""authors' reply to """"a note on architectures for large-capacity cams"""""""	cam;integrated circuit;large capacity;taxonomy;content addressable memory;associative memory	Recently, we presented a survey and analysis of large-capacity content addressable memories (CAMs). In his note, Parhami correctly points out that we omitted a reference to his seminal survey paper in this field; we regret this omission. Other contentions in his note are addressed in this reply. The motivation and structure of the original paper are reviewed to set the context. The parametric and graphical aspects of the paper, which we call the ''taxonomy'', are indeed original, and reiterated as accurate. Our emphasis in the paper was, justifiably, on integrated circuit implementations and real physical issues relating to such implementations.		Kenneth J. Schultz;P. Glenn Gulak	1997	Integration	10.1016/S0167-9260(97)00011-4	embedded system;cam;computer science;electrical engineering;artificial intelligence;integrated circuit;content-addressable memory;algorithm;taxonomy	Vision	7.021770878684822	53.81886149302757	100739
a7fb4a8b755eee6542d844d898d55b639a1b61bc	efficient virtual memory sharing via on-accelerator page table walking in heterogeneous embedded socs		Shared virtual memory is key in heterogeneous systems on chip (SoCs) that combine a general-purpose host processor with a many-core accelerator, both for programmability and performance. In contrast to the full-blown, hardware-only solutions predominant in modern high-end systems, lightweight hardware-software co-designs are better suited in the context of more power- and area-constrained embedded systems and provide additional benefits in terms of flexibility and predictability. As a downside, the latter solutions require the host to handle in software synchronization in case of page misses as well as miss handling. This may incur considerable run-time overheads.  In this work, we present a novel hardware-software virtual memory management approach for many-core accelerators in heterogeneous embedded SoCs. It exploits an accelerator-side helper thread concept that enables the accelerator to manage its virtual memory hardware autonomously while operating cache-coherently on the page tables of the user-space processes of the host. This greatly reduces overhead with respect to host-side solutions while retaining flexibility. We have validated the design with a set of parameterizable benchmarks and real-world applications covering various application domains. For purely memory-bound kernels, the accelerator performance improves by a factor of 3.8 compared with host-based management and lies within 50% of a lower-bound ideal memory management unit.	arm architecture;application programming interface;cpu cache;central processing unit;embedded system;general-purpose markup language;library (computing);linked data structure;linux;manycore processor;memory bound function;memory management unit;overhead (computing);page table;parallel computing;processor design;synthetic intelligence;system on a chip;user space;virtual machine manager	Pirmin Vogel;Andreas Kurth;Johannes Weinbuch;Andrea Marongiu;Luca Benini	2017	ACM Trans. Embedded Comput. Syst.	10.1145/3126560	page table;data diffusion machine;real-time computing;parallel computing;memory management unit;overlay;demand paging;computer science;memory management;page fault;embedded system;virtual memory	Arch	-3.087535594785881	50.04235368000001	100749
05e374d514141cd4aa08385864f486d2fbb984c4	a polymorphous computing fabric	image processing;generic model;dynamic reconfiguration;fabrics finite impulse response filter digital signal processing chips image processing computer architecture control system synthesis signal synthesis process control communication system control control systems;implementation;computer model;accelerators;performance;parallel architectures signal processing;mathematics and computing;hardware accelerator;system on a chip;synthesis;particle accelerators;parallel architectures;signal processing;fir filter;polymorphism;firs;altera excalibur arm embedded processor system polymorphous computing fabric based system digital signal processing image processing system on a programmable chip parameterized cellular architecture global memory host control processor random access computing models mimd spmd systolic flow dynamic communication pattern reconfiguration k means clustering algorithm fir filters n tap fir filter vector by matrix multiplication;high performance;architecture;random access	The notion of cellular computation—collective computation by an array of regularly interconnected nodes—has been a recurring theme in computer architecture since the early days of computer science. Ranging from cellular automata to systolic arrays, a large body of algorithms and architectures have centered on the notion of a fabric composed of simple automata interacting with immediate neighbors in an n-dimensional mesh. Fabric-based architectures are particularly attractive for efficient layout in VLSI, prompting scores of designs for specialized chips to solve specific computational problems, especially in image and array processing. However, this approach is not cost-effective, with very few designs having ever been fabricated and virtually none seeing widespread use. We propose a polymorphous computing fabric-based system (FBS) well suited to digital signal processing (DSP) and image processing applications. We have implemented our design on a system on a programmable chip (SoPC). The fabric’s highly parameterized cellular architecture enables customized synthesis of fabric instances to achieve high performance for different classes of applications. The system’s innovative global memory provides a host control processor with random access to all the variables and instructions on the fabric. The fabric supports several computing models including multiple instruction, multiple data (MIMD); single program, multiple data (SPMD); and systolic flow and permits dynamic reconfiguration of communication patterns. To illustrate the capabilities of our approach, we present two fabric instances with implementations of representative applications including a k-means clustering algorithm, a bank of finite impulse response (FIR) filters, an N-tap FIR filter (N is the number of taps of the filter), and a vector-by-matrix multiplication. Each fabric instance holds 52 cells on the Altera Excalibur ARM embedded processor system.	algorithm;array processing;automata theory;cellular architecture;cellular automaton;cluster analysis;computation;computational problem;computer architecture;computer science;digital signal processing;embedded system;fabric computing;finite impulse response;function-behaviour-structure ontology;image processing;interaction;k-means clustering;mimd;matrix multiplication;random access;spmd;system on a chip;systolic array;very-large-scale integration	Christophe Wolinski;Maya Gokhale;Kevin McCabe	2002	IEEE Micro	10.1109/MM.2002.1044300	system on a chip;embedded system;polymorphism;parallel computing;real-time computing;performance;image processing;computer science;architecture;operating system;signal processing;programming language;implementation;random access	Arch	4.539719219722508	46.83196256868616	100845
a1d16145b6d761cfb01c0e6f18ca4e57546087e7	optimising designs by combining model-based and pattern-based transformations	optimising compilers;syntax driven pattern matching transformation design pattern optimisation mathematical model based transformation pattern based transformation high level description;pattern matching object oriented programming optimising compilers;random access memory;design optimization hardware mathematical model solid modeling field programmable gate arrays design methodology pattern matching pipeline processing biological system modeling transforms;design pattern optimisation;data reuse;object oriented programming;system performance;system on a chip;high level description;arrays;syntax driven pattern matching transformation;mathematical model based transformation;pattern matching;transforms;pattern based transformation;mathematical model;optimization;hardware	We present a methodology for optimising designs written in high-level descriptions, combining mathematical model-based transformations with syntax-driven pattern-matching transformations, showing how the two kinds of transformation can benefit each other. We evaluate thismethodology by implementing an instance, combining a model-based transformation for data reuse with pattern-based transformations to improve its output. Results for three benchmarks show the implemented framework can improve system performance by up to 57 times.	high- and low-level;mathematical model;pattern matching	Qiang Liu;Tim Todman;José Gabriel F. Coutinho;Wayne Luk;George A. Constantinides	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272283	system on a chip;computer science;theoretical computer science;machine learning;pattern matching;mathematical model;computer performance;object-oriented programming;algorithm	EDA	2.732415862380607	51.440480205271456	101149
84da9dfd1408073ebd21592da87dd892d253d460	automatic software hardware co-design for reconfigurable computing systems	resource utilization;field programmable gate array;microprocessor;reconfigurable system;hardware software codesign;reconfigurable computing;resource allocation;reconfigurable architectures;co scheduling;scheduling algorithm field programmable gate arrays processor scheduling microprocessors routing hardware design languages cryptography embedded computing computer architecture resource management;scheduling field programmable gate arrays hardware software codesign reconfigurable architectures resource allocation;hardware software partitioning;scheduling;high performance reconfigurable computers automatic software hardware co design reconfigurable computing systems co scheduling field programmable gate array automatic task partitioning fpga hardware microprocessor resource utilization;automatic task partitioning;field programmable gate arrays;high performance reconfigurable computers;high performance;automatic software hardware co design;fpga hardware;reconfigurable computing systems	A formal methodology for automatic hardware-software partitioning and co-scheduling between the muP and the field programmable gate array (FPGA) has not yet been established. Current work in automatic task partitioning and scheduling for the reconfigurable systems strictly addresses the FPGA hardware, and does not take advantage of the synergy between the microprocessor and the FPGA. In this research, we consider the problem of formalizing a co-scheduling methodology and develop a set of intuitive tools to assist users in realizing the full potential of an RC architecture. Scheduling is critical for efficient resource utilization and achieving speedup in high performance reconfigurable computers (HPRC). The primary targets of this research are reconfigurable computing (RC) systems that have both microprocessors and FPGAs.	computer;field-programmable gate array;microprocessor;reconfigurable computing;scheduling (computing);speedup;synergy	Proshanta Saha	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380702	embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;field-programmable gate array	EDA	1.5382498762673906	51.19448661911548	101333
4acc5bd8ccf018cf94d61d9796fb521ec1c142f2	a runtime framework for robust application scheduling with adaptive parallelism in the dark-silicon era	silicon;runtime silicon multicore processing voltage control parallel processing integrated circuit reliability;voltage control;runtime;soft error reliability application parallelism dark silicon multicore scheduling process variations;multicore processing;integrated circuit reliability;parallel processing	With deeper technology scaling accompanied by a worsening power wall, an increasing proportion of chip area on a chip multiprocessor (CMP) is expected to be occupied by dark silicon. At the same time, design challenges due to process variations and soft errors in integrated circuits are projected to become even more severe. It is well known that spatial variations in process parameters introduce significant unpredictability in the performance and power profiles of CMP cores. By mapping applications onto the best set of cores, process variations can potentially be used to our advantage in the dark-silicon era. In addition, the probability of occurrence of soft errors during application execution has been found to be strongly related to the supply voltage and operating frequency values, thus necessitating reliability awareness within runtime voltage scaling schemes in contemporary CMPs. In this paper, we present a novel framework that leverages the knowledge of variations on the chip to perform runtime application mapping and dynamic voltage scaling to optimize system performance and energy, while satisfying dark-silicon power constraints of the chip as well as application-specific performance and reliability constraints. Our experimental results show average savings of 10%–71% in application service times and 13%–38% in energy consumption, compared with prior work.	clock rate;dark silicon;dynamic voltage scaling;frequency scaling;heuristic;image scaling;integrated circuit;multi-core processor;multiprocessing;network on a chip;run time (program lifecycle phase);scheduling (computing);soft error;spectral leakage	Nishit Ashok Kapadia;Sudeep Pasricha	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2016.2594238	multi-core processor;embedded system;parallel processing;electronic engineering;parallel computing;real-time computing;computer science;operating system;silicon	EDA	-4.489273542557437	55.84023401672753	101683
ca1ec7fd9a74c66a0786e6b1a0790cbfc3c2d5d5	instruction transfer and storage exploration for low energy vliws	mediabench application;storage exploration;low energy;energy efficient;iste;software controlled clustered loop buffer;multimedia application;energy use;energy storage buffer storage application software registers vliw energy consumption multiprocessor interconnection networks energy efficiency embedded system multimedia systems;vliw;embedded systems;embedded processor multimedia application software controlled clustered loop buffer vliw iste instruction transfer storage exploration mediabench application;memory hierarchy;embedded processor;instruction sets embedded systems;instruction transfer;instruction sets	For multimedia applications, loop buffering is an efficient mechanism to reduce the power in the instruction memory of embedded processors. In particular, software controlled clustered loop buffers are potentially very energy efficient. However current compilers for VLIW do not fully exploit the potentials offered by such a clustered organization. This paper presents ITSE, Instruction Transfer and Storage Exploration, a methodology to minimize the instruction memory energy using a software controlled clustered loop buffer as a basis. Results for the MediaBench application suite show 61% reduction (on average) in energy in the instruction memory hierarchy as compared to traditional, existing non-clustered approaches to the loop buffer without compromising performance	central processing unit;cluster analysis;compiler;cyc;electronic data processing;embedded system;h.262/mpeg-2 part 2;iteration;mpeg-2;mathematical optimization;memory hierarchy;video decoder;z-buffering	Tom Vander Aa;Murali Jayapala;Henk Corporaal;Francky Catthoor;Geert Deconinck	2006	2006 IEEE Workshop on Signal Processing Systems Design and Implementation	10.1109/SIPS.2006.352597	embedded system;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;operating system;instruction set;efficient energy use	Arch	-2.696800710475648	53.68828528415672	101757
8f68b79e413a44760327b24fd357eeb0f9f9dcec	occam-pi for programming of massively parallel reconfigurable architectures	embedded systems;engineering and technology;teknik och teknologier;inbaddad systemteknik	Massively parallel reconfigurable architectures, which offer massive parallelism coupled with the capability of undergoing run-time reconfiguration, are gaining attention in order to meet the increased computational demands of high-performance embedded systems. We propose that the occam-pi language is used for programming of the category of massively parallel reconfigurable architectures. The salient properties of the occam-pi language are explicit concurrency with built-in mechanisms for interprocessor communication, provision for expressing dynamic parallelism, support for the expression of dynamic reconfigurations, and placement attributes. To evaluate the programming approach, a compiler framework was extended to support the language extensions in the occam-pi language and a backend was developed to target the Ambric array of processors. We present two case-studies; DCT implementation exploiting the reconfigurability feature of occam-pi and a significantly large autofocus criterion calculation based on the dynamic parallelism capability of the occam-pi language. The results of the implemented case studies suggest that the occam-pi-language-based approach simplifies the development of applications employing run-time reconfigurable devices without compromising the performance benefits.		Zain-ul-Abdin;Bertil Svensson	2012	Int. J. Reconfig. Comp.	10.1155/2012/504815	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;implicit parallelism	HPC	-1.8020772577044333	49.00220616454489	101838
0ad1f754b1b12b55704c791a0537f109b2731c16	a new bus assignment algorithm for a shared bus switch fabric		An architecture for a Cell Switch Fabric (CSF) with a new bus assignment strategy is presented. The proposed architecture has a modular structure with a chip partitioning oriented to avoid the system from falling down totally, thus achieving expandability and increasing reusability. A discussion about different algorithms for the bus assignment is given. The shared bus is assigned in a cyclic and rotative way, switching microcells instead of cells. The CSF is built in four PCBs where every one has a capacity for four ports at 164.323 Mbps, the external port rate is 155.52 Mbps. A microcontroller realizes some tests and communicates with a PC, which runs a test, verification and CSF configuration program. Some parameters about the CSF behavior are measured too. Each Port was implemented on a CPLD FLEX10K100 and the Switch Control Block on a circuit MAX7128, both from Altera Company.		D. Torres Roman;J. Gonzalez;M. Guzman	2000	VLSI Design	10.1155/2000/26425	embedded system;electronic engineering;parallel computing;computer hardware;telecommunications;engineering;local bus;operating system;system bus;control bus;back-side bus;bus network;computer network	EDA	6.781392459028982	49.054928570995	101942
0aa01ae11da9516aa48e9fce6a0d5be5e89132e1	design and implementation of an object tracker on a reconfigurable system on chip	reconfigurable system;xilinx evaluation board;data flow management;hardware software partitioning feature tracker embedded reconfigurable hardware system data flow management xilinx evaluation board;system on a chip field programmable gate arrays hardware pervasive computing computer science ubiquitous computing object detection real time systems embedded system streaming media;chip;embedded systems;embedded reconfigurable hardware system;design and implementation;hardware software partitioning;data flow analysis;feature tracker;ubiquitous computing;field programmable gate arrays;data flow;reconfigurable hardware;ubiquitous computing data flow analysis embedded systems field programmable gate arrays	This paper presents the design and implementation of a feature tracker on an embedded reconfigurable hardware system. Contrary to other works, the focus here is on the efficient hardware/software partitioning of the feature tracker algorithm, a viable data flow management as well as an efficient use of memory and processor features. The implementation is done on a Xilinx evaluation board and the results provided show the superiority of our implementation compared to the other works	algorithm;dataflow;embedded system;field-programmable gate array;microprocessor development board;reconfigurable computing;robot;software deployment;system on a chip	Felix Mühlbauer;Christophe Bobda	2006	Seventeenth IEEE International Workshop on Rapid System Prototyping (RSP'06)	10.1109/RSP.2006.13	chip;embedded system;data flow diagram;computer architecture;real-time computing;reconfigurable computing;computer science;data-flow analysis;ubiquitous computing;field-programmable gate array	EDA	3.2911471134659998	49.24973805349093	101977
5c8a7df56ac870edd944b61808edf0e2b5bd2e26	high-level synthesis with simd units	resource utilization;simd functional units;energy efficient;resource allocation;rtl circuit generation high level synthesis hls design methodology parallelism resource utilization energy efficiency single instruction multiple data units simd functional units scheduler;circuit cad high level synthesis parallel processing scheduling resource allocation;design space;single instruction multiple data;high performance design;high level synthesis;performance improvement;energy consumption;scheduling;circuit cad;functional unit;high performance;high level synthesis computer architecture hardware space exploration application software arithmetic logic circuits computer science high performance computing;parallel processing;energy saving;design methodology	This paper presents novel techniques to integrate the use of Single Instruction Multiple Data (SIMD) functional units in a high-level synthesis (HLS) design methodology. SIMD functional units can be configured to operate in one or more SIMD modes, in which they process multiple sets of smaller bitwidth operands in parallel. Conceptually, the use of SIMD functional units en-ables HLS to (i) exploit parallelism to a higher degree without using additional resources, (ii) improve resource utilization by enabling hardware re-use at a fine-grained level, and (iii) improve energy efficiency for a given area and/or performance constraint.We illustrate the issues involved in performing high-level syn-thesis with SIMD functional units, and discuss how algorithms involved in a typical high-level synthesis flow can be enhanced to result in maximal performance and energy improvements. These techniques are not restricted to specific high-level synthesis tools/algorithms, and can be plugged into any generic high-level synthesis system. Experimental results indicate that, the use of SIMD units can improve performance by up to 1.9X (average of 1.57X), and simultaneously reduce energy consumption by up to 33.16% (average of 28.03%) compared to well-optimized conven-tional designs, with minimal area overheads (average of 2.18%). The performance improvements can be translated into additional energy savings, resulting in upto 66.26% (average of 55.88%) en-ergy reductions. Further, our experiments demonstrate that, the use of SIMD units in a HLS tool results in a shift in the entire area-delay- energy tradeoff envelope that can be obtained, to include de-sirable parts of the design space (i.e., higher quality designs) that were hitherto unreachable.	algorithm;experiment;high- and low-level;high-level synthesis;mathematical optimization;maximal set;operand;parallel computing;simd;unreachable memory	Vijay Raghunathan;Anand Raghunathan;Mani B. Srivastava;Milos D. Ercegovac	2002		10.1109/ASPDAC.2002.994955	embedded system;parallel processing;computer architecture;in situ resource utilization;parallel computing;real-time computing;simd;design methods;resource allocation;computer science;operating system;efficient energy use;high-level synthesis;scheduling	EDA	0.41778440100612413	50.38970601460151	102013
6aeccedea155ee146f99e8ccb872814f78790fc3	design and test of mems	semiconductor device testing;computer aided design;design automation;microelectromechanical systems;rivers;mems;intellectual property;testing;open research areas;testing mems microelectromechanical systems open research areas cad environment cad frameworks intellectual property;cad frameworks;micromechanical devices;technology cad electronics;micromechanical devices design automation productivity microelectronics system testing circuit testing rivers mechanical systems read only memory microelectromechanical systems;system testing;circuit testing;industrial property;microelectromechanical system;microelectronics;productivity;mechanical systems;read only memory;semiconductor device testing micromechanical devices technology cad electronics industrial property;cad environment	This paper is dealing with design, simulation and test of MEMS (microelectromechanical systems). Both existing tools and open research areas are addressed. An appropriate Computer-Aided Design (CAD) environment is presented. Similarities between the present development of MEMS and the development of microelectronics decades ago are pointed out, including the migration from point tools to CAD frameworks, testing and Intellectual Property (IP) issues.	bridging (networking);computer-aided design;design flow (eda);etching (microfabrication);goto;integrated circuit design;microelectromechanical systems;microprocessor;open research;schematic;schematic-driven layout;semiconductor fabrication plant;simulation;systems design	Bernard Courtois;Jean-Michel Karam;Salvador Mir;Marcelo Lubaszewski;Vladimír Székely;Márta Rencz;Klaus Hofmann;Manfred Glesner	1999		10.1109/ICVD.1999.745160	embedded system;electronic engineering;productivity;computer science;engineering;electrical engineering;microelectromechanical systems;software testing;mechanical system;system testing;microelectronics;read-only memory;intellectual property;manufacturing engineering;mechanical engineering	EDA	9.66004561397212	53.58070968547254	102045
7e7ca3f70eb943eb6de49a90f90f5c163de4f491	the vlsi-programming language tangram and its translation into handshake circuits		In this paper we view VLSI design as a programming activity. VLSI designs are described in the algorithmic programming language Tangram. The paper gives an overview of Tangram, providing sufficient detail to invite the reader to try a small VLSI program himself. Tangram programs can be translated into handshake circuits, networks of elementary components that interact by handshake signaling. We have constructed a silicon compiler that automates this translation and converts these handshake circuits into asynchronous circuits and subsequently into VLSI layouts.	programming language;silicon compiler;vlsi project;very-large-scale integration	Kees van Berkel;Joep L. W. Kessels;Marly Roncken;Ronald Saeijs;Frits D. Schalij	1991			computer architecture;electronic engineering;computer science;theoretical computer science	EDA	9.958478803066384	51.0311170919842	102046
c9ed19007902f376de2ea6b9aa40d2236f2c009f	reconfigurable rram for lut logic mapping: a case study for reliability enhancement	arrays table lookup cmos integrated circuits benchmark testing reliability adders;failure analysis;table lookup cmos logic circuits failure analysis microprocessor chips random access storage;size 45 nm reconfigurable rram lut logic mapping reliability enhancement hybrid cmos nanoscale devices high power densities hard error soft error frequency process variations system reliability reactive design technique cmos resistive ram architecture cmos logic units fail look up table logic blocks spec2006 benchmarks component failures;cmos logic circuits;random access storage;table lookup;microprocessor chips	Emerging hybrid-CMOS nanoscale devices and architectures offer greater degree of integration and performance capabilities. However, the high power densities, hard error/soft error frequency, process variations, and device wearout affect the overall system reliability. Reactive design techniques, such as redundancy, account for component failures by detecting and correcting the system failures. These techniques incur high area and power overhead. Our research focuses on enhancing the system reliability in hybrid CMOS/Resistive RAM (RRAM) architectures by performing computation in RRAM, whenever the CMOS logic units fail. In particular, we propose dynamically reconfiguring the RRAM cache by mapping the failed CMOS units as look up table (LUT) logic blocks in the RRAM. The proposed approach is validated on a 45nm single core processor with three levels of cache for various SPEC2006 benchmarks. Our results demonstrate that the core is fully functional when failed units are reconfigured in RRAM. Performance degradation of up to one order of magnitude and energy increase of up to two orders of magnitude is observed.	adder (electronics);arithmetic logic unit;cmos;computation;degradation (telecommunications);electronic circuit;elegant degradation;lookup table;overhead (computing);redundancy (engineering);resistive random-access memory;sensor;soft error;speedup;triple modular redundancy	Matthew Catanzaro;Dhireesha Kudithipudi	2012	2012 IEEE International SOC Conference	10.1109/SOCC.2012.6398384	embedded system;failure analysis;electronic engineering;parallel computing;real-time computing;computer science	EDA	8.20305770976913	59.60254852650612	102153
30305496d8dab1832b2ea3ad7bdbe983a405dc17	automated bus generation for multiprocessor soc design	bus generation;vlsi system on chip integrated circuit design system buses multiprocessing systems;multiprocessor systems;impact factor;system on a chip multiprocessing systems databases computer architecture hybrid power systems application software ofdm transmitters decoding design methodology;system on a chip;65;synthesis;system buses;integrated circuit design;performance improvement;system on a chip soc bus architecture bus generation design space exploration synthesis;design space exploration automated bus generation multiprocessor soc design multiprocessor system on a chip bus synthesis tool bussynth bus systems bidirectional first in first out bus architecture global bus architecture gba version i gba version iii hybrid bus architecture split bus architecture orthogonal frequency division multiplexing wireless transmitter mpeg2 decoder database example bus types processor types software programming style;system on chip;vlsi;soc;multiprocessing systems;design space exploration;bus architecture;orthogonal frequency division multiplex	The performance of a system, especially a multiprocessor system, heavily depends upon the efficiency of its bus architecture. This paper presents a methodology to generate a custom bus system for a multiprocessor System-on-a-Chip (SoC). Our bus synthesis tool (BusSyn) uses this methodology to generate five different bus systems as examples: Bi-FIFO Bus Architecture (BFBA), Global Bus Architecture Version I (GBAVI), Global Bus Architecture Version III (GBAVIII), Hybrid bus architecture (Hybrid) and Split Bus Architecture (SplitBA). We verify and evaluate the performance of each bus system in the context of two applications: an Orthogonal Frequency Division Multiplexing (OFDM) wireless transmitter and an MPEG2 decoder. This methodology gives the designer a great benefit in fast design space exploration of bus architectures across a variety of performance impacting factors such as bus types, processor types and software programming style. In this paper, we show that BusSyn can generate buses that achieve superior performance when compared to a simple General Global Bus Architecture (GGBA) (e.g., 16.44% performance improvement in the case of OFDM transmitter) or when compared to the CoreConnect Bus Architecture (CCBA) (e.g., 15.54% peformance improvement in the case of MPEG2 decoder). In addition, the bus architecture generated by BusSyn is designed in a matter of seconds instead of weeks for the hand design of a custom bus system.	computer programming;coreconnect;design space exploration;fifo (computing and electronics);mpeg-2;multiplexing;multiprocessing;programming style;system on a chip;transmitter	Kyeong Keol Ryu;Vincent John Mooney	2003	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2004.835119	system on a chip;bus;std bus;embedded system;bus error;memory bus;computer architecture;parallel computing;can bus;three-state logic;iebus;computer science;engineering;local bus;operating system;conventional pci;system bus;control bus;back-side bus;bus network;address bus	EDA	5.73962482812622	51.98896288193553	102522
12f6f9db86776aa23f1a6e3c424030b0db178fe2	the importance of synchronization structure in parallel program optimization	routing;programming model;cost estimation;parallel programs;routers;multicomputer networks	In automatic retargetable compilation low cost analytic cost estimation techniques are crucial in order to e ciently steer the optimization process Programming models aimed at optimum expressiveness of parallelism however are not amenable to static cost estimation We present a new co ordination model called SPC that imposes speci c restric tions in the synchronization structures that can be pro grammed Imposing these restrictions enables the e cient computation of reliable cost estimations paving the way for automatic optimization Regarding SPC s limited express iveness we present a conjecture stating that the loss of par allelism when programming in SPC is typically limited to a constant factor of compared to the unrestricted case This limited loss is outweighed by the unlocked potential of auto matic performance optimization as well as the portability that is achieved We demonstrate how SPC enables auto matic program optimizations through a compilation case study involving a line relaxation kernel on a distributed memory machine	compiler;computation;distributed memory;linear programming relaxation;mathematical optimization;parallel computing;program optimization	Arjan J. C. van Gemund	1997		10.1145/263580.263625	routing;parallel computing;real-time computing;computer science;distributed computing;programming paradigm;programming language;cost estimate	PL	-4.360638949425411	48.85444785347487	102626
4bbad5b16e4641277fb9fa8c5fa9e52abbfe9b85	a compiler framework for restructuring data declarations to enhance cache and tlb effectiveness	heuristic algorithm;polynomial time;satisfiability;translation lookaside buffer	It has been observed that memory access perfor mance can be improved by restructuring data dec larations using simple transformations such as ar ray dimension padding and inter array padding array alignment to reduce the number of misses in the cache and TLB translation lookaside bu er These transformations can be applied to both static and dynamic array variables In this paper we provide a padding algorithm for select ing appropriate padding amounts which takes into account various cache and TLB e ects collectively within a single framework In addition to reducing the number of misses we identify the importance of reducing the impact of cache miss jamming by spreading cache misses more uniformly across loop iterations We translate undesirable cache and TLB behav iors into a set of constraints on padding amounts and propose a heuristic algorithm of polynomial time complexity to nd the padding amounts to satisfy these constraints The goal of the padding algorithm is to select padding amounts so that there are no set con icts and no o set con icts in the cache and TLB for a given loop In practice this algorithm can e ciently nd small padding amounts to satisfy these constraints	algorithm;cpu cache;cache (computing);compiler;dynamic array;heuristic (computer science);iteration;naruto shippuden: clash of ninja revolution 3;radio jamming;time complexity;translation lookaside buffer	David F. Bacon;Jyh-Herng Chow;Dz-ching Ju;Kalyan Muthukumar;Vivek Sarkar	1994		10.1145/782188	heuristic;time complexity;parallel computing;real-time computing;computer science;theoretical computer science;operating system;translation lookaside buffer;database;distributed computing;programming language;cache algorithms;satisfiability	Arch	-3.628275595496852	52.728767048757724	102642
17a3cf6cff6188abab0dfb0ae0a34341690e78c4	go ahead: a partial reconfiguration framework	run time reconfiguration fpga partial reconfiguration goahead spartan 6;partial reconfiguration;reconfigurable architectures;circuit layout;run time reconfiguration;fpga;goahead;reconfigurable architectures circuit layout field programmable gate arrays low power electronics;scripting interface partial run time reconfiguration goahead tool run time reconfigurable systems xilinx fpga low cost low power spartan 6 fpga floorplanning constraint generation xilinx vendor tools final configuration bitstreams flexible systems device reusability device portability migration paths;low power electronics;field programmable gate arrays wires routing clocks design methodology table lookup timing;field programmable gate arrays;spartan 6	Exploiting the benefits of partial run-time reconfiguration requires efficient tools. In this paper, we introduce the tool Go Ahead that is able to implement run-time reconfigurable systems for all recent Xilinx FPGAs. This includes in particular support for low cost and low power Spartan-6 FPGAs. Go Ahead assists during floor planning and automates the constraint generation. It interacts with the Xilinx vendor tools and triggers the physical implementation phases all the way down to the final configuration bit streams. Go Ahead enables the building of flexible systems for integrating many reconfigurable modules very efficiently into a system. The tool targets (re)usability, portability to future devices, and migration paths among reconfigurable systems featuring different FPGAs or even FPGA families. Moreover, it provides a scripting interface and all features can be accessed remotely.	bitstream;database trigger;documentation;field-programmable gate array;floorplan (microelectronics);overhead (computing);place and route;reconfigurable computing;relocation (computing);routing;software portability;usability	Christian Beckhoff;Dirk Koch;Jim Tørresen	2012	2012 IEEE 20th International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2012.17	embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;field-programmable gate array	Arch	6.866775798265442	55.19410893224699	102764
88b7aafca4d54b719ab7caa09c151a94a403a555	low-power and real-time address translation through arithmetic operations for virtual memory support in embedded systems	storage allocation;systeme temps reel;virtual memory;arithmetic operation;architecture systeme;program compiler arithmetic based address translation virtual memory support real time embedded system translation lookaside buffer consecutive virtual page number consecutive physical page frame low power address translation operating system loader memory manager;compilateur;consecutive virtual page number;memory management;systeme embarque;integrated circuit;execution time;operation arithmetique;real time;program processors virtual private networks memory management embedded system real time systems registers computer architecture;buffer storage;program compiler;real time processing;translation lookaside buffer;circuito integrado;real time embedded system;operacion aritmetica;compiler;virtual memory support;embedded system;tratamiento tiempo real;computer architecture;embedded systems;traitement temps reel;low power;operating system loader;registers;virtual storage buffer storage embedded systems low power electronics operating systems computers program compilers storage allocation table lookup;low power electronics;memoire virtuelle;memory manager;temps execution;arquitectura sistema;consecutive physical page frame;real time system;sistema tiempo real;information system;power consumption;consommation energie electrique;tiempo ejecucion;system architecture;program compilers;table lookup;electronique faible puissance;arithmetic based address translation;program processors;operating systems computers;systeme information;memoria virtual;virtual storage;circuit integre;compilador;low power address translation;real time systems;sistema informacion;virtual private networks	An arithmetic-based address translation technique is presented for low-power and real-time embedded processors with virtual memory support. General-purpose virtual memory support comes with its disadvantages of excessive power consumption and nondeterministic execution times, which are the main reasons for not adopting virtual memory in energy-efficient and real-time embedded systems. To address these issues, an application-driven address translation is proposed, where most of the address translations, which are traditionally performed as translation lookaside buffer (TLB) lookups, are replaced with fast and energy-efficient addition operations. To achieve this, a program and system-wide information is used to identify sequences of consecutive virtual page numbers, which are mapped to sequences of consecutive physical page frames. For such pairs of page sequences, only the addition of a constant to the virtual page number is needed to produce the physical page frame. The proposed methodology relies on the combined efforts of compiler, operating system, and hardware architecture to achieve a significant power reduction. As the approach fundamentally eliminates conflicts inherent in the hardware translation table, execution time is not only improved but also made predictable for a large number of memory reference instructions. Experiments show power reductions in the range of 80–95% compared to a general-purpose TLB.	central processing unit;compiler;embedded system;experiment;frame language;general-purpose markup language;general-purpose modeling;low-power broadcasting;operating system;page (computer memory);real-time clock;run time (program lifecycle phase);translation lookaside buffer	Xiangrong Zhou;Peter Petrov	2008	IET Computers & Digital Techniques	10.1049/iet-cdt:20070090	zero page;demand paging;embedded system;parallel computing;real-time computing;page fault;real-time operating system;virtual address space;memory management unit;computer science;physical address;virtual memory;kernel virtual address space;operating system;page table;translation lookaside buffer;logical address;overlay;address space;flat memory model;algorithm;memory map;systems architecture;page address register;memory management	Arch	-2.696996789979454	53.66602709065925	102771
4996aa365b4fb471489aa566aedc8fb9ff880ee5	software-directed combined cpu/link voltage scaling fornoc-based cmps	network on chip;chip multiprocessor;communication link;compiler;frequency assignment problem;execution environment;scientific computing;power consumption;noc;cpu;parallel programs;cmp;voltage scaling;high performance;integer linear program;embedded computing;energy saving	Network-on-Chip (NoC) based chip multiprocessors (CMPs) are expected to become more widespread in future, in both high performance scientific computing and low-end embedded computing. For many execution environments that employ these systems, reducing power consumption is an important goal. This paper presents a software approach for reducing power consumption in such systems through compiler-directed voltage/frequency scaling. The unique characteristic of this approach is that it scales the voltages and frequencies of select CPUs and communication links in a coordinated manner to maximize energy savings without degrading performance. Our approach has three important components. The first component is the identification of phases in the application. The next step is to determine the critical execution paths and slacks in each phase. For implementing these two components, our approach employs a novel parallel program representation. The last component of our approach is the assignment of voltages and frequencies to CPUs and communication links to maximize energy savings. We use integer linear programming (ILP) for this voltage/frequency assignment problem. To test our approach, we implemented it within a compilation framework and conducted experiments with applications from the SPEComp suite and SPECjbb. Our results show that the proposed combined CPU/link scaling is much more effective than scaling voltages of CPUs or communication links in isolation. In addition, we observed that the energy savings obtained are consistent across a wide range of values of our major simulation parameters such as the number of CPUs, the number of voltage/frequency levels, and the thread-to-CPU mapping.	assignment problem;central processing unit;compiler;computational science;dynamic voltage scaling;embedded system;experiment;frequency scaling;image scaling;integer programming;linear programming;network on a chip;simulation	Mahmut T. Kandemir;Ozcan Ozturk	2008		10.1145/1375457.1375498	computer architecture;compiler;parallel computing;real-time computing;computer science;operating system;central processing unit;network operations center;network on a chip	Metrics	-1.605760916114625	54.79164861039884	102772
c29038219c4dd0f72e0314ca7a63669a87c7ec93	functional dif for rapid prototyping	digital signal processing;functionally correct simulation;impedance;software prototyping;processor scheduling;signal design;prototypes;verilog;hardware description languages;design practice;prototypes computational modeling signal design digital signal processing design optimization processor scheduling impedance packaging polynomials field programmable gate arrays;software prototyping data flow analysis hardware description languages parallel languages scheduling;dataflow interchange format package;packaging;design optimization;polynomials;dataflow formalisms;heterogeneous design dataflow rapid prototyping;fpga implementation;computational modeling;rapid prototyping;dataflow;polynomial evaluation accelerator;design environment;scheduling;functional simulation;enable invoke dataflow;heterogeneous design;data flow analysis;optimization functional dif rapid prototyping dataflow formalisms digital signal processing systems system complexity dataflow languages dsp oriented dataflow models functional simulation scheduling enable invoke dataflow dataflow interchange format package polynomial evaluation accelerator fpga implementation functionally correct simulation verilog;optimization;functional dif;field programmable gate arrays;parallel languages;system complexity;dsp oriented dataflow models;polynomial evaluation;digital signal processing systems;dataflow languages	Dataflow formalisms have provided designers of digital signal processing systems with optimizations and guarantees to arrive at quality prototypes quickly. As system complexity increases, designers are expressing more types of behavior in dataflow languages to retain these implementation benefits. While the semantic range of DSP-oriented dataflow models has expanded to cover quasi-static and dynamic applications, efficient functional simulation of such applications has not. Complexity in scheduling and modeling has impeded efforts towards functional simulation that matches the final implementation. We provide this functionality by introducing a new dataflow model of computation, called enable-invoke dataflow (EIDF), that supports flexible and efficient prototyping of dataflow-based application representations. EIDF permits the natural description of actors for dynamic and static dataflow models. We integrate EIDF into the dataflow interchange format (DIF) package and demonstrate the approach on the design of a polynomial evaluation accelerator targeting an FPGA implementation. Our experiments show that a design environment based on EIDF can achieve functionally-correct simulation compared to Verilog, allowing the application designer to arrive at a verified functional simulation faster, and therefore at a functional prototype much more quickly than traditional design practices.	complexity;data integrity field;data interchange format;dataflow programming;digital signal processing;experiment;field-programmable gate array;functional design;logic simulation;model of computation;polynomial;prototype;rapid prototyping;scheduling (computing);semantics (computer science);verilog	William Plishker;Nimish Sane;Mary Kiemb;Kapil Anand;Shuvra S. Bhattacharyya	2008	2008 The 19th IEEE/IFIP International Symposium on Rapid System Prototyping	10.1109/RSP.2008.32	dataflow architecture;embedded system;packaging and labeling;computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;computer science;operating system;digital signal processing;data-flow analysis;dataflow;electrical impedance;prototype;hardware description language;programming language;computational model;scheduling;field-programmable gate array;polynomial	Embedded	2.3529253388669202	51.53612583494083	102815
278b65ed3029fa6bdba83207fe3b58a8b48175a0	profiling multilevel partitioning for asynchronous vlsi distributed simulation		Partitioning is a crucial factor in VLSI distributed simulation. This paper focuses on the partitioning problem for asynchronous handshake circuits generated by the Balsa asynchronous hardware synthesis environment. A quantitative analysis is presented for multilevel partitioning, as exemplified by the metis library.	asynchronous circuit;graph partition;simulation;very-large-scale integration	Elias Tsirogiannis;Georgios Theodoropoulos	2013		10.1007/978-3-642-45037-2_29	computer architecture;parallel computing;real-time computing	EDA	5.937205421877363	51.503588940593836	103109
415b9ef1357df6dd96d1bc4781f8b0359c9569d3	stochastic energy-efficiency optimization in photonic networking by use of master-slave equipment configurations	energy efficiency;system reliability;network design;photonic switching systems;optical switch;telecommunication network reliability;system configuration;telecommunication network reliability optical communication photonic switching systems stochastic programming;optical fiber amplifier;energy efficient;loading;optical switches;optical fiber amplifiers;optical communication;carbon footprint;stochastic programming;master slave;energy efficiency master slave optical switches optical fiber amplifiers power demand loading;power demand;system reliability stochastic energy efficiency optimization master slave equipment configurations stochastically based master slave dual system configuration approach photonic networking design intrinsic energy efficiencies single device systems traffic loads energy efficiency improvements master slave design configuration lower carbon footprint system switching speed tolerances	We describe a stochastically-based master-slave dual system configuration approach to photonic networking design enabling intrinsic energy-efficiencies to dynamically approach comparable single-device systems that are fully elastic with respect to presented traffic loads. We show analytically that energy-efficiency improvements of more than 90% are statistically possible by adopting a master-slave design configuration. In addition to enabling a lower carbon footprint, our approach allows system switching-speed tolerances to be relaxed, and also improves system reliability as compared with a fully elastic, single-device approach.	input/output;master/slave (technology);mathematical optimization;networking hardware;optical amplifier;system configuration	Michael C. Parker;Stuart D. Walker	2012	2012 16th International Conference on Optical Network Design and Modelling (ONDM)	10.1109/ONDM.2012.6210187	telecommunications;computer science;efficient energy use;optical switch	EDA	3.060936633572681	57.24948228399018	103303
74f18b6a86cd2c1ff8edb5cb125544a07ab44ed6	intraprogram dynamic voltage scaling: bounding opportunities with analytic modeling	mixed integer linear program;dynamic voltage scaling;compiler;low power;mixed integer linear programming;energy consumption;timing optimization;scheduling problem;power consumption;dynamic power management;analytical model;energy saving	Dynamic voltage scaling (DVS) has become an important dynamic power-management technique to save energy. DVS tunes the power-performance tradeoff to the needs of the application. The goal is to minimize energy consumption while meeting performance needs. Since CPU power consumption is strongly dependent on the supply voltage, DVS exploits the ability to control the power consumption by varying a processor's supply voltage and clock frequency. However, because of the energy and time overhead associated with switching DVS modes, DVS control has been used mainly at the interprogram level.In this paper, we explore the opportunities and limits of intraprogram DVS scheduling. An analytical model is derived to predict the maximum energy savings that can be obtained using intraprogram DVS given a few known program and processor parameters. This model gives insights into scenarios where energy consumption benefits from intraprogram DVS and those where there is no benefit. The model helps us extrapolate the benefits of intraprogram DVS into the future as processor parameters change. We then examine how much of these predicted benefits can actually be achieved through compile-time optimal settings of DVS modes. We extend an existing mixed-integer linear program formulation for this scheduling problem by accurately accounting for DVS energy switching overhead, by providing finer-grained control on settings and by considering multiple data categories in the optimization. Overall, this research provides a comprehensive view of intraprogram compile-time DVS management, providing both practical techniques for its immediate deployment as well theoretical bounds for use into the future.	a new kind of science;cpu power dissipation;central processing unit;clock rate;compile time;compiler;computation;descriptive video service;dynamic voltage scaling;extrapolation;image scaling;linear programming;mathematical optimization;memory bound function;mode setting;network switch;overhead (computing);scheduling (computing);software deployment	Fen Xie;Margaret Martonosi;Sharad Malik	2004	TACO	10.1145/1022969.1022973	embedded system;job shop scheduling;compiler;parallel computing;real-time computing;computer science;programming language	PL	-4.059026662062752	56.81301250950907	103498
7b2f0d795cae4eda19171b3138f412bccb0637d7	mswat: low-cost hardware fault detection and diagnosis for multicore systems	software;detectors;lightweight isolated deterministic replay;program diagnostics;multi threading;reliability;multicore systems;fault free cores;low cost monitors;symptom based detectors;prior knowledge;failure mode;transient analysis;reliability solutions;technology scaling;reliability fault diagnosis microprocessor chips multiprocessing systems multi threading program diagnostics;low cost monitors low cost hardware fault detection fault diagnosis multicore systems mswat multithreaded software symptom based detectors software anomaly treatment fault free cores lightweight isolated deterministic replay reliability solutions;multicore processing;low cost hardware fault detection;hardware fault detection fault diagnosis multicore processing detectors application software redundancy watches costs computer architecture;multicore processors;multithreaded software;fault injection error detection multicore processors architecture;multiprocessing systems;software anomaly treatment;error detection;magnetic cores;fault injection;architecture;fault detection and diagnosis;fault diagnosis;microprocessor chips;hardware;mswat	Continued technology scaling is resulting in systems with billions of devices. Unfortunately, these devices are prone to failures from various sources, resulting in even commodity systems being affected by the growing reliability threat. Thus, traditional solutions involving high redundancy or piecemeal solutions targeting specific failure modes will no longer be viable owing to their high overheads. Recent reliability solutions have explored using low-cost monitors that watch for anomalous software behavior as a symptom of hardware faults. We previously proposed the SWAT system that uses such low-cost detectors to detect hardware faults, and a higher cost mechanism for diagnosis. However, all of the prior work in this context, including SWAT, assumes single-threaded applications and has not been demonstrated for multithreaded applications running on multicore systems.  This paper presents mSWAT, the first work to apply symptom based detection and diagnosis for faults in multicore architectures running multithreaded software. For detection, we extend the symptom-based detectors in SWAT and show that they result in a very low Silent Data Corruption (SDC) rate for both permanent and transient hardware faults. For diagnosis, the multicore environment poses significant new challenges. First, deterministic replay required for SWAT's single-threaded diagnosis incurs higher overheads for multithreaded workloads. Second, the fault may propagate to fault-free cores resulting in symptoms from fault-free cores and no available known-good core, breaking fundamental assumptions of SWAT's diagnosis algorithm. We propose a novel permanent fault diagnosis algorithm for multithreaded applications running on multicore systems that uses a lightweight isolated deterministic replay to diagnose the faulty core with no prior knowledge of a known good core. Our results show that this technique successfully diagnoses over 95% of the detected permanent faults while incurring low hardware overheads. mSWAT thus offers an affordable solution to protect future multicore systems from hardware faults.	failure cause;fault detection and isolation;image scaling;medical algorithm;multi-core processor;multithreading (computer architecture);swat;secure digital container;sensor;thread (computing);threat (computer)	Siva Kumar Sastry Hari;Man-Lap Li;Pradeep Ramachandran;Byn Choi;Sarita V. Adve	2009	2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1145/1669112.1669129	multi-core processor;embedded system;parallel computing;real-time computing;computer science;operating system	Arch	6.6629456258942525	58.89205749048488	103536
9c4ac6160b2280bc6fd7f82f9b0fbcd5a8a8dfed	customized instruction-sets for embedded processors	instruction level parallel;mass customization;maintenance cost;instruction set architecture;instruction sets hardware costs computer architecture permission lead compounds application software personal digital assistants laboratories microprocessors;mass customization of toolchains;chip;vliw;custom processors;parallel architectures;vliw customized instruction sets embedded processors instruction set architectures embedded cpu design toolchain developmen maintenance costs chip cost customized processors hardware development costs product development cycle;embedded processors;parallel architectures instruction sets logic cad microprocessor chips;instruction level parallelism;logic cad;embedded processor;microprocessor chips;instruction sets;product development	It is generally believed that there will be little more variety in CPU architectures, and thus the design of Instruction-set Architectures (ISAs) will have no role in the future of embedded CPU design. Nonetheless, it is argued in this paper that architectural variety will soon again become an important topic, with the major motivation being increased performance due to the customization of CPUs to their intended use. Five major barriers that could hinder customization are described, including the problems of existing binaries, toolchain development and maintenance costs, lost savings/higher chip cost due to the lower volumes of customized processors, added hardware development costs, and some factors related to the product development cycle for embedded products. Each is discussed, along with potential, sometimes surprising, solutions.	binary file;central processing unit;embedded system;new product development;toolchain	Joseph A. Fisher	1999		10.1145/309847.309923	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;instruction set	EDA	8.741894052063069	55.42573721091469	103642
7a69bf4dc05ff9d5f98a7b99edb85aa07b4240bd	a co-design approach for hardware optimizations in multicore architectures using mcapi	hw sw co design;multicore progammability hw sw co design multicore communication mcapi;synchronisation application program interfaces hardware software codesign multiprocessing systems optimisation;multicore progammability;mcapi;synchronization phase codesign approach hardware optimization multicore architecture application programming interface mcapi software level leveraging;multicore communication;synchronization software hardware multicore processing receivers system on chip	Current SoC platforms targeting high-performance with high power efficiency rely on replicating several processing cores while adding dedicated hardware units for specific tasks. However, programming such architectures demand a high effort when compared to homogeneous multiprocessors since there is no widely used standard for heterogeneous embedded systems. The use of standard application programming interfaces (APIs) increases the programmability but also costs performance/memory usage overheads. Providing mechanisms at the software level leveraging on dedicated hardware resources can help reducing that impact. To address this point, this work presents a co-design approach for improving programming based on a standard API deployed through a mix of hardware and software support for tasks synchronization. Results present a reduction of up to 88% in network traffic and processor active times during synchronization phases when compared to a pure software implementation.	application programming interface;computer hardware;embedded system;mcapi;multi-core processor;network packet;performance per watt	Thiago Raupp da Rosa;Romain Lemaire;Fabien Clermidy	2015	2015 Ninth International Workshop on Interconnection Network Architectures: On-Chip, Multi-Chip	10.1109/INA-OCMC.2015.11	computer architecture;parallel computing;real-time computing;computer science	Arch	-2.888707724622017	49.35842265248666	103837
0657d5bc1f6186a80b92063f3ade16efd6b0fd3d	synchronizing code execution on ultra-low-power embedded multi-channel signal analysis platforms	single instruction multiple data;hardware;synchronization;data level parallelism;simd;parallel computing;multicore processing;network on chip;benchmark testing	Embedded biosignal analysis involves a considerable amount of parallel computations, which can be exploited by employing low-voltage and ultra-low-power (ULP) parallel computing architectures. By allowing data and instruction broadcasting, single instruction multiple data (SIMD) processing paradigm enables considerable power savings and application speedup, in turn allowing for a lower voltage supply for a given workload. The state-of-the-art multi-core architectures for biosignal analysis however lack a bare, yet smart, synchronization technique among the cores, allowing lockstep execution of algorithm parts that can be performed using the SIMD, even in the presence of data-dependent execution flows. In this paper, we propose a lightweight synchronization technique to enhance an ULP multi-core processor, resulting in improved energy efficiency through lockstep SIMD execution. Our results show that the proposed improvements accomplish tangible power savings, up to 64% for an 8-core system operating at a workload of 89 MOps/s while exploiting voltage scaling.	algorithm;computation;data dependency;dynamic voltage scaling;embedded system;image scaling;low-power broadcasting;multi-core processor;parallel computing;programming paradigm;simd;signal processing;speedup	Ahmed Yasir Dogan;Rubén Braojos;Jeremy Constantin;Giovanni Ansaloni;Andreas Peter Burg;David Atienza	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;simd;computer science;operating system;network on a chip	EDA	-4.009680018742948	54.93151173424159	103847
bcf784baf720954fbcbcde01bd1dadd47920161f	constructing hardware-software systems from a single description	hardware software co design;executable specification;software systems	The study of computing is split at an early stage between the separate branches that deal with hardware and software; there is also a corresponding split in later professional specialisation. This paper explores the essential unity of the two branches and attempts to point to a common framework within which hardware-software codesigns can be expressed as a single executable speciication, reasoned about, and transformed into implementations. We also describe a hardware/software co-design environment which has been built, and we show how designs can be realised within this environment. A rapid development cycle is achieved by using FPGAs to host the hardware components of the system. The architecture of a hardware platform for supporting experimental hardware/software co-designs is presented. A particular example of a real-time video processing application built using this design environment is also described.	executable;field-programmable gate array;real-time clock;real-time computing;software system;video processing	Ian Page	1996	VLSI Signal Processing	10.1007/BF00936948	hardware compatibility list;embedded system;computer architecture;parallel computing;real-time computing;formal methods;hardware acceleration;computer science;operating system;hardware architecture;programming language;open source hardware;software system	EDA	4.030706777163508	51.605218494944076	104030
20b297f9fcc373fe8578b229235624bdda2b8ae1	an fpga-based distributed computing system with power and thermal management capabilities	field programmable gate array;dynamic frequency scaling fpga based distributed computing system runtime power management runtime thermal management multicore distributed embedded performance evaluation emulation speed fpga based platform network topology hardware sniffer performance monitoring ethernet multicore system intercore communication inter fpga communication temperature monitoring;performance monitoring;multi core distributed system;performance evaluation;distributed embedded system;clocks;temperature sensors;distributed computing;fpga;power aware computing embedded systems field programmable gate arrays local area networks multiprocessing systems;temperature sensor;network topology;embedded systems;power aware computing;distributed computing system;power management;clocks temperature sensors field programmable gate arrays temperature measurement hardware distributed computing switches;multiprocessing systems;temperature measurement;field programmable gate arrays;dvfs;switches;thermal management;dynamic frequency scaling;local area networks;hardware	Runtime power and thermal management has attracted substantial interests in multi-core distributed embedded systems. Fast performance evaluation is an essential step in the research of distributed power and thermal management. Compared to software simulation, an FPGA-based evaluation platform provides fast emulation speed which enables us to test the performance of power/thermal management policies with real-life applications and OS. Compared to computer clusters, an FPGA-based platform has the flexibility to be configured into any network topology and hardware sniffer for performance monitoring can be added easily. This paper presents an FPGA based emulator of multi-core distributed embedded system designed to support the research in runtime power/thermal management. The system consists of multiple FPGAs connecting through Ethernet with each FPGA configured as a multi-core system. Hardware and software supports are provided to carry out basic power/thermal management actions including inter-core or inter-FPGA communications, runtime temperature monitoring and dynamic frequency scaling.	computer cluster;computer simulation;distributed computing;dynamic frequency scaling;embedded system;emulator;field-programmable gate array;image scaling;multi-core processor;network topology;operating system;performance evaluation;real life;thermal management (electronics);thermal management of high-power leds	Hao Shen;Qinru Qiu	2011	2011 Proceedings of 20th International Conference on Computer Communications and Networks (ICCCN)	10.1109/ICCCN.2011.6005802	embedded system;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	-3.311555684452962	55.65136638970515	104223
e208f108d51cda69f2105e41cafc8186a93b8059	approximate computing: challenges and opportunities	approximation algorithms;training;computer architecture;synchronization;approximate computing;hardware;synthetic aperture radar	Approximate computing is gaining traction as a computing paradigm for data analytics and cognitive applications that aim to extract deep insight from vast quantities of data. In this paper, we demonstrate that multiple approximation techniques can be applied to applications in these domains and can be further combined together to compound their benefits. In assessing the potential of approximation in these applications, we took the liberty of changing multiple layers of the system stack: architecture, programming model, and algorithms. Across a set of applications spanning the domains of DSP, robotics, and machine learning, we show that hot loops in the applications can be perforated by an average of 50% with proportional reduction in execution time, while still producing acceptable quality of results. In addition, the width of the data used in the computation can be reduced to 10-16 bits from the currently common 32/64 bits with potential for significant performance and energy benefits. For parallel applications we reduced execution time by 50% using relaxed synchronization mechanisms. Finally, our results also demonstrate that benefits compounded when these techniques are applied concurrently. Our results across different applications demonstrate that approximate computing is a widely applicable paradigm with potential for compounded benefits from applying multiple techniques across the system stack. In order to exploit these benefits it is essential to re-think multiple layers of the system stack to embrace approximations ground-up and to design tightly integrated approximate accelerators. Doing so will enable moving the applications into a world in which the architecture, programming model, and even the algorithms used to implement the application are all fundamentally designed for approximate computing.	approximate computing;approximation algorithm;computation;concurrent computing;digital signal processing;file spanning;machine learning;programming model;quality of results;robotics;run time (program lifecycle phase);stack machine;traction teampage	Ankur Agrawal;Jungwook Choi;Kailash Gopalakrishnan;Suyog Gupta;Ravi Nair;Jinwook Oh;Daniel A. Prener;Sunil Shukla;Vijayalakshmi Srinivasan;Zehra Sura	2016	2016 IEEE International Conference on Rebooting Computing (ICRC)	10.1109/ICRC.2016.7738674	real-time computing;computer science;theoretical computer science;distributed computing	HPC	-2.299125114420407	47.16596972244557	104370
850e688508215abdd8d3dcf7d5ede8e2279139da	uncle - an rtl approach to asynchronous design	logic gates latches delay registers equations transistors finite element methods;null convention logic;viterbi decoder designs asynchronous design unified null convention logic environment verilog rtl datapath elements register specification datapath elements finite state machines datapath sequencing commercial synthesis tool gate level netlist primitive logic gates storage elements ncl netlist uncle mapping flow performance optimizations net buffering target slew delay balancing latch stages data driven schemes control driven schemes transistor count balsa generated netlists gcd;hardware description languages;rtl asynchronous null convention logic synthesis;viterbi decoding asynchronous circuits finite state machines hardware description languages logic gates;synthesis;finite state machines;logic gates;asynchronous circuits;rtl;viterbi decoding;asynchronous	Uncle (Unified NULL Convention Logic Environment) is an end-to-end toolset for creating asynchronous designs using NULL Convention Logic (NCL). Designs are specified in Verilog RTL, with the user responsible for specifying registers, data path elements, and finite state machines for controlling data path sequencing. A commercial synthesis tool is used to produce a gate-level net list of primitive logic gates and storage elements, which is then transformed into an NCL net list by the Uncle mapping flow. Performance optimizations supported by the flow are net buffering for target slew and delay balancing between latch stages. Both data-driven and control-driven (i.e. Balsa-style) schemes are supported. Transistor count, performance, and energy comparisons are made for Uncle versus Balsa-generated net lists for GCD and Viterbi decoder designs, with the Uncle designs comparing favorably in all three areas.	asynchronous circuit;end-to-end principle;finite-state machine;logic gate;nested context language;transistor count;verilog;viterbi decoder	Robert B. Reese;Scott C. Smith;Mitchell A. Thornton	2012	2012 IEEE 18th International Symposium on Asynchronous Circuits and Systems	10.1109/ASYNC.2012.14	parallel computing;logic synthesis;real-time computing;computer science;pass transistor logic;algorithm	EDA	6.4818171966600024	51.21830067397534	104496
0fc47cdfd5ecfd3cde21ef4ec5d0d34a41b6738a	application-specific reconfigurable xor-indexing to eliminate cache conflict misses	application-specific reconfigurable;reconfigurable selector;application-specific xor-functions;simpler bit-selecting function;cache size;reconfigurable xor-function selector;heuristic algorithm;previous work;bit-selecting function;cache conflict;address bit;application-specific optimizations;hardware;algorithm design and analysis;embedded systems;microelectronics;indexing;logic gates;embedded system;indexation;cache memory	Embedded systems allow application-specific optimizations to improve the power/performance trade-off. In this paper, we show how application-specific hashing of the address can eliminate a large number of conflict misses in caches. We consider XOR-functions: each set index bit is computed as the XOR of a subset of the address bits. Previous work has considered simpler bit-selecting functions. Compared to such work, the contributions of this paper are two-fold. Firstly, we present a heuristic algorithm to construct application-specific XOR-functions. Secondly, in order to adapt the hashing to the application, we show that a reconfigurable XOR-function selector is inherently less complex than a reconfigurable selector for bit-selecting functions. This is possible by placing restrictions on the allowed XOR-functions. Our evaluation shows a reduction of cache misses for standard benchmarks averaging between 30% and 60%, depending on the cache size	algorithm;cpu cache;cache (computing);embedded system;heuristic (computer science);xor	Hans Vandierendonck;Philippe Manet;Jean-Didier Legat	2006	Proceedings of the Design Automation & Test in Europe Conference		heuristic;embedded system;algorithm design;search engine indexing;parallel computing;real-time computing;cpu cache;logic gate;computer science;theoretical computer science;operating system;microelectronics	EDA	-2.2994837143144444	53.28097340588417	104732
bbf87b0f94ec7fd9adbbf568ecc9dee33e9f45e8	a register allocation framework for banked register files with access constraints	entrada salida;fichero registro;calculateur embarque;register allocation;estimation cout;costs and benefits;network processor;tiempo acceso;analisis coste beneficio;input output;computer architecture;analyse avantage cout;architecture ordinateur;energy consumption;boarded computer;estimacion costo;consommation energie;temps acces;register file;arquitectura ordenador;fichier registre;power consumption;cost estimation;consommation energie electrique;embedded processor;calculador embarque;cost benefit analysis;access time;consumo energia;entree sortie	Banked register file has been proposed to reduce die area, power consumption, and access time. Some embedded processors, e.g. Intel’s IXP network processors, adopt this organization. However, they expose some access constraints in ISA, which complicates the design of register allocation. In this paper, we present a register allocation framework for banked register files with access constraints for the IXP network processors. Our approach relies on the estimation of the costs and benefits of assigning a virtual register to a specific bank, as well as that of splitting it into multiple banks via copy instructions. We make the decision of bank assignment or live range splitting based on analysis of these costs and benefits. Compared to previous works, our framework can better balance the register pressure among multiple banks and improve the performance of typical network applications.	access time;central processing unit;embedded system;network processor;register allocation;register file	Feng Zhou;Junchao Zhang;Chengyong Wu;Zhaoqing Zhang	2005		10.1007/11572961_22	embedded system;parallel computing;real-time computing;control register;computer science;cost–benefit analysis;operating system;stack register;processor register;register allocation;memory address register	Arch	-2.813052882135459	53.32051128793203	104736
aa89f8c76161706af8002370eed4773d2bfe87d9	asip architecture exploration for efficient ipsec encryption: a case study	block ciphering;developpement logiciel;internet protocol;outil logiciel;cryptage bloc;processor architecture;design automation;software tool;calculateur embarque;compilateur;shared memory;protocolo internet;renemen t;memoria compartida;block cipher;protocole internet;customization;personnalisation;automatisation;automatizacion;coprocessor;compiler;system on a chip;automatic generation;computer architecture;architecture ordinateur;instruction set simulator;sistema sobre pastilla;system on chip;coprocesador;criptografia;desarrollo logicial;cryptography;coprocesseur;software development;boarded computer;personalizacion;cifrado en bloque;cryptographie;arquitectura ordenador;systeme sur puce;herramienta software;calculador embarque;compilador;memoire partagee;automation	Application Specific Instruction Processors (ASIPs) are increasingly becoming popular in the world of customized, application-driven System-on- Chip (SoC) designs. Efficient ASIP design requires an iterative architecture exploration loop-gradual refinement of processor architecture starting from an initial template. To accomplish this task, design automation tools are used to detect bottlenecks in embedded applications, to implement application-specific instructions and to automatically generate the required software tools (such as instruction set simulator, C-compiler, assembler, profiler etc.) as well as to synthesize the hardware. This paper describes an architecture exploration loop for an ASIP coprocessor which implements common encryption functionality used in symmetric block cipher algorithms for IPsec. The coprocessor is accessed via shared memory and as a consequence, our approach is easily adaptable to arbitrary processor architectures. In the case study, we used Blowfish as encryption algorithm and a MIPS architecture as main processor.	application-specific instruction set processor;encryption;ipsec	Hanno Scharwächter;David Kammler;Andreas Wieferink;Manuel Hohenauer;Kingshuk Karuri;Jianjiang Ceng;Rainer Leupers;Gerd Ascheid;Heinrich Meyr	2004		10.1007/978-3-540-30113-4_4	system on a chip;embedded system;computer architecture;parallel computing;electronic design automation;computer science;operating system	Crypto	3.5693738840692366	50.91405584512787	104760
19a8037cb6a7484de696ea1e79f2ccc1cf02e306	a dynamically reconfigurable pattern matcher for regular expressions on fpga	run time reconfiguration;fpga;dynamic circuit specialization;technology and engineering	In this article we describe how to expand a partially dynamic reconfigurable pattern matcher for regular expressions presented in previous work by Divyasree and Rajashekar [2]. The resulting, extended, pattern matcher is fully dynamically reconfigurable. First, the design is adapted for use with parameterisable configurations, a method for Dynamic Circuit Specialization. Using parameterisable configurations allows us to achieve the same area gains as the hand crafted reconfigurable design, with the benefit that parameterisable configurations can be applied automatically. This results in a design that is more easily adaptable to specific applications and allows for an easier design exploration. Additionally, the parameterisable configuration implementation is also generated automatically, which greatly reduces the design overhead of using dynamic reconfiguration. Secondly, we propose a number of expansions to the original design to overcome several limitations in the original design that constrain the dynamic reconfigurability of the pattern matcher. We propose two different solutions to dynamically change the character that is matched in a certain block. The resulting pattern matcher, after these changes, is fully dynamically reconfigurable, all aspects of the implemented regular expression can be changed at run-time.	field-programmable gate array;overhead (computing);partial template specialization;reconfigurability;regular expression	Tom Davidson;Mattias Merlier;Karel Bruneel;Dirk Stroobandt	2011		10.3233/978-1-61499-041-3-611	embedded system;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	0.6773586782458451	49.91891086443754	104829
c1b7ce7abd506ad74c9ffb3e3b1025759ec3ec81	microsystems microprocessor networks	microprocessors;control systems;hierarchical systems;chip;guidelines;aggregates;process control;microprocessors multiprocessing systems guidelines costs control systems aggregates communication system control hierarchical systems process control;multiprocessing systems;communication system control	The phenomenal growth of the microprocessor business over the last two years has been well covered in the technical literature. Extrapolating from this growth, typical predictions of future trends have suggested that by 1980, microprocessor complexity will increase five to ten times while the speed of the chip will triple.1It seems reasonable to expect this performance at approximately today's microprocessor prices. If instruction set efficiency is assumed to show a direct correspondence with chip complexity, these chips can be expected to be 15 to 30 times as capable in a given application.	big data;code coverage;computer fraud;computer security;confidentiality;extrapolation;microprocessor;multiprocessing;privacy	William L. Spetz	1977	Computer	10.1109/C-M.1977.217786	chip;embedded system;parallel computing;real-time computing;computer science;operating system;process control;computer security	Arch	8.49801256997036	56.179942151873355	104871
176dfca42f0e85f4d3039d2cf191eb4f199a7242	a design of a processor architecture for codes with explicit data dependencies.	processor architecture;data dependence			Sunghyun Jee;Sukil Kim	2001			reference architecture;space-based architecture;computer architecture;parallel computing;real-time computing;cellular architecture;information processor;processor register	Theory	-1.3904768694753604	49.2585895148996	104915
7b68338b2f4d3a2dc64ac4fac388782f36598f32	path selection based acceleration of conditionals in cgras	resource management;kernel;registers;polyhedral model;reconfigurable computing;field programmable gate arrays;hardware;acceleration;affine transformation	Coarse Grain Reconfigurable Arrays (CGRAs) are promising accelerators capable of achieving high performance at low power consumption. While CGRAs can efficiently accelerate loop kernels, accelerating loops with control flow (loops with if-then-else structures) is quite challenging. Existing techniques use predication to handle control flow execution - in which they execute operations from both the paths, but commit only the result of operations from the path taken by branch at run time. However, this results in inefficient resource usage and therefore poor mapping and lower acceleration. The state-of-the-art dual issue scheme fetches instructions from both the paths, but executes only the ones from the correct path but this scheme has an overhead in instruction fetch bandwidth. In this paper, we propose a solution in which after resolving the branching condition, we fetch and execute instructions only from the path taken by branch. Experimental results show that our solution achieves 34.6% better performance and 52.1% lower energy consumption on an average compared to state of the art dual issue scheme.	conditional (computer programming);control flow;overhead (computing);run time (program lifecycle phase)	ShriHari RajendranRadhika;Aviral Shrivastava;Mahdi Hamzeh	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;parallel computing;real-time computing;reconfigurable computing;computer science;theoretical computer science;operating system;affine transformation	EDA	-4.0336215509699835	51.15230231733233	105150
6be326ba557c963bcc0a0831de79b6c6e3bfcf19	an optimal algorithm for testing stuck-at faults in random access memories	random access memories;random access memory;memory access;fault detection stuck at aults;fault detection;fault detection stuck at aults optimal algorithm random access memories;optimal algorithm	"""This correspondence presents an optimal algorithm to detect any single """"stuck-at-i,"""" """"stuck-at-O"""" fault and any combination of """"stuck-at-I,"""" """"stuck-at-O"""" multiple faults in a random access memory using only the n-bit memory address register input and m-bit memory buffer register input and output lines. It is shown that this algorithm requires 4 X 2n memory accesses."""	algorithm;data buffer;input/output;memory address register;memory buffer register;random access;random-access memory	John Knaizuk;Carlos R. P. Hartmann	1977	IEEE Transactions on Computers	10.1109/TC.1977.1674761	parallel computing;page fault;computer science;theoretical computer science;distributed computing;flat memory model;fault detection and isolation	HPC	8.238291664786916	58.47553316651271	105305
9133ea461afa717d76beb938318bb6ab09d93f52	a low-cost, radiation-hardened method for pipeline protection in microprocessors	microprocessors;circuit faults;registers pipelines circuit faults radio frequency delays clocks microprocessors;clocks;fault analysis technique low cost pipeline protection radiation hardened method microprocessor pipeline protection error tolerant microprocessor self checking architecture soft error tolerant flip flop timing error tolerant flip flop sequential cells replay recovery mechanism openrisc microprocessor gate level transient fault injection technique;reduced instruction set computing error detection flip flops integrated circuit reliability microprocessor chips pipeline processing radiation hardening electronics;qa75 electronic computers computer science;radio frequency;registers;pipelines;timing error te fault injection fault tolerance reliability single event transient set single event upset seu soft errors;delays	The aggressive scaling of semiconductor technology has significantly increased the radiation-induced soft-error rate in modern microprocessors. Meanwhile, due to the increasing complexity of modern processor pipelines and the limited error-tolerance capabilities that previous radiation hardening techniques can provide, the existing pipeline protection mechanisms cannot achieve complete protection. This paper proposes a complete and cost-effective pipeline protection mechanism using a self-checking architecture. The radiation-hardened pipeline is achieved by incorporating soft-error- and timing-error-tolerant flip-flop (SETTOFF)-based self-checking cells into the sequential cells of the pipeline. A replay recovery mechanism is also developed at the architectural level to recover the detected errors. The proposed pipeline protection technique is implemented in an OpenRISC microprocessor in a 65-nm technology. A gate-level transient fault-injection and analysis technique is used to evaluate the error-tolerance capability of the proposed hardened pipeline design. The results show that compared with the techniques such as triple modular redundancy, the SETTOFF-based self-checking technique requires over 30% less area and 80% less power overheads. Meanwhile, the error-tolerant and self-checking capabilities of the register allow the proposed pipeline protection technique to provide a noticeably higher level of reliability for different parts of the pipeline compared with the previous pipeline protection techniques.	error-tolerant design;flops;fault injection;fault tolerance;flip-flop (electronics);image scaling;microprocessor;openrisc;overhead (computing);pipeline (computing);protection mechanism;radiation hardening;radio frequency;requirement;semiconductor;simulation;soft error;triple modular redundancy	Yang Lin;Mark Zwolinski;Basel Halak	2016	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2015.2475167	embedded system;electronic engineering;parallel computing;real-time computing;telecommunications;computer science;engineering;operating system;pipeline transport;processor register;pipeline;radio frequency	Arch	8.363138966060342	59.783113432062464	105402
f1c43bbbd8fe87d4b5623c997d0a666343b4a30a	the next generation of exascale-class systems: the exanest project		The ExaNeSt project started on December 2015 and is funded by EU H2020 research framework (call H2020-FETHPC-2014, n. 671553) to study the adoption of low-cost, Linux-based power-efficient 64-bit ARM processors clusters for Exascale-class systems. The ExaNeSt consortium pools partners with industrial and academic research expertise in storage, interconnects and applications that share a vision of an Euro-pean Exascale-class supercomputer. Their goal is designing and implementing a physical rack prototype together with its cooling system, the storage non-volatile memory (NVM) architecture and a low-latency interconnect able to test different options for interconnection and storage. Furthermore, the consortium is to provide real HPC applications to validate the system. Herein we provide a status report of the project initial developments.	64-bit computing;arm architecture;central processing unit;computer cooling;electrical connection;interconnection;linux;non-volatile memory;prototype;supercomputer;volatile memory	Roberto Ammendola;Andrea Biagioni;Paolo Cretaro;Ottorino Frezza;Francesca Lo Cicero;Alessandro Lonardo;Michele Martinelli;Pier Stanislao Paolucci;Elena Pastorelli;Francesco Simula;Piero Vicini;Giuliano Taffoni;Jose Antonio Pascual;Javier Navaridas;Mikel Luján;John Goodacre;Nikolaos Chrysos	2017	2017 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2017.20	real-time computing;field-programmable gate array;architecture;interconnection;computer science;network topology;arm architecture;supercomputer	HPC	-2.8861406361934003	46.38647112612684	105466
8358a9440f76b232279303a0ed1079eca17eedbb	hardware resource virtualization for dynamically partially reconfigurable systems	dynamically partially reconfigurable systems;field programmable gate array;logic arrays;multimedia application hardware resource virtualization dynamically partially reconfigurable systems embedded system reconfigurable hardware functions conventional hardware devices partial reconfiguration technology reconfigurable hardware designs virtual hardware mechanism logic virtualization reconfigurable hardware design hardware device virtualization field programmable gate array network security reconfigurable system;resource virtualization;reconfigurable system;partial reconfiguration;application software;network security;reconfigurable architectures;application virtualization;reconfigurable logic;virtual reality;reconfigurable hardware designs;virtual reality embedded systems field programmable gate arrays logic cad reconfigurable architectures;operating system for reconfigurable systems;multimedia application;runtime;system performance;embedded system;network security reconfigurable system;embedded systems;hardware device virtualization;operating system;reconfigurable hardware design;operating system for reconfigurable systems hardware resource virtualization;time use;reconfigurable hardware functions;hardware resource virtualization application virtualization application software runtime logic devices reconfigurable logic logic arrays field programmable gate arrays embedded system;logic virtualization;field programmable gate arrays;partial reconfiguration technology;logic cad;reconfigurable hardware;conventional hardware devices;virtual hardware mechanism;logic devices;hardware resource virtualization;hardware	The dynamic partial reconfiguration technology enables an embedded system to adapt its hardware functionalities at run-time to changing environment conditions. However, reconfigurable hardware functions are still managed as conventional hardware devices, and the enhancement of system performance using the partial reconfiguration technology is thus still limited. To further raise the utilization of reconfigurable hardware designs, we propose a virtual hardware mechanism, including the logic virtualization and the hardware device virtualization, for dynamically partially reconfigurable systems. Using the logic virtualization technique, a hardware function that has been configured in the field-programmable gate array (FPGA) can be virtualized to support more than one software application at run-time. Using the hardware device virtualization, a software application can access two or more different hardware functions through the same device node. In a network security reconfigurable system for multimedia applications, our experimental results also demonstrate that the utilization of reconfigurable hardware functions can be further raised using the virtual hardware mechanism. Furthermore, the virtual hardware mechanism can also reduce up to 26% of the time required by using the conventional hardware reuse.	embedded system;field-programmability;field-programmable gate array;hardware virtualization;network security;reconfigurable computing;scalability;virtual machine	Chun-Hsian Huang;Pao-Ann Hsiung	2009	IEEE Embedded Systems Letters	10.1109/LES.2009.2028039	hardware compatibility list;embedded system;computer architecture;piperench;full virtualization;real-time computing;virtualization;application virtualization;hardware acceleration;reconfigurable computing;computer science;virtual machine;network security;hardware virtualization;hardware architecture;virtual reality;hardware register;fpgac;field-programmable gate array	Arch	-1.9598652676195494	49.85444072124605	105697
25f8f57386e63330e857ee362830d35d4025228c	analyzing the effects of compiler optimizations on application reliability	optimisation;reliability;microarchitecture;measurement;program compilers circuit reliability fault tolerance microprocessor chips optimisation;fit target compiler optimizations application reliability transistor sizes transient faults processor designers reorder buffer instruction fetch queue load store queue;circuit reliability;fault tolerance;compiler optimization;transient fault;mathematical model;optimization;optimization benchmark testing reliability measurement equations mathematical model microarchitecture;program compilers;soft error;benchmark testing;microprocessor chips	As transistor sizes decrease, transient faults are becoming a significant concern for processor designers. A rich body of research has focused on ways to estimate the vulnerability of systems to transient errors and on techniques to reduce their sensitivity to soft errors. In this research, we analyze how compiler optimizations impact the expected number of failures during the execution of an application. Typically, optimizations have two effects. First, they increase structures occupancies by allowing more instructions in flight, which in turn increases their susceptibility to soft errors. Additionally, they decrease execution time, decreasing the time during which the application is exposed to transient errors. In particular, we focus on how optimizations impact occupancies in three processor structures, namely the Reorder Buffer, the Instruction Fetch Queue and the Load Store Queue. We explain the interplay between compiler and reliability by studying the changes in the code made by the compiler and the resulting responses at the microarchitectural level. Results from this research allow us to make decisions to keep an application within its performance goals and its vulnerability during its runtime within a well defined FIT target.	best, worst and average case;mathematical optimization;microarchitecture;optimizing compiler;re-order buffer;reliability engineering;run time (program lifecycle phase);secure digital container;soft error;spec#;transistor;vulnerability (computing)	Melina Demertzi;Murali Annavaram;Mary W. Hall	2011	2011 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2011.6114178	benchmark;fault tolerance;computer architecture;parallel computing;real-time computing;soft error;microarchitecture;computer science;operating system;mathematical model;reliability;optimizing compiler;programming language;measurement;statistics	Arch	-2.789550909317564	56.056969919665384	106089
9ec2278834d20f6ecfa124bedc9bb70a4c52417e	supporting differentiated services in computers via networking technologies		Contemporary data centers confront with challenges in managing the trade-offs between resource utilization and applications’ quality of services (QoS). To resolve this issue, as suggested in the community white paper 21st Century Computer Architecture [1], computer architecture needs to provide new, higher-level interfaces beyond a conventional instruction set architecture (ISA) to convey an application’s QoS requirements to the hardware. This work proposes PARD, a programmable architecture for resourcing-on-demand that provides such a new programming interface. PARD is inspired by the observation that a computer is inherently a network in which hardware components communicate via packets (e.g., over the NoC or PCIe). So we can apply networking technologies, e.g., principles of software-defined networking (SDN) [3], to this intra-computer network. PARD addresses three major technical challenges. First, to deal with the semantic gap between high-level applications and underlying hardware packets, PARD attaches a high-level semantic tag (e.g., a virtual machine or thread ID) to each memory-access, I/O, or interrupt packet. Second, to allow a variety of hardware components to be programmed and managed via a common interface, PARD designs programmable control planes that can be integrated into various shared resources (e.g., cache, DRAM, and I/O devices) and can differentially process packets according to tag-based rules. Third, to facilitate programming, PARD abstracts all control planes as a device file tree to provide a uniform programming interface via which users create and apply tag-based rules. With these efforts, PARD enables new functionalities like fully hardware-supported virtualization and differentiated services in computers. More details are described in this paper [2]. BODY A computer is inherently a network. It is promising to apply networking technologies like SDN to computer architecture.	application programming interface;cpu cache;computer architecture;data center;differentiated services;dynamic random-access memory;high- and low-level;input/output;network on a chip;network packet;pci express;requirement;software-defined networking;virtual machine	Jiuyue Ma;Xiufeng Sui;Yupeng Li;Zihao Yu;Bowen Huang;Yungang Bao	2015	TinyToCS		virtualization;quality of service;instruction set;architecture;differentiated services;computer network;thread (computing);network packet;virtual machine;computer science	Arch	-2.388817088870253	49.66287604974499	106384
3dbf9cf019e2dbd653292cc7abc8ce922e99f283	an automated technique for topology and route generation of application specific on-chip interconnection networks	circuit layout cad;integrated circuit layout;network routing;network topology;network-on-chip;noc design;application specific soc design;nanoscale technologies;network-on-chip;on-chip interconnection networks;performance aware layout;route generation;system-on-chip design;three phase technique	Network-on-chip (NoC) has been proposed as a solution to the communication challenges of system-on-chip (SoC) design in nanoscale technologies. Application specific SoC design offers the opportunity for incorporating custom NoC architectures that are more suitable for a particular application, and do not necessarily conform to regular topologies. Custom NoC design in nanoscale technologies must address performance requirements, power consumption and physical layout considerations. This paper presents a novel three phase technique that i) generates a performance aware layout of the SoC, ii) maps the cores of the SoC to routers, and iii) generates a unique route for every trace that satisfies the performance and architectural constraints. We present an analysis of the quality of the results of the proposed technique by experimentation with realistic benchmarks.	benchmark (computing);integrated circuit layout;interconnection;network on a chip;requirement;system on a chip	Krishnan Srinivasan;Karam S. Chatha;Goran Konjevod	2005	ICCAD-2005. IEEE/ACM International Conference on Computer-Aided Design, 2005.		chip;system on a chip;physical design;embedded system;routing;electronic engineering;parallel computing;ic layout editor;computer science;engineering;design layout record;integrated circuit layout;network on a chip;network topology;power network design;satisfiability	EDA	3.2509154812213867	59.3325679299648	106565
0836216ad58eb31d33fa20e6c7c57a9424aa8b57	design and verification of large-scale computers by using ddl	design engineer;gate level description;total support system;powerful function;gate level design;large-scale computer;register transfer level;registers;software systems;automatic control;design optimization;power function;feature extraction;computational modeling;logic design	This paper describes the total support system for DDL which has been approved by design engineers at Fujitsu. A simulator is used not only at register transfer level but also with gate level description. The translator generates gate level designs which are then optimized by designers. The verifier has powerful functions to detect conflicts in specification and its implementation.	computer;register-transfer level;simulation	Nobuaki Kawato;Takao Saito;Fumihiro Maruyama;Takao Uehara	1979	16th Design Automation Conference		embedded system;computer architecture;electronic engineering;logic synthesis;multidisciplinary design optimization;power function;feature extraction;computer science;theoretical computer science;automatic control;processor register;programming language;computational model;register-transfer level;software system;computer engineering	EDA	8.11238154238255	51.7237590460036	106582
c677fd9626b725fbc72c554e439df92e03713983	boundary scan test used at board level: moving towards reality	adaptive design;printed circuit testing automatic testing circuit layout cad logic testing microcomputer applications;binary search trees circuit testing test pattern generators software testing manufacturing automation design for testability prototypes hardware design engineering assembly;automatic testing;circuit design;logic testing;circuit layout cad;validation tool software availability pcb testing board level circuit design testing data flow test vectors test pattern generation pc based bst boundary scan testing;test pattern generator;printed circuit testing;data flow;microcomputer applications	Nederlandse Philips Bedrijven BV, Centre For manufacturing Technology, Automation, P.O. Box 218,5600 MD Eindhoven, The Netherlands Using Boundary Scan Test a t board level gives the board designer and the test engineer new possibilities and challenging solutions to test the assembled design. However, not all details of handling this technique have been dealt with yet. Some problems relate to the design environment, others directly affect the test equipment. Test pattern generation is dealt with, the dataflow around this generation software is discussed and a format for test vectors is presented. An implementation of a low cost Boundary Scan Test design validation tool is also presented.	automation;boundary scan;built-in test equipment;dataflow;test card;test design;test engineer	Frans de Jong	1990		10.1109/TEST.1990.114022	non-regression testing;keyword-driven testing;embedded system;data flow diagram;electronic engineering;scan chain;model-based testing;boundary scan;orthogonal array testing;white-box testing;manual testing;integration testing;engineering;software reliability testing;automatic test pattern generation;software engineering;circuit design;test compression;design for testing;software testing;system testing;engineering drawing;test management approach	EDA	9.752505163066385	52.786244247838574	106656
815b2284b64c0fe8250bb01da8db5b394c28578e	partial error masking to reduce soft error failure rate in logic circuits	dominant value reduction fault tolerance partial error masking soft error failure rate reduction logic circuits node soft error susceptibility reduction heuristics cluster sharing reduction;logic design;satisfiability;logic circuits computer errors costs error correction error correction codes single event transient single event upset error analysis design engineering design methodology;circuit reliability;fault tolerance;error handling;cost effectiveness;failure rate;soft error;error handling logic design fault tolerance circuit reliability	A new methodology for designing logic circuits with partial error masking is described. The key idea is to exploit the asymmetric soft error susceptibility of nodes in a logic circuit by targeting the error masking capability towards the nodes with the highest soft error susceptibility to achieve cost-effective tradeoffs between overhead and reduction in the soft error failure rate. Such techniques can be used in cost-sensitive high volume mainstream applications to satisfy soft error failure rate requirements at minimum cost. Two reduction heuristics, cluster sharing reduction and dominant value reduction, are used to reduce the soft error failure rate significantly with a fraction of the overhead required for conventional TMR.	error detection and correction;failure rate;fault tolerance;heuristic (computer science);logic gate;overhead (computing);requirement;retry;soft error;triple modular redundancy	Kartik Mohanram;Nur A. Touba	2003		10.1109/DFTVS.2003.1250141	exception handling;reliability engineering;fault tolerance;electronic engineering;logic synthesis;real-time computing;cost-effectiveness analysis;soft error;computer science;failure rate;satisfiability	EDA	7.842566191510055	60.28087926795012	106697
4e7aca5ec9a67cb8d076143ca18b9828d27b4989	advanced processor design using hardware description language aidl	hardware design languages;logic design;clocks;signal design;aidl;hardware description languages;process design;vhdl descriptions advanced processor design hardware description language aidl design simulation architecture and implementation level description language;logic design hardware description languages timing microprocessor chips;design simulation;computational modeling;advanced processor design;process design hardware design languages timing clocks computer science computational modeling computer simulation delay circuits signal design;circuits;vhdl descriptions;computer science;hardware description language;architecture and implementation level description language;computer simulation;microprocessor chips;timing	| In order to design advanced processors in a short time, designers must simulate their designs and re ect the results to the designs at the very early stages. However, conventional hardware description languages (HDLs) do not have enough ability to describe designs easily and accurately at these stages. Then, we have proposed a new hardware description language AIDL. In this paper, in order to evaluate the e ectiveness of AIDL, we describe and compare three processors in AIDL and VHDL descriptions.	central processing unit;display resolution;hardware description language;interface description language;level design;processor design;prototype;simulation;vhdl	Takayuki Morimoto;Kazushi Saito;Hiroshi Nakamura;Taisuke Boku;Kisaburo Nakazawa	1997		10.1109/ASPDAC.1997.600261	computer simulation;computer architecture;parallel computing;computer science;theoretical computer science;hardware description language	EDA	6.595426475718422	51.54331234424992	106837
ab797fd862a3b635523ff7b3e9c117e24096bfca	fpga based particle identification in high energy physics experiments	signal processing curve fitting field programmable gate arrays parallel processing particle detectors;detectors;pulse shape discrimination;pulse shape discrimination curve fitting daq high energy physics parallelism pipelining;positron emission tomography;conference paper;physics;computer architecture;pipeline processing field programmable gate arrays data acquisition physics computer architecture detectors positron emission tomography;particle detectors;parallelism;high energy physics;signal processing;iipp fpga particle identification high energy physics experiments on the fly signal processing particle detectors pulse shape hints particle type general purpose daq card real time pulse detection high precision curve fitting loop pipelining interleaved identical parallel processors;pipelining;field programmable gate arrays;curve fitting;daq;data acquisition;parallel processing;pipeline processing	High energy physics experiments require on-the-fly processing of signals from many particle detectors. Such signals contain a high and fluctuating rate of pulses. Pulse shape hints particle type, and the amplitude relates to energy of the particle, while pulse occurrence times are needed for event reconstruction. Traditionally, these parameters have been extracted with the help of complete racks of dedicated electronics. Our FPGA design on a general-purpose DAQ card does real-time pulse detection and high-precision curve fitting. It greatly shrinks required equipment in terms of form factor, cost, power usage, and setup time. Unlike traditional systems, we can handle bursts of back-to-back pulses, pulses as narrow as 6 ns and at rates over 1M pulses per second. We have a novel scalable architecture that combines pipelining and parallelism. Moreover, the parallel part of the architecture uses loop pipelining in each of its interleaved identical parallel processors (IIPPs). An IIPP is a specialized CPU, which executes nested loops, with number of iterations that varies from pulse to pulse. IIPPs are fed data from a FIFO by a priority encoder based dispatcher. Number of IIPPs can be calculated to meet any pulse rate and average pulse width. The architecture is flexible enough to work with a variety of curve fitting algorithms.	algorithm;central processing unit;curve fitting;experiment;fifo (computing and electronics);field-programmable gate array;flip-flop (electronics);general protection fault;general-purpose markup language;iteration;network switch;numerical analysis;parallel computing;pipeline (computing);priority encoder;pulse-width modulation;real-time clock;scalability;sensor;throughput;verilog	H. Fatih Ugurdag;Ali Basaran;Taylan Akdogan;V. Ugur Güney;Sezer Gören	2012	2012 IEEE 23rd International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2012.22	embedded system;parallel processing;computer architecture;parallel computing;real-time computing;computer hardware;computer science;electrical engineering;operating system;signal processing;data acquisition	HPC	-0.7268653391233065	58.259273332698235	106939
aca28da0bc41a212aeccd653dff7121083afe478	path planning using a neuron array integrated circuit	neuromorphic avlsi path planning robotics;path planning;path planning analogue integrated circuits integrated circuit design neural chips;maze environment scenario path planning neuron array integrated circuit neuromorphic engineering neuromorphic integrated circuit integrated circuit design biological nervous systems;integrated circuit design;neural chips;analogue integrated circuits;neurons robots planning hardware	Neuromorphic Engineering is an interdisciplinary field which combines concepts from fields such as biology, neuroscience, computer science and engineering. The goal of this field is to design systems that are based on the principles of biological nervous systems. This paper presents hardware results for path planning using a neuron array integrated circuit. The algorithm is explained and experimental results are presented showing 100% correct performance for a large number of maze environment scenarios. Although there is still more work to be completed before this is a fielded system, this work represents an new application of a neuromorphic Integrated Circuit and the results demonstrate definite potential.	algorithm;computer science;integrated circuit;motion planning;neuromorphic engineering;neuron	Scott Koziol;Stephen Brink;Jennifer Hasler	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6736978	mixed-signal integrated circuit;control engineering;physical design;electronic engineering;computer science;neuromorphic engineering;computer engineering	EDA	9.963674416513658	57.04444110649972	107150
4e7ebdfa1e7efe1210d191845f5696fa636c0549	a formal method for optimal high-level casting of heterogeneous fixed-point adders and subtractors	high level decision taking heterogeneous fixed point adders heterogeneous fixed point subtractors fixed point arithmetic datapaths heterogeneous scaling formal method;adders vectors casting optimization educational institutions indexes quantization signal;electronic design automation adders computational complexity;eda fixed point arithmetic datapath hls	Fixed-point arithmetic datapaths with heterogeneous scaling and wordlengths are commonplace in resource, latency, or power constrained designs. This paper describes and proves correct a formal method for accurate high-level casting of optimal adders and subtractors. The proposed approach allows for an early accurate estimation of resource usage which is then available for high-level decision-taking in the design flow. As a result, decoupling between high-level and low-level synthesis is achieved. Results concerning the impact of the approach on resource estimates and a discussion on the wide applicability of the method are presented.	algorithm;coupling (computer programming);design flow (eda);fixed-point arithmetic;formal methods;high- and low-level;image scaling;language-independent specification	Roberto Sierra;Carlos Carreras;Gabriel Caffarena;Carlos A. López Bario	2015	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2014.2365094	discrete mathematics;parallel computing;theoretical computer science;mathematics;combinational logic	EDA	2.4420251165400493	51.97089245705159	107269
6f4c48427c075b944a952bab21d356a4ad54b491	embedded system covalidation with rtos model and fpga	system level verification;embedded system		embedded system;field-programmable gate array	Seiya Shibata;Shinya Honda;Yuko Hara-Azumi;Hiroyuki Tomiyama;Hiroaki Takada	2008	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.1.126	embedded system;computer science	HCI	4.819609014475817	50.534648624095446	107277
18ae3109eab00d1d84f579732e385cb421e5d294	low overhead fault-tolerant fpga systems	reconfiguration;automotive engineering;concepcion asistida;field programmable gate array;microprocessor;system reliability;computer aided design;concepcion circuito;reconfiguracion;reliability;arquitectura circuito;interconnection;unique reconfiguration;fault tolerant;fault tolerant fpga systems;integrated circuit;reconfigurable architectures;funcion logica;integrated circuit reliability fault tolerant computing field programmable gate arrays reconfigurable architectures redundancy timing;circuit design;space exploration;circuit architecture;circuito integrado;physical design;overhead;fault tolerant systems field programmable gate arrays tiles application specific integrated circuits automotive engineering space exploration reliability costs system testing energy consumption;functionally equivalent tile;functional equivalence;indexing terms;red puerta programable;reseau porte programmable;logical function;fonction logique;interconexion;fault tolerant system;fault tolerant computing;redundancy;design method;redundant backup;fault tolerant systems;energy consumption;application specific integrated circuits;application specific integrated circuit;fault tolerance;interconnexion;architecture circuit;sistema tolerando faltas;conception assistee;system testing;systeme tolerant les pannes;reconfiguration unique;conception circuit;microprocesseur;tiles;field programmable gate arrays;integrated circuit reliability;microprocesador;circuit integre;system metric;reconfiguration capabilities;timing overhead fault tolerant fpga systems system metric system reliability reconfiguration capabilities functionally equivalent tile redundant backup mcnc benchmarks;mcnc benchmarks;timing	Fault-tolerance is an important system metric for many operating environments, from automotive to space exploration. The conventional technique for improving system reliability is through component replication, which usually comes at significant cost: increased design time, testing, power consumption, volume, and weight. We have developed a new fault-tolerance approach that capitalizes on the unique reconfiguration capabilities of FPGAs. The physical design is partitioned into a set of tiles. In response to a component failure, a functionally equivalent tile that does not rely on the faulty component replaces the affected tile. Unlike ASIC and microprocessor design methods, which result in fixed structures, this technique allows a single physical component to provide redundant backup for several types of components. Experimental results conducted on a subset of the MCNC benchmarks demonstrate a high level of reliability with low timing and hardware overhead.	application-specific integrated circuit;backup;failure cause;fault tolerance;field-programmable gate array;high-level programming language;microprocessor;overhead (computing);physical design (electronics);processor design	John Lach;William H. Mangione-Smith;Miodrag Potkonjak	1998	IEEE Trans. VLSI Syst.	10.1109/92.678870	embedded system;fault tolerance;electronic engineering;real-time computing;computer science;engineering;operating system;computer aided design	EDA	8.210907005848116	58.155862205061275	107419
b5395cf22fa5110d33582a48c0aff0db86280e6e	virtual prototyping and performance analysis of two memory architectures	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices;virtual prototyping;memory architecture;performance analysis	The gap between CPU and memory speed has always been a critical concern that motivated researchers to study and analyze the performance of memory hierarchical architectures. In the early stages of the design cycle, performance evaluation methodologies can be used to leverage exploration at the architectural level and assist in making early design tradeoffs. In this paper, we use simulation platforms developed using the VisualSim tool to compare the performance of two memory architectures, namely, the Direct Connect architecture of the Opteron, and the Shared Bus of the Xeon multicore processors. Key variations exist between the two memory architectures and both design approaches provide rich platforms that call for the early use of virtual system prototyping and simulation techniques to assess performance at an early stage in the design cycle.	c++;central processing unit;clock rate;connect:direct;crossbar switch;direct connect (protocol);front-side bus;hypertransport;memory controller;microprocessor;multi-core processor;performance evaluation;performance prediction;profiling (computer programming);prototype;random-access memory;routing;simulation;systemc;systems modeling;vhdl;verilog	Huda S. Muhammad;Assim Sagahyroon	2009	EURASIP J. Emb. Sys.	10.1155/2009/984891	uniform memory access;shared memory;embedded system;computer architecture;parallel computing;computer science;physical address;virtual memory;operating system;memory protection;flat memory model;computing with memory	Arch	-3.471886044507967	54.57895567963684	107421
a82303bae23f80425bd923f5e67be3b7951f5f18	using design space exploration for finding schedules with guaranteed reaction times of synchronous programs on multi-core architecture		The synchronous model of computation is well suited for real-time systems, because it allows static analysis in order to find and guarantee their reaction times. Today’s multi-core systems are becoming the predominant computing platforms. Synchronous programs are typically compiled into single threaded code, which makes them unsuitable for exploiting parallelism of the multi-core platforms. Moreover, static timing analysis becomes highly intractable for multi-core systems. This article proposes a novel methodology that aims at finding the mapping and schedule of synchronous programs that guarantees, statically, reaction times when mapped onto a multi-core system consisting of two types of time-predictable cores. The proposed methodology combines design space exploration based on evolutionary algorithm and scheduling of parts of synchronous programs. It allows minimizing the resource usage in terms of number of cores by finding the mapping and schedule with the guaranteed reaction time for architectures with different number of cores. In particular, we: (a) transform a synchronous program written in synchronous SystemJ to a graph-based model represented with two types of computation nodes suitable for execution on two types of time-predictable cores, (b) perform mapping of computation nodes on a customizable multi-core platform using genetic operations, and (c) generate a resulting static schedule of computation nodes for each mapping as part of the design space exploration. The design flow, from program specification and node mapping to the design space exploration and multi-core scheduling is completely	compiler;design space exploration;emoticon;evolutionary algorithm;formal specification;intel core (microarchitecture);model of computation;multi-core processor;multiprocessing;parallel computing;real-time clock;real-time computing;scheduling (computing);static program analysis;static timing analysis;thread (computing);threaded code	Zhenmin Li;HeeJong Park;Avinash Malik;Kevin I-Kai Wang;Zoran A. Salcic;Boris Kuzmin;Michael Glaß;Jürgen Teich	2017	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2016.12.003	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing;programming language;algorithm	Embedded	-2.5327789827974	51.2272776087816	107475
a55b3a91a7d27ed7ea62c907d2b4bb84f136b060	the cbp parameter-a useful annotation to aid block-diagram compilers for dsp	software libraries;processor scheduling;data flow graphs;buffer storage;function block;computer aided software engineering;general methods;fir filter;digital filters;fir filters;software synthesis;program compilers;digital signal processing finite impulse response filter delay educational institutions optimizing compilers filtering computational modeling process design signal design communication channels;computer aided software engineering program compilers data flow graphs software libraries fir filters digital filters buffer storage processor scheduling;multi rate fir filtering operation cbp parameter block diagram compilers dsp software synthesis block diagram specifications functional block data values consumed before produced parameter token transfer buffer sizes	Memory consumption is an important metric during software synthesis from block-diagram specifications of DSP applications. Conventionally, no assumption is made about when during the execution of a functional block (actor), the associated data values (tokens) are actually consumed and produced. However, we show in this paper that it is possible to concisely and precisely capture key properties pertaining to the relative times at which tokens are produced and consumed by an actor. We show this by introducing the consumed-before-produced (CBP) parameter, which provides a general method for characterizing the token transfer of an actor. Good bounds on the CBP parameter can aid a block-diagram compiler in performing more aggressive optimizations for reducing buffer sizes on the edges between actors. We formally define the CBP parameter; derive some useful properties of this parameter; illustrate how the value of the parameter can be derived by examining in detail the multi-rate FIR filtering operation; and examine CBP parameterizations for several other practical DSP actors.	compiler;diagram;digital signal processor;finite impulse response;library (computing);pareto efficiency;requirement;sample rate conversion	S. S. Bhattacharyya;P. K. Murthy	2000		10.1109/ISCAS.2000.858725	embedded system;parallel computing;real-time computing;telecommunications;computer science;engineering;electrical engineering;theoretical computer science;operating system;finite impulse response;algorithm	PL	2.570577278315299	52.24982891750506	107599
01f2d4a22dfe6b505dbb78df480c729e18c6802f	wearcore: a core for wearable workloads?	google;wearbench;image recognition;wearables;speech;industries;wearcore;digital assistant;speech recognition;quality of service;dnn;program processors;hardware	Lately, the industry has recognized immense potential in wearables (particularly, smartwatches) being an attractive alternative/supplement to the smartphone. To this end, there has been recent activity in making the smartwatch `self-sufficient' i.e. using it to make/receive calls, etc. independently of the phone. This marked shift in the way wearables will be used in future calls for changes in the core micro-architecture of smartwatch processors.  In this work, we first identify ten key target applications for the smartwatch users that the processor must be able to quickly and efficiently execute. We show that seven of these workloads are inherently parallel, and are compute- and data-intensive. We therefore propose to use a multi-core processor with simple out-of-order cores (for compute performance) and augment them with a light-weight software-assisted hardware prefetcher (for memory performance). This simple core with the light-weight prefetcher, called WearCore, is 2.9x more energy-efficient and 2.8x more area-efficient over an in-order core. The improvements are similar with respect to an out-of-order core.	data-intensive computing;microarchitecture;multi-core processor;parallel computing;prefetcher;smartphone;smartwatch;wearable computer	Sanyam Mehta;Josep Torrellas	2016	2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)	10.1145/2967938.2967956	embedded system;parallel computing;real-time computing;quality of service;wearable computer;computer hardware;computer science;speech;operating system	Arch	-1.8730905647714635	58.17843570247674	107922
854ebcf62799734a790ee586f16a7f6269d09ed2	the next 700 cpu power models		Software power estimation of CPUs is a central concern for energy efficiency and resource management in data centers. Over the last few years, a dozen of ad hoc power models have been proposed to cope with the wide diversity and the growing complexity of modern CPU architectures. However, most of these CPU power models rely on a thorough expertise of the targeted architectures, thus leading to the design of hardware-specific solutions that can hardly be ported beyond the initial settings. In this article, we rather propose a novel toolkit that uses a configurable/interchangeable learning technique to automatically learn the power model of a CPU, independently of the features and the complexity it exhibits. In particular, our learning approach automatically explores the space of hardware performance counters made available by a given CPU to isolate the ones that are best correlated to the power consumption of the host, and then infers a power model from the selected counters. Based on a middleware toolkit devoted to the implementation of software-defined power meters, we implement the proposed approach to generate CPU power models for a wide diversity of CPU architectures (including Intel, ARM, and AMD processors), and using a large variety of both CPU and memory-intensive workloads. We show that the CPU power models generated by our middleware toolkit estimate the power consumption of the whole CPU or individual processes with an accuracy of 98.5% on average, thus competing with the state-of-the-art power models.		Maxime Colmant;Romain Rouvoy;Mascha Kurpicz;Anita Sobe;Pascal Felber;Lionel Seinturier	2018	Journal of Systems and Software	10.1016/j.jss.2018.07.001	resource management;porting;real-time computing;efficient energy use;software;computer science;central processing unit;cpu power dissipation;middleware	OS	-1.4777924317235176	47.39581925403464	108013
7b65d4961b9cd5c1c015a900f301d238eaf29714	vast: virtualization-assisted concurrent autonomous self-test	software;online self test;virtualization software;logic testing built in self test circuit testing concurrent engineering;failure detection;virtualization assisted concurrent autonomous self test;system on a chip;chip;virtual machine monitors;computer architecture;large scale;built in self test;complex system;registers;system design;logic testing;normal operator;circuit testing;vast;built in self test vast virtualization assisted concurrent autonomous self test online self test large scale robust systems circuit failure prediction failure detection self healing virtualization software;circuit failure prediction;self healing;concurrent engineering;built in self test system testing circuit testing automatic testing large scale systems robustness hardware computer architecture design optimization system performance;hardware;large scale robust systems;failure prediction	Virtualization-assisted concurrent, autonomous self-test, or VAST, enables a multi-/many-core system to test itself, concurrently during normal operation, without any user-visible downtime. Such on-line self-test is required for large-scale robust systems with built-in support for circuit failure prediction, failure detection, diagnosis, and self-healing. The main idea behind VAST is hardware and software co-design of on-line self-test features in a multi-/many-core system through integration of: 1. multi-/many-core architecture, 2. virtualization software, and, 3. special self-test techniques such as BIST (built-in self-test) or CASP (concurrent autonomous chip self-test using stored patterns). As a result, optimized trade-offs in system design complexity, system performance and power impact, and test thoroughness are possible. Experimental results from an actual multi-core system demonstrate that: 1. VAST is practical and effective; and, 2. Special VAST-supported self-test policies enable extremely thorough on-line self-test with very small performance impact.	autonomous robot;built-in self-test;casp;downtime;intel core (microarchitecture);multi-core processor;online and offline;systems design;video ad serving template	Hiroaki Inoue;Yanjing Li;Subhasish Mitra	2008	2008 IEEE International Test Conference	10.1109/TEST.2008.4700583	chip;system on a chip;reliability engineering;embedded system;complex systems;real-time computing;telecommunications;computer science;engineering;operating system;processor register;normal operator;concurrent engineering;computer engineering;systems design	EDA	6.614156367755132	57.941694982580344	108018
05ef37f05766c7c9cfe15d76c992ac0be415369c	a graph drawing based spatial mapping algorithm for coarse-grained reconfigurable architectures	processing element;graph theory;resource utilization;topology;evaluation performance;optimisation;kernel;interconnection;compilateur;multimedia;performance evaluation;graph drawing;optimizacion;integrated circuit;coarse grained reconfigurable architectures;application software;routing;flexibilidad;reconfigurable architectures;evaluacion prestacion;performance optimization techniques;spatial mapping;circuito integrado;reconfigurable architectures routing fasteners topology kernel energy consumption computer science parallel processing application software throughput;compiler;carta de datos;partage des ressources;algorithme;interconexion;algorithm;reconfigurable architectures graph theory operating system kernels program compilers;reconfigurable architecture;performance improvement;power optimization techniques graph drawing based spatial mapping algorithm coarse grained reconfigurable architectures cgra compilers split push kernel mapping performance optimization techniques;energy consumption;mappage;resource sharing;interconnexion;particion recursos;cgra compilers;flexibilite;fasteners;optimization;mapping;procesador;computer science;power consumption;operating system kernels;consommation energie electrique;processeur;coarse grained;power optimization techniques;program compilers;performance optimization;split push kernel mapping;architecture reconfigurable;kernel mapping;parallel processing;graph drawing based spatial mapping algorithm;processor;circuit integre;compilador;flexibility;reconfigurable architecture compiler kernel mapping;throughput;algoritmo	Recently coarse-grained reconfigurable architectures (CGRAs) have drawn increasing attention due to their efficiency and flexibility. While many CGRAs have demonstrated impressive performance improvements, the effectiveness of CGRA platforms ultimately hinges on the compiler. Existing CGRA compilers do not model the details of the CGRA, and thus they are i) unable to map applications, even though a mapping exists, and ii) using too many processing elements (PEs) to map an application. In this paper, we model several CGRA details, e.g., irregular CGRA topologies, shared resources and routing PEs in our compiler and develop a graph drawing based approach, split-push kernel mapping (SPKM), for mapping applications onto CGRAs. On randomly generated graphs our technique can map on average 4.5times more applications than the previous approach, while generating mappings which have better qualities in terms of utilized CGRA resources. Utilizing fewer resources is directly translated into increased opportunities for novel power and performance optimization techniques. Our technique shows less power consumption in 71 cases and shorter execution cycles in 66 cases out of 100 synthetic applications, with minimum mapping time overhead. We observe similar results on a suite of benchmarks collected from Livermore loops, Mediabench, Multimedia, Wavelet and DSPStone benchmarks. SPKM is not a customized algorithm only for a specific CGRA template, and it is demonstrated by exploring various PE interconnection topologies and shared resource configurations with SPKM.	algorithm;compiler;control flow;design flow (eda);experiment;graph drawing;interconnection;livermore loops;mathematical optimization;overhead (computing);performance tuning;procedural generation;reconfigurability;reconfigurable computing;routing;run time (program lifecycle phase);synthetic intelligence;wavelet	Jonghee W. Yoon;Aviral Shrivastava;Sanghyun Park;Minwook Ahn;Yunheung Paek	2009	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2008.2001746	shared resource;embedded system;parallel processing;routing;throughput;in situ resource utilization;compiler;electronic engineering;application software;parallel computing;kernel;real-time computing;computer science;graph theory;theoretical computer science;operating system;integrated circuit;interconnection;graph drawing;algorithm	EDA	-0.29525512146660676	51.976030746239	108073
7a97f855f4dcf863af0faaac281ff71582bad0b6	integrated circuit design. power and timing modeling, optimization and simulation	digital circuit;delay degradation model ddm;glitch;propagation delay;cmos	Many people are trying to be smarter every day. How's about you? There are many ways to evoke this case you can find knowledge and lesson everywhere you want. However, it will involve you to get what call as the preferred thing. When you need this kind of sources, the following book can be a great choice. integrated circuit design power and timing modeling optimization and simulation 12th international workshop patmos 2002 seville spain september 2002 proceedings is the PDF of the book.	integrated circuit design;mathematical optimization;portable document format;simulation	Christian Piguet;Gilles Sicard	2002		10.1007/3-540-45716-X	electronic engineering;real-time computing;delay calculation;telecommunications	EDA	8.804024732003182	54.638324169860056	108272
3c5ed309f7dfa1b54be85953caebd020238e2682	efficient and reliable high-level synthesis design space explorer for fpgas	libraries;rpcl model high level synthesis hls design space explorer dse field programmable gate arrays fpga c based vlsi design rtl design microarchitectures functional unit fu constraint file pareto optimal designs logic primitives logic synthesis adaptive windowing method learning method rival penalized competitive learning;lead;vlsi circuit optimisation field programmable gate arrays high level synthesis integrated circuit design pareto optimisation unsupervised learning;lead libraries algorithm design and analysis;algorithm design and analysis	This paper presents a dedicated High-Level Synthesis (HLS) Design Space Explorer (DSE) for FPGAs. C-based VLSI design has the advantage over conventional RTL design that it allows the generation of micro-architectures with unique area vs. performance trade-offs without having to modify the original behavioral description (in this work area vs. latency). This is typically done by modifying the Functional Unit (FU) constraint file or setting different synthesis directives e.g. unroll loops or synthesize arrays as RAM or registers. The result of the design space exploration is a set of Pareto-optimal designs. In this work, we first investigate the quality of the exploration results when using the results reported after HLS (in particular the area) to guide the explorer in finding Pareto-optimal designs. We found that due to the nature of how HLS tools pre-characterize, the area and delay of basic logic primitives and the FPGAs internal structure the area results are not accurate and hence making it necessary to perform a logic synthesis after each newly generated design. This in turn leads to unacceptable long running time. This work therefore presents a dedicated DSE for FPGAs based on a pruning with adaptive windowing method to extract the design candidates to be further (logic) synthesized after HLS. The adaptive windowing is based on a learning method inspired from Rival Penalized Competitive Learning (RPCL) model in order to classify which designs need to be synthesized to find the true Pareto-optimal designs. Results show that our method leads to similar results compared to an explorer which performs a logic synthesis for each newly generated design, while being much faster.	competitive learning;design space exploration;field-programmable gate array;high-level synthesis;logic synthesis;loop unrolling;pareto efficiency;random-access memory;register-transfer level;time complexity;very-large-scale integration	Dong Liu;Benjamin Carrión Schäfer	2016	2016 26th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2016.7577370	embedded system;algorithm design;lead;logic synthesis;real-time computing;simulation;computer science;operating system	EDA	1.3018307476609912	51.40070442322191	108376
812a5ec5d2018d1efb19b9affdddf0054b0c046a	mapping loops onto coarse-grained reconfigurable array using genetic algorithm	cgra;genetic algorithm;loop mapping	Coarse-grained reconfigurable array (CGRA) is a competitive hardware platform for computation intensive tasks in many application domains. The performance of CGRA heavily depends on the mapping algorithm which exploits different level of parallelisms. Unfortunately, the mapping problem on CGRA is proved to be NP-complete. In this paper, we propose a genetic based modulo scheduling algorithm to map application kernels onto CGRA. An efficient routing heuristic is also presented to reduce the mapping time. Experiment result shows our algorithm outperforms other heuristic algorithms both in solution's quality and mapping time. © Springer-Verlag Berlin Heidelberg 2013.	genetic algorithm	Li Zhou;Dongpei Liu;Min Tang;Hengzhu Liu	2013		10.1007/978-3-642-37502-6_95	parallel computing;genetic algorithm;computation;scheduling (computing);heuristic;computer science;modulo	Robotics	-0.39687046444285523	50.63962174746975	108398
c2447ce58036805df4b3cbb4ac2fd6587d0051a5	thwarting software attacks on data-intensive platforms with configurable hardware-assisted application rule enforcement	reconfigurable architectures;bluespec;tailored trustworthy space software attacks data intensive platforms configurable hardware assisted application rule enforcement resource sharing data intensive computing systems reconfigurable hardware streaming operations formally verified hardware controller real time monitoring controller wrapped datapath hardware plug in cryptographic operations cryptographic keys;formal verification;cognitive radio;cryptography;hardware software registers monitoring field programmable gate arrays cryptography;reconfigurable architectures cryptography;software attacks;data intensive computing;cognitive radio security software attacks tailored trustworthy space data intensive computing reconfigurable hardware formal verification bluespec;security;tailored trustworthy space;reconfigurable hardware	Security is difficult to achieve on general-purpose computing platforms due to their complexity, excess functionality, and resource sharing. An alternative is the creation of a Tailored Trustworthy Space for the system or application class of interest. We focus on data-intensive computing systems using reconfigurable hardware to implement streaming operations, and provide security assurances that are independent of application software, middleware, or operating system integrity and correctness. All interaction between software and the dataflow hardware passes through an automatically synthesized and formally verified hardware controller incorporating enforcement and real-time monitoring of application-specific rules. Abstractions provided by the Blue spec high-level language assist in the translation of domain-specific policy rules to synthesized logic. For the cognitive radio example used, hardware-enforced policies include physical layer rules such as sanctioned spectrum usage. Policy changes cause the secure generation and transfer of a new controller-wrapped datapath hardware plug-in. Datapath dynamic block swaps and cryptographic operations are managed entirely by the hardware controller rather than software drivers. Design for performance and design for security are therefore simultaneously addressed since the datapath is configured and monitored at hardware speeds, and software has no access to datapath configurations and cryptographic keys.	cognitive radio;computer security;correctness (computer science);cryptography;data-intensive computing;dataflow;datapath;embedded system;encapsulation (networking);field-programmable gate array;formal verification;general-purpose markup language;high- and low-level;high-level programming language;high-throughput computing;key (cryptography);logic programming;middleware;offset binary;operating system;plug-in (computing);real-time locating system;requirement;spec#;system integrity;throughput;trustworthy computing;zero-day (computing)	Mohammed M. Farag;Lee W. Lerner;Cameron D. Patterson	2011	2011 21st International Conference on Field Programmable Logic and Applications	10.1109/FPL.2011.45	hardware compatibility list;embedded system;cognitive radio;parallel computing;real-time computing;hardware acceleration;formal verification;reconfigurable computing;computer science;cryptography;information security;operating system;hardware architecture	Arch	0.17540356645712776	47.476198677915924	108449
10e2c362e94bb175d5f80fae943cf7d52c2a0c0b	compiling path expressions into vlsi circuits	asynchronous circuit;total length;path expressions	Path expressions were originally proposed by Campbell and Habermann [2] as a mechanism for process synchronization at the monitor level in software. Not surprisingly, they also provide a useful notation for specifying the behavior of asynchronous circuits. Motivated by these potential applications we investigate how to directly translate path expressions into hardware. Our implementation is complicated in the case of multiple path expressions by the need for synchronization on event names that are common to more than one path. Moreover, since events are inherently asynchronous in our model, all of our circuits must be self-timed. Nevertheless, the circuits produced by our construction have are proportional to N · log(N) where N is the total length of the multiple path expression under consideration. This bound holds regardless of the number of individual paths or the degree of synchronization between paths. Furthermore, if the structure of the path expression allows partitioning, the circuit can be laid out in a distributed fashion without additional area overhead.	asynchronous circuit;nico habermann;overhead (computing);path expression;self-replication;synchronization (computer science)	Thomas S. Anantharaman;Edmund M. Clarke;Michael J. Foster;Bud Mishra	1986	Distributed Computing	10.1007/BF01661169	asynchronous circuit;computer science	PL	5.420996207257094	55.95623863234235	108726
af23b601d50a0063040cebbe2174d53391015429	exploring algorithmic trading in reconfigurable hardware	software;field programmable gate array;pediatrics;clocks;reconfigurable architectures field programmable gate arrays;reconfigurable architectures;algorithmic trading;consumer electronics;computer networks;hardware architecture;computer architecture;engines;hardware software algorithms field programmable gate arrays delay engines throughput computer architecture consumer electronics computer networks educational institutions;software algorithms;field programmable gate arrays;xilinx vertex xc5vlx30 fpga reconfigurable hardware field programmable gate array software implementation;reconfigurable hardware algorithmic trading;reconfigurable hardware;parallel processing;software implementation;throughput;hardware	This paper describes an algorithmic trading engine based on reconfigurable hardware, derived from a software implementation. Our approach exploits parallelism and reconfigurability of field-programmable gate array (FPGA) technology. FPGAs offer many benefits over software solutions, including a reduction in latency, while increasing overall throughput and computational density. All of which are important attributes to a successful algorithmic trading engine. Experiments show that the peak performance of our hardware architecture for algorithmic trading is 133 times faster than the corresponding software implementation. Six implementations can operate simultaneously on a Xilinx Vertex 5 xc5vlx30 FPGA on average, maximising performance and available resource usage.	algorithmic trading;field-programmability;field-programmable gate array;parallel computing;reconfigurability;throughput	Stephen Wray;Wayne Luk;Peter R. Pietzuch	2010	ASAP 2010 - 21st IEEE International Conference on Application-specific Systems, Architectures and Processors	10.1109/ASAP.2010.5540966	embedded system;parallel processing;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;hardware architecture;field-programmable gate array	EDA	7.827348780642724	47.026191540429345	108928
09d10fede9ef9b316ba2911fa9e2e52ef8caadda	an infrastructure ip for online testing of network-on-chip based socs	network on a chip system testing system on a chip life testing power system reliability costs power system management energy consumption energy management runtime;infrastructure ip;reliability;on line testing;network on chip;integrated circuit testing infrastructure ip online testing network on chip soc reliability concerns systems on a chip designs;runtime;system on a chip;chip;built in self test;life testing;complex system;energy consumption;power system management;integrated circuit testing;reliability built in self test integrated circuit testing network on chip;system testing;soc;power system reliability;network on a chip;online testing;systems on a chip designs;reliability concerns;energy management	To address the reliability concerns that affect the lifetime of complex systems-on-a-chip (SoC) designs, a concurrent on-line SoC test scheme is essential to circumvent the prohibitive costs - test time and test power $associated with off-line SoC test. A test infrastructure IP (TI-IP) is deployed within the network-on-chip (NoC) based SoC design to provide on-line test support while managing the intrusion of test into the executing applications within the system. This research describes the architecture and operation of a TI-IP capable of testing SoCs and demonstrates its operation in two SoC test configurations developed using research domain application and test benchmarks	complex systems;network on a chip;online and offline;system on a chip	Praveen Bhojwani;Rabi N. Mahapatra	2007	8th International Symposium on Quality Electronic Design (ISQED'07)	10.1109/ISQED.2007.35	system on a chip;embedded system;electronic engineering;real-time computing;computer science;engineering;network on a chip	EDA	6.555126378265045	58.019106559649885	108939
962ef011472a0d5e4ebdc0e7ba5ab22a674d306e	interoperability and quality of new eda tools for sequential logic synthesis	electronic design automation and methodology collaborative tools circuit synthesis collaborative work process design modems microelectronics logic circuits logic design timing;information driven logic synthesis tools interoperability microelectronic systems eda tool chain quality sequential logic synthesis information driven logic synthesis approach;sequential circuits;integrated circuit design;logic synthesis;sequential circuits open systems integrated circuit design circuit cad logic cad;circuit cad;experimental research;open systems;logic cad	One of the main problems in design of modem microelectronic systems is achieving consistent high quality results along the entire EDA tool chain. Using the sequential logic synthesis tools for a case study, this paper shows how important is the consistent tool collaboration for the quality of the final result. In the paper, a new uniform and consistent information-driven logic synthesis approach is proposed and compared to some other logic synthesis flows, including the traditional flow involving JEDI and SIS. The experimental research demonstrates that the quality of the new information-driven logic synthesis tools and the harmony of the new uniform approach results in much better circuits than the circuits from all other flows. The information-based synthesis flow produced circuits that are on average 25% smaller and 30% faster than the circuits from traditional flow.	interoperability;logic synthesis;sequential logic	Aleksander Slusarczyk;Lech Józwiak	2002		10.1109/ISQED.2002.996700	computer architecture;electronic engineering;logic synthesis;logic optimization;asynchronous circuit;electronic design automation;logic family;computer science;engineering;programmable logic device;sequential logic;common power format;hardware description language;open system;digital electronics;register-transfer level;computer engineering;integrated circuit design	EDA	9.872701681752075	52.91512445340206	109363
a8f149f299c740697607c3be6f55e6a4804d4e9a	improving performance and quality-of-service through the task-parallel model : optimizations and future directions for openmp	communication systems;datalogi;kommunikationssystem;computer science	With the failure of Dennard's scaling, which stated that shrinking transistors will be more power-efficient, computer hardware has today become very divergent. Initially the change only concerned t ...	openmp;quality of service	Artur Podobas	2015			simulation;computer science;theoretical computer science	HPC	-3.5116230652893337	48.28996040812782	109576
6f7481015c2c67775889473c8e11700234ab3812	improving evolutionary real-time testing by seeding structural test data	evolutionary computation;system testing evolutionary computation real time systems timing embedded system algorithm design and analysis robustness pathology vehicle safety genetic mutations;real time;real time embedded system;satisfiability;embedded system;embedded systems;structural testing;worst case execution time;program testing;source code evolutionary real time testing seeding structural test data timing constraints embedded systems extreme execution times;source code;search problems;evolutionary algorithm;evolutionary testing;search problems embedded systems evolutionary computation program testing;time constraint	Timing constraints in embedded systems must be satisfied so that real-time embedded systems work properly and safely. Execution time testing involves finding the best and worst case execution times. Evolutionary testing is used to dynamically search for the extreme execution times. During the evolutionary search, some parts of the source code are never accessed. Moreover, it turns out that the search delivers different extreme execution times in a high number of generations. We propose a new approach which makes use of seeding the evolutionary algorithm with test data achieved a high structural coverage. This new method leads to raise the confidence in the results and to gain in efficiency in terms of number of generations needed.	best, worst and average case;code coverage;embedded system;evolutionary algorithm;real-time clock;real-time testing;run time (program lifecycle phase);test data	Marouane Tlili;Harmen Sthamer;Stefan Wappler;Joachim Wegener	2006	2006 IEEE International Conference on Evolutionary Computation	10.1109/CEC.2006.1688405	parallel computing;real-time computing;computer science;evolutionary algorithm;distributed computing;evolutionary computation;worst-case execution time;satisfiability;source code	Embedded	0.5003108795926811	56.82003212235685	109595
f89353a4d264ec695e8c6114a98ec6c26f0ff3c5	design and development of multi-wavelet image fusion system based on rf5 framework	digital signal processing;image fusion;wavelet transforms;algorithm design and analysis;matlab	In order to overcome some shortcomings of realizing image processing using traditional DSP programming, such as long development time, low efficiency, complex, not universal system, irregularities and other issues, this paper puts forward a solution of taking TMS320DM642's TI chip as the development platform and adopting RF5 (Reference Framework 5) to realize multi-wavelet image fusion. Through this solution, we can load, fuse, and display the images that have been registered and transformed by multi-wavelet. The authors comprehensively utilize Matlab/Simulink software, CCS4.0 (embedded software tools, connection) platform, to achieve the design and development of DSP multi-wavelet image fusion system by means of automatically generating code. We implemented the fusion algorithm according to the standard of TMS320DSP algorithm and RF5, so as to realize the standardization of algorithm and software interface, which can reduce system integration tasks, shorten the development cycle, make expansion more convenient and easier.	algorithm;digital signal processing;digital signal processor;embedded software;embedded system;image fusion;image processing;matlab;simulink;system integration;texas instruments tms320;wavelet	Zhihui Wang;Gefei Xu;Yue Lv;Xudong Wu;Minjun Wang;Li Du	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852794	embedded system;algorithm design;computer vision;real-time computing;computer science;theoretical computer science;digital signal processing;image fusion;wavelet transform	Robotics	3.3155294652226854	49.79767274869484	109699
a4ee7cf445e773000829f202d4c006e726bb8e67	rule-based data communication optimization using quantitative communication profiling	multiprocessor interconnection networks;optimisation;data communication;parallel architectures;parallel architectures data communication multiprocessing systems multiprocessor interconnection networks optimisation;hardware data communication acceleration pipeline processing memory management software;data communication bottleneck rule based data communication optimization quantitative communication profiling multicore architectures hardware accelerator systems heterogeneous processing elements digital systems data communication overhead reduction heuristic based approach interconnect designs accelerator functions;multiprocessing systems;design rules hardware accelerator data communication bottleneck quantitative profiling	Multicore architectures, especially hardware accelerator systems with heterogeneous processing elements, are being increasingly used due to the increasing processing demand of modern digital systems. However, data communication in multicore architectures is one of the main performance bottle-necks. Therefore, reducing data communication overhead is an important method to improve the speed-up of such systems. In this paper, we propose a heuristic-based approach to address the data communication bottleneck. The proposed approach uses a detailed quantitative data communication profiling to generate interconnect designs automatically that are relatively simple, low overhead and low area solutions. Experimental results show that we can gain speed-up of 3.05× for the whole application and up to 7.8× speed-up for accelerator functions in comparison with software.	digital electronics;hardware acceleration;heuristic;logic programming;mathematical optimization;memory hierarchy;multi-core processor;network on a chip;overhead (computing);pipeline (computing);powerpc;run time (program lifecycle phase);speedup	Cuong Pham-Quoc;Zaid Al-Ars;Koen Bertels	2012	2012 International Conference on Field-Programmable Technology	10.1109/FPT.2012.6412119	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	1.2302806104620583	50.556124496296775	109707
5d3b93aa54a788b6ee31b3899dad7b71d07952ff	enhancing gnu radio with heterogeneous computing	software defined radio;heterogeneous computing;gnu radio;coprocessors;software radio;sdr;soc;architecture;dsp;radio accelerators	Software radio system performance can be significantly enhanced with appropriate architectural choices, such as the GNU Radio USRP's division of roles between an FPGA and a general purpose CPU. Heterogeneous architectures provide the most flexibility for achieving high computational performance; yet have not been fully exploited by today's software radio architectures. This paper reports on a GNU Radio implementation for the Texas Instruments TCI663X SOC DSP/ARM that was publically released in 2014 and has seen widespread use in the telecommunications industry.	arm architecture;central processing unit;computation;field-programmable gate array;gnu radio;heterogeneous system architecture;heterogeneous computing;system on a chip;universal software radio peripheral	Alfredo Muniz	2015		10.1145/2801676.2801689	embedded system;parallel computing;computer science;operating system;universal software radio peripheral	Arch	2.5772507838101255	48.117569592853485	109775
089bd27711d5b19fc5c07834ac277412e605ce4f	accelerating dynamic fault tree analysis based on stochastic logic utilizing gpgpus	reliability;logic gates fault trees graphics processing units stochastic processes reliability mathematical model computational modeling;microprocessor chips fault trees graphics processing units;stochastic models dynamic fault tree analysis stochastic logic gpgpu fault trees probability models dynamic gates cold spare gates cpu simulation time;computational modeling;logic gates;stochastic processes;stochastic logic cuda fault tree analysis gpgpu static and dynamic gates;graphics processing units;mathematical model;fault trees	This paper demonstrates on speeding up an accurate analysis of fault trees using stochastic logic through GPGPUs. Actually, probability models of dynamic gates and new accurate models for different combinations of cold spare gate e.g., two cold spare gates with a share spare and a cold spare gate with more than one spare inputs are developed in this paper. Experimental results show that on average, the proposed analysis method is 235 times faster than CPU simulation time. Moreover, proposing new stochastic models results accuracy and simplicity as additional advantages of the proposed method.	central processing unit;fault tree analysis;general-purpose computing on graphics processing units;simulation;stochastic process	Elham Cheshmikhani;Hamid R. Zarandi	2016	2016 24th Euromicro International Conference on Parallel, Distributed, and Network-Based Processing (PDP)	10.1109/PDP.2016.130	stochastic process;parallel computing;real-time computing;fault tree analysis;logic gate;computer science;theoretical computer science;operating system;mathematical model;reliability;distributed computing;computational model;statistics	EDA	5.026446446599527	56.91290386874253	109994
aec941a0106f921793fdf32bae84f28b06576b65	reducing fifo buffer power using architectural alternatives at rtl		NoC has a significant impact on the power, area and performance of multi-core architectures. The contribution of NoC in the total power budget of a CMP is approximately 30 to 40% [1], and the input buffers of router consume most of it. Therefore, the designers need to design a low power communication architecture of NoC by reducing the power consumption of buffers. In the existing techniques, virtual channel buffer power has been optimized by employing buffer sharing, power gating with flexible virtual channels and Dynamic Voltage Frequency Scaling (DVFS). In this paper, we have proposed a) Routing Logic enabled clock gating at input channel buffers b) Further, we applied the clock gating on FPGA slices on our proposed design. Our approach provides a significant improvement in FIFO buffer power in 2D NoC as the FIFO power is optimized by 10.70%. Furthermore, 37% of improvement in dynamic power has been achieved by applying clock gating on slices and block RAM on our proposed routing logic enabled input channel buffers clock gating technique.	clock gating;dynamic voltage scaling;emoticon;fifo (computing and electronics);field-programmable gate array;frequency scaling;multi-core processor;network on a chip;power gating;random-access memory;router (computing);routing;virtual channel	Ashish Sharma;Ruby Ansar;Manoj Singh Gaur;Lava Bhargava;Vijay Laxmi	2016	2016 20th International Symposium on VLSI Design and Test (VDAT)	10.1109/ISVDAT.2016.8064897	electronic engineering;real-time computing;parallel computing;clock gating;fifo (computing and electronics);architecture;power budget;computer science;frequency scaling;dynamic demand;power gating;communication channel	Arch	2.8325097909210335	60.30719973364916	110006
bbc6158cd30e7047bd05414718468a8550f6b52c	efficient asip design for configurable processors with fine-grained resource sharing	design automation;computer architecture;configurable processor;resource sharing;compilation;register file;multi cycle io;asip;application specific instruction set processor	Application-Specific Instruction-set Processors (ASIP) can improve execution speed by using custom instructions. Several ASIP design automation flows have been proposed recently. In this paper, we investigate two techniques to improve these flows, so that ASIP can be efficiently applied to simple computer architectures in embedded applications. Firstly, we efficiently generate custom instructions with multi-cycle IO (which allows multi-outputs), thus removing the constraint imposed by the ports of the register file. Secondly, we allow identical portions of different custom instructions to be shared, thus allowing more custom instructions under the same area constraint. To handle the greatly increased exploration space, we propose several heuristics to keep the problem tractable. Experimental results show that we can achieve 3x speedup in some cases	application-specific instruction set processor;central processing unit;cobham's thesis;computer architecture;embedded system;heuristic (computer science);nam;register file;speedup	Quang Dinh;Deming Chen;Martin D. F. Wong	2008		10.1145/1344671.1344687	shared resource;embedded system;computer architecture;parallel computing;real-time computing;electronic design automation;computer science;operating system;register file	EDA	-0.09161050723789699	51.8361580032744	110590
28ab684810f9b879bd796cf97dcd75442e5568b4	optimizing energy-performance trade-offs in solar-powered edge devices		Power modes can be used to save energy in electronic devices but a low power level typically degrades performance. This trade-off is addressed in the so-called EP-queue model, which is a queue depth dependent M/GI/1 queue augmented with power-down and power-up phases of operation. The ability to change service times by power settings allows us to leverage a Markov Decision Process (MDP), which approach we illustrate using a simple fully solar-powered case study with finite states representing levels of battery charge and solar intensity.	approximation;dynamic programming;expectation propagation;markov chain;markov decision process;mathematical optimization;numerical analysis;optimizing compiler;power supply;powerup (accelerator);real-time clock;response time (technology)	Peter G. Harrison;Naresh M. Patel	2018		10.1145/3184407.3184426	battery (electricity);edge device;response time;control engineering;efficient energy use;electronics;queueing theory;engineering;queue;markov decision process	Metrics	-3.628535083710546	59.42082107122662	110620
9422a7d323aac54206bf86924669b1881028edc6	processor array design with the use of genetic algorithm	processor array design;larger input matrix size;processors arrays design;specimen linear algebra algorithm;genetic algorithm;graph decomposition;projection result;evolutionary algorithm;information dependency graph projection;parallel realization;larger input matrix	In this paper a method for processors arrays design dedicated to realization of specimen linear algebra algorithms in FPGA devices is presented. Within an allocation mapping process a genetic algorithm for information dependency graph projection is used and the runtime of the given algorithm is optimized. For larger input matrices, graph decomposition is used which allows the projection results to be obtained. The obtained projection results, with and without graph decomposition, for a specimen linear algebra algorithm are compared. Additionally, a parallel realization of the evolutionary algorithm for multicore processors is presented, which allows projection results to be obtained for larger input matrix sizes.	genetic algorithm;processor array	Piotr Ratuszniak	2011		10.1007/978-3-642-29843-1_27	mathematical optimization;discrete mathematics;theoretical computer science;mathematics	EDA	0.6194782685451672	51.5112832134441	110948
8ffef5e3e05dbdd4e03f46747e15a3ac579fc99b	a software pipelining algorithm in high-level synthesis for fpga architectures	software;circular dependency;memory address aliasing;modulo scheduling;modulo scheduling algorithm;fpga architectures;pipeline processing software algorithms high level synthesis field programmable gate arrays computer architecture processor scheduling scheduling algorithm system buses digital signal processing software performance;software engineering;computer architecture;high level synthesis;memory address aliasing software pipelining modulo scheduling circular dependency memory lifetime hole;fpga architecture;memory lifetime hole;software algorithms;software pipelining algorithm;memory lifetime holes;circuit cad;software pipelining;field programmable gate arrays;memory lifetime holes software pipelining algorithm high level synthesis fpga architectures modulo scheduling algorithm;algorithm design and analysis;pipeline processing;software engineering circuit cad field programmable gate arrays pipeline processing	In this paper, we present a variation of the Modulo Scheduling algorithm to exploit software pipelining in the high-level synthesis for FPGA architectures. We demonstrate the difficulties of implementing software pipelining for FPGA architectures, and propose a modified version of Modulo Scheduling that utilizes memory lifetime holes and addresses circular dependencies. Experimental results demonstrate a 35% improvement on average over the non-pipelined implementation, and 15% improvement on average over the traditional Modulo Scheduling algorithm.	algorithm;central processing unit;circular dependency;field-programmable gate array;high- and low-level;high-level synthesis;mathematical optimization;modulo operation;pipeline (computing);scheduling (computing);sobel operator;software pipelining	Lei Gao;David Zaretsky;Gaurav Mittal;Dan Schonfeld;Prithviraj Banerjee	2009	2009 10th International Symposium on Quality Electronic Design	10.1109/ISQED.2009.4810311	software pipelining;algorithm design;computer architecture;parallel computing;real-time computing;circular dependency;computer science;high-level synthesis;field-programmable gate array	EDA	-0.7638118766379465	51.751077880485475	110970
5e2abab0b2726382ab566196a6345a77b891f1b0	mini-exec: a portable executive for 8-bit microcomputers	software portability;real time;software systems;microprocessor control systems;chip;control system;multitasking;real time executives	As microprocessor systems and single-chip microcomputers become more complex, so do the software systems developed for them. In many cases, software is being designed that incorporates multiple control functions running asynchronously on a single microprocessor. Here, discussion focuses on the motivation for running such multiple functions under the control of a real-time multitasking executive. A successfully implemented executive whose design is portable and suitable for use on most 8-bit microprocessors is presented.	8-bit;computer multitasking;control function (econometrics);microcomputer;microprocessor;real-time clock;software system	Thomas L. Wicklund	1982	Commun. ACM	10.1145/358690.358693	chip;software portability;embedded system;real-time computing;human multitasking;computer hardware;computer science;control system;operating system;software system	Graphics	4.650973090908948	48.18288062190218	110990
8e0474039df31e675cc6a2e81995c804102dd64c	long term trends for embedded system design	hardware platform;embedded system design;specific hardware sub-systems;system design;design methodologies;larger system;software strategy;long term trends;hardware strategy;hw/sw interfaces;cpu sub-systems;embedded software;application specific electronic sub-system;embedded system;integrated circuit design;system on chip;hardware accelerator;embedded systems;chip	An embedded system is an application specific electronic subsystem used in a larger system such as an appliance, an instrument or a vehicle. An embedded system is generally made of software (called embedded software) and a hardware platform. The evolution of technologies is enabling to the integration of complex platforms in a single chip (called system-on-chip, SoC) including one or several CPU subsystems to execute software and sophisticated interconnect in addition to specific hardware subsystems. Mastering the design of these embedded systems is a challenge for both system and semiconductor houses that used to apply only software strategy or only hardware strategy. This paper analyzes this evolution and defines long term roadmaps for embedded system design.	application-specific integrated circuit;bus mastering;central processing unit;embedded software;embedded system;mpsoc;parallel computing;plan;programming paradigm;requirement;semiconductor;shattered world;software appliance;software design;system integration;system on a chip;systems design	Ahmed Amine Jerraya	2004	Euromicro Symposium on Digital System Design, 2004. DSD 2004.	10.1109/DSD.2004.1333254	chip;system on a chip;hardware compatibility list;embedded system;embedded operating system;computer architecture;real-time computing;embedded software;hardware acceleration;computer science;computer-on-module;hardware architecture;software system;integrated circuit design;avionics software	Embedded	4.4225928231076885	51.20460505437074	111218
d6704de2ff60764269af9935b88fe92eba3fd82a	indexys, a logical step beyond genesys	european commission;cost saving;embedded computing systems;system on a chip;embedded system;chip;computer architecture;mobile communication;embedded platform;time to market;genesys reference architecture;reference architecture;embedded computing;indexys;driver assistance system	Embedded computing systems have become a pervasive aspect in virtually all application domains, such as industrial, mobile communication, transportation and medical. Due to increasing computational capabilities of microcomputers and their decreasing cost, new functionality has been enabled (e.g., driver assistance systems) and cost savings have become possible, e.g., by the replacement of mechanical components by embedded computers. Conventionally, each application domain tends to develop customized solutions, often re-inventing concepts that are already applied in other domains. It is therefore expedient to invest into a generic embedded system architecture that supports the development of dependable embedded applications in many different application domains, using the same hardware devices and software modules. INDEXYS targets to pave the way from the European Commission Framework 7 GENESYS Project reference computing architecture approach towards pilot applications in the automotive-, railwayand aerospace industrial domains. INDEXYS will follow-up GENESYS project results and will implement selected industrial-grade services of GENESYS architectural concepts. The results of laying together GENESYS, INDEXYS and the new ARTEMIS project ACROSS, which will develop multi processor systems on a chip (MPSoC) using GENESYS reference architecture and services, will provide integral cross-domain architecture and platform, designand verificationtools, middleware and flexible FPGAor chipbased devices lowering OEM cost of development and production at faster time-to market.n of COOPERS.	application domain;computer architecture;embedded system;mpsoc;microcomputer;middleware;pervasive informatics;reference architecture;system on a chip;systems architecture	Andreas Eckel;Paul Milbredt;Zaid Al-Ars;Stefan Schneele;Bart Vermeulen;György Csertán;Christoph Scheerer;Neeraj Suri;Abdelmajid Khelil;Gerhard Fohler	2010		10.1007/978-3-642-15651-9_32	chip;system on a chip;reference architecture;embedded system;real-time computing;mobile telephony;computer science;engineering;software engineering;computer security;computer engineering	EDA	4.369971496299716	51.20498560216345	111262
65eb99bb5e8cdf981acf2155918b0e9cc96253df	power and thermal modeling for communication systems	communication systems;thermal behavior;vlsi;power consumption	Power and thermal characteristics have emerged as first-order design goals for all types of semiconductors, including embedded signal and information processing systems. This paper surveys the basic physical principles of power and thermal behavior and argues that statistical models, such as Markov decision processes, are well-suited to the management of power and thermal behavior at both design time and run time.	embedded system;first-order predicate;information processing;markov chain;markov decision process;mathematical optimization;run time (program lifecycle phase);semiconductor;state space;statistical model;thermal management of high-power leds	Marilyn Wolf;Shuvra S. Bhattacharyya;Jacques Florence;Adrian E. Sapio	2016	2016 IEEE International Workshop on Signal Processing Systems (SiPS)	10.1109/SiPS.2016.32	simulation;telecommunications;computer science;very-large-scale integration;communications system	EDA	7.421706119558473	54.708937284253075	111399
f2f43a9af50789a8a16dad103642640cb270105e	a high throughput power-efficient optical memory subsystem for kilo-core processor		High throughput and power-efficient processor-memory communications are of great importance for kilo-core processor design. This paper proposes a hybrid photonic architecture for such communications. Bandwidth-efficient photonic burst switching is used for memory accesses between last-level HBM caches and off-chip HMC memory pools. Simulation results show that the hybrid network achieves up to 25% of system speedup and up to 10 times of energy savings, when compared to conventional electric interconnects.	throughput	Quanyou Feng;Chao Peng;Shuangyin Ren;Hongwei Zhou;Rangyu Deng	2017		10.1007/978-981-10-7844-6_6	throughput;parallel computing;architecture;speedup;burst switching;multi-core processor;kilo-;processor design;memory pool;computer science	Theory	-3.0119293028382184	48.526851848808334	111562
9696c28521af841ced44ad09ab3e63e7b11ac11f	new cad framework extends simulation of dynamically reconfigurable logic	outil logiciel;concepcion asistida;field programmable gate array;computer aided design;software tool;reconfigurable system;integrated circuit;dynamic reconfiguration;reconfigurable logic;simulation;design flow;design practice;circuit switched;simulacion;circuito integrado;circuito logico;red puerta programable;reseau porte programmable;design method;circuit logique;design framework;herramienta controlada por logicial;conception assistee;logic circuit;circuit integre	New design methods and tools are needed to improve the process of designing dynamically reconfigurable logic. This paper reports on the revision and extension of Dynamic Circuit Switching (DCS), a CAD tool for specifying and simulating dynamically reconfigurable systems. The work introduces a new design framework, which exploits existing design practices where feasible, and proposes a new design flow for reconfigurable logic.		David Robinson;Gordon Charles McGregor;Patrick Lysaght	1998		10.1007/BFb0055227	embedded system;real-time computing;design methods;logic gate;computer science;design flow;integrated circuit;computer aided design;circuit switching;field-programmable gate array	EDA	8.96739258011162	51.30782833858468	111599
67f88ffd4590c92ab81d5cc8b0e938dfdcd403cd	a delay distribution methodology for the optimal systolic synthesis of linear recurrence algorithms	red sistolica;graph theory;filtro respuesta impulsion inacabada;filtre reponse impulsion finie;lra;very large scale integration;data flow graphs;systolic arrays;flot donnee;finite impulse response filter;iir filter;circuit vlsi;systolic scheduling;flujo datos;flow graphs;infinite impulse response filter;optimal systolic synthesis;finite impulse response iir filters fir filters delay distribution methodology optimal systolic synthesis linear recurrence algorithms systolic scheduling data flow graphs vlsi systolic implementation infinite impulse response;recurrence;algorithme;algorithm;data flow graph;computer architecture;scheduling algorithm;filtro respuesta impulsion acabada;vlsi circuit;finite impulse response;distribution retard;delay systolic arrays signal processing algorithms very large scale integration scheduling algorithm flow graphs finite impulse response filter iir filters costs computer architecture;systolic network;mathematical programming;optimal scheduling;recurrencia;fir filter;delay distribution methodology;filtre reponse impulsion infinie;reseau systolique;vlsi;vlsi systolic implementation;infinite impulse response;vlsi circuit cad graph theory systolic arrays;recurrence lineaire;circuit cad;fir filters;circuito vlsi;signal processing algorithms;data flow;linear recurrence algorithms;programmation mathematique;programacion matematica;iir filters;algoritmo	A systematic methodology based on the concept of delay distribution to optimize the systolic scheduling of data flow graphs, which represent one-dimensional linear recurrence algorithms (LRAs) is introduced. Step-by-step examples are given to illustrate the procedure. It is shown that this procedure produces optimally scheduled data flow graphs (DFGs) for VLSI systolic implementation. The goal is to transform the DFG of a linear recurrence algorithm into an optimal form for VLSI systolic implementation. Considerable improvements have been achieved over previous works. Comparisons are made between the authors' and previous designs on the examples of infinite impulse response (IIR) filters and finite impulse response (FIR) filters. Finally, a generalization of the procedure to N-dimensional linear recurrence algorithms is given, using the one-dimensional case as its basis. >	algorithm	C. Y. Roger Chen;Michael Z. Moricz	1991	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.137498	mathematical optimization;electronic engineering;real-time computing;computer science;graph theory;theoretical computer science;finite impulse response;mathematics;infinite impulse response;algorithm	EDA	0.7820165387884398	53.1669709009344	111809
34b925a111ba29f73f5c0d1b363f357958d563c1	sapphire: an always-on context-aware computer vision system for portable devices	asic;computational modeling;energy efficiency;application specific integrated circuits;data reduction;object recognition;algorithm design and analysis;ubiquitous computing;hardware acceleration;engines;mobile cpu;computer vision;image classification	Being aware of objects in the ambient provides a new dimension of context awareness. Towards this goal, we present a system that exploits powerful computer vision algorithms in the cloud by collecting data through always-on cameras on portable devices. To reduce communication-energy costs, our system allows client devices to continually analyze streams of video and distill out frames that contain objects of interest. Through a dedicated image-classification engine SAPPHIRE, we show that if an object is found in 5% of all frames, we end up selecting 30% of them to be able to detect the object 90% of the time: 70% data reduction on the client device at a cost of ≤ 60 mW of power (45 nm ASIC). By doing so, we demonstrate system-level energy reductions of ≥ 2×. Thanks to multiple levels of pipelining and parallel vector-reduction stages, SAPPHIRE consumes only 3.0 mJ/frame and 38 pJ/OP - estimated to be lower by 11.4× than a 45 nm GPU - and a slightly higher level of peak performance (29 vs. 20 GFLOPS). Further, compared to a parallelized sofware implementation on a mobile CPU, it provides a processing speed up of up to 235× (1.81 s vs. 7.7 ms/frame), which is necessary to meet the real-time processing needs of an always-on context-aware system.	algorithm;application-specific integrated circuit;central processing unit;cloud computing;computer vision;context awareness;flops;graphics processing unit;high availability;mobile device;mobile processor;parallel computing;pipeline (computing);real-time clock	Swagath Venkataramani;Paramvir Bahl;Xian-Sheng Hua;Jie Liu;Jin Li;Matthai Philipose;Bodhi Priyantha;Mohammed Shoaib	2015	2015 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;electronic engineering;real-time computing;hardware acceleration;computer hardware;computer science;operating system;cognitive neuroscience of visual object recognition;efficient energy use	Mobile	-1.060332647306095	59.21994494756472	112035
0f450ad16b7f09869bc456475a790bd876023173	opportunistic transient-fault detection	microprocessors;cmos integrated circuits;instruction reuse;redundancy degradation microprocessors proposals pipelines yarn fault detection voltage packaging power system reliability;degradation;reuse implicit redundancy;yarn;transient fault detection;issue queue;packaging;soft error rate;redundancy;pipelines;fault detection;fault tolerance;voltage;transient fault;implicit redundancy through reuse;performance coverage trade off;power system reliability;redundancy microprocessor chips fault tolerance cmos integrated circuits;ilp phase;performance degradation;performance degradation transient fault detection cmos scaling soft error rate partial explicit redundancy implicit redundancy through reuse ilp phase instruction reuse performance coverage trade off reuse implicit redundancy spec2000;cmos scaling;proposals;spec2000;microprocessor chips;partial explicit redundancy	CMOS scaling increases susceptibility of microprocessors to transient faults. Most current proposals for transient-fault detection use full redundancy to achieve perfect coverage while incurring significant performance degradation. However, most commodity systems do not need or provide perfect coverage. A recent paper explores this leniency to reduce the soft-error rate of the issue queue during L2 misses while incurring minimal performance degradation. Whereas the previous paper reduces soft-error rate without using any redundancy, we target better coverage while incurring similarly-minimal performance degradation by opportunistically using redundancy. We propose two semi-complementary techniques, called partial explicit redundancy (PER) and implicit redundancy through reuse (IRTR), to explore the trade-off between soft-error rate and performance. PER opportunistically exploits low-ILP phases and L2 misses to introduce explicit redundancy with minimal performance degradation. Because PER covers the entire pipeline and exploits not only L2 misses but all low-ILP phases, PER achieves better coverage than the previous work. To achieve coverage in high-ILP phases as well, we propose implicit redundancy through reuse (IRTR). Previous work exploits the phenomenon of instruction reuse to avoid redundant execution while falling back on redundant execution when there is no reuse. IRTR takes reuse to the extreme of performance-coverage trade-off and completely avoids explicit redundancy by exploiting reuseýs implicit redundancy within the main thread for fault detection with virtually no performance degradation. Using simulations with SPEC2000, we show that PER and IRTR achieve better trade-off between soft-error rate and performance degradation than the previous schemes.	cmos;elegant degradation;fault detection and isolation;image scaling;microprocessor;register renaming;semiconductor industry;simulation;soft error	Mohamed A. Gomaa;T. N. Vijaykumar	2005	32nd International Symposium on Computer Architecture (ISCA'05)	10.1109/ISCA.2005.38	triple modular redundancy;packaging and labeling;fault tolerance;parallel computing;real-time computing;voltage;degradation;pipeline transport;redundancy;cmos;fault detection and isolation	Arch	7.233553210945211	59.758440315744274	112168
e42ba9f1174f5b47cded17eda90587fd1f690862	segmented bitline cache: exploiting non-uniform memory access patterns	circuit decodeur;salida;instruction cache;power saving;haute performance;shared memory;storage access;memoria compartida;non uniform memory access;distributed computing;cache memory;segmentation;fuite;circuito desciframiento;design space;chip;antememoria;decoding circuit;antememoire;data cache;leakage power;comportement utilisateur;acces memoire;leak;segment droite;pattern classification;alto rendimiento;acceso memoria;calculo repartido;segmento recta;line segment;user behavior;high performance;calcul reparti;segmentacion;comportamiento usuario;memoire partagee;classification forme	On chip caches in modern processors account for a sizable fraction of the dynamic and leakage power. Much of this power is wasted, required only because the memory cells farthest from the sense amplifiers in the cache must discharge a large capacitance on the bitlines. We reduce this capacitance by segmenting the memory cells along the bitlines, and turning off the segmenters to reduce the overall bitline capacitance. The success of this cache relies on accessing segments near the sense-amps much more often than remote segments. We show that the access pattern to the first level data and instruction cache is extremely skewed. Only a small set of cache lines are accessed frequently. We exploit this non-uniform cache access pattern by mapping the frequently accessed cache lines closer to the sense amp. These lines are isolated by segmenting circuits on the bitlines and hence dissipate lesser power when accessed. Modifications to the address decoder enable a dynamic re-mapping of cache lines to segments. In this paper, we explore the design-space of segmenting the level one data and instruction caches. Instruction and data caches show potential power savings of 10% and 6% respectively on the subset of benchmarks simulated.	address decoder;benchmark (computing);cpu cache;cache (computing);central processing unit;computer cluster;context switch;discharger;memory cell (binary);mike lesser;non-uniform memory access;overhead (computing);run time (program lifecycle phase);sense amplifier;spectral leakage;uniform memory access	Ravishankar Rao;Justin Wenck;Diana Franklin;Rajeevan Amirtharajah;Venkatesh Akella	2006		10.1007/11945918_17	chip;bus sniffing;shared memory;embedded system;least frequently used;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;line segment;cache;computer science;write-once;cache invalidation;operating system;database;smart cache;segmentation;cache algorithms;cache pollution;non-uniform memory access	Arch	-2.527134218912989	54.20966814437281	112245
eb689df2692df5fd724075efae57f0c73184faa2	a latch-latch composition of metastability-based true random number generator for xilinx fpgas		Metastability of RS latches can be a source of entropy for true random number generators (TRNGs). This study presents a new composition of an RS latch using the latch functionality of storage elements of Xilinx FPGAs. Our TRNG is implemented as a soft macro, or RTL description with directives, which is easily integrated into other logic components. According to our evaluation with an Artix-7 FPGA (XC7A35T), our TRNG with 320 latches (716 LUTs and 974 registers) passed the NIST SP 800-22 test suite without post-processing. Also, our new TRNG presented a 2.3x better areadelay product than the existing design to pass the diehard test.	directive (programming);field-programmable gate array;flip-flop (electronics);hardware random number generator;institute of electronics, information and communication engineers;metastability in electronics;random number generation;reed–solomon error correction;register-transfer level;routing;software portability;test suite;video post-processing;xilinx ise	Naoki Fujieda;Shuichi Ichikawa	2018	IEICE Electronic Express	10.1587/elex.15.20180386	metastability;field-programmable gate array;electronic engineering;computer science;random number generation	EDA	10.023284705505098	48.05045057225131	112246
509511a5db59ae69c300993c2be7382e3ea7f07e	split wisely: when work partitioning is energy-optimal on heterogeneous hardware	computational modeling;current measurement;energy measurement;system on chip;graphics processing units;predictive models;power measurement	Heterogeneous System-on-Chip (SoC) processors are increasingly gaining traction in the High Performance Computing (HPC) community as alternate building blocks for future exascale systems. Key issues relating to their promise of energy efficiency include i) absolute performance, ii) finding an energy-optimal balance in the use of different on-chip devices and iii) understanding the performance-energy trade-offs while using different on-chip devices. In this paper we explore these issues through an energy usage model designed to predict the existence of an energy-optimal work partition between different processing elements on heterogeneous systems for any application. We validate our model by measuring performance and energy consumption of matrix multiplication on the NVIDIA Tegra K1 and X1 systems. An environment for monitoring and responding to energy usage is also outlined and used to perform high resolution measurements. Comparisons are drawn with conventional HPC systems housing Intel Xeon CPUs alongside NVIDIA GPUs.	atlas;algorithm;blas;benchmark (computing);central processing unit;computation;flip-flop (electronics);graphics processing unit;heterogeneous computing;library (computing);load balancing (computing);mpsoc;matrix multiplication;nc (complexity);national computational infrastructure national facility (australia);power supply;substitution model;system of measurement;system on a chip;tegra;the australian;traction teampage	Gaurav Mitra;Andrew Haigh;Anish Varghese;Luke Angove;Alistair P. Rendell	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0113	system on a chip;embedded system;parallel computing;computer hardware;computer science;operating system;distributed computing;predictive modelling;computational model	HPC	-2.5826778285637375	47.93285606019837	112294
90eb5fa01f37d254b6537ae1b51dabe4a0f218a5	guest editors' introduction: clockless vlsi systems	special issues and sections very large scale integration circuits technological innovation delay product design timing logic devices frequency manufacturing industries;technological innovation;clocks;very large scale integration;manufacturing industries;circuits;product design;frequency;logic devices;timing	Presents the guest editorial for this issue of the publication.	asynchronous circuit;very-large-scale integration	Soha Hassoun;Yong-Bin Kim;Fabrizio Lombardi	2003	IEEE Design & Test of Computers	10.1109/MDT.2003.1246158	electronic circuit;electronic engineering;engineering;electrical engineering;industrial engineering;frequency;very-large-scale integration;manufacturing;product design;computer engineering	Embedded	9.379704636607796	54.53384562969592	112315
3e14b6d00f78dd0c974d66d09377b1d5f336e262	an embedded reachability analyzer and invariant checker (eraic)	observability;invariant checker embedded reachability analyser post silicon verification test automation model checking;random access memory;invariant checker;integrated circuit;test automation;computer model;prototypes;controllability;reachability analysis controllability digital circuits field programmable gate arrays formal verification observability prototypes;context model;computational modeling;formal verification;model checking;implementation under test embedded reachability analyzer invariant checker formal verification digital circuits model checking field programmable gate array circuit prototype eraic core state expansion universal core expander wishbone compatible structure observability controllability;post silicon verification;integrated circuit modeling;field programmable gate arrays;digital circuits;context modeling;hardware integrated circuit modeling computational modeling concrete reachability analysis random access memory context modeling;reachability analysis;embedded reachability analyser;concrete;hardware	"""ERAIC (Embedded Reachability Analyzer And Invariant Checker) is an essential component in our new methodology for Formal Verification of """"Concrete'' Digital Circuits. We apply Model checking to a Field Programmable Gate Array (FPGA)-based prototype of the circuit[1]. At the core of ERAIC is the process of state expansion of the reachability analysis in Hardware. We aimed at a universal core expander with a Wishbone compatible structure. Its mechanism relies on full state controllability and observability offering more performance, flexibility, portability, and furthermore, the possibility of checking invariants on the Implementation Under Test (IUT) before submitting it to the model checker."""	field-programmable gate array;formal verification;host (network);invariant (computer science);model checking;parallel computing;prototype;reachability;simulation;software portability;vhdl;wishbone (computer bus)	Ouiza Dahmoune;Robert de B. Johnston	2010	2010 11th International Workshop on Microprocessor Test and Verification	10.1109/MTV.2010.17	computer simulation;electronic engineering;real-time computing;computer science;theoretical computer science;context model;programming language	Logic	8.128461121788352	52.58856099255801	112441
14c490c08103b341df1b0c4a2c7bb7788b65fc21	a new scheduling algorithm for synthesizing the control blocks of control-dominated circuits	transition state;optimisation;synthese circuit;optimizacion;circuit commande;automata estado finito;control dominated circuit synthesis;flot donnee;flujo datos;prise decision;algorithme;algorithm;unification;high level synthesis;finite state machines;scheduling algorithm;control data flow graph;estado transitorio;scheduling;operation chaining;flot commande;control flow;control circuit;sintesis circuito;ordonamiento;optimization;finite automaton;circuito control;automate fini;flujo control;data flow;toma decision;state transition graph;finite state machine;etat transition;ordonnancement;circuit synthesis;unificacion;algoritmo	This paper describes a new scheduling algorithm for automatic synthesis of the control blocks of control-dominated circuits. The proposed scheduling algorithm is distinctive in its approach to partition a control/data flow graph (CDFG) into an equivalent state transition graph. It works on the CDFG to exploit operation relocation, chaining, duplication, and unification. The optimization goal is to schedule each execution path as fast as possible. Benchmark data shows that this approach achieved better results over the previous ones in terms of the speedup of the circuit and the number of states and transitions.	algorithm;scheduling (computing)	Shih-Hsu Huang;Yu-Chin Hsu;Yen-Jen Oyang	1995	Microprocessing and Microprogramming	10.1016/0165-6074(95)00016-H	fair-share scheduling;parallel computing;dynamic priority scheduling;computer science;theoretical computer science;operating system;finite-state machine;programming language;scheduling;algorithm	EDA	-2.3478039168955678	52.30159473399963	112449
8b6c2f45fb2c9de3f75fbefc7fd62a42dd257cb9	microcode compaction via microblock definition	microblock definition;microistruction modelling;proposed technique;microoperation semantics;explicit description;microinstruction field;target machine;microcode compaction;microprogram compaction technique;different type;machine timing	The paper describes a microprogram compaction technique based on a microoperation and microistruction modelling, applicable to different types of target machine. The model describes microoperation semantics by relating them to microcodes used in microinstruction fields, without any explicit description of machine timing. Evaluation of the proposed technique is given in terms of efficiency of the automatically generated microcode.	data compaction;microcode	Marco Mezzalama;Paolo Prinetto;G. Filippi	1982			computer science;theoretical computer science;programming language;algorithm	EDA	7.133324601617672	51.76695134840806	112507
4ec6cb9a40055ee802a9004ea13cd911e283ef30	variable resizing for area improvement in behavioral synthesis	noncombinatorial area variable resizing area improvement behavioral synthesis high level synthesis tools algorithmic description register transfer language description c language c language hardware specification language vhdl verilog combinatorial area;behavioral synthesis;register transfer;high level synthesis;c language;high level synthesis graphics specification languages hardware design languages wires circuit synthesis optimizing compilers scheduling libraries very large scale integration;c language high level synthesis c language	High level synthesis tools transform an algorithmic description to a register transfer language (RTL) description of the hardware. The algorithm behavior is typically described in languages like C, C++ or their variants. The generated RTL is described in a hardware specification language like VHDL or Verilog. The size of the variables specified in the algorithm has a significant impact on the area of the generated hardware. The language accepted by the high level synthesis tools typically allow the size or bit width of a variable to be specified explicitly. This paper describes a method to automatically determine the minimum bit width of the variables from a performance profile. This would be effective to reduce the combinatorial and the non-combinatorial area of the generated hardware.	algorithm;c++;high-level programming language;high-level synthesis;register transfer language;specification language;vhdl;verilog	R. Gopalakrishnan;Rajat Moona	2005	18th International Conference on VLSI Design held jointly with 4th International Conference on Embedded Systems Design	10.1109/ICVD.2005.168	computer architecture;parallel computing;specification language;vhdl;computer science;hardware description language;high-level synthesis;programming language;fpgac	EDA	2.58004213119111	51.47459234887808	112697
458098980dd50a8d16f78028a81dc9a977d738ce	partitioning of hardware-software embedded systems: a metrics-based approach	complete example;software component;metrics-based approach;complete synthesis cycle;design process;software embedded system;control-dominated hardware;cad methodology;digital synthesis cad environment;cost evaluation figure;hardware part;embedded system	"""The """"pervasiveness"""" of silicon is going to enable the realization of all-on-a-chip systems for a wide range of embedded applications such as automotive, process control and telecom. The maturity of the digital synthesis CAD environments is shifting the designer's attention towards problems more related to a system-level trade-of analysis than in the past. However, common time-to-market constraints are forcing the CAD methodologies to provide quality/cost evaluation figures early in the design process, possibly in the valuable form of prediction. The goal is to dramatically reduce the number of complete synthesis cycles to achieve the optimal final product. The aim of this paper is to define some evaluation metrics to predict the characteristics of the final implementation, tailored for control-dominated hardware{software embedded systems, and to show their application in a complete example. The relevant features considered by the presented analysis cover the modularization of the system specification, the power consumption for both hardware and software, the silicon area of the hardware part and the memory size of the software components. The use of these estimation metrics within a co-design environment, called TOSCA, is also presented through an illustrative example."""		Alessandro Balboni;William Fornaciari;Donatella Sciuto	1998	Integrated Computer-Aided Engineering		real-time computing;computer science	EDA	4.225573019345523	54.78908565915174	112747
2707078f9de585329893290cb72975f1397cd14e	novel task migration framework on configurable heterogeneous mpsoc platforms	higher processor usage rate;novel task migration framework;tasks migration;motion-jpeg case;power consumption;operating system realization;specific heterogeneous mpsoc platform;heterogeneous processor;processor instruction sets;system-on-chip;multiprocessing systems;target processor;general heterogeneous mpsocs;configurable heterogeneous mpsoc platform;configurable heterogeneous mpsoc platforms;heterogeneous processors;processor instruction set;heterogeneous mpsoc architecture;task migration framework;different processor;instruction sets;digital signal processing;operating systems;operating system;computer architecture;motion jpeg;switches;system on chip;system performance;registers	Heterogeneous MPSoC architectures can provide higher performance and flexibility with less power consumption and lower cost than homogeneous ones. However, as processor instruction sets of general heterogeneous MPSoCs are not identical, tasks migration between two heterogeneous processors is not possible. To enable this function, we propose to build one specific heterogeneous MPSoC platform in which all heterogeneous processors are based on the same core instruction set for the operating system realization. Different extended instructions can be added for different processors to improve the system performance. Tasks can be migrated from one processor to another only if the target processor has all instructions which can meet the execution requirement of this task. This paper concentrates on the infrastructure that is necessary to support the scheduling and migration of tasks between the processors. By using the Motion-JPEG case study, we confirm that our task migration framework can achieve higher processor usage rate and more flexibility.	central processing unit;jpeg;mpsoc;operating system;realization (systems);scheduling (computing)	Hao Shen;Frédéric Pétrot	2009	2009 Asia and South Pacific Design Automation Conference		system on a chip;embedded system;computer architecture;parallel computing;real-time computing;network switch;computer science;operating system;digital signal processing;instruction set;computer performance;processor register;motion jpeg	EDA	-2.220816913645085	50.33341542903135	112789
ad53f777ea0a2cb09aba3d2e2e0088d39ba0570f	an effective instruction optimization method for embedded real-time java processor	optimising compilers;optimization methods java real time systems operating systems embedded system virtual machining hardware pipelines out of order computer science;instruction optimization;optimal method;virtual machining;wcet worst;embedded system;out of order;embedded systems;worst case execution time;out of order execution;wcet worst case execution time;pipelines;cycles per instruction instruction optimization embedded real time java processor;embedded systems java instruction sets optimising compilers microprocessor chips;real time java;wcet worst case execution time java processor embedded real time java processor instruction optimization;computer science;java processor;microprocessor chips;embedded real time java processor;operating systems;instruction sets;hardware;cycles per instruction;java;real time systems;optimization methods	A method to optimize instructions in embedded real-time Java processors is proposed. It can reduce the CPI (cycles per instruction) and simplify the implementation of complex instructions, as well as make the WCET (worst case execution time) of the optimized instructions more predictable. Because this method optimizes instruction itself it can be used together with other architectural optimization methods such as pipeline or out-of-order execution etc.	best, worst and average case;central processing unit;cycles per instruction;embedded system;java processor;mathematical optimization;out-of-order execution;pipeline (computing);real time java;real-time computing;run time (program lifecycle phase);worst-case execution time	ZhiLei Chai;ZhiQiang Tang;Liming Wang;Shi-liang Tu	2005	2005 International Conference on Parallel Processing Workshops (ICPPW'05)	10.1109/ICPPW.2005.21	computer architecture;parallel computing;real-time computing;computer science;out-of-order execution;operating system;instructions per cycle	EDA	-2.987419475409316	53.07483179083839	112838
b6f9ff1728eb78cd91f8c3c22e7ef7806376db64	collaborative distributed computing in the field of digital electronics testing		Computation tasks used in digital design flow for test quality evaluation can require a lot of processor and memory resources. To speed up execution and to overcome memory restrictions, a collaborative computing approach was proposed in this paper. Web-based system architecture allows seamlessly aggregate many remote computers for one application. Efficient collaboration requires credit based priority concept, issues of task partitioning, task allocation, load balancing and model security must be handled. Experimental results show feasibility of proposed solution and gain in	aggregate data;computation;computer;digital electronics;distributed computing;fault coverage;load balancing (computing);logic synthesis;systems architecture	Eero Ivask;Sergei Devadze;Raimund Ubar	2010		10.1007/978-3-642-14341-0_17	theoretical computer science;distributed computing;distributed design patterns;computer engineering	EDA	1.4692673596537198	48.40840541679102	112846
2f69337ee1075ca7d268c112ba44e2419d796b23	analyzing the advantages of run-time reconfiguration in radar signal processing	engineering and technology;teknik och teknologier;run time reconfigurable;radar signal processing	Configurable architectures have emerged as one of the most powerful programmable signal processing platforms commercially available, obtaining their performance through the use of spatial parallelism. By changing the functionality of these devices during run-time, flexible mapping of signal processing applications can be made. The run-time flexibility puts requirements on the reconfiguration time that depend both on the application and on the mapping strategy. In this paper we analyze one such application, Space Time Adaptive Processing for radar signal processing, and show three different mappings and their requirements. The allowed time for run-time reconfiguration in these three cases varies from 1 ms down to 1 μs. Each has its own advantages, such as data reuse and optimization of computational kernels. Architectures with reconfiguration times in the order of 10 μs provide the flexibility needed for mapping the example in an efficient way, allowing for on-chip data reuse between the different processing stages.	algorithm;configuration management;existential quantification;field-programmable gate array;ipflex;mathematical optimization;parallel computing;reconfigurable computing;requirement;run time (program lifecycle phase);signal processing;space-time adaptive processing	Dennis Johnsson;Anders Ahlander;Bertil Svensson	2005			embedded system;parallel computing;real-time computing;computer science;operating system	HPC	2.122230961729844	47.85019155110283	113074
3d9635b579a3a5cb4d88e19854b6f3b352aca2e3	fast design space exploration using vivado hls: non-binary ldpc decoders	decoding parity check codes field programmable gate arrays space exploration computer architecture hardware optimization;vivado;non binary ldpc codes;high level synthesis;parity check codes decoding field programmable gate arrays high level synthesis logic design low power electronics;design space exploration;non binary ldpc codes fast hardware design high level synthesis design space exploration vivado;fast hardware design;nonbinary gf low density parity check decoder fast design space exploration vivado hls tools nonbinary ldpc decoders field programmable gate arrays fpgas nonrecurring engineering cost asic designs low power budgets nre costs register transfer level languages software design flows design process high level synthesis tools rtl architecture description	Computing on field-programmable gate arrays (FPGAs) has been receiving continued interest as it provides high performance at relatively low power budgets, while avoiding the high non-recurring engineering (NRE) costs associated with ASIC designs. However, FPGA development is typically performed using register transfer level (RTL) languages which make the design process protracted and error-prone when compared to software design flows. To ease these problems, high-level synthesis (HLS) tools have been introduced which abstract away the RTL architecture description from the designer. In this work we explore the design space of a nonbinary GF (q) low-density parity-check (LDPC) decoder using Vivado HLS and compare it with state-of-the-art RTL designs.	application-specific integrated circuit;cognitive dimensions of notations;design space exploration;field-programmability;field-programmable gate array;grammatical framework;high- and low-level;high-level synthesis;low-density parity-check code;register-transfer level;right-to-left;software design	João Andrade;Nithin George;Kimon Karras;David Novo;Vítor Manuel Mendes da Silva;Paolo Ienne;Gabriel Falcão Paiva Fernandes	2015	2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2015.63	computer architecture;parallel computing;computer science;high-level synthesis	Arch	2.673869439619856	50.79858591955429	113331
6fdc3e85d0a9144591bd753b37d16b674c5fa42e	features, design tools, and application domains of fpgas	field programmable gate array;reconfigurable systems;design tool;communication system;programmable devices;reconfigurable system;intellectual property;dynamic reconfiguration;communication systems;reconfigurable architectures;rapid system prototyping;mass production;fpga;coprocessor;intellectual property protection;embedded system;embedded systems;rapid prototyping;industrial property field programmable gate arrays;dynamically reconfigurable systems;signal processing communication systems coprocessor data acquisition embedded systems intellectual property ip protection rapid prototyping reconfigurable systems;signal processing;communication processors;industrial property;configurable computing;field programmable gate arrays;field programmable gate arrays application software software prototyping prototypes hardware protocols intellectual property mass production computer interfaces signal processing;data acquisition;high speed;intellectual property protection fpga programmable devices field programmable gate arrays configurable computing dynamically reconfigurable systems rapid system prototyping communication processors signal processing data acquisition;intellectual property ip protection	In the past two decades, advances in programmable device technologies, in both the hardware and software arenas, have been extraordinary. The original application of rapid prototyping has been complemented with a large number of new applications that take advantage of the excellent characteristics of the latest devices. High speed, very large number of components, large number of supported protocols, and the addition of ready- to-use intellectual property cores make programmable devices the preferred choice of implementation and even deployment in mass production quantities. This paper surveys the advanced features, design tools, and application domains for field-programmable gate arrays (FPGAs). The main characteristics and structure of modern FPGAs are first described to show their versatility and abundance of available design resources. Software resources are also discussed, as they are the main enablers for the efficient exploitation of the design capabilities of these devices. Current application domains are described, such as configurable computing, dynamically reconfigurable systems, rapid system prototyping, communication processors and interfaces, and signal processing. This paper also presents the authors' prospective view of how FPGAs will evolve to enter new application domains in the future.	application domain;central processing unit;computation;digital signal processing;digital signal processor;embedded system;field-programmability;field-programmable gate array;general-purpose modeling;input/output;mathematical optimization;microprocessor;performance per watt;prospective search;rapid prototyping;reconfigurability;reconfigurable computing;requirement;signal processing;software deployment;system on a chip;verification and validation	Juan J. Rodríguez-Andina;María José Moure;María Dolores Valdés	2007	IEEE Transactions on Industrial Electronics	10.1109/TIE.2007.898279	embedded system;real-time computing;computer science;engineering;signal processing;intellectual property;field-programmable gate array;computer engineering	EDA	3.3143116435752873	48.339088289339315	113420
5843b4b417cc6e495a1059b68c07ca7d462e958b	flysig: dataflow oriented delay-insensitive processor for rapid prototyping of signal processing	software prototyping;design cycle flysig dataflow oriented delay insensitive processor rapid prototyping complex fpga one chip integration hw modules hw design timing behavior prototype stage reliability robust design styles delay insensitivity early timing evaluation delay insensitive implementation unified architecture configurable delay insensitive implemented processor dataflow oriented delay insensitive signal processing bit serial operators delay insensitive design style;delay prototypes signal processing field programmable gate arrays timing signal processing algorithms partitioning algorithms hardware protocols concrete;delay insensitive;chip;rapid prototyping;signal processing;data flow computing software prototyping signal processing field programmable gate arrays;data flow computing;field programmable gate arrays;high performance;robust design	As the one-chip integration of HW-modules designed by different companies becomes more and more popular reliability of a HW-design and evaluation of the timing behavior during the prototype stage are absolutely necessary. One way to guarantee reliability is the use of robust design styles, e.g., delay-insensitivity. For early timing evaluation two aspects must be considered: a) The timing needs to be proportional to technology variations and b) the implemented architecture should be identical for prototype and target. The first can be met also by delay-insensitive implementation. The latter one is the key point. A unified architecture is needed for prototyping as well as implementation. Our new approach to rapid prototyping of signal processing tasks is based on a configurable, delay-insensitive implemented processor called F LYSIG2. In essence, the FLYSIG processor can be understood as a complex FPGA where the CLBs are substituted by bit-serial operators. In this paper the general concept is detailed and first experimental results are given for demonstration of the main advantages: delay-insensitive design style, direct correspondence between prototyping and target architecture, high performance and reasonable shortening of the design cycle.	dataflow;delay insensitive circuit;emoticon;field-programmable gate array;lazy evaluation;prototype;rapid prototyping;serial communication;signal processing	Wolfram Hardt;Bernd Kleinjohann	1998		10.1109/IWRSP.1998.676682	chip;embedded system;electronic engineering;real-time computing;telecommunications;computer science;operating system;signal processing;field-programmable gate array	EDA	6.571651774093807	53.554391054100606	113518
979d4eb8223eb430c8e1897ed333a39391a37876	interface optimization for concurrent systems under timing constraints	vlsi circuit cad concurrent engineering graph theory logic cad;graph theory;timing constraints;vlsi concurrent systems timing constraints high level synthesis behavioral model control data flow graph complex system designs interprocess communication constraint graph interface matching interface cost scheduling blocking communication shared physical media serialization interface synchronization cost timing consistency logic cad;interprocess communication;constraint optimization;behavior modeling;senior members;constraint optimization timing image edge detection control system synthesis costs high level synthesis hardware graphics image enhancement senior members;behavioral model;blocking communication;timing consistency;high level synthesis;image enhancement;serialization;concurrent systems;control data flow graph;image edge detection;complex system;control system synthesis;scheduling;interface matching;vlsi;shared physical media;circuit cad;constraint graph;complex system designs;logic cad;interface cost;graphics;interface synchronization cost;concurrent engineering;hardware;timing;time constraint	| The scope of most high-level synthesis eeorts to date has been at the level of a single behavioral model represented as a control/data-ow graph. The communication between concurrently executing processes and its requirements in terms of timing and resources have largely been neglected. This restriction limits the applicability of most existing approaches for complex system designs. This paper describes a methodology for the synthesis of interfaces in concurrent systems under detailed timing constraints. We model inter-process communication using blocking and non-blocking messages. We show how the relationship between messages over time can be abstracted as a constraint graph that can be extracted and used during synthesis. We describe a novel technique called interface matching that minimizes the interface cost by scheduling each process with respect to timing information of other processes communicating with it. By scheduling the completion of operations, some blocking communication can be converted to non-blocking while ensuring the communication remains valid. To further reduce hardware costs, we describe the synthesis of interfaces on shared physical media. We show how this sharing can be increased through rescheduling and serialization of the communication. In addition to systematically reducing the interface synchronization cost, this approach permits analysis on the timing consistency of inter-process communication .	behavioral modeling;blocking (computing);complex system;concurrency (computer science);constraint graph;high- and low-level;high-level synthesis;inter-process communication;mathematical optimization;non-blocking algorithm;requirement;scheduling (computing);serialization	David Filo;David C. Ku;Claudionor José Nunes Coelho;Giovanni De Micheli	1993	IEEE Trans. VLSI Syst.	10.1109/92.238441	behavioral modeling;embedded system;electronic engineering;parallel computing;real-time computing;computer science;graph theory;operating system;distributed computing;programming language	EDA	1.785496074581636	53.60570994926725	113735
0234487f94f37e77a4abcb01ac3adcc2aa548c2f	a new way of estimating compute-boundedness and its application to dynamic voltage scaling	logical relation;virtual machine;evaluation performance;power saving;regime dynamique;performance evaluation;integrated circuit;execution time;performance estimation;evaluacion prestacion;dynamic voltage scaling;cache memory;circuito integrado;machine virtuelle;dynamic conditions;predictor;antememoria;algorithme;algorithm;predicteur;antememoire;regulacion voltaje;real world application;virtual machines;voltage regulation;low power electronics;regimen dinamico;horloge;temps execution;tiempo ejecucion;maquina virtual;electronique faible puissance;clock;regulation tension;reloj;circuit integre;algoritmo;dvs	Many recent dynamic voltage-scaling (DVS) algorithms use hardware events (such as cache misses, memory bus transactions, or instruction execution rates) as the basis for deciding how much a program region can be slowed down with acceptable performance loss. Although these approaches result in power savings, the hardware events measured are at best indirectly related to execution time and clock frequency. We propose a new metric for evaluating the performance loss caused by DVS, a metric that is logically related to clock frequency and execution time, namely the percentage drop in cycles. Further, we show that we can predict with high accuracy the execution time of a code region at any clock frequency after measuring the total number of cycles spent in that region for two clock frequencies—the maximum and the second highest clock frequency. Measurements using several real-world applications show that this “two-point” model predicts execution times with an accuracy that is greater than 95% in many cases. This result can be used to develop low-overhead DVS algorithms that are more system-aware than many of the current algorithms, which rely on measuring indirect effects.	algorithm;benchmark (computing);cpu cache;clock rate;clock signal;dynamic voltage scaling;extrapolation;ibm notes;image scaling;memory bus;microsoft research;overhead (computing);run time (program lifecycle phase);tracing (software);word lists by frequency	Vasanth Venkatachalam;Michael Franz;Christian W. Probst	2007	IJES	10.1504/IJES.2007.016030	embedded system;parallel computing;real-time computing;computer science;virtual machine;operating system	Metrics	-3.3807711415619606	54.83040821120222	113830
83af52978b4a00f73ac5fc65af1e7ac9e3b39673	a complete multi-processor system-on-chip fpga-based emulation framework	logic design;video processing;physical characteristic;embedded systems;system on chip;multi processor system on chip;interconnected system;uct;interconnection systems multiprocessor system on chip architectures fpga based emulation framework consumer embedded products process technology improvements hw sw design interactions processing cores memory systems;time to market;consumer products;field programmable gate arrays;real time application;modular architecture;system on a chip emulation statistics embedded system costs hardware design languages context modeling real time systems energy consumption time to market;system on chip consumer products embedded systems field programmable gate arrays logic design microprocessor chips;microprocessor chips	With the growing complexity in consumer embedded products and the improvements in process technology, multiprocessor system-on-chip (MPSoC) architectures have become widespread. These new systems are very complex to design as they must execute multiple complex real-time applications (e.g. video processing, or videogames), while meeting several additional design constraints (e.g. energy consumption or time-to-market). Therefore, mechanisms to efficiently explore the different possible HW-SW design interactions in complete MPSoC systems are in great need. In this paper, we present a new FPGA-based emulation framework that allows designers to rapidly explore a large range of MPSoC design alternatives at the cycle-accurate level. Our results show that the proposed framework is able to extract a number of critical statistics from processing cores, memory and interconnection systems, with a speed-up of three orders of magnitude compared to cycle-accurate MPSoC simulators	embedded system;emulator;field-programmable gate array;interaction;interconnection;mpsoc;multiprocessing;real-time clock;real-time locating system;simulation;video processing	Pablo García Del Valle;David Atienza;Ivan Magan;Javier Garcia Flores;Esther Andres Perez;Jose Manuel Mendias;Luca Benini;Giovanni De Micheli	2006	2006 IFIP International Conference on Very Large Scale Integration	10.1109/VLSISOC.2006.313218	system on a chip;embedded system;computer architecture;electronic engineering;logic synthesis;real-time computing;computer science;engineering;final good;operating system;video processing;field-programmable gate array	EDA	2.9381289416700764	53.70712706300983	114084
a2ac92676423737cb2beebdc92b7550d9ceed352	on the cusp of a validation wall	debugging;formal specification;university teaching;reference specifications postsilicon validation product specifications;product specifications;product life cycle management;postsilicon validation;logic;formal verification formal specification;program verification;satisfiability;computer architecture;computational modeling;formal verification;reference specifications;system on chip;low dpm;modular validation;validation wall;modular validation validation wall logic low dpm virtual platform;floating point;virtual platform;circuit testing computer bugs debugging logic silicon observability computer architecture energy management fasteners turning;product development	"""Traditionally, universities teach how to make or build things but not so much how to """"break"""" things or find, patch, or prevent breaks. However, much of industry validation hinges on the latter skills. Validation is something that does not get noticed when done well, but everyone notices when something goes wrong - such as the infamous Pentium floating-point division bug. Major semiconductor companies experience postsilicon validation turning into a very expensive, time-consuming proposition, yet very few college graduates are formally trained in the area. Validation is the activity of ensuring a product satisfies its reference specifications, runs with relevant software and hardware, and meets user expectations. Here, I discuss some of the key challenges to successful validation and show why a radical transformation is necessary if validation is to be effective in the near future."""	data validation;reference architecture;semiconductor	Priyadarsan Patra	2007	IEEE Design & Test of Computers	10.1109/MDT.2007.54	reliability engineering;embedded system;electronic engineering;real-time computing;simulation;formal verification;computer science;floating point;operating system;software engineering;formal specification;programming language;debugging;logic;satisfiability	Arch	10.026326671149672	54.36944936911826	114163
2febe229adc2b9d06849cd19790e290aa1ac252a	exploiting data-redundancy in reliability-aware networked embedded system design	reliability;embedded system;embedded system design;networked embedded systems;system level design;design space exploration;symbolic analysis	This paper presents a system-level design methodology for networked embedded systems that exploits existing data-redundancy to increase their reliability. The presented approach not only supports a reliability-aware embedded system design from scratch, but also enables the redesign of existing systems to increase the reliability with a minimal communication overhead. The proposed approach contributes (a) algorithms to automatically identify inherent data-redundancy and (b) an automatic design space exploration that is capable of exploiting the revealed data-redundancy. A symbolic analysis is presented that quantifies the reliability of a system, enabling the usage of reliability as one of multiple conflicting optimization objectives. The proposed approach is applied to a realworld case study from the automotive area, showing a significantly increased reliability with a negligible communication overhead.	algorithm;data redundancy;design space exploration;electronic system-level design and verification;embedded system;emoticon;exploit (computer security);level design;mathematical optimization;overhead (computing);systems design	Martin Lukasiewycz;Michael Glaß;Jürgen Teich	2009		10.1145/1629435.1629468	embedded system;real-time computing;engineering;distributed computing	EDA	5.615171770329376	57.86784643741373	114183
3fcc4d9d2d5e115e26aa3acb56f0232f89f8cf2f	improving power efficiency through fine-grain performance monitoring in hpc clusters		Nowadays, power and energy consumption are of paramount importance. Further, reaching the Exascale target will not be possible in the short term without major breakthroughs in software and hardware technologies to meet power consumption constraints. In this context, this papers discusses the design and implementation of a system-wide tool to monitor, analyze and control power/energy consumption in HPC clusters. We developed a lightweight tool that relies on a fine-grain sampling of two CPU performance metrics: instructions throughput (IPC) and last level cache bandwidth. Thanks to the information provided by these metrics about hardware resources' activity, and using DVFS to control power/performance, we show that it is possible to achieve up to 16% energy savings at the cost of less than 3% performance degradation on real HPC applications.	algorithm;benchmark (computing);cpu cache;cpu power dissipation;central processing unit;clock gating;computer cluster;concurrency (computer science);data structure;distributed computing;dynamic voltage scaling;elegant degradation;existential quantification;hpcg benchmark;mathematical optimization;matrix multiplication;performance per watt;power management;random-access memory;requirement;sampling (signal processing);sparse matrix;stream (computing);supercomputer;system administrator;throughput;unbalanced circuit;user requirements document	Mathieu Stoffel;Abdelhafid Mazouz	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00071	computer science;parallel computing;throughput;real-time computing;cache;computer performance;energy consumption;software;electrical efficiency;bandwidth (signal processing)	HPC	-4.4244077709616	54.46654998774437	114188
1a38a2263c051430d39aa1582cd8f00d9a8c307b	configuration-sensitive process scheduling for fpga-based computing platforms	field programmable gate arrays;operating systems (computers);processor scheduling;reconfigurable architectures;fpga configuration;fpga-based computing platforms;os support;architectural aspects;compilation-programming aspects;computer architecture;field programmable gate arrats;first-come-first-serve;operating system;process scheduling;reconfigurable computing;reconfigurable hardware;scheduling algorithms;shortest job first;software systems	Reconfigurable computing has become an important part of research in software systems and computer architecture. While prior research on reconfigurable computing have addressed architectural and compilation/programming aspects to some extent, there is still not much consensus on what kind of operating system (OS) support should be provided. In this paper, we focus on OS process scheduler, and demonstrate how it can be customized considering the needs of reconfigurable hardware. Our process scheduler is configuration sensitive, that is, it reuses the current FPGA configuration as much as possible. Our extensive experimental results show that the proposed scheduler is superior to classical scheduling algorithms such First-Come-First-Serve (FCFS) and Shortest Job First (SJF).	algorithm;compiler;computer architecture;operating system;reconfigurable computing;scheduling (computing);shortest job next;software system	Guilin Chen;Mahmut T. Kandemir;Ugur Sezer	2004	Proceedings Design, Automation and Test in Europe Conference and Exhibition		embedded system;computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;scheduling	EDA	-2.297690189863419	50.49240000751016	114276
cdfeb1e0280fa550572c3ad90f28ac13975c37e4	multilevel mpsoc performance evaluation, iss model with timing and priority management		To deploy the enormous hardware resources available in Multi-Processor Systems-on-Chip (MPSoC) efficiently, rapidly and accurately, methods of Design Space Exploration (DSE) are needed to evaluate several design choices. In this article, we provide a framework that makes fast simulation and performance evaluation of MPSoC early in flow of design, therefore reducing the time of design. In this platform and within the Transaction Level Modeling (TLM) approach, we present a new definition of ISS level by introducing two complementary modeling ISST and ISSPT sublevels. This later, that we illustrate an arbiter modeling approach that allows a high performance MPSoC communication. A round-robin method is chosen for algorithm arbiter modeling because it is simple, minimizes the communication latency and has an accepted speed-up. Two applications are tested and used to validate our platform: Game of life and JPEG Encoder.	mpsoc;performance evaluation	Abdelhakim Alali;Ismail Assayad;Mohamed Sadik	2015		10.1007/978-981-287-990-5_34	parallel computing;real-time computing;simulation	Metrics	0.1943848089239696	56.089661278647164	114302
1a6e384c27c7d469dc3f3d73b3d807fba36fd8ec	efficient high-level modeling in the networking domain	data flow computing;multiprocessing systems;ibm system z mainframes;actor-oriented modeling;concurrent processing;dynamic dataflow models;electronic system level design flows;high-level modeling;modeling complexity;network controller design;parallel sysplex architecture;real-world system designs	Starting Electronic System Level (ESL) design flows with executable High-Level Models (HLMs) has the potential to sustainably improve productivity. However, writing good HLMs for complex systems is still a challenging task. In the context of network controller design, modeling complexity has two major sources: (1) the functionality to handle a single connection, and (2) the number of connections to be handled in parallel. In this paper, we will propose an efficient actor-oriented modeling approach for complex systems by (1) integrating hierarchical FSMs into dynamic dataflow models, and (2) providing new channel types to allow concurrent processing of multiple connections. We will show the applicability of our proposed modeling approach to real-world system designs by presenting results from modeling and simulating a network controller for the Parallel Sysplex architecture used in IBM System z mainframes.	complex systems;computer multitasking;dataflow;electronic system-level design and verification;executable;ibm parallel sysplex;ibm system z;mainframe computer;network interface controller;simulation;world-system	Christian Zebelein;Joachim Falk;Christian Haubelt;Jürgen Teich;Rainer Dorsch	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)		embedded system;programming;computer architecture;productivity;parallel computing;real-time computing;computer science;design flow;software design;operating system;modeling and simulation;component;semantics;context model;programming language;computational model;writing;multiplexing;systems design	EDA	-1.5814626654415052	48.091882225726025	114397
156200979261c754b28aa980844fa0542ab4b953	a cost-efficient self-checking register architecture for radiation hardened designs	registers delays computer architecture error correction codes redundancy tunneling magnetoresistance;radiation hardening electronics cmos logic circuits flip flops;qa75 electronic computers computer science;delay overhead cost efficient self checking register architecture radiation hardened design cmos technology electronic system susceptibility radiation induced soft errors error tolerant technique soft error mitigation redundant circuitry self checking soft error tolerant register settoff soft error timing error tolerant flip flop set seu power consumption overhead;single event transient soft error self checking reliability single event upset	The rapid development of CMOS technology has significantly increased the susceptibility of electronic systems to radiation-induced soft errors. Conventional error-tolerant techniques typically use redundancies to mitigate soft errors and increase system immunity. However they do not have self-checking capabilities, and therefore are still vulnerable to the errors in the redundant circuitry added for error-tolerance. This paper proposes a novel self-checking soft error-tolerant register based on SETTOFF, a Soft Error and Timing error Tolerant Flip-Flop. The register significantly improves the error-tolerant capability over previous techniques since it has a self-checking capability, which allows the register to tolerate both the errors in the original flip-flops and the redundant circuitry. In addition, the register can also tolerate both soft errors (SETs and SEUs) and timing errors. Compared with other previous techniques such as TMR, the proposed register reduces the power consumption overhead by 81%, and the delay overhead by 54% in 65nm technology; The area overhead is also reduced by 25%.	cmos;cost efficiency;electronic circuit;error-tolerant design;flops;flip-flop (electronics);mbus (sparc);overhead (computing);processor register;radiation hardening;soft error;triple modular redundancy	Yang Lin;Mark Zwolinski	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865087	embedded system;electronic engineering;real-time computing;soft error;engineering	Arch	8.447625245733521	60.0327313896336	114587
ee069871b9a7dd398585e2c24fdbfa303fed7df3	a multilevel computing architecture for embedded multimedia applications	embedded multimedia applications multilevel computing architecture template architecture soc system superscalar techniques synchronizing task;multimedia application;computer architecture multimedia systems embedded systems system on chip multiprocessing systems;multimedia systems;programming model;computer architecture;embedded systems;system on chip;embedded computing computer architecture multimedia computing registers streaming media concurrent computing communication system control parallel processing programming profession digital signal processing;multiprocessing systems	We have designed a new architecture that simplifies integration of heterogeneous IP for multimedia and streaming applications. The multilevel computing architecture (MLCA) is a template architecture featuring multiple processing units. This template architecture for SOC systems uses superscalar techniques to exploit task-level parallelism among different processing units. It supports a natural programming model that relieves programmers from explicitly synchronizing tasks and communicating data. code transformations that improve application performance are easy to incorporate in compilers for this architecture.	compiler;computer architecture;embedded system;multiprocessing;parallel computing;programmer;programming model;superscalar processor;task parallelism	Faraydon Karim;Alain Mellan;Anh Tuan Le Nguyen;Utku Aydonat;Tarek S. Abdelrahman	2004	IEEE Micro	10.1109/MM.2004.1	dataflow architecture;enterprise architecture framework;system on a chip;reference architecture;space-based architecture;computer architecture;parallel computing;real-time computing;database-centric architecture;computer science;applications architecture;cellular architecture;operating system;service-oriented modeling;programming paradigm;data architecture;systems architecture;systems design	Arch	-1.794789585591791	49.37450583588686	114613
776c0583ef15503affecc506a0fc047eed3e8c98	scalable, adaptive, time-bounded node failure detection	error detection and correction;single bit flips;field programmable gate array;triple modular redundant;computer engineering;radiation effects;sram based fpgas;fault tolerant;partial reconfiguration;sram chips field programmable gate arrays parity check codes radiation effects;parity check codes;high energy neutrons;high energy;parity checking;consumer electronics;event detection;triple modular redundancy systems single event detection single event reconfiguration sram based fpgas field programmable gate arrays single bit flips high energy neutrons alpha particles cosmic radiation single event upset seu detection parity checking;charged particles;cosmic radiation;xilinx virtex;thesis;triple modular redundancy systems;seu detection;single event reconfiguration;normal operator;single event detection;single event upset;field programmable gate arrays;high performance;field programmable gate arrays single event upset consumer electronics space vehicles costs neutrons alpha particles random access memory packaging fault tolerance;sram chips;alpha particles	"""This paper presents a scalable, adaptive and time-bounded general approach to assure reliable, real-time node-failure detection (NFD) for large-scale, high load networks comprised of commercial off-the-shelf (COTS) hardware and software. Nodes in the network are independent processors which may unpredictably fail either temporarily or permanently. We present a generalizable, multilayer, dynamically adaptive monitoring approach to NFD where a small, designated subset of the nodes are communicated information about node failures. This subset of nodes are notified of node failures in the network within an interval of time after the failures. Except under conditions of massive system failure, the NFD system has a zero false negative rate (failures are always detected with in a finite amount of time after failure) by design. The NFD system continually adjusts to decrease the false alarm rate as false alarms are detected. The NFD design utilizes nodes that transmit, within a given locality, """"heartbeat"""" messages to indicate that the node is still alive. We intend for the NFD system to be deployed on nodes using commodity (i.e. not hard-real-time) operating systems that do not provide strict guarantees on the scheduling of the NFD processes. We show through experimental deployments of the design, the variations in the scheduling of heartbeat messages can cause large variations in the false-positive notification behavior of the NFD subsystem. We present a per-node adaptive enhancement of the NFD subsystem that dynamically adapts to provide run-time assurance of low false-alarm rates with respect to past observations of heartbeat scheduling variations while providing finite node-failure detection delays. We show through experimentation that this NFD subsystem is highly scalable and uses low resource overhead."""	central processing unit;heartbeat message;locality of reference;operating system;overhead (computing);real-time clock;scalability;scheduling (computing)	Matthew Gillen;Kurt Rohloff;Prakash Manghwani;Richard E. Schantz	2007	10th IEEE High Assurance Systems Engineering Symposium (HASE'07)	10.1109/HASE.2007.9	embedded system;electronic engineering;real-time computing;computer science;engineering;field-programmable gate array	Arch	4.902755586688467	59.46749616763905	114619
a18228315cb43ce530e920bcb9979d4a4e106e68	adaptive balanced computing (abc) microprocessor using reconfigurable functional caches (rfcs)	microprocessors;cache storage;dynamic resource configuration adaptive balanced computing on chip cache memory cache memory reconfigurable functional caches cache organizations microarchitecture;digital signal processing;microarchitecture;on chip cache memory;application software;reconfigurable architectures;reconfigurable logic;cache memory;software performance;chip;multimedia computing;general purpose processor;adaptive balanced computing;microprocessors cache memory reconfigurable logic multimedia computing application software microarchitecture power engineering computing digital signal processing streaming media software performance;power engineering computing;dynamic resource configuration;streaming media;memory architecture;cache organizations;memory architecture cache storage reconfigurable architectures;reconfigurable functional caches	A general-purpose computing processor performs a wide range of functions. Although the performance of general-purpose processors has been steadily increasing, certain software technologies like multimedia and digital signal processing applications demand ever more computing power. If the computing resources are variable to the needs of an application, a better performance can be achieved. Adaptive Balanced Computing (ABC) performs a dynamic resource configuration of on-chip cache memory by converting the cache into a specialized computing unit. With a small amount of additional logic and slightly modified microarchitecture, a part of the cache memory can be configured to perform specialized computations in a conventional processor. In this paper, we evaluate the ABC using RFCs in various cache organizations to see the impact of resource reconfiguration. The simulations with multimedia and DSP applications show that the resource configuration speedups ranging from 1.04X to 3.94X in overall applications and from 2.61X to 27.4X in the core computations.		Huesung Kim;Arun K. Somani;Akhilesh Tyagi	2002		10.1109/ICCD.2002.1106761	chip;pipeline burst cache;computer architecture;application software;parallel computing;real-time computing;cache coloring;cpu cache;software performance testing;microarchitecture;cache;computer science;operating system;digital signal processing;cache algorithms	EDA	0.23879313289330562	49.65907765933607	114791
12e7a7718129cbf4e396425eb779c745c1c76852	use of c/c++ models for architecture exploration and verification of dsps	verification;digital signal processing algorithm design and analysis signal processing algorithms instruments image processing documentation pipelines signal processing logic design signal design;hardware design languages;digital signal processors;digital signal processing;image processing;architecture exploration;formal;logic design;logic design c language digital signal processing chips hardware description languages integrated circuit design;simulation;hardware description languages;c c;dsp verification;high level c models;integrated circuit design;c language;c c models;digital signal processing c c models architecture exploration dsp verification high level c models register transfer level;algorithms;design;digital signal processing chips;rtl algorithms documentation design languages verification verification c c simulation formal;register transfer level;rtl;languages;formal language;documentation	"""Architectural decisions for DSP modules are often analyzed using high level C models. Such high-level explorations allow early examination of the algorithms and the architectural trade-offs that must be made for a practical implementation. The same models can be reused during the verification of the RTL subsequently developed, provided that various """"hooks"""" which are desirable during the verification process are considered while creating these high level models. In addition, consideration must be given to the qualitative content of these high level models to permit an optimal verification flow allowing for compromise between features of the model and the completeness of the verification. Thus, high quality design and verification are achieved by the use of valid models and the valid use of models. In this paper, we describe our approach and show examples from a typical image processing application."""	algorithm;c++;co-simulation;digital signal processor;display resolution;formal methods;formal verification;high- and low-level;high-level programming language;image processing;imperative programming;requirement;simulation;systemc;test case	David Brier;Raj S. Mitra	2006	2006 43rd ACM/IEEE Design Automation Conference	10.1145/1146909.1146935	embedded system;design;computer architecture;verification and validation of computer simulation models;formal language;parallel computing;verification;image processing;software verification;documentation;computer science;theoretical computer science;digital signal processing;high-level verification;runtime verification;programming language;intelligent verification;algorithm;functional verification;integrated circuit design	EDA	4.613696265382413	52.85557434542051	114808
25b16ea63fda7d00121c575528408b5789fb801e	a novel cgra architecture and mapping algorithm for application acceleration		Coarse grained reconfigurable array (CGRA) is an architecture which offers hardware like high performance and software like flexibility. The two characteristics make CGRA an effective candidate for computational intensive applications. In this paper, we propose a novel cluster base CGRA architecture which achieves high efficiency of CGRA. The reconfigurable processing elements in CGRA clusters share complex function units and registers. Area is reduced due to the resource sharing and the performance is improved. Besides, an ant colony based mapping algorithm is proposed. Experiments show that the cluster base CGRA outperforms some existing architectures in the efficiency; the proposed mapping algorithm also outperforms other mapping heuristics.	algorithm	Li Zhou;Hengzhu Liu;Dongpei Liu	2013		10.1007/978-3-642-41635-4_23	software;architecture;shared resource;heuristics;algorithm;computer science	Robotics	-0.310809412394888	50.42265239870696	114896
092e328fe24b264d569535f41e582d00bc3763b7	rapid design space exploration of heterogeneous embedded systems using symbolic search and multi-granular simulation	modelizacion;eficacia sistema;evaluation performance;system core;diagrama binaria decision;architecture systeme;diagramme binaire decision;symbolic search;calculateur embarque;performance evaluation;intellectual property;concepcion sistema;performance estimation;evaluacion prestacion;performance systeme;nucleo sistema;systeme recherche;constraint satisfaction;noyau systeme;system performance;gran sistema;design space;ordered binary decision diagram;embedded system;simulator;search system;modelisation;satisfaction contrainte;tension electrique;energy performance;simulador;energy optimization;sistema investigacion;large system;system design;multi granular simulation;model integrated computing;model integration;voltage;propiedad intelectual;boarded computer;simulateur;arquitectura sistema;design space exploration;satisfaccion restriccion;hierarchical design;search model;system architecture;voltaje;modeling;propriete intellectuelle;calculador embarque;conception systeme;grand systeme;binary decision diagram	In addition to integrating different Intellectual Property cores, heterogeneous embedded systems provide several architecture knobs such as voltage, operating frequency, configuration, etc. that can be varied to optimize performance. Such flexibilities results in a large design space making system optimization a very challenging task. Moreover, such systems operate in mobile and other power constrained environments. Therefore, in addition to rapid exploration of a large design space a designer has to optimize both time and energy performance. To address these issues, we propose a hierarchical design space exploration methodology. Our methodology initially uses symbolic constraint satisfaction to rapidly prune the design space. This pruning process is followed by a system wide performance estimation to further reduce the number of candidate designs. Finally, detailed simulation using low-level simulators are performed to select an appropriate design. Our methodology is implemented by integrating two tools, DESERT and HiPerE, into the M model based Integrated simuLAtioN (MILAN) framework. DESERT uses Ordered Binary Decision Diagrams based symbolic search to rapidly explore a large design space and identifies candidate designs that meet the user specified performance constraints. HiPerE provides rapid estimation of system wide energy and latency based on component level simulations and also facilitates energy optimization. MILAN provides the required modeling support for these tools and also facilitates component specific multi-granular simulations through seamless integration of various simulators.	binary decision diagram;clock rate;constraint satisfaction;design space exploration;embedded system;high- and low-level;low-pass filter;mathematical optimization;program optimization;seamless3d;simulation	Sumit Mohanty;Viktor K. Prasanna;Sandeep Neema;James R. Davis	2002		10.1145/513829.513835	voltage;simulation;computer science;artificial intelligence;computer performance;systems architecture	EDA	2.4723975261115023	55.38656418251488	114951
1bd4a64296cd5f4b98441e1984606fa94d23064c	interface and cache power exploration for core-based embedded system design	search space;satisfiability;embedded system;performance metric;chip;memory access;simulation experiment;data cache;low power;embedded system design;energy consumption;search space pruning;memory;time constraint	Minimizing power consumption is of paramount importance during the design of embedded (mobile computing) systems that come as systems-on-a-chip, since interdependencies of design characteristics like power, performance, and area for various system parts (cores) become increasingly influential. In this scenario, interfaces play a key role since they allow one to control/exploit these interdependencies with the aim to meet design constraints like power. In this paper, we present the first comprehensive approach to explore this impact. We consider a whole system comprising a CPU, caches, a main memory and interfaces between those cores and demonstrate the high impact that an adequate adaptation between core parameters and interface parameters in terms of power consumption has. We especially found that cache parameters and bus configurations of cache buses have a significant impact in this respect. In addition, we made the important observation that optimizing for performance no longer implies that power is optimized as well in deep submicron technologies. Instead, we found out that especially for newer technologies, the relative interface power contribution increases, leading to scenarios where we obtain a real power/performance tradeoff. In summary, our explorations unveiled not yet investigated interdependencies that represent the first step towards future efforts to optimize/adapt interfaces and caches in core-based systems for low power designs.	cpu cache;central processing unit;computer data storage;embedded system;interdependence;mobile computing;system on a chip;systems design;very-large-scale integration	Wen-Tsong Shiue;Sathishkumar Udayanarayanan;Chaitali Chakrabarti	1999	1999 IEEE/ACM International Conference on Computer-Aided Design. Digest of Technical Papers (Cat. No.99CH37051)	10.1145/502175.502182	chip;uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;memory refresh;telecommunications;computer science;theoretical computer science;overlay;extended memory;flat memory model;memory;cache pollution;cache-only memory architecture;memory map;non-uniform memory access;satisfiability;memory management	EDA	-1.6981630312761384	55.877184641486586	115015
5d8a7105296223bcbb654afd47aabd8f88f7d556	explicitly isolating data and computation in high level synthesis: the role of polyhedral framework	art;parallel processing field programmable gate arrays high level synthesis;data mining;arrays;high level synthesis;computational modeling;input programs explicit data isolation explicit computation high level synthesis polyhedral framework computational power large scale computing system core heterogeneous computational elements field programmable gate array fpga based heterogeneous systems power efficiency principal factors polyhedral analysis pa data level parallelism extraction sequential code hls;field programmable gate arrays;arrays hardware computational modeling high level synthesis field programmable gate arrays data mining art;hardware	The increased computational power required by modern large-scale computing system is pushing the adoption of heterogeneous components into mainstream. While Graphic Processing Units (GPUs) are frequently adopted as core heterogeneous computational elements, Field Programmable Gate Array (FPGA) based heterogeneous systems are being investigated and adopted due to their claimed superiority in power efficiency. However, the lack of proper approaches and methodologies to systematically push the performance of such devices are among the principal factors limiting the adoption of these devices into mainstream. In this paper, we investigate the adoption of Polyhedral Analysis (PA) to extract data level parallelism from sequential code, defining a methodology for High Level Synthesis (HLS) aimed at FPGA based system. We show how our approach systematically produces speedups proportional to the amount of data level parallelism available in the input programs.	computation;data parallelism;field-programmable gate array;graphics processing unit;high-level programming language;high-level synthesis;level design;parallel computing;performance per watt;polyhedral;polytope model	Riccardo Cattaneo;Gabriele Pallotta;Donatella Sciuto;Marco D. Santambrogio	2015	2015 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2015.7393304	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;high-level synthesis;computational model;field-programmable gate array	EDA	-1.585584165878801	47.28248752714219	115127
646db23c0a09d01f1a97d89de69eb43784f2cac5	exploring the diversity of multimedia systems	instruction level parallel;programmable circuit;microprocessors;concepcion asistida;computer aided design;assp design multimedia system application specific programmable processor instruction level parallelism cad chip area hardware optimization algorithm system level synthesis hewlett packard pa risc architecture model;optimisation;concepcion circuito;procesador risc;chip area;memory management;optimizacion;integrated circuit;circuit programmable;systeme multimedia;cad;circuit design;reduced instruction set computing;assp design;circuito integrado;multimedia systems microprocessors hardware production parallel processing marketing and sales digital signal processing chips memory management semiconductor device manufacture process design;hardware optimization algorithm;hewlett packard pa risc architecture model;indexing terms;application specific programmable processor;design space;multimedia systems;system level synthesis;circuit a la demande;process design;chip;integrated circuit design;custom circuit;circuito programable;circuito integrato personalizado;application specific integrated circuits;programmable circuits;risc processor;semiconductor device manufacture;hewlett packard;conception assistee;calidad produccion;production;digital signal processing chips;optimization;conception circuit;integrated circuit design microprocessor chips programmable circuits multimedia systems reduced instruction set computing application specific integrated circuits;functional unit;processeur risc;product quality;instruction level parallelism;optimal algorithm;qualite production;production quality;parallel processing;circuit integre;multimedia system;microprocessor chips;hardware;marketing and sales	We evaluate the validity of the fundamental assumption behind application-specific programmable processors: that applications differ from each other in key parameters which are exploitable, such as the available instruction-level parallelism (ILP), demand on various hardware resources, and the desired mix of function units. Following the tradition of the CAD community, we develop an accurate chip area estimate and a set of aggressive hardware optimization algorithms. We follow the tradition of the architecture community by using comprehensive real-life benchmarks and production quality tools. This combination enables us to build a unique framework for system-level synthesis and to gain valuable insights about design and use of application-specific programmable processors for modern applications. We explore the application-specific programmable processor (ASSP) design space to understand the relationship between performance and area. The architecture model we used is the Hewlett Packard PA-RISC [1] with single level caches. The system, including all memory and bus latencies, is simulated and no other specialized ALU or memory structures are being used. The experimental results reveal a number of important characteristics of the ASSP design space. For example, we found that in most cases a single programmable architecture performs similarly to a set of architectures that are tuned to individual application. A notable exception is highly cost sensitive designs, which we observe need a small number of specialized architectures that require smaller areas. Also, it is clear that there is enough parallelism in the typical media and communication applications to justify use of high number of function units. We found that the framework introduced in this paper can be very valuable in making early design decisions such as area and architectural configuration tradeoff, cache and issue width tradeoff under area constraint, and the number of branch units and issue width.	algorithm;anti-spam smtp proxy;arithmetic logic unit;benchmark (computing);central processing unit;instruction-level parallelism;mathematical optimization;pa-risc;parallel computing;real life;reduced cost	Johnson Kin;Chunho Lee;William H. Mangione-Smith;Miodrag Potkonjak	2001	IEEE Trans. VLSI Syst.	10.1109/92.929581	embedded system;parallel processing;reduced instruction set computing;computer architecture;electronic engineering;real-time computing;computer science;electrical engineering;operating system;computer aided design;programming language	EDA	1.5759543092673802	51.50811729167982	115159
3896efcf3ff24ae89dc7bfed04200c666d1bed9b	form: a frame-oriented representation method for digital telecommunication system design	digital signal processing;protocols;design automation;synchronous digital hierarchy;multimedia;sdh atm interface form frame oriented representation method digital telecommunication system design timing design function design system level behavioral level register transfer level;design automation timing design methodology b isdn costs digital signal processing process design system level design protocols hardware;cad;frame oriented representation method;specification;form;sdh atm interface;behavioral level;system level;timing design;process design;synchronisation synchronous digital hierarchy asynchronous transfer mode frame based representation digital communication cad;synchronisation;function design;digital communication;design method;system design;telecommunication;system level design;b isdn;digital telecommunication system design;frame;frame based representation;atm;high level design;register transfer level;asynchronous transfer mode;hardware;design methodology;timing	This paper proposes a new design method called FORM (Frame-Oriented Representation Method) for digital telecommunication systems with the aiming of efficient system design and development. FORM has the unique feature wherein timing design and function design are performed independently. FORM can translate system-level specifications at the behavioral level into RTL. This method is applied to the SDH/ATM interface and it is proved that FORM relaxes design complications and simplifies system design.	atm turbo;form;software design;synchronous optical networking;systems design	Kazuhiro Shirakawa;Kazushige Higuchi;Toshiaki Miyazaki;Kazuhiro Hayashi;Kazuhisa Yamada	1996		10.1109/EDTC.1996.494349	embedded system;electronic engineering;real-time computing;computer science	EDA	5.165011306434738	52.70048269552708	115336
01c326c4960be304d3f55ba74e98b4efe832a0c3	power estimation methodology for a high-level synthesis framework	analytical models;design model;catl design;logic simulation;power analysis;vectorless high level synthesis power estimation register transfer level system level;probability;power estimation;system level simulation;prototype processor model;prototypes;design flow;filters;data mining;system level;high level synthesis;integrated circuit design;accuracy;vespa processor;estimation;cycle accurate transaction level;application specific integrated circuits;control system synthesis;eda tool;asic design flow;transmitters;system level design;mathematical model;hardware design;probabilistic logic;power system modeling;probabilistic power estimation;high level synthesis hardware application specific integrated circuits power system modeling control system synthesis system level design data mining prototypes transmitters filters;probability application specific integrated circuits computer interfaces high level synthesis integrated circuit design logic simulation;computer interfaces;eda tool high level synthesis system level hardware design register transfer level probabilistic power estimation system level simulation power analysis asic design flow cycle accurate transaction level catl design prototype processor model vespa processor universal asynchronous receiver and transmitter;register transfer level;system level hardware design;vectorless;transaction level;hardware;universal asynchronous receiver and transmitter	As adoption of system-level hardware design is increasing in industry and academia, accurate power estimation at this level is becoming important. In this paper, we present a system-level power estimation methodology, which is based on a high-level synthesis framework and supports sufficiently accurate power estimation of hardware designs at the systemlevel. For early and accurate power estimation, the proposed methodology utilizes register transfer level (RTL) probabilistic power estimation technique controlled by the system-level simulation. Furthermore, our methodology does not require a designer to move to the traditional RTL power estimation methodology, thus facilitating easy and early power analysis and aiding the cause of adoption of system-level design practices in ASIC design flow. This paper provides detailed description of our methodology including tools used, algorithm for extracting activity from system-level value change dump and finally mapping this information for RTL power estimation. We show the usefulness of our approach by performing power estimation on synthesizable cycle-accurate transaction-level (CATL) design models of reasonable complexity such as prototype processor model (VeSPA processor), universal asynchronous receiver and transmitter (UART), FFT filter, etc. We demonstrate our methodology through industry standard EDA tools used in the ASIC design flow and show that the loss in accuracy for the proposed approach with respect to the state-of-the-art RTL power estimation techniques ranges from 3¿9%. The speed up gained using our approach is upto 12 times more than RTL simulation based power estimation approach.	algorithm;application-specific integrated circuit;complex systems;design flow (eda);electronic system-level design and verification;fits;fast fourier transform;high- and low-level;high-level synthesis;level design;plug and play;prototype;register-transfer level;seamless3d;system on a chip;system-level simulation;technical standard;transmitter	Sumit Ahuja;Deepak Mathaikutty;Gaurav Singh;Joe Stetzer;Sandeep K. Shukla;Ajit Dingankar	2009	2009 10th International Symposium on Quality Electronic Design	10.1109/ISQED.2009.4810352	embedded system;estimation;transmitter;computer architecture;electronic engineering;real-time computing;power analysis;computer science;design flow;operating system;logic simulation;probability;mathematical model;accuracy and precision;prototype;application-specific integrated circuit;probabilistic logic;electronic system-level design and verification;high-level synthesis;register-transfer level;statistics;integrated circuit design	EDA	3.753525546875737	55.34390585426148	115486
594457deeb41958f6c20268e3b2be95c7f9d7c7e	efficient mapping of nondeterministic automata to fpga for fast regular expression matching	look up table;automata field programmable gate arrays intrusion detection pattern matching hardware algorithm design and analysis acceleration viruses medical logic flip flops;nfa split architecture;deterministic automata;memory management;computer network security;decoding;network attacks fpga fast regular expression matching intrusion detection systems malicious attacks pattern matching nfa split architecture deterministic automaton nondeterministic automaton transition table look up tables flip flops sparse transition table viruses;viruses medical;logic;flip flops;intrusion detection;fpga;look up tables;pattern matching computer network security deterministic automata field programmable gate arrays;acceleration;viruses;automata;nondeterministic automaton transition table;network attacks;deterministic automaton;sparse transition table;pattern matching;intrusion detection systems;field programmable gate arrays;table lookup;malicious attacks;regular expression;doped fiber amplifiers;algorithm design and analysis;flip flop;intrusion detection system;hardware;fast regular expression matching	With the growing number of viruses and network attacks, Intrusion Detection Systems have to match a large set of regular expressions at multi-gigabit speed to detect malicious activities on the network. Many algorithms and architectures have been designed to accelerate pattern matching, but most of them can be used only for strings or a small set of regular expressions. We propose new NFA-Split architecture, which reduces the amount of consumed FPGA resources in order to match larger set of regular expressions at multi-gigabit speed. The proposed reduction uses model of nondeterministic and deterministic automaton for effective mapping of regular expressions to FPGA. A new algorithm is designed to split the nondeterministic automaton transition table in order to map a part of the table into memory. The algorithm can place more than 49% of transition table to memory, which reduces the amount of look-up tables by more than 43% and flip-flops by more than 38% for all selected sets of regular expressions. Moreover, a sparse transition table is mapped to memory with overlapped rows, which enables to store the table in a highly compact form.	automata theory;deterministic automaton;flops;field-programmable gate array;flip-flop (electronics);gigabit;lookup table;nondeterministic algorithm;nondeterministic finite automaton;pattern matching;regular expression;sparse matrix;state transition table	Jan Korenek;Vlastimil Kosar	2010	13th IEEE Symposium on Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2010.5491816	intrusion detection system;embedded system;nondeterministic finite automaton with ε-moves;electronic engineering;parallel computing;computer science;theoretical computer science;network security;operating system;generalized nondeterministic finite automaton;algorithm	Arch	8.8355757293255	47.098595323722584	115659
3a8487e4969c48d2921c11f342680bdf299d7d6a	an algorithm for hardware/software partitioning using mixed integer linear programming	mixed integer linear program;hardware software codesign;embedded system;objective function;embedded systems;mixed integer linear programming;hardware software partitioning;cost estimation;integer program	One of the key problems in hardware/software codesign is hardware/software partitioning. This paper describes a new approach to hardware/software partitioning using integer programming (IP). The advantage of using IP is that optimal results are calculated for a chosen objective function. The partitioning approachworks fully automatic and supports multi-processor systems, interfacing and hardware sharing. In contrast to other approaches where special estimators are used, we use compilation and synthesis tools for cost estimation. The increased time for calculating values for the cost metrics is compensated by an improved quality of the values. Therefore, fewer iteration steps for partitioningare needed. The paper presents an algorithmusing integer programming for solving the hardware/software partitioning problem leading to promising results.	algorithm;cool;compiler;integer programming;iteration;linear programming;loss function;multiprocessing;open shading language;optimization problem;partition problem;subroutine	Ralf Niemann;Peter Marwedel	1997	Design Autom. for Emb. Sys.	10.1023/A:1008832202436	embedded system;mathematical optimization;real-time computing;integer programming;computer science;branch and price;theoretical computer science;cost estimate	EDA	0.10282079235949744	52.73408554190241	115776
0d2b97c0daab710edec5c03e3c7fe753c7810cd1	reliability improvement of hardware task graphs via configuration early fetch	computers;circuit faults;failure analysis fault tolerance field programmable gate arrays flip flops graph theory random access storage table lookup;runtime;redundancy;early fetch fault tolerance ft field programmable gate arrays fpgas reliability scheduling task graph tg;field programmable gate arrays;hardware field programmable gate arrays computers circuit faults runtime redundancy;reliability improvement hardware task graph configuration early fetch mean time to failure mttf hardware tg reconfigurable computer task early fetch redundancy level no fault tolerant tg fault tolerant tg flip flops look up tables block random access memory xilinx virtex ultrascale xcvu095 2ffva2104e field programmable gate array runtime computation;hardware	This paper presents a technique to improve the reliability and the mean time to failure (MTTF) of hardware task graphs (TGs) running on reconfigurable computers. This technique, which has been named task early fetch, can be applied to a sequence of one or several applications, represented as TGs. It consists in carrying out the reconfiguration of some tasks within the execution of the previous TG, plus increasing the redundancy level of the early fetched tasks. Experimental results on actual TGs show the positive impacts of the proposed technique. Thus, without deteriorating the execution time (makespan), on average, a 114% MTTF improvement is achieved for no-fault-tolerant TGs, and the improvement is more significant when applying to fault-tolerant TGs. Finally, this paper presents a hardware implementation of a manager that applies these techniques at runtime and steers the execution of the running TGs. It demonstrates that, with 0.03% consumption of flip-flops and look-up tables and also 1.22% occupancy of block random access memory available on the Xilinx Virtex Ultra- Scale XCVU095-2FFVA2104E field programmable gate array, the required runtime computations can be carried out in negligible delays.		Reza Ramezani;Yasser Sedaghat;Juan Antonio Clemente	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2016.2631724	embedded system;electronic engineering;parallel computing;real-time computing;computer science;operating system;redundancy;field-programmable gate array	HPC	6.703159668177092	58.278925455223764	115801
494fa74b53ffd9727faa35dde921798aaa8b18b6	network processing challenges and an experimental npu platform	tool support;network processor;high performance;simulation environment	The fast-changing communications market requires high-performance yet flexible network-processing platforms. StepNP¿ is an exploratory network processor simulation environment for exploring router applications, multiprocessor network-processing architectures, and SoC tools. Supporting model interaction, instrumentation, and analysis, the platform lets R&D teams easily add new processors, coprocessors, and interconnects.	central processing unit;coprocessor;electrical connection;multiprocessing;network processor;router (computing);simulation;system on a chip	Pierre G. Paulin;Chuck Pilkington;Essaid Bensoudane	2003			embedded system;computer architecture;real-time computing;computer science;operating system;network processor	Arch	3.9535942585876724	50.06807244845522	115905
7faa67b28a6787a20249d105dc2ada252c58e157	ultra-fast and efficient algorithm for energy optimization by gradient-based stochastic voltage and task scheduling	encoding decoding;efficient algorithm;slack distribution;realtime system;energy optimization;system level design;power management;power optimization;stochastic gradient;design space exploration;task scheduling;voltage and task scheduling;heuristic algorithm	This paper presents a new technique, called Adaptive Stochastic Gradient Voltage-and-Task Scheduling (ASG-VTS), for power optimization of multicore hard realtime systems. ASG-VTS combines stochastic and energy-gradient techniques to simultaneously solve the slack distribution and task reordering problem. It produces very efficient results with few mode transitions. Our experiments show that ASG-VTS reduces number of mode transitions by 4.8 times compared to traditional energy-gradient-based approaches. Also, our heuristic algorithm can quickly find a solution that is as good as the optimal for a real-life GSM encoder/decoder benchmark. The runtime of ASG-VTS is 150 times and 1034 times faster than energy-gradient based and optimal ILP algorithms, respectively. Since the runtime of ASG-VTS is very low, it is ideal for design space exploration in system-level design tools. We have also developed a web-based interface for ASG-VTS algorithm.	asg software solutions;algorithm;benchmark (computing);design space exploration;electronic system-level design and verification;encoder;experiment;gradient;heuristic (computer science);level design;mathematical optimization;multi-core processor;power optimization (eda);real life;real-time computing;scheduling (computing);slack variable;vehicle tracking system;web application	Bita Gorjiara;Nader Bagherzadeh;Pai H. Chou	2007	ACM Trans. Design Autom. Electr. Syst.	10.1145/1278349.1278352	heuristic;fair-share scheduling;mathematical optimization;parallel computing;real-time computing;dynamic priority scheduling;computer science;least slack time scheduling;electronic system-level design and verification;power optimization	EDA	0.9261890291814813	55.92809086594741	115968
75433f4cc4e7f6ea9f36849d8ea029538970bca1	in order issue out-of-order execution floating-point coprocessor for calmrisc32	microprocessors;data dependency;floating point unit;precise exception;ieee 754 standard single precision operations;in order issue out of order execution floating point coprocessor;floating point multiplication;reduced instruction set computing;risc coprocessor;floating point addition;floating point division;out of order coprocessors pipelines embedded system delay laboratories microprocessors floating point arithmetic hardware large scale integration;format conversion;comparison;coprocessors;embedded system;out of order;resource conflict;exception prediction;rounding;embedded systems;calmrisc32 floating point unit;design technique;gate level simulation;out of order execution;large scale integration;embedded systems reduced instruction set computing floating point arithmetic coprocessors;floating point subtraction;data dependence;worst case delay;pipelines;cell base design techniques;floating point;floating point arithmetic;worst case delay in order issue out of order execution floating point coprocessor calmrisc32 floating point unit risc coprocessor embedded system applications floating point addition floating point subtraction floating point multiplication floating point division format conversion comparison rounding load store ieee 754 standard single precision operations rounding modes precise exception data dependency resource conflict exception prediction cell base design techniques gate level simulation;store;rounding modes;embedded system applications;hardware	The CalmRISC32 FPU (Floating-Point Unit) is a RISC coprocessor for embedded system applications. It supports IEEE-754 standard single precision floating-point addition, floating-point subtraction, floating-point multiplication, floating-point division, format conversion, comparison, rounding, load, store, etc. It also supports four rounding modes, and precise exception. It can execute and complete instructions out of order, if constraints such as data dependency, resource conflict, and exception prediction are resolved. Standard cell-base design techniques were used to reduce design time and expense. The first prototype operated at approximately 70 MHz with the worst-case delay in gate level simulation.		Cheol-Ho Jeong;Woo-Chan Park;Tack-Don Han;Moon Key Lee;Sang-Woo Kim	2001		10.1109/ARITH.2001.930119	floating-point unit;double-precision floating-point format;parallel computing;real-time computing;computer hardware;computer science;floating point;out-of-order execution;operating system;x87;half-precision floating-point format	Logic	7.088155561728249	49.43014116305073	116097
46d53cdb87a98519d7406582254006d4648e0b67	exploration of the flexray signal integrity using a combined prototyping and simulation approach	distributed application;virtual prototyping hardware space exploration network topology testing signal analysis vehicles power cables analytical models informatics;analytical models;automotive electronics;topology;distributed processing;signal integrity;network topology;computational modeling;mathematical model;transceivers;electronic engineering computing;flexray topology flexray signal integrity teodacs test;network topology automotive electronics digital simulation distributed processing electronic engineering computing;simulation model;digital simulation;hardware	Ensuring a correct signal integrity within the entire FlexRay network and for all the possible environmental situations is mandatory for reliable operation of the distributed application. However, this is a goal difficult to reach due to the large number of parameters that influence the signal integrity. The use of simulation is a natural answer to efficiently support space exploration. We discuss in this work how the TEODACS test approach supports the validation process of the simulation models for FlexRay topologies and provides trustfulness for the simulation results even if hardware reference is not available. Further, we introduce a new method for the advanced analysis and evaluation of signal integrity in FlexRay networks.	avl tree;co-simulation;comet (programming);diagram;distributed computing;flexray;fundamental fysiks group;semiconductor;signal integrity;signal-flow graph;simulation	Martin Krammer;Federico Clazzer;Eric Armengaud;Michael Karner;Christian Steger;Reinhold Weiss	2010	13th IEEE Symposium on Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2010.5491806	embedded system;electronic engineering;real-time computing;computer science;signal integrity;engineering;simulation modeling;mathematical model;computational model;network topology;transceiver;computer engineering	Security	5.069448688829054	56.16670104163225	116121
ce8bf8fdb828146734a7c0484a34b57cf1b6f01c	kpn2gpu: an approach for discovery and exploitation of fine-grain data parallelism in process networks	data parallel;process network;fpga;embedded system;system on chip;graphic processing unit;source code;parallel programs;high performance;driver assistance system	With advances in manycore and accelerator architectures, the high performance and embedded spaces are rapidly converging. Emerging architectures feature different forms of parallelism. The Polyhedral Processes Networks (PPNs) are a proven model of choice for automated generation of pipeline and task parallel programs from sequential source code, however data parallelism is not addressed. In this paper, we present asystematic approach for identification and extraction of fine grain data parallelism from the PPN specification. The approach is implemented in a tool, called kpn2gpu, which produces fine-grain data parallel CUDA kernels for graphics processing units (GPUs). First experiments indicate that generated applications have a potential to exploit different forms of parallelism provided by the architecture and that kernels feature a highly regular structure that allows subsequent optimizations.	cuda;computer graphics;data parallelism;embedded system;experiment;graphics processing unit;kahn process networks;manycore processor;multi-core processor;parallel computing;pipeline (computing);polyhedral	Ana Balevic;Bart Kienhuis	2011	SIGARCH Computer Architecture News	10.1145/2082156.2082173	system on a chip;computer architecture;parallel computing;real-time computing;computer science;operating system;data parallelism;programming language;instruction-level parallelism;implicit parallelism;field-programmable gate array;task parallelism;source code	Arch	-0.5294057984599339	49.884522839306435	116356
182436ac8dabb4d1c45c5a844e2c1b079ede39f1	designing an application oriented terminal	cobol	"""Application oriented Terminals have been around for quite some time and traditionally they have been hard wired. The approach, however, has been changing recently due to the availability of low cost RAM memory and microprocessors. The main reason for the trend is that although the hard wired Terminals cost less, they do not provide or have the flexibility a customer is provided with a programmable Terminal. However, the cost of programming becomes an important consideration and will vary depending upon what data is available with the Terminal to aid the programmer. In designing the Banking Terminal, Honeywell used a combination of the traditional hard wired implementation approach and the complete programmability approach. Honeywell also provided for a COBOL type """"FITAL"""" (Financial Terminal Application Language) user level language for the customer, to aid programming the transaction sequences which allowed the customer to reduce programming cost. The limited programmability approach reduces the amount of programming, while it does not sacrifice flexibility available to the customer to tailor the Terminal transactions to suit his requirements."""	cobol;computer terminal;control function (econometrics);design rationale;field-programmability;microprocessor;programmer;random-access memory;read-only memory;requirement	J. P. Kohli	1975		10.1145/1499949.1499961	embedded system;real-time computing;engineering;operations management	AI	9.506718983855423	55.10965943426865	116375
2d884928744d83ad5cad793a9fb1a355f3deba10	dmt and dt2: two fault-tolerant architectures developed by cnes for cots-based spacecraft supercomputers	mainframes;triple modular redundant;fault tolerant;structural duplex architecture;mainframes aircraft computers fault tolerant computing;triple modular redundancy;time redundancy;single event effect;computer fault tolerance;cots components;structural duplex architecture dmt fault tolerant architectures dt2 fault tolerant architectures cots based spacecraft supercomputers commercial off the shelf electronic components see single event effect fault tolerance cots components tmr triple modular redundancy time redundancy;see;dmt fault tolerant architectures;fault tolerant computing;commercial off the shelf;cots based spacecraft supercomputers;fault tolerance;dt2 fault tolerant architectures;electronic components;aircraft computers;ofdm modulation fault tolerance space vehicles supercomputers costs redundancy computer architecture electronic components application software computer applications;power consumption;tmr;space application	COTS (commercial off-the-shelf) electronic components are attractive for space applications. However, computer designers need to solve a main problem as regards their SEE (single event effect) sensitivity. The purpose of fault tolerance studies conducted at CNES (the French Space Agency) is to prepare the space community for the significant evolution linked to the usage of COTS components. CNES has patented two fault-tolerant architectures with low recurring costs, mass and power consumption, as compared to conventional architectures as e.g. the TMR (triple modular redundancy) one. The former, referred to as DMT, is time redundancy based and minimises recurring costs. It is mainly intended for but not limited to scientific missions. The latter, referred to as DT2, is based on a structural duplex architecture with minimum duplication and is suited for high-end application missions	digital monetary trust;duplex (telecommunications);electronic component;fault tolerance;supercomputer;triple modular redundancy	Michel Pignol	2006	12th IEEE International On-Line Testing Symposium (IOLTS'06)	10.1109/IOLTS.2006.24	reliability engineering;embedded system;fault tolerance;parallel computing;real-time computing;computer science;engineering;operating system	Embedded	7.898999629790356	57.7463163713217	116429
aa8fabdb78a2fc81fcfc11c2ded63ef525b3f02b	hybrid real-time operating systems: deployment of critical freertos features on fpga	system latency;critical rtos services;fpga;hybrid operating systems;hardware accelerators;rtoss;embedded systems;predictability;real time operating systems;field programmable gate arrays;determinism	Performance and determinism are two critical metrics in most embedded systems with real-time requirements. Owing to the complexity of current embedded systems, along with increased application demands, real-time operating systems (RTOSs) have become a de facto solution providing specific services to the system tasks. However, this extra layer, which abstracts the hardware from the software, makes it harder for a system to achieve good performance and determinism. To ease the impact of a RTOS in the system, RTOS run-time services are offloaded to the hardware layer. This paper presents a hybrid RTOS implementation, where several critical RTOS services were migrated from software to hardware, improving system latency and predictability. Special focus was given to the RTOS scheduler and to the mutexes handling subsystem. The developed hardware accelerators were synthesised on a field-programmable gate array (FPGA), exploiting the point-to-point fast simplex link (FSL) bus to interconnect to the Xilinx Microb...	field-programmable gate array;freertos;real-time clock;real-time operating system;software deployment	Tiago Gomes;Jorge Pereira;Paulo Garcia;Filipe Salgado;Vitor Alberto Silva;Sandro Pinto;Mongkol Ekpanyapong;Adriano Tavares	2016	IJES	10.1504/IJES.2016.10001338	embedded system;embedded operating system;parallel computing;real-time computing;real-time operating system;computer science;operating system;field-programmable gate array	Embedded	-1.8756021959942126	50.24598094458136	116475
266009395dc0541a008a31dc314717b52f9e41b6	floating-point unit reuse in an fpga implementation of a ray-triangle intersection algorithm	floating-point reuse;fpga;reconfigurable computing;floating point unit;floating point	The recent emergence of high-quality floatingpoint libraries for FPGAs has sparked a renewed interest in accelerating scientific applications th rough Reconfigurable Computing (RC) techniques. Unfortunately, the sheer size of these floating-poi nt units makes it difficult to house a large number of units in a single FPGA. In order to support the adaptation of non-trivial algorithms to hardware, i t is therefore necessary to consider methods by which a set of floating-point units can be reused to perfor m different operations in an algorithm. In this paper we discuss a “recycling architecture” that reuses a fixed number of floatin gpoint units to implement an algorithm. We customize the hardware data path for this architecture at compile time based on a static computational schedu le that is generated for an algorithm. As a means of illustrating tradeoffs, we step through the adaptat ion process with an example application that computes ray-triangle intersection points. By reusing hardwa re, we are able to halve resource requirements while maintaining acceptable performance. As a means of motivating future work, we also discuss our experiences constructing tools that translate an algorithm’s equations into a synthesizable netlist .	compile time;compiler;computation;emergence;field-programmable gate array;floating-point unit;intersection algorithm;library (computing);netlist;reconfigurable computing;requirement	Craig Ulmer;Adrian Javelo	2006			netlist;parallel computing;computer architecture;field-programmable gate array;reuse;architecture;floating point;reconfigurable computing;algorithm;floating-point unit;computer science;compile time	EDA	0.7957075682021162	47.49488937238101	116496
d7b9e561f9e898cd53faecc8d33d1a4686fe183a	reducing dram image data access energy consumption in video processing	temporal correlation;energy conservation;dram energy reduction;random access memory;image data access;mobile device;power estimation;video signal processing;energy efficient;routing;domain specific design techniques dram image data access energy consumption reduction domain specific techniques mobile devices energy hungry tasks domain specific techniques dram energy efficiency video processing logic energy consumption data manipulation techniques heterogeneous dram architecture power estimation dram modeling;wires;video processing;system on a chip;random access memory energy consumption wires routing encoding system on a chip couplings;video coding;power aware computing;design technique;energy consumption;memory architecture;data access;spatiotemporal phenomena;heterogeneous dram architecture;image frame buffer recompression;couplings;encoding;video coding dram energy reduction heterogeneous dram architecture image data access image frame buffer recompression;dram chips;domain specificity;video signal processing dram chips energy conservation energy consumption memory architecture power aware computing spatiotemporal phenomena	This paper presents domain-specific techniques to reduce DRAM energy consumption for image data access in video processing. In mobile devices, video processing is one of the most energy-hungry tasks, and DRAM image data access energy consumption becomes increasingly dominant in overall video processing system energy consumption. Hence, it is highly desirable to develop domain-specific techniques that can exploit unique image data access characteristics to improve DRAM energy efficiency. Nevertheless, prior efforts on reducing DRAM energy consumption in video processing pale in comparison with that on reducing video processing logic energy consumption. In this work, we first apply three simple yet effective data manipulation techniques that exploit image data spatial/temporal correlation to reduce DRAM image data access energy consumption, then propose a heterogeneous DRAM architecture that can better adapt to unbalanced image access in most video processing to further improve DRAM energy efficiency. DRAM modeling and power estimation have been carried out to evaluate these domain-specific design techniques, and the results show that they can reduce DRAM energy consumption by up to 92%.	data access;domain-specific language;dynamic random-access memory;mobile device;norm (social);routing;unbalanced circuit;video decoder;video processing	Yiran Li;Tong Zhang	2012	IEEE Transactions on Multimedia	10.1109/TMM.2011.2177079	system on a chip;data access;embedded system;routing;parallel computing;real-time computing;energy conservation;computer science;operating system;mobile device;video processing;efficient energy use;coupling;encoding;computer network	Arch	1.0021146844728692	55.15068750432199	116520
c2fa42f0add7bd7d0cdea8d78170de95d6291bac	evolution in architectures and programming methodologies of coarse-grained reconfigurable computing	computer engineering;reconfigurable computing;reconfigurable architectures;computation models;computer model;coarse grained arrays;multimedia application;reconfigurable architecture;engineering and technology;teknik och teknologier;data och systemvetenskap;datavetenskap datalogi;computer and systems science;datorteknik;computer science;coarse grained;globally asynchronous locally synchronous	In order to meet the increased computational demands of, e.g., multimedia applications, such as video processing in HDTV, and communication applications, such as baseband processing in telecommunication systems, the architectures of reconfigurable devices have evolved to coarse-grained compositions of functional units or program controlled processors, which are operated in a coordinated manner to improve performance and energy efficiency. In this survey we explore the field of coarse-grained reconfigurable computing on the basis of the hardware aspects of granularity, reconfigurability, and interconnection networks, and discuss the effects of these on energy related properties and scalability. We also consider the computation models that are being adopted for programming of such machines, models that expose the parallelism inherent in the application in order to achieve better performance. We classify the coarse-grained reconfigurable architectures into four categories and present some of the existing examples of these categories. Finally, we identify the emerging trends of introduction of asynchronous techniques at the architectural level and the use of nano-electronics from technological perspective in the reconfigurable computing discipline. 2008 Elsevier B.V. All rights reserved.	baseband;computation;email;gnu nano;interconnection;parallel computing;reconfigurability;reconfigurable computing;scalability;video processing	Zain-ul-Abdin;Bertil Svensson	2009	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2008.10.003	computer simulation;embedded system;computer architecture;parallel computing;reconfigurable computing;computer science;theoretical computer science;operating system	Arch	-1.0370710361471707	48.91706482151362	116641
37de7833a354f485ce0ae04c5df5f8e6a48326d3	a mathematical hard disk timing model for full system simulation	probability;performance evaluation;disk arrays mathematical hard disk timing model design execution driven full system performance simulator disk simulators disk emulators disk operations disk structure disk physical layout mathematical modeling probabilistic modeling platter layout reorder queues disk intensive workload modeling benchmark averaging error;hard disks;layout;computational modeling;computer engineering a mathematical hard disk timing model for full system simulation purdue university vijay s pai parsons;mathematical model timing layout equations hard disks computational modeling tracking;mathematical model;probability digital simulation hard discs performance evaluation;benjamin s;hard discs;tracking;digital simulation;timing	This paper introduces and validates a mathematical hard disk timing model designed for use in an execution-driven full-system simulator. While very accurate disk simulators and emulators exist, their complexity is often not warranted when disk operations are not the sole focus of an experiment. This model depends far less on the details of the disk structure or the physical layout of the disk, making it less complex and easier to configure. By combining traditional hard disk modeling methods with novel ways of mathematically and probabilistically modeling the platter layout and reorder queues, the need for a physical model of the disk platter is eliminated. Current full-system simulators do not include a realistic disk timing model, giving them large and variable errors when modeling disk intensive workloads. With this model, we show that disk intensive full-system simulation can be accurate, with benchmarks averaging 12% error for individual disks and 18% error for disk arrays. This model will benefit full-system performance simulations by bridging the gap between complex and highly specific disk simulators and the actual performance modeling requirements needed for effective use of full-system simulators.	bridging (networking);computer architecture simulator;data-intensive computing;disk array;disk storage;emulator;hard disk drive;hard disk drive performance characteristics;integrated circuit layout;mathematical model;performance prediction;requirement;simulation	Benjamin S. Parsons;Vijay S. Pai	2013	2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2013.6557165	layout;parallel computing;real-time computing;simulation;computer hardware;computer science;disk mirroring;probability;mathematical model;tracking;logical disk;computational model;statistics	Metrics	-2.3179467844905637	55.79951747232228	116720
0cca5426635d15768e5cee12468b1b83510c4dcb	low power design from technology challenge to great products	moores law;design tool;reliability;design for low power;emerging market;system on chip design for manufacture integrated circuit design low power electronics system in package;power density;leakage current;service provider;systems on a single chip;system in package;semiconductor designer;cell phones;design for manufacture;design for low power management design soc;service providers;mp3 players;sip;signal integrity;semiconductor process technology;pda;chip;design for manufacturing;integrated circuit design;low power;sensor networks;manufacturing packaging moore s law semiconductor device manufacture market opportunities process design design for manufacture production silicon cellular phones;leakage currents;system on chip;power dissipation;content providers;low power electronics;power optimization;user requirements;systems in package;battery life;design;soc;entertainment products;low power design;on chip power density;multiple chips;management;eda design tools;light weight embedded systems;communications products;supply voltages;eda design tools low power design semiconductor process technology moores law communications products entertainment products semiconductor designer systems on a single chip soc service providers content providers design for manufacturing reliability cell phones pda mp3 players battery life supply voltages leakage currents on chip power density packaging techniques multiple chips systems in package sip signal integrity power dissipation;packaging techniques	Each generation of semiconductor process technology enables increased levels of integration and density on a single chip, Moores Law continues to prevail and the users of portable and hand held communications and entertainment products enjoy greater functionality and features. Lifestyles and user paradigms indicate that no matter how many features are added, service providers and manufacturers think of more and users cannot wait to acquire the latest products. Features, performance, fashion and, of course, fierce competition, drive the market and thereby set the challenge for the semiconductor designer. The challenge for designers is to create systems on a single chip (SoC) to provide these features for the user and to enable service and content providers to realize new emerging market opportunities. This has been a challenge for many years but, now that nanometer process technologies form the enabling process technology, the design challenge is much greater. Nanometer design effects must be considered from the initial SoC architecture all the way through to manufacturing where design for manufacturing (DFM) effects must be overcome to enable reliable, high volume production. At the silicon level the features demanded by the users require extensive efforts to provide acceptable performance and reliability. Users of cell phones, PDAs and MP3 players will be most familiar with the need for long battery life while, to achieve this, the SoC designer worries about how to design with lower supply voltages, higher leakage currents, on chip power density and reliability. Packaging techniques which assemble multiple chips to form systems in package (SiP) also create signal integrity and power dissipation issues. At the same time designers must be able to design with EDA design tools and methodology's that are still emerging and where no standards for low power design exist today to make the task easier. This keynote will talk about the low power design techniques available to SoC designers, how they are implemented in SoCs and how they are implemented in existing and new designs. The talk will end with a view on the challenges coming up next and what needs to be done to prepare for them	cpu power dissipation;design for manufacturability;electronic design automation;mp3;mobile device;mobile phone;personal digital assistant;reliability engineering;semiconductor device fabrication;signal integrity;single sign-on;spectral leakage;system on a chip	Barry Dennington	2006	ISLPED'06 Proceedings of the 2006 International Symposium on Low Power Electronics and Design	10.1145/1165573.1165625	service provider;system on a chip;embedded system;electronic engineering;telecommunications;computer science;engineering;electrical engineering;operating system;design for manufacturability	EDA	9.931745525949955	55.80394511922297	116722
02c996c64ed9a9bc6ae8a5b03bcadfdc4964b612	on the simulation and design of manycore cmps	machine learning algorithms;thesis or dissertation;cache coherency;parallelism;inter processor communication	The progression of Moore’s Law has resulted in both embedded and performance computing systems which use an ever increasing number of processing cores integrated in a single chip. Commercial systems are now available which provide hundreds of cores, and academics have proposed architectures for up to 1024 cores. Embedded multicores are increasingly popular as it is easier to guarantee hard-realtime constraints using individual cores dedicated for tasks, than to use traditional time-multiplexed processing. However, finding the optimal hardware configuration to meet these requirements at minimum cost requires extensive trial and error approaches to investigate the design space. This thesis tackles the problems encountered in the design of these large scale multicore systems by first addressing the problem of fast, detailed micro-architectural simulation. Initially addressing embedded systems, this work exploits the lack of hardware cache-coherence support in many deeply embedded systems to increase the available parallelism in the simulation. Then, through partitioning the NoC and using packet counting and cycle skipping reduces the amount of computation required to accurately model the NoC interconnect. In combination, this enables simulation speeds significantly higher than the state of the art, while maintaining less error, when compared to real hardware, than any similar simulator. Simulation speeds reach up to 370MIPS (Million (target) Instructions Per Second), or 110MHz, which is better than typical FPGA prototypes, and approaching final ASIC production speeds. This is achieved while maintaining an error of only 2.1%, significantly lower than other similar simulators. The thesis continues by scaling the simulator past large embedded systems up to 64-1024 core processors, adding support for coherent architectures using the same packet counting techniques along with low overhead context switching to enable the simulation of such large systems with stricter synchronisation requirements. The new interconnect model was partitioned to enable parallel simulation to further improve simulation speeds in a manner which did not sacrifice any accuracy. These innovations were leveraged to investigate significant novel energy saving optimisations to the coherency protocol, processor ISA, and processor micro-architecture. By introducing a new instruction, with the name wait-on-address, the energy spent during spin-wait style synchronisation events can be significantly reduced. This functions by putting the core into a low-power idle state while the cache line of the indicated		Christopher Callum Thompson	2015			computer architecture;parallel computing;real-time computing;computer science;cache algorithms	EDA	-0.7249748286570077	56.477635194328656	116970
a0b942bad00a8ab45a7c7c6d09590fd35d38b583	prometheus: coherent exploration of hardware and software optimizations using aspen		With the dramatic increase in scale expected for Exascale computing, there is a dire need for tuning of hardware configurations and software optimizations such that they are in unison. However, the expected increase in tunable hardware parameters makes searching through the design space for optimal hardware-and-software configurations much more challenging. Towards this end, we propose a composable hardware-software optimization framework called Prometheus. Prometheus uses a combination of analytical and machine-learning techniques to capture application characteristics and subsequently determine the hardware-software configuration for near-optimal performance. We evaluate Prometheus for its efficacy using two widely used proxy applications: LULESH and CoMD. We demonstrate that Prometheus identifies near-optimal hardware-software configurations and verify the results via brute-force search of the design space.	brute-force search;coherent;computer hardware;digital subscriber line;machine learning;mathematical optimization;program optimization;prometheus;supervised learning;unison	Mariam Umar;Shirley V. Moore;Jeffrey S. Vetter;Kirk W. Cameron	2018	2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)	10.1109/MASCOTS.2018.00032	computer hardware;exascale computing;reconfigurable computing;computer science;domain-specific language;software;supercomputer	Arch	-1.7543944923432966	50.70168746099902	117011
f26803a8aece6e2a7feb840920d98cd77abec1f4	a framework for mpsoc generation and distributed applications evaluation	multiprocessor system on chip automatic mpsoc generation distributed applications evaluation mapping heuristic operating system design space exploration distributed management cycle accurate systemc description;system on chip;load modeling program processors solid modeling random access memory hardware computational modeling computer architecture;system on chip microprocessor chips;reclustering mpsoc noc framework distributed management;microprocessor chips	The design of MPSoCs is a complex task. From the designer side point of view, a new feature inserted into the system (e.g. a mapping heuristic or a new function in the operating system) must be validated with a large set of the MPSoC configurations. From the application developer side point of view, the performance of a set of applications running simultaneously in the MPSoC platform must be also evaluated for different MPSoC configurations. Therefore, for both designers and application developers a framework enabling the automatic MPSoCs generation and simulation is mandatory for design space exploration. This is the goal of the present work, present a parameterizable MPSoC, including distributed management, and a framework to generate and simulate several MPSoCs configurations automatically. Results show that it is feasible to simulate large platforms, up to 400 processing elements, using a cycle accurate SystemC description.	design space exploration;distributed computing;heuristic;mpsoc;network on a chip;operating system;point of view (computer hardware company);quality of service;run time (program lifecycle phase);simulation;systemc;throughput	Guilherme M. Castilhos;Eduardo Wächter;Guilherme A. Madalozzo;Augusto Erichsen;Thiago Monteiro;Fernando Gehm Moraes	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783353	system on a chip;embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	2.112805010652802	52.87367834427271	117027
5a882aeb47e4fc58bbcb0d94f2aeca16542a3fb1	an efficient fault-tolerant vlsi architecture using parallel evolvable hardware technology	system reliability;evolutionary computation;parallel evolutionary algorithm;fault tolerant;reconfigurable architectures;very large scale integration;parallel evolvable hardware technology;systems reliability fault tolerant architecture vlsi architecture parallel evolvable hardware technology parallel evolutionary algorithm controller global positioning system attitude determination system;global position system;controller;evolvable hardware;attitude determination system;null;integrated circuit design;fault tolerant computing;parallel architectures;fault tolerant architecture;structure and function;global positioning system;fault tolerance;vlsi;systems reliability;attitude measurement;nasa;fault tolerance very large scale integration hardware nasa;attitude measurement vlsi integrated circuit design parallel architectures reconfigurable architectures fault tolerant computing evolutionary computation global positioning system;hardware;vlsi architecture	This paper proposes a novel, fault-tolerant, VLSI architecture, which utilizes an evolvable hardware (EHW) framework using a parallel evolutionary algorithm (EA). The architecture consists of two layers. The first layer considers the application in hand, whereas the second is used as a controller that monitors the performance of the first layer and reconfigures when appropriate, its computational elements. The demonstration of this architecture is done through a practical example of a Global Positioning System (GPS) attitude determination system. Firstly the structure and functionality of both layers is described. Subsequently, the paper provides results that demonstrate both the reliability and performance of the system, while various quantities of faults are simultaneously injected in both layers. According to these results, the first layer is capable to cope with higher amount of faults (worst-case scenario 35-40%) than the second (control) layer, which copes with faults that capture in worst-case scenario the 30% of its resources. Finally, an additional mechanism to this architecture is proposed that needs further investigation and promises further enhancing of the systems reliability.	best, worst and average case;controller (computing);evolutionary algorithm;evolvable hardware;fault tolerance;global positioning system;reliability engineering;very-large-scale integration;worst-case complexity;worst-case scenario	Evangelos F. Stefatos;Tughrul Arslan	2004	Proceedings. 2004 NASA/DoD Conference on Evolvable Hardware, 2004.	10.1109/EH.2004.1310816	embedded system;real-time computing;engineering;computer engineering	HPC	9.230030698375156	58.78938425779265	117132
04ab72a46c93bf939c5c22ec731c308184865017	low-power data memory communication for application-specific embedded processors	data intensive application;data stream;storage management;power efficiency;onchip memory;embedded systems;low power;energy optimization;memory architecture;reprogrammable hardware support low power data memory communication application specific embedded processors customization methodology power reduction communication link data memory address bus memory references data intensive program loops address communication protocol data memory controller minimal run time information;energy consumption hardware engines system on a chip energy dissipation computer science power engineering and energy batteries protocols power generation;communication protocol;power reduction;embedded processor;storage management embedded systems memory architecture	We propose a novel customization methodology for power reduction on the communication link between an embedded processor and its data memory. We target the address bus and show how by utilizing application information about the memory references in the data intensive program loops, a power efficient address communication protocol can be established between the processor core and the data memory. The data memory controller thus generates the addresses for the various data streams with minimal run-time information from the processor engine, achieving significant power reductions on the address bus. An efficient reprogrammable hardware support is presented for enabling the proposed methodology. The experimental results demonstrate the efficacy of the approach for a set of data intensive applications.	address bus;addressing mode;communications protocol;control flow;data-intensive computing;embedded system;mathematical optimization;memory controller;multi-core processor;power optimization (eda)	Alex Orailoglu;Peter Petrov	2002		10.1145/581199.581248	memory address;uniform memory access;shared memory;embedded system;interleaved memory;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer science;physical address;virtual memory;computer data storage;computer memory;overlay;memory controller;conventional memory;extended memory;flat memory model;registered memory;address bus;memory map;memory management	EDA	-2.936299061325126	53.94655914914113	117186
c6f3c7f4f1349c8480d6bbbf21de51e7c580d8c8	scalable and realistic benchmark synthesis for efficient noc performance evaluation: a complex network analysis approach	complex networks;runtime;computer architecture;stochastic processes;heuristic algorithms;mathematical model;benchmark testing	The complexity of the design-space exploration of large-scale NoCs is exacerbated not only by the ever-increasing number of cores, but also by the increased runtime uncertainties in both the scale and task structure of the emerging applications. Consequently, it is crucial to develop rigorous mathematical frameworks for capturing the task dependencies of varied applications to foster the generation of realistic benchmarks that can guide the NoC design. However, the current NoC benchmark suites either lack portability and poorly scale as they require intensive development efforts on specific architectures and simulation time, or are synthesized based on purely stochastic models that are disconnected with real applications, which may easily lead to biased and/or delayed design choices. To overcome these drawbacks, we propose a benchmark synthesis framework that i) not only allows extraction of dynamical task dependencies of the application and synthesize traffic workloads spatio-temporally consistent with realistic traffic behavior, ii) but can also be easily scaled by the proposed complex-network inspired algorithm for large benchmark generation while preserving key structural features that governs application communication behaviors. We validate the proposed framework on a large-scale simulation environment by running a set of real applications. Experimental results show that the synthesized benchmarks respect the traffic patterns of the original applications and preserve key features of application task structures.	algorithm;benchmark (computing);complex network;network on a chip;performance evaluation;simulation;stochastic process	Yuankun Xue;Paul Bogdan	2016	2016 International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS)	10.1145/2968456.2968471	real-time computing;simulation;computer science;theoretical computer science	EDA	0.9833421851130526	57.25133365751886	117243
35b2ee90be255c27f49693d29486a4b673cdc7c8	manual rescheduling and incremental repair of register-level datapaths	interactive systems circuit layout cad;high level synthesis tool manual rescheduling incremental repair register level datapaths rlext register level exploration tool interactive tool datapath design transformations manual changes data path structure schedule changes;high level synthesis;high level synthesis timing topology algorithm design and analysis computer science hardware design languages synthesizers engines programmable logic arrays software performance;circuit layout cad;interactive systems	A description is given of new capabilities of the RLEXT register level exploration tool. RLEXT is an interactive tool that takes a datapath design and allows a user to modify it freely, using transformations that do not themselves preserve correctness. By maintaining a representation of the desired behavior and timing as well as structure, RLEXT is able to 'repair' the design when the user creases modifying, so that the ability of the design to express the specified behavior is once again guaranteed. A description is also given of RLEXT's support for manual changes of schedule in the presence of an existing data-path structure. Such schedule changes invalidate the structure, but the structure is incrementally repaired rather than just thrown out. The author knows of no other high-level synthesis tool that supports this kind of functionality. >	datapath	David W. Knapp	1989		10.1109/ICCAD.1989.76904	embedded system;computer architecture;electronic engineering;real-time computing;computer science;operating system;high-level synthesis;programming language;engineering drawing	Logic	5.879592683579131	54.099870364547684	117394
8cfa26ad306b48a3bde5fa539f603ad7a2c0f30b	performance-reliability tradeoff analysis for multithreaded applications	jacobian matrices;fast fourier transforms;integrated circuit reliability;microprocessor chips;fft application;jacobi kernel;tvf;chip multiprocessors;computer systems;execution clock cycles;multicore architectures;multithreaded applications;performance-reliability tradeoff analysis;thread vulnerability factor;transient errors;unrolled jacobi code;water simulation;distributed algorithms;multi-core architectures and support;reliable parallel	Modern architectures become more susceptible to transient errors with the scale down of circuits. This makes reliability an increasingly critical concern in computer systems. In general, there is a tradeoff between system reliability and performance of multithreaded applications running on multicore architectures. In this paper, we conduct a performance-reliability analysis for different parallel versions of three data-intensive applications including FFT, Jacobi Kernel, and Water Simulation. We measure the performance of these programs by counting execution clock cycles, while the system reliability is measured by Thread Vulnerability Factor (TVF) which is a recently-proposed metric. TVF measures the vulnerability of a thread to hardware faults at a high level. We carry out experiments by executing parallel implementations on multicore architectures and collect data about the performance and vulnerability. Our experimental evaluation indicates that the choice is clear for FFT application and Jacobi Kernel. Transpose algorithm for FFT application results in less than 5% performance loss while the vulnerability increases by 20% compared to binary-exchange algorithm. Unrolled Jacobi code reduces execution time up to 50% with no significant change on vulnerability values. However, the tradeoff is more interesting for Water Simulation where nsquared version reduces the vulnerability values significantly by worsening the performance with similar rates compared to faster but more vulnerable spatial version.	clock signal;data-intensive computing;experiment;fast fourier transform;greedy algorithm;high-level programming language;jacobi method;kernel (operating system);multi-core processor;multithreading (computer architecture);run time (program lifecycle phase);simulation;thread (computing);vulnerability (computing)	Isil Oz;Haluk Topcuoglu;Mahmut T. Kandemir;Oguz Tosun	2012	2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)		multi-core processor;embedded system;distributed algorithm;fast fourier transform;parallel computing;real-time computing;computer science;theoretical computer science;operating system;instruction set;reliability;processor register;measurement	Metrics	-2.537294471382328	56.2531334385569	117453
02b7f82439ae523391cd762beb006a93081521a8	a generic wrapper architecture for multi-processor soc cosimulation and design	protocols;formal specification;hardware software codesign;parameterised systems;parallel architectures protocols code division multiple access formal specification embedded systems hardware software codesign;protocols design methodology system on a chip permission laser sintering laboratories multiaccess communication cellular phones system performance runtime;exploration of system configurations;embedded systems;code division multiple access;parallel architectures;is 95 cdma cellular phone system generic wrapper architecture multiprocessor soc cosimulation communication refinement multiple communication protocols abstraction levels system specification communication protocols heterogeneous component mixed level cosimulation;communication protocol;genetic algorithms	In communication refinement with multiple communication protocols and abstraction levels, the system specification is described by heterogeneous components in terms of communication protocols and abstraction levels. To adapt each heterogeneous component to the other part of system, we present a generic wrapper architecture that can adapt different protocols or different abstraction levels, or both. In this paper, we give a detailed explanation of applying the generic wrapper architecture to mixed-level cosimulation. As preliminary experiments, we applied it to mixed-level cosimulation of an IS-95 CDMA cellular phone system.	experiment;mobile phone;multiprocessing;refinement (computing);system on a chip	Sungjoo Yoo;Gabriela Nicolescu;Damien Lyonnard;Amer Baghdadi;Ahmed Amine Jerraya	2001		10.1145/371636.371722	embedded system;computer architecture;real-time computing;computer science	Embedded	4.225492394849858	53.142785137795485	117841
835a71fb6abb2aaa546e4d63da8d622317c203ed	fault-tolerance through reconfiguration of vlsi and wsi awards	electronic circuits;fault tolerant;fault model;integrated circuits;parallel processing;integrated circuit;indexation;computer architecture		fault tolerance;very-large-scale integration;wafer-scale integration	Roberto M. Negrini;Mariagiovanna Sami;Renato Stefanelli	1989				Theory	8.966303824493036	50.5720956712408	117997
782bd1c3dba6a5886839026bae43b6b02ac11783	the implementation of buttons driver using the linux input subsystem based on s3c6410 platform	linux input subsystem;operating system kernels device drivers linux microprocessor chips;kernel;pins;s3c6410;presses;buttons driver;operating system;device drivers;registers;linux;kernel linux registers hardware pins presses educational institutions;operating system kernels;linux kernel buttons driver s3c6410 platform structure system linux input subsystem programming external interrupt resource analysis s3c6410 microprocessor;microprocessor chips;hardware;buttons driver operating system linux input subsystem s3c6410	This paper firstly introduces the structure system and programming of the Linux input subsystem, and then detailed analyses the external interrupt resources of S3C6410 microprocessor as well as the handling of them in the Linux kernel, finally implements a simple buttons driver using the input subsystem based on the S3C6410 platform. Moreover, an application to demonstrate the function of this buttons driver is designed as well for testing.	device driver;high- and low-level;input device;linux;microprocessor;user space	Yuanlou Gao;Xixin Zhao	2012	IEEE 10th International Conference on Industrial Informatics	10.1109/INDIN.2012.6301361	embedded system;computer hardware;engineering;operating system	Embedded	6.61776601784336	48.86246631439389	118008
75d05e515d994c10a57dd2a72284f90eff252905	refining power consumption estimations in the component based aadl design flow	estimation power demand software hardware computational modeling sdram field programmable gate arrays;software;power estimation tool;power estimation;design flow;spice embedded systems power consumption simulation languages;real time embedded systems;software components;processor binding;embedded systems;computational modeling;estimation;european spices project;modelling language;software component;architecture design language;architecture analysis;real time embedded systems power consumption estimations specification refinement power estimation tool european spices project processor binding software components architecture analysis architecture design language modelling language;simulation languages;power consumption;field programmable gate arrays;power demand;spice;power consumption estimations;specification refinement;power modeling;sdram;hardware	This paper presents a method that permits to quickly estimate the power consumption at the first steps of a systempsilas design. We present multi-level power models and show how to use them at different levels of the specification refinement in the component based AADL design flow. PET, a power estimation tool, is being developed in the frame of the European SPICES project. It first prototype gives, in the case of a processor binding, power consumption estimations, for software components in the AADL component assembly model, with a maximal error ranging roughly from 5% to 30% depending on the refinement level. We illustrate our approach with the power model of the PowerPC 405, and its use at different levels in the AADL flow.	arm7;architecture analysis & design language;central processing unit;component-based software engineering;design flow (eda);digital signal processor;maximal set;parsing;pipeline (computing);powerpc 400;prototype;refinement (computing);xscale	Eric Senn;Johann Laurent;Emmanuel Juin;Jean-Philippe Diguet	2008	2008 Forum on Specification, Verification and Design Languages	10.1109/FDL.2008.4641441	embedded system;parallel computing;real-time computing;computer science;component-based software engineering;operating system;programming language	EDA	3.3798672332611654	52.90898817227845	118097
453406e11225015a2526a56d4c90a06242e4ba2e	mapping parameterized cyclo-static dataflow graphs onto configurable hardware	parameterized dataflow;4g communication systems;fpga implementation;scheduling;dataflow modeling	In recent years, parameterized dataflow has evolved as a useful framework for modeling synchronous and cyclo-static graphs in which arbitrary parameters can be changed dynamically. Parameterized dataflow has proven to have significant expressive power for managing dynamics of DSP applications in important ways. However, efficient hardware synthesis techniques for parameterized datafow representations are lacking. This paper addresses this void; specifically, the paper investigates efficient field programmable gate array (FPGA)-based implementation of parameterized cyclostatic dataflow (PCSDF) graphs. We develop a scheduling technique for throughputconstrained minimization of dataflow buffering requirements when mapping PCSDF representations of DSP applications onto FPGAs. The proposed scheduling technique is integrated with an existing formal schedule model, called the generalized schedule tree, to reduce schedule cost. To demonstrate our new, hardware-oriented PCSDF scheduling technique, we have designed a real-time base station emulator prototype based on a subset of long-term evolution (LTE), which is a key cellular standard. Hojin Kee National Instruments Corp., Austin, TX 78759, USA E-mail: hojin.kee@ni.com Chung-Ching Shen Department of ECE and UMIACS, University of Maryland, College Park, MD20742, USA E-mail: ccshen@umd.edu Shuvra S. Bhattacharyya Department of ECE and UMIACS, University of Maryland, College Park, MD20742, USA E-mail: ssb@umd.edu Ian Wong National Instruments Corp., Austin, TX 78759, USA E-mail: ian.wong@ni.com Yong Rao National Instruments Corp., Austin, TX 78759, USA E-mail: Yong.Rao@ni.com Jacob Kornerup National Instruments Corp., Austin, TX 78759, USA E-mail: jacob.kornerup@ni.com Journal of Signal Processing Systems, 66(3):285-301, 2012. DOI: DOI 10.1007/s11265-011-0599-5.	algorithm;compaq lte;data buffer;data-flow analysis;dataflow;digital signal processor;electrical engineering;emulator;field-programmable gate array;high- and low-level;kee games;level design;prototype;real-time clock;requirement;scheduling (computing);signal processing;throughput;transmitter	Hojin Kee;Chung-Ching Shen;Shuvra S. Bhattacharyya;Ian C. Wong;Yong Rao;Jacob Kornerup	2012	Signal Processing Systems	10.1007/s11265-011-0599-5	dataflow architecture;parallel computing;real-time computing;computer science;operating system;dataflow;signal programming;distributed computing;scheduling	Mobile	0.21419837240520267	54.8681583258383	118158
c6aa062ec6f33579b7cc26c254865995179c3f30	application of symbolic analysis in the industrial analog ic design	symbolic analysis		integrated circuit design	Ralf Sommer;Eckhard Hennig	2002			computer architecture;symbolic data analysis;integrated circuit design;computer science	EDA	9.102420056960984	52.07575266550806	118216
ff4d850fc1fdcf9180181766d07bb4d754eecf18	sensing user context and habits for run-time energy optimization	signal image and speech processing;circuits and systems;control structures and microprogramming;electronic circuits and devices	Optimizing energy consumption in modern mobile handheld devices plays a very important role as lowering energy consumption impacts battery life and system reliability. With next-generation smartphones and tablets, the number of sensors and communication tools will increase and more and more communication interfaces and protocols such as Wi-Fi, Bluetooth, GPRS, UMTS, and LTE will be incorporated. Consequently, the fraction of energy consumed by these components will be larger. Nevertheless, the use of the large amount of data from the different sensors can be beneficial to detect the changing user context, to understand habits, and to detect running application needs. All these information, when used properly, may lead to an efficient energy consumption control. This paper proposes a tool to analyze user/application interaction to understand how the different hardware components are used at run-time and optimize them. The idea here is to use machine learning methods to identify and classify user behaviors and habit information. Using this tool, a software has been developed to control at run-time system component activities that have high impacts on the energy consumption. The tool allows also to predict future applications usages. By this way, screen brightness, CPU frequency, Wi-Fi connectivity, and playback sound level can be optimized while meeting the applications and the user requirements. Our experimental results show that the proposed solution can lower the energy consumption by up to 30% versus the out-of-the-box power governor, while maintaining a negligible system overhead.	bluetooth;central processing unit;compaq lte;machine learning;mathematical optimization;mobile device;optimizing compiler;out of the box (feature);overhead (computing);requirement;run time (program lifecycle phase);runtime system;sensor;smartphone;user requirements document	Ismat Chaib Draa;Smaïl Niar;Jamel Tayeb;Emmanuelle Grislin-Le Strugeon;Mikael Desertot	2017	EURASIP J. Emb. Sys.	10.1186/s13639-016-0036-8	software;real-time computing;computer science;embedded system;general packet radio service;energy consumption;efficient energy use;umts frequency bands;mobile device;user requirements document;bluetooth	Mobile	-2.8791422801628728	59.27878786869665	118302
b7416c6d9076a15dbeb13fc10b0c6525033284e4	face detection on embedded systems	automated face detection and recognition;embedded system;face detection;real time image processing	Over recent years automated face detection and recognition (FDR) have gained significant attention from the commercial and research sectors. This paper presents an embedded face detection solution aimed at addressing the real-time image processing requirements within a wide range of applications. As face detection is a computationally intensive task, an embedded solution would give rise to opportunities for discrete economical devices that could be applied and integrated into a vast majority of applications. This work focuses on the use of FPGAs as the embedded prototyping technology where the thread of execution is carried out on an embedded softcore processor. Custom instructions have been utilized as a means of applying software/hardware partitioning through which the computational bottlenecks are moved to hardware. A speedup by a factor of 110 was achieved from employing custom instructions and software optimizations.	32-bit;algorithm;amdahl's law;cpu cache;cache (computing);digital signal processor;discrete mathematics;embedded system;experience;experiment;face detection;field-programmable gate array;hardware acceleration;image processing;jones calculus;real-time clock;requirement;run time (program lifecycle phase);software performance testing;speedup;the australian;thread (computing);viola–jones object detection framework	Abbas Bigdeli;Colin Sim;Morteza Biglari-Abhari;Brian C. Lovell	2007		10.1007/978-3-540-72685-2_28	embedded system;face detection;real-time computing;computer hardware;computer science;operating system	Embedded	3.328382317809937	49.06875258606711	118341
dc1fe56de614bed94635ac5d605b1782be863bb3	dynamic power management in an embedded system for multiple service requests	m m 1 and m g 1 queues;service provider;arrival time;service time;embedded system;power management;nonstationary decision policy;power manager;dynamic power management	Power is increasingly becoming a design constraint for embedded systems. Dynamic Power Management algorithms enable optimal utilization of hardware at runtime. The present work attempts to arrive at an optimal policy to reduce the energy consumption at system level, by selectively placing components into low power states. A new, simple algorithm for power management systems involving multiple requests and services, proposed here, has been obtained from stochastic queuing models. The proposed policy is event driven and based on a Deterministic Markov Nonstationary Policy model (DMNSP). The proposed policy has been tested using a Java-based event driven simulator. The test results show that there is about 23% minimum power saving over the existing schemes with less impact on performance or reliability.	embedded system;power management;requests	Lakshmi Prabha Viswanathan;Elwin Chandra Monie	2005	Journal of Circuits, Systems, and Computers	10.1142/S0218126605002696	service provider;embedded system;real-time computing;simulation;computer science;operating system;distributed computing	EDA	-4.0799359548388425	60.20090501843886	118445
cce771f5ccbe91613f1864f7d4a28d854a2ec350	the design automation standards environment	design automation;distributed computing;systems engineering and theory;standards development;permission;electrical products;design automation standards development electrical products permission standardization very high speed integrated circuits hardware systems engineering and theory distributed computing machinery;very high speed integrated circuits;machinery;standardization;hardware	(VHDL), Description Language Electronic Design Format (EDIF) Interchange and Initial GraDhic Exchanue Specification/Product Definition Specification (IGEs/PDEs) Exchange may give the impression of conflict in the DA standards arena. These emerging standards may work together harmoniously. Some with this premise. may disagree Are there conflict? overlap and The panelists represent standards developers, users of standards and those who impose the use of standards.	automation;electronic design;vhdl	Ronald Waxman	1987		10.1145/37888.37971	embedded system;machine;electronic design automation;computer science;systems engineering;electrical engineering;process automation system;software engineering;standardization;totally integrated automation;computer engineering	EDA	9.49198094375877	53.79800948208323	118635
507a3c6cf3e4bd5ad0237578cdeb22d77991d5a2	design rule verification based on one dimensional scans	novel data;efficient processing;rule checking concept;n-channel mask;integrated circuit mask;design rule verification;cpu time;design check;geometric data;bell-northern research;design rule checking;job design;silicon;design rules;circuits;integrated circuit;costing;algorithms	Bell-Northern Research has developed a program for design rule checking of integrated circuit masks, based on a one-dimensional scanning technique using a novel data coding scheme for efficient processing of large volumes of geometric data. The rule checking concept is very simple and the program is small and easily implemented. The technique is also extremely economical, costing less than $100 to apply 25 design checks to a high density 5200 µm square silicon gate n-channel mask. CPU time varies approximately as the power 1.2 of the amount of data in the mask.	central processing unit;fortran;graphical user interface;ibm personal computer;integrated circuit;microsoft windows;netware;spectral mask;subroutine;variable shadowing;z/vm	Philip S. Wilcox;H. Rombeek;D. M. Caughey	1978	15th Design Automation Conference		electronic engineering;computer science;electrical engineering;design rule checking;engineering drawing	EDA	8.022868051440541	49.105379525899004	118764
10b7472a98fc38a3418a0af1c35201b24d0a937e	retargetable binary utilities	program assemblers;free software processor retargetable binary utilities system on chip soc retargetable compilation downstream system tools assemblers linkers debuggers programming languages operating systems automatic retargeting gnu binutils tool kit open source software software standards systematic design techniques instruction set architecture formal model isa application binary interface abi;software portability;automated library mapping;computer languages;symbolic algebra;programming language;application software;systolic arrays;isa;instruction set architecture;processor retargetable binary utilities;system on a chip;downstream system tools;systematic design techniques;retargetable compilation;embedded software optimization;computer architecture;debuggers;software architecture;computer aided software engineering;operating system;system on chip;automatic retargeting;assemblers;abi;utility programs;instruction set architecture formal model;computation intensive software;linkers;assembly systems;soc;software standards;modems;open source software system on a chip assembly systems modems computer languages operating systems software standards computer architecture systolic arrays application software;program debugging;application binary interface;product quality;program compilers;utility programs program assemblers program compilers program debugging microprocessor chips instruction sets software standards computer architecture computer aided software engineering software portability software architecture;gnu binutils tool kit;programming languages;microprocessor chips;open source software;operating systems;instruction sets;free software;polynomial representation	Since software is playing an increasingly important role in system-on-chip, retargetable compilation has been an active research area in the last few years. However, the retargetting of equally important downstream system tools, such as assemblers, linkers and debuggers, has either been ignored, or falls short of meeting the requirements of modern programming languages and operating systems. In this paper, we present techniques that can automatically retarget the GNU binutils tool kit, which contains a large array of production-quality downstream tools. Other than having all the advantages enjoyed by open-source software by aligning to a de facto standard, our techniques are systematic, as a result of using a formal model of instruction set architecture (ISA) and application binary interface (ABI); and simple, as a result of leveraging free software to the largest extent.	application binary interface;assembly language;compiler;debugger;downstream (software development);formal language;gnu binutils;linker (computing);list of toolkits;open-source software;operating system;programming language;requirement;system on a chip	Maghsoud Abbaspour;Jianwen Zhu	2002		10.1145/513918.514004	system on a chip;embedded system;computer architecture;parallel computing;symbolic computation;computer science;operating system;programming language	SE	6.90423640327711	51.43326416210168	118791
74a19494211ac2d7e1cec4d6de8652a1b21a44b9	a dynamically reconfigurable architecture for embedded systems	data computing efficiency;reconfigurable network;multimedia communications;mobile telephone networks;chip area;umts;dynamic reconfiguration;3rd generation networks;telephone networks;reconfigurable architectures;cellular radio;data oriented content;dynamically reconfigurable architecture;technological forecasting embedded systems reconfigurable architectures cellular radio internet multimedia communication telephone networks computer telephony integration;embedded system;text messaging;chip;computer architecture;embedded systems;internet;reconfigurable network dynamically reconfigurable architecture embedded systems internet mobile telephone networks umts multimedia communications data oriented content system on chip solutions chip area power data computing efficiency 3rd generation networks;streaming media;multimedia communication;mobile handsets;access protocols;reconfigurable architectures embedded system mobile handsets ip networks streaming media mobile computing computer architecture circuits access protocols multiaccess communication;ip networks;circuits;system on chip solutions;mobile computing;computational efficiency;power;computer telephony integration;multiaccess communication;technological forecasting	Internet is becoming one of the key features of tomorrow's communication world. The evolution of mobile phones networks such as UMTS will soon allow everyone to be connected everywhere. These new network technologies bring the abilit), to deal not only with classical voice or text messages, but also with improved content: multimedia. At the mobile level, this kind of data oriented content requires highly efJicient architectures; and nowadays embedded system-on-chip solutions will no longer be able to manage the critical constraints like area, power and data computing efficiency. In this paper we will propose a new dynamically reconfigurable network, dedicated to data oriented applications such as the one targeted for instance on third generation networks. Principles, realisations and comparative results will be exposed for some classical applications, targeted on different architectures.	embedded system;internet;mobile phone;reconfigurability;reconfigurable computing;system on a chip	Gilles Sassatelli;Gaston Cambon;Jérôme Galy;Lionel Torres	2001		10.1109/IWRSP.2001.933835	chip;embedded system;electronic circuit;real-time computing;the internet;telephone network;computer science;operating system;power;mobile computing;umts frequency bands;computer network	EDA	1.8497341454611644	57.95760641284501	118832
02f71001bc40d077cfd851b3d4e72407503968b5	a network based functional verification method of ieee 1394a phy core	topology;network topology network based functional verification method ieee 1394a phy core very complex digital systems design verification coverage reaching processors verification communication network designs;protocols;coverage reaching;pediatrics;communication networks;ieee standards;functional verification;peripheral interfaces;design flow;network topology;formal verification;monitoring;community networks;continuous improvement;digital systems;protocols formal verification ieee standards peripheral interfaces;very complex digital systems;communication network designs;design verification;processors verification;network based functional verification method;physical layer communication networks continuous improvement design methodology digital systems acceleration design automation signal generators signal design network topology;ieee 1394a phy core;parallel processing;design methodology	The continuous improvement on the design methodologies and processes has made possible the creation of huge and very complex digital systems. Design verification is one of the main tasks in the design flow, aiming to certify the system functionality has been accomplished accordingly to the specification. A simulation based technique known as functional verification has been followed widely. In recent years, articles in functional verification have been presented, focusing on specific design verification and on methods to improve and accelerate coverage reaching. The majority of the papers are aimed to processors verification, while communication network designs were not such commonly reported. In the present paper, we present a novel and efficient network based functional verification method applied to an IEEE 1394a PHY core. The method presented reduces verification time, by automation in generating input signals from network, and parallelism in multi designs under verification of different physical and logical roles in network topology. The method can also be used in the functional verification of similar communication network designs.	central processing unit;design flow (eda);digital electronics;ieee 1394;network topology;phy (chip);parallel computing;simulation;telecommunications network;test bench	Colin Yu Lin;Song Cao;Junshe An;Fei Han;Qifei Fan	2008	2008 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2008.44	verification and validation of computer simulation models;real-time computing;verification;software verification;physical verification;computer science;theoretical computer science;distributed computing;high-level verification;runtime verification;intelligent verification;functional verification	EDA	4.863304147415869	56.15613100133706	118999
856515ea27152cfad97f536470e8bd1fd3a9a85a	an optimized mapping algorithm based on simulated annealing for regular noc architecture	simulated annealing energy consumption multiprocessing systems network on chip power aware computing;network on chip;simulated annealing;power aware computing;artificial neural networks;energy consumption;pipelines;artificial neural networks optimization pipelines;optimization;mapping;multiprocessing systems;noc;multi core system;system simulation;high performance;average energy consumption reduction optimized mapping algorithm simulated annealing network on chip architecture interconnect demands multicore system task mapping noc design packet latency multicore architecture task allocation;multi core system simulated annealing mapping noc;artificial neural network	Network on chip (NoC) architecture is viewed as a potential solution for the interconnect demands of the emerging multi-core systems since it renders the system high performance, flexibility and low-cost. Mapping tasks onto different cores of the network is a critical phase in NoC design because it determines the energy consumption and packet latency. In order to reduce the energy consumption of applications running on multi-core architecture, we propose a new mapping strategy based on Simulated Annealing (SA). By allocating tasks that have big communication volume to adjacent places on the mesh, the proposed method overcomes the shortcoming of blind search in traditional SA. The experiment results reveal that the solutions generated by the proposed algorithm reduce average energy consumption by 56.56% in mapping 16 tasks and 66.32% in mapping 49 tasks compared with traditional Simulated Annealing (SA).1	algorithm;intel core (microarchitecture);multi-core processor;multiprocessing;network on a chip;network packet;rendering (computer graphics);simulated annealing	Liulin Zhong;Jiayi Sheng;Ming-e Jing;Zhiyi Yu;Xiaoyang Zeng;Dian Zhou	2011	2011 9th IEEE International Conference on ASIC	10.1109/ASICON.2011.6157203	parallel computing;real-time computing;computer science;distributed computing	EDA	-2.2301540144203718	58.96549018636183	119050
e1db73594cd334b95cecf7b5296e551170362cfb	modeling energy-time trade-offs in vlsi computation	computers;computational modeling wires semiconductor device modeling schedules algorithm design and analysis computers integrated circuit modeling;energy constraint minimization;models of computation;energy time trade offs;power constrained environment vlsi computation energy time trade off modeling power consumption vlsi circuits parallel computation cmos circuits energy constrained execution time energy time cost communication distance energy constraint minimization;energy constrained execution time;time trade off;performance evaluation;integrated circuit;sorting;computer model;cmos circuits;wires;energy time cost;communication distance;energy requirement;parallel computation;power aware computing;scheduling algorithm;computational modeling;energy time trade off modeling;energy aware computing models of computation energy time trade offs parallel computation;vlsi circuits;semiconductor device modeling;cmos logic circuits;integrated circuit modeling;schedules;parallel computer;vlsi;energy aware computing;vlsi computation;power consumption;power constrained environment;algorithm design and analysis;vlsi cmos logic circuits parallel processing performance evaluation power aware computing sorting;parallel processing;lower bound	The performance of today's computers is limited primarily by power consumption rather than the number of instructions executed. Because the energy required to perform an operation using VLSI circuits drops rapidly with the time allowed for the operation, many slow processors can complete a parallel computation using less time and less energy than a fast uniprocessor that can execute the best sequential algorithm. This motivates designing algorithms for minimum execution time subject to energy constraints. We propose a simple model for analyzing algorithms that reflects the energy-time trade-offs of CMOS circuits. Using this model, we derive lower bounds for the energy-constrained execution time of sorting, addition, and multiplication, each with bitwise inputs, and we present algorithms that meet these bounds. These lower bounds are based on the energy-time costs of communication distance, rather than bisectional bandwidth arguments typical of area-time lower bounds. We show that minimizing time under energy constraints is not the same as minimizing operation count or computation depth. This work establishes a tractable method for the evaluation of parallel computations in a power-constrained environment.	adder (electronics);algorithm design;analysis of algorithms;approximation algorithm;binary number;bit-level parallelism;bitwise operation;bottleneck (software);cmos;cell (microprocessor);central processing unit;cobham's thesis;computer performance;dynamic voltage scaling;embedded system;graphics processing unit;hoc (programming language);image scaling;model of computation;network topology;parallel computing;privacy-enhanced electronic mail;programmer;run time (program lifecycle phase);scheduling (computing);sequential algorithm;shortest path problem;software design;sorting;spectral leakage;supercomputer;uniprocessor system;very-large-scale integration	Brad D. Bingham;Mark R. Greenstreet	2012	IEEE Transactions on Computers	10.1109/TC.2011.40	embedded system;parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;very-large-scale integration;algorithm	Arch	-1.3673095684514531	54.97398382213848	119284
df7675299b0e9c64f22a01f94279b223b6e8d716	optimal synthesis of application specific heterogeneous pipelined multiprocessors	libraries;mixed integer linear program;graph theory;minimization;pipelined design style;optimisation;parameter minimization;sos;network synthesis;formal model;cost function;tool support;convergence of numerical methods;specialized heterogeneous multiprocessors;task flow graphs;algorithm design and analysis flow graphs hdtv libraries integrated circuit technology network synthesis cost function humans convergence of numerical methods minimization;initiation rate;optimisation special purpose computers pipeline processing multiprocessing systems graph theory linear programming integer programming;flow graphs;optimal synthesis;numerical convergence;special purpose computers;nonpipelined case;mixed integer linear programming;integer programming;system synthesis;integrated circuit technology;binary variables;hdtv;linear programming;cost application specific heterogeneous pipelined multiprocessors optimal synthesis formal model specialized heterogeneous multiprocessors task flow graphs sos system synthesis mixed integer linear programming pipelined design style computation time nonpipelined case binary variables numerical convergence parameter minimization initiation rate latency;humans;latency;multiprocessing systems;computation time;cost;algorithm design and analysis;pipeline processing;application specific heterogeneous pipelined multiprocessors	In this paper we present a technique and formal model for optimal synthesis of specialized heterogeneous multiprocessors, given task flow graphs to be executed in a pipelined (periodic) fashion. SOS is a formal appnmch to system aynthesis using mixed integer-linear programming, ensuring optimally of the final solutions. SOS was extended to cover the pipelined design style. The eztensions wem made while tying to avoid a considerable increase in computation time over the non-pipelined case. The eztensions include new binay variables as well as new constraints used to ensum numerical convergence. The present tool supports minimization of pammeters such as initiation mte, latency and cost.	computation;linear programming;mathematical model;numerical analysis;time complexity	J. C. DeSouza-Batista;Alice C. Parker	1994		10.1109/ASAP.1994.331812	network synthesis filters;embedded system;algorithm design;computer architecture;latency;parallel computing;real-time computing;integer programming;computer science;linear programming;graph theory;theoretical computer science;operating system;programming language;algorithm	Logic	1.1979855554218255	53.18416474031736	119351
547ea136ef71f1ed19ba751d4e69ac63580fcf45	the reliability of fpga circuit designs in the presence of radiation induced configuration upsets	field programmable gate array;radiation effects;reliability;configuration fault simulation fpga circuit design space based remote sensing field programmable gate array single event upset slaac 1v computing board based simulation configuration memory ground based radiation testing seu mitigation;circuit faults;fault simulation;radiation effect;configuration fault simulation;space based remote sensing;circuit design;simulation;testing;fpga;ground based radiation testing;slaac 1v computing board based simulation;circuit simulation;mathematical methods and computing;accuracy;simulators;computational modeling;circuit reliability;remote sensing;shift registers;field programmable gate arrays computational modeling single event transient single event upset space vehicles remote sensing circuit faults circuit simulation circuit testing laboratories;single event transient;radiation hardening electronics;radiations;mitigation;circuit testing;single event upset;configuration memory;low earth orbit;circuit reliability field programmable gate arrays radiation hardening electronics fault simulation remote sensing circuit simulation shift registers;field programmable gate arrays;configuration;simulation tool;simulation environment;seu mitigation;space vehicles	FPGAs are an appealing solution for space-based remote sensing applications. However, in a low-earth orbit, FPGAs are susceptible to Single-Event Upsets (SEUs). In an effort to understand the effects of SEUs, an SEU simulator based on the SLAAC-1V computing board has been developed. This simulator artifically upsets the configuration memory of an FPGA and measures its impact on FPGA designs. The accuracy of this simulation environment has been verified using ground-based radiation testing. This simulation tool is being used to characterize the reliability of SEU mitigation techniques for FPGAs.	bitstream;checksum;ddos mitigation;failure rate;field-programmable gate array;input/output;random-access memory;routing;sensor;simulation;single event upset;virtex (fpga)	Michael J. Wirthlin;Darrel Eric Johnson;Nathan Rollins;Michael P. Caffrey;Paul S. Graham	2003		10.1109/FPGA.2003.1227249	embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;field-programmable gate array	EDA	9.157971853832716	59.66689181417932	119742
0e6b8753eb0c362b9e99241b215f29056b97f39c	improving the effectiveness of tmr designs on fpgas with seu-aware incremental placement		TMR combined with configuration scrubbing is an effective technique to mitigate against radiation-induced CRAM upsets on SRAM-based FPGAs. However, its effectiveness is limited by low-level common mode failures due to the physical mapping of a design to the FPGA device. This paper describes how common mode failures are introduced during the implementation process and introduces an approach for resolving them through a custom incremental placement tool for Xilinx 7-Series FPGAs. Multiple designs across multiple generations of devices are shown to be sensitive to common mode failures. Applying the incremental placement technique yields an improvement of 10,721x over an unmitigated design through fault-injection testing. Radiation testing is then performed to show that the MTTF of this technique is 91,500 days in GEO orbit, a 367x improvement over the unmitigated design and a 5x improvement over baseline TMR.	baseline (configuration management);benchmark (computing);data scrubbing;fault injection;field-programmable gate array;high- and low-level;list of content management frameworks;mean time between failures;netlist;single event upset;static random-access memory;triple modular redundancy	Matthew Cannon;Andrew M. Keller;Michael Wirthlin	2018	2018 IEEE 26th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)	10.1109/FCCM.2018.00031	static random-access memory;physical mapping;field-programmable gate array;real-time computing;embedded system;maintenance engineering;common-mode signal;geosynchronous orbit;mean time between failures;computer science	EDA	8.128958559679145	59.558489457180805	119858
1c0526e657401cfb6400bbc4d217b84452bb29a2	process oriented power management	dynamic voltage scaling;power aware computing embedded systems linux operating system kernels;embedded systems;power aware computing;operating system;power consumption control;power management;energy management energy consumption power system management voltage control control systems frequency runtime switches operating systems dynamic voltage scaling;process oriented power management mechanism;linux;power consumption;operating system kernels;dynamic frequency scaling;linux kernel 2 6;linux kernel 2 6 operating systems power consumption control dynamic voltage scaling dynamic frequency scaling process oriented power management mechanism;dynamic voltage and frequency scaling;operating systems	Though modern operating systems are capable of controlling power consumption using the DVFS (dynamic voltage and frequency scaling) mechanism, power consumption is controlled for some duration according to the runtime statistics. Thus, some interactive processes suffer in response time when the system is slowed. This paper proposes the process-oriented power management mechanism (POPM), that controls the operating speed of each process separately, instead of some interval. When a process context switch occurs, POPM determines the most appropriate speed for the next process, and changes CPU frequency to the corresponding value. In order to determine the speed of each process, POPM analyzes the runtime information of each process: e.g., I/O or CPU usage. We develop a prototype of POPM on the Linux Kernel 2.6, and evaluate it on a Laptop PC. Our experimental results show that POPM reduces power consumption of the system without reminding users that the system is slowed down.	abstraction layer;algorithm;central processing unit;clock rate;context switch;dynamic frequency scaling;dynamic voltage scaling;embedded system;image scaling;input/output;inter-process communication;interaction;interactivity;laptop;linux;modern operating systems;operating system;power management;prototype;response time (technology);run time (program lifecycle phase);scheduling (computing);x window system	Daisuke Miyakawa;Yutaka Ishikawa	2007	2007 International Symposium on Industrial Embedded Systems	10.1109/SIES.2007.4297310	embedded system;real-time computing;computer science;operating system;linux kernel	Embedded	-4.470084262831855	57.24931749232513	119947
35494f6105e51c37b4369b649252a1e4e94d4da8	a graph theoretic approach to cache-conscious placement of data for direct mapped caches	graph theory;acces contenu;compilacion;optimisation;gestion memoire;teoria grafo;memory management;storage access;cache optimization;measurement;optimizacion;resolucion conflicto;data placement in cache;gollete estrangulamiento;storage management;heuristic method;performance;offline algorithms;metodo heuristico;cache memory;theorie graphe;antememoria;cache consciousness;memory access;performance programme;gestion memoria;antememoire;goulot etranglement;content access;resolution conflit;consciousness;retard;acces memoire;cache performance;data access;acceso contenido;acceso memoria;compilation;conscience;algorithms;eficacia programa;optimization;methode heuristique;program performance;conciencia;retraso;conflict resolution;bottleneck;data placement	Caches were designed to amortize the cost of memory accesses by moving copies of frequently accessed data closer to the processor. Over the years the increasing gap between processor speed and memory access latency has made the cache a bottleneck for program performance. Enhancing cache performance has been instrumental in speeding up programs. For this reason several hardware and software techniques have been proposed by researchers to optimize the cache for minimizing the number of misses. Among these are compile-time data placement techniques in memory which improve cache performance. For the purpose of this work, we concern ourselves with the problem of laying out data in memory given the sequence of accesses on a finite set of data objects such that cache-misses are minimized. The problem has been shown to be hard to solve optimally even if the sequence of data accesses is known at compile time. In this paper we show that given a direct-mapped cache, its size, and the data access sequence, it is possible to identify the instances where there are no conflict misses. We describe an algorithm that can assign the data to cache for minimal number of misses if there exists a way in which conflict misses can be avoided altogether. We also describe the implementation of a heuristic for assigning data to cache for instances where the size of the cache forces conflict misses. Experiments show that our technique results in a 30% reduction in the number of cache misses compared to the original assignment.	algorithm;amortized analysis;cpu cache;clock rate;compile time;compiler;data access;experiment;graph theory;heuristic	Mirza Beg;Peter van Beek	2010		10.1145/1806651.1806670	bus sniffing;cache language model;data access;least frequently used;pipeline burst cache;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;page cache;cpu cache;performance;cache;computer science;write-once;memory-level parallelism;graph theory;cache invalidation;operating system;conflict resolution;consciousness;conscience;database;smart cache;cache algorithms;cache pollution;measurement;non-uniform memory access;memory management	DB	-3.396618548222137	52.999233010671155	120105
7e08dcf008e6e5614996fb69182323eda8db0164	slack analysis in the system design loop	timing update;cycle time;time dependent;slack analysis;design flow;system design loop;system design;linear time;performance analysis	We present a system-level technique to analyze the impact of design optimizations on system-level timing dependencies. This technique enables us to speed up the design cycle by substituting, in the design the loop, the time-consuming simulation step with a fast timing update routine. As a result, we can significantly reduce the design time from on the order of hours/days to the order of seconds/minutes. The update algorithm is defined on the Transaction Level Model (TLM) and can be used by any design flow that invokes TLM-based optimizations. This algorithm has linear-time complexity in the program size and experimental results indicate that any loss of accuracy due to this technique is negligible (< ±1%); the benefit is a reduction in total design cycle time from several hours to a matter of seconds.	algorithm;simulation;slack variable;systems design;time complexity	Girish Venkataramani;Seth Copen Goldstein	2008		10.1145/1450135.1450189	parallel computing;real-time computing;simulation;computer science	EDA	0.41171761328931844	57.12129128191484	120298
7456d113accf24de1ab93865012c5132cdbb6220	susceptible workload evaluation and protection using selective fault tolerance	selective fault tolerance;workload susceptibility analysis;susceptible workload;output deviations;permanent faults;transient faults	Lowpower fault tolerance design techniques trade reliability to reduce the area cost and the power overhead of integrated circuits by protecting only a subset of their workload or their most vulnerable parts. However, in the presence of faults not all workloads are equally susceptible to errors. In this paper, we present a low power fault tolerance design technique that selects and protects the most susceptible workload. We propose to rank the workload susceptibility as the likelihood of any error to bypass the logic masking of the circuit and propagate to its outputs. The susceptible workload is protected by a partial Triple Modular Redundancy (TMR) scheme. We evaluate the proposed technique on timing-independent and timing-dependent errors induced by permanent and transient faults. In comparison with unranked selective fault tolerance approach, we demonstrate a) a similar error coverage with a 39.7% average reduction of the area overhead or b) a 86.9% average error coverage improvement for a similar area overhead. For Responsible Editor: M. Goessel Mauricio D. Gutierrez mdga1g11@soton.ac.uk Vasileios Tenentes v.tenentes@soton.ac.uk Daniele Rossi d.rossi@westminster.ac.uk Tom Kazmierski tjk@soton.ac.uk 1 Electronics and Computer Science, University of Southampton, Southampton, UK 2 Applied DSP and VLSI Research Group, University of Westminster, Westminster, UK the same area overhead case, we observe an error coverage improvement of 53.1% and 53.5% against permanent stuck-at and transition faults, respectively, and an average error coverage improvement of 151.8% and 89.0% against timing-dependent and timing-independent transient faults, respectively. Compared to TMR, the proposed technique achieves an area and power overhead reduction of 145.8% to 182.0%.	computer science;emoticon;fault tolerance;integrated circuit;overhead (computing);tom;triple modular redundancy;very-large-scale integration	Mauricio D. Gutierrez;Vasileios Tenentes;Daniele Rossi;Tom J. Kazmierski	2017	J. Electronic Testing	10.1007/s10836-017-5668-7	computer science;real-time computing;fault tolerance;workload;triple modular redundancy;reliability engineering	EDA	7.333564680921439	59.931583306869456	120389
436ae8162c064d7dcfd9e0da8782721244dcdc56	mtac - a multithreaded vliw architecture for pram simulation	instruction level parallel;vliw processor;parallel computer;functional unit	Multithreaded processors are the key components for hiding the latency of long operations slowing down the execution of parallel programs, because they can execute other processes while a process is waiting for a long operation to complete. In this paper we outline and evaluate a MultiThreaded VLIW processor Architecture with functional unit Chaining (MTAC), which is specially designed for efficient PRAM simulation. According to experiments MTAC offers remarkably better performance than a basic pipelined RISC architecture and chaining improves the exploitation of instruction level parallelism to the level where the achieved speedup corresponds the number of functional units in a processor.	central processing unit;execution unit;experiment;instruction-level parallelism;parallel computing;simulation;speedup;thread (computing)	Martti Forsell	1997	J. UCS	10.3217/jucs-003-09-1037	computer architecture;parallel computing;real-time computing;execution unit;computer science;operating system	Arch	-3.958441427850095	50.33977539384316	120401
b0caf1f866ff4ae39715d678afb9b955311f0d49	variation tolerant design of a vector processor for recognition, mining and synthesis?	hw sw co design;software;voltage control;variation aware design;delays registers vector processors software voltage control arrays;variations;arrays;vector processor systems fault tolerant computing;variation tolerance;registers;hw sw co design vector processors variations variation aware design variation tolerance;vector processors;rms applications variation tolerant vector processor design recognition mining and synthesis domain specific application vector reduction operations processing element circuit delay microarchitectural state joint hardware software variation tolerance mechanism dynamic voltage control mechanism;delays	Variations have emerged as one of the most significant challenges facing the design of integrated circuits in nanoscale technologies. As a consequence, variation tolerant design has become essential at all levels of design abstraction.  In this work, we investigate the design of a variation tolerant vector processor for applications from the emerging domains of recognition, mining and synthesis (RMS). We demonstrate how leveraging domain-specific application and architectural characteristics can lead to new and highly effective variation tolerance mechanisms. A predominant fraction of the processing elements in the target processor perform vector reduction operations, which leads to two key properties that we exploit for variation tolerance. First, the circuit delay of a processing element can be bounded a few cycles in advance based on its micro-architectural state. Second, vector reduction operations may be decomposed by performing operations on smaller vectors and combining the partial results. These properties allow us to create a joint hardware-software variation tolerance mechanism, wherein the hardware is enhanced with the ability to predict timing errors during the execution of vector instructions and effectively preempt their occurrence, while software is tasked with restoring the correct outputs. We enhance the proposed scheme with a dynamic voltage control mechanism that further improves energy efficiency by exploiting variations in data characteristics seen across different applications. Our experiments on six RMS applications demonstrate that the proposed variation tolerant design technique achieves an average of 32% energy improvement over a traditional guardband based design.	architectural state;central processing unit;experiment;integrated circuit;preemption (computing);software framework;speech synthesis;vector processor	Vivek Joy Kozhikkottu;Swagath Venkataramani;Sujit Dey;Anand Raghunathan	2014	2014 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)	10.1145/2627369.2627636	embedded system;electronic engineering;parallel computing;real-time computing;computer science;engineering;operating system;processor register	Arch	-1.0444851026611555	56.575046295817565	120539
94532bf45422250e4b352de379a23099db017753	a multiple-isa reconfigurable architecture	software;transparent execution;code optimization;arrays;reconfigurable architecture;registers;binary translation;bt system multiple isa reconfigurable architecture hardware feature instruction set architecture code recompilation code adaptation legacy hardware issues embedded systems binary code transformation performance penalties dynamic two level binary translation system x86 code arm code dynamic reconfigurable array;optimization;reconfigurable architectures binary codes embedded systems instruction sets;hardware arrays software optimization benchmark testing registers;benchmark testing;transparent execution binary translation reconfigurable architecture code optimization;hardware	In these days, every new added hardware feature must not change the underlying instruction set architecture (ISA), in order to avoid adaptation or recompilation of existing code. Nevertheless, this need for compatibility imposes a great number of restrictions to the designers, because it keeps them tied to a specific ISA and all its legacy hardware issues. Considering that the market is mainly dominated by two different ISAs (and, very likely, more to come): x86, used in the general purpose field, and ARM, used in embedded systems, the need for another level (at the Instruction Set Architecture) of adaptability is evident. Binary Translation (BT) appears as a solution for that, since it is capable of transforming binary code so it can be executed on another target architecture. However, BT adds another layer between code and actual execution, therefore bringing huge performance penalties. To overcome this drawback, we propose a new mechanism based on a dynamic two-level binary translation system. The first level translates ARM or X86 code to an intermediate code, which will be optimized by the second level: a dynamic reconfigurable array. In this way, the designer can take advantage of a BT system and program for two different fields of application, without worrying about the underlying architecture. Even though two case studies are presented, the first BT level is easily expandable to other ISAs.	arm architecture;binary code;binary translation;embedded system;machine translation;x86	Fernanda M. Capella;Marcelo Brandalero;Jair Fajardo Junior;Antonio Carlos Schneider Beck;Luigi Carro	2013	2013 III Brazilian Symposium on Computing Systems Engineering	10.1109/SBESC.2013.23	dead code;computer architecture;parallel computing;real-time computing;computer science;redundant code;source code	Arch	-0.07980774232028166	48.59265983243912	120565
e78bf2961c8a7a13f00e5141825c91ca75a23de3	hyper-programmable architectures for adaptable networked systems	chip;programmable logic devices;network processor;application specific integrated circuits;programmable logic device	We explain how modern programmable logic devices have capabilities that are well suited for them to assume a central role in the implementation of networked systems, now and in the future. To date, such devices have featured largely in ASIC substitution roles within networked systems; this usage has been highly successful, allowing faster times to market and reduced engineering costs. We argue that there are many additional opportunities for productively using these devices. The requirement is exposure of their high inherent computational concurrency matched by concurrent memory accessibility, their rich on-chip interconnectivity and their complete programmability, at a higher level of abstraction that matches the implementation needs of networked systems. We discuss specific examples supporting this view, and present a highly flexible soft platform architecture at an appropriate level of abstraction from physical devices. This may be viewed as a particularly configurable and programmable type of network processor, offering scope both for innovative networked system implementation and for new directions in networking research. In particular, it is aimed at facilitating scalable solutions, matching differently resourced programmable logic devices to differing performance and sophistication requirements of networked systems, from cheap consumer appliances to high-end network switching.	accessibility;application-specific integrated circuit;computer programming;concurrency (computer science);network processor;programmable logic device;requirement;scalability	Gordon J. Brebner;Philip James-Roxby;Eric Keller;Chidamber Kulkarni	2004	Proceedings. 15th IEEE International Conference on Application-Specific Systems, Architectures and Processors, 2004.	10.1109/ASAP.2004.10037	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;programmable logic device;simple programmable logic device	EDA	0.2428395440124131	49.33191910233516	120625
3b5759cf629bd7ddab50760f54cf4cfa64bcca7d	hardware evolution - automatic design of electronic circuits in reconfigurable hardware by artificial evolution			electronic circuit;evolutionary algorithm;field-programmable gate array	Adrian Thompson	1998				EDA	9.026890885207319	48.97787999978474	120869
10941c452f31095b1a44efb9fba4e88581967828	gals-based lpsp: performance analysis of a novel architecture for low power high performance security processors	low power processor;high performance processor;dpa countermeasure;globally asynchronous locally synchronous gals;security processor	The past two decades have witnessed a revolution in the use of electronic devices in our daily activities. Increasingly, such activities involve the exchange of personal and sensitive data by means of portable and light weight devices. This implied the use of security applications in devices with tight processing capability and low power budget. Current architectures for processors that run security applications are optimized for either high-performance or low energy consumption. We propose an implementation for an architecture that not only provides high performance and low energy consumption but also mitigates security attacks on the cryptographic algorithms which are running on it. The proposed architecture of the GloballyAsynchronous Locally-Synchronous-based Low Power Security Processor (GALS-based LPSP) inherits the scheduling freedom and high performance from the dataflow architectures and the low energy consumption and flexibility from the GALS systems. In this paper, a prototype of the GALS-based LPSP is implemented as a soft core on the Virtex-5 (xc5-vlx155t) FPGA. The architectural features that allow the processor to mitigate Side-Channel attacks are explained in detail and tested on the current encryption standard, the AES. The performance analysis reveals that the GALS-based LPSP achieves two times higher throughput with one and a half times less energy consumption than the currently used embedded processors.	algorithm;central processing unit;clock rate;coupling (computer programming);cryptography;dataflow;dynamic-link library;embedded system;encryption;field-programmable gate array;globally asynchronous locally synchronous;interaction;performance evaluation;profiling (computer programming);prototype;scheduling (computing);throughput	Hala A. Farouk;Mahmoud T. El-Hadidi;Ahmed A. El-Farag	2012	IJNC	10.15803/ijnc.2.1_56	embedded system;parallel computing;real-time computing;computer science	Arch	-1.8976301684001373	54.681875515943	120966
105f0af9de778ad3f1c9a56ee60e0f2cbba8a83f	hardware-based text-to-braille translator	algorithm reconfiguration;field programmable gate array;the australian standard research classification 210000 science general;markov theory;braille translation;fpga;chip;thesis;algorithms design languages verification fpga design braille;vhdl;fpgas;assistive technology;vhol;hardware description language;high speed;hardware implementation	This paper describes the hardware implementation of a text to Braille Translator using Field-Programmable Gate Arrays (FPGAs). Different from most commercial software-based translators, the circuit presented is able to carry out text-to-Braille translation in hardware. The translator is based on the translating algorithm, proposed by Paul Blenkhorn [1]. The Very high speed Hardware Description Language (VHDL) was used to describe the chip in a hierarchical way. The test results indicate that the hardware-based translator achieves the same results as software-based commercial translators.	algorithm;braille translator;commercial software;field-programmable gate array;hardware description language;vhdl	Xuan Zhang;Cesar Ortega-Sanchez;Iain Murray	2006		10.1145/1168987.1169029	hardware compatibility list;computer architecture;computer science;theoretical computer science;operating system;field-programmable gate array	EDA	6.515306487889589	48.49739376705819	121138
360a2faf34e5f6ec13dfe1f7ea054f46e70d00ee	fast power and performance evaluation of fpga-based wireless communication systems	performance evaluation;system modeling;wireless communication;logic gates;mathematical model;radio networks field programmable gate arrays performance evaluation;field programmable gate arrays;systemc wireless communication field programmable gate array power consumption system modelling;power demand;performance evaluation wireless communication field programmable gate arrays power demand system modeling mathematical model logic gates;maximal relative error performance evaluation fpga based wireless communication systems power consumption evaluation wireless communication baseband systems field programmable gate arrays power estimation performance estimation fpga devices power performance tradeoff low level characterization process high level system modeling hardware baseband processing speedup factor	In this paper, a new and efficient methodology is proposed to quickly and precisely evaluate the power consumption and performance of wireless communication base-band systems implemented in field-programmable gate arrays (FPGAs). As the complexity of such systems is still growing, being able to estimate both power and performance of a design has become a major issue. FPGA devices constitute a promising technology in this highly constrained context. However, to respect their power budget, designers need to explore the design space very soon in the design process. This is performed prior to any implementation. Based on the innovative definition of a scenario, which enables comparison among wireless communication applications in a formal manner, each parameter can be evaluated to meet the power-performance tradeoff. In this paper, the proposed methodology is realized in two steps using a low-level characterization process and high-level system modeling. Another major contribution consists in considering components' time activity to refine power estimations results. We demonstrate the effectiveness of the proposed methodology throughout several domain-specific use cases, with a focus on hardware base-band processing in the wireless communication domain. As compared with current low-level FPGAs vendor tools, an important speedup factor is obtained, and a maximal relative error lower than 5% is reached.	approximation error;field-programmability;field-programmable gate array;high- and low-level;maximal set;performance evaluation;speedup;systems modeling	Jordane Lorandel;Jean-Christophe Prévotet;Maryline Hélard	2016	IEEE Access	10.1109/ACCESS.2016.2559781	embedded system;real-time computing;systems modeling;logic gate;telecommunications;computer science;electrical engineering;operating system;mathematical model;wireless;field-programmable gate array;statistics	EDA	3.4584319802752397	55.366438727585255	121211
02221b85fce108f542158ef87cf8b6b7ff1a99d3	flexible application software generation for heterogeneous multi-processor system-on-chip	design process;multiple software stacks;difference operator;motion jpeg decoder flexible application software generation heterogeneous multiprocessor system on chip multimedia applications i o components multiprocessor architectures hardware dependent software hardware abstraction layer time to market software generation flow multiple software stacks;multimedia applications;multimedia application;communication model;multimedia computing;hardware abstraction layer;time to market multimedia computing multiprocessing systems software architecture;software architecture;multiprocessor architectures;motion jpeg;operating system;heterogeneous multiprocessor system on chip;software generation flow;application software system on a chip hardware operating systems computer architecture software libraries buildings process design time to market communication system software;hardware dependent software;multiprocessor architecture;multi processor system on chip;flexible application software generation;motion jpeg decoder;time to market;multiprocessing systems;off the shelf;i o components	Multimedia applications require heterogeneous multiprocessor architectures with specific I/O components in order to achieve computation and communication performances. The different processors run different software stacks, which are composed by the application s tasks and a hardware dependent software (HDS). The HDS contains an operating system, a specific communication library and a hardware abstraction layer (HAL), granting accesses to hardware resources. Building these software stacks may be the trouble maker of the MP-SoC design process when trying to reduce its time-to-market. In this paper, we present our application software generation flow and tools starting from a high level application model. They are able to handle heterogeneous MP-SoC, running multiple software stacks while using different operating systems and communication models. The application software generation tool builds the application s sofware stacks by producing optimized and multi-tasked C code and using a flexible operating system and communication programming interfaces management. In order to validate the effectiveness of our approach, we generated the software stacks of a Motion JPEG decoder, partitioned and mapped on an off-the-shelf multimedia platform.	abstraction layer;central processing unit;computation;hal;hardware abstraction;high-level programming language;holographic data storage;input/output;jpeg;multimedia framework;multiprocessing;operating system;performance;software architecture;system on a chip	Xavier Guerin;Katalin Popovici;Wassim Youssef;Frédéric Rousseau;Ahmed Amine Jerraya	2007	31st Annual International Computer Software and Applications Conference (COMPSAC 2007)	10.1109/COMPSAC.2007.117	embedded system;software architecture;computer architecture;real-time computing;models of communication;design process;software sizing;computer science;package development process;software framework;component-based software engineering;software development;software design description;operating system;software engineering;monolithic application;software construction;resource-oriented architecture;software deployment;motion jpeg;software system	Embedded	3.6206098048372115	50.87957384779869	121238
471bfa6b0a8c884d5d772154e258fadc546cdad2	reduction of fpga resources for regular expression matching by relation similarity	look up table;flip flops field programmable gate arrays fpga regular expression matching relation similarity intrusion detection systems malicious traffic multi gigabit networks pattern matching formal methods nondeterministic finite automata look up tables;decoding;logic design;flip flops;formal methods;fpga;look up tables;formal method;relation similarity;field programmable gate arrays table lookup pattern matching computer science decoding estimation automata;automata;table lookup field programmable gate arrays flip flops logic design pattern matching security of data;estimation;pattern matching;regular expression matching;intrusion detection systems;multi gigabit networks;computer science;field programmable gate arrays;malicious traffic;table lookup;security of data;regular expression;nondeterministic finite automata;flip flop;intrusion detection system	Intrusion Detection Systems have to match large sets of regular expressions to detect malicious traffic on multi-gigabit networks. Many algorithms and architectures have been proposed to accelerate pattern matching, but formal methods for reduction of Nondeterministic finite automata have not been used yet. We propose to use reduction of automata by similarity to match larger set of regular expressions in FPGA. Proposed reduction is able to decrease the number of states by more than 32% and the amount of transitions by more than 31%. The amount of look-up tables is reduced by more than 15% and the amount of flip-flops by more than 34%.	algorithm;automata theory;flops;field-programmable gate array;finite element method;finite-state machine;flip-flop (electronics);formal methods;gigabit;lookup table;nondeterministic finite automaton;pattern matching;regular expression	Vlastimil Kosar;Jan Korenek	2011	14th IEEE International Symposium on Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2011.5783121	intrusion detection system;embedded system;electronic engineering;parallel computing;formal methods;lookup table;computer science;theoretical computer science;operating system;distributed computing;programming language;algorithm;field-programmable gate array	Arch	9.141622367944395	47.11355148621589	121250
66c07bdbbff38166040c761a35adc127ab553c5a	enforcing architectural contracts in high-level synthesis	microarchitecture registers contracts hardware computer architecture equations pipelines;e unification;microarchitecture;reconfigurable architectures;contracts;microarchitecture architectural contracts high level synthesis;synthesis;computer architecture;high level synthesis;registers;architectural contracts;pipelines;synthesis architecture microarchitecture e unification;reconfigurable architectures contracts high level synthesis languages;architecture;languages;hardware	We present a high-level synthesis technique that takes as input two orthogonal descriptions: (a) a behavioral architectural contract between the implementation and the user, and (b) a microarchitecture on which the architectural contract can be implemented. We describe a prototype compiler that generates control required to enforce the contract, and thus, synthesizes the pair of descriptions to hardware.	algorithm;compiler;emoticon;high- and low-level;high-level synthesis;microarchitecture;prototype	Nikhil A. Patil;Ankit Bansal;Derek Chiou	2011	2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2024724.2024909	computer architecture;parallel computing;real-time computing;microarchitecture;computer science;architecture;operating system	EDA	2.24108243772873	51.254918715235505	121291
021ee0ea711ee1df2e1c099d697d4f85ab0b13a3	locality-conscious workload assignment for array-based computations in mpsoc architectures	multiprocessor system on a chip;locality optimization strategy workload assignment array based computations mpsoc architectures multiprocessor system on a chip data locality load balance parallel architectures;frequency synchronization;programming language;design engineering;application software;data locality;locality optimization strategy;parallel programming;mpsoc architectures;system on a chip;computer architecture;embedded systems;parallel architectures;logic programming;permission;system on chip;programming profession;workload assignment;design verification;load balance;experimental evaluation;computer science;parallel architecture;array based computations;microprogramming;parallel architectures system on chip benchmark testing embedded systems microprocessor chips microprogramming;computer architecture programming profession application software parallel programming permission logic programming frequency synchronization computer science design engineering parallel processing;mpsoc;parallel processing;benchmark testing;microprocessor chips	While the past research discussed several advantages of multiprocessor-system-on-a-chip (MPSOC) architectures from both area utilization and design verification perspectives over complex single core based systems, compilation issues for these architectures have relatively received less attention. Programming MPSOCs can be challenging as several potentially conflicting issues such as data locality, parallelism and load balance across processors should be considered simultaneously. Most of the compilation techniques discussed in the literature for parallel architectures (not necessarily for MPSOCs) are loop based, i.e., they consider each loop nest in isolation. However, one key problem associated with such loop based techniques is that they fail to capture the interactions between the different loop nests in the application. This paper takes a more global approach to the problem and proposes a compiler-driven data locality optimization strategy in the context of embedded MPSOCs. An important characteristic of the proposed approach is that, in deciding the workloads of the processors (i.e., in parallelizing the application) it considers all the loop nests in the application simultaneously. Our experimental evaluation with eight embedded applications shows that the global scheme brings significant power/performance benefits over the conventional loop based scheme.	central processing unit;compiler;computation;embedded system;interaction;load balancing (computing);locality of reference;mpsoc;mathematical optimization;multiprocessing;parallel computing;system on a chip	Feihui Li;Mahmut T. Kandemir	2005	Proceedings. 42nd Design Automation Conference, 2005.	10.1145/1065579.1065609	system on a chip;embedded system;parallel processing;computer architecture;parallel computing;real-time computing;loop fission;computer science;loop nest optimization;operating system;programming language	EDA	-4.5335784934690615	50.599761586168206	121348
85fca174d87117fc027d3180e64c467e36bde991	a fault tolerant hardware based file system manager for solid state mass memory	fault tolerant systems hardware file systems solid state circuits memory management degradation fault tolerance routing satellites field programmable gate arrays;degradation;memory management;architecture reliability;fault tolerant;routing;storage management;fpga;dynamic routing;fpga implementation;solid state circuits;satellite system fault tolerant hardware file system manager solid state mass memory fpga architecture reliability graceful degradation;semiconductor storage storage management fault tolerant computing field programmable gate arrays memory architecture;fault tolerant computing;fault tolerant systems;solid state mass memory;memory architecture;file system;satellites;fault tolerance;graceful degradation;satellite system;settore ing inf 01 elettronica;distributed file system;fault tolerant hardware;field programmable gate arrays;semiconductor storage;hardware implementation;file system manager;file systems;hardware	In rhis paper the hardwore implementation of a file sysfem manager for afrmlt tolerant Solid Srate Mass Memory (SSMM) is presented. A hardwore implcmenrarion ofthefile sysrem manager implies the following advantages: ad hoc faulr rolerant design and graceful .degradation capability The former meanr developing special fault tolerant hardwore for each file system basic function (read, write and delete). For each function different faulr toleranr techniques have been opplied by conridering the impact of different faults on the architecture reliability. Also rhe area overhead introduced by the chosenfault tolerant reehnique hm been evaluated. Graceful degradation is obtained in terms of data conneclion reconfiguration and reducedfuncrionality set. We exploited the modulariry ofrhe design IO implement a distriburedfile sysrem by means of local handlers on each memory module connected to a dynamic routing module. Thefile sysrem manager has been used in a SSMMoriented IO satellite applicatiom. An FPGA implementation for the complete SSMM has been obtained in order IO evaluate the performances and reliability of the SSMM architecture and in particular of rhe file syslem manager.	elegant degradation;fault tolerance;field-programmable gate array;gigabyte;graceful exit;hoc (programming language);memory module;multiuser dos;overhead (computing);performance;routing;socket.io;solid-state drive	Gian-Carlo Cardarilli;Marco Ottavi;Salvatore Pontarelli;Marco Re;Adelio Salsano	2003		10.1109/ISCAS.2003.1206396	embedded system;fault tolerance;parallel computing;real-time computing;computer science;engineering;operating system	Embedded	7.430101609873446	59.09906741205885	121434
366c5c5b5bfa779d8db71907570b37dd6788aa3e	embedded processor validation environment using a cycle-accurate retargetable instruction-set simulator	retargetable;architecture description language;reference model;system on chip soc;instruction set simulator;system on chip;cycle accurate;architecture description language adl;validation;embedded processor;application specific instruction set processor	In the advent of System-on-Chip (SoC) technology, validation scope is further expanded from a single core to the system. Modules are also substituted by application specific instruction-set processors (ASIP) in order to raise abstraction level of systems from signals to instructions. As embedded processors are diversified according to their specific application, they suffer from an increasing number of irregular constraints and architectural idiosyncrasies. They also have control paths of pipeline which shows quite complicated timing. In order to alleviate these design complexities, validation must take retargetability and cycle-accuracy into consideration. We have thus proposed efficient embedded processor validation environment (EPVE) using a cycle-accurate retargetable instruction-set simulator (CARISS) as a reference model. The designed CARISS is based on an architecture description language (ADL), which provides improved retargetability for instruction-set machines. It also uses a scheduling method which can capture complex processor behavior more accurately than the ones used by the previous ADLs. We have applied the proposed EPVE for the 32bit embedded processors and investigated effectiveness of our approach by analyzing statistics on detected errors.	32-bit;abstraction layer;application-specific instruction set processor;architecture description language;central processing unit;computer architecture simulator;embedded system;instruction set simulator;mpsoc;reference model;scheduling (computing);system on a chip;test vector	Hoonmo Yang;Moonkey Lee	2005	The Journal of Supercomputing	10.1007/s11227-005-0218-y	system on a chip;architecture description language;computer architecture;parallel computing;real-time computing;computer architecture simulator;reference model;computer science;operating system	EDA	2.435438723394201	52.21486412994983	121468
90fdc8e75cf39b175cea7cd06c5227a01fe72c5b	online architectures: a theoretical formulation and experimental prototype	resource utilization;online algorithm;reconfigurable computing;run time reconfiguration;fpga;run time reconfigurable;power consumption;online architecture;trace driven simulation	This article describes a class of reconfigurable computing system called online architectures. These architectures use an online algorithm to make run-time reconfiguration decisions that continually adapt the underlying architecture to match the application’s current computational demand. Online architectures have several potential advantages, including better resource utilization (reduced cost), faster execution, and reduced (static) power consumption. However, to realize these benefits, online architectures must balance the overhead (reconfiguration, profiling, and decision costs) against expected gains of reconfiguration. In this article, the basic foundation of online architecture is formulated, core challenges enumerated, and results reported based on a simple prototype and trace-driven simulations. These results suggest that the overhead is manageable and that a more comprehensive investigation is worthwhile.	computer architecture;online algorithm;overhead (computing);prototype;reconfigurable computing;reduced cost;simulation	Ron Sass;Brian Greskamp;Brian Leonard;Jeff Young;Srinivas Beeravolu	2006	Microprocessors and Microsystems	10.1016/j.micpro.2006.02.020	embedded system;online algorithm;in situ resource utilization;parallel computing;real-time computing;reconfigurable computing;computer science;operating system;distributed computing;field-programmable gate array	Arch	-3.002030115137226	51.19694211639268	121608
52c395a82eb1a81289bc07e4c760e2ac6c295bb0	a framework for parallelizing load/stores on embedded processors	optimising compilers;corporate acquisitions;microprocessors;digital signal processing;enumeration;dual bank memory;register allocation;partitioned memory banks;graph colouring simulated annealing embedded systems optimising compilers;registers educational institutions electrostatic precipitators corporate acquisitions hardware bandwidth process design digital signal processing chips digital signal processing microprocessors;memory resident values;program representation;simulated annealing;process design;embedded systems;graph coloring problem;range of motion;registers;embedded processors;control flow;electrostatic precipitators;dictionary;np complete problem embedded processors load stores parallelization partitioned memory banks x y memory dual bank memory memory resident values iterated coalescing based register allocator graph coloring problem;bandwidth;digital signal processing chips;x y memory;load stores parallelization;instruction scheduling;configurable code generation;embedded processor;resource modeling;variable instruction set;np complete problem;graph colouring;hardware;iterated coalescing based register allocator	Many modern embedded processors (esp. DSPs) support partitioned memory banks (also called X-Y memory or dual bank memory) along with parallel load/store instructions to achieve code density and/or performance. In order to effectively utilize the parallel load/store instructions, the compiler must partition the memory resident values into X or Y bank. This paper gives a post-register allocation solution to merge the generated load/store instructions into their parallel counterparts. Simultaneously, our framework performs allocation of values to X or Y memory banks.We first remove as many load/stores and register-register moves through an excellent iterated coalescing based register allocator by Appel and George [14]. We then attempt to maximally parallelize the generated load/stores using a multi-pass approach with minimalgrowth in terms of memory requirements. The first phase of our approach attempts the merger of load stores without replication of values in memory. We model this problem in terms of a graph coloring problem in which each value is colored X or Y. We then construct a Motion Scheduling Graph (MSG) based on the range of motion for each load/store instruction. MSG reflects potential instructions which could be merged. We propose a notion of pseudo-fixedboundaries so that the load/store movement is minimally affected by register dependencies. We prove that the coloring problem for MSG is NP-complete. We then propose a heuristic solution, which minimally replicates load/stores on different control flow paths if necessary.Finally, the remaining load/stores are tackled by register rematerialization and local conflicts are eliminated. Registers are re-assigned to create motion ranges if opportunities are found for merger which are hindered by local assignment of registers. We show that our framework results in parallelization of a large number of load/stores without much growth in data and code segments.	central processing unit;code segment;compiler;control flow;embedded system;graph coloring;heuristic;iteration;memory bank;np-completeness;parallel computing;processor register;register allocation;rematerialization;requirement	Xiaotong Zhuang;Santosh Pande;John S. Greenland	2002		10.1109/PACT.2002.1106005	process design;parallel computing;real-time computing;np-complete;simulated annealing;range of motion;computer science;theoretical computer science;operating system;digital signal processing;graph coloring;processor register;instruction scheduling;programming language;control flow;enumeration;register allocation;load/store architecture;bandwidth	PL	-1.3042948709117432	51.89733016159307	121697
63130c608806102b11eda33855d6983a1aa805f2	application-specific processors and system-on-chips for embedded and pervasive applications			central processing unit;embedded system;pervasive informatics;system on a chip	Nadia Nedjah;Lech Józwiak;Luiza de Macedo Mourelle	2013	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2013.08.004	parallel computing;computer science	EDA	4.491281646620377	49.87045987683246	121714
ddd9c9c3ab5e0750ae6b91d414715551cb9fb043	noc: network or chip?	layered architecture;multiple instance;network design;noc optimization problem;logic design;network on chip;point to point;wireless network;silicon chips noc optimization problem network layer architectures;network on chip circuit optimisation computer architecture logic design;interconnection network;chip;optimization problem;home network;computer architecture;community networks;congestion control;network on a chip design optimization communication networks joining processes costs scalability telecommunication network reliability constraint optimization multiprocessor interconnection networks home automation;cost effectiveness;quality of service;network layer architectures;circuit optimisation;constrained optimization problem;silicon chips	Summary form only given. The concept of a communication network emerged, many times in the past, for connecting a large number of systems, replacing dedicated point-to-point connection and other small-scale interconnection mechanisms. Each network needs to provide a cost effective solution for a large number of possibly conflicting requirements such as flexibility, scalability, reliability and performance. Therefore, the task of network architects and designers is to solve multiple instances of a complex constrained optimization problem resulting in numerous and diverse network solutions. For example, there are different standards and architectures associated with interconnection networks, home networks, LANs, MANs, WANs and wireless networks. In this paper, we map the common lessons and concepts from the networking research to the emerging NoC field. We argue that the NoC optimization problem consists of several distinguished types that should lead to multiple diverse solutions. NoC network layer architectures pose new challenges in exploring solutions to traditional networking problems such as routing, quality-of-service, flow and congestion control and reliability. The unique characteristics of silicon chips require new solutions to these classical problems, and define a new set of NoC specific problems, such as automatic network design process, power and area optimization and specialized system functionalities. We speculate which class of solutions is likely to fit the different NoC types	constrained optimization;constraint (mathematics);integrated circuit;interconnection;mathematical optimization;network congestion;network on a chip;network planning and design;optimization problem;point-to-point protocol;quality of service;requirement;routing;scalability;telecommunications network	Israel Cidon	2007	First International Symposium on Networks-on-Chip (NOCS'07)	10.1109/NOCS.2007.33	chip;optimization problem;embedded system;network planning and design;logic synthesis;real-time computing;cost-effectiveness analysis;quality of service;point-to-point;computer science;multitier architecture;wireless network;distributed computing;network on a chip;network congestion;computer network	Arch	3.2512564356333766	59.98566973580881	121776
1ed3c6377907a9cc346e63768a496eae16a0eee6	mitigating soft errors in system-on-chip design	transistor downsizing;cmos integrated circuits;fault tolerant soft error mitigation system on chip design reliability cmos technology transistor downsizing high performance computer architecture nanotechnology;error correction codes;cmos technology;fault tolerant cmos soft errors system on chip;fault tolerant;system on chip design reliability;nanotechnology;system on a chip;system on a chip cmos technology fault tolerance integrated circuit noise computer errors power system reliability semiconductor device noise fault tolerant systems costs electromagnetic interference;computer architecture;integrated circuit design;system on chip cmos integrated circuits fault tolerance integrated circuit design integrated circuit reliability;redundancy;fault tolerant systems;system on chip;high performance computer architecture;fault tolerance;high performance computer;next generation;soft errors;soft error mitigation;integrated circuit reliability;soft error;cmos;hardware	With the continuous downscaling of CMOS technologies, the reliability has become a major bottleneck in the evolution of the next generation scaling. Technology trends such as transistor downsizing, use of new materials and high performance computer architecture continue to increase the sensitivity of systems to soft errors. Today the technologies are moving into the period of nanotechnologies and system-on-chip (SoC) designs are widely used in most of the applications, the issues of soft errors and reliability in complex SoC designs are set to become and increasingly challenging. This paper gives a review to the soft error in SoC designs and then presents the fault tolerant solution.	cmos;computer architecture;downscaling;fault tolerance;image scaling;next-generation network;soft error;supercomputer;system on a chip;transistor	Hai Yu;Fan Xiaoya	2008	2008 The 9th International Conference for Young Computer Scientists	10.1109/ICYCS.2008.413	embedded system;real-time computing;computer science;cmos	EDA	9.839225962010975	58.369621625667364	121807
244b73d622f4e403b88c787db4645cd465aedff5	a new processor architecture for digital signal transport systems	hardware design languages;processor architecture;atm communication processor architecture digital signal transport systems protocols flexible system high performance system application specific hardware cpu computer simulation;control systems;protocols;degradation;application specific hardware;performance evaluation;frequency estimation;telecommunication computing;high performance system;transport system;software performance;transport protocols;computer architecture;virtual machines;application specific integrated circuits;signal processing;signal processing central processing unit field programmable gate arrays control systems transport protocols software performance hardware design languages frequency estimation pipeline processing degradation;virtual machines computer architecture signal processing protocols asynchronous transfer mode telecommunication computing performance evaluation application specific integrated circuits;atm communication;field programmable gate arrays;digital signal transport systems;cpu;high performance;computer simulation;flexible system;asynchronous transfer mode;central processing unit;pipeline processing	This paper proposes a new processor architecture for manipulating protocols in digital signal transport systems. To realize flexible and high-performance digital signal transport systems, the architecture has unique application-specific hardware with a core CPU. It is derived from an analysis of functions in real systems. A computer simulation confirms the efficiency of the architecture.	microarchitecture	Minoru Inamori;Kenji Ishii;Akihiro Tsutsui;Kazuhiro Shirakawa;Hiroshi Nakada;Toshiaki Miyazaki	1997		10.1109/ICCD.1997.628863	computer simulation;reference architecture;embedded system;digital signal processor;space-based architecture;computer architecture;electronic engineering;real-time computing;digital signal;computer science;applications architecture;cellular architecture;operating system;central processing unit;signal processing;transport triggered architecture;data architecture;systems architecture;systems design	EDA	3.764541124685566	47.77246531346609	121861
47e6effdde181d65c1d5416ace2bc3e0614df8e6	illustrative design space studies with microarchitectural regression models	pareto optimisation;prediction error;logic design;microarchitecture computational modeling sampling methods predictive models costs space exploration pareto analysis pipelines design optimization performance analysis;regression analysis logic design microprocessor chips pareto optimisation;regression model;design space;benchmark optimal architectures microarchitectural regression models design space evaluation design space optimization design space sampling statistical inference pareto frontier analysis pipeline depth analysis multiprocessor heterogeneity analysis pareto optima;statistical inference;regression analysis;computational efficiency;power modeling;microprocessor chips	We apply a scalable approach for practical, comprehensive design space evaluation and optimization. This approach combines design space sampling and statistical inference to identify trends from a sparse simulation of the space. The computational efficiency of sampling and inference enables new capabilities in design space exploration. We illustrate these capabilities using performance and power models for three studies of a 260,000 point design space: (1) Pareto frontier analysis, (2) pipeline depth analysis, and (3) multiprocessor heterogeneity analysis. For each study, we provide an assessment of predictive error and sensitivity of observed trends to such error. We construct Pareto frontiers and find predictions for Pareto optima are no less accurate than those for the broader design space. We reproduce and enhance prior pipeline depth studies, demonstrating constrained sensitivity studies may not generalize when many other design parameters are held at constant values. Lastly, we identify efficient heterogeneous core designs by clustering per benchmark optimal architectures. Collectively, these studies motivate the application of techniques in statistical inference for more effective use of modern simulator infrastructure	benchmark (computing);cpu cache;cluster analysis;computation;design space exploration;heuristic;mathematical optimization;mean squared error;microarchitecture;multiprocessing;pareto efficiency;pipeline (computing);sampling (signal processing);scalability;simulation;sparse matrix	Benjamin C. Lee;David M. Brooks	2007	2007 IEEE 13th International Symposium on High Performance Computer Architecture	10.1109/HPCA.2007.346211	mathematical optimization;computer science;machine learning;regression analysis	Arch	1.4432742933440927	56.55652805287246	121987
eef97b005fdccbea8053248837ecfa1e2a81d9a3	hardware compilation based on communicating processes			compiler;logic synthesis	Richard Sandiford	2001				PL	5.774924876727226	50.31001226209538	122066
4cbdc69d1ef5587e7a61e6a7015154e421c1a0f2	fault-tolerant vertical link design for effective 3d stacking	3d chip fault tolerant vertical link design effective 3d stacking memory bandwidth limitation chip multiprocessors cmp external memory through silicon vias tsv;fault tolerant;network on chip;articulo;chip multiprocessor;wires;memory management stacking fault tolerant systems three dimensional displays;through silicon via;three dimensional;chip;fault tolerant system;fault tolerant systems;three dimensional displays;fault tolerance;it adoption;storage management chips;three dimensional integrated circuits fault tolerance microprocessor chips network on chip storage management chips;3d stacking;external memory;noc;switches;memory bandwidth;three dimensional integrated circuits;microprocessor chips;fault tolerance noc 3d stacking;through silicon vias	Recently, 3D stacking has been proposed to alleviate the memory bandwidth limitation arising in chip multiprocessors (CMPs). As the number of integrated cores in the chip increases the access to external memory becomes the bottleneck, thus demanding larger memory amounts inside the chip. The most accepted solution to implement vertical links between stacked dies is by using Through Silicon Vias (TSVs). However, TSVs are exposed to misalignment and random defects compromising the yield of the manufactured 3D chip. A common solution to this problem is by over-provisioning, thus impacting on area and cost. In this paper, we propose a fault-tolerant vertical link design. With its adoption, fault-tolerant vertical links can be implemented in a 3D chip design at low cost without the need of adding redundant TSVs (no over-provision). Preliminary results are very promising as the fault-tolerant vertical link design increases switch area only by 6.69% while the achieved interconnect yield tends to 100%.	fault tolerance;memory bandwidth;provisioning;stacking;three-dimensional integrated circuit	Carles Hernández;Antoni Roca;Jose Flich;Federico Silla;José Duato	2011	IEEE Computer Architecture Letters	10.1109/L-CA.2011.17	embedded system;fault tolerance;parallel computing;computer science;network on a chip	Arch	6.085483654132555	59.8496295224603	122315
f8586f27dced575cde13893ae2b9098b9bc2f840	battery-efficient task execution on reconfigurable computing platforms with multiple processing units	field programmable gate array;parallel processing reconfigurable architectures field programmable gate arrays multiprocessing systems system on chip power consumption benchmark testing;iterative algorithms;reconfigurable computing;reconfigurable architectures;energy use;chip;multiple processing unit;total power;system on chip;energy consumption;heuristic algorithms;displays;voltage;field programmable gate arrays tiles voltage iterative algorithms benchmark testing hardware energy consumption displays heuristic algorithms algorithm design and analysis;precedence task graph battery efficient task execution reconfigurable computing platform multiple processing unit soft processor embedded processor field programmable gate array voltage scalable processor;soft processor;reconfigurable computing platform;voltage scalable processor;tiles;multiprocessing systems;task graphs;power consumption;field programmable gate arrays;precedence task graph;embedded processor;hardware implementation;algorithm design and analysis;parallel processing;benchmark testing;battery efficient task execution;heuristic algorithm;hardware	"""This paper presents a battery-efficient task execution methodology on reconfigurable computing (RC) Platforms which have multiple processing units. These processing units can be on-chip in the form of soft-processors, embedded processors or """"Reconfigurable Tiles"""" where the reconfigurable area of an field programmable gate array (FPGA) is divided into fixed reconfigurable slots. Processing units can also be off-chip in the form of individual FPGAs and voltage-scalable processors. An application is modeled in the form of a precedence task graph. We assume that for each task in the task graph several different design-points are available which correspond to different voltage-frequency combinations for processors and different hardware implementations for FPGAs and """"Reconfigurable Tiles"""". It is assumed that performance and total power consumption estimates for each design-point are available for any given implementation, including the peripheral components such as memory and display power usage. First we present an iterative heuristic algorithm for a single processing unit, which finds a sequence of tasks along with an appropriate design-point for each task, such that a deadline is met and the amount of battery energy used is as small as possible. Next, we extend this algorithm to multiple processing units in an RC platform. We used several real-world benchmarks to test the effectiveness of this methodology. Each benchmark was executed on one, two, three and four processing units and its power utilization was characterized by implementing it on a portable RC Platform called iPACE-VI. We present the results which show that choosing an appropriate execution mode is crucial for battery-efficient execution. We also show that parallel execution on multiple-processing units can actually be more battery-efficient than sequential execution on a single processing unit under certain circumstances."""	algorithm;assignment problem;benchmark (computing);central processing unit;embedded system;field-programmable gate array;heuristic (computer science);iterative method;multiprocessing;peripheral;reconfigurable computing;scalability	Jawad Khan;Ranga Vemuri	2005	19th IEEE International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2005.122	parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	EDA	-0.6022127537861386	53.19680581617763	122366
1c755050a1cbdf8fddbac4a78774662b4434bc7a	very rapid prototyping of wearable computers: a case study of vuman 3 custom versus off-the-shelf design methodologies	wearable computers;rapid prototyping;embedded system design;wearable computer;experimental measurement;off the shelf;design methodology	The Wearable Computer Project is a testbed integrating research on rapid design and prototyping. Based on representative examples from six generations of wearable computers, the paper focuses on the differences in rapid prototyping using custom des ign versus off-the-shelf components. The attributes characterizing these two design styles are defined and illustrated by experimental measurements. The off-the-shelf approach required ten times the overhead, 30% more cost, fifty times the storage resources, 20% more effort, five times more power, but 30% less effort to port software than the embedded approach. An evaluation of the Vuman 3 design is presented to show its superior advantages in comparison to the off-the-shelf approach.	documentation;embedded system;emoticon;logistics;overhead (computing);rapid prototyping;software portability;testbed;wearable computer	Asim Smailagic;Daniel P. Siewiorek;Richard Martin;John Stivoric	1998	Design Autom. for Emb. Sys.	10.1023/A:1008850609458	embedded system;simulation;wearable computer;computer science;operating system	EDA	4.25278342873013	50.16427885279815	122416
38b12876246d48ccfb1d27f16c0fb6b1f9f30c3e	designing a risc cpu in reversible logic	software;program assemblers;sequential elements;syrec reversible circuits synthesis design cpu hardware describing language;logic design;reversible circuits;syrec;registers central processing unit logic gates radiation detectors hardware software adders;radiation detectors;reduced instruction set computing;hardware description languages;synthesis;software programs;reduced instruction set computing assembly language hardware description languages logic design program assemblers;design technique;reversible logic;logic gates;hardware describing language;registers;assembly language;adders;assembler language;design;risc cpu design;hardware description language;cpu;assembler language reversible logic risc cpu design sequential elements hardware description languages software programs;circuit synthesis;central processing unit;hardware	Driven by its promising applications, reversible logic received significant attention. As a result, an impressive progress has been made in the development of synthesis approaches, implementation of sequential elements, and hardware description languages. In this paper, these recent achievements are employed in order to design a RISC CPU in reversible logic that can execute software programs written in an assembler language. The respective combinational and sequential components are designed using state-of-the-art design techniques.	assembly language;benchmark (computing);central processing unit;circuit design;combinational logic;hardware description language;integrated circuit;janus;mathematical optimization;processor design;reversible computing;sequential logic	Robert Wille;Mathias Soeken;Daniel Große;Eleonora Schönborn;Rolf Drechsler	2011	2011 41st IEEE International Symposium on Multiple-Valued Logic	10.1109/ISMVL.2011.39	embedded system;computer architecture;parallel computing;computer science;central processing unit;hardware description language;programming language;register-transfer level;assembly language	Arch	7.05186829140387	51.71424123935935	122487
5ff277fe0920fbbb49dea6b591d5651cca776e0f	deployment of run-time reconfigurable hardware coprocessors into compute-intensive embedded applications	flexible hardware;hardware accelerators;reconfiguration controller;fpga dynamic partial reconfiguration;run time reconfigurable computing	Day after day, embedded systems add more compute-intensive applications inside their end products: cryptography or image and video processing are some examples found in leading markets like consumer electronics and automotive. To face up these ever-increasing computational demands, the use of hardware accelerators synthesized in field-programmable gate arrays (FPGA) lets achieve processing speedups of orders of magnitude versus their counterpart CPU-based software approaches. However, the inherent increment in physical resources penalizes in cost. To address this issue, dynamically reconfigurable hardware technology definitively reached its maturity. SRAM-based reconfigurable logic goes beyond the classical conception of static hardware resources distributed in space and held invariant for the entire application life cycle; it provides a new design abstraction featured by the temporal partitioning of such resources to promote their continuous reuse, reconfiguring them on the fly to play a different role in each instant. This new computing paradigm lets balance the design of embedded applications by partitioning their functionality in space and time--through a series of mutually-exclusive processing tasks synthesized multiplexed in time on the same set of resources--and achieving thus cost savings in both area and power metrics. However, the exploitation of this system versatility requires special attention to avoid performance degradation. Such technical aspects are addressed in this work intended to be a survey on reconfigurable hardware technology and aimed at defining an open, standard and cost-effective system architecture driven by flexible coprocessors instantiated on demand on reconfigurable resources of an FPGA. This concept fits well with the functional features demanded to many embedded applications today and its feasibility has been proved with a state-of-the-art commercial SRAM-based FPGA platform. The achieved results highlight dynamic partial reconfiguration as a potential technology to lead the next computing wave in the industry.	coprocessor;embedded system;field-programmable gate array;software deployment	Francisco Fons;Mariano Fons;Enrique Cantó;Mariano López	2012	Signal Processing Systems	10.1007/s11265-011-0607-9	embedded system;parallel computing;real-time computing;reconfigurable computing;computer science;electrical engineering;operating system	EDA	-1.2017370428347371	49.79674356212215	122520
f8a314dccadf727ecd170e5d3079d56cad145016	tackling the qubit mapping problem for nisq-era quantum devices		Due to little consideration in the hardware constraints, e.g., limited connections between physical qubits to enable two-qubit gates, most quantum algorithms cannot be directly executed on the Noisy Intermediate-Scale Quantum (NISQ) devices. Dynamically remapping logical qubits to physical qubits in the compiler is needed to enable the two-qubit gates in the algorithm, which introduces additional operations and inevitably reduces the fidelity of the algorithm. Previous solutions in finding such remapping suffer from high complexity, poor initial mapping quality, and limited flexibility and controllability. To address these drawbacks mentioned above, this paper proposes a SWAP-based BidiREctional heuristic search algorithm (SABRE), which is applicable to NISQ devices with arbitrary connections between qubits. By optimizing every search attempt, globally optimizing the initial mapping using a novel reverse traversal technique, introducing the decay effect to enable the trade-off between the depth and the number of gates of the entire algorithm, SABRE outperforms the best known algorithm with exponential speedup and comparable or better results on various benchmarks.	benchmark (computing);compiler;controlled not gate;heuristic;overhead (computing);quantum algorithm;qubit;run time (program lifecycle phase);sabre (computer system);scalability;search algorithm;speedup;time complexity;tree traversal	Gushu Li;Yufei Ding;Yuan Xie	2018	CoRR		compiler;parallel computing;qubit;speedup;fidelity;tree traversal;heuristic;quantum computer;computer science;quantum algorithm	PL	-0.801140134116027	48.077879130993374	122599
fc6e9674213a7814ed70739738f15c936546a2a3	energy-efficient fault tolerance approach for internet of things applications	energy efficiency;reliability;3d integration;machine learning;vfi;adaptive routing;noc	Fault tolerance (FT) is essential in many Internet of Things (IoT) applications, in particular in the domains such as medical devices and automotive systems where a single fault in the system can lead to serious consequences. Non-volatile memory (NVM), on the other hand, is commonly used to improve system reliability due to its unique properties to retain data even if the power supply is lost. However, one of the most important drawbacks of NVM is that it imposes significant overhead regarding timing and energy. In this paper, we have proposed a unique technique with the use of NVM to create FT application specific architecture with almost no timing overhead and low energy overhead.  We address the implementation of applications that are specified using synchronous data flow model of computation. We combine the use of NVM and classical CMOS transistors so that NVM judiciously stores selected complete states of the pertinent program. It allows the program to resume from the saved state in NVM when faults occur. The frequency of the state selection can be flexibly adjusted for an arbitrarily specified FT timing/energy overhead. Moreover, to find an optimal state selection (with low overhead), we have applied an improved min-cut max-flow algorithm. On a variety of typical benchmarks, the simulation results indicate that our approach incurs only a small overhead over lower bounds. It is also generic in a sense that it can be applied to a wide spectrum of underlying IoT architectures and computational models.	algorithm;benchmark (computing);cmos;computational model;data flow diagram;dataflow;fault tolerance;internet of things;max-flow min-cut theorem;maxima and minima;maximum flow problem;minimum cut;model of computation;non-volatile memory;overhead (computing);power supply;relevance;simulation;synchronous data flow;transistor;volatile memory	Teng Xu;Miodrag Potkonjak	2016	2016 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/2966986.2967034	embedded system;electronic engineering;parallel computing;real-time computing;adaptive routing;telecommunications;computer science;engineering;electrical engineering;operating system;reliability;network operations center;efficient energy use;algorithm;statistics	EDA	5.771166368254893	58.68400760761126	122687
e3292ccc319c75a7b304fb39ee579b1bb1083ad4	100fps camera-based ugv localization system using cyclone v fpsoc		This paper presents a camera-based localization system for controlling unmanned ground vehicles in a 2D space using Field Programmable System-on-Chip devices, whose architecture combines powerful hard processors and FPGA fabric in the same chip. The FPGA captures and preprocesses images in real time while the hard processor is in charge of most image processing tasks. In these architectures, efficient data exchange between the hardware and software subsystems is a key feature to achieve high performance. In this case, versatile image writers have been designed to efficiently share images between the FPGA and the processor. The proposed free-software solution has been tested in Cyclone V devices, and compared with a previous pure hardware version of the system, as well as with other localization systems. Experimental results demonstrate that this system achieves better accuracy and higher frame rate than non-commercial localization systems in the literature, while being less expensive than commercial (closed) ones.		Alexandre Muniz Garcia;Roberto Fernandez Molanes;Juan J. Rodríguez-Andina;José Fariña	2018	IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2018.8592717		Robotics	3.4489850622834846	46.74110825254332	122701
1124282faedf56c01ec1d147c7c378b5fc3c7c55	an architecture for fail-silent operation of fpgas and configurable socs	fault-tolerance;programmable routing;single event upset;fault detection;system on chip;fault tolerant;field programmable gate array	We present an architecture for fail-silent operation of Field Programmable Gate Arrays (FPGAs) and configurable System-on-Chip (SoC) implementations. The architecture includes replication of the system function separated by a guard band region that guarantees that no single fault can allow interaction between the replicated system functions. The outputs of the replicated system functions are monitored such that, once an error is detected, the FPGA is shut down through a sequence of operations to prevent the transmission of erroneous data. This architecture can detect single event upsets (SEUs) in the configuration memory of FPGAs and FPGA cores in configurable SoCs for subsequent scrubbing operations. The guard band is also particularly effective in isolating operational regions of an FPGA to help to reduce erroneous operation due to SEUs in the configuration memory. 1	fail-safe;fault tolerance;field-programmable gate array;halting problem;input/output;mpsoc;memory scrubbing;overhead (computing);single event upset;system on a chip;three-state logic;triangular function;triple modular redundancy	Lee Lerner;Charles E. Stroud	2006			parallel computing;system on a chip;field-programmable gate array;single event upset;real-time computing;fault tolerance;computer science;architecture;fault detection and isolation	OS	7.135180933671295	57.940873126243375	122730
7403fc58be07b1ef530285fa14fc43a4074ed392	distinguished paper: automatic local memory architecture generation for data reuse in custom data paths		Traditional high level synthesis is able to yield high computational resource utilisation and short critical paths. The shortcomings of the generated designs usually lies in the memory architecture. To achieve good performance on a FPGA, the data must reside in the fast on-chip memories, but these are commonly too small for the data being processed. Traditional high level synthesis cannot cope with this situation. In this paper we present a technique for automatic generation of a memory architecture, data paths and associated controllers from a high level language such as C. Data reused during the processing are stored in a local memory, resulting in high performance even when the data are stored in shared off-chip memory. The technique is based on data dependence and data access pattern analysis. Commonly used data are duplicated in on-chip memory. High memory efficiency is achieved by rearranging the data memory layout during copying. We have applied our technique to typical signal analysis tasks. The results show that the data path does not need to stall waiting for data, even when all data are stored in a shared off-chip memory. The experiments have been carried out on a Xilinx Virtex2 FPGA.	computation;computational resource;computer memory;cost efficiency;data access;data dependency;dependence analysis;experiment;field-programmable gate array;high-level programming language;high-level synthesis;memory architecture;memory bandwidth;memory controller;memory hierarchy;memory management;pattern recognition;signal processing	Per Andersson;Krzysztof Kuchcinski	2004			computer architecture;data diffusion machine;memory management;memory architecture;memory map;data architecture;reuse;applications architecture;uniform memory access;computer science	HPC	-4.523921567146983	51.03713032241248	122922
bc00baaf7b9eea1750d0059e0a5862a3288556ab	on resilient system performance binning	resilient system;yield improvement;performance binning	By allowing timing errors to occur and recovering them online, resilient systems can be used to eliminate the voltage/frequency guardband to improve energy efficiency/throughput. Due to the nature of fault tolerance computing, resilient systems have a different binning strategy from traditional circuits. In this paper, we study, for the first time, the binning metrics of resilient systems. We propose a solution for resilient system binning based on structural at-speed delay testing. Then an adaptive clock configuration technique is proposed for yield improvement. Experimental results demonstrate the effectiveness of our proposed binning method, and significant yield improvement accomplished by the adaptive clock configuration technique.	clock signal;fault tolerance;product binning;throughput	Qiang Han;Jianghao Guo;Qiang Xu;Wen-Ben Jone	2015		10.1145/2717764.2717774	real-time computing;simulation;computer science	EDA	7.914854698846499	60.07646867998569	123243
ffabeb5f8a41aab89029bf8d4384c0fdd6e27695	automating the design of mlut mpsopc fpgas in the cloud	instruction sets operating systems field programmable gate arrays hardware computer architecture ip networks;hardware software codesign;logic design;system on chip;internet mlut mpsopc fpga design million lut level heterogeneous multiprocessor system on chips vendor specific cad tools cloud processors automated system assembly approaches system complexity level automated design flow next generation heterogeneous mpsoc general purpose pthreads compliant hw sw co designed operating system heterogeneous compiler;table lookup field programmable gate arrays hardware software codesign logic design multiprocessing systems operating systems computers program compilers system on chip;multiprocessing systems;field programmable gate arrays;program compilers;table lookup;operating systems computers	Modern platform FPGAs are over the million-LUT level, large enough to support complete heterogeneous Multiprocessor System-On-Chips (MPSoCs). Constructing systems with 10's of processors is currently feasible using existing manual methods within vendor-specific CAD tools. However these manual, by-hand, approaches will not be feasible for constructing future systems with 100's to 1,000's of processors. Instead, new automated system assembly approaches will be required to handle these levels of system complexity and diversity. In this paper we present a new automated design flow for creating such next generation heterogeneous MPSoCs. An integral part of the MPSoPC system created is the inclusion of a general purpose PThreads-compliant HW/SW co-designed operating system and heterogeneous compiler. Our design flow has been placed in the cloud and is freely accessible across the Internet.	central processing unit;cloud computing;compiler;computer-aided design;design flow (eda);field-programmable gate array;floor and ceiling functions;internet;multiprocessing;operating system;posix threads;system on a chip	Eugene Cartwright;Azad Fahkari;Sen Ma;Christina Smith;Miaoqing Huang;David L. Andrews;Jason Agron	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339186	system on a chip;embedded system;computer architecture;parallel computing;logic synthesis;real-time computing;computer science;operating system;field-programmable gate array	EDA	2.1913121791790284	50.26001755404692	123246
c5250a135989b8c466eb8145344ef2a953bb37f8	mascot: microarchitecture synthesis of control paths	optimal method;design space;finite state machine	This paper presents MASCOT (MicroArchitecture Synthesis of ConTrol paths). This synthesis system constructs the optimal microarchitecture for a control path of an instruction set processor. Input to the system is the behavioural specification of a control path. This specification is in finite state machine form which is mapped initially onto a single programmed logic array (PLA) microarchitecture. The synthesis strategy then applies a sequence of decompositions on this initial microarchitecture. This strategy follows a decision scheme until all design objectives are met. It transforms the initial microarchitecture into a complex microarchitecture of several PLAs and ROMs. Where it is impossible to meet the design objectives, the system constructs a microarchitecture which comes as close as possible to given design objectives. Design objectives are allowed on floorplan dimensions 'and delay. Our strategy integrates a number of known optimization methods for specific microarchitectures. Therefore this synthesis method explores a larger part of the design space than do other control path synthesis methods. Other methods are mostly bound to one microarchitecture which they optimize. Our system is not only very flexible in microarchitecture construction but also open for extension by other optimizations.	control flow;finite-state machine;mascot;mathematical optimization;microarchitecture;programmable logic array;read-only memory	A. J. W. M. ten Berg	1994	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(94)90091-4	computer architecture;parallel computing;real-time computing;computer science;finite-state machine	EDA	1.2135417778533577	51.368546294109436	123420
30ddd6ac65988de2a65e4ea3cf174d10edf449d9	speeding up fault injection for asynchronous logic by fpga-based emulation	fpga based emulation;off the shelf fpga;circuit faults;fault tolerant;logic design asynchronous circuits fault tolerance field programmable gate arrays;logic design;asynchronous four state logic designs;logic emulation circuit faults robust stability circuit stability delay asynchronous circuits protection fault tolerance prototypes;synchronous circuits;delay insensitive;asynchronous design;asynchronous circuit;four state logic;asynchronous four state logic designs fault injection fpga based emulation synchronous circuits delay insensitive asynchronous circuits asynchronous logic fault tolerance prototyping platforms soft core processor off the shelf fpga;system recovery;soft core processor;logic gates;registers;fault tolerance;prototyping platforms;asynchronous circuits;asynchronous logic;delay insensitive asynchronous circuits;experimental evaluation;field programmable gate arrays;asynchronous processor design four state logic asynchronous design fault injection;fault injection;table lookup;encoding;off the shelf;asynchronous processor design	While stability and robustness of synchronous circuits becomes increasingly problematic due to shrinking feature sizes, delay-insensitive asynchronous circuits are supposed to provide inherent protection against various fault types. However, results on experimental evaluation and analysis of these fault tolerance properties are scarce, mainly due to the lack of suitable prototyping platforms. Using a soft-core processor as an example, this paper shows how an off-the-shelf FPGA can be used for asynchronous Four State Logic designs, on which future fault injection experiments will be conducted.	asynchronous circuit;delay insensitive circuit;emulator;experiment;fault injection;fault tolerance;field-programmable gate array;simulation;software prototyping;state logic;synchronous circuit	Marcus Jeitler;Jakob Lechner	2009	2009 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2009.35	embedded system;fault tolerance;computer architecture;real-time computing;asynchronous circuit;computer science;stuck-at fault	EDA	6.142820821986684	54.92124186819781	123770
9c152b4a6dd6b86291fe7d9e7cbd81c83da79e90	high-level scheduling model and control synthesis for a broad range of design applications	microcontrollers;multiphase clocking schemes;synthesis system;processor scheduling;computer peripheral equipment;efficient control synthesis methodology;microprocessors;data-dependent delays;control-dominated peripherals;microprocessor chips;multiple threading;scheduling model;cost-effective design methodology;control system cad;behavioral synthesis;multi-threading;high-level scheduling model;delays;broad range;relative scheduling;architectural synthesis;architectural power optimization.;multi-phase clocking;pipelining;architectural design application;control synthesis methodology;computer architecture;commodity integrated circuits;high-level synthesis;traditional datapath-dominated dsp application;control synthesis;high level synthesis;data-dependent delay;datapath-dominated digital signal processing;architectural design applications;pipeline processing;design methodology;multi threading;power optimization;cost effectiveness;digital signal processing	This paper presents a versatile scheduling model and an efficient control synthesis methodology which enables architectural (high-level) design/synthesis systems to seamlessly support a broad range of architectural design applications from datapath-dominated digital signal processing (DSP) to micro-processors/controllers and control-dominated peripherals, utilizing multi-phase clocking schemes, multiple threading, data-dependent delays, pipelining, and combinations of the above. The work presented in this paper is an enabling technology for high-level synthesis to go beyond traditional datapath-dominated DSP applications and to start becoming a viable and cost-effective design methodology for commodity ICs such as micro-processors/controllers and control-dominated peripherals.	central processing unit;clock rate;cryptography;data dependency;datapath;digital signal processing;encryption;hierarchical control system;high- and low-level;high-level synthesis;industrial pc;input/output;mathematical optimization;microcontroller;peripheral;pipeline (computing);scheduling (computing);thread (computing)	Chih-Tung Chen;Kayhan Küçükçakar	1997	1997 Proceedings of IEEE International Conference on Computer Aided Design (ICCAD)	10.1145/266388.266481	embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;cost-effectiveness analysis;multithreading;design methods;computer science;operating system;digital signal processing;high-level synthesis;pipeline;power optimization	EDA	1.9377624545761754	51.72610282896921	123788
e231345acbecfb2be92dd880cbb00913b3904f11	hardware/software cost analysis of interrupt processing strategies	architectural design;checkpoint repair hardware software cost analysis interrupt processing strategies architectural design implementation costs pipelined processors precise interrupts;hardware costs buffer storage history registers software maintenance counting circuits software algorithms;cost analysis;system recovery;system recovery interrupts pipeline processing;interrupts;pipeline processing	A new study compares the architectural design and implementation costs of five strategies that let pipelined processors support precise interrupts. Hardware dominates the cost of all strategies except checkpoint repair, which, depending on the implementation, can incur either high software or hardware costs.		Mansur H. Samadzadeh;Loai E. Garalnabi	2001	IEEE Micro	10.1109/40.928766	computer architecture;parallel computing;real-time computing;computer science;cost–benefit analysis;operating system;interrupt;interrupts in 65xx processors	Embedded	-3.9921682814912693	51.918441915434514	123792
59ea77bb92ac022f4ac235e06f1738772848185c	code selection for media processors with simd instructions	multimedia application;data type;media processor;assembly pattern matching dynamic programming libraries computer aided instruction computer science instruction sets signal processing algorithms computer applications instruments;signal processing;digital signal processing chips;source code;dsp media processors simd instructions signal processing algorithms code selection technique c source code compilers;program compilers;digital signal processing chips instruction sets program compilers signal processing parallel processing;parallel processing;instruction sets	Media processors show special instruction sets for fast execution of signal processing algorithms on different media data types. They provide SIMD instructions, capable of executing one operation on multiple data in parallel within a single instruction cycle. Unfortunately, their use in compilers is so far very restricted and requires either assembly libraries or compiler intrinsics. This paper presents a novel code selection technique capable of exploiting SIMD instructions also when compiling plain C source code. It permits to take advantage of SIMD instructions for multimedia applications, while still using portable source code.	algorithm;assembly language;central processing unit;compiler;experiment;instruction cycle;intrinsic function;library (computing);media processor;programming paradigm;simd;signal processing;software portability	Rainer Leupers	2000		10.1145/343647.343679	parallel processing;computer architecture;parallel computing;media processor;data type;computer science;operating system;signal processing;instruction set;redundant code;programming language;source code	HPC	0.9657801521192875	46.530220750830736	123875
23a1323fa3d702d10d385068e72ed82a2181f4f1	a model for peak matrix performance on fpgas	peak matrix performance;field programmable gate array;memory management;performance evaluation;conference_paper;computer model;fpga;matrix algebra;system on a chip;computational modeling;matrix operations;peak performance;model fpga matrix peak performance;model;bandwidth;matrix;field programmable gate arrays;matrix matrix multiplication;matrix algebra field programmable gate arrays;load modeling;matrix matrix multiplication peak matrix performance fpga matrix operations i o bandwidth matrix vector multiplication;matrix vector multiplication;i o bandwidth;field programmable gate arrays system on a chip computational modeling bandwidth performance evaluation load modeling memory management	Computations involving matrices form the kernel of a large spectrum of computationally demanding applications for which FPGAs have actively been utilized as accelerators. The performances of such matrix operations on FPGAs are related to underlying architectural parameters such as computational resources, memory and I/O bandwidth. A model that gives bounds on the peak performance of matrix-vector and matrix-matrix multiplication operations on FPGAs based on these parameters is presented. The architecture and efficiency of existing implementations are compared against the model. Future trends in matrix performance on FPGA devices are estimated based on the performance model and system parameters from the past decade.	computation;computational resource;field-programmable gate array;input/output;matrix multiplication;performance	Colin Yu Lin;Hayden Kwok-Hay So;Philip Heng Wai Leong	2011	2011 IEEE 19th Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2011.51	computer simulation;embedded system;parallel computing;computer science;theoretical computer science;field-programmable gate array	Arch	-2.7938887616393324	46.534350464685076	123897
0429557439d863af790f3ad2d0d237eb6317dcc1	tuning coarse-grained reconfigurable architectures towards an application domain	resource distribution coarse grained reconfigurable architecture application domain domain specific fpga design exploration methodology;dynamic reconfiguration;resource allocation;reconfigurable architectures;domain specific fpga;resource allocation field programmable gate arrays reconfigurable architectures;design exploration methodology;reconfigurable architectures cyclic redundancy check arithmetic field programmable gate arrays resource management registers context modeling clocks convolution design engineering;reconfigurable architecture;application domain;resource distribution;functional unit;field programmable gate arrays;coarse grained;domain specificity;coarse grained reconfigurable architecture	Design decisions, such as type and ratio of functional units, strongly determine the later flexibility of domain-specific FPGAs and coarse-grained dynamically reconfigurable arrays. For that reason the design of such reconfigurable architectures is done considering a set of target applications. However, it is not yet clear how to simultaneously consider all applications to determine an appropriate distribution of resources. In this paper, we deal with this problem through a design exploration methodology that exposes how each application contends in the use of coarse-grained resources. Our method is used to assess the adequacy of one given resource distribution answering two intermediary questions: 1) how evenly does it addresses all applications in the domain, and 2) how flexible remains the architecture on achieving diverse optimization goals	application domain;field-programmable gate array;mathematical optimization;reconfigurability;reconfigurable computing	Julio A. de Oliveira Filho;Thomas Schweizer;Tobias Oppold;Tommy Kuhn;Wolfgang Rosenstiel	2006	2006 IEEE International Conference on Reconfigurable Computing and FPGA's (ReConFig 2006)	10.1109/RECONF.2006.307755	embedded system;computer architecture;parallel computing;application domain;real-time computing;resource allocation;computer science;field-programmable gate array	EDA	0.39297796194520296	50.69663870731352	124065
3153467ed85f5670f0b5e92e717f14af5f8ac872	an efficient and flexible host-fpga pcie communication library	libraries;software;field programmable gate arrays libraries software hardware throughput ip networks registers;communication library fpga pcie efficiency flexibility;registers;ip networks;field programmable gate arrays;software libraries field programmable gate arrays peripheral interfaces;pcie gen2 x8 mode host fpga pcie communication library epee xilinx fpga half duplex aggregate full duplex aggregate;throughput;hardware	A high-performance interconnection between a host processor and FPGA accelerators is in much demand. Among various interconnection methods, a PCIe bus is an attractive choice for loosely coupled accelerators. Because there is no standard host-FPGA communication library, FPGA developers have to write significant amounts of PCIe related code at both the FPGA side and the host processor side. A high-performance host-FPGA PCIe communication library holds the key to broadening the use of FPGA accelerators. In this paper we target efficiency and flexibility as two important features in such a library. We discuss the challenges in providing these features, and present our solution to these challenges. We propose EPEE, an efficient and flexible host-FPGA PCIe communication library and describe its design. We implemented EPEE in various generations of Xilinx FPGAs with up to 26.24 Gbps half-duplex and 43.02 Gbps full-duplex aggregate throughput in the PCIe Gen2 X8 mode; these are at the best utilization levels that a host-FPGA PCIe library can achieve. The EPEE library has been integrated into four different FPGA applications with different data usage patterns in various institutes.	aggregate data;data rate units;duplex (telecommunications);field-programmable gate array;interconnection;loose coupling;pci express;throughput	Jian Gong;Tao Wang;Jiahua Chen;Haoyang Wu;Fan Ye;Songwu Lu;Jason Cong	2014	2014 24th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2014.6927459	embedded system;throughput;real-time computing;computer hardware;computer science;operating system;processor register;field-programmable gate array	EDA	-1.8654350459822926	48.19163478663463	124386
2ac4998d86e0face020bb886285b92a55253e14e	practical challenges of ilp-based spm allocation optimizations	integer linear programming;real time;compiler;wcet;optimization	Scratchpad Memory (SPM) allocation is a well-known technique for compiler-based code optimizations. Integer-Linear Programming has been proven to be a powerful technique to determine which parts of a program should be moved to the SPM. Although the idea is quite straight-forward in theory, the technique features several challenges when being applied to modern embedded systems. In this paper, we aim to bring out the main issues and possible solutions which arise when trying to apply those optimizations to existing hardware platforms.	compiler;embedded system;integer programming;linear programming;mathematical optimization;scratchpad memory;super paper mario	Dominic Oehlert;Arno Luppold;Heiko Falk	2016		10.1145/2906363.2906371	compiler;parallel computing;real-time computing;integer programming;computer science;programming language	Embedded	-1.5333800791753514	51.34390440583145	124389
45bb1b14489159f2f2f08e91c013afa118a8041c	exploration and synthesis of dynamic data sets in telecom network applications	application software;telecommunications;intellectual property;intelligent networks;system on a chip;hardware;dynamic data;cores;embedded systems;cost function;protocols;abstraction level;data structures;network synthesis;embedded system	We present a new exploration and optimization method to select customized implementations for dynamic data sets, as encountered in telecom network, database and multimedia applications. Our method fits in the context of embedded system synthesis for such applications, and enables to further raise the abstraction level of the initial specification, where dynamic data sets can be specified without low-level details. Our method is suited for hardware and software implementations. In this paper, it mainly aims at minimizing the memory power consumption, although it can also be driven by other cost functions such as area or performance. Compared with existing methods, it can save up to 2/3 of the memory power consumption and 3/4 of the memory area.	2.5d;abstraction layer;computer hardware;computer performance;database;dynamic data;embedded system;fits;high- and low-level;mathematical optimization	Chantal Ykman-Couvreur;J. Lambrecht;Diederik Verkest;Francky Catthoor;Hugo De Man	1999			embedded system;parallel computing;real-time computing;computer science	EDA	2.2594175710244992	52.595757813064964	124433
a14939ef6b22a2224a890e7a6b4b516c9300e655	improving design turnaround time via two-levels hw/sw co-simulation	computer debugging;design flow;hardware description languages;embedded system;computer debugging virtual machines hardware description languages real time systems high level synthesis tuning formal verification program debugging;high level synthesis;virtual prototyping;formal verification;tuning;virtual machines;design automation timing electronic mail software prototyping energy management software development management embedded software information resources field programmable gate arrays embedded system;program debugging;design flow management improved design turnaround time system tuning virtual prototyping two level hardware software co simulation embedded systems high level simulation architecture timing performance commercial vhdl cad tools functional debugging tradeoff analysis occam based system level model fine grain verification;real time systems	The steadily growing demand of fast turnaround time will shift system tuning from physical prototyping to virtual prototyping. The paper proposes a novel approach for mixed HW-SW implementation of embedded systems, allowing high-level simulation of the overall architecture as well as a deeper analysis of timing performance by exploiting commercial VHDL CAD tools. At the higher level, functional debugging and tradeoff analysis is performed on an OCCAM-based system-level model and at the lower level a VHDL-based description for both the HW and SW is built for fine grain verification of the system. The paper introduces the two levels of simulation, showing their impact in terms of design flow management and design time.	co-simulation;simulation	Alberto Allara;S. Filipponi;William Fornaciari;Fabio Salice;Donatella Sciuto	1997		10.1109/ICCD.1997.628901	embedded system;computer architecture;real-time computing;formal verification;computer science;virtual machine;design flow;operating system;logic simulation;hardware description language;algorithmic program debugging;high-level synthesis;programming language	EDA	2.9993266895126123	53.806848617897856	124467
c85bdbbfa7ad1bf93a781ff501894d7eafccf215	communication refinement in video systems on chip	communication interface;protocols;esprit omi cosy project communication refinement video systems intellectual property application level transactions virtual component interface system transactions system design methodology;spine;system transactions;intellectual property;video signal processing;application software;esprit omi cosy project;performance of systems;video processing;functional programming;system on a chip;communication refinement;computational modeling;permission;systems analysis;system on chip;ip;application level transactions;system design;virtual component interface;industrial property;video signal processing industrial property systems analysis;video systems;hardware implementation;system on a chip application software hardware delay intellectual property functional programming protocols spine permission computational modeling;system design methodology;hardware	Two levels of interfaces are introduced for modules of Intellectual cation-level transactions are used for programming the functionality of the system. They are refined into system transactions when mapping to the architecture model that underpins our approach. This supports the definition of IPs that target software and/or hardware implementation and can be hooked to N any n bus through the Virtual Component Interface of the VSI Alliance. We implement this approach, including a model for assessing the performance of system transactions, in a system design methodology that is refined in the ESPRIT/OMI COSY project.	algorithm;computation;digital video broadcasting;electrical connector;refinement (computing);semiconductor;system integration;system on a chip;systems design;video processing	Jean-Yves Brunel;Erwin A. de Kock;Wido Kruijtzer;H. J. H. N. Kenter;W. J. M. Smits	1999		10.1145/301177.301511	embedded system;real-time computing;simulation;computer science	EDA	4.632762589162069	52.6101414852104	124533
ba66fe2035c9a499e676c13113922fd17550573d	fast and scalable hybrid functional verification and debug with dynamically reconfigurable co-simulation	databases;hardware design languages;debugging;debug;functional issues hybrid functional verification dynamically reconfigurable cosimulation debug systems logic emulators software simulators scalability problem system on chip designs soc design under test dut logic blocks piecewise cosimulator design recompilation phase;co simulation;clocks;debug co simulation scalability;emulation;program verification;system on chip;synchronization;system on chip circuit cad digital simulation program debugging program verification;circuit cad;scalability;program debugging;hardware design languages emulation synchronization debugging clocks load modeling databases;load modeling;digital simulation	Hybrid functional verification and debug systems which combine high execution speed of logic emulators and full observability and controllability of software simulators are widely used, but suffer from scalability problem since software simulators cannot handle large and complex System-on-chip (SoC) designs efficiently, restricting their application to only relatively small designs. This paper presents a completely scalable hybrid verification and debug system based on dynamically reconfigurable co-simulation. Unlike existing systems, it allows one or more component logic blocks of a SoC to run on simulator for debugging while rest of the design still runs on emulator. The full design under test (DUT) is run on emulator at near hardware speed for long test sequences, and on error detection one or more logic blocks are transparently switched over to simulation for debugging, initializing the system as a piecewise co-simulator. Logic blocks can be flexibly relocated between simulator and emulator dynamically, without going through time consuming design recompilation phase, allowing designers to quickly debug functional issues. Application of the system to verification of real complex designs shows the effectiveness of our approach.	co-simulation;complex system;complexity;debugging;device under test;emulator;error detection and correction;hybrid functional;logic block;reconfigurability;scalability;simulation;system on a chip;technical standard;very-large-scale integration	Somnath Banerjee;Tushar Gupta	2012	2012 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1145/2429384.2429406	embedded system;computer architecture;electronic engineering;parallel computing;real-time computing;computer science;operating system;programming language;background debug mode interface;debugging;functional verification	EDA	6.785743813480616	53.4143099799757	124636
4d581378c3d641452842b36942b5635c352fa6d4	finite-state-machine overlay architectures for fast fpga compilation and application portability	intermediate fabrics;fpga;virtual architecture;overlays	Despite significant advantages, wider usage of field-programmable gate arrays (FPGAs) has been limited by lengthy compilation and a lack of portability. Virtual-architecture overlays have partially addressed these problems, but previous work focuses mainly on heavily pipelined applications with minimal control requirements. We expand previous work by enabling more flexible control via overlay architectures for finite-state machines. Although not appropriate for control-intensive circuits, the presented architectures reduced compilation times of control changes in a convolution case study from 7 hours to less than 1 second, with no performance overhead and an area overhead of 0.2%.	compiler;convolution;field-programmability;field-programmable gate array;finite-state machine;overhead (computing);requirement;software portability	Patrick Cooke;Lu Hao;Greg Stitt	2015	ACM Trans. Embedded Comput. Syst.	10.1145/2700082	embedded system;parallel computing;real-time computing;computer science;operating system;overlay;field-programmable gate array	Arch	0.3810998345248841	49.27938490499821	124673
eb469d12dfc3ac21d86e9cc2b18c4a3b08844c73	transient fault analysis of cordic processor	probability circuit optimisation digital arithmetic fault location fault tolerance;probability;circuit faults computer architecture transient analysis fault tolerance fault tolerant systems adders logic gates;fault tolerance;digital arithmetic;fault tolerant improvement transient fault analysis cordic processor spatial fault localization temporal fault localization probability fault impact coefficient arithmetic block optimized strategy fault tolerance penalty;circuit optimisation;selective hardening cordic arithmetic operators transient fault fault tolerance;fault location	In this paper, we propose a method to evaluate the impact of a transient fault in CORDIC processors. The proposed approach takes into account the spatial and temporal localization of the fault. It also embeds the probability that such a fault occurs. By defining a fault impact coefficient, it is possible to identify the most critical arithmetic blocks and thus to implement an optimized strategy for fault tolerance. We analyzed two structures of CORDIC processors and we showed how to get a better tradeoff between the penalty (area and delay overhead) and the fault tolerant improvement.	cordic;central processing unit;coefficient;fault tolerance;hardening (computing);iterative method;overhead (computing);performance	Ting An;Matteo Causo;Lirida A. B. Naviner;Philippe Matherat	2012	2012 19th IEEE International Conference on Electronics, Circuits, and Systems (ICECS 2012)	10.1109/ICECS.2012.6463644	electronic engineering;fault;parallel computing;real-time computing;fault coverage;fault indicator;computer science;stuck-at fault;software fault tolerance	Robotics	8.253780353297202	59.935017909238454	124903
66a9ebbea349d2a131f6a02909b0ea61732453da	highly efficient parallel atpg based on shared memory	shared memory;built in efficiency;automatic test pattern generation;dynamic fault partition;parallel atpg;test coverage;built in efficiency parallel atpg shared memory multicore machines dynamic fault partition;multicore machines	To leverage the computing power of multicore machines in ATPG, we developed a highly efficient parallel ATPG system based on dynamic fault partition and shared memory. The system takes advantage of built-in efficiency of parallel search to achieve good performance speedup with no sacrifices in pattern quality or test coverage.	built-in self-test;fault coverage;multi-core processor;shared memory;speedup	X. Cai;Peter Wohl;John A. Waicukauski;Pramod Notiyath	2010	2010 IEEE International Test Conference	10.1109/TEST.2010.5699236	shared memory;computer architecture;parallel computing;real-time computing;computer science;automatic test pattern generation;operating system;code coverage	HPC	-4.203069644804142	48.142575538326	124906
64ddeeb6fcc698317f8c7ee67c53ae63af655e32	a hardware accelerated semi analytic approach for fault trees with repairable components	monte carlo methods digital simulation fault trees field programmable gate arrays;field programmable gate array;fault tree;hardware accelerated semianalytic approach;circuit faults;research outputs;software based simulation program hardware accelerated semianalytic approach fault trees monte carlo simulation field programmable gate array;fpgas monte carlo simulation fault trees;software based simulation program;maintenance engineering;hardware accelerator;research publications;acceleration;electronic design automation and methodology;computational modeling;logic gates;complex system;integrated circuit modeling;fpgas;software tools;hardware acceleration fault trees field programmable gate arrays computational modeling integrated circuit modeling software tools circuit faults monte carlo methods electronic design automation and methodology;field programmable gate arrays;monte carlo simulation;monte carlo methods;digital simulation;fault tree analysis;hardware;fault trees	Fault tree analysis of complex systems with repairable components can easily be quite complicated and usually requires significant computer time and power despite significant simplifications. Invariably, software-based solutions, particularly those involving Monte Carlo simulation methods, have been used in practice to compute the top event probability. However, these methods require significant computer power and time. In this paper, a hardware-based solution is presented for solving fault trees. The methodology developed uses a new semi analytic approach embedded in a Field Programmable Gate Array (FPGA) using accelerators. Unlike previous attempts, the methodology developed properly handles repairable components in fault trees. Results from a specially written software-based simulation program confirm the accuracy and validate the efficacy of the hardware-oriented approach.	algorithm;complex systems;embedded system;fault tree analysis;field-programmable gate array;monte carlo method;monte carlo methods for option pricing;power supply unit (computer);semiconductor industry;simulation	Chakib Kara-Zaitri;Enver Ever	2009	2009 11th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2009.83	computer architecture;parallel computing;computer science;computer engineering	EDA	4.9102039667871	56.619551235442025	124936
07fd971a227cacfbbc5fbb92cfaf4b90c86191bb	a codesigned on-chip logic minimizer	access control list;hardware software codesign;minimisation of switching nets;logic minimization;system on a chip;embedded system;on chip logic minimization;network routing;chip;dynamic logic;embedded systems;logic synthesis;system on chip;hardware software partitioning;embedded cad;logic cad;on chip synthesis;microcomputers;dynamic optimization	Boolean logic minimization is traditionally used in logic synthesis tools running on powerful desktop computers. However, logic minimization has recently been proposed for dynamic use in embedded systems, including network route table reduction, network access control list table reduction, and dynamic hardware/software partitioning. These new uses require logic minimization to run dynamically as part of an embedded system's active operation. Performing such dynamic logic minimization on-chip greatly reduces system complexity and security versus an approach that involves communication with a desktop logic minimizer. An on-chip minimizer must be exceptionally lean yet yield good enough results. Previous software-only on-chip minimizer results have been good, but we show that a codesigned minimizer can be much better, executing nearly 8 times faster and consuming nearly 60% less energy, while yielding identical results.	access control list;access network;circuit minimization for boolean functions;desktop computer;embedded system;logic synthesis;logical connective;network access control;principle of good enough;routing table	Roman L. Lysecky;Frank Vahid	2003	First IEEE/ACM/IFIP International Conference on Hardware/ Software Codesign and Systems Synthesis (IEEE Cat. No.03TH8721)	10.1145/944645.944677	computer architecture;parallel computing;real-time computing;logic optimization;computer science	EDA	7.005866433977527	54.939990731903045	125082
47354a74e4b467114c684a33cd9941f94c494e6f	fastcuda: open source fpga accelerator &amp; hardware-software codesign toolset for cuda kernels	kernel;hardware software codesign;public domain software;integrated circuit design;parallel architectures;processing power fastcuda open source fpga accelerator hardware software codesign toolset cuda kernels hardware accelerators central cpu embedded design world opencl graphical processing unit programming gpu programming software toolset hardware architecture fpga design flow minimal user intervention embedded micro cpu logic capacity;graphics processing units;multicore processing;field programmable gate arrays;public domain software field programmable gate arrays graphics processing units hardware software codesign integrated circuit design parallel architectures;graphics processing units kernel field programmable gate arrays instruction sets multicore processing hardware;instruction sets;hardware	Using FPGAs as hardware accelerators that communicate with a central CPU is becoming a common practice in the embedded design world but there is no standard methodology and toolset to facilitate this path yet. On the other hand, languages such as CUDA and OpenCL provide standard development environments for Graphical Processing Unit (GPU) programming. FASTCUDA is a platform that provides the necessary software toolset, hardware architecture, and design methodology to efficiently adapt the CUDA approach into a new FPGA design flow. With FASTCUDA, the CUDA kernels of a CUDA-based application are partitioned into two groups with minimal user intervention: those that are compiled and executed in parallel software, and those that are synthesized and implemented in hardware. A modern low power FPGA can provide the processing power (via numerous embedded micro-CPUs) and the logic capacity for both the software and hardware implementations of the CUDA kernels. This paper describes the system requirements and the architectural decisions behind the FASTCUDA approach.	ampersand;cuda;central processing unit;compiler;embedded system;field-programmable gate array;graphical user interface;graphics processing unit;hardware acceleration;high- and low-level;high-level synthesis;interaction;multi-core processor;open-source software;opencl api;programming tool;requirement;shared memory;system requirements	Iakovos Mavroidis;Ioannis Mavroidis;Ioannis Papaefstathiou;Luciano Lavagno;Mihai T. Lazarescu;Eduardo de la Torre;Florian Schäfer	2012	2012 15th Euromicro Conference on Digital System Design	10.1109/DSD.2012.58	multi-core processor;embedded system;computer architecture;parallel computing;kernel;hardware acceleration;reconfigurable computing;computer science;operating system;instruction set;hardware architecture;public domain software;general-purpose computing on graphics processing units;field-programmable gate array;integrated circuit design	EDA	1.5045780958119417	49.84308881724087	125086
74d30a6af362591a40e1a627a3d43fcf71a88414	memory access optimization in compilation for coarse-grained reconfigurable architectures	shared memory;power efficiency;data reuse;bank conflict;memory access;reconfigurable architecture;compiler optimization;compilation;coarse grained;multibank memory;array mapping;high performance;article;high power;coarse grained reconfigurable architecture	Coarse-grained reconfigurable architectures (CGRAs) promise high performance at high power efficiency. They fulfil this promise by keeping the hardware extremely simple, and moving the complexity to application mapping. One major challenge comes in the form of data mapping. For reasons of power-efficiency and complexity, CGRAs use multibank local memory, and a row of PEs share memory access. In order for each row of the PEs to access any memory bank, there is a hardware arbiter between the memory requests generated by the PEs and the banks of the local memory. However, a fundamental restriction remains in that a bank cannot be accessed by two different PEs at the same time. We propose to meet this challenge by mapping application operations onto PEs and data into memory banks in a way that avoids such conflicts. To further improve performance on multibank memories, we propose a compiler optimization for CGRA mapping to reduce the number of memory operations by exploiting data reuse. Our experimental results on kernels from multimedia benchmarks demonstrate that our local memory-aware compilation approach can generate mappings that are up to 53% better in performance (26% on average) compared to a memory-unaware scheduler.	arbiter (electronics);benchmark (computing);computational complexity theory;experiment;heuristic;lr parser;mathematical optimization;memory bank;optimizing compiler;performance per watt;reconfigurable computing;routing;scheduling (computing)	Yongjoo Kim;Jongeun Lee;Aviral Shrivastava;Yunheung Paek	2011	ACM Trans. Design Autom. Electr. Syst.	10.1145/2003695.2003702	uniform memory access;distributed shared memory;shared memory;embedded system;interleaved memory;computer architecture;parallel computing;real-time computing;distributed memory;electrical efficiency;computer science;physical address;operating system;optimizing compiler;overlay;conventional memory;extended memory;flat memory model;cache-only memory architecture;memory map;memory management	EDA	-4.311843976340081	50.5289050383182	125133
2e5405beb265a1060ed5bff0f3c66feca4e9286b	performance evaluation of uml2-modeled embedded streaming applications with system-level simulation	signal image and speech processing;circuits and systems;control structures and microprogramming;performance evaluation;electronic circuits and devices	This article presents an efficient method to capture abstract performance model of streaming data real-time embedded systems (RTESs). Unified Modeling Language version 2 (UML2) is used for the performance modeling and as a front-end for a tool framework that enables simulation-based performance evaluation and design-space exploration. The adopted application metamodel in UML resembles the Kahn Process Network (KPN) model and it is targeted at simulation-based performance evaluation. The application workload modeling is done using UML2 activity diagrams, and platform is described with structural UML2 diagrams and model elements. These concepts are defined using a subset of the profile for Modeling and Analysis of Realtime and Embedded (MARTE) systems fromOMGand custom stereotype extensions. The goal of the performancemodeling and simulation is to achieve early estimates on task response times, processing element, memory, and on-chip network utilizations, among other information that is used for design-space exploration. As a case study, a video codec application onmultiple processors is modeled, evaluated, and explored. In comparison to related work, this is the first proposal that defines transformation between UML activity diagrams and streaming data application workload meta models and successfully adopts it for RTES performance evaluation.	activity diagram;central processing unit;codec;embedded system;kahn process networks;metamodeling;modeling and analysis of real time and embedded systems;network on a chip;performance evaluation;profiling (computer programming);real-time clock;scheduling (computing);shattered world;simulation;stream (computing);streaming media;unified modeling language	Tero Arpinen;Erno Salminen;Timo Hämäläinen;Marko Hännikäinen	2009	EURASIP J. Emb. Sys.	10.1155/2009/826296	embedded system;real-time computing;simulation;computer science;operating system	Embedded	-0.08273712257106679	55.61535195625937	125277
336134ffc3fbf5773f0146195fb4f5270351e32a	a multi-level strategy for software power estimation	hardware software codesign;compilers multilevel strategy software power estimation mathematical models power consumption tosca hardware software co design embedded systems;power estimation;program compilers hardware software codesign embedded systems;embedded systems;design environment;levels of abstraction;mathematical model;power consumption;microprocessors energy consumption performance analysis software performance assembly algorithm design and analysis application software mathematical model embedded system constraint optimization;program compilers	In this paper a comprehensive methodology for software power estimation is presented. The methodology is supported by rigorous mathematical models of power consumption at three different levels of abstraction. The methodology has been validated in a complete framework developed within the TOSCA co-design environment.	mathematical model;oasis tosca;principle of abstraction	Carlo Brandolese;William Fornaciari;Luigi Pomante;Fabio Salice;Donatella Sciuto	2000		10.1145/501790.501830	computer architecture;verification and validation;parallel computing;real-time computing;software sizing;computer science;software design;software framework;component-based software engineering;software development;software design description;software construction;software deployment;software metric;software system;avionics software	EDA	2.154870018566958	53.074125917408224	125314
72742235e1b55c30f1da55addf9860ad0ed5ea98	a fast runtime fault recovery approach for noc-based mpsocs for performance constrained applications	fault tolerant communication;fault tolerant nocs;fault recovery noc based mpsoc fault tolerant nocs fault tolerant communication;noc based mpsoc;ports computers protocols routing fault tolerance fault tolerant systems program processors system recovery;fast runtime fault recovery approach random fault injections memory footprint silicon area overhead aging effects process variability nanotechnologies permanent faults transient faults multiprocessor system on chips performance constrained applications noc based mpsoc;network on chip fault tolerant computing integrated circuit reliability multiprocessor interconnection networks;fault recovery	Mechanisms for runtime fault-tolerance in Multi-Processor System-on-Chips (MPSoCs) are mandatory to cope with transient and permanent faults. This issue is even more relevant in nanotechnologies due to process variability, aging effects, and susceptibility to upsets, among other factors. The literature presents isolated solutions to deal with faults in the MPSoC communication infrastructure. In this context, one gap to be fulfilled is to integrate all layers, resulting in a solution to cope with NoC faults from the physical layer up to the application layer. The goal of this work is to present a runtime integrated approach to cope with NoC faults in MPSoCs. The original contribution is the proposal of a set of hardware and software mechanisms to ensure both efficient and reliable communication in NoC-based MPSoCs. The proposal has an acceptable silicon area overhead and a small memory footprint. Experiments demonstrate that benchmarks (synthetic and real MPSoC applications) were simulated with thousands of random fault injections, and all of them were executed correctly. Moreover, the average application execution time overhead is lower than 0.5%. This suggests the proposed fault tolerant method could be used in applications with reliability and performance constraints.	benchmark (computing);fault (technology);fault tolerance;heart rate variability;mpsoc;memory footprint;multiprocessing;network on a chip;overhead (computing);run time (program lifecycle phase);synthetic intelligence;system on a chip	Eduardo Wächter;Augusto Erichsen;Leonardo Juracy;Alexandre M. Amory;Fernando Gehm Moraes	2014	2014 27th Symposium on Integrated Circuits and Systems Design (SBCCI)	10.1145/2660540.2660986	embedded system;parallel computing;real-time computing;engineering;software fault tolerance	EDA	6.097787262671115	59.42125266926441	125373
791c6aea5a57f5213d3eaf6e84b571f2462109ee	model-based precision analysis and optimization for digital signal processors	analytical models;digital signal processing;model based precision optimization model based precision analysis digital signal processors complex high performance signal processing systems dsps design flow eigenvalue decomposition evd jacobi algorithm;testing;computational modeling;dynamic range;jacobian matrices dynamic range digital signal processing analytical models computational modeling algorithm design and analysis testing;signal processing circuit optimisation digital signal processing chips eigenvalues and eigenfunctions matrix decomposition;jacobian matrices;algorithm design and analysis	Embedded signal processing has witnessed explosive growth in recent years in both scientific and consumer applications, driving the need for complex, high-performance signal processing systems that are largely application driven. In order to efficiently implement these systems on programmable platforms such as digital signal processors (DSPs), it is important to analyze and optimize the application design from early stages of the design process. A key performance concern for designers is choosing the data format. In this work, we propose a systematic and efficient design flow involving model-based design to analyze application data sets and precision requirements. We demonstrate this design flow with an exploration study into the required precision for eigenvalue decomposition (EVD) using the Jacobi algorithm. We demonstrate that with a high degree of structured analysis and automation, we are able to analyze the data set to derive an efficient data format, and optimize important parts of the algorithm with respect to precision.	algorithm;central processing unit;data integrity field;dataflow;design flow (eda);digital signal processor;embedded system;high-level programming language;jacobi method;mathematical optimization;model-driven engineering;norm (social);qr decomposition;requirement;seamless3d;signal processing;simulation;structured analysis;synergy;unit testing	Soujanya Kedilaya;William Plishker;Aleksandar Purkovic;Brian Johnson;Shuvra S. Bhattacharyya	2011	2011 19th European Signal Processing Conference		multidimensional signal processing;electronic engineering;parallel computing;computer science;theoretical computer science;digital signal processing	EDA	3.1002640199345453	53.65724637319678	125436
16a7e06878123adea4ffe4c4f9f91aec1d6bf06c	modeling yield, cost, and quality of an noc with uniformly and non-uniformly distributed redundancy	costs network on a chip programmable logic arrays wires multicore processing redundancy logic devices field programmable gate arrays manufacturing processes telecommunication network reliability;quality metric;reliability;communication system;telecommunication network reliability;network on chip;costing;inner links;spare wires;non uniform spare distributtion soc noc yield and cost modeling quality analysis;semiconductor device measurement;wires;programmable logic arrays;spare enhanced multi core chip subject;system on a chip;chip;yield and cost modeling;manufacturing processes;quality constraint;redundancy;quality analysis;multicore processing;cost efficiency;soc;outer links;field programmable gate arrays;noc;spare wires cost quality metric spare enhanced multi core chip subject quality constraint mesh based noc inner links outer links;mesh based noc;network on a chip;cost;non uniform spare distributtion;monte carlo methods;cost model;logic devices;uniform distribution;network on chip costing	In this paper, we propose a quality metric for an NoC and model the yield and cost of a spare-enhanced multi-core chip subject to a given quality constraint. Our experiments show that the overall quality of a mesh-based NoC depends more on the reliability of the inner links than the outer links; therefore, a non-uniform distribution of spare wires could be more effective and cost efficient than a uniform approach.	cost efficiency;experiment;multi-core processor;network on a chip	Saeed Shamshiri;Kwang-Ting Cheng	2010	2010 28th VLSI Test Symposium (VTS)	10.1109/VTS.2010.5469579	system on a chip;embedded system;electronic engineering;real-time computing;telecommunications;computer science;engineering;network on a chip;statistics	Logic	9.087500914216015	57.55057402738125	125542
b22689b275ab633076761509b33523ee3009a729	energy savings and speedups from partitioning critical software loops to hardware in embedded systems	low energy;platforms;fpga;embedded system;synthesis;chip;embedded systems;hardware software partitioning;application specific integrated circuit;speedup;energy saving	We present results of extensive hardware/software partitioning experiments on numerous benchmarks. We describe our loop-oriented partitioning methodology for moving critical code from hardware to software. Our benchmarks included programs from PowerStone, MediaBench, and NetBench. Our experiments included estimated results for partitioning using an 8051 8-bit microcontroller or a 32-bit MIPS microprocessor for the software, and using on-chip configurable logic or custom application-specific integrated circuit hardware for the hardware. Additional experiments involved actual measurements taken from several physical implementations of hardware/software partitionings on real single-chip microprocessor/configurable-logic devices. We also estimated results assuming voltage scalable processors. We provide performance, energy, and size data for all of the experiments. We found that the benchmarks spent an average of 80% of their execution time in only 3% of their code, amounting to only about 200 bytes of critical code. For various experiments, we found that moving critical code to hardware resulted in average speedups of 3 to 5 and average energy savings of 35% to 70%, with average hardware requirements of only 5000 to 10,000 gates. To our knowledge, these experiments represent the most comprehensive hardware/software partitioning study published to date.	32-bit;8-bit;application-specific integrated circuit;benchmark (computing);byte;central processing unit;embedded system;experiment;intel mcs-51;microcontroller;microprocessor;requirement;run time (program lifecycle phase);scalability	Greg Stitt;Frank Vahid;Shawn Nematbakhsh	2004	ACM Trans. Embedded Comput. Syst.	10.1145/972627.972637	chip;hardware compatibility list;embedded system;computer architecture;parallel computing;real-time computing;speedup;computer science;operating system;application-specific integrated circuit;field-programmable gate array	Arch	-1.4783052374447727	53.5761810602148	125687
3b598e04ceb63cf7032298fc9f931f38814fe516	silicon compilation-a hierarchical use of plas	silicon;logic simulation;silicon programmable logic arrays equations logic design counting circuits delay pulse inverters program processors complexity theory programming profession;complexity theory;logic design;event driven and fixed event list scheduling techniques;programmable logic arrays;satisfiability;circuit complexity;integrated circuit design;counting circuits;programming profession;rt gate level descriptions;pulse inverters;software breadboard;software design;program processors	This paper proposes a way to compile a silicon layout directly from synchronous logic specification. The motivation for introducing compilation into the silicon world comes from its extreme success in the software world. As we see silicon area increasing and circuit complexity increasing, we might feel much in common with the early day programmers who faced increasing memory availability along with increasing program complexity.  Software and hardware revolve around the same basic concern: The software designer lays out a one dimensional array of memory whereas the integrated circuit designer lays out a two dimensional area of silicon. In each case, various constraints must be satisfied in order to obtain a working product. In addition, both efforts involve lots of modification.	circuit complexity;compiler;integrated circuit;programmable logic array;programmer;software design;synchronous circuit	Ron Ayres	1979	16th Design Automation Conference	10.1109/DAC.1979.1600126	circuit complexity;embedded system;electronic engineering;logic synthesis;real-time computing;computer science;electrical engineering;software design;theoretical computer science;operating system;logic simulation;silicon;programming language;algorithm;integrated circuit design;satisfiability	EDA	7.204409075996522	51.6936320415533	125967
22e54f1fe5f9fe2926e45fc6a35f7bfae3e2ed3d	simulaton of microprocessor emulation using gasp-pl/i	microprocessors;microprocessors emulation hardware registers microprogramming aerospace electronics computational modeling cmos technology control systems power generation economics;control systems;cmos technology;integrable system;emulation;chip;computational modeling;registers;system design;aerospace electronics;microprogramming;power generation economics;hardware	As microprocessors find wider and wider application in system design, it is becoming increasingly important to simulate these elements in large integrated systems. At Southern Illinois University, NASA-sponsored work on avionics system design has motivated related work in the simulation of microprocessor systems. In particular, an effort was undertaken to simulate an Intel 3000 chip set, which was in turn emulating a PDP-8/A.	avionics;chipset;compiler;computer;data structure;emulator;gordon mccalla;high- and low-level;high-level programming language;list of intel microprocessors;microprocessor;pl/i;preprocessor;simulation;systems design;systems engineering;systems theory	M. S. Jayakumar;Thomas M. McCalla	1977	Computer	10.1109/C-M.1977.217705	chip;integrable system;emulation;computer architecture;computer science;control system;operating system;microcode;processor register;cmos;computational model;hardware emulation;systems design	Arch	7.898896608057298	51.31108373560581	125980
6a44cb852f0b79852856b5abfb68e22c17635544	compiler support for dynamic pipeline scaling	power saving;optimization technique;dynamic voltage scaling;embedded system;low voltage;low power;energy consumption;power dissipation;lower bound;analytical model	Low power has played an increasingly important role for embedded systems. To save power, lowering voltage and frequency is very straightforward and effective; therefore dynamic voltage scaling (DVS) has become a prevalent low-power technique. However, DVS makes no effect on power saving when the voltage reaches a lower bound. Fortunately, a technique called dynamic pipeline scaling (DPS) can overcome this limitation by switching pipeline modes at low-voltage level. Approaches proposed in previous work on DPS were based on hardware support. From viewpoint of compiler, little has been addressed on this issue. This paper presents a DPS optimization technique at compiler time to reduce power dissipation. The useful information of an application is exploited to devise an analytical model to assess the cost of enabling DPS mechanism. As a consequence we can determine the switching timing between pipeline modes at compiler time without causing significant run-time overhead. The experimental result shows that our approach is effective in reducing energy consumption.	cpu power dissipation;compile time;compiler;descriptive video service;dynamic voltage scaling;embedded system;image scaling;inter-process communication;low-power broadcasting;mathematical optimization;overhead (computing);pictbridge	Kuan-Wei Cheng;Tzong-Yen Lin;Rong-Guey Chang	2007		10.1007/978-3-540-77092-3_7	embedded system;parallel computing;real-time computing;computer science;dissipation;upper and lower bounds;low voltage	EDA	-4.127286115641454	55.876443659229224	126069
42e2497786e49ed4b542d21477622db387376908	sun's sparcstation 1: a workstation for the 1990s	workstations computer architecture;sun workstations cmos technology bandwidth personal communication networks costs heart engines joining processes fddi;floating point unit;chip;computer architecture;workstations;process engineering;64 mb sun sparcstation 1 computer architecture cmos gate arrays single chip sparc integer unit floating point unit sbus i o expansion interconnect sbus ipi interprocessor interrupt fddi interfaces graphics accelerator gx 12 5 mips 1 4 mflops	The architecture and features of the SPARCstation 1 are described and compared with those of other workstations and PCs of approximately the same cost. The heart of the machine is implemented using seven custom CMOS gate arrays plus a single-chip SPARC integer unit and a single-chip SPARC floating-point unit. The architecture of SPARCstation 1 reflects the use of CMOS technology, especially in the design of the SBus, which is SPARCstation 1's memory and I/O expansion interconnect. As a processing engine, SPARCstation 1 provides the user with 12.5 MIPS, 1.4 MFLOPS, and 64 MB of memory. The SBus provides the means for connecting peripheral devices such as IPI (interprocessor interrupt) drives and FDDI interfaces. The SBus accommodates these devices by having a peak bandwidth of 80 MB/s and a sustained bandwidth in the SPARCstation 1 implementation of approximately 25 to 30 MB/s. An optional graphics accelerator, the GX, can render almost 5 Mvectors/s.<<ETX>>	arithmetic logic unit;cmos;flops;floating-point unit;graphics processing unit;input/output;inter-processor interrupt;mebibyte;peripheral;sbus;sparc;workstation	Andreas von Bechtolsheim;Edward H. Frank	1990	Digest of Papers Compcon Spring '90. Thirty-Fifth IEEE Computer Society International Conference on Intellectual Leverage	10.1109/CMPCON.1990.63671	embedded system;sbus;parallel computing;real-time computing;computer science	Arch	7.104451836899542	49.7986108913782	126228
06e97c67d34ec1efe60e2eeadfa714c56035e921	accelerating monte carlo based ssta using fpga	fpga;mathematical programming;pattern matching;ssta;monte carlo	Monte Carlo based SSTA serves as the golden standard against alternative SSTA algorithms, but it is seldom used in practice due to its high computation time. In this paper, we accelerate Monte Carlo based SSTA using the FPGA platform. A simple dataflow pipeline technique will not work well due to the excessive usage of FPGA logic slices. We leverage the recently proposed pattern matching method to identify common circuit structures, and further use a mathematical programming based formulation to explore the trade-off between performance and logic slices consumption. The proposed design provides two orders of magnitude speedup compared to the CPU-based implementation.	algorithm;central processing unit;computation;dataflow;field-programmable gate array;mathematical optimization;monte carlo method;pattern matching;pipeline (computing);speedup;statistical static timing analysis;time complexity	Jason Cong;Karthik Gururaj;Wei Jiang;Bin Liu;Kirill Minkovich;Bo Yuan;Yi Zou	2010		10.1145/1723112.1723132	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;pattern matching;field-programmable gate array;monte carlo method	EDA	1.3796811396072248	55.56603521720489	126231
04f1d436d6ee3ec569ce3906654b1c001318f9a0	power minimization for dynamically reconfigurable fpga partitioning	reconfigurable computing;temporal partitioning;power optimization;dynamically reconfigurable fpga	Dynamically reconfigurable FPGA (DRFPGA) implements a given circuit system by partitioning it into stages and then executing each stage sequentially. Traditionally, the number of communication buffers is minimized. In this article, we study the partitioning problem targeting at power minimization for the DRFPGAs that have lookup table (LUT) based logic blocks. We analyze the power consumption caused by the communication buffers in the temporal partitioning. Based on the analysis, we use a flow network to represent a given circuit so that the power consumption of buffers is correctly evaluated and the temporal constraints are satisfied in circuit partitioning. The well known flow-based FBB algorithm is then applied to the network to find the area-balanced partitioning of minimum power consumption. Experimental results show that our method outperforms the conventional partitioning algorithms in terms of power consumption. The problem is then extended to include constraints on the number of communication buffers. We provide a net modeling for this extended problem and present an extension of the FBB algorithm to obtain a power-optimal solution. Experimental results demonstrate the effectiveness of the proposed algorithm in reducing power consumption as compared to the previous partitioning algorithms without exceeding the buffer number limit.	algorithm;binary space partitioning;field-programmable gate array;flow network;lookup table;partition problem;reconfigurability;reconfigurable computing;whole earth 'lectronic link	Tzu-Chiang Tai;Yen-Tai Lai	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2435227.2435248	parallel computing;real-time computing;reconfigurable computing;computer science;distributed computing;power optimization	EDA	0.21351164181321597	52.743018331507976	126559
e06cbfc8e5eb18df15765ac27ddb766d064d8e7d	a retargetable static binary translator for the arm architecture	performance;retargeting;compiler;binary translation;design;article;languages;intermediate representation	Machines designed with new but incompatible Instruction Set Architecture (ISA) may lack proper applications. Binary translation can address this incompatibility by migrating applications from one legacy ISA to a new one, although binary translation has problems such as code discovery for variable-length ISA and code location issues for handling indirect branches. Dynamic Binary Translation (DBT) has been widely adopted for migrating applications since it avoids those problems. Static Binary Translation (SBT) is a less general solution and has not been actively researched. However, SBT performs more aggressive optimizations, which could yield more compact code and better code quality. Applications translated by SBT can consume less memory, processor cycles, and power than DBT and can be started more quickly. These advantages are even more critical for embedded systems than for general systems.  In this article, we designed and implemented a new SBT tool, called LLBT, which translates ARM instructions into LLVM IRs and then retargets the LLVM IRs to various ISAs, including ×86, ×86--64, ARM, and MIPS. LLBT leverages two important functionalities from LLVM: comprehensive optimizations and retargetability. More importantly, LLBT solves the code discovery problem for ARM/Thumb binaries without resorting to interpretation. LLBT also effectively reduced the size of the address mapping table, making SBT a viable solution for embedded systems. Our experiments based on the EEMBC benchmark suite show that the LLBT-generated code can run more than 6× and 2.3× faster on average than emulation with QEMU and HQEMU, respectively.	arm architecture;associative entity;benchmark (computing);binary translation;eembc;embedded system;emulator;experiment;llvm;session-based testing;software incompatibility;software quality;static library;x86	Bor-Yeh Shen;Wei-Chung Hsu;Wuu Yang	2014	TACO	10.1145/2629335	design;compiler;parallel computing;real-time computing;performance;computer science;operating system;programming language	Arch	-0.8695617298300368	48.16003811216813	126688
5f8a161806c173b3fa11fe625955da83eb86a130	design and implementation trade-offs in the clipper c400 architecture	silicon;reduced instruction set computer;microprocessors;reduced instruction set computing architecture;integer unit design;floating point unit;performance evaluation;clocks;computer aided instruction;performance;circuit design;reduced instruction set computing;superscalar operation;multichip implementation;reduced instruction set computing microprocessor chips;floating point unit design;programming model;computer architecture;multichip implementation clipper c400 architecture reduced instruction set computing architecture performance superscalar operation clock cycle superpipelining execution pipelines programming model hardware implementations integer unit design load store pipeline floating point unit design circuit design;design and implementation;pipelines;superpipelining;clipper c400 architecture;arithmetic;clocks computer architecture silicon pipelines performance evaluation computer aided instruction parallel processing microprocessors costs arithmetic;load store pipeline;hardware implementations;instructions per clock;clock cycle;hardware implementation;parallel processing;microprocessor chips;execution pipelines	A description is given of the C400, the first complete redesign of the Clipper reduced instruction-set computing architecture since its introduction in 1985. The C400 delivers three times the performance of the C300, yet retains full-code compatibility with earlier Clippers. The C400 combines two architectural approaches to attain its performance goals. The first approach, superscalar operation, allows the processor to begin the execution of more than one instruction during each clock cycle. The C400, which is moderately superscalar, can dispatch two instructions per clock cycle. The C400 also embodies the design concept of superpipelining, an approach that emphasizes high clock rates and deep execution pipelines in attaining high computational performance. The discussion covers the programming model, early hardware implementations, the C400 project goals and approaches, C400 performance, the integer unit design, the load/store pipeline, the floating-point unit design, the superscalar/superpipelined architecture, circuit design, and the advantages of the multichip implementation.<<ETX>>	arithmetic logic unit;circuit design;clipper chip;clock signal;computation;computer architecture;dynamic dispatch;floating-point unit;instruction pipelining;instructions per cycle;pipeline (computing);programming model;superscalar processor	Howard Sachs;Harlan McGhan;Lee F. Hanson;Nathan A. Brookwood	1991	IEEE Micro	10.1109/40.87566	floating-point unit;parallel processing;reduced instruction set computing;computer architecture;parallel computing;real-time computing;performance;computer science;operating system;circuit design;pipeline transport;cycles per instruction;programming paradigm;silicon;instructions per cycle	Arch	6.064072544327039	48.88213539308051	126873
74d9687ed23693260e6abea9199c815454795bba	enabling ultra low voltage system operation by tolerating on-chip cache failures	international organizations;power density;fault tolerant;ultra low voltage;dynamic voltage scaling;chip;low voltage;leakage power;technology integration;low voltage operation;high performance;fault tolerant cache	Extreme technology integration in the sub-micron regime comes with a rapid rise in heat dissipation and power density for modern processors. Dynamic voltage scaling is a widely used technique to tackle this problem when high performance is not needed. However, the minimum achievable supply voltage is often bounded by SRAM cells since they fail at a faster rate than logic cells. In this work, we propose a novel fault-tolerant cache architecture, that by reconfiguring its internal organization can efficiently tolerate SRAM failures that arise when operating in the ultra low voltage region. Using our approach, the operational voltage of a processor can be reduced to 420mV, which translates to 80% dynamic and 73% leakage power savings in 90nm.	cpu cache;central processing unit;dynamic voltage scaling;fault tolerance;spectral leakage;static random-access memory;thermal management (electronics);ultra-low-voltage processor	Amin Ansari;Shuguang Feng;Shantanu Gupta;Scott A. Mahlke	2009		10.1145/1594233.1594309	chip;embedded system;fault tolerance;electronic engineering;real-time computing;telecommunications;computer science;engineering;power density;low voltage;thermodynamics;voltage optimisation	Arch	5.497806077081917	59.64852394292951	126877
bc9c4d10cedec72b4b9431de81381fa771276bd8	maximum convex subgraphs under i/o constraint for automatic identification of custom instructions	time complexity;output constraints maximum convex subgraphs io constraint automatic custom instructions identification ci application source code convexity constraints input constraints;source code software constraint handling graph theory instruction sets;application specific integrated circuits;clustering algorithms;hardware software codesign customizable processors asips;program processors;algorithm design and analysis;benchmark testing;program processors hardware algorithm design and analysis benchmark testing clustering algorithms application specific integrated circuits time complexity;hardware	Automatic identification of custom instructions (CI) is the process of supporting the programmer in choosing automatically beneficial parts of the application source code that can then be synthesized and run on dedicated hardware. Identification is typically modeled as choosing a subgraph from a graph, representing the application, that has the highest speedup potential when implemented in custom hardware, and that fulfills the constraints of convexity and of a given maximum number of inputs and outputs. Existing algorithms for CI identification either enumerate all the valid subgraphs under the constraints of convexity and I/O, or return the subset of all maximal valid subgraphs with respect to convexity only. The downside of the former approach is that enumerating all valid subgraphs is costly, especially for large values of input and output constraints, while we may be interested in the subgraphs which obtain the best speedup only. Instead, the latter approach may fail to find a feasible solution, since the valid subgraphs with respect to convexity only can be too large to be useful. In this paper, we present a novel approach which attempts to fill the gap between the existing methods. In particular, we present an algorithm that enumerates the subset of all maximum valid subgraphs with respect to convexity and number of inputs and outputs. Our method revisits and combines the existing approaches and yields an algorithm which is effective and outperforms the state-of-the-art for large values of input and output constraints.	algorithm;automatic identification and data capture;benchmark (computing);convex function;convex subgraph;enumerated type;input/output;maximal set;programmer;speedup	Emanuele Giaquinta;Anadi Mishra;Laura Pozzi	2015	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2014.2387375	time complexity;algorithm design;benchmark;mathematical optimization;computer science;theoretical computer science;operating system;mathematics;application-specific integrated circuit;cluster analysis;algorithm	EDA	1.3357221642129635	51.709961729363165	126930
895ba2f9c5b7c383d2d1bdf8955dd9bea2e4e826	quantifying acceleration: power/performance trade-offs of application kernels in hardware	power performance trade-offs;design;experimentation;types and design styles;design space exploration;measurement;electronics;accelerator;performance;low power electronics;system on chip;integrated circuit design	As the traditional performance gains of technology scaling diminish, one of the most promising directions is building special purpose fixed function hardware blocks, commonly referred to as accelerators. Accelerators have become prevalent in industrial SoC designs for their low power, high performance potential. In this work we explore thousands of implementations of classical software workloads in hardware. This thorough, detailed design space search of hardware accelerators gives architects a quantitative way to reason about the differences in implementations. The exploration presented in this work shows that the space is full of poor design choices. By thoroughly analyzing each benchmark, we show which provide the most performance when implemented in hardware given a fixed power budget and explain which design techniques work best for each workload.	benchmark (computing);hardware acceleration;image scaling;optimal design;pareto efficiency;semiconductor intellectual property core;system on a chip	Brandon Reagen;Yakun Sophia Shao;Gu-Yeon Wei;David M. Brooks	2013	International Symposium on Low Power Electronics and Design (ISLPED)		system on a chip;embedded system;electronic engineering;real-time computing;hardware acceleration;computer science;engineering;operating system;low-power electronics;integrated circuit design	Arch	0.5821031700754743	54.77925370707511	126974
c7d3565575c4ad28a3761919334c3b671710a33c	the dual-eval hardware description language and its use in the formal specification and verification of the fm9001 microprocessor	microprocessors;hardware verification;formal specification;hardware description languages;formal semantics;fm9001;behavioral specification hardware description language dual eval formal specification verification fm9001 microprocessor occurrence oriented simulator synchronous mealy machines;formal verification;computer testing;finite automata;integrated circuit testing;hardware design languages formal specifications microprocessors circuit simulation cmos logic circuits mathematical model circuit testing large scale integration computational modeling application specific integrated circuits;formal verification formal specification microprocessor chips computer testing integrated circuit testing hardware description languages finite automata;hardware description language;formal specification and verification;dual eval;microprocessor chips	We present an introduction to the DUAL-EVAL hardware description language. DUAL-EVAL is a hierarchical, occurrence-oriented simulator for synchronous Mealy machines. We briefly describe the FM9001 microprocessor, whose design has been formally specified with the DUAL-EVAL language and mechanically proved correct with respect to a behavioral specification. The FM9001 has been fabricated as a CMOS ASIC and tested extensively.		Bishop Brock;Warren A. Hunt	1995	Formal Methods in System Design	10.1023/A:1008685826293	computer architecture;formal methods;specification language;computer science;theoretical computer science;hardware description language;finite-state machine;programming language	Logic	8.062994624282737	52.009312227984175	127091
8f08814139d04e3d9d6026fcaae40c6987ba7b6c	design of a universal logic block for fault-tolerant realization of any logic operation in trapped-ion quantum circuits	quantum physical mapping;quantum computer aided design;quantum instruction placement;quantum universal logic block	This paper presents a physical mapping tool for quantum circuits, which generates the optimal Universal Logic Block (ULB) that can, on average, perform any logical fault-tolerant (FT) quantum operations with the minimum latency. The operation scheduling, placement, and qubit routing problems tackled by the quantum physical mapper are highly dependent on one another. More precisely, the scheduling solution affects the quality of the achievable placement solution due to resource pressures that may be created as a result of operation scheduling whereas the operation placement and qubit routing solutions influence the scheduling solution due to resulting distances between predecessor and current operations, which in turn determines routing latencies. The proposed flow for the quantum physical mapper captures these dependencies by applying (i) a loose scheduling step, which transforms an initial quantum data flow graph into one that explicitly captures the no-cloning theorem of the quantum computing and then performs instruction scheduling based on a modified force-directed scheduling approach to minimize the resource contention and quantum circuit latency, (ii) a placement step, which uses timing-driven instruction placement to minimize the approximate routing latencies while making iterative calls to the aforesaid force-directed scheduler to correct scheduling levels of quantum operations as needed, and (iii) a routing step that finds dynamic values of routing latencies for the qubits. In addition to the quantum physical mapper, an approach is presented to determine the single best ULB size for a target quantum circuit by examining the latency of different FT quantum operations mapped onto different ULB sizes and using information about the occurrence frequency of operations on critical paths of the target quantum algorithm to weigh these latencies. Experimental results show an average latency reduction of about 40% compared to previous work.	approximation algorithm;boolean algebra;computer-aided design;dataflow;fault tolerance;force-directed graph drawing;instruction scheduling;iteration;list scheduling;logic block;no-cloning theorem;quantum algorithm;quantum circuit;quantum computing;quantum mechanics;qubit;resource contention;routing;simpl;scheduling (computing)	Hadi Goudarzi;Mohammad Javad Dousti;Alireza Shafaei;Massoud Pedram	2014	Quantum Information Processing	10.1007/s11128-013-0725-3	quantum information;theoretical computer science;mathematics;algorithm;one-way quantum computer;quantum phase estimation algorithm;quantum sort	Arch	-0.7730009453248005	48.10360520986775	127132
2444d6e88c9f18ea35c329fc78a2cb22688076ae	towards on-chip fault-tolerant communication	cmos integrated circuits;stochastic processes failure analysis fault tolerant computing vlsi system on chip cmos integrated circuits integrated circuit reliability;fault tolerant;fault tolerance tiles cmos technology network on a chip protocols costs integrated circuit interconnections design methodology very large scale integration design automation;chip;failure analysis;technology scaling;fault tolerant computing;stochastic processes;system on chip;randomized algorithm;vlsi;integrated circuit reliability;failure model on chip fault tolerant communication cmos technology vlsi chips deep submicron domain dsm failure patterns stochastic communication generic tile based architecture randomized algorithm performance metrics on chip failures communicating ip cores system on chip design soc design network on chip architecture noc architecture system level fault tolerance high level model;design methodology	As CMOS technology scales down into the deep-submicron (DSM) domain, devices and interconnects are subject to new types of malfunctions and failures that are harder to predict and avoid with the current system-on-chip (SoC) design methodologies. Relaxing the requirement of 100% correctness in operation drastically reduces the costs of design but, at the same time, requires SoCs be designed with some degree of system-level fault-tolerance. In this paper, we introduce a high-level model of DSM failure patterns and propose a new communication paradigm for SoCs, namely stochastic communication. Specifically, for a generic tile-based architecture, we propose a randomized algorithm which not only separates computation from communication, but also provides the required fault-tolerance to on-chip failures. This new technique is easy and cheap to implement in SoCs that integrate a large number of communicating IP cores.	cmos;computation;correctness (computer science);electrical connection;fault tolerance;high- and low-level;programming paradigm;randomized algorithm;system on a chip;very-large-scale integration	Tudor Dumitras;Sam Kerner;Radu Marculescu	2003		10.1145/1119772.1119817	chip;system on a chip;stochastic process;embedded system;failure analysis;fault tolerance;electronic engineering;real-time computing;design methods;telecommunications;computer science;engineering;very-large-scale integration;network on a chip;randomized algorithm;cmos;algorithm	EDA	9.038142818027051	57.838725896893884	127264
7233301ff4e6f5029bea79099e647d910d223b2c	reconfigurable accelerator for wfs-based 3d-audio	wfs based 3d audio;audio systems;personal communication networks;xilinx xpower;reconfigurable accelerator;pentium d;energy efficient;virtex4fx60 fpga;3d audio systems;reconfigurable architectures;prototypes;real time;wfs sound systems;frequency 200 mhz;frequency estimation;hardware accelerator;reconfigurable architectures audio systems field programmable gate arrays;real time sound sources;arrays;audio systems loudspeakers energy consumption hardware personal communication networks laboratories prototypes field programmable gate arrays signal synthesis frequency estimation;distance measurement;frequency 3 4 ghz reconfigurable accelerator wfs based 3d audio hardware accelerator 3d audio systems wave field synthesis technology wfs sound systems real time sound sources rendering units virtex4fx60 fpga wfs processing speedup software implementation pentium d xilinx xpower frequency 200 mhz;wfs processing speedup;engines;loudspeakers;energy consumption;frequency 3 4 ghz;wave field synthesis;signal synthesis;field programmable gate arrays;rendering computer graphics;high power;rendering units;software implementation;wave field synthesis technology;hardware;real time systems	In this paper, we propose a reconfigurable and scalable hardware accelerator for 3D-audio systems based on the Wave Field Synthesis technology. Previous related work reveals that WFS sound systems are based on using standard PCs. However, two major obstacles are the relative low number of real-time sound sources that can be processed and the high power consumption. The proposed accelerator alleviates these limitations by its performance and energy efficient design. We propose a scalable organization comprising multiple rendering units (RUs), each of them independently processing audio samples. The processing is done in an environment of continuously varying number of sources and speakers. We provide a comprehensive study on the design trade-offs with respect to this multiplicity of sources and speakers. A hardware prototype of our proposal was implemented on a Virtex4FX60 FPGA operating at 200 MHz. A single RU can achieve up to 7× WFS processing speedup compared to a software implementation running on a Pentium D at 3.4 GHz, while consuming, according to Xilinx XPower, approximately 3 W of power only.	algorithm;bottleneck (software);field-programmable gate array;hardware acceleration;multiple encryption;prototype;real-time clock;real-time locating system;reconfigurable computing;scalability;speedup;virtex (fpga);watts humphrey	Dimitris Theodoropoulos;Georgi Kuzmanov;Georgi Gaydadjiev	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5161218	loudspeaker;embedded system;parallel computing;real-time computing;hardware acceleration;computer science;operating system;prototype;efficient energy use;field-programmable gate array	Arch	2.6232666402629436	47.7771619090617	127460
ba30a8edccafe1e0f3b8b180e68f66d7a813fded	libpowermon: a lightweight profiling framework to profile program context and system-level metrics	context measurement radiation detectors random access memory hardware monitoring temperature sensors;random access memory;measurement;profiling;radiation detectors;temperature sensors;performance;program context;thermal measurements;monitoring;performance profiling program context power thermal measurements;system enforced power limit libpowermon lightweight profiling framework profile program context system level metrics hpc systems processor level system level power thermal measurements profiling tools application level events lightweight user level profiling framework user specified application events source level phase markup interface capture application context mpi events openmp events temporal granularity dram power cluster level power;power aware computing application program interfaces message passing parallel processing;power;context;hardware	As power becomes one of the most important re-sources to provision while building modern HPC systems and applications, it becomes crucial to obtain deeper insights into applications' power and thermal characteristics. There exists a need to correlate application context with processor-level and system-level power and thermal measurements. Existing profiling tools to monitor power and thermal measurements either operate at a granularity that is not fine enough to correlate with application-level events that describe application context or are not equipped to sample application-level events. In this work, we introduce libPowerMon, a lightweight user-level profiling framework to simultaneously sample user-specified application events and system-level metrics at up to 1 kHz sampling intervals. At the application level, libPowerMon provides a source-level phase markup interface to capture application context. It records MPI and OpenMP events, and samples processor state at a finer temporal granularity. At the system level, libPowerMon samples power and thermal characteristics and provides an interface to set processor and DRAM power. We present three case studies that demonstrate the benefits of libPowerMon in saving cluster-level power and improving application performance within a system-enforced power limit.	application level events;context (computing);dynamic random-access memory;interaction;markup language;message passing interface;openmp;runtime system;sampling (signal processing);user space	Aniruddha Marathe;Hormozd Gahvari;Jae-Seung Yeom;Abhinav Bhatele	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.199	embedded system;parallel computing;real-time computing;performance;computer science;operating system;power;distributed computing;profiling;particle detector;measurement	Arch	-3.470239903258207	56.55108871136689	127558
a0b833b7a616e1a657ea07333a3e8cff5d7f4e63	a new homogeneous microprogrammable computer architecture	computer architecture		computer architecture;microcode	Victor Cherniavsky;H. Broer	1983			computer architecture;parallel computing;cellular architecture;computer engineering	Arch	4.5252036267067774	49.45429973152893	127609
3b66f5800b59a9742e46e449bdb669042b8a9351	single-event effects on optical transceiver		All communications systems use optical modules to achieve data transfer speeds in the 10's of GBPS range. With increasing reliance on communication systems, the impact of soft errors in optical transceiver modules has become a primary concern for system performance. This paper examines neutron-induced soft error rates and associated failure modes of optical transceivers.	data rate units;neutrons;single event upset;soft error;transceiver device component	K. J. Lezon;S.-J. Wen;Y.-F. Dan;Rick T J Wong;B. L. Bhuva	2018	2018 IEEE International Reliability Physics Symposium (IRPS)	10.1109/IRPS.2018.8353693	communications system;electronic engineering;data transmission;transceiver;single event upset;engineering;soft error	Embedded	9.28413020767162	60.26221077726768	127618
34aa45a0732d2c2a7b84414489fd775b7d31b8ca	designing wireless protocols: methodology and applications	automatic control;architectural design;protocols;teleconferencing;formal models;hardware software codesign;formal model;full duplex voice communication;control component;wireless application protocol;wireless protocols;specification;protocol design;data processing;orthogonalization of concerns;picoradio;sensor network;next generation wireless systems;automata;correctness;computer architecture;land mobile radio finite state machines voice communication protocols radiotelemetry teleconferencing hardware software codesign;function design;finite state machines;data component;computational modeling;low power;radiotelemetry;intercom;land mobile radio;voice communication;hw sw partitions;communication protocol;architecture design;time to market;correct the first time implementations;model of computation;sensor network wireless protocols communication protocols next generation wireless systems correct the first time implementations design methodology orthogonalization of concerns function design architecture design formal models correctness co design finite state machines cfsms control component data component specification automatic hardware synthesis software synthesis hw sw partitions intercom full duplex voice communication picoradio;software synthesis;communication system control;automatic hardware synthesis;wireless systems;cfsms;finite state machine;communication protocols;co design finite state machines;design methodology wireless application protocol time to market computer architecture automata computational modeling automatic control communication system control data processing partitioning algorithms;partitioning algorithms;design methodology	Communication protocols are essential components of wireless systems. In particular, tight requirements on adaptivity and on energy consumption make the problem of designing wireless protocols quite difficult. Present methods for protocol design are heuristic in nature and are not suited for next generation wireless systems where time-to-market concerns require correct-the-first-time implementations. A wireless system, Intercom, a bidirectional voice link system targeted to provide communication capabilities among different users, is presented and the design of its protocols is described. A new design methodology for wireless protocols based on the principle of orthogonalization of concerns is presented. In particular, the methodology separates function and architecture design and emphasizes the use of formal models to ensure correctness and reduce design time. Protocols are described using Co-design Finite State Machines (CFSMs), a model of computation that has been designed to allow the efficient capture of both the control and the data parts of the specification. Further, algorithms for automatic hardware and software synthesis from CFSMs are available that allow a fast exploration of different partitioning and the analysis of tradeoffs involved. The design methodology presented here will be used for the design of PicoRadio, a low-power and highly adaptive network of sensors.	algorithm;communications protocol;correctness (computer science);duplex (telecommunications);finite-state machine;heuristic;low-power broadcasting;model of computation;next-generation network;sensor	Marco Sgroi;Julio Leao da Silva;Fernando De Bernardinis;Fred Burghardt;Alberto L. Sangiovanni-Vincentelli;Jan M. Rabaey	2000		10.1109/ICASSP.2000.860212	communications protocol;real-time computing;computer science;theoretical computer science;distributed computing;finite-state machine	Embedded	4.525308814308632	53.731325493187846	128324
aa0cb9512567165bf16c484c5e24406a532f2df6	maximizing energy saving of dual-architecture processors using dvfs	energy efficiency;modeling techniques;hybrid architecture;performance per watt;dvfs	Multi-core computing has gone mobile. Managing power consumption within energy-constrained mobile devices demands low-power architectures to increase battery lifespan. One of the promising solutions offered today by microprocessor architects is hybrid microprocessors that integrate different core architectures on a single die and that are equipped with dynamic frequency-scaling techniques. This paper presents analytical models based on an energy consumption metric to analyze the impact of dynamic frequency scaling on the energy consumption of various architectural design choices for hybrid-architecture chips. The power consumption implications of different processing schemes and various chip configurations were also analyzed. The analysis shows that by choosing the optimal hardware configuration, the energy savings can be increased considerably while keeping sacrifices in performance at tolerable levels.	amdahl's law;central processing unit;clock rate;dynamic frequency scaling;dynamic voltage scaling;image scaling;low-power broadcasting;manycore processor;microprocessor;mobile device;multi-core processor;performance per watt;scalability;speedup	Ami Marowka	2014	The Journal of Supercomputing	10.1007/s11227-014-1147-4	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use;object-modeling technique	Arch	-3.9103353854658094	55.60972826276786	128383
21f120c0fa0483a3779bbd8b33eada93795618ef	mapping method for dynamically reconfigurable architecture	data flow analysis;decoding;reconfigurable architectures;video coding;h.264 decoder;dynamically reconfigurable architecture;generic dataflow graph;heuristic mapping algorithm;mapping method	We present a mapping algorithm for our dynamically reconfigurable architecture suitable for stream applications such as H.264. Because our target architecture consists heterogeneously of four different configuration format units, it's difficult to apply the conventional algorithms. We propose a heuristic mapping algorithm enabling the mapping of generic dataflow graph onto this complex hardware automatically. We mapped five main functions of H.264 decoder onto our architecture and compared the results with those of manual mapping performed by an experienced engineer. The results show optimization of three of the five functions is equal to that in the case of the manual mapping.	brute-force search;cluster analysis;data-flow analysis;dataflow;h.264/mpeg-4 avc;heuristic;mathematical optimization;reconfigurability;search algorithm	Akira Kuroda;Mayuko Koezuka;Hidenori Matsuzaki;Takashi Yoshikawa;Shigehiro Asano	2009	2009 Asia and South Pacific Design Automation Conference		embedded system;probability density function;parallel computing;real-time computing;computer science;theoretical computer science;operating system;data-flow analysis;application-specific integrated circuit;cluster analysis;statistics	EDA	0.007297773600302379	51.9353725130738	128463
d849ec64ca8642ba79813a1dab6b36301beb865c	optimal solutions to a class of power management problems in mobile robots	gestion energia;robot movil;optimal solution;time optimal control;control optimo;scheduling of real time systems;calculateur embarque;computacion informatica;state constraint;mobile robot;real time;heuristic method;grupo de excelencia;discrete time;metodo heuristico;robotics;probabilistic approach;power function;embedded system;optimal control;tension espacio estado;commande optimale temps;general solution;gestion energie;robot mobile;energy consumption;ciencias basicas y experimentales;scheduling;enfoque probabilista;approche probabiliste;commande optimale;temps reel;state constraints;power management;boarded computer;consommation energie;robotica;tiempo real;procesador;robotique;low power design;methode heuristique;power consumption;processeur;tiempo discreto;temps discret;low power design of embedded systems;calculador embarque;processor;ordonnancement;moving robot;contrainte espace etat;reglamento;consumo energia;energy management;optimal control problem;real time systems	This paper studies an approach to minimize the power consumption of a mobile robot by controlling its traveling speed and the frequency of its on-board processor simultaneously. The problem is formulated as a discrete-time optimal control problem with a random terminal time and probabilistic state constraints. A general solution procedure suitable for arbitrary power functions of the motor and the processor is proposed. Furthermore, for a class of realistic power functions, the optimal solution is derived analytically. Interpretations of the optimal solution in the practical context are also discussed. Simulation results show that the proposed method can save a significant amount of energy compared with some heuristic schemes.	central processing unit;first-order predicate;heuristic;mathematical optimization;mobile robot;nonlinear programming;nonlinear system;on-board data handling;optimal control;optimization problem;power management;run time (program lifecycle phase);simulation	Wei Zhang;Yung-Hsiang Lu;Jianghai Hu	2009	Automatica	10.1016/j.automatica.2008.11.004	control engineering;mobile robot;discrete time and continuous time;simulation;power function;optimal control;engineering;control theory;mathematics;robotics;scheduling;energy management	Robotics	-3.430712330868685	59.39503556871542	128571
430503804f859a2347c1ac2e3b4eebc792b13212	a narrative of uvm testbench environment for interconnection routers: a practical approach	network on chip noc;universal verification methodology uvm;system on chip soc	In contrast to past projections using conventional bus-based interconnections, the use of Network on Chip (NoC) as an interconnection platform has become more promising to solve complex on-chip communication problems due to what it offers from scalability, reusability and efficiency. Moreover, providing a suitable test base to inspect and verify functionality of any IP core is a compulsory stage. To elaborate; Universal Verification Methodology (UVM) is introduced as a standardized and reusable methodology for verifying integrated circuit designs. In this paper, we present an architecture of a complete UVM environment to test generic routers through various test cases providing different scenarios to be applied. We also aim to establish a base on which other researchers can build to proceed towards finding better solutions.	algorithm;integrated circuit;interconnection;network on a chip;router (computing);scalability;semiconductor intellectual property core;test bench;test case;universal verification methodology;verification and validation	Ahmed El-Naggar;Essraa Massoud;Ahmed Medhat;Hala Ibrahim;Bassma Al-Abassy;Sameh El-Ashry;Mostafa Khamis;Ahmed Shalaby	2016	2016 11th International Design & Test Symposium (IDT)	10.1109/IDT.2016.7843022	embedded system;electronic engineering;real-time computing;engineering;operating system;computer network;computer engineering	Arch	8.512548570586045	53.87629012721088	128687
e936e7a15d48723bf17840eec9a1517556abd23b	the ld-rls algorithmwith directional forgetting implemented on a vector-like hardware accelerator	control application;software;digital signal processing;high level abstraction;hardware signal processing algorithms software algorithms bismuth software matrix decomposition digital signal processing;recursive estimation;least squares approximations;vector processor systems embedded systems field programmable gate arrays floating point arithmetic least squares approximations matrix decomposition recursive estimation;bismuth;time varying parameter;hardware accelerator;fpga;embedded system;directional forgetting implementation;ld rls algorithm;embedded systems;directional forgetting;matrix decomposition;signal processing;software algorithms;vector processor systems;a priori information;rls;floating point arithmetic;field programmable gate arrays;vector oriented hardware accelerator;signal processing algorithms;high level abstraction ld rls algorithm vector oriented hardware accelerator ldu decomposition directional forgetting implementation embedded system time varying parameter tracking fpga;hardware accelerator fpga rls ldu decomposition directional forgetting;time varying parameter tracking;ldu decomposition;hardware	The paper discusses an RLS algorithm based on the LDU decomposition (LD-RLS) with directional forgetting implemented on an embedded system with a vector-oriented hardware accelerator. The LD-RLS algorithm can be attractive for control applications to identify an unknown system or to track time-varying parameters. A solution of the LD-RLS algorithm directly contains the estimated parameters. It also offers a possibility to use a priori information about the identified system and its parameters. The implementation of the LD-RLS algorithm is done on an FPGA-based accelerator from a high-level abstraction. It is compared with an implementation of the same algorithm in software on the same platform.	algorithm;embedded system;field-programmable gate array;hardware acceleration;high- and low-level;lu decomposition;recursive least squares filter	Roman Bartosinski	2011	2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2011.5946817	real-time computing;computer science;theoretical computer science;signal processing;field-programmable gate array	EDA	4.958240058145484	47.06290577577258	128755
a6560eba611b2ea3d2d433f53dd699f2d2d887cc	modeling and simulation of steady state and transient behaviors for emergent socs	formal specification;hardware software codesign;system modeling;modeling and simulation;steady state hardware computational modeling computer architecture system on a chip clocks real time systems permission system analysis and design design engineering;transient response;system on chip;static mapping hardware software codesign computer system modeling simulation system on chip design transient behaviors formal specification hierarchical decomposition;transient response hardware software codesign formal specification digital simulation microprocessor chips;computer system modeling and simulation;system on chip design;simulation environment;digital simulation;microprocessor chips;steady state;transient behavior	We introduce a formal basis for viewing computer systems as mixed steady state and non-steady state (transient) behaviors to motivate novel design strategies resulting from simultaneous consideration of function, scheduling and architecture. We relate three design styles: hierarchical decomposition, static mapping and directed platform that have traditionally been separate. By considering them together, we reason that once a steady state system is mapped to an architecture, the unused processing and communication power may be viewed as a platform for a transient system, ultimately resulting in more effective design approaches that ease the static mapping problem while still allowing for effective utilization of resources. Our simulation environment, frequency interleaving, mixes a formal and experimental approach as illustrated in an example.	digital electronics;emergence;formal system;forward error correction;general-purpose modeling;real-time clock;real-time computing;requirement;scheduling (computing);semantics (computer science);simulation;steady state;system on a chip;systems design;transient state	JoAnn M. Paul;Arne J. Suppé;Donald E. Thomas	2001		10.1145/500001.500062	embedded system;computer architecture;real-time computing;computer science	EDA	5.131538024401487	52.79134257677657	128791
a8fd78ba06decb59ff72ea3409554154e0b880dd	an architectural comparison of 32-bit microprocessors	microprocessors;cmos technology;memory management;packaging;computer architecture;electronics industry;circuits;microelectronics;microprocessors circuits central processing unit memory management packaging cmos technology microelectronics computer architecture electronics industry ieee news;ieee news;central processing unit	The supermicros have arrived. Their 32-bit structure-and their performance-quite justifiably earn them the name micromainframes.	32-bit;microprocessor	Amar Gupta;Hoo-Min D. Toong	1983	IEEE Micro	10.1109/MM.1983.291068	embedded system;packaging and labeling;electronic circuit;computer science;operating system;central processing unit;cmos;microelectronics;memory management	Visualization	7.527103580506239	50.63189617943762	128837
c77d20a9047525049ca1ce766f078c0ae76e379f	an autonomous mobile robot system for advanced microcontroller education	a d converter;control engineering education;microcontrollers;keyboards;converters;mobile robots control engineering education educational courses embedded systems microcontrollers;advanced microcontroller education;mobile robots microcontrollers optical control displays control systems light emitting diodes switching circuits optical switches keyboards optical sensors;mobile robots;autonomous mobile robot;optical switches;serial i o extensions;interface autonomous mobile robot microcontroller at89s51;chip;embedded systems;interface circuitries autonomous mobile robot system advanced microcontroller education main i o controls parallel i o extensions serial i o extensions at89s51;educational courses;interface;parallel i o extensions;main i o controls;parallel i o;optical sensors;dc motors;interface circuitries;at89s51;optical sensor;microcontroller;autonomous mobile robot system;project planning	This paper introduces an autonomous mobile robot (AMR) system to enhance students’ project planning, designing, and implementing capabilities while taking the advanced microcontroller course. The AMR system includes three parts of interface circuitries: the main I/O controls, parallel I/O extensions, and serial I/O extensions. In the circuitries of the main I/O controls, an AT89S51 single chip is used as the major controller to trigger LEDs, switches, voice, and RS232 controls; the interface circuitries of the parallel I/O extensions consist of LCM display, keyboard, and optical sensors; as for the serial I/O extensions, 7-segment display, A/D converter, DC motors’ drivers, and memory controls are included. According to the students’ feedbacks, introducing the ARM system in the advanced microcontroller course can attract more students interesting in designing the microcontroller-based applications and influence them in preparation for the senior laboratory project.	adaptive multi-rate audio codec;autonomous robot;input/output;latent class model;microcontroller;mobile robot;network switch;parallel i/o;rs-232;sensor;seven-segment display	Chin-Ming Hsu;Hui-Mei Chao	2009	2009 Fifth International Joint Conference on INC, IMS and IDC	10.1109/NCM.2009.287	microcontroller;embedded system;real-time computing;telecommunications;computer science;operating system	Robotics	6.815556073935122	48.416378001036065	128858
50d6a2a48ffd8c2f9360f27a3f32a1e653141f11	memory-map selection for firm real-time sdram controllers	real-time requirement;sdram memory;memory-map selection;memory map configuration;different memory map;firm real-time sdram controller;overall memory power budget;memory-map configuration;real-time application requirement;modern real-time embedded system;firm real-time requirement;memory controller;switches;microcontrollers;stochastic analysis;bandwidth;real time;satisfiability;bismuth;system on chip;memory management;schedules;real time systems	A modern real-time embedded system must support multiple concurrently running applications. To reduce costs, critical SoC components like SDRAM memories are often shared between applications with a variety of firm real-time requirements. To guarantee that the system works as intended, the memory controller must be configured such that all the real-time requirements of all sharing applications are satisfied. The attainable worst-case bandwidth, latency, and power of the memory depend largely on memory map configuration. Sharing SDRAM amongst multiple applications is challenging, since their requirements might call for different memory maps.  This paper presents an exploration of the memory-map design space. Two contributions improve the memory-map selection procedure. The first contribution reduces the minimum access granularity by interleaving requests over a configurable number of banks instead of all banks. This technique is beneficial for worst-case performance in terms of bandwidth, latency and power. As a second contribution, we present a methodology to derive a memory-map configuration, i.e. the access granularity and number of interleaved banks, from a specification of the real-time application requirements and an overall memory power budget.	best, worst and average case;cartography;embedded system;forward error correction;memory controller;memory map;real-time clock;real-time computing;requirement;system on a chip	Sven Goossens;Tim Kouters;Benny Akesson;Kees G. W. Goossens	2012	2012 Design, Automation & Test in Europe Conference & Exhibition (DATE)		system on a chip;microcontroller;stochastic process;embedded system;interleaved memory;electronic engineering;parallel computing;real-time computing;network switch;schedule;computer science;operating system;bismuth;memory controller;registered memory;memory bandwidth;bandwidth;satisfiability;memory management	Embedded	-3.9968129759130244	57.578635168547464	128915
96c7781c944fd3897f297de18ce64d18985448dd	spill-free parallel scheduling of basic blocks	scheduling algorithm;functional unit;machine model;divide and conquer	Software pipelining is a technique that reforms the loop to improve execution time. Iterations are executed in overlapped fashion to increase parallelism. Modulo scheduling places each operation so that the schedule is legal when replicated and offset by a target initiation interval. This process is repeated with larger initiation intervals until success is achieved. Kernel recognition methods schedule operations as rapidly as possible until a pattern is recognized. These two distinctly different methods have various strengths and weaknesses. This paper explores the benefits and draw-backs of each.	basic block;iteration;modulo operation;parallel computing;pipeline (computing);run time (program lifecycle phase);scheduling (computing);software pipelining	B. Natarajan;Mike Schlansker	1995	Proceedings of the 28th Annual International Symposium on Microarchitecture	10.1145/225160.225182	fair-share scheduling;parallel computing;divide and conquer algorithms;computer science;theoretical computer science;operating system;two-level scheduling;distributed computing;scheduling	Arch	-2.072421123777539	51.572804696517125	128960
1e60fbbf41e1faa25b577b4f9c21d1d4cbd32e7f	genetic-algorithm-based fpga architectural exploration using analytical models	analytical models;general purpose fpga architectures;application specific fpga architectures;genetic algorithm;power	FPGA architectural optimization has emerged as one of the most important digital design challenges. In recent years, experimental methods have been replaced by analytical ones to find the optimized architecture. Time is the main reason for this replacement. Conventional Geometric Programming (GP) is a routine framework to solve analytical models, including area, delay, and power models. In this article, we discuss the application of the Genetic Algorithm (GA) to the design of FPGA architectures. The performance model has been integrated into the Genetic Algorithm framework in order to investigate the impact of various architectural parameters on the performance efficiency of FPGAs. This way, we are able to rapidly analyze FPGA architectures and select the best one. The main advantages of using GA versus GP are concurrency and speed. The results show that concurrent optimization of high-level architecture parameters, including lookup table size (K) and cluster size (N), and low-level parameters, like scaling of transistors, is possible for GA, whereas GP does not capture K and N under its concurrency and it needs to exhaustively search all possible combinations of K and N. The results also show that more than two orders of magnitude in runtime improvement in comparison with GP-based analysis is achieved.	benchmark (computing);computer-aided design;concurrency (computer science);field-programmable gate array;genetic algorithm;geometric programming;high- and low-level;high-level architecture;image scaling;logic synthesis;lookup table;mathematical optimization;optimization problem;software release life cycle;transistor	Hossein Mehri;Bijan Alizadeh	2016	ACM Trans. Design Autom. Electr. Syst.	10.1145/2939372	embedded system;computer architecture;parallel computing;genetic algorithm;computer science;theoretical computer science;power	EDA	1.346331606936812	55.65186599158943	129192
3cb25077d975e02f71344d80c4472f522037017b	test exploration and validation using transaction level models	tlms suit;test exploration;transaction level model;test infrastructure;test design space exploration;significant amount;communication-centric view;functional design space;test stimulus;test strategy;computational modeling;system on a chip;design for testability;system testing;transaction level modeling;concurrent computing;solid modeling;system on chip;design for test;context modeling;discrete event simulation;space exploration;scheduling	The complexity of the test infrastructure and test strategies in systems-on-chip approaches the complexity of the functional design space. This paper presents test design space exploration and validation of test strategies and schedules using transaction level models (TLMs). Since many aspects of testing involve the transfer of a significant amount of test stimuli and responses, the communication-centric view of TLMs suits this purpose exceptionally well.	design space exploration;functional design;system on a chip;test design;transaction-level modeling	Michael Andreas Kochte;Christian G. Zoellin;Michael E. Imhof;Rauf Salimi Khaligh;Martin Radetzki;Hans-Joachim Wunderlich;Stefano Di Carlo;Paolo Prinetto	2009	2009 Design, Automation & Test in Europe Conference & Exhibition		system on a chip;embedded system;computer architecture;parallel computing;real-time computing;concurrent computing;computer science;operating system;design for testing;test method;test management approach	EDA	5.428768771284636	52.78488447334764	129230
c59db213b5db9d8a46a4596b9597b3005025d550	high-level algorithmic complexity evaluation for system design	outil logiciel;conception conjointe;evaluation systeme;software hardware partitioning;software tool;instruments;algorithmique;algorithm complexity;instrumentation;analisis sistema;gestion labor;concepcion sistema;diseno conjunto;systeme multimedia;teoria sistema;instrumentacion;complejidad algoritmo;evaluacion sistema;complexity analysis;analisis automatico;multimedia systems;analisis programa;system evaluation;automatic analysis;gestion tâche;complexite algorithme;complexity measure;codesign;systems theory;algorithmics;algoritmica;herramienta controlada por logicial;system design;lts3;theorie systeme;signal processing;mesure complexite;analyse automatique;system analysis;analyse systeme;program analysis;task scheduling;analyse programme;medida complexidad;conception systeme;software implementation;software instrumentation	The increasing complexity of processing algorithms has led to the need of more and more intensive specification and validation by means of software implementations. As the complexity grows, the intuitive understanding of the specific processing needs becomes harder and harder. Hence, the architectural implementation choices or the choices between different possible software/hardware partitioning become extremely difficult tasks. Moreover, it is also desirable to understand and measure the algorithm complexity at the highest possible level near to the algorithmic level so as to be able to take the more appropriate actions. Automatic tools to perform such analysis become nowadays a fundamental need. In this paper, the requirements of a suitable algorithmic complexity evaluation technology are discussed, with a particular emphasis to the problem of the analysis of multimedia systems and signal processing algorithms. A brief review about limitations and weaknesses of existing tools is given, specifying the characteristics of ideal ‘‘complexity evaluation systems’’. A new approach is described, called here Software Instrumentation Tool, SIT, yielding an automatic software tool able to extract information not depending on the simulation platform, keeping into account specific input data and resulting in a good and useful measure of the desired high-level algorithmic complexity. 2003 Elsevier Science B.V. All rights reserved.	abstraction layer;algorithm;algorithm design;analysis of algorithms;c++;computational complexity theory;critical path method;data parallelism;database;executable;high- and low-level;memory hierarchy;operator overloading;parallel computing;programming tool;requirement;rewriting;sensor;signal processing;simulation;systems design;working set	Massimo Ravasi;Marco Mattavelli	2003	Journal of Systems Architecture	10.1016/S1383-7621(03)00038-9	program analysis;co-design;instrumentation;complexity;simulation;computer science;signal processing;worst-case complexity;system analysis;algorithmics;systems theory;instrumentation;algorithm;programming complexity;systems design	SE	2.371597485426869	53.33492389496145	129257
10b8d4571f0a68cfe8828179dfa4ed4638af9e89	implementing low-cost fault tolerance via hybrid synchronous/asynchronous checks	superscalar pipeline;transient faults;hybrid synchronous asynchronous checks;compile time analysis	As semiconductor technologies scale down to deep sub-micron dimensions, transient faults will soon become a critical reliability concern. Due to their prohibitive costs, traditional high-end solutions are unacceptable for the mainstream commodity market. This paper presents Ftpipe, a hybrid software/hardware solution, which provides su±cient fault coverage with a®ordable overhead for single-threaded programs running on commodity systems. Leveraging existing exception mechanisms with minor modi ̄cations to handle exception-causing faults, Ftpipe focuses on tolerating silent data corruptions by using compile-time analysis and performing selective instruction replication in a modern superscalar pipeline extended with minimal hardware overhead. Unlike existing instruction replication-based solutions, which detect faults by synchronous checks, the Ftpipe platform has exploited a novel hybrid synchronous/asynchronous check method for the replicated instructions. In this manner, better performance can be obtained without degradation of fault coverage. By synchronous checks, the validation of the result of a replicated instruction must be ̄nished before it is committed, whereas such a guarantee is not required by an asynchronous check. Evaluation using a set of nine programs from the Mibench benchmark suite demonstrates that Ftpipe can tolerate 89.8% of transient faults under a modest performance overhead of 20.1%.	benchmark (computing);compile time;compiler;elegant degradation;fault coverage;fault injection;fault tolerance;overhead (computing);semiconductor;superscalar processor;thread (computing)	Jianli Li;QingPing Tan;Lanfang Tan	2013	Journal of Circuits, Systems, and Computers	10.1142/S0218126613500588	embedded system;real-time computing;engineering;operating system;distributed computing	Arch	6.7139913349058675	59.313925823804915	129274
4d68cec2a4959be79264268c7c38326307f4eb57	migrating software to hardware on fpgas	hardware software codesign;logic design;design flow;hardware field programmable gate arrays application software acceleration software performance throughput fabrics software algorithms software maintenance encoding;general purpose processor;embedded systems;application specific integrated circuits;g 729 audio encoding algorithm software migration fpga embedded processors tensilica arc hardware extensions design flow application specific hardware blocks software execution general purpose processor;embedded systems hardware software codesign logic design field programmable gate arrays microprocessor chips application specific integrated circuits;field programmable gate arrays;critical section;embedded processor;hardware implementation;microprocessor chips	The demands on embedded processors are growing faster than CPU developers can respond. This has lead to a number of academic and commercial processors, such as Tensilica (Leibson, 2003) and ARC, which allow hardware extensions in the form of new instructions to improve the overall throughput. Unfortunately, it is rarely obvious at the application level which new instruction would accelerate a given function. This paper proposes a design flow that migrates performance critical sections of software into hardware by automatically creating application specific hardware blocks that accelerate the overall software execution. The hardware implementation of the function is interfaced to the general-purpose processor, which runs the remainder of the software. Targeting FPGA fabric maintains the reprogrammable nature of the algorithm that was originally in software. We present our results on using this flow on the G.729 audio encoding algorithm.	algorithm;central processing unit;critical section;customer relationship management;design flow (eda);embedded system;field-programmable gate array;formal verification;g.729;general-purpose markup language;logic synthesis;overhead (computing);prototype;semiconductor intellectual property core;simulation;test bench;throughput	Russell Klein;Rajat Moona	2004	Proceedings. 2004 IEEE International Conference on Field- Programmable Technology (IEEE Cat. No.04EX921)	10.1109/FPT.2004.1393271	hardware compatibility list;embedded system;computer architecture;computing;parallel computing;logic synthesis;real-time computing;embedded software;hardware acceleration;reconfigurable computing;computer science;design flow;component-based software engineering;operating system;software construction;hardware architecture;critical section;application-specific integrated circuit;field-programmable gate array;software system	EDA	4.104017612663301	49.793268164825264	129349
4b9293919cafdbf6a7d0bb43d87365306cd591bb	a parameterized programmable mimo decoding architecture with a scalable instruction set and compiler	decoding;multiple input multiple output;modulation antennas decoding digital signal processing chips instruction sets matrix algebra mimo communication;hardware accelerator;user friendly design flow parameterized programmable mimo decoding architecture scalable instruction set compiler multiple input multiple output decoder accelerator integrated design environment accelerator architecture antenna configuration modulation scheme user programming digital signal processor matrix processing linear mimo decoding access scheme data memory scalable machine level instruction set hardware configuration;matrix algebra;signal processing;mimo accelerator application specific processor configurable multiple input multiple output mimo decoder;antennas;digital signal processing chips;linear accelerator;mimo decoding program processors hardware accelerator architectures signal processing algorithms modulation coding bandwidth runtime linear accelerators;mimo communication;instruction sets;modulation	We present a novel multiple-input multiple-output (MIMO) decoder accelerator and its associated integrated design environment. The accelerator architecture allows tradeoffs in decoding algorithm, antenna configuration, modulation scheme, and bandwidth at run-time via user programming. The accelerator delivers an improvement over a general purpose digital signal processor (DSP) reaching three orders of magnitude for matrix processing and linear MIMO decoding. The hardware architecture is user-configurable through ten independently set parameters. The parameterization allows independent control over the size and structure of the processing core as well as the structure, size, and access scheme of data memory. We provide a custom high level script and a scalable machine level instruction set and compiler. The elements of hardware configuration and programmability are combined in a user-friendly design flow that takes the MIMO decoder designer from simulation to hardware with dedicated-hardware-like performance in no time.	algorithm;compiler;digital signal processor;high-level programming language;integrated development environment;mimo;modulation;scalability;signal processing;simulation;usability	Karim Mohammed;Mohamed I. A. Mohamed;Babak Daneshrad	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2049592	embedded system;electronic engineering;parallel computing;real-time computing;hardware acceleration;computer science;operating system;signal processing;antenna;instruction set;linear particle accelerator;modulation	Arch	3.8379259898949845	47.95470814229488	129375
671a1004342358cfab494cf8ab638af2d2285d98	a software-based error detection technique using encoded signatures	software process improvement error detection fault simulation microcontrollers program control structures;circuit faults error correction control systems microcontrollers runtime fault detection hardware computer bugs computerized monitoring laboratories;microcontrollers;safety critical embedded systems;encoded signatures;fault simulation;software process improvement;monitoring routine;program control structures;fault injection software error detection encoded signatures control flow checking processors microcontrollers monitoring routine atmel mcs51;control flow checking;software based control flow checking;fault tolerance;control flow;atmel mcs51;experimental evaluation;error detection;fault injection;software error detection;processors	In this paper, a software-based control flow checking technique called SWTES (software-based error detection technique using encoded signatures) is presented and evaluated. This technique is processor independent and can be applied to any kind of processors and microcontrollers. To implement this technique, the program is partitioned to a set of blocks and the encoded signatures are assigned during the compile time. In the run-time, the signatures are compared with the expected ones by a monitoring routine. The proposed technique is experimentally evaluated on an ATMEL MCS51 microcontroller using software implemented fault injection (SWIFI). The results show that this technique detects about 90% of the injected errors. The memory overhead is about 135% on average, and the performance overhead varies between 11% and 191% depending on the workload used	algorithm;antivirus software;atmel avr;central processing unit;compile time;compiler;control flow;electronic signature;error detection and correction;experiment;fault injection;microcontroller;overhead (computing);type signature	Yasser Sedaghat;Seyed Ghassem Miremadi;Mahdi Fazeli	2006	2006 21st IEEE International Symposium on Defect and Fault Tolerance in VLSI Systems	10.1109/DFT.2006.11	microcontroller;embedded system;fault tolerance;electronic engineering;parallel computing;real-time computing;error detection and correction;computer science;operating system;central processing unit;distributed computing;control flow	Embedded	7.497460748937536	58.59664386002489	129432
6036ed313813c15a1da611b8c06220e769a2670f	lupis: latch-up based ultra efficient processing in-memory system		Internet of Things (IoT) involves processing massive data. This poses a huge challenge in the current computing systems due to the limited memory bandwidth. Processing in-memory (PIM) is a promising candidate to minimize this bottleneck and reduce the performance gap between processor and memory latency. We propose LUPIS (Latch-Up based Processing In-memory System) for nonvolatile memory (NVM). Unlike existing PIM techniques, which mainly focus on bitwise operation based computations and involve considerable latency and area penalty, our design facilitates computations like addition and multiplication with very low latency. This makes the system faster and more efficient as compared to the state-of-the-art technologies. We evaluate LUPIS at both circuit-level and application-level. Our evaluations show that LUPIS can enhance the performance and energy efficiency by 62× and 484× respectively as compared to a recent GPGPU architecture. Compared to the state-of-the-art PIM accelerator, our design presents 12.7× and 20.9× improvement in latency and energy consumption with insignificant overhead of 21% for area increase and one cycle for latency delay.	bitwise operation;cas latency;computation;electronic circuit;general-purpose computing on graphics processing units;in-memory database;internet of things;latch-up;memory bandwidth;non-volatile memory;nonvolatile bios memory;overhead (computing);requirement;thyristor	Joonseop Sim;Mohsen Imani;Woojin Choi;Yeseong Kim;Tajana Simunic	2018	2018 19th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2018.8357265	latency (engineering);real-time computing;latency (engineering);architecture;cas latency;non-volatile memory;computer science;general-purpose computing on graphics processing units;memory bandwidth;bottleneck	Arch	-3.2439892264070314	47.982186120512836	130087
46ff90e9cf7269af82048ff54c9317fe42be9ab1	scalable and flexible cosimulation of soc designs with heterogeneous multi-processor target architectures	is 95 cdma cellular phone system design flexible cosimulation soc designs heterogeneous multi processor target architectures modularity scalability flexibility object oriented simulation environment simulation performance;is 95 cdma cellular phone system design;object oriented methods hardware software codesign circuit simulation application specific integrated circuits multiprocessing systems;baseband;simulation performance;object oriented methods;hardware software codesign;signal design;object oriented simulation;soc designs;embedded system;computer architecture;circuit simulation;object oriented;application specific integrated circuits;system design;modulation coding;multiaccess communication computer architecture scalability cellular phones embedded system baseband modems embedded computing modulation coding signal design;heterogeneous multi processor target architectures;modems;modularity;scalability;multiprocessing systems;object oriented simulation environment;flexibility;embedded computing;cellular phones;flexible cosimulation;multiaccess communication	In this paper, we present a cosimulation environment that provides modularity, scalability, and flexibility in cosimulation of SoC designs with heterogeneous multi-processor target architectures. Our cosimulation environment is based on an object-oriented simulation environment, SystemC. Exploiting the object orientation in SystemC representation, we achieve modularity and scalability of cosimulation by developing modular cosimulation interfaces. The object orientation also enables mixed-level cosimulation to be easily implemented thereby the designer can have flexibility in trade off between simulation performance and accuracy. Experiments with an IS-95 CDMA cellular phone system design show the effectiveness of the cosimulation environment.	mobile phone;multiprocessing;scalability;simulation;systemc;systems design	Patrice Gerin;Sungjoo Yoo;Gabriela Nicolescu;Ahmed Amine Jerraya	2001		10.1145/370155.370276	embedded system;computer architecture;real-time computing;scalability;computer science;operating system;baseband;modularity;application-specific integrated circuit;object-oriented programming;systems design	EDA	4.392878490419734	53.03373564191071	130092
5a09b78ad855501bc0b90e1c1fd0cad27bf11467	cohra: hardware-software cosynthesis of hierarchical heterogeneous distributed embedded systems	processing element;distributed system;outil logiciel;concepcion asistida;hierarchical system;evaluation performance;computer aided design;software tool;architecture systeme;systeme reparti;cosynthese;hardware software codesign;performance evaluation;fault tolerant;integrated circuit;distributed embedded system;real time;evaluacion prestacion;embedded system computer architecture real time systems hardware embedded software large scale systems costs scheduling algorithm pipeline processing concurrent computing;systeme hierarchise;cohra;circuito integrado;indexing terms;multimedia systems;embedded system;embedded systems;sistema jerarquizado;large scale;sistema repartido;low power;application specific integrated circuits hardware software codesign embedded systems scheduling fault tolerance;system synthesis;periodic tasks;application specific integrated circuits;herramienta controlada por logicial;scheduling;synthese systeme;fault tolerance;sintesis sistema;conception assistee;arquitectura sistema;ordonamiento;computer hardware;task graphs;system architecture;materiel informatique;material informatica;low power objectives cohra hardware software cosynthesis hierarchical heterogeneous distributed embedded systems embedded system architecture partitioning acyclic task graphs processing elements nonhierarchical task graphs hierarchical task graphs periodic task graphs pipelining concurrent modes sequential modes preemptive static scheduling nonpreemptive static scheduling task clustering technique association arrays multimedia systems fault tolerance;ordonnancement;circuit integre;cosynthesis;distributed architecture	Hardware-software cosynthesis of an embedded system architecture entails partitioning of its specification into hardware and software modules such that its real-time and other constraints are met. Embedded systems are generally specified in terms of a set of acyclic task graphs. For medium- to large-scale embedded systems, the task graphs are usually hierarchical in nature. The embedded system architecture, which is the output of the cosynthesis system, may itself be nonhierarchical or hierarchical. Traditional nonhierarchical architectures create communication and processing bottlenecks and are impractical for large embedded systems. Such systems require a large number of processing elements and communication links connected in a hierarchical manner, thus forming a hierarchical distributed architecture, to meet performance and cost objectives. In this paper, we address the problem of hardware-software cosynthesis of hierarchical heterogeneous distributed embedded system architectures from hierarchical or nonhierarchical task graphs. Our cosynthesis algorithm has the following features: 1) it supports periodic task graphs with real-time constraints, 2) it supports pipelining of task graphs, 3) it supports a heterogeneous set of processing elements and communication links, 4) it allows both sequential and concurrent modes of communication and computation, 5) it employs a combination of preemptive and nonpreemptive static scheduling, 6) it employs a new task-clustering technique suitable for hierarchical task graphs, and 7) it uses the concept of association arrays to tackle the problem of multirate tasks encountered in multimedia systems. We show how our cosynthesis algorithm can be easily extended to consider fault tolerance or low-power objectives or both. Although hierarchical architectures have been proposed before, to the best of our knowledge, this is the first time the notion of hierarchical task graphs and hierarchical architectures has been supported in a cosynthesis algorithm.		Bharat P. Dave;Niraj K. Jha	1998	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.728913	embedded system;fault tolerance;electronic engineering;parallel computing;real-time computing;computer science;operating system;computer aided design;systems architecture	EDA	1.3070104647202418	53.696313739113826	130144
71bd3d1825d5dd84820eb6f8c35a5a49c4f4a77f	grok-int: generating real on-chip knowledge for interconnect delays using timing extraction	cyclones;logic;registers;field programmable gate arrays;routing;network routing;fpga	With continued scaling, all transistors are no longer created equal. The delay of a length 4 horizontal routing segment at coordinates (23,17) will differ from one at (12,14) in the same FPGA and from the same segment in another FPGA. The vendor tools give conservative values for these delays, but knowing exactly what these delays are can be invaluable. In this paper, we show how to obtain this information, inexpensively, using only components that already exist on the FPGA (configurable PLLs, registers, logic, and interconnect). The techniques we present are general and can be used to measure the delays of any resource on any FPGA with these components. We provide general algorithms for identifying the set of useful delay components, the set of measurements necessary to compute these delay components, and the calculations necessary to perform the computation. We demonstrate our techniques on the interconnect for an Altera Cyclone III (65nm). As a result, we are able to quantify over a 100 ps spread in delays for nominally identical routing segments on a single FPGA.	algorithm;computation;computer-aided design;cyclone;field-programmable gate array;grok;image scaling;processor register;ps (unix);routing;transistor	Benjamin Gojman;André DeHon	2014	2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2014.31	embedded system;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	Arch	7.2066555216351444	50.488179395778324	130410
b9b86613428b564e44632c9dcfa610222a36a2c9	throughput-memory footprint trade-off in synthesis of streaming software on embedded multiprocessors	modulo scheduling;iteration overlapping;software pipelining;stream applications;distributed memory message passing multiprocessor soc	We study the trade-off between throughput and memory footprint of embedded software that is synthesized from acyclic static dataflow (task graph) specifications targeting distributed memory multiprocessors. We identify iteration overlapping as a knob in the synthesis process by which one can trade application throughput for its memory requirement. Given an initial processor assignment and non-overlapped task schedule, we formally present underlying properties of the problem, such as constraints on a valid iteration overlapping, maximum possible throughput, and minimum memory footprint. Moreover, we develop an effective algorithm for generation of a rich set of design points that provide a range of trade-off options. Experimental results on a number of applications and architectures validate the effectiveness of our approach.	algorithm;control knob;dataflow;directed acyclic graph;distributed memory;embedded software;embedded system;iteration;memory footprint;throughput	Matin Hashemi;Mohammad H. Foroozannejad;Soheil Ghiasi	2013	ACM Trans. Embedded Comput. Syst.	10.1145/2539036.2539042	software pipelining;embedded system;parallel computing;real-time computing;distributed memory;computer science;operating system;distributed computing;programming language;cache-only memory architecture	Embedded	-0.11941102092795505	51.39758708289321	130487
5910dfcccb21ad8e49b279ca24db8625906b6550	cell phone integration: sip, soc, and pop	mobile phone application;mobile phone application cost effective decision product design cycle system in package system on chip package on package integration;package on package integration;cost effective decision;rf integration;system in package;memory integration;sip;mobile phone;analog integration;memory integration sip soc pip pop analog integration rf integration;system on chip;mobile radio;product design cycle;pop;cellular phones random access memory packaging energy management variable structure systems leakage current gsm multiaccess communication costs bandwidth;cost effectiveness;soc;system on chip mobile radio system in package;pip	Engineers must make many cost-effective decisions during a product's design cycle. One challenge is deciding on the best packaging for their products. This article presents trade-offs among system-in-package, system-on-chip, and package-on-package integration for mobile phone applications	mobile app;mobile phone;package on package;software engineer;system in package;system on a chip	Peter Rickert;William Krenik	2006	IEEE Design & Test of Computers	10.1109/MDT.2006.64	system on a chip;embedded system;electronic engineering;computer hardware;computer science;engineering	EDA	9.999860520993586	55.37314628929506	130520
3c3036de952a4d4e41b2160bfe332694ba44e28b	on-chip checkpointing with 3d-stacked memories	random access memory;error correction codes;memory management;three dimensional integrated circuits energy consumption integrated circuit testing integrated memory circuits low power electronics microprocessor chips;energy consumption deep submicron process technologies soft errors computing systems program execution energy overheads 3d stacking technologies 3d stacked processor memory module on chip checkpointing mechanism;checkpointing;system on chip;energy consumption;three dimensional displays;checkpointing system on chip three dimensional displays memory management energy consumption error correction codes random access memory	Since deep-submicron process technologies induce soft errors, advanced computing systems face a low dependability problem. Checkpointing, which copies data required for continuing the program execution as a backup, is expected as a promising approach to keeping a high dependability. However, checkpointing causes additional memory accesses, which cause performance and energy overheads. To reduce these overheads, this paper focuses on 3D-stacking technologies. Since the technologies realize large on-chip memories with a short latency and a high bandwidth, the overheads of checkpointing are expected to decrease. In order to examine the reduction of the overheads, this paper supposes a future 3D-stacked processor-memory module, and proposes an on-chip checkpointing mechanism. The evaluation results indicate that the on-chip checkpointing with 3D-stacked memories can reduce the execution time by 15% and energy consumption by 26% on average, compared with the checkpointing mechanism with off-chip memories.	application checkpointing;backup;dependability;disk mirroring;ll parser;memory hierarchy;memory module;microprocessor;numerical aperture;page (computer memory);page table;paging;public-key cryptography;run time (program lifecycle phase);soft error;stacking;very-large-scale integration;weatherstar	Masayuki Sato;Ryusuke Egawa;Hiroyuki Takizawa;Hiroaki Kobayashi	2014	2014 International 3D Systems Integration Conference (3DIC)	10.1109/3DIC.2014.7152173	embedded system;parallel computing;real-time computing;computer science	HPC	6.881849449798158	60.2310226572805	130618
8c97c3bc993ddfad6c40da07e6a3a0081e673f86	system-level scheduling on instruction cell based reconfigurable systems	high performance instruction cell;system level scheduling;scheduling;microprocessor chips;typical scheduling method;reconfigurable architectures;instruction level parallelism;chaining reconfigurable scheduling algorithm;new operation;logic design;reconfigurable system;reconfigurable scheduling algorithm;list scheduling;instruction sets;maximizes instruction level parallelism;vliw architectures;advanced operation;system-level scheduling;instruction cell based reconfigurable systems;reconfigurable instruction cell;higher performance;digital signal processing;scheduling algorithm;field programmable gate arrays;throughput;vliw;computer architecture;application specific integrated circuits;job shop scheduling;parallel processing	This paper presents a new operation chaining reconfigurable scheduling algorithm (CRS) based on list scheduling that maximizes instruction level parallelism available in distributed high performance instruction cell based reconfigurable systems. Unlike other typical scheduling methods, it considers the placement and routing effect, register assignment and advanced operation chaining compilation technique to generate higher performance scheduled code. The effectiveness of this approach is demonstrated here using a recently developed industrial distributed reconfigurable instruction cell based architecture [Lee,2003]. The results show that schedules using this approach achieve equivalent throughput to VLIW architectures but at much lower power consumption	algorithm;compiler;instruction scheduling;instruction-level parallelism;list scheduling;parallel computing;place and route;placement (eda);routing;scheduling (computing);throughput;very long instruction word	Ying Yi;Ioannis Nousias;Mark Milward;Sami Khawam;Tughrul Arslan;Iain Lindsay	2006	Proceedings of the Design Automation & Test in Europe Conference		fair-share scheduling;job shop scheduling;parallel processing;computer architecture;parallel computing;real-time computing;computer science;operating system;instruction scheduling;scheduling	EDA	-0.4223631809403712	51.791534771320876	130700
ab670386d8846170962b83f6394fbf8874fc24e5	hot chips-hot stuff	industrial electronics;manufacturing industries;computer industry;chip;electronics industry;computer aided manufacturing;workstations;semiconductor device manufacture;manufacturing industries semiconductor device manufacture computer industry electronics industry industrial electronics workstations hardware economic indicators computer aided manufacturing marketing and sales;hardware;economic indicators;marketing and sales	First Page of the Article	hot chips;hot spare	Allen J. Baum;Allan L. Smith	1998	IEEE Micro	10.1109/MM.1998.671398	chip;electronics;telecommunications;computer science;operating system;computer-aided manufacturing	Visualization	8.72359676417308	55.27656339308315	130955
b419ce5f38c3a05fef49dc21ac5acd6fa2a91e9e	an intermediate format for automatic generation of mpsoc virtual prototypes	executable virtual prototypes;mjpeg decoder;design automation;mjpeg decoder mpsoc virtual prototypes multiprocessor systems on chip data structure api executable virtual prototypes heterogeneous tool environment;decoding;legged locomotion;api;software prototyping;heterogeneous tool environment;prototypes;design flow;mpsoc virtual prototypes;automatic generation;design representation;virtual prototyping application program interfaces data structures digital signal processing chips integrated circuit design system on chip;integrated circuit design;virtual prototyping;system on chip;data structures;application program interfaces;focal point;multiprocessor system on chip;intermediate format;digital signal processing chips;multiprocessing systems;multiprocessor systems on chip;network on a chip;data structure;virtual prototyping hardware software prototyping prototypes multiprocessing systems design automation decoding network on a chip legged locomotion laboratories;hardware	The design of multiprocessor systems-on-chip (MPSoC) involves many abstraction levels and executable models, from specification to implementation. The usage of a design representation that remains homogeneous along the design flow is a key element of design automation. Although many standard languages exists, their computer representation is not defined or does not allow model interchange. In this paper, we propose an intermediate format based on a data structure and an API to capture MPSoC designs and automate the generation of executable virtual prototypes. The format is compatible with existing tools and languages, making it an interesting focal point of a heterogeneous tool environment. The format is used to generate the virtual prototype of an MJPEG decoder, showing immediate benefits regarding architecture exploration and validation.	application programming interface;data structure;design flow (eda);electronic design automation;executable;focal (programming language);iterator;mpsoc;multiprocessing;prototype;refinement (computing);simulation;system on a chip;systemc	Alexandre Chureau;Frédéric Pétrot	2008	2008 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation	10.1109/ICSAMOS.2008.4664860	embedded system;computer architecture;parallel computing;computer science	EDA	3.1335815978372903	51.033028043108416	131184
713d303c95231d9e84fa35fc4047928095b30065	a second-generation sensor network processor with application-driven memory optimizations and out-of-order execution	energy efficiency;energy per instruction;microprocessor;lithium ion battery;energy efficient;instruction set architecture;sensor network;pressure sensor;out of order execution;low power;energy optimization;memory optimization;memory architecture;ultra compact;memory systems;memory organization;pareto optimality	In this paper we present a second-generation sensor network processor which consumes less than one picoJoule per instruction (typical processors use 100's to 1000's of picoJoules per instruction). As in our first-generation design effort, we strive to build microarchitectures that minimize area to reduce leakage, maximize transistor utility to reduce the energy-optimal voltage, and optimize CPI for efficient processing. The new design builds on our previous work to develop a low-power subthreshold-voltage sensor processor, this time improving the design by focusing on ISA, memory system design, and microarchitectural optimizations that reduce overall design size and improve energy-per-instruction. The new design employs 8-bit datapaths and an ultra-compact 12-bit wide RISC instruction set architecture, which enables high code density via micro-operations and flexible operand modes. The design also features a unique memory architecture with prefetch buffer and predecoded address bits, which permits both faster access to the memory and smaller instructions due to few address bits. To achieve efficient processing, the design incorporates branch speculation and out-of-order execution, but in a simplified form for reduced area and leakage-energy overheads. Using SPICE-level timing and power simulation, we find that these optimizations produce a number of Pareto-optimal designs with varied performance-energy tradeoffs. Our most efficient design is capable of running at 142 kHz (0.1 MIPS) while consuming only 600 fJ/instruction, allowing the processor to run continuously for 41 years on the energy stored in a miniature 1g lithium-ion battery. Work is ongoing to incorporate this design into an intra-ocular pressure sensor.	12-bit;8-bit;branch predictor;central processing unit;datapath;joule;low-power broadcasting;micro-operation;microarchitecture;network processor;operand;out-of-order execution;pareto efficiency;spice;sensor;simulation;spectral leakage;systems design;transistor	Leyla Nazhandali;Michael Minuth;Bo Zhai;Javin Olson;Todd M. Austin;David Blaauw	2005		10.1145/1086297.1086330	embedded system;parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use	Arch	-1.7871074305470327	56.042084888478	131256
6f448cb9c093696c543eb6c5271396bc88e9a5c9	a high performance seu-tolerant latch for nanoscale cmos technology	hardened-by-design;high performance;nanoscale cmos technology;higher robustness;power-delay product;muller c-element;integrated circuit reliability;high performance seu-tolerant;size 45 nm;power dissipation;lower power-delay product;radiation-induced single event upset;spice simulation;nm cmos technology;soft error masking;c-element;classical implementation;propagation delay;alternative hardened solution;cmos logic circuits;redundancy;spice simulations;flip-flops;and dual modular redundancy hardening;nanoelectronics;single event upset;radiation hardening (electronics);dual modular redundancy hardening;high performance seu-tolerant latch reliability;soft error;transient fault;robustness	This paper presents a high performance latch to tolerate radiation-induced single event upset in 45 nm CMOS technology. The latch can improve robustness by masking the soft errors utilizing Muller C-element and dual modular redundancy hardening. The power dissipation, propagation delay and reliability of the presented SEU-tolerant latch are analyzed by SPICE simulations. The results show that the presented latch provides a higher robustness and lower power-delay product than classical implementations and alternative hardened solutions.	c-element;cmos;clock gating;dual modular redundancy;power–delay product;propagation delay;spice;simulation;single event upset;software propagation;triple modular redundancy	Zhengfeng Huang	2014	2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)		nanoelectronics;embedded system;electronic engineering;real-time computing;soft error;computer science;engineering;redundancy	EDA	9.356881422035107	59.90771512537612	131437
e0e8076b8a7ba99f1926439d4fe5ff65acb292ba	an extended scheduling technique for software pipelining	modulo scheduling;vliw;superscalar processors;software pipelining;instruction level parallelism	Abstract   Software pipelining is a practical and efficient loop scheduling technique used in generating efficient code for VLIW architectures, superscalar processors and microcode compaction for horizontal micro-architectures. Software pipelining allows exploitation of parallelism inside and across loop iterations. In this paper we propose an algorithm capable of generating an optimal schedule for initiating loop iterations with constant initiation interval. Extension of standard modulo scheduling technique provides the framework for including resource and precedence constraints. The proposed algorithm is based on generating t he schedule from more than one loop iterations.	pipeline (computing);scheduling (computing);software pipelining	Dagung Lu;Prasenjit Biswas	1993	Microprocessing and Microprogramming	10.1016/0165-6074(93)90025-G	software pipelining;loop inversion;computer architecture;parallel computing;real-time computing;computer science;very long instruction word;instruction-level parallelism	PL	-1.9056503436592334	51.64250380480235	131779
51ae8a4572cc1562226a327141f5e72dadebddaf	mpsocs and multicore microcontrollers for embedded pid control: a detailed study	three term control field programmable gate arrays microcontrollers multiprocessing systems system on chip;pd control;field programmable gate arrays multicore processing algorithm design and analysis fuzzy logic multiprocessing systems pd control;multicore processing;hil simulations mpsoc multicore microcontrollers embedded pid control multiprocessor implementations proportional integral derivative controller field programmable gate array fpga multiprocessor system on chip mcu hardware in the loop;multiprocessing systems;field programmable gate arrays;algorithm design and analysis;benchmark testing	This paper presents different multiprocessor implementations of the proportional-integral-derivative (PID) controller using two technologies: 1) field programmable gate array (FPGA)-based multiprocessor system-on-chip (MPSoC); and 2) multicore microcontrollers (MCUs). Techniques to implement a parallelized PID controller, a multi-PID controller, and a self-tuning PID controller are proposed. These techniques are verified using hardware (HW) in the loop (HIL) simulations. Then, the paper presents a detailed case study of an embedded real-time (RT) self-tuning PID controller for a 1-degree-of-freedom (1-DOF) aerodynamical system. This includes controller design, parameters tuning, and implementation using a multiprocessor system. Results proved the effectiveness of the proposed techniques to improve performance and functionality. It is shown that customizing HW and software (SW) within MPSoCs provides higher RT performance. Moreover, using multicore MCUs can reduce design time, implementation time, and cost, while keeping adequate performance. Therefore, it is possible to realize and implement complex RT embedded controllers that employ advanced control algorithms in rapid, effective, and cost-efficient fashion.	advanced process control;algorithm;computer hardware;control system;cost efficiency;embedded controller;embedded system;field-programmable gate array;hil bus;hardware-in-the-loop simulation;mpsoc;microcontroller;multi-core processor;multiprocessing;pid;parallel computing;performance tuning;real-time clock;real-time computing;requirement;self-tuning;speedup;system on a chip	Hassan A. Youness;Mohammed Moness;Mahmoud Khaled	2014	IEEE Transactions on Industrial Informatics	10.1109/TII.2014.2355036	multi-core processor;embedded system;algorithm design;benchmark;computer architecture;real-time computing;computer science;operating system;field-programmable gate array	Embedded	1.3105252653666852	55.022404706038174	131847
00eef94cbf04e8779e367e8a1aae811bda05c575	a structured system methodology for fpga based system-on-a-chip design	video signal processing field programmable gate arrays system on chip logic design;image processing;logic design;video signal processing;physical layer;heterogeneous integrated performance;high end fpga;reconfigurability;system on a chip;chip;computer architecture;large scale;engines;system on chip;sonic on a chip;video image processing structured system methodology high end fpga system on a chip design logic resources heterogeneous integrated performance productivity reconfigurability sonic on a chip;structured system methodology;field programmable gate arrays system on a chip productivity physical layer engines design methodology costs computer architecture timing hardware;productivity;field programmable gate arrays;system on a chip design;logic resources;video image processing;hardware;design methodology;timing	The ever increasing quantities of logic resources combined with heterogeneous integrated performance enhancing primitives in high-end FPGAs creates a design complexity challenge that requires new methodologies to address. We present a structured system based design methodology which aims to increase productivity and exploit reconfigurability in large scale FPGAs. The methodology is exemplified by sonic-on-a-chip, a video image processing system.	field-programmable gate array;image processing;reconfigurability;system on a chip	N. Pete Sedcole;Peter Y. K. Cheung;George A. Constantinides;Wayne Luk	2004	12th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2004.10	system on a chip;embedded system;parallel computing;real-time computing;image processing;computer science	EDA	3.4605293702284343	52.29013551178682	131858
f5348432c4a87da704c2fbacaea4486202a03400	uninterpreted modeling using the vhsic hardware description language (vhdl)	design process;queuing model;queueing theory;vlsi circuit analysis computing circuit cad digital integrated circuits petri nets queueing theory software tools specification languages;system performance;continuous single path design environment very high speed ic circuit cad vhsic hardware description language petri nets queuing models tokens design analysis verification uninterpreted models;digital integrated circuits;specification languages;design environment;vhsic hardware description language;vlsi;hardware design;software tools;circuit cad;petri nets;petri net;circuit analysis computing;very high speed integrated circuits hardware design languages digital systems process design petri nets design engineering systems engineering and theory switches queueing analysis design methodology	The authors discuss methodologies and tools that allow a system to be analyzed using Petri nets or queuing models. Models at this level contain tokens rather than values, and the function of blocks remains undefined. Such analysis is performed early in the design process to evaluate overall system performance. Different methodologies and tools are available to allow design analysis and verification at interpreted levels through hardware design language (HDL) descriptions. Tokens are replaced with specific values for the representation of signals. The methodology presented allows the designer to create uninterpreted models in an environment already capable of interpreted modeling, the VHSIC hardware description language (VHDL). Uninterpreted modeling in an HDL is the first step in the creation of a continuous single-path design environment. >	hardware description language;vhdl;vhsic	F. T. Hady;James H. Aylor;Ronald D. Williams;Ronald Waxman	1989		10.1109/ICCAD.1989.76929	embedded system;computer architecture;real-time computing;computer science;theoretical computer science;operating system;computer performance;programming language;petri net;statistics	Robotics	5.36176623273657	52.45558991894901	131888
c4946b15dc4afa7343a6c5c6326a6c350ed7b54c	behavioral synthesis optimization using multiple precision arithmetic	instruction level parallel;timing constraints;optimisation;fixed point arithmetic timing computer aided instruction concurrent computing hardware computer architecture digital arithmetic parallel processing costs application specific integrated circuits;concurrent computing;behavioral synthesis;image processing;multiple precision arithmetic units;video signal processing;architectural scheme;cost sensitivity;computer aided instruction;video processing;data type;computer architecture;parallel architectures;cmos digital integrated circuits;application specific integrated circuits;application specific integrated circuit;variable precision data;assignment methodology behavioral synthesis optimization multiple precision arithmetic image processing video processing fixed point arithmetic data types instruction level parallelism timing constraints computational requirements cost sensitivity multiple precision arithmetic units application specific integrated circuits architectural scheme collaborate addition variable precision data allocation methodology;cmos digital integrated circuits optimisation application specific integrated circuits digital signal processing chips image processing video signal processing digital arithmetic parallel architectures;multiple precision arithmetic;fixed point arithmetic;digital signal processing chips;digital arithmetic;assignment methodology;behavioral synthesis optimization;instruction level parallelism;allocation methodology;parallel processing;computational requirements;data types;collaborate addition;hardware;timing;time constraint	Modern image and video processing applications are characterized by a unique combination of arithmetic and computational features: fixed point arithmetic, a variety of short data types, high degree of instruction-level parallelism, strict timing constraints, high computational requirements, and high cost sensitivity. The current generation of behavioral synthesis tools does not address well this type of application. In this paper we explore the potential of using multiple precision arithmetic units to effectively support implementation of image and video processing applications as application specific integrated circuits. A new architectural scheme for collaborate addition of sets of variable precision data is proposed as well as an allocation and assignment methodology for multiple precision arithmetic units. Experimental results indicate the strong advantages of the proposed approach.	arbitrary-precision arithmetic;high-level synthesis;mathematical optimization	Milos D. Ercegovac;Darko Kirovski;George Mustafa;Miodrag Potkonjak	1998		10.1109/ICASSP.1998.678185	parallel processing;parallel computing;real-time computing;concurrent computing;data type;image processing;computer science;theoretical computer science;application-specific integrated circuit	EDA	2.1661689476213644	51.74319467612694	131922
6955d9528b640ba75029f863a7285435dc7d1360	configurable processors for embedded computing	field programmable gate array;microprocessor chips embedded systems;embedded system;chip;embedded systems;system design;application specific integrated circuit;embedded computing configurable processors software content embedded systems autonomous programs programmable processors cost constraints application specific integrated circuits reprogrammable fabrics field programmable gate arrays;embedded computing embedded software hardware application software field programmable gate arrays embedded system software design software performance costs application specific integrated circuits;embedded computing;embedded software;microprocessor chips	Computer W e have all heard about the increasing software content of embedded systems. To those who think of embedded software as autonomous programs hidden deep within the system, plugging away transparently and reliably on dedicated tasks, this increase might suggest that these programs are somehow becoming larger. In reality, the ongoing increases in processor performance let system designers implement in software what previously required dedicated or custom hardware blocks and accelerators. Indeed, given a choice, system designers might actually prefer the flexibility of implementing all embedded applications in software on programmable processors. However, parts of the applications must often run under critical time, performance, power, and cost constraints. Thus, designers have traditionally mapped these segments into custom hardware, such as application-specific integrated circuits (ASICs), or into reprogrammable fabrics, such as fieldprogrammable gate arrays (FPGAs).	application-specific integrated circuit;autonomous robot;central processing unit;computer performance;embedded software;embedded system;field-programmable gate array;semiconductor intellectual property core	Nikil D. Dutt;Kiyoung Choi	2003	IEEE Computer	10.1109/MC.2003.1160063	chip;embedded system;embedded operating system;computer architecture;real-time computing;embedded software;computer science;operating system;computer-on-module;application-specific integrated circuit;field-programmable gate array;software system;systems design;avionics software	EDA	8.40021067074497	55.32913206011495	131967
0943046d1a4535ed39bf95eff1d81bff06e5d1db	an instruction fetch unit for a graph reduction machine	decoupled risc architecture;complex instruction;high throughput;graph reduction machine;von neumann architecture;abstract architecture;hybrid instruction set;risc execution unit;architecture support;instruction pipeline;simple instruction;functional programming language;memory bandwidth;low latency	The G-machine provides architecture support for the evaluation of functional programming languages by graph reduction. This paper describes an instruction fetch unit for such an architecture that provides a high throughput of instructions, low latency and adequate elasticity in the instruction pipeline. This performance is achieved by a hybrid instruction set and a decoupled RISC architecture. The hybrid instruction set consists of complex instructions that reflect the abstract architecture and simple instructions that reflect the hardware implementation. The instruction fetch unit performs translation from complex instruction to a sequence of simple instructions which can be executed rapidly. A suitable mix of techniques, including cache, buffers and the translation scheme, provide the memory bandwidth required to feed a RISC execution unit. The simulation results identify the performance gains, maximum throughput and minimum latency achieved by various techniques. Results achieved here are in general applicable to von Neumann architectures.	cpu cache;elasticity (data store);execution unit;functional programming;graph reduction machine;instruction pipelining;maximum throughput scheduling;memory bandwidth;programming language;simulation;syntax-directed translation;von neumann architecture	Shreekant S. Thakkar;William E. Hostmann	1986		10.1145/17407.17366	instruction cycle;high-throughput screening;reduced instruction set computing;computer architecture;parallel computing;real-time computing;addressing mode;computer science;out-of-order execution;operating system;central processing unit;instruction set;instruction register;delay slot;transport triggered architecture;cycles per instruction;programming language;functional programming;control unit;memory bandwidth;instructions per cycle;low latency	Arch	-3.736379511840962	49.47448891528354	132112
258ea5db213d66fede3189d0d7ec9955a4898cd6	an emulation-based real-time power profiling unit for embedded software	emulation based real time power profiling unit;smart card;power saving;power analysis;power aware system;deep submicron smart card controller;application software;embedded software embedded system energy consumption software systems application software system analysis and design real time systems potential well discrete event simulation time to market;real time;system analysis and design;software systems;deep submicron smart card controller emulation based real time power profiling unit embedded software power consumption battery powered device energy scavenging device power budgets power aware system;energy harvesting;embedded system;embedded systems;power aware computing;energy consumption;system design;time to market;battery powered device;power consumption;power consumption embedded systems energy harvesting power aware computing;energy scavenging;energy scavenging device;power budgets;embedded software;real time systems;potential well;discrete event simulation	The power consumption of battery-powered and energy-scavenging devices has become a major design metric for embedded systems. Increasingly complex software applications as well as rising demands in operating times while having restricted power budgets make power-aware system design indispensable. In this paper we present an emulation-based power profiling approach allowing for real-time power analysis of embedded systems. Power saving potential as well as power-critical events can be identified in much less time compared to power simulations. Hence, the designer can take countermeasures already in early design stages, which enhances development efficiency and decreases time-to-market. Accuracies achieved for a deep submicron smart-card controller are greater than 90% compared to gate-level simulations.	countermeasure (computer);embedded software;embedded system;emulator;fpga prototyping;real-time clock;real-time transcription;simulation;smart card;software design;systems design;very-large-scale integration	Andreas Genser;Christian Bachmann;Josef Haid;Christian Steger;Reinhold Weiss	2009	2009 International Symposium on Systems, Architectures, Modeling, and Simulation	10.1109/ICSAMOS.2009.5289243	embedded system;real-time computing;computer hardware;computer science;operating system;energy harvesting	Embedded	2.792880320901917	55.070953160590385	132180
349611078fbba081264f2ed5f3bdb4cd77bf8720	analyzing the impact of heterogeneous blocks on fpga placement quality	quantitative approach;placement algorithm;wire length;会议论文;design quality;optimality;netlist;heterogeneous placement algorithms heterogeneous blocks field programmable gate arrays fpga synthetic heterogeneous placement benchmarks quantitative analysis wirelength optimal heterogeneous placement;a optimality;field programmable gate arrays benchmark testing algorithm design and analysis partitioning algorithms pins search methods statistical analysis;logic design chemical analysis field programmable gate arrays	In this paper we propose a quantitative approach to analyze the impact of heterogeneous blocks (H-blocks) on the FPGA placement quality. The basic idea is to construct synthetic heterogeneous placement benchmarks with known optimal wire-length to facilitate the quantitative analysis. To the best of our knowledge, this is the first work that enables the construction of wirelength-optimal heterogeneous placement examples. Besides analyzing the quality of existing placers, we further decompose the impacts of H-blocks from the architectural aspect and netlist aspect. Our analysis shows that a heterogeneous design hides the wirelength degradation by a more compact netlist than its homogeneous version; however, the heterogeneity results in a optimality gap of 52% in wirelength, where 25% is from architectural heterogeneity and 27% is from netlist heterogeneity. Therefore, new heterogeneous placement algorithms are needed to bridge the optimality gap and improve design quality.	algorithm;analysis of algorithms;benchmark (computing);elegant degradation;field-programmable gate array;gap analysis;information;netlist;synthetic data;synthetic intelligence	Chang Xu;Wentai Zhang;Guojie Luo	2014	2014 International Conference on Field-Programmable Technology (FPT)	10.1109/FPT.2014.7082750	embedded system;parallel computing;netlist;quantitative research;computer science;theoretical computer science;placement	EDA	2.8341587897998575	57.878904819593075	132235
cd6875c9adcbe94b420bb0774b1909affea52153	ultrasparc-iii: designing third-generation 64-bit performance	operating system;sun process design power generation microprocessors heart workstations mission critical systems scalability application software operating systems;berkeley risc i processor ultrasparc iii third generation 64 bit performance sun microsystems microprocessors solaris operating system;microprocessor chips	0272-1732/99/$10.00  1999 IEEE The UltraSPARC-III is the third generation of Sun Microsystems’ most powerful microprocessors, which are at the heart of Sun’s computer systems. These systems, ranging from desktop workstations to large, missioncritical servers, require the highest performance that the UltraSPARC line has to offer. The newest design permits vendors the scalability to build systems consisting of 1,000+ UltraSPARC processors. Furthermore, the design ensures compatibility with all existing SPARC applications and the Solaris operating system. The UltraSPARC-III design extends Sun’s SPARC Version 9 architecture, a 64-bit extension to the original 32-bit SPARC architecture that traces its roots to the Berkeley RISC-I processor. Table 1 (next page) lists salient microprocessor pipeline and physical attributes. The UltraSPARC-III design target is a 600-MHz, 70-watt, 19-mm die to be built in 0.25-micron CMOS with six metal layers for signals, clocks, and power.	32-bit;64-bit computing;berkeley risc;cmos;central processing unit;desktop computer;microprocessor;operating system;pipeline (computing);sparc;scalability;tracing (software);ultrasparc;workstation	Tim Horel;Gary Lauterbach	1999	IEEE Micro	10.1109/40.768506	embedded system;sun microsystems;parallel computing;real-time computing;computer science;operating system;ultrasparc	Arch	6.49106120792703	49.42453260773325	132488
18907cd0156fbb59223dc7f09208bb6f67dffac0	brief announcement: performance potential of an easy-to-program pram-on-chip prototype versus state-of-the-art processor	pram;parallel algorithm;performance of systems;chip;paraleap;xmt;ease of programming;explicit multi treading;on chip parallel processor;parallel algorithms	We compare the Paraleap FPGA computer, a 64-processor hardware prototype of the PRAM-driven XMT architecture, with an Intel Core 2 Duo processor and show that Paraleap outperforms the Intel processor by up to 13.89x in terms of cycle counts. The comparison favors the Intel design, since the silicon area of an ASIC implementation of the 64-processor XMT design is the same as that of a single core.	application-specific integrated circuit;cray xmt;field-programmable gate array;prototype	George C. Caragea;A. Beliz Saybasili;Xingzhi Wen;Uzi Vishkin	2009		10.1145/1583991.1584038	computer architecture;parallel computing;single-chip cloud computer;computer hardware;computer science;operating system;parallel algorithm	Arch	0.8023565249428328	47.90795643679397	132588
59e1a2eece344d31f129fa65496c3aac74c88189	near real-time optimization of activity-based notifications		In recent years, social media applications (e.g., Facebook, LinkedIn) have created mobile applications (apps) to give their members instant and real-time access from anywhere. To keep members informed and drive timely engagement, these mobile apps send event notifications. However, sending notifications for every possible event would result in too many notifications which would in turn annoy members and create a poor member experience.  In this paper, we present our strategy of optimizing notifications to balance various utilities (e.g., engagement, send volume) by formulating the problem using constrained optimization. To guarantee freshness of notifications, we implement the solution in a stream computing system in which we make multi-channel send decisions in near real-time. Through online A/B test results, we show the effectiveness of our proposed approach on tens of millions of members.	a/b testing;approximation algorithm;constrained optimization;downstream (software development);fan-out;mathematical optimization;mobile app;nearline storage;optimization problem;real-time clock;real-time computing;real-time transcription;relevance;replay attack;social media;stream processing;turing test	Yan Gao;Viral Gupta;Jinyun Yan;Changji Shi;Zhongen Tao;P. J. Xiao;Curtis Wang;Shipeng Yu;Rómer Rosales;Ajith Muralidharan;Shaunak Chatterjee	2018		10.1145/3219819.3219880	computer science;real-time computing;constrained optimization;social media;stream	HCI	-2.860521370895819	60.299586431730106	132613
cf7cda468aaa4c6d4bb964f22bec932289ae7e94	quantitative evaluation of register vulnerabilities in rtl control paths	registers model checking probabilistic logic scalability benchmark testing error analysis europe;selective register protection quantitative evaluation register vulnerabilities rtl control paths radiation induced soft error nanoscale technology nodes probabilistic model checking soft error vulnerabilities register transfer level efficient abstraction techniques model simplification techniques rtl design;radiation hardening electronics flip flops	Radiation-induced soft error is a significant reliability issue in nanoscale technology nodes. In this paper, a novel approach based on probabilistic model checking is proposed to quantify the soft error vulnerabilities of the registers in the control paths at the Register-Transfer Level (RTL). Efficient abstraction and model simplification techniques are proposed to significantly improve the scalability of our method. The experimental results show the effectiveness of proposed techniques to successfully quantify the register vulnerabilities in the RTL design, to be used for cost-effective selective register protection.	level of detail;model checking;register-transfer level;scalability;soft error;statistical model	Liang Chen;Mojtaba Ebrahimi;Mehdi Baradaran Tahoori	2014	2014 19th IEEE European Test Symposium (ETS)	10.1109/ETS.2014.6847837	parallel computing;real-time computing;computer science;theoretical computer science	Embedded	8.474892815289632	59.75941862553084	132693
57394b6e9399a428b4d8cef9bc6235d7cffa8c5c	a new approach for automatic development of reconfigurable real-time systems		In the industry, reconfigurable real-time systems are specified as a set of implementations and tasks with timing constraints. The reconfiguration allows to move from one implementation to another by adding/removing real-time tasks. Implementing those systems as threads generates a complex system code due to the large number of threads and the redundancy between the implementation sets. This paper shows an approach for software synthesis in reconfigurable uniprocessor realtime embedded systems. Starting from the specification to a program source code, this approach aims at minimizing the number of threads and the redundancy between the implementation sets while preserving the system feasibility. The proposed approach adopts Mixed Integer Linear Programming (MILP) techniques in the exploration phase in order to provide feasible and optimal task model. An optimal reconfigurable POSIX-based code of the system is manually generated as an output of this technique. An application to a case study and performance evaluation show the effectiveness of the proposed approach.	complex system;embedded system;integer programming;linear programming;mathematical optimization;multiprocessing;numerical analysis;posix;performance evaluation;programming model;real-time clock;real-time computing;real-time operating system;real-time transcription;reconfigurable computing;scalability;uniprocessor system	Wafa Lakhdhar;Rania Mzid;Mohamed Khalgui;Nicolas Trèves	2016		10.1007/978-3-319-62569-0_2	complex system;control reconfiguration;redundancy (engineering);real-time computing;thread (computing);real-time operating system;source code;integer programming;uniprocessor system;computer science	Embedded	0.5793280018344702	52.04801133996102	132761
60e62f1540d3d6970ad4409b4d7ada61252d7f96	fpga implementation of serial peripheral interface of flexray controller	field programmable gate array;xilinx software fpga implementation serial peripheral interface flexray controller high speed protocol safety critical features;protocols;peripheral interfaces;clocks;serial peripheral interface;data exchange;safety critical features;fpga implementation;protocols clocks field programmable gate arrays shift registers payloads;safety critical software;shift registers;flexray controller;communication protocol;payloads;xilinx software;field programmable gate arrays;safety critical software field programmable gate arrays peripheral interfaces protocols;high speed;high speed protocol	FlexRay is a general purpose high-speed protocol with safety-critical features .Flexray is a new communication protocol designed to provide message and data exchange between electronic devices installed in a vehicle. This paper gives information about Serial Peripheral Interface and Configuration registers of the bus guardian module of a flexray controller .Authors have done FPGA synthesis of Serial Peripheral Interface and Configuration registers using Xilinx software and shown output results on FPGA X3S400 KIT Board. Keywords-FPGA, Flexray, SPI, BG	communications protocol;field-programmable gate array;flexray;job control (unix);serial peripheral interface bus	Awani N. Gaidhane;Manish Pankaj Khorgade	2011	2011 UkSim 13th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2011.33	embedded system;real-time computing;computer hardware;engineering	Robotics	6.795157374391643	49.067173103228946	132836
85e1d6a1eb10bbe8cef99687f1ba9a62e45b17fc	data-path synthesis of vliw video signal processors	data-path synthesis;vliw video signal processor;testing;vliw;embedded systems;signal generators;digital signal processing;reduced instruction set computing;very long instruction word;high level synthesis;signal processing;algorithm design and analysis;embedded system	This paper describes a methodology for synthesizing the data-path of a Very Long Instruction Word (VLIW) based Video Signal Processor (VSP). Offering both performance and programmability, VSPs are important for their roles in digital video applications, which are omnipresent in today’s world. Among many different architectures, VLIW is becoming increasingly popular and widely used due to its efficiency in exploiting high degree of parallelism inherent in multimedia applications. While architectural syntheses of embedded systems have been studied in depth, little literature has addressed similar issues for VLIW-based VSPs. Using an MPEG-2 video encoder as a case study, in this paper we present a combined application of trace-driven simulation and performance estimation in the data-path synthesis of a VLIW VSP. Results show that our estimations are quite precise and helpful, let alone that they are orders of magnitude faster than simulation.	degree of parallelism;digital video;embedded system;encoder;h.262/mpeg-2 part 2;mpeg-2;parallel computing;simulation;very long instruction word;virtual storage platform	Zhao Wu;Wayne H. Wolf	1998			computer architecture;parallel computing;real-time computing;computer science	EDA	2.7358945288632017	53.13455290219875	133076
e4f001867f4093bf2db2d0751cfaf82c9a7ecb4c	a dependability solution for homogeneous mpsocs	self repair;reliability;circuit faults;self test mpsoc dependability fault tolerance self repair reliability availability embedded instruments noc tam;availability;tile wrappers homogeneous mpsoc electronic devices safety critical applications many processor system on chip fault free processor cores dependability manager infrastructural ip self test software based repair test fault coverage repair time test time dependability attributes network on chip;automatic testing;maintenance engineering;built in self test availability circuit faults system on a chip vectors maintenance engineering;system on a chip;integrated circuit design;built in self test;vectors;system on chip;system on chip automatic testing integrated circuit design integrated circuit reliability microprocessor chips;fault tolerance;dependability;embedded instruments;integrated circuit reliability;self test;noc tam;mpsoc;microprocessor chips	Nowadays highly dependable electronic devices are demanded by many safety-critical applications. Dependability attributes such as reliability and availability/maintainability of a many-processor system-on-chip (MPSoC) should already be examined at the design phase. Design for dependability approaches such as using available fault-free processor-cores and introducing a dependability manager infrastructural IP for self-test and evaluation can greatly enhance the dependability of an MPSoC. This is further supported by subsequent software-based repair. Design choices such as test fault coverage, test and repair time are examined to optimize the dependability attributes. Utilizing existing infrastructures like a network-on-chip (NoC) and tile-wrappers are needed to ensure a test can be performed at application run-time. An example design following the proposed design for dependability approach is shown. The MPSoC has been processed and measurement results have validated the proposed dependability approach.	algorithm;cmos;dependability;fault coverage;fault model;mpsoc;network on a chip;system on a chip	Xiao Zhang;Hans G. Kerkhoff	2011	2011 IEEE 17th Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2011.16	maintenance engineering;system on a chip;reliability engineering;embedded system;real-time computing;computer science;engineering;dependability	EDA	7.331355350072966	58.672762421772894	133668
8c795b239e78f56d78df31805323654320934d12	local microcode compaction techniques	generic model;prior knowledge;data dependence;critical path;locally compact;list scheduling;branch and bound;high level language	Microcode compaction is an essential tool for the compilation of high-level language microprograms into microinstructions with parallel microoperations. Although guaranteeing minimum execution time is an exponentially complex problem, recent research indicates that it is not difficult to obtain practical results. This paper, which assumes no prior knowledge of microprogramming on the part of the reader, surveys the approaches that have been developed for compacting microcode. A comprehensive terminology for the area is presented, as well as a general model of processor behavior suitable for comparing the algorithms. Execution examples and a discussion of strengths and weaknesses are given for each of the four classes of loc .al compaction algorithms: linear, critical path, branch and bound, and list scheduling. Local compaction, which applies to jump-free code, is fundamental to any compaction technique. The presentation emphasizes the conceptual distinction between data dependency and conflict analysis.	algorithm;boolean algebra;branch and bound;compiler;critical path method;data compaction;data dependency;data integrity;dependent type;glossary;heuristic (computer science);high- and low-level;high-level programming language;hypervisor;itil;list scheduling;management information system;micro-operation;microcode;moment of inspiration;n-gram;operand;original order;polyphase matrix;precondition;run time (program lifecycle phase);scheduling (computing);secure digital;spatial light modulator;undefined behavior	David Landskov;Scott Davidson;Bruce D. Shriver;Patrick W. Mallett	1980	ACM Comput. Surv.	10.1145/356819.356822	locally compact space;parallel computing;computer science;theoretical computer science;operating system;critical path method;database;programming language;high-level programming language;branch and bound;algorithm	Arch	-0.9703002230820627	48.3312146833119	133683
b792aa2054f46062635f08916e3383a42cf82f77	hydrologic modeling methodology	frequency modulation;chip design hydrologic modeling methodology complex hydrological model real world behavior electronic design automation eda methodologies;computational modeling;monitoring;mathematical model;electronic design automation eda methodologies hydrological modeling systemc ams;ports computers;atmospheric modeling;mathematical model equations atmospheric modeling computational modeling ports computers frequency modulation monitoring;electronic design automation	Complex hydrological models are employed to mimic real world behavior and, if integrated with other hydrological complex models from different domains, may lead to a new powerful hydrological model that will provide answers to ever more sophisticated queries. However, integration will be a slow process since each hydrological model may be self-contained, with different timescales and simulation speeds. Electronic Design Automation (EDA) methodologies have evolved for chip design for precisely such situations, but in a different domain. Integration of hydrological models can benefit with such EDA techniques; there, however, is also an added advantage. A complete detailed model can take days to simulate and yield useful information to the end user. However, trading off precision in some sub models with overall system response time may be acceptable, thus returning useful information much sooner. We will present methodology, model, and simulation results for a hydrologic model that is based on concepts, languages, and tools used in EDA. This results in multiple models that can trade-off precision with response time. We hope this helps open many new lines of inquiries and potential practical uses.	electronic design automation;response time (technology);simulation	Frank Wissinger;Ravi Shankar;Jorge Restrepo	2014	2014 IEEE International Systems Conference Proceedings	10.1109/SysCon.2014.6819299	simulation;computer science;theoretical computer science	EDA	2.106892885480625	56.24897442971844	133914
d40fdcd23e0a9cce17c7f856fda043e7e0f7daf9	amleto: a multi-language environment for functional test generation	embedded systems automatic test software automatic test pattern generation hardware description languages programming environments fault simulation;programming environments;fault simulation;functional testing;modeling and simulation;automatic test pattern generation;hardware description languages;system testing hardware design languages computational modeling software testing error correction power system modeling embedded system history delay electronic design automation and methodology;embedded system;embedded systems;automatic test software;iir amleto multi language environment functional test generation systemc description language embedded systems ip cores hdl language independent representation customized tpgs design under test design error simulation vhdl designs	More and more people are starting to use the Sysnew designs. This is due mainly to the simplicity and power of the language. els written in SystemC currently available is still very limited and testing SystemC descriptions is still scriptions [7, 6, 9, an open issue, since the language is new and researchers are looking for efJici ent error models and multi-language environment developed to e&i ently test embedded systems and IP-Cores. Using IIR, a HDL language independent representation, it supplies: fast translation fiom VHDL to SystemC of design descriptions and viceversa, generation and and generation of erroneous models capable to simulate the presence of design errors. nario of testing SystemC designs has less examples [3]. This is due to the short history of the language and the latency of many EDA researchers to size of the library of available components developed The number Of modin SystemC is small and the attention of the testing community is still focused on Verilog and VHDL deThe reputation of systemc is increasing faster and faster as the number of major companies, which started to use it to design their port the SystemC-based design is growing quickly. Cadence, Synopsys, CoWare, etc. are developing the most effective tools to support SystemC designers. On top of that, the rapid diffusion of embedded systems and the SystemC aptitude to efficient ly design as an effective alternative in the world of HDL languages. The aim of this paper is to present a testing envil Introduction ronment for both VHDL [16] and SystemC [17] designs. It is composed of some significant extensions Design verificatio n is one of the most complex task of Savant, an original tool developed at the Univerof the entire design flo w due to the continuous insity Of Cincinnati [2]. The savant fUnCtiOnditieS creasing of the design complexity. This aspect has alhave been extended and completed obtaining AMready influen ced the work flo w of VHDL and verilog LETO, the testing-oriented environment presented in designers and it is starting to affect also the growing this Paper. AMLET0 key feat~res are: community of SystemC designers. due to their computation complexity, can be applied with success just to medium-small projects, thus the common practice for functional verificat ion is still based on simulation. Many effective simulation approaches have been presented in literature [12,4, 111 for Verilog and VHDL designs. Whereas the scedescription language to and adopt a new design language. Moreover, the 'Overage metricS. This paper presents AMLET'> a new products. m e availability of new tools to supsetup O f customized TPGs for the design under test hardware/software systems are proposing Syste.& The internal elaboration procedures are indeverifica tion techniques i l , 8? 13i9 pendent from the input HDL language adopted. In fact, all of them are applied to an intemediate representation (IIR) of the design, see Section 2. The suggested output representation, for testing purpose, is a SystemC design, since its compiITC INTERNATIONAL TEST CONFERENCE 0-7803-71 69-0/01 $1 0.00	computation;electronic design automation;embedded system;functional testing;hardware description language;infinite impulse response;simulation;software system;systemc;vhdl;verilog;aptitude	Alessandro Fin;Franco Fummi;Graziano Pravadelli	2001		10.1109/TEST.2001.966704	embedded system;computer architecture;real-time computing;computer science;automatic test pattern generation;functional testing;modeling and simulation;hardware description language;programming language	EDA	6.4465153276953995	52.5707513081853	133922
7b2a5fc3841815bc008d257020ef905c9d5759b8	radio frequency identification prototyping	design automation;spectrum analysis;real time;rfid tag;chip;prototyping;center of excellence;low power;rfid;radio frequency identification	While RFID is starting to become a ubiquitious technology, the variation between different RFID systems still remains high. This paper presents several prototyping environments for different components of radio frequency identification (RFID) tags to demonstrate how many of these components can be standardized for many different purposes. We include two active tag prototypes, one based on a microprocessor and the second based on custom hardware. To program these devices we present a design automation flow that allows RFID transactions to be described in terms of primitives with behavior written in ANSI C code. To save power with active RFID devices we describe a passive transceiver switch called the “burst switch” and demonstrate how this can be used in a system with a microprocessor or custom hardware controller. Finally, we present a full RFID system prototyping environment based on real-time spectrum analysis technology currently deployed at the University of Pittsburgh RFID Center of Excellence. Using our prototyping techniques we show how transactions from multiple standards can be combined and targeted to several microprocessors include the Microchip PIC, Intel StrongARM and XScale, and AD Chips EISC as well as several hardware targets including the Altera Apex, Actel Fusion, Xilinx Coolrunner II, Spartan 3 and Virtex 2, and cell-based ASICs.	ansi c;application-specific integrated circuit;microprocessor;network switch;pic microcontroller;prototype;radio frequency;radio-frequency identification;real-time clock;spartan;strongarm;transceiver;virtex (fpga);xscale	Alex K. Jones;Swapna R. Dontharaju;Shen Chih Tung;Leonid Mats;Peter J. Hawrylak;Raymond R. Hoare;James T. Cain;Marlin H. Mickle	2008	ACM Trans. Design Autom. Electr. Syst.	10.1145/1344418.1344425	radio-frequency identification;embedded system;parallel computing;real-time computing;electronic design automation;telecommunications;computer science;operating system	EDA	6.829334113896687	49.19550073499139	134048
29fafd15d07ee8f945640a1a258cb6a2d926e279	force-directed scheduling for the behavioral synthesis of asics	minimisation;optimisation sous contrainte;constrained optimization;concepcion asistida;minimization;computer aided design;multicycle operations;resource constraint;heart;metodologia;behavioral synthesis;functional unit costs;cost function;integrated circuit;job shop scheduling;processor scheduling;cad;chained operations;circuito integrado;methodologie;register costs;local time constraints;circuit a la demande;algorithme;optimizacion con restriccion;application specific integrated circuits job shop scheduling scheduling algorithm pipeline processing processor scheduling hardware control system synthesis heart cost function high level synthesis;algorithm;high level synthesis;minimizacion costo;scheduling algorithm;global interconnect requirements;minimisation cout;cost minimization;digital integrated circuits;integrated circuit technology;application specific integrated circuits;control system synthesis;scheduling;functional pipelining;structural pipeline multicycle operations cad computer aided design asic digital ic behavioral synthesis general scheduling methodology high level synthesis systems force directed scheduling algorithm chained operations mutually exclusive operations fixed global timing constraints minimization functional unit costs register costs global interconnect requirements local time constraints fixed hardware resource constraints functional pipelining;structural pipeline;conception assistee;mutually exclusive operations;circuit cad;asic;functional unit;methodology;digital ic;high level synthesis systems;fixed hardware resource constraints;circuit integre;pipeline processing;general scheduling methodology;scheduling application specific integrated circuits circuit cad digital integrated circuits integrated circuit technology minimisation pipeline processing;hardware;algoritmo;force directed scheduling algorithm;local time;fixed global timing constraints;time constraint	The HAL system described performs behavior synthesis using a global scheduling and allocation scheme that proceeds by step-wise refinement. The force-directed scheduling algorithm at the heart of this scheme reduces the number of functional units, storage units, and buses required by balancing the concurrency of operations assigned to them. The algorithm supports a comprehensive set of constraint types and scheduling modes. These include: multicycle and chained operations; mutually exclusive operations; scheduling under j x e d global timing constraints with: minimization of functional unit costs, minimization of register costs, minimization of global interconnect requirements: scheduling with local time constraints (on operation pairs): scheduling under fixed hardware resource constraints; functional pipelining; structural pipelining (use of pipelined functional units). Examples from current literature, one of which was chosen as a benchmark for the 1988 High-Level Synthesis Workshop, are used to illustrate the effectiveness of the approach.	algorithm;application-specific integrated circuit;benchmark (computing);concurrency (computer science);execution unit;force-directed graph drawing;functional derivative;hal;high-level synthesis;pipeline (computing);refinement (computing);requirement;scheduling (computing)	Pierre G. Paulin;John R P Knight	1989	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.31522	fair-share scheduling;nurse scheduling problem;fixed-priority pre-emptive scheduling;embedded system;job shop scheduling;mathematical optimization;constrained optimization;electronic engineering;parallel computing;real-time computing;earliest deadline first scheduling;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;genetic algorithm scheduling;operating system;computer aided design;two-level scheduling;deadline-monotonic scheduling;scheduling;application-specific integrated circuit;lottery scheduling;round-robin scheduling;scheduling	EDA	0.9655035424957412	53.06914441482849	134168
82e610e1a2bfb2c789c635d48335ad5e8e024227	refinement of mixed-signal systems with systemc	mixed-signal systems;concrete mixed-signal architecture;design methodology specific extension;design methodology;asc library;executable specification;process control;design;concrete;network flow;computational modeling;floorplanning;prototypes;simulated annealing;algorithms;c;management;signal processing;process design;hierarchical	This paper gives an overview of a design methodology specific extension library for SystemC. The supported design methodology successively refines an executable specification to a concrete mixed-signal architecture. A first prototype, the ASC library, has been implemented and evaluated.	executable;mixed-signal integrated circuit;prototype;systemc;ti advanced scientific computer	Christoph Grimm;Christian Meise;Wilhelm Heupke;Klaus Waldschmidt	2003			process design;design;computer architecture;parallel computing;real-time computing;concrete;simulated annealing;computer science;signal processing;process control	EDA	4.953899230677127	52.32857483596545	134180
6f6708636b35ab6e7b27430c61322c4fc078e23e	dynamic reconfiguration of cache indexing in embedded processors	microprocessor;evaluation performance;optimisation;gestion memoire;degradation;tecnologia electronica telecomunicaciones;methode section divisee;haute performance;calculateur embarque;compilateur;algorithm performance;performance evaluation;optimizacion;integrated circuit;embedded processor microprocessor architecture;dynamic reconfiguration;reconfigurable architectures;storage management;evaluacion prestacion;degradacion;microprocessor architecture;cache memory;circuito integrado;ejecucion programa;endommagement;compiler;deterioracion;cache indexing;program execution;circuit a la demande;cache organization;antememoria;algorithme;algorithm;gestion memoria;antememoire;custom circuit;circuito integrato personalizado;indexing;resultado algoritmo;execution programme;indexation;boarded computer;indizacion;performance algorithme;alto rendimiento;optimization;procesador;microprocesseur;tecnologias;damaging;processeur;grupo a;high performance;embedded processor;microprocesador;architecture reconfigurable;calculador embarque;processor;circuit integre;compilador;algoritmo;multistage method	Cache performance optimization is an important design consideration in building high-performance embedded processors. Unlike general-purpose microprocessors, embedded processors can take advantages of application-specific information in optimizing the cache performance. One of such examples is to use modified cache index bits (over conventional index bits) based on memory access traces from key target embedded applications so that the number of conflict misses can be reduced. In this paper, we present a novel fine-grained cache reconfiguration technique which allows an intra-program reconfiguration of cache index bits, thus better reflecting the changing characteristics of a program execution. The proposed technique, called dynamic reconfiguration of index bits (DRIB), dynamically changes cache index bits in the function level. This compiler-directed and fine-grained approach allows each function to be executed using its own optimal index bits with no additional hardware support. In order to avoid potential performance degradation by frequent cache invalidations from reconfiguring cache index bits, we describe an efficient algorithm for selecting target functions whose cache index bits are reconfigured. Our algorithm ensures that the number of cache misses reduced by DRIB outnumbers the number of cache misses increased from cache invalidations. We also propose a new cache architecture, Two-Level Indexing (TLI) cache, which further reduces the number of conflict misses by intelligently dividing indexing steps into two stages. Our experimental results show that the DRIP approach combined with the TLI cache reduces the number of cache misses by 35% over the conventional cache indexing technique.		Junhee Kim;Sung-Soo Lim;Jihong Kim	2007	IEICE Transactions	10.1093/ietisy/e90-d.3.637	bus sniffing;embedded system;least frequently used;pipeline burst cache;search engine indexing;compiler;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;degradation;cpu cache;cache;computer science;write-once;cache invalidation;operating system;integrated circuit;adaptive replacement cache;smart cache;mesi protocol;cache algorithms;cache pollution;algorithm;mesif protocol	EDA	-2.6278817061863338	53.4612729816469	134212
79bc76ecb38d070ba3200385511ceeebbcd40583	new dsp benchmark based on selectable mode vocoder (smv)	benchmark;optimizing compiler;vocoder;instruction level parallelism.;digital signal processor;dsp;kernel function;digital signal processing;speech coding;static analysis;selectable mode vocoder	Digital signal processing (DSP) industry has been growing rapidly over the past few years; it remains the technology driver for the recovering semiconductor industry. Performance evaluation is essential for the users and manufacturers of DSP processors. Since DSP application programs become larger and more complicated, people need new benchmarks for performance evaluation of different DSP processors. We build a new DSP benchmark based on Selectable Mode Vocoder (SMV), a speech-coding program from the 3G wireless applications. Our new DSP benchmark, called SMV benchmark, consists of eight kernel functions. In this paper, we introduce the criteria of selecting kernels and our methodology to build SMV benchmark. We also discuss the characteristics and static analysis of the kernels.	benchmark (computing);central processing unit;digital signal processing;performance evaluation;selectable mode vocoder;semiconductor industry;speech coding;static program analysis	Erh-Wen Hu;Cyril Ku;Andrew Russo;Bogong Su;Jian Wang	2006			parallel computing;digital signal processor;kernel (statistics);digital signal processing;wireless;real-time computing;selectable mode vocoder;speech coding;optimizing compiler;computer science	EDA	1.5468009190852259	46.644987433339146	134257
a8e556e4a99de38bca418023d47f8dc009ce103b	design and evaluation of a low power cgra accelerator for biomedical signal processing		This work presents the design and analysis of a biological signal processing accelerator, including an interface controller and memory subsystem for a low-power CGRA. The controller design supports several operation modes, which can perform several applications when paired with the CGRA reconfiguration capabilities. Physical synthesis shows that the controller introduces only a 6 percent area and power overhead compared to the CGRA core, while allowing independent processing of inner loops at high frequencies and the exploitation of pipelining and parallelism. In-depth power analysis based on layout information was performed, including an evaluation of the use of power gating techniques. A practical case study (ECG signal processing) was also evaluated.	cmos;low-power broadcasting;overhead (computing);parallel computing;pipeline (computing);power gating;reconfigurable computing;sampling (signal processing);signal processing;token reconfiguration;wearable computer	Helder H. Avelar;João Canas Ferreira	2018	2018 21st Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2018.00087	power analysis;real-time computing;parallel computing;control theory;process control;memory management;control reconfiguration;signal processing;power gating;pipeline (computing);computer science	EDA	1.8535360719997804	51.49252257390601	134371
9556a1d0472ecb55c5ca56fced1f016e00eb0cb9	reconfigurable processor architectures: varieties and representations	reconfigurable architectures;reconfigurable architectures application specific integrated circuits microprocessor chips program compilers;application specific integrated circuits;ontology based representation reconfigurable processor architectures application specific integrated circuits asic digital processors software tools compilers;computer architecture owl radio frequency ontologies topology integrated circuit interconnections xml;program compilers;microprocessor chips	Reconfigurable processors combine the speed of application specific integrated circuits (ASIC) and the universality of classical digital processors by means of adaptability to the currently executed code. There is a great number of varieties in today's reconfigurable processor architectures. Processor architectures can be specified in many ways. Architecture specifications are usually used during the architecture design phase and in software tools such as compilers and simulators. The aim of this paper is to propose various comparison criteria for reconfigurable processor architectures and to give an overview of the specification methods for them. The main contribution of this paper is a proposal of the new ontology-based representation of reconfigurable processor architectures.	application-specific integrated circuit;central processing unit;compiler;microprocessor;reconfigurable computing;simulation;universal turing machine	Damir Kirasic;Danko Basch	2012	2012 Proceedings of the 35th International Convention MIPRO		computer architecture;parallel computing;computer science;operating system;application-specific integrated circuit	EDA	3.6140559948925874	49.69209079594173	134457
aaa9998a755187d1000dcd53884ffc26f25da4bd	a systolic regular expression pattern matching engine and its application to network intrusion detection	pattern matching engines intrusion detection circuits field programmable gate arrays application software telecommunication traffic hardware design languages computer architecture computer applications;clocks;network intrusion detection;computer architecture;engines;pattern matching;telecommunication security;latches;field programmable gate arrays;string matching;network intrusion detection system;high speed;security of data;regular expression;telecommunication security field programmable gate arrays security of data string matching;fpga implementation pattern matching engine network intrusion detection string matching circuit systolic architecture;hardware	This paper proposes a high-speed string matching circuit for searching a pattern in a given text. In the circuit, a pattern is specified by a class of restricted regular expressions. The architecture of the proposed circuit is a one-dimensional systolic architecture consisting of simple processing units. It can be effectively used for network intrusion detection systems (NIDSs).	intrusion detection system;pattern matching;regular expression;string searching algorithm	Yosuke Kawanaka;Shin'ichi Wakabayashi;Shinobu Nagayama	2008	2008 International Conference on Field-Programmable Technology	10.1109/FPT.2008.4762402	embedded system;real-time computing;computer science;theoretical computer science;operating system;pattern matching;regular expression;field-programmable gate array;string searching algorithm	EDA	8.997081567758425	46.92957307771118	134858
0fdcb50b4e20bde1eef822ff8ef947a9275a6918	improving code density using compression techniques	computer architecture;decoding;instruction sets;source coding;arm instruction set;powerpc instruction set;spec cint95 programs;code density;common instruction sequences;compressed instruction sequences;cost;decoding stage;embedded processors;execution stages;i386 instruction set;instruction codeword fetching;instruction memory size;microprocessor;post-compilation analyser;program compression techniques;size reduction	We propose a method for compressing programs in embedded processors where instruction memory size dominates cost. A post-compilation analyzer examines a program and replaces common sequences of instructions with a single instruction codeword. A microprocessor executes the compressed instruction sequences by fetching code words from the instruction memory, expanding them back to the original sequence of instructions in the decode stage, and issuing them to the execution stages. We apply our technique to the PowerPC, ARM, and i386 instruction sets and achieve an average size reduction of 39%, 34%, and 26%, respectively, for SPEC CINT95 programs.	arm architecture;central processing unit;code word;compiler;embedded system;microprocessor;powerpc	Charles Lefurgy;Peter L. Bird;I-Cheng K. Chen;Trevor N. Mudge	1997			program counter;instruction cycle;one instruction set computer;self-modifying code;reduced instruction set computing;computer architecture;parallel computing;real-time computing;addressing mode;computer science;out-of-order execution;minimal instruction set computer;central processing unit;instruction set;instruction register;cycles per instruction;instruction scheduling;compression;instruction path length;control unit;orthogonal instruction set;source code	Arch	-2.2285971412556935	52.133642725029716	134860
b38cbf9841ab93c05350276ea8289d720d6a2754	genetic algorithm-aided fixed-point design of e-utra prach detector on multi-core dsp	detectors;digital signal processing;digital signal processing detectors genetic algorithms bandwidth vectors algorithm design and analysis hardware;fixed point;vectors;digital signal processor;bandwidth;genetic algorithm;genetic algorithms;e utra prach detector design problem genetic algorithm aided fixed point design multicore dsp ga aided methodology software implementation digital signal processors computational accuracy bus bandwidth constraints fixed point design problems constrained integer programming cip problem fixed point evolved e utra prach detector;integer program;algorithm design and analysis;multiprocessing systems constraint theory digital signal processing chips genetic algorithms integer programming integrated circuit design;software implementation;exhaustive search;hardware	This paper presents a new genetic algorithm (GA)-aided methodology of software implementation in Digital Signal Processors (DSPs) under both the computational accuracy and bus bandwidth constraints. The design issue is firstly stated as two classes of fixed-point design problems, one of which is then formulated to a constrained integer programming (CIP) problem. And the genetic algorithm is proposed to treat with such CIP problem for the sake of efficiency. Then the fixed-point evolved (E)-UTRA PRACH detector is presented, which further underlines the feasibility and convenience of applying this methodology to practice. Finally, the numeric results justify the proposed GA-aided approach and demonstrate that a speedup by a factor of 33 can be achieved compared to the exhaustive search for the solution of E-UTRA PRACH detector design problem.	brute-force search;digital signal processor;fixed point (mathematics);fixed-point arithmetic;genetic algorithm;integer programming;internet access;matlab;mathematical optimization;multi-core processor;quantization (signal processing);software release life cycle;speedup	Rongrong Qian;Tao Peng;Yuan Qi;Wenbo Wang	2009	2009 17th European Signal Processing Conference	10.5281/zenodo.41297	electronic engineering;real-time computing;computer science;theoretical computer science	EDA	9.674222596715618	47.59682924439097	134906
46f615173a141936a6c76ab174e23f4e70cbe4a2	register-constrained address computation in dsp programs	optimising compilers;signal processing;storage allocation;dsp algorithms;dsp programs;code optimization technique;dedicated address generation units;digital signal processors;heuristic technique;iterative accesses;program loop;register-constrained address computation	This paper describes a new code optimization technique for digital signal processors (DSPs). One important characteristic of DSP algorithms are iterative accesses to data array elements within loops. DSPs support efficient address computations for such array accesses by means of dedicated address generation units (AGUs). We present a heuristic technique which, given an AGU with a fixed number of address registers, minimizes the number of instructions needed for array address computations in a program loop.	address generation unit;algorithm;central processing unit;computation;control flow;digital signal processor;heuristic;iteration;mathematical optimization;program optimization	Anupam Basu;Rainer Leupers;Peter Marwedel	1998			embedded system;digital signal processor;computer architecture;parallel computing;computer science;theoretical computer science;operating system;signal processing	EDA	0.25162906801256985	51.5813129227658	134972
b4492bb6443723600645bccdbaa00ad10b285d7c	partially reconfigurable vector processor for embedded applications	field programmable gate array;performance evaluation;partial reconfiguration;embedded system;performance improvement;low power;fpga reconfiguration;embedded applications;industrial control;power consumption;vector processor;vector processing	Embedded systems normally involve a combination of hardware and software resources designed to perform dedicated tasks. Such systems have widely crept into industrial control, automotive, networking, and consumer products. These systems require efficient devices that occupy small area and consume low power. The device area can be minimized by reusing the same hardware for different applications. If possible, reconfiguring the hardware to adapt to the application needs is important for reducing execution time and/or power consumption. Partial reconfiguration facilitates minimum hardware changes to form a new configuration. We have designed a reconfigurable vector processor for embedded applications. Benchmark results on Xilinx FPGAs (Field-Programmable Gate Arrays) are presented involving partial reconfiguration for embedded applications that process vectors. Two approaches are studied toward performance evaluation. The first one estimates the required partial reconfiguration time based on the resources consumed by the corresponding vector kernels. The second approach uses the actual measurement of partial reconfiguration time on a platform that supports a particular type of partial reconfiguration. More than 20% performance improvement has been observed for benchmark kernels, without neglecting the reconfiguration overhead. A framework is proposed as well to efficiently manage the reconfiguration overhead for applications involving multiple kernels.	benchmark (computing);embedded system;field-programmable gate array;overhead (computing);performance evaluation;run time (program lifecycle phase);vector processor	Muhammad Z. Hasan;Sotirios G. Ziavras	2007	JCP	10.4304/jcp.2.9.60-66	embedded system;vector processor;parallel computing;real-time computing;computer science;operating system	EDA	-0.6911970491316435	53.52616530949946	134978
43f91afc0186d535b9813abee8402768e5328b10	is it possible to achieve a teraflop/s on a chip? from high performance algorithms to architectures	high density;parallel architectures microprocessor chips parallel algorithms;chip;parallel architectures;floating point;single chip processor teraflop operations high density computations floating point operations;signal processing algorithms computer architecture application software microelectronics circuits content addressable storage design methodology digital signal processing chips societies biographies;microprocessor chips;parallel algorithms	The forumnists address the question of high density computations on a single chip. The surface of a chip offers an ideal medium not only to store information or to process data, but also to execute computations. The 1 Giga floating point operations per second per chip mark has been achieved, we are now moving towards the teraflop mark. How is this going to happen, what are the limitations, what are the opportunities-those are central questions. >	algorithm;flops	Francky Catthoor;Ed F. Deprettere;Yu Hen Hu;Jan M. Rabaey;Heinrich Meyr;Lothar Thiele	1994		10.1109/ISCAS.1994.408922	chip;embedded system;computer architecture;parallel computing;computer hardware;telecommunications;computer science;floating point;operating system;parallel algorithm	HPC	7.702106302605182	50.15717270318862	135364
732d702d9a856bdd497bd8c7f5899dbb65480b13	the evolution of architecture exploration of programmable devices	experimental tests;programmable logic devices;programmable logic devices field programmable gate arrays hardware description languages logic cad microprocessor chips program compilers;integrated circuit;hdl level integrated circuit fabrication process transistors ic architecture exploration programmable logic hardware fpga programmable instruction set processors cad compiler flow;compiler flow;cad;field programmable gate arrays strontium routing transistors hardware design automation fabrication integrated circuit testing logic devices programmable logic arrays;hardware description languages;fpga;programmable instruction set processors;hdl level;transistors;integrated circuit fabrication process;field programmable gate arrays;program compilers;programmable logic;ic architecture exploration;logic cad;programmable logic hardware;microprocessor chips	"""As integrated circuit fabrication processes continue to provide exponential increases in density of transistors with each generation, the question of what to do with those transistors becomes ever more interesting. The most fundamental part of that question is the global organization of the structures created from the transistors, most commonly referred to as the *architecture* of the device. Most IC architecture exploration that is done is quite empirical, with example uses driving through tools to experimentally test new ideas for structures and organizations. This method is used in both programmable logic hardware such as FPGAs, and in programmable instruction set processors. As the processor world now seeks to gain performance through parallelism, its architecture questions have begun to look more similar to those in the FPGA domain. In this talk I discuss the evolution of the architecture exploration processes that we have worked on at the University of Toronto, and of the new levels that we are currently trying to build. The current effort focusses on HDL-level circuits as """"example uses"""" and this turns out to be rather intricate in the face of the kinds of architecture questions that could be posed. In the future, it may well be that some form of software is the input """"example use,"""" thus bringing the processor and FPGA world closer together. For this to work, there needs to be an effective CAD/compiler flow from software to the HDL level. I will give perspective on the state of this art, and discuss what kind of commonality might evolve in architecture exploration tools for FPGAs and processors."""	central processing unit;compiler;computer-aided design;driving simulator;experiment;field-programmable gate array;hardware description language;integrated circuit;parallel computing;programmable logic device;semiconductor device fabrication;time complexity;transistor	Jonathan Rose	2009		10.1109/FPL.2009.5272566	embedded system;computer architecture;computer science;programmable logic device;field-programmable gate array	Arch	7.121832138833717	51.84819894068894	135380
dd1edb526c41be28fe87a6130db7632d8d6e3931	dynamically reconfigurable dataflow architecture for high-performance digital signal processing on multi-fpga platforms	digital signal processing;parallel algorithm;fpga resource;reconfigurable architectures;dynamically reconfigurable dataflow architecture;reconfigurable architectures data flow computing digital signal processing chips field programmable gate arrays;multi fpga chip design;interconnection pattern;fpga based dataflow architecture;field programmable gate arrays dynamically reconfigurable dataflow architecture digital signal processing multi fpga platform fpga based dataflow architecture parallel algorithm fpga resource multi fpga chip design communication bandwidth interconnection pattern single clock cycle;single clock cycle;digital signal processing computer architecture field programmable gate arrays digital signal processing chips concurrent computing parallel algorithms chip scale packaging bandwidth topology switches;communication bandwidth;data flow computing;digital signal processing chips;field programmable gate arrays;multi fpga platform	In this paper we present an FPGA-based dataflow architecture that both efficiently computes parallel algorithms using dedicated FPGA resources and scales well to multi-FPGA chip designs while the overall communication bandwidth increases. The basic idea is based on reconfiguration. In contrast to the concept of partially reconfiguring FPGAs, our approach is to connect computational units via a dynamically variable topology. The latter consists of dedicated switches which are individually controlled by simple shift registers. Hence, the computational result is a function of the currently configured interconnection pattern that can be updated within one single clock cycle. The scalability of this architecture is shown on a high-performance parallel FFT.	clock signal;computation;dataflow architecture;digital signal processing;fast fourier transform;field-programmable gate array;interconnection;network switch;network topology;parallel algorithm;reconfigurability;reconfigurable computing;scalability;shift register;tip (unix utility)	Sven-Ole Voigt	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380734	dataflow architecture;embedded system;computer architecture;parallel computing;computer science;digital signal processing;parallel algorithm;field-programmable gate array	HPC	4.643204241512836	46.78124680207368	135492
2b97995fd6df9bb770b6230ae3105213b6cc8eae	chimaera: a high-performance architecture with a tightly-coupled reconfigurable functional unit	automatic control;data parallel;chimaera c compiler chimaera high performance architecture tightly coupled reconfigurable functional unit prototype system pipeline superscalar processor fine grain data parallel model;tightly coupled reconfigurable functional unit;reconfigurable architectures;prototypes;computer aided instruction;virtual processor;high performance architecture;converter control;prototype system;out of order;performance improvement;chimaera;pipelines;multimedia communication;pim;superscalar processor;lattice gas;fine grain data parallel model;communication system control;hardware prototypes pipelines automatic control communication system control instruction sets multimedia communication computer aided instruction timing out of order;cellular automata;high performance;pipeline processing reconfigurable architectures;chimaera c compiler;reconfigurable hardware;pipeline;pipeline processing;reconfigurable functional unit;dynamic scheduling;instruction sets;hardware;timing	Reconfigurable hardware has the potential for significant performance improvements by providing support for application-specific operations. We report our experience with Chimaera, a prototype system that integrates a small and fast reconfigurable functional unit (RFU) into the pipeline of an aggressive, dynamically-scheduled superscalar processor. Chimaera is capable of performing 9-input/1-output operations on integer data. We discuss the Chimaera C compiler that automatically maps computations for execution in the RFU. Chimaera is capable of: (1) collapsing a set of instructions into RFU operations, (2) converting control-flow into RFU operations, and (3) supporting a more powerful fine-grain data-parallel model than that supported by current multimedia extension instruction sets (for integer operations). Using a set of multimedia and communication applications we show that even with simple optimizations, the Chimaera C compiler is able to map 22% of all instructions to the RFU on the average. A variety of computations are mapped into RFU operations ranging from as simple as add/sub-shift pairs to operations of more than 10 instructions including several branches. Timing experiments demonstrate that for a 4-way out-of-order superscalar processor Chimaera results in average performance improvements of 21%, assuming a very aggressive core processor design (most pessimistic RFU latency model) and communication overheads from and to the RFU.	best, worst and average case;compiler;computation;control flow;execution unit;experiment;field-programmable gate array;out-of-order execution;processor design;prototype;reconfigurable computing;superscalar processor	Zhi Alex Ye;Andreas Moshovos;Scott Hauck;Prithviraj Banerjee	2000		10.1145/339647.339687	computer architecture;chimera;parallel computing;real-time computing;dynamic priority scheduling;reconfigurable computing;computer science;out-of-order execution;operating system;automatic control;instruction set;pipeline transport;prototype;pipeline;personal information manager	Arch	1.7194264190948116	50.52218038701464	135579
c43ae729ec936649acc8d7f614b8813077bac4f0	on-line self-healing of circuits implemented on reconfigurable fpgas	sram chips fault tolerance field programmable gate arrays nanotechnology;triple modular redundant;logic density;circuit faults;fault tolerant;multibit upsets;clocks;real time;real time on line self healing reconfigurable fpga logic density sram based fpga manufacturers nanometric technologies fault tolerant implementations triple modular redundancy infrastructures multibit upsets fault accumulation;flip flops;field programmable gate arrays circuit faults redundancy single event upset computer aided manufacturing electromigration fault detection flip flops clocks power engineering computing;nanotechnology;null;power engineering computing;redundancy;conferenceobject;triple modular redundancy infrastructures;on line self healing;computer aided manufacturing;fault detection;fault tolerance;sram based fpga manufacturers;normal operator;reconfigurable fpga;electromigration;single event upset;power consumption;field programmable gate arrays;fault tolerant implementations;nanometric technologies;fault accumulation;sram chips	To boost logic density and reduce per unit power consumption SRAM-based FPGAs manufacturers adopted nanometric technologies. However, this technology is highly vulnerable to radiation-induced faults, which affect values stored in memory cells, and to manufacturing imperfections. Fault tolerant implementations, based on Triple Modular Redundancy (TMR) infrastructures, help to keep the correct operation of the circuit. However, TMR is not sufficient to guarantee the safe operation of a circuit. Other issues like module placement, the effects of multi- bit upsets (MBU) or fault accumulation, have also to be addressed. In case of a fault occurrence the correct operation of the affected module must be restored and/or the current state of the circuit coherently re-established. A solution that enables the autonomous restoration of the functional definition of the affected module, avoiding fault accumulation, re-establishing the correct circuit state in real-time, while keeping the normal operation of the circuit, is presented in this paper.	autonomous robot;circuit restoration;emergence;field-programmable gate array;memory scrubbing;overhead (computing);place and route;real-time clock;routing;static random-access memory;tree accumulation;triple modular redundancy	Manuel G. Gericota;Luís F. Lemos;Gustavo Ribeiro Alves;José M. Ferreira	2007	13th IEEE International On-Line Testing Symposium (IOLTS 2007)	10.1109/IOLTS.2007.50	reliability engineering;embedded system;fault tolerance;electronic engineering;real-time computing;computer science;engineering;stuck-at fault;computer-aided manufacturing	EDA	8.81874188592702	59.2600543605331	135683
7253dfd193597d1a3d981f0f65caeb98377f4e38	floorplan-driven multivoltage high-level synthesis	high level synthesis	As the semiconductor technology advances, interconnect plays a more and more important role in power consumption in VLSI systems. This also imposes a challenge in high-level synthesis, in which physical information is limited and conventionally considered after high-level synthesis. To close the gap between high-level synthesis and physical implementation, integration of physical synthesis and high-level synthesis is essential. In this paper, a technique named FloM is proposed for integrating floorplanning into high-level synthesis of VLSI system with multivoltage datapath. Experimental results obtained show that the proposed technique is effective and the energy consumed by both the datapath and the wires can be reduced by more than 40%.		Xianwu Xing;Ching-Chuen Jong	2009	VLSI Design	10.1155/2009/156751	electronic engineering;parallel computing;real-time computing;computer science;engineering;high-level synthesis	EDA	3.2586866771079874	55.061149335625515	135945
27369b19bc698c457d5cf8de45bccb1a6ba3a9d5	creating a serial driver chip for commanding robotic arms	motion control;peripheral interfaces;robots application specific integrated circuits driver circuits field programmable gate arrays motion control peripheral interfaces;robotic arm control serial driver chip commanding robotic arms fpga rs 232 interface lynxmotion al5 type robotic arms bidirectional communication scpi standard commands for programmable instruments asic application specific integrated circuit;field programmable gate arrays manipulators universal serial bus connectors ports computers hardware;application specific integrated circuits;robots;driver circuits;field programmable gate arrays	In this paper we shall present a serial driver chip creation on FPGA. We created this serial driver chip for the RS-232 interface and we programmed it to 115200 baud rate to be able to communicate with the Lynxmotion AL5 type robotic arms. This serial driver chip was made for bidirectional communication to be able to send and receive SCPI (Standard Commands for Programmable Instruments) commands on the serial interface. If we create the layout of this chip we can create our own ASIC (Application-Specific Integrated Circuit) and this way we shall have a standalone chip which can control a robotic arm.	application-specific integrated circuit;coat of arms;field-programmable gate array;rs-232;robot;robotic arm;serial communication;standard commands for programmable instruments	Roland Szabó;Aurel Stefan Gontean	2013	2013 Federated Conference on Computer Science and Information Systems		robot;motion control;embedded system;computer hardware;computer science;application-specific integrated circuit;field-programmable gate array	EDA	6.864615669116029	48.82617379321961	136069
bc24fb9140254310c15cb0949dfc9d0101693ee3	a modular peripheral to support self-reconfiguration in socs	time scale;modular peripheral;performance evaluation;partial reconfiguration;self reconfiguration;api;dynamic reconfiguration;reconfigurable architectures;fpga;coprocessors;embedded system;dynamic reconfiguration capability;computer architecture;embedded systems;system on chip;registers;peripheral structure;application program interfaces;hardware coprocessor modular peripheral self reconfiguration soc run time read back embedded system dynamic reconfiguration capability partial reconfiguration capability peripheral structure api software application fpga;core relocation;hardware field programmable gate arrays registers embedded systems performance evaluation computer architecture;soc;scalability fpga dynamic reconfiguration core relocation;system on chip application program interfaces coprocessors electronic engineering computing embedded systems field programmable gate arrays reconfigurable architectures;electronic engineering computing;scalability;partial reconfiguration capability;field programmable gate arrays;software application;use case;run time read back;hardware;hardware coprocessor	In this paper, a solution to support the run-time read back, relocation and replication of cores in embedded systems with dynamic and partial reconfiguration capabilities is presented. The proposal shows a peripheral structure that allows an easy integration and communication with the rest of the system, including an API to make the reconfiguration details to be more transparent to software applications. Differently to other proposals, all functionality is implemented in hardware, achieving a higher reconfiguration speed. In addition, different design decisions have been taken in order to increase the portability of the solution to existing and, possibly, future FPGAs. Finally, a use case is provided, which shows the features of this module applied to the run-time scaling of a hardware coprocessor.	application programming interface;coprocessor;embedded system;field-programmable gate array;image scaling;peripheral;relocation (computing);smart;software portability;system on a chip	Andrés Otero;Angel Morales-Cas;Jorge Portilla;Eduardo de la Torre;Teresa Riesgo	2010	2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools	10.1109/DSD.2010.100	system on a chip;embedded system;computer architecture;real-time computing;computer science;operating system;field-programmable gate array	EDA	-1.9543460437552311	49.72623385459623	136077
22f61d376aac83fb2a9cdce2bc836c4f04045a2b	simple instruction-set computer for area and energy-sensitive iot edge devices		In this paper, we address a novel instruction set architecture (ISA) to realize a simple (small) yet performance-and-energy efficient processor targeting data-centric applications in the ipoT. Focusing on the fact that many of our target applications are mainly composed of light operations which do not need complex arithmetic operations, our proposed processor, SubRISC, supports limited number/types of instructions and simple arithmetic units so that the resources are effectively used (i.e., energy waste can be mitigated) while achieving sufficient performance. We also define efficient instruction formats considering the features of the proposed ISA and target application domains. Our evaluations with two case studies demonstrated the effectiveness of SubRISC against two previous works and revealed important findings on how efficient ISAs desian in ioT.	application domain;central processing unit;experiment;open-source software	Kaoru Saso;Yuko Hara-Azumi	2018	2018 IEEE 29th International Conference on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2018.8445085	computer science;real-time computing;edge device;memory management;energy consumption;instruction set;internet of things;distributed computing	EDA	-1.4358819243076058	49.983642526656986	136353
da429fdf41d1310e0831729e4673d92666b05ea9	dma++: on the fly data realignment for on-chip memories	digital signal processing;random access memory;memory management;code generation;software performance;chip;single instruction multiple data;multimedia computing;computer architecture;engines;registers;multicore processing;hardware computer architecture software performance digital signal processing random access memory memory management instruction sets bandwidth multicore processing multimedia computing;on the fly;bandwidth;program processors;multiprocessor interconnection;instruction sets;hardware	Multimedia extensions based on Single-Instruction Multiple-Data (SIMD) units are widespread. They have been used, for some time, in processors and accelerators (e.g., the Cell SPEs). SIMD units usually have significant memory alignment constraints in order to meet power requirements and design simplicity. This increases the complexity of the code generated by the compiler as, in the general case, the compiler cannot be sure of the proper alignment of data. For that, the ISA provides either unaligned memory load and store instructions, or a special set of instructions to perform realignments in software. In this paper, we propose a hardware realignment unit that takes advantage of the DMA transfers needed in accelerators with local memories. While the data are being transferred, it is realigned on the fly by our realignment unit, and stored at the desired alignment in the accelerator memory. This mechanism can help programmers to better organize data in the accelerator memory so that the accelerator can possibly access the data with no special instructions. Finally, the data are realigned properly also when put back to main memory. Our experiments with nine applications show that with our approach, the bandwidth of the DMA transfers is not penalized.	central processing unit;compiler;computer data storage;data structure alignment;direct memory access;experiment;on the fly;overhead (computing);programmer;random-access memory;requirement;simd;speedup	Nikola Vujic;Marc González;Felipe Cabarcas;Alex Ramírez;Xavier Martorell;Eduard Ayguadé	2010	IEEE Transactions on Computers	10.1109/HPCA.2010.5463057	chip;multi-core processor;computer architecture;parallel computing;real-time computing;simd;software performance testing;computer science;operating system;digital signal processing;instruction set;processor register;programming language;bandwidth;code generation;memory management	Arch	-3.5776700598995164	48.90038675796278	136423
4127cc611c9d2b14f0b4482ba10917429af9c137	lightweight dma management mechanisms for multiprocessors on fpga	software;computer architecture engines indexes field programmable gate arrays hardware programming software;multiprocessing systems application program interfaces field programmable gate arrays file organisation;dma;memory management;api;direct memory access;perforation;lightweight dma management;emulation;fpga;assembly;computer architecture;indexes;embedded multiprocessor architecture lightweight dma management multiprocessor system fpga field programmable gate arrays dma direct memory access mechanisms application programming interface api;engines;multiprocessor architecture;system integration;application program interfaces;access protocols;multiprocessing systems;external memory;field programmable gate arrays;direct memory access mechanisms;multiprocessor system;high performance;embedded multiprocessor architecture;programming;computer buffers;application programming interface;hardware;file organisation	This paper presents a multiprocessor system on FPGA that adopts Direct Memory Access (DMA) mechanisms to move data between the external memory and the local memory of each processor. The system integrates all standard DMA primitives via a fast Application Programming Interface (API) and relies on interrupts having also the possibility to manage a command list. This interface allows to program the embedded multiprocessor architecture on FPGA with simple DMAs using the same DMA techniques adopted on high performance multiprocessors with complex DMA controllers. Several experiments demonstrate the performance of our solution, allowing 57% improvement on the execution time of a selected set of benchmarks. We furthermore show how some DMA programming techniques (double and multi-buffering) can be effectively used within our platform, thus easing the design and development of the hardware and the software in a reconfigurable DMA-based environment.	application programming interface;benchmark (computing);clist;direct memory access;embedded system;experiment;field-programmable gate array;interrupt;multiprocessing;prototype;run time (program lifecycle phase)	Antonino Tumeo;Matteo Monchiero;Gianluca Palermo;Fabrizio Ferrandi;Donatella Sciuto	2008	2008 International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2008.4580191	embedded system;computer architecture;parallel computing;application programming interface;computer science;operating system;direct memory access;field-programmable gate array	EDA	0.7543512642708288	49.08557001109035	136619
a11af7ed3b890487a8bcc4245c9cad6202f3fce1	automated partial reconfiguration design for adaptive systems with copr for zynq	logic design;computer architecture;design automation;das;field programmable gate arrays;fpga;adaptive systems	Dynamically adaptive systems (DAS) respond to environmental conditions, by modifying their processing at runtime and selecting alternative configurations of computation. Field programmable gate arrays, with their support for partial reconfiguration (PR) represent an ideal platform for implementing such systems. Designing partially reconfigurable systems has traditionally been a difficult task requiring FPGA expertise. This paper presents a fully automated framework for implementing PR based adaptive systems. The designer specifies a set of valid configurations containing instances of modules from a standard library. The tool automates partitioning of modules into regions, floorplanning regions on the FPGA fabric, and generation of bitstreams. A runtime system manages the loading of bitstreams automatically through API calls.	adaptive system;application programming interface;computation;constant object proportion rendering;field-programmable gate array;floorplan (microelectronics);high- and low-level;linux;operating system;run time (program lifecycle phase);runtime system;standard library	Kizheppatt Vipin;Suhaib A. Fahmy	2014	2014 IEEE 22nd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2014.63	embedded system;distributed algorithm;parallel computing;real-time computing;electronic design automation;resource allocation;computer science;adaptive system;operating system;peer-to-peer;distributed computing;queueing theory;bandwidth;vegetation;field-programmable gate array;computer network	Arch	2.0002506121455763	52.94489088753122	136714
91b26751d6ba4b831cbbe658cb2a667271d16994	look into details: the benefits of fine-grain streaming buffer analysis	developpement logiciel;streaming applications;compilacion;optimisation;streaming;calculateur embarque;optimizacion;traitement flux donnee;geometrie algorithmique;range analysis;real time;performance;computational geometry;polygone;buffer management;flux donnee;flujo datos;specification programme;buffer system;synchronous data flow;embedded system;sistema amortiguador;polygon;data flow graph;transmission en continu;desarrollo logicial;temps reel;data flow processing;software development;boarded computer;compilation;poligono;tiempo real;algorithme evolutionniste;algorithms;geometria computacional;algoritmo evolucionista;optimization;transmision fluyente;evolutionary algorithm;software synthesis;data flow;systeme tampon;program specification;calculador embarque;especificacion programa;product development	Many embedded applications demand processing of a seemingly endless stream of input data in real-time. Productive development of such applications is typically carried out by synthesizing software from high-level specifications, such as data-flow graphs. In this context, we study the problem of inter-actor buffer allocation, which is a critical step during compilation of streaming applications. We argue that fine-grain analysis of buffers' spatio-temporal characteristics, as opposed to conventional live range analysis, enables dramatic improvements in buffer sharing. Improved sharing translates to reduction of the compiled binary memory footprint, which is of prime concern in many embedded systems. We transform the buffer allocation problem to two-dimensional packing using complex polygons. We develop an evolutionary packing algorithm, which readily yields buffer allocations. Experimental results show an average of over 7X and 2X improvement in total buffer size, compared to baseline and conventional live range analysis schemes, respectively.	actor model;baseline (configuration management);compiler;dataflow;embedded system;evolutionary algorithm;high- and low-level;memory footprint;real-time transcription;register allocation;set packing;static program analysis;streaming media	Mohammad H. Foroozannejad;Matin Hashemi;Trevor L. Hodges;Soheil Ghiasi	2010		10.1145/1755888.1755894	real-time computing;simulation;computational geometry;computer science;evolutionary algorithm;polygon;algorithm	Embedded	-0.4344167200507834	54.14180049248466	136743
a7ab40fac79bc7d65f6043f2bccf4c63d727432f	fault tolerance in linear systolic arrays using time redundancy	reconfiguration;switching;linear systolic arrays;degradation;gracefully degradable mode;interconnection;triple time redundancy;logic testing cellular arrays fault tolerant computing;measurement;fault tolerant;systolic arrays;systolic array;indexing terms;fault tolerant capabilities;cellular arrays;performance metric;control structures;computer architecture;fault tolerant computing;redundancy;control structure;fault tolerance;graceful degradation;logic testing;running time;reliability analysis;performance metrics reliability analysis linear systolic arrays fault tolerant capabilities triple time redundancy reconfiguration gracefully degradable mode interconnection switching control structures running time throughput;fault tolerance systolic arrays redundancy degradation signal processing algorithms hardware computer architecture switches throughput measurement;signal processing algorithms;switches;performance metrics;throughput;hardware	A linear systolic array with fault-tolerant capabilities is developed. Fault tolerance is based on triple time redundancy, and the requisite modifications of the interconnection, switching and control structures to achieve this are discussed. The array is capable of reconfiguring itself in a distributed manner, leading to graceful degradation. Because of the degradations, the algorithms executing on the array need to be remapped. A method for restructuring algorithms and executing them on a degraded array is presented. The reliability analysis of the system is carried out and is compared with the reliability of nonredundant systolic arrays. The reliability analysis of a system with only the capability of single error correction is also presented. The performability of the system, with running time and throughput as performance metrics, is estimated.<<ETX>>	algorithm;control flow;elegant degradation;error detection and correction;fault tolerance;interconnection;systolic array;throughput;time complexity	A. Majumdar;Cauligi S. Raghavendra;Melvin A. Breuer	1988	[1988] Proceedings of the Twenty-First Annual Hawaii International Conference on System Sciences. Volume I: Architecture Track	10.1109/12.45214	fault tolerance;parallel computing;real-time computing;computer science	HPC	7.762215419094837	57.23377756555056	137027
5a3a33a84a43d0edd94760bc28cabe791c0a51f7	a design-space exploration for allocating security tasks in multicore real-time systems		The increased capabilities of modern real-time systems (RTS) expose them to various security threats. Recently, frameworks that integrate security tasks without perturbing the real-time tasks have been proposed, but they only target single core systems. However, modern RTS are migrating towards multicore platforms. This makes the problem of integrating security mechanisms more complex, as designers now have multiple choices for where to allocate the security tasks. In this paper we propose HYDRA, a design space exploration algorithm that finds an allocation of security tasks for multicore RTS using the concept of opportunistic execution. HYDRA allows security tasks to operate with existing real-time tasks without perturbing system parameters or normal execution patterns, while still meeting the desired monitoring frequency for intrusion detection. Our evaluation uses a representative real-time control system (along with synthetic task sets for a broader exploration) to illustrate the efficacy of HYDRA.		Monowar Hasan;Sibin Mohan;Rodolfo Pellizzoni;Rakesh Bobba	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342007	task analysis;real-time computing;computer science;single-core;design space exploration;multi-core processor;intrusion detection system;control system	Embedded	-2.2383173206360656	50.610490357024894	137068
d0452f10f83a4fc331352211ee70198708f55dbf	real world soc experience for the classroom	hardware design languages;microprocessors;microprocessor;cmos technology;soc design training course;integrated circuit;software libraries;peripherals;dual processor soc;hardware description languages;electronic engineering education integrated circuit design logic cad educational courses educational aids system on chip hardware description languages;system on a chip;process design;180 nm;integrated circuit design;design libraries;design technique;hdl code blocks;educational aids;training materials;system on chip;digital logic design software;educational courses;cad software;electronic engineering education;180 nm vlsi design courses digital logic design software soc design training course system on chip design microprocessor memory peripherals cad software design libraries hdl code blocks training materials dual processor soc cell library;software tools;vlsi design courses;system on a chip software libraries ethernet networks education educational institutions cmos technology software tools microprocessors software design hardware design languages;software design;logic cad;ethernet networks;cell library;system on chip design;memory	System-on-chip design is an important new trend in the design of complex integrated circuits. The integration of a microprocessor, memory and peripherals onto a single die opens new possibilities, but also presents new design challenges. Teaching system-on-chip design techniques to students requires not only CAD software but also design libraries and HDL code blocks. This paper presents a reference system-on-chip design as well as a set of training materials. The design is a dual-processor SOC with several peripheral blocks. It is based on a generic 180 nm process design kit and cell library. All materials are available at no charge to students and universities.	code::blocks;computer-aided design;die (integrated circuit);hardware description language;integrated circuit;library (computing);microprocessor;peripheral;process design kit;system on a chip	Johannes Grad;James E. Stine;David D. Neiman	2005	2005 IEEE International Conference on Microelectronic Systems Education (MSE'05)	10.1109/MSE.2005.46	system on a chip;iterative design;embedded system;computer architecture;computer science;operating system;computer aided design;design education;computer engineering	EDA	9.83539599167836	52.52269552201746	137222
c5baf722bb7ebfee422dc1d39b87f578ec4c100b	register spilling via transformed interference equations for pac dsp architecture	register allocation;vliw dsp;spilling	Digital signal processors DSPs with very long instruction word VLIW data-path architectures are increasingly being deployed on embedded devices for multimedia processing applications. To reduce the power consumption and design cost of VLIW DSP processors, distributed register files and multibank register architectures are being adopted to reduce the number of read and write ports associated with register files, which presents new challenges for devising compiler optimization schemes. This paper addresses the issues of reducing the spill code for a VLIW DSP with distributed register files. Spill code produced by register allocation is traditionally handled by memory spills, but the multibank register-file architecture provides the opportunity to spill-out register values onto different register banks. We present a conceptual framework based on the universal and the proxy interference graphs to model the live ranges of registers for spilling codes to different register banks. Heuristic algorithms are then developed on the basis of this concept. By heuristically estimating the register pressure for each register file, we treat different register banks as optional spilling locations in addition to traditional spilling to memory. Experiments were performed on the parallel architecture core VLIW DSP with distributed register files by incorporating our proposed optimization schemes into an Open64-based compiler. The experimental results show that our approach can improve the performances on average for DSPStone and MiBench benchmarks with spilling cases by 7.1% and 21.6%, respectively, compared with the one always handling spill code in memory. Copyright © 2013 John Wiley & Sons, Ltd.	interference (communication);register allocation;register file	Chung-Ju Wu;Chia-Han Lu;Jenq Kuen Lee	2014	Concurrency and Computation: Practice and Experience	10.1002/cpe.3051	parallel computing;real-time computing;register window;computer hardware;control register;computer science;operating system;register renaming;stack register;index register;processor register;flags register;register allocation;register file;status register;memory address register	Arch	-3.738213681244172	52.65230203060596	137269
24a877011d767e9aba5113dade0a61baab7c66fe	networks-on-chip: emerging research topics and novel ideas	network on chip	Networks-on-chip (NoCs) are being devoted intensive research efforts by R&D institutions all around the word, and it is our pleasure to host in this special issue the latest contributions on key design issues at different levels of abstraction , namely, physical link design, architecture design and optimization, performance and power characterization, and design technology. Applying the networking concept to on-chip communication is part of the breakthrough solutions urged by the advances in silicon manufacturing technology (which keeps scaling well beyond 100 nm), by increased time-to-market pressures and by the growing computing requirements of current and future embedded applications. In fact, scalable computation horsepower has been traditionally provided through an increase of clock frequency of mono-lithic processor cores at each technology node. This trend is, however, running into the barriers of nanoscale technologies , such as heating levels beyond the capability of state-of-the-art packaging and cooling technologies, limited scaling of memory access times and the von Neumann bottleneck. These limitations are being increasingly overcome by breaking up functions into concurrent tasks, assigning them to parallel computational units and operating them at a lower frequency than monolithic cores. This approach paves the way for energy-efficient massively parallel chip-level computation architectures, which are at the core of multiprocessor system-on-chip (MPSoC) technology. This trend has profound implications on the communication architecture as well, since the communication requirements of an increasing number of processor cores have to be accommodated by the system interconnect. In contrast , state-of-the-art interconnect fabrics will soon incur severe scalability limitations. The International Technology Roadmap for Semiconductors foresees that they will represent the limiting factor for performance and power consumption in next generation SoCs. In the last few years, a number of advances in on-chip interconnect architectures have tried to relieve the limitations of the communication subsystem. First, more parallel topologies have been proposed to increase the amount of delivered bandwidth, such as partial or full crossbars. However, scalability limitations of crossbar-based interconnection fabrics are well known, and they will not be a long-term solution. Second, new communication protocols have been developed, aiming at a more effective exploitation of the available bandwidth. AMBA 3.0 AXI and the open-core protocol (OCP) are examples thereof. Interestingly, these latest protocols provide support for point-to-point communication only (e.g., an IP core with a bus or directly with another IP core) and do not provide any specification on the interconnect fabric, which can (almost) freely evolve …	automated x-ray inspection;clock rate;computation;computer cooling;crossbar switch;embedded system;image scaling;interconnection;mpsoc;mathematical optimization;multiprocessing;next-generation network;open core protocol;point-to-point protocol;point-to-point (telecommunications);principle of abstraction;requirement;scalability;semiconductor device fabrication;semiconductor intellectual property core;switched fabric;von neumann architecture	Davide Bertozzi;Shashi Kumar;Maurizio Palesi	2007	VLSI Design	10.1155/2007/26454	embedded system;applied mathematics;computer science;management science;network on a chip	EDA	3.5366313115906984	57.79967230127456	137271
bcf42e20a31319f4357f3fb4d9221f331709df65	coordinated concurrent memory accesses on a reconfigurable multimedia accelerator	shared memory;reconfigurable accelerator;multimedia application;embedded system;memory access;embedded systems;shared memory model and multimedia application;system on chip;data access;timing analysis;configurable computing;high throughput;concurrent process	Reconfigurable fabrics are designed by tiling operators and memory banks. In the context of system on chip, the inclusion of multiple local memories is critical for algorithmic performance, as they provide concurrent data accesses for configured compute processes. This paper considers a practical case where internal fabric buses and connectivity give a shared memory characteristic to the architecture. This relies on static reconfigurability and high-level programming techniques to render automated memory access scheduling feasible in a deterministic manner. A complete flow has been developed starting from the programming model down to micro-code enabling task synchronization on memory resources. Compile time analysis is achieved by observing the sequence of operations in the concurrent processes, and by synthesizing a controller program to support the best schedule of operations favoring high throughput. The hardware target is a reconfigurable fabric designed at STMicroelectronics in 65 nm. This hardware/software solution is scalable, flexible and provides high throughput on shared memory. 2008 Elsevier B.V. All rights reserved.	compile time;compiler;datapath;distributed memory;high memory;high- and low-level;high-level programming language;iterative method;memory bank;microcode;moore's law;parallel computing;pipeline (computing);programmer;programming model;programming tool;reconfigurability;reconfigurable computing;requirement;routing;scalability;scheduling (computing);shared memory;system on a chip;throughput;tiling window manager;usability;very-large-scale integration	Samar Yazdani;Joel Cambonie;Bernard Pottier	2009	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2008.08.005	memory address;system on a chip;uniform memory access;distributed shared memory;high-throughput screening;data access;shared memory;embedded system;interleaved memory;transactional memory;semiconductor memory;parallel computing;real-time computing;distributed memory;memory refresh;computer science;operating system;overlay;extended memory;flat memory model;registered memory;static timing analysis;data diffusion machine;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-0.4695713329865729	50.50048996086176	137457
1efe19d383370bc58cdbf1b617b85e03ae57fd8f	body bias domain partitioning size exploration for a coarse grained reconfigurable accelerator			reconfigurable computing	Yusuke Matsushita;Hayate Okuhara;Koichiro Masuyama;Yu Fujita;Ryuta Kawano;Hideharu Amano	2017	IEICE Transactions		computer science;artificial intelligence;computer vision	EDA	4.040742506934165	49.170523180536094	137472
5a20f86fffad669c994d6db6282fcd6d32bfdeb4	concurrent generation of concurrent programs for post-silicon validation	silicon;parallel programming;elemental semiconductors;silicon elemental semiconductors multiprocessing systems parallel programming program testing;program testing;instruction sets integrated circuit interconnections generators silicon timing hardware integrated circuit modeling;multiprocessing systems;si concurrent generation post silicon validation processor design processor core on core hardware mechanism multithreading cache hierarchies hardware exerciser parallel hardware pseudorandom concurrent test program test generation parallel platform threadmill hardware exerciser ibm power7 processor;stimuli generation functional verification multiprocessor verification multithreading post silicon validation	The continuing trend toward increased parallelism in processor design can be seen in both the growing number of processor cores per system and in on-core hardware mechanisms that assist parallelism, such as multithreading and cache hierarchies. This complexity exacerbates the problem of ensuring the functional correctness of such hardware systems. The growing importance of post-silicon validation is leading to an emerging type of parallel application, namely, the hardware exerciser. We describe a method for exercising parallel hardware by generating pseudorandom concurrent test programs. The test generation is carried out on the tested parallel platform and thus the generator itself is also a concurrent program. We describe the challenges associated with this technology and the approach used by the Threadmill hardware exerciser, a tool developed for the post-silicon validation of the IBM POWER7 processor.	cpu cache;computer;concurrent computing;correctness (computer science);multiprocessing;multithreading (computer architecture);parallel computing;processor design;pseudorandomness;software bug;thread (computing)	Allon Adir;Amir Nahir;Avi Ziv	2012	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2012.2189394	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;silicon;programming language	EDA	7.7794427144966125	52.421945252980294	137552
0763a8959dca2e02f3f55df77f9240a44ea60076	evolving robust digital designs	circuit noise;circuit faults;robust digital designs;multipliers;logic design;faults tolerance;working environment noise;evolvable hardware;noise robustness;robust electronics;fault tolerant computing;noise level;logic design fault tolerant computing;circuit faults semiconductor device noise noise robustness hardware adders circuit noise working environment noise circuit testing fault tolerance noise level;evolvable hardware robust digital designs robust electronics faults tolerance digital abstraction flexible technology environment simulator multipliers adders;adders;fault tolerance;digital design;digital abstraction;semiconductor device noise;environment simulator;circuit testing;flexible technology;hardware	Robust electronics is a challenge that the evolvable hardware field is addressing. This paper focusses on tolerance of faults where faults may arise through defects in the technology or unforeseen events. By relaxing the digital abstraction we enable evolution to find robust circuit architectures, exploiting non digital signal variations where necessary. Using a flexible technology and environment simulator, multipliers and adders are extrinsically evolved in noisy environments where gates may fail. The robustness of these evolved circuits are further tested in various noise and gate failure environments.	and gate;baldur's gate (series);digital electronics;evolution;evolvable hardware;experiment;image noise;noise (electronics);redundancy (engineering);reliability engineering;spice 2;scalability;simulation;single point of failure;software bug;test case	Morten Hartmann;Pauline C. Haddow;Frode Eskelund	2002		10.1109/EH.2002.1029863	electronic engineering;real-time computing;engineering;computer engineering	EDA	9.091238108095725	58.34839340115696	137581
f2ba599a730d6cfefbce70e46da9ec184eebd68e	testing asynchronous circuits: help is on the way!	packaging machines;design for testability;ate architectures;automatic testing;integrated logic circuits asynchronous circuits logic testing automatic testing automatic test equipment design for testability integrated circuit testing;automatic test equipment;semiconductor device packaging;circuit testing asynchronous circuits integrated circuit testing laboratories automatic testing test equipment system on a chip semiconductor device packaging packaging machines asynchronous communication;time management;system on a chip;asynchronous circuit;test time management;asynchronous designs;asynchronous communication;asynchronous circuit testing;test translation;logic testing;integrated circuit testing;test generation;asynchronous circuits;circuit testing;integrated logic circuits;test equipment;dft;test time management asynchronous circuit testing asynchronous designs ate architectures design for testability dft test generation test translation	Testing of asynchronous circuits is considered to be a difficult task. The presence of asynchronous components in an Integrated Circuit (IC), let alone the whole IC itself, sends shivers into any test manager. This general sense of difficulty can be attributed to lack of understanding, lack of interest, lack of support from various players in the testing game including Automated Test Equipment (ATE) vendors. However designs are more and more starting to have asynchronous looks. The device size, the performance specs are making the signals appear as asynchronous. In addition the design styles like SoC (System on a Chip) and SoP (System on a Package) may attract the asynchronous communications among the blocks, which constitute these systems. Also going forward, International Technology Roadmap for Semiconductors predicts the need for systems that do not require system wide synchronicity. It also asks for block level handshaking protocols. Test Community is responding to the design changes by supporting various features either onchip or on ATEs to enable testing of these devices. Therefore even though the test community is not directly addressing the problems of testing asynchronous circuits one can expect support for various features that will help in the testing of asynchronous circuits in general. In this talk I will describe the synergies in test problems with synchronous designs that will help in testing of asynchronous designs. In particular, I will touch upon the ATE architectures and point out where the current architectures are inadequate in supporting testing of asynchronous circuits. I will then describe the current and planned features in ATEs that will be helpful in testing asynchronous circuits. I will then describe test/manufacturing flow for a typical IC and point out the gaps that will have to be bridged to make testing of asynchronous circuits practical. In this portion of the talk, I will cover various topics including Design for Testability, Test Generation, Test translation, Test time management, where the development in synchronous circuit domain can be used as platform to build similar capabilities for asynchronous circuits. Proceedings Seventh IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC 01) 1522-8681/01 $10.00 © 2001 IEEE	asynchronous i/o;asynchronous circuit;built-in test equipment;design for testing;handshaking;integrated circuit;semiconductor;software testability;synchronicity;synchronous circuit;synergy;system on a chip;test automation	Ajay Koche	2001		10.1109/ASYNC.2001.914085	asynchronous system;computer architecture;electronic engineering;computer science;computer engineering	EDA	9.987760144325076	54.19907852738748	137633
72968a91f5adf1a133445e446a675d17cca93402	temporal partitioning of data flow graph for dynamically reconfigurable architecture	reconfigurable system;dynamic reconfiguration;temporal partitioning;fpga engineering;algorithm;data flow graph;reconfigurable architecture;vlsi applications;eigenvectors	In this paper, we present a novel temporal partitioning algorithm that temporally partitions a data flow graph on reconfigurable system. Our algorithm can be used to resolve the temporal partitioning problem at the behaviour level. Our algorithm optimizes the whole latency of the design; this aim can be reached by minimizing the latency of the graph and the number of partitions at the same time. Consequently, our algorithm starts by the lowest possible number of partitions; and next it uses the eigenvectors of the graph to find the best schedule of nodes that minimizes the latency of the graph. The proposed methodology was tested on several examples on reconfigurable architecture based on Xilinx Vertex-II XC2V1000 FPGA device. The results show significant reduction in the design latency compared to famous related algorithms used in this field.	dataflow;reconfigurability	Bouraoui Ouni;Ramzi Ayadi;Abdellatif Mtibaa	2011	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2011.05.002	parallel computing;real-time computing;eigenvalues and eigenvectors;computer science;graph partition;theoretical computer science;data-flow analysis;distributed computing;algorithm	EDA	0.18750657392314962	52.65237516598957	137659
dc3ea787edac2f06221ec50a3b00b54fd5298b2e	d2m: datadriven model for fast and accurate timing error simulation in statically scheduled microprocessors				Yuanbo Fan;Russ Joseph	2017			real-time computing;word error rate;computer science	EDA	-0.5998714366696168	57.591485648718326	137823
8f047bafd283c461e1931e962095d35a1e79e943	a hardware approach to concurrent error detection capability enhancement in cots processors	concurrency control microprocessor chips systems analysis safety critical software;concurrent error detection;hardware monitoring error correction field programmable gate arrays delay costs control systems time to market test equipment computer crashes;commercial off the shelf;systems analysis;safety critical software;concurrency control;control flow;safety critical system;error detection;altera flex 10k30 fpga concurrent error detection capability enhancement cots processors commercial off the shelf cots based design safety critical systems hardware based control flow checking execution tracing watchdog processor;microprocessor chips	To enhance the error detection capability in COTS (commercial off-the-shelf)-based design of safety-critical systems, a new hardware-based control flow checking (CFC) technique is presented. This technique, control flow checking by execution tracing (CFCET), employs the internal execution tracing features available in COTS processors and an external watchdog processor (WDP) to monitor the addresses of taken branches in a program. This is done without any modification of application programs, therefore, the program overhead is zero. The external hardware overhead is about 3.5% using an Altera Flex 10K30 FPGA. For different workload programs, the execution time overhead and the error detection coverage of the technique vary between 33.3 and 140.8% and between 79.7 and 84.6% respectively. The errors are detected with about zero latency.	central processing unit;control flow;error detection and correction;field-programmable gate array;overhead (computing);run time (program lifecycle phase);watchdog timer	Amir Rajabzadeh;Seyed Ghassem Miremadi	2005	11th Pacific Rim International Symposium on Dependable Computing (PRDC'05)	10.1109/PRDC.2005.7	reliability engineering;embedded system;systems analysis;parallel computing;real-time computing;error detection and correction;computer science;operating system;concurrency control;programming language;control flow	Arch	7.438571323324668	58.53987571933642	138132
e6927d40f359fc952d88bd9aef75392dd84c122b	fpga-based customizable systolic architecture for image processing applications	processing element;digital signal processors;image processing computer architecture systolic arrays application software real time systems convolution routing buffer storage field programmable gate arrays computer science;image processing equipment;image processing;data memory fpga customizable systolic architecture low level image processing 2d systolic processing array image buffers;systolic arrays digital signal processing chips field programmable gate arrays image processing equipment pipeline arithmetic reconfigurable architectures;reconfigurable architectures;systolic arrays;customizable systolic architecture;systolic array;real time processing;low level image processing;image buffers;fpga;data memory;digital signal processing chips;field programmable gate arrays;pipeline arithmetic;2d systolic processing array	The present work focuses on the development of a reconfigurable systolic-based architecture for low-level image processing. The architecture is customizable providing the possibility to perform window operations for masks of 3 times3, 5 times5 and 7 times7 coefficients. A 2D systolic array of processing elements have been implemented, based on parallel modules with internal pipeline operation where every processing element can be configured according to a control word. In addition the array is provided with a group of image buffers to reduce the number of access to data memory and to extend the array capabilities allowing the possibility of chaining interconnection of multiple processing blocks. Every buffer constitutes a repository of data that can be reused for different processing blocks. Preliminary results using 640 times 480 gray level images show that window-based operations can be performed in real time, processing an image frame in 5 ms, achieving a throughput of around 3.6 GOPS	coefficient;field-programmable gate array;grayscale;high- and low-level;image processing;interconnection;software repository;systolic array;throughput	Griselda Saldaña-González;Miguel Arias-Estrada	2005	2005 International Conference on Reconfigurable Computing and FPGAs (ReConFig'05)	10.1109/RECONFIG.2005.20	embedded system;computer architecture;parallel computing;computer hardware;image processing;computer science;digital image processing;field-programmable gate array	Robotics	4.318490028412399	46.61133201935708	138150
6bb9595a9ef7c9a01f8d40c153b5f9928029eff5	energy efficient noc design	energy efficiency;network on chip;energy efficient;hierarchical architecture;energy efficiency network on a chip power system interconnection energy consumption art fabrics design optimization energy management power system management system on a chip;chip;hfpaa;fpaa;integrated circuit design;management problem;on chip interconnect;energy optimization;system on chip;communication fabrics;differential difference amplifier;integrated circuit interconnections;network on chip integrated circuit design integrated circuit interconnections;rents rule;soc platform;non permuting grouped combinations listing algorithm;second generation current conveyor;power consumption;network on chip interconnect;energy efficient on chip communication;system level energy optimization;interconnectivity analysis;system on chip energy efficiency soc platform communication fabrics network on chip interconnect management problem on chip interconnect energy efficient on chip communication system level energy optimization	"""Energy efficiency is a key concern in the design of advanced SoC platforms. In this talk we will explore the delicate interplay between on-chip communication and power consumption. We will move from state-of-the art communication fabrics (shared buses, crossbars), to advanced, """"revolutionary"""" network-on-chip interconnects. We will touch upon several energy optimization and management problems emerging in the design and tuning of on-chip interconnects. Our analysis will show that energy-efficient on-chip communication is one of the cornerstones of system-level energy optimization."""	electrical connection;mathematical optimization;network on a chip;system on a chip	Luca Benini	2005	2005 18th Symposium on Integrated Circuits and Systems Design	10.1145/1081081.1081087	embedded system;electronic engineering;real-time computing;computer science;engineering;efficient energy use;network on a chip	EDA	3.526456742361566	55.515418724690086	138200
36c7a7d6bb3b64db5e27a2b991e8f967e6456393	the performance and powerpc platform (tm) specification implementation of the mpc106 chipset	motorola products;mpc106 chipset;chrp powerpc platform specification implementation mpc106 chipset powerpc microprocessors personal computer hardware environment secondary cache control high performance memory controller dram rom motorola products system level support industry standard interfaces common hardware reference platform;personal computer hardware environment;performance evaluation;personal computer;high performance memory controller;performance evaluation microprocessor chips;read only memory microprocessors hardware random access memory centralized control system performance error correction timing bridges microcomputers;secondary cache control;chrp;powerpc microprocessors;powerpc platform specification implementation;system design;system level support;common hardware reference platform;dram;high performance;rom;industry standard interfaces;microprocessor chips	The MPC106 provides a PowerPPM Platform specification compliant bridge between the family of PowerPC Microprocessors and the PCI bus. The MPC106’s PCI support will allow system designers to rapidly design systems using peripherals already designed for PCI and the other standard interfaces available in the personal computer hardware environment. The MFC106 also integrates secondq cache control and a high-performance memory controller which supports various types of DRAM and ROM. The MPC106 is the second of a family of Motorola products that provide system level support for industry standard interfaces to be used with PowerPC microprocessors. This paper describes the MPC106, its performance, and its implementation of the PowerPC Platform specification. The PowerPC Platform specification is formally known as the Common Hardware Reference Platform or CHRPTM. Architectural Overview The MPC106 connects directly to the PCI bus and the processor bus, and shares the data bus to system memory with the processor. The MPC106 is partitioned into four interfaces, the processor interface, the second level (L2) cache interface, the memory interface, and the PCI interface. A central control unit provides &itration and coherency control between each of the interfaces. This central control unit suppoas concurrent operations on the processodmemory bus and the PCI bus. The processor interface is a high bandwidth, high p e r f o m c e , TTZ compatible interface which supports any of the MPC60x PowerPC microprocessors. The processor interface supports multiprocessor configurations from 1 up to 4 processors. The L2 interface is highly programmable and flexible and supports both on chip and off chip interfaces. The on chip L2 interface supports four different sizes, 256KBytes, 512KBytes, IMByte, and 2MBytes, and both write through and copy back modes. The memory interface supports either DRAM or ED0 DRAM in sizes up to one gigabyte, which can be split into one to eight banks. The memory interface also supports page mode accesses and parity checking, Read Modify Write parity checking, or ECC checking of single and double bit errors and correction of single bit errors. The ROM interface supports both ROM and FLASH ROM and allows the ROM to be located on the memory interface or the PCI bus. The PCI interface is fully compliant with the PCI Local Bus Specification Revision 2.1 and all its supplements and functions as both a master and target device. 1063-6390/96 $5.00	bandwidth (signal processing);bus (computing);cpu cache;central processing unit;chipset;common hardware reference platform;computer hardware;control unit;conventional pci;dynamic random-access memory;flash memory;gigabyte;local bus;memory controller;microprocessor;multiprocessing;parity bit;peripheral;personal computer;powerpc 600;radio frequency;read-modify-write;read-only memory;system bus;technical standard	Christopher D. Bryant;Michael J. Garcia;Brian K. Reynolds;Laura A. Weber;Glen E. Wilson	1996		10.1109/CMPCON.1996.501759	embedded system;computer architecture;powerpc;computer hardware;computer science;ibm power microprocessors;power architecture	Arch	6.695196922101874	49.35901948862858	138259
8a02d2a036a621d125e4c787980a37dce9585a28	dynamic power management of electronic systems	electronic system;dynamic power management;operating system;design methodology;stochastic model;digital circuits;power dissipation;model identification;satisfiability;design of experiments;power system;equivalence class;benchmarking	| Dynamic power management is a design methodology aiming at controlling performance and power levels of digital circuits and systems, with the goal of extending the autonomous operation time of battery-powered systems, providing graceful performance degradation when supply energy is limited, and adapting power dissipation to satisfy environmental constraints. We survey dynamic power management applied at the system level. We analyze rst idleness detection and shutdown mechanisms for idle hardware resources. We review industrial standards for operating systembased power management, such as the Advanced Con guration and Power Interface (ACPI) standard proposed by Intel, Microsoft and Toshiba. Next, we review system-level modeling techniques, and describe stochastic models for the power/performance behavior of systems. We analyze di erent modeling assumptions and we discuss their validity. Last, we describe a method for determining optimum policies and validation methods, via simulation at di erent abstraction levels, for power managed systems. 2 GIOVANNI DE MICHELI ET AL.	advanced configuration and power interface;autonomous robot;computation;computer-aided design;digital electronics;electronic system-level design and verification;elegant degradation;handheld game console;heuristic;hibernation (computing);hoc (programming language);level design;low-power broadcasting;markov property;mathematical optimization;open road tolling;operating system;operation time;power electronics;power management;shutdown (computing);simulation;stochastic process;systems design	Luca Benini;Alessandro Bogliolo;Giovanni De Micheli	1998		10.1109/ICCAD.1998.10001	equivalence class;embedded system;mathematical optimization;electronic engineering;real-time computing;simulation;design methods;system identification;engineering;electrical engineering;stochastic modelling;dissipation;operating system;mathematics;electric power system;design of experiments;digital electronics;statistics;benchmarking;satisfiability	EDA	7.582587194476027	54.70381102964594	138709
4987489c26de11a55e47c91554297c130dfb70bb	energy-aware modeling of scaled heterogeneous systems		Many-core processors are accelerating the performance of contemporary high-performance systems. Managing power consumption within these systems demands low-power architectures to increase power savings. One of the promising solutions offered today by microprocessor architects is asymmetric microprocessors that integrate different core architectures on a single die. This paper presents analytical models based on scaled power metrics to analyze the impact of various architectural design choices on scaled performance and power savings. The power consumption implications of different processing schemes and various chip configurations were also analyzed. Analysis shows that by choosing the optimal chip configuration, energy efficiency and energy savings can be increased considerably.	central processing unit;die (integrated circuit);fat object;gustafson's law;http 404;joule;low-power broadcasting;manycore processor;microprocessor;multi-core processor;performance per watt;scalability;simulation;speedup	Ami Marowka	2016	International Journal of Parallel Programming	10.1007/s10766-016-0453-2	embedded system;parallel computing;real-time computing;operating system	Arch	-3.5652526420737023	54.95294175997437	138720
ccf7e3ab11a17b5f062a7bb3c196b9fec9fbb3e5	adaptive ecc for tailored protection of nanoscale memory	error correction codes error correction decoding nanoscale devices reliability runtime memory management;robust nanoscale memory memory failures error correction code ecc variable ecc run time protection	Editor’s note: Following technology scaling, runtime failure has emerged as one of the major challenges in modern VLSI designs under the increased parametric variability and low supply voltage. This issue is especially severe in nanoscale memory due to its high density and large capacity. In this work the authors present a novel reconfigurable Error Correction Code (ECC) to improve the reliability of nanoscale memory. —Yiran Chen, University of Pittsburgh	bit error rate;entity–relationship model;error-tolerant design;forward error correction;image scaling;low-power broadcasting;operating point;overhead (computing);spatial variability;very-large-scale integration	Dongyeob Shin;Jongsun Park;Jangwon Park;Somnath Paul;Swarup Bhunia	2017	IEEE Design & Test	10.1109/MDAT.2016.2615844	parallel computing;computer engineering;very-large-scale integration;redundant array of independent memory;memory management;error detection and correction;computer science	EDA	6.951468624795069	60.37402250031587	138770
a7c0c730ccc8f50cd29e31206a4d922c3cfa2aa3	bti aware thermal management for reliable dvfs designs	stress;reliability;aging;qa75 electronic computers computer science;logic gates;propagation delay;transistors;integrated circuit modeling	In this paper, we show that dynamic voltage and frequency scaling (DVFS) designs, together with stress-induced BTI variability, exhibit high temperature-induced BTI variability, depending on their workload and operating modes. We show that the impact of temperature-induced variability on circuit lifetime can be higher than that due to stress and exceed 50% over the value estimated considering the circuit average temperature. In order to account for these variabilities in lifetime estimation at design time, we propose a simulation framework for the BTI degradation analysis of DVFS designs accounting for workload and actual temperature profiles. A profile is generated considering statistically probable workload and thermal management constraints by means of the HotSpot tool. Using the proposed framework we explore the expected lifetime of the Ethernet circuit from the IWLS05 benchmark suite, synthesized with a 32nm CMOS technology library, for various thermal management constraints. We show that margin-based design can underestimate or overestimate lifetime of DVFS designs by up to 67.8% and 61.9%, respectively. Therefore, the proposed framework allows designers to select appropriately the dynamic thermal management constraints in order to tradeoff long-term reliability (lifetime) and performance with upto 35.8% and 26.3% higher accuracy, respectively, against a temperature-variability unaware BTI analysis.	benchmark (computing);btrieve;cmos;dynamic frequency scaling;dynamic voltage scaling;elegant degradation;heart rate variability;image scaling;java hotspot virtual machine;norm (social);simulation;thermal management of high-power leds	Hardeep Chahal;Vasileios Tenentes;Daniele Rossi;Bashir M. Al-Hashimi	2016	2016 IEEE International Symposium on Defect and Fault Tolerance in VLSI and Nanotechnology Systems (DFT)	10.1109/DFT.2016.7684059	reliability engineering;embedded system;propagation delay;electronic engineering;real-time computing;logic gate;computer science;engineering;operating system;reliability;stress;transistor;statistics	EDA	-3.316219140293087	57.14594043763645	138894
4b52dfee3da78c14dd63144908c713226625476f	maximizing area-constrained partial fault tolerance in reconfigurable logic	field programmable gate array;fault tolerant;area constrained;reconfigurable logic;fpga;fault coverage	As field programmable gate arrays find increasing use in aerospace and terrestrial applications, a number of methods of fault tolerance have been developed to ensure reliable operation. Most current techniques output the required circuit area based on the desired level of fault tolerance with some techniques increasing the area by over 200%. In deployed systems, however, the FPGA is fixed and the area available for adding fault tolerance is limited. As a consequence, protecting an updated, larger circuit using the same fault tolerance scheme may result in a design that no longer fits in the deployed FPGA. This situation dictates the need for techniques that can trade fault tolerance for lower area penalties. Area constrained approaches accept available hardware resources as an input and outputs a partially fault tolerant circuit. The open question with these approaches is selecting the circuit subset to protect which will maximize the fault coverage. This paper presents several methodologies for selecting subsets and analyzes their performances on several circuits based on fault coverage provided and additional delay.	fits;fault coverage;fault tolerance;field-programmable gate array;performance;reconfigurable computing;terrestrial television	David L. Foster;Darrin M. Hanna	2010		10.1145/1723112.1723155	embedded system;real-time computing;fault coverage;fault indicator;computer science;stuck-at fault;fault model;general protection fault;software fault tolerance;field-programmable gate array	EDA	7.422582142594557	58.24862181623775	139048
d84d3a988e38bb7bae87b376d5efd72cfee4e3d0	a framework for self-healing radiation-tolerant implementations on reconfigurable fpgas	radiation effects;logic density;fault tolerant implementation;fault tolerant;field programmable gate arrays circuit faults redundancy flip flops protection reconfigurable logic manufacturing fault tolerance logic devices single event transient;multibit upsets;real time;triple modular redundancy infrastructure;nanotechnology;self healing radiation tolerant implementation;fault accumulation self healing radiation tolerant implementation reconfigurable fpga sram based fpga nanometric technology logic density nanometric scales radiation induced faults fault tolerant implementation triple modular redundancy infrastructure multibit upsets;redundancy;conferenceobject;fault tolerance;normal operator;reconfigurable fpga;nanometric scales;sram chips fault tolerance field programmable gate arrays integrated circuit reliability nanotechnology radiation effects redundancy;field programmable gate arrays;integrated circuit reliability;nanometric technology;radiation induced faults;sram based fpga;fault accumulation;sram chips	To increase the amount of logic available in SRAM-based FPGAs manufacturers are using nanometric technologies to boost logic density and reduce prices. However, nanometric scales are highly vulnerable to radiation-induced faults that affect values stored in memory cells. Since the functional definition of FPGAs relies on memory cells, they become highly prone to this type of faults. Fault tolerant implementations, based on triple modular redundancy (TMR) infrastructures, help to keep the correct operation of the circuit. However, TMR is not sufficient to guarantee the safe operation of a circuit. Other issues like the effects of multi-bit upsets (MBU) or fault accumulation, have also to be addressed. Furthermore, in case of a fault occurrence the correct operation of the affected module must be restored and the current state of the circuit coherently re-established. A solution that enables the autonomous correct restoration of the functional definition of the affected module, avoiding fault accumulation, re-establishing the correct circuit state in realtime, while keeping the normal operation of the circuit, is presented in this paper.	autonomous robot;circuit restoration;fault-tolerant computer system;field-programmable gate array;memory cell (binary);memory scrubbing;overhead (computing);static random-access memory;tree accumulation;triple modular redundancy	Manuel G. Gericota;Luís F. Lemos;Gustavo Ribeiro Alves;José M. Ferreira	2007	2007 IEEE Design and Diagnostics of Electronic Circuits and Systems	10.1109/DDECS.2007.4295300	embedded system;fault tolerance;electronic engineering;real-time computing;computer science;engineering	EDA	8.73538317887717	59.33697566482428	139072
3847d8e9e506f1ac9533e194e551105c534e75e5	the convergence of physical/digital worlds: implications on workloads & architecture	convergence;sensors;computer architecture;distance measurement;data analysis;low power electronics;artificial intelligence	Summary form only given. As sensing and computing have become ultra-low power and ubiquitous, we have entered an era of physical/digital convergence. This physical/digital convergence is bringing about new opportunities (usages) as well as challenges (end-to-end architectures). The talk will show examples ranging from static sensing and simple data analytics to the emerging (r)evolution towards artificial intelligence. Using these examples, I will attempt to outline areas of research in workload analysis as well as novel end-to-end architectures.	artificial intelligence;cuecat;end-to-end principle	Ravi R. Iyer	2016	2016 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2016.7581260	computer architecture;parallel computing;real-time computing;simulation;convergence;computer science;sensor;theoretical computer science;operating system;data analysis;low-power electronics	Arch	-1.485265507422979	57.89129993027582	139111
ed06ff360a9785e29370ad5e708c86fd6014c11f	fpga place & route challenges	routing;placement;physical design;fpga	In this paper, we describe the challenges that Place and Route tools face to implement the user designs on modern FPGAs while meeting the timing and power constraints.	field-programmable gate array;place and route	Rajat Aggarwal	2014		10.1145/2560519.2568050	physical design;embedded system;routing;parallel computing;real-time computing;computer science;field-programmable gate array;placement	EDA	7.104786037638738	54.943060912137064	139149
4af321a440fa1ffee991c438741608f6a2309c34	design of risc microcontroller with dynamic thermal management unit for temperature-controlled oscillator	microcontrollers oscillators registers frequency control thermal management clocks;cmos risc tco dpm;thermal management packaging cmos analogue integrated circuits integrated circuit design integrated circuit packaging microcontrollers oscillators reduced instruction set computing;word length 8 bit processor core verilog hardware description language top down technique full custom technique thermal chamber status control bits oscillator frequency control signal chip temperature registers thermal interrupt control unit clock source multiplexer risc structure octalynx d microcontroller umc cmos temperature controlled oscillator dynamic thermal management unit risc microcontroller design size 0 18 mum voltage 1 8 v	The paper describes UMC CMOS 0.18 μm (1.8 V) implementation of OctaLynx D microcontroller. The processor is 8-bit RISC structure with built-in Dynamic Thermal Management Unit cooperating with Temperature-Controlled Oscillator. The Dynamic Thermal Management Unit consists of clock source multiplexer, thermal interrupts control unit and special registers containing information of present chip temperature, oscillator frequency control signal and status/control bits. Processor core was written in Verilog hardware description language and designed in top-down technique while memories and analogue parts were designed in full custom technique. The system is dedicated for tests in thermal chamber for development of Dynamic Power Management methods.	16-bit;8-bit;cmos;clock generator;control unit;frequency scaling;full custom;general-purpose input/output;hardware description language;image scaling;interrupt;microcontroller;multiplexer;power management;programmer;read-only memory;serial peripheral interface bus;thermal management of high-power leds;timer;top-down and bottom-up design;umc (computer);verilog	Maciej Frankiewicz;Piotr Kocanda;Ryszard Gal;Andrzej Kos	2014	2014 Proceedings of the 21st International Conference Mixed Design of Integrated Circuits and Systems (MIXDES)	10.1109/MIXDES.2014.6872203	embedded system;electronic engineering;computer hardware;computer science	EDA	6.938375352336024	49.7970681133403	139268
5883bddb5613b56ddea9efc9692563f73d4a2481	modeling the temperature bias of power consumption for nanometer-scale cpus in application processors	power measurement approximation theory microprocessor chips power aware computing;temperature measurement leakage currents program processors heating voltage control computational modeling temperature distribution;temperature 20 c to 85 c system on chips power management unit thermal management unit temperature induced bias power measurements temperature neutral power data power requirements system failure rates quadratic model linear approximation nanometer scale application processors cpu temperature power relationship macrolevel model nanometer scale cpus power consumption temperature bias modeling	We introduce and experimentally validate a new macro-level model of the CPU temperature/power relationship within nanometer-scale application processors or system-on-chips. By adopting a holistic view, this model is able to take into account many of the physical effects that occur within such systems. Together with two algorithms described in the paper, our results can be used, for instance by engineers designing power or thermal management units, to cancel the temperature-induced bias on power measurements. This will help them gather temperature-neutral power data while running multiple instance of their benchmarks. Also power requirements and system failure rates can be decreased by controlling the CPU's thermal behavior. Even though it is usually assumed that the temperature/power relationship is exponentially related, there is however a lack of publicly available physical temperature/power measurements to back up this assumption, something our paper corrects. Via measurements on two pertinent platforms sporting nanometer-scale application processors, we show that the power/temperature relationship is indeed very likely exponential over a 20°C to 85°C temperature range. Our data suggest that, for application processors operating between 20°C and 50°C, a quadratic model is still accurate and a linear approximation is acceptable.	algorithm;backup;benchmark (computing);central processing unit;experiment;holism;linear approximation;quadratic equation;relevance;requirement;system on a chip;thermal management of high-power leds;time complexity	Karel De Vogeleer;Gérard Memmi;Pierre Jouvelot;Fabien Coelho	2014	2014 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS XIV)	10.1109/SAMOS.2014.6893209	embedded system;electronic engineering;real-time computing;engineering;electrical engineering;operating system;low-power electronics	EDA	-3.2050634444854897	57.363020714545364	139434
1360290f4bc2edd7179d8666a452dfa33d181de8	synaptic project: regularity applied to enhance manufacturability and yield at several abstraction levels			design for manufacturability	Martin Elhøj;André Inácio Reis;Renato P. Ribas;Fabrizio Ferrandi;Christian Pilato;Francesc Moll;Miguel Corbalan;Petr Dobrovolný;Nigel Woolaway;Arnaud Grasset;Philippe Bonnot;Giuseppe Desoli;Davide Pandini	2011			reliability engineering;simulation;abstraction;design for manufacturability;computer science	Logic	8.795927450026454	53.55740090481889	139454
895c86ba0185a6d8f7048cd9c593ad7315a9a27a	improving the memory bandwidth utilization using loop transformations	bande passante;passband;evaluation performance;gestion memoire;multimedia;performance evaluation;banda pasante;systeme embarque;integrated circuit;storage management;real time;evaluacion prestacion;circuito integrado;paralelisacion;memory access;embedded systems;gestion memoria;loop transformation;energy consumption;parallelisation;parallelization;hierarchie memoire;memory hierarchy;power consumption;consommation energie electrique;jerarquia memoria;memory bandwidth;circuit integre;embedded device	Embedded devices designed for various real-time multimedia and telecom applications, have a bottleneck in energy consumption and performance that becomes day by day more crucial. This is imposed by the increasing gap between processor and memory speed. Many authors have addressed this problem, but all existing techniques either consider only performance without any other trade-off, or they operate at the level of individual loops. We fill this gap, by presenting a technique which achieves parallelization in the memory accesses through four loop transformations. Our estimations from two real-life applications from the multimedia and telecom domain, reveal that using our technique, we can either increase the performance (up to 35%) or lower the energy consumption (up to 20%) for the same cost.	memory bandwidth	Minas Dasygenis;Erik Brockmeyer;Francky Catthoor;Dimitrios Soudris;Adonios Thanailakis	2005		10.1007/11556930_13	embedded system;real-time computing;computer science;integrated circuit;passband;memory bandwidth	Arch	-0.7529727601076359	54.43094657091424	139512
be0162935514716a1758b36d31276f1193724741	a soft error vulnerability analysis framework for xilinx fpgas	single events upsets seus;sensitive configuration bits;fpgas;soft errors;placement algorithms;seu mitigation	Today's SRAM-based FPGAs provide a reach set of computing resources which makes them attractive in demanding and critical application domains, such as avionics and space. Unfortunately, their high reliance on SRAM configuration memory arise reliability issues due to the single-event upsets (SEUs). Considering the criticality of these applications, the vulnerability analysis of FPGA designs to SEUs becomes essential part of the design flow. In this context, we present an open-source framework for the soft error vulnerability analysis of Xilinx FPGA devices. The proposed framework will allow researchers to evaluate their reliability-aware CAD algorithms and estimate the soft error susceptibility of the designs at early stages of the implementation flow for the latest Xilinx architectures.	algorithm;avionics;computer-aided design;criticality matrix;experiment;fault injection;field-programmable gate array;graham scan;l (complexity);open-source software;p (complexity);routing;set packing;simulation;single event upset;soft error;static random-access memory;steiner tree problem	Aitzan Sari;Dimitris Agiakatsikas;Mihalis Psarakis	2014		10.1145/2554688.2554767	embedded system;parallel computing;real-time computing;computer science;field-programmable gate array	EDA	7.955572606189923	58.938976359985	139516
d0f20921dba0f0a94524a17bdd13bbb9b79b0fe2	fast transaction-level dynamic power consumption modelling in priority preemptive wormhole switching networks on chip	cycle approximate tlm methodology;network on chip;flit by flit progress;time varying systems;wires;noc simulations;time domain analysis;priority preemptive wormhole switching networks on chip;indexes;computational modeling;bus invert coding;fast transaction level dynamic power consumption modelling;cycle approximate transaction level modelling;mathematical model;power consumption network on chip;cycle accurate simulation;power consumption;bus invert coding fast transaction level dynamic power consumption modelling priority preemptive wormhole switching networks on chip cycle approximate transaction level modelling noc simulations cycle accurate simulation high fidelity noc power modelling cycle approximate tlm methodology flit by flit progress;power demand;high fidelity noc power modelling;power demand computational modeling time varying systems time domain analysis mathematical model wires indexes	This paper specifies an architecture for power consumption modelling integrated within cycle-approximate transaction level modelling for network-on-chip (NoC) simulation. NoC simulations during design validation have traditionally been limited to very short durations, due to the necessity to perform cycle-accurate simulation to represent fully the low level system simulated. Due to the high proportion of overall system power that may be consumed by a busy NoC, high-fidelity NoC power modelling is especially important to accurately assess the effectiveness of link coding and other strategies to reduce NoC power consumption. The paper describes the extension of a cycle-approximate TLM methodology to encompass power modelling in NoCs, considering its operation with real application traffic. The proposed scheme avoids modelling of flit-by-flit progress during non-preemptive periods of packet transmission. The simulation performance and accuracy are contrasted with theoretical models and a flit-by-flit scheme (in which each flow control digit passing along a bus wire is simulated). The power consumption reduction delivered by encoding schemes such as bus-invert coding are considered and compared with analytical models to verify the correct performance of the simulation models.	approximation algorithm;low-power broadcasting;matlab;network on a chip;network packet;preemption (computing);simulation;transaction-level modeling;wormhole switching	James Harbin;Leandro Soares Indrusiak	2013	2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)	10.1109/SAMOS.2013.6621120	embedded system;parallel computing;real-time computing;computer science	EDA	1.367223704591719	59.421558296326154	140020
3f1f329151e155300e65d74da4d17eea1b06185b	debug facilities in the trimedia cpu64 architecture	vliw processor;virtual memory;multimedia application;design for debug;application debug;vector data;embedded processor	This paper describes debug facilities in the Philips TriMedia CPU64, which is an embedded processor core for multimedia applications. Its architecture provides a VLIW pipeline, support for 64-bit vector data, and virtual memory management. The debug hardware in the TriMedia CPU64 supports two complementary debug strategies. One strategy provides a snapshot of the processor state at certain moments in time, which is achieved by single-step execution and various breakpoint types. The other debug strategy provides continuous monitoring of the processor state by using a PC trace buffer. Precise exceptions are used to provide accurate context switching from application software to debugger software.	64-bit computing;breakpoint;context switch;debug;debugger;embedded system;front and back ends;interrupt;memory management;multi-core processor;real-time transcription;rudolf m. tromp;simulation;snapshot (computer storage);trimedia (mediaprocessor)	Harald P. E. Vranken	1999	European Test Workshop 1999 (Cat. No.PR00390)	10.1023/A:1008307818771	x86 debug register;debug code;embedded system;computer architecture;parallel computing;real-time computing;trimedia;computer science;virtual memory;debug menu;background debug mode interface	Arch	-4.018205278818756	51.8694858529846	140135
5d2cd62aac26c50bd7a5af05a52221b108d5b7ae	dsp development with full-speed prototyping based on hw/sw codesign techniques	elektroteknik och elektronik;electrical engineering electronic engineering information engineering;data collection;cost efficiency	The automatic HW/SW tool provides an efficient tool for DSP system optimization. In addition to prototyping, the filtered data can also be closer analysed on Matlab with the real data collected from the system environment. The time to change the system takes only a few minutes to complete while the changing of hardware prototype on FPGAs takes usually even with synthesis tools some days in complex changes. The proposed approach allows a fast search of the best filtering solution with minimal effort for implementation. As no DSP functions are needed to implement it allows typically much faster operating speed than which is possible with normal FPGA prototypes or ASIC emulators and provides a very cost efficient environment for the system level development. Also, unlike in a full prototyping with FPGAs, the maximum operating speed and the system capacity is quite easy to approximate.	shattered world	Jouni Isoaho;Axel Jantsch;Hannu Tenhunen	1994		10.1007/3-540-58419-6_112	embedded system;computer science;cost efficiency;data collection	EDA	3.444048292769825	55.36223129874231	140226
85dead934dd6486232238cbd6d6ba52fdeba6424	hybrid hardware/software floating-point implementations for optimized area and throughput tradeoffs	computers;hardware throughput program processors computer architecture kernel computers;kernel;floating point fp arithmetic and logic structures computer arithmetic fine grained system;computer architecture;program processors;throughput;hardware	Hybrid floating-point (FP) implementations improve software FP performance without incurring the area overhead of full hardware FP units. The proposed implementations are synthesized in 65-nm CMOS and integrated into small fixed-point processors with a RISC-like architecture. Unsigned, shift carry, and leading zero detection (USL) support is added to a processor to augment an existing instruction set architecture and increase FP throughput with little area overhead. The hybrid implementations with USL support increase software FP throughput per core by 2.18× for addition/subtraction, 1.29× for multiplication, 3.07-4.05× for division, and 3.11-3.81× for square root, and use 90.7-94.6% less area than dedicated fused multiply- add (FMA) hardware. Hybrid implementations with custom FP-specific hardware increase throughput per core over a fixed-point software kernel by 3.69-7.28× for addition/subtraction, 1.22-2.03× for multiplication, 14.4× for division, and 31.9× for square root, and use 77.3-97.0% less area than dedicated FMA hardware. The circuit area and throughput are found for 38 multiply-add, 8 addition/subtraction, 6 multiplication, 45 division, and 45 square root designs. Thirty-three multiply- add implementations are presented, which improve throughput per core versus a fixed-point software implementation by 1.11-15.9× and use 38.2-95.3% less area than dedicated FMA hardware.	cmos;central processing unit;fma instruction set;fixed point (mathematics);fixed-point arithmetic;floating-point unit;leading zero;multiply–accumulate operation;overhead (computing);throughput;unix system laboratories	Jon J. Pimentel;Brent Bohnenstiehl;Bevan M. Baas	2017	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2016.2580142	embedded system;throughput;computer architecture;parallel computing;kernel;real-time computing;computer science;operating system	Arch	1.0556542957252546	48.393966627988334	140398
9263d4fad0522516d0ec49ddbc24740954eaa234	a taxonomy of reconfigurable single-/multiprocessor systems-on-chip	multiprocessor system on chip	Runtime adaptivity of hardware in processor architectures is a novel trend, which is under investigation in a variety of research labs all over the world. The runtime exchange of modules, implemented on a reconfigurable hardware, affects the instruction flow (e.g., in reconfigurable instruction set processors) or the data flow, which has a strong impact on the performance of an application. Furthermore, the choice of a certain processor architecture related to the class of target applications is a crucial point in application development. A simple example is the domain of high-performance computing applications found in meteorology or high-energy physics, where vector processors are the optimal choice. A classification scheme for computer systems was provided in 1966 by Flynn where single/multiple data and instruction streams were combined to four types of architectures. This classification is now used as a foundation for an extended classification scheme including runtime adaptivity as further degree of freedom for processor architecture design. The developed scheme is validated by a multiprocessor system implemented on reconfigurable hardware as well as by a classification of existing static and reconfigurable processor systems.	system on a chip;taxonomy (general)	Diana Göhringer;Thomas Perschke;Michael Hübner;Jürgen Becker	2009	Int. J. Reconfig. Comp.	10.1155/2009/395018	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	EDA	-1.874638638426657	49.72857968508811	140410
f6208aadc95008b111b450617456bb8ecbcdb4f1	microprogrammed architecture for front end processing	new facility;new data transmission;front end processing;modular microprogrammed architecture;front-end system;major commercial operating system;commercial microprogrammed front-end processor;economical system structure;corresponding architecture;control function;functional level;design automation;data transmission;economic system;register transfer level;operating system;front end	The increasing diversity of hardware devices and software procedures developed for remote processing has yielded a multiplicity of new facilities and telecommunication network structures. The corresponding architectures for front-end systems range from specialized device control to sophisticated multi-purpose multi-terminal support. Simultaneously, the very complexity of new data transmission and processing techniques has created a need for flexible and powerful yet transparent communications processors. The functions of a front-end system will be analyzed in order to derive the concepts which will be used to establish a classification.  In a first part, telecommunications and control functions are analyzed in detail, as well as the user facilities at the functional level.  The level of support provided by major commercial operating systems with respect to front-end systems is then examined. Their facilities and shortcomings provide the basis for front-end systems.  Finally, the architecture of a commercial microprogrammed front-end processor is presented. A modular microprogrammed architecture of this type allows efficient and economical system structuring for a wide range of teleprocessing services.	central processing unit;control function (econometrics);end system;front-end processor;microcode;multi-purpose viewer;operating system;telecommunications network	Rodnay Zaks	1973		10.1145/800123.803989		Arch	0.9761283597077587	58.35244879902807	140439
e52eff074894ac508c3c627a164b0beeaaf10219	rapid-prototyping of embedded systems via reprogrammable devices	xilinx fpga device;programmable device;automotive world;flexible board-level rapid-prototyping environment;fpga device;aptix device;fast programming data;aptix board;reprogrammable devices;field-programmable inter-connect;software synthesis;automotive electronics;micro-controller emulator;rapid prototyping;hardware synthesis;embedded systems;hardware/software co-design;complex system;embedded system;hardware;logic;application software;logic programming;field programmable gate arrays;logic design;programmable logic devices;real time systems	This paper describes a flexible board-level rapid-prototyping environment for embedded control applications. The environment is based on an APTIX board populated by Xilinx FPGA devices, a 68HC11 emulator, and APTIX programmable interconnect devices. Given a design consisting of logic and of software running on a micro-controller that implement a set of tasks, the prototype is obtained by programming the FPGA devices, the micro-controller emulator and the APTIX devices. This environment being based on programmable devices offers the flexibility to perform engineering changes, the performance needed to validate complex systems and the hardware setup for field tests. The key point in our approach is the use of results of our previous research on software and hardware synthesis as well as of some commercial tools to provide the designer with fast programming data from a high-level description of the algorithms to be implemented. We demonstrate the effectiveness of the approach by showing a close-to real-life example from the automotive world.	embedded system;rapid prototyping	Stefano Cardelli;Massimiliano Chiodo;Paolo Giusto;Attila Jurecska;Luciano Lavagno;Claudio Sansoè;Alberto L. Sangiovanni-Vincentelli	1998	Design Autom. for Emb. Sys.	10.1023/A:1008890323570	embedded system;complex systems;computer architecture;application software;computer science;logic programming;logic;computer engineering	EDA	4.703063357596481	51.601270392972204	140506
834cb12942e092256e2d59b7ba4355114ae6784f	integrated loop optimizations for data locality enhancement of tensor contraction expressions	computer science;space exploration;quantum computing;optimizing compiler;loop optimization;measurement;compiler optimization;tensile stress;cost function	A very challenging issue for optimizing compilers is the phase ordering problem: In what order should a collection of compiler optimizations be performed? We address this problem in the context of optimizing a sequence of tensor contractions. The pertinent loop transformations are loop permutation, tiling, and fusion; in addition, the placement of disk I/O statements crucially affects performance. The space of possible combinations is exponentially large. We develop novel pruning strategies whereby a search problem in a larger space is replaced by a large number of searches in a much smaller space, to determine the optimal permutation, fusion, tiling and placement of disk I/O statements. Experimental results show that we obtain an improvement in I/O cost by a factor of up to 2.6 over an equi-tile-size approach.	amdahl's law;charge-coupled device;computation;computer performance;heuristic (computer science);input/output;locality of reference;loop invariant;loop optimization;mathematical optimization;optimizing compiler;placement (eda);relevance;search problem;speedup;tiling window manager	Swarup Kumar Sahoo;Sriram Krishnamoorthy;Rajkiran Panuganti;P. Sadayappan	2005	ACM/IEEE SC 2005 Conference (SC'05)		loop tiling;parallel computing;real-time computing;computer science;theoretical computer science;operating system;optimizing compiler;programming language	HPC	-4.3839772428638515	47.67092882595443	140509
7c962fcc393e2630cd2e37ff13d6d1d1c958d19a	quantitative analysis of floating point arithmetic on fpga based custom computing machines	automatic control;high level languages;performance evaluation;ccms;prototypes;fpga;programmable logic arrays;acceleration;speed;control system synthesis;speed floating point arithmetic fpga custom computing machines quantitative analysis ccms area consumption;dynamic range;custom computing machines;quantitative analysis;floating point;performance evaluation floating point arithmetic field programmable gate arrays programmable logic arrays;floating point arithmetic;field programmable gate arrays;signal processing algorithms;area consumption;real time application;floating point arithmetic field programmable gate arrays signal processing algorithms dynamic range acceleration real time systems high level languages prototypes control system synthesis automatic control;real time systems	Many algorithms rely on floating point arithmetic for the dynamic range of representations and require millions of calculations per second. Such computationally intensive algorithms are candidates for acceleration using custom computing machines (CCMs) being tailored for the application. Unfortunately, floating point operators require excessive area (or time) for conventional implementations. Instead, custom formats, derived for individual applications, are feasible on CCMs, and can be implemented on a fraction of a single FPGA. Using higher-level languages, like VHDL, facilitates the development of custom operators without significantly impacting operator performance or area. Properties, including area consumption and speed of working arithmetic operator units used in real-time applications, are discussed.	adder (electronics);algorithm;amazon simple storage service;coefficient;computer;convolution;dynamic range;fast fourier transform;field-programmable gate array;finite impulse response;real-time transcription;sparc;vhdl;workstation	Nabeel Shirazi;Al Walters;Peter M. Athanas	1995		10.1109/FPGA.1995.477421	embedded system;parallel computing;real-time computing;computer hardware;computer science;floating point;operating system;automatic control;field-programmable gate array	Graphics	3.9688640685894	47.28371675679636	140516
71f226cb64572ffd10faa8cc1d8fe07c5a6d1e4d	a two-level co-design framework for xputer-based data-driven reconfigurable accelerators	application development;optimisation;programming environments;field programmable analog arrays;hardware software co design;transputer systems;programming paradigm;reconfigurable architectures;application development methodology;reconfigurable logic;parallelizing compilers;systolic array;hardware platform;parallel programming;parallelizing compilation environment;software engineering;code x;computer applications;profiling driven host accelerator partitioning;acceleration;simultaneous programming;high level synthesis;reconfigurable resource utilization optimization;parallel architectures;logic programming;design framework;parallelising compilers;resource driven sequential structural partitioning;development systems high level synthesis software engineering parallelising compilers reconfigurable architectures transputer systems programming environments parallel programming parallel architectures optimisation;field programmable logic;transputer based data driven reconfigurable accelerators;hardware platform two level co design framework transputer based data driven reconfigurable accelerators parallelizing compilation environment code x simultaneous programming hardware software co design strategies profiling driven host accelerator partitioning performance optimization resource driven sequential structural partitioning accelerator source code reconfigurable resource utilization optimization application development methodology;two level co design framework;development systems;software tools;source code;space technology;field programmable gate arrays;acceleration hardware field programmable analog arrays software tools space technology logic programming field programmable gate arrays reconfigurable logic computer applications parallel processing;accelerator source code;performance optimization;hardware software co design strategies;parallel processing;hardware	The paper presents the parallelizing compilation environment CoDe-X for simultaneous programming of Xputer-based accelerators and their host. The paper introduces its hardware/software co-design strategies at two levels of partitioning. CoDe-X performs both, at first level a profiling-driven host/accelerator partitioning for performance optimization, and at second level a resourcedriven sequential/structural partitioning of the accelerator source code to optimize the utilization of its reconfigurable resources. To stress the significance of this application development methodology, the paper first gives an introduction to the underlying hardware platform. 1. Software-only Accelerator Implementation Very often hardware and software are alternatives to implement the same algorithm. Hardware is the structural implementation, whereas software results in a procedural implementation. We may distinguish two different worlds of computation: computation in space (structural), and, computation in time (procedural). At the area of systolic arrays, also called ASAPs (application-specific array processors), both worlds overlap. ASAP synthesis methods use time and space within the same formula, where mappings are links between both worlds: an emerging dual new computing science. Hardware has become soft. Emanating from the technology of field-programmable logic (FPL) the new paradigm of structural programming has evolved. So we now have two programming paradigms: programming in time and programming in space. Now it is time to become aware of the really existing two kinds of software: ● procedural software (code downloaded to RAM) ● structural software (downloaded to hidden RAM) Tools like XACT and others are the code generators for structural software. The R&D area of Custom Computing Machines (CCMs: [1], [2], [4]), such as FPGA-based CCMs (FCCMs [5] [6]), merge both kinds of software to an integrated methodology to speed-up performance. We have obtained a dual software-only implementation: procedural software running on the host, together with structural software running on the reconfigurable accelerator. Implementing CCMs has a lot in common with hardware/ software co-design to optimize hardware/software trade-off and to reduce total design time [7], [8], [9]. The main difference is the target platform: the structural program is frozen into ASICs or other hardware structures. Instead of structural programming we call this hardware synthesis. With the FCCM approach implementations on a von Neumann host are accelerated by using add-on fieldprogrammable circuits like FPGAs, to be structurally programmed. But currently this ”structural programming“ usually requires hardware experts, since contemporary FPGAbased accelerator architectures are far from being general purpose platforms. FPGAs currently available commercially have several severe draw-backs: ● by far too area-inefficient ● designed for random logic only FPAA: reconfigurable ALU Array FPGA: reconfigurable Gate Array Figure 1. Toward field-programmable ALU Arrays. rALU reconfigurable ALU reconfigurable interconnect •• bus Figure 2. Illustrating the Kress Array Principles. A Two-level Co-Design Framework for Xputer-based data-driven reconfigurable Accelerators Reiner W. Hartenstein, Jürgen Becker Universitaet Kaiserslautern Erwin-Schroedinger-Straße, D-67663 Kaiserslautern, Germany Fax: ++49 631 205 2640, e-mail: hartenst@rhrk.uni-kl.de URL: http://xputers.informatik.uni-kl.de/ 1060-3425/97 $10.00 (c) 1997 IEEE Proceedings of The Thirtieth Annual Hawwaii International Conference on System Sciences ISBN 0-8186-7862-3/97 $17.00 © 1997 IEEE ● too fine grain and too slow ● poor cost/performance ratio for highly computing-intensive applications The problem is the wide variety of architectures urged by optimization needs which stem from these draw-backs. Neither in CCMs nor in hardware/software co-design a common model is available. The von Neumann paradigm does not efficiently support ”soft“ hardware because of its tight coupling between instruction sequencer and ALU [10]. You need a new instruction sequencer, as soon as the data path is changed by structural programming: the architecture falls apart. 2. The Kress ALU Array A new kind of structurally programmable technology platform is needed. But also a new paradigm is needed like the one of Xputers (figure 7), which conveniently supports soft ALUs like in the rALU Array concept (reconfigurable ALU Array) [11]. For more about Xputers see section 3. To support highly computing-intensive applications we need structurally programmable platforms providing word level parallelism instead of the bit level parallelism of FPGAs. We need FPAAs (field-programmable ALU Arrays) instead of FPGAs (figure 1). For area efficiency this new platform should be suitable for full custom design, like known from ASAPs operating in SIMD mode. But ASAPs are not structurally programmable and support only problems with completely regular data dependencies. What we need is a platform as dense as ASAPs, but structurally programmable. For flexibility to implement applications with highly irregular data dependencies, each PE (processing element) should be programmable individually. Each PE should be a reconfigurable ALU (rALU). Also the local interconnect between particular PEs or rALUs should be programmable individually. A good solution is the Kress Array, illustrated by figure 2. It permits to map highly irregular applications onto a regularly structured hardware platform. Figure 5 shows a mapping example. Eight equations expressed in C language (figure 5 a) are mapped onto the Kress Array in figure 2 by the DPSS subsystem (figure 5 b). The result of this structural programming effort is the configured data path within the Kress ALU Array (figure 5 c: only internal data paths shown). DPSS (Data Path Synthesis System [12], [13]) is a simulated annealing optimizer, which carries out placement and routing [15]. This example also illustrates part of the role of the CoDeX application compiler introduced later in this paper. The Kress Array instruction level parallelism has following features. ● transparently scalable ● fast routing and placement (seconds only) ● dynamic partial reconfiguration (microseconds) ● suitable for full custom design ● much higher acceleration than by caches ● fast and low power by full custom design IF Kress IF Local Data Memory X pu te r M od ul e SW reconf. Data Sequencer ALU DS GAGs CPU Host	add-ons for firefox;algorithm;application-specific integrated circuit;arithmetic logic unit;automatic parallelization;bit-level parallelism;central processing unit;compiler;computation;computer science;data dependency;donald becker;email;emoticon;fax;field-programmability;field-programmable analog array;field-programmable gate array;full custom;functional programming;instruction-level parallelism;international standard book number;mathematical optimization;microsequencer;parallel computing;place and route;procedural programming;programmable logic device;programming paradigm;random logic;random-access memory;reconfigurable computing;routing;simd;scalability;simulated annealing;systolic array;von neumann architecture;xputer	Reiner W. Hartenstein;Jürgen Becker	1997		10.1109/HICSS.1997.663167	acceleration;parallel processing;computer architecture;parallel computing;real-time computing;systolic array;computer science;space technology;programming paradigm;computer applications;high-level synthesis;programming language;rapid application development;logic programming;field-programmable gate array;source code	Arch	0.45850867718052185	47.20918167713611	140697
4a65599c8497f0f47574d96149c77aba21cb9f33	component interconnect and data access interface for embedded vision applications		IP-based design is used to tackle complexity and reduce time-to-market in systems-on-chip with high-performance requirements. Component integration, the main part in this process, is a complicated and time-consuming task, largely due to interfacing issues. Standard interfaces can help to reduce the integration efforts. However, existing implementations use more resources than necessary and lack of a formalism to capture and manipulate resource requirements and design constraints. In this paper, we propose a novel interface, the Component Interconnect and Data Access (CIDA), and its implementation, based on the interface automata formalism. CIDA can be used to capture system-on-chip architecture, with primarily focus on video processing applications, which are mostly based on data streaming paradigm, with occasional direct memory accesses. We introduce the notion of component-interface clustering for resource reduction and provide a method to automatize this process. With real-life video processing applications implemented in FPGA, we show that our approach can reduce the resource usage (#slices) by an average of 20 % and reduce power consumption by 5 % compared to implementation based on vendor interfaces.	algorithm;architecture as topic;automata theory;automaton;cluster analysis;data access;embedded system;embedding;experiment;field-programmable gate array;formal system;harris affine region detector;interface device component;mathematical optimization;optimization problem;programming paradigm;real life;requirement;semantics (computer science);system on a chip;video processing;statistical cluster	Michael Mefenza;Franck Yonga;Christophe Bobda	2015	Journal of Real-Time Image Processing	10.1007/s11554-015-0515-5	embedded system;real-time computing;computer science;theoretical computer science	EDA	1.1822740747167724	54.56800184039143	141042
b7ab3c25c6392af5fdaa26da9e0b3c05f2786aba	fpgas, gpus and the ps2 - a single programming methodology	field programmable gate array;paper;program compilers coprocessors field programmable gate arrays monte carlo methods;hardware accelerator;a stream compiler;fpga;coprocessors;monte carlo simulation single programming methodology field programmable gate arrays graphics processing units sony playstation 2 vector units a stream compiler;single programming methodology;field programmable gate arrays graphics coprocessors hardware acceleration computer architecture programming profession throughput object oriented modeling runtime;graphics processing units;nvidia;nvidia geforce 6800 ultra;computer science;field programmable gate arrays;sony playstation 2 vector units;program compilers;monte carlo simulation;monte carlo methods	Field programmable gate arrays (FPGAs), graphics processing units (GPUs) and Sony's Playstation 2 vector units offer scope for hardware acceleration of applications. Implementing algorithms on multiple architectures can be a long and complicated process. We demonstrate an approach to compiling for FPGAs, GPUs and PS2 vector units using a unified description based on A Stream Compiler (ASC) for FPGAs. As an example of its use we implement a Monte Carlo simulation using ASC. The unified description allows us to evaluate optimisations for specific architectures on top of a single base description, saving time and effort	algorithm;compiler;computer graphics;field-programmable gate array;graphics processing unit;hardware acceleration;monte carlo method;simulation;ti advanced scientific computer	Lee W. Howes;Paul Price;Oskar Mencer;Olav Beckmann	2006	2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2006.42	embedded system;computer architecture;parallel computing;computer hardware;computer science;operating system;field-programmable gate array	Arch	0.4296820675760423	47.03580086859855	141129
3bcb28e6678725985a8fe0cc434f5e829a037ac8	reconfigurable baseband processing architecture for communication	multicore architectures reconfigurable baseband processing architecture communication standards baseband signal processing heterogeneous reconfigurable execution units homogeneous control mechanism software tools hardware generation algorithm mapping high level language hardware resource usage system performance evaluation finite impulse response filter fast fourier transform digital signal processor chips;outil logiciel;traitement signal;evaluation performance;software tool;heterogeneous reconfigurable execution units;multicore architectures;performance evaluation;integrated circuit;filtre reponse impulsion finie;flexibilidad;reconfigurable architectures;banda base;bande base;evaluacion prestacion;finite impulse response filter;langage evolue;circuito integrado;service telecommunication;carta de datos;digital filter;homogeneous control mechanism;transformacion fourier rapida;base band;fast fourier transform;algorithme;digital signal processor chips;algorithm;filtro respuesta impulsion acabada;hardware resource usage;filtro numerico;mappage;signal processing;baseband signal processing;communication standards;reconfigurable baseband processing architecture;telecommunication services;software tools digital signal processing chips fast fourier transforms fir filters performance evaluation reconfigurable architectures;digital signal processor;fast fourier transforms;digital signal processing chips;flexibilite;lenguaje evolucionado;hardware generation;software tools;mapping;processeur signal numerique;fir filters;procesador senal numerica;algorithm mapping;herramienta software;transformation fourier rapide;procesamiento senal;high level language;architecture reconfigurable;system performance evaluation;circuit integre;flexibility;fast fourier transformation;filtre numerique;algoritmo	The development of multiple communication standards and services has created the need for a flexible and efficient computational platform for baseband signal processing. Using a set of heterogeneous reconfigurable execution units (RCEUS) and a homogeneous control mechanism, the proposed reconfigurable architecture achieves a large computational capability while still providing a high degree of flexibility. Software tools and a library of commonly used algorithms are also proposed in this paper to provide a convenient framework for hardware generation and algorithm mapping. In this way, the architecture can be specified in a high-level language and it also provides increased hardware resource usage. Finally, we evaluate the system’s performance on representative algorithms, specifically a 32-tap finite impulse response (FIR) filter and a 256-point fast Fourier transform (FFT), and compare them with commercial digital signal processor (DSP) chips as well as with other reconfigurable and multi-core architectures.	algorithm;baseband;digital signal processor;execution unit;fast fourier transform;finite impulse response;high- and low-level;high-level programming language;multi-core processor;programming tool;signal processing	Wenqing Lu;Shuang Zhao;Xiaofang Zhou;J. Y. Ren;Gerald E. Sobelman	2011	IET Computers & Digital Techniques	10.1049/iet-cdt.2009.0121	embedded system;fast fourier transform;electronic engineering;parallel computing;computer science;operating system;signal processing;algorithm	Arch	3.6836800789012996	47.390308926598806	141161
56a5d9f9cf02acb245f03345b38a2938374e8cc0	carmel-2: a second generation vlsi architecture for flat concurrent prolog	lessons learned;optimal design;high performance;vlsi architecture	CARMEL-2 is a high performance VLSI uniprocessor, tuned forFlat Concurrent Prolog (FCP). CARMEL-2 shows almost 5-fold speedup over its predecessor, CARMEL-1, and it achieves 2,400 KLIPS executingappend. This high execution rate was gained as a result of an optimized design, based on an extensive architecture-oriented execution analysis of FCP, and the lessons learned with CARMEL-1. CARMEL-2 is a RISC processor in its character and performance. The instruction set includes only 29 carefully selected instructions. The 10 special instructions, the prudent implementation and pipeline scheme, as well as sophisticated mechanisms such as intelligent dereference, distinguish CARMEL-2 as a RISC processor for FCP.	fibre channel protocol;front controller;logic programming;prolog;speedup;uniprocessor system	Arie Harsat;Ran Ginosar	1988	New Generation Computing	10.1007/BF03037206	computer architecture;parallel computing;real-time computing;computer science;optimal design;programming language	Arch	0.8147843408897374	47.75612174793772	141272
d5a15c0c24670a537eeebd2e84d1922010859a80	specification and formal verification of power gating in processors	formal specification;correspondence checking method formal verification power gating feature instruction set architecture model processor model equivalence checking symbolic simulation property checking mips processor verification time reduction;formal verification processor verification power gating verification;program processors registers formal verification load modeling logic gates power demand latches;formal verification;microprocessor chips formal specification formal verification instruction sets;microprocessor chips;instruction sets	This paper presents a method for specification as well as efficient formal verification of power gating feature of processors. Given an instruction-set architecture model of a processor, as the golden model, and a detailed processor model with power gating feature, we propose an efficient method for equivalence checking of the two models using symbolic simulation and property checking. Our experimental results on a MIPS processor shows that our method reduces the verification time compared to the correspondence checking method at least by 3.4x.	central processing unit;formal equivalence checking;formal verification;power gating;symbolic simulation;turing completeness	Amir Masoud Gharehbaghi;Masahiro Fujita	2014	Fifteenth International Symposium on Quality Electronic Design	10.1109/ISQED.2014.6783382	computer architecture;parallel computing;real-time computing;formal methods;formal verification;software verification;computer science;instruction set;formal specification;formal equivalence checking;high-level verification;runtime verification;intelligent verification;functional verification	Arch	7.745443925139195	52.49351129534134	141335
2058c3bf1debcccbfaf5a6c29f7a18d7a820ca5c	an adaptive chip-multiprocessor architecture for future mobile terminals	energy aware scheduling;power saving;chip multiprocessor;power consump tion;datorsystem;computer systems;computer architecture;chip multiprocessor cmp;power consumption;mobile systems;simulation model;mobile terminals;mobile terminal;software implementation	Power consumption has become an increasingly important factor in the field of computer architecture. It affects issues such as heat dissipation and packaging cost, which in turn affects the design and cost of a mobile terminal. Today, a lot of effort is put into the design of architectures and software implementation to increase performance. However, little is done on a system level to minimize power consumption, which is crucial in mobile systems.We propose an adaptive chip-multiprocessor (CMP) architecture, where the number of active processors is dynamically adjusted to the current workload need in order to save energy while preserving performance. The architecture is suitable in future mobile terminals where we anticipate a bursty and performance demanding workload.We have carried out an evaluation of the performance and power consumption of the proposed architecture using previously validated high-level simulation models. Our experiments show that orders of magnitude in power consumption can be saved compared to a conventional architecture to a negligable performance cost. The method used is complementary to other power saving techniques such as voltage and frequency scaling.	central processing unit;computer architecture;dynamic frequency scaling;dynamic voltage scaling;experiment;high- and low-level;image scaling;mobile phone;multi-core processor;multiprocessing;simulation;thermal management (electronics)	Mladen Nikitovic;Mats Brorsson	2002		10.1145/581630.581638	embedded system;parallel computing;real-time computing;simulation;telecommunications;computer science;operating system;simulation modeling	Arch	-3.7265873673438343	55.66613292835654	141343
031170f40678626970c74fa5ba5183d784403e72	processor architecture exploration and synthesis of massively parallel multi-processor accelerators in application to ldpc decoding	processors micro macro architecture design;massively parallel mpsocs;4g communication systems;design space exploration;ldpc decoding;highly demanding applications;eda tools	Numerous modern applications in various fields, such as communication and networking, multimedia, encryption, etc., impose extremely high demands regarding performance while at the same time requiring low energy consumption, low cost, and short design time. Often these very high demands cannot be satisfied by application implementations on programmable processors. Massively parallel multi-processor hardware accelerators are necessary to adequately serve these applications. The accelerator design for such applications has to decide both the micro-architectures of particular processors and the multi-processor system macro-architecture. Due to complex tradeoffs between the micro-architectures and macro-architectures, the micro- and macro-architecture design has to be performed in combination and not in separation, as with the state-of-the-art design methods and tools. To ensure effective and efficient application implementations, an adequate design space exploration (DSE) is necessary. It has to construct and analyze several most promising micro- and macro-architecture combinations and to select the best of them. In this paper, we will show that the lack of such a design space exploration would not only make it very difficult to satisfy the ultra-high performance demands of such applications, but it would also seriously degrade the accelerator quality in other design dimensions. To adequately design the multi-processor accelerators for highly-demanding applications, we proposed a quality-driven model-based design method. This paper is devoted to the processor architecture exploration and synthesis of the heterogeneous multi-processor system being one of the most important aspects of our method. The method is implemented in our automatic DSE tool. Using our DSE tool and the LDPC decoding application as a case study, we performed an extensive experimental research of automatic synthesis of various hardware multi-processors for LDPC decoding to show various complex issues and tradeoffs in the processor architecture design, and to demonstrate the high quality of our method and DSE tool in relation to this aspect.	low-density parity-check code;microarchitecture;multiprocessing	Yahya Jan;Lech Józwiak	2014	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2013.11.007	embedded system;computer architecture;parallel computing;real-time computing;electronic design automation;computer science;operating system	EDA	2.297753578279841	48.91188658131256	141429
ecf6e2a84b326a8524371fdbc1e8f41b02572364	a functional test algorithm for the register forwarding and pipeline interlocking unit in pipelined microprocessors	power architecture functional test algorithm register forwarding pipeline interlocking unit pipelined microprocessors data hazard data corruption performance penalty data hazard handling microprocessor control logic functional approach specific test algorithm low functional testability rf pi logic software based self test sbst test responses operational phase academic mips like processor industrial system on chip;pipelines;decision support systems;data handling;sbst test processors register forwarding;microprocessor chips;pipelines data handling microprocessor chips	When the result of a previous instruction is needed in the pipeline before it is available, a “data hazard” occurs. Register Forwarding and Pipeline Interlock (RF&PI) are mechanisms suitable to avoid data corruption and to limit the performance penalty caused by data hazards in pipelined microprocessors. Data hazards handling is part of the microprocessor control logic; its test can hardly be achieved with a functional approach, unless a specific test algorithm is adopted. In this paper we analyze the causes for the low functional testability of the RF&PI logic and propose some techniques able to effectively perform its test. In particular, we describe a strategy to perform Software-Based Self-Test (SBST) on the RF&PI unit. The general structure of the unit is analyzed, a suitable test algorithm is proposed and the strategy to observe the test responses is explained. The method can be exploited for test both at the end of manufacturing and in the operational phase. Feasibility and effectiveness of the proposed approach are demonstrated on both an academic MIPS-like processor and an industrial System-on-Chip based on the Power ArchitectureTM.	algorithm;functional approach;functional testing;hazard (computer architecture);interlock (engineering);microcontroller;microprocessor;system on a chip;uninterruptible power supply	Paolo Bernardi;D. Boyang;Lyl M. Ciganda Brasca;Ernesto Sánchez;Matteo Sonza Reorda;Michelangelo Grosso;Oscar Ballan	2013	2013 8th IEEE Design and Test Symposium	10.1109/IDT.2013.6727120	operand forwarding;embedded system;electronic engineering;parallel computing;real-time computing;decision support system;computer science;operating system;machine learning;group method of data handling;test compression;pipeline transport	EDA	6.305608029806245	57.24583454386687	141547
4f1b4d8c286394674ab558878d60ac0328e1232c	tailoring graph-coloring register allocation for runtime compilation	optimising compilers;register allocation;resource allocation;graph coloring;optimizing compilers runtime environment design optimization java virtual machining algorithm design and analysis profitability program processors virtual manufacturing costs;allocative efficiency;just in time compiler;resource allocation optimising compilers graph colouring;graph coloring register allocator graph coloring register allocation tailoring runtime compilation just in time compilers compile time intensive optimization algorithms;graph colouring	Just-in-time compilers are invoked during application execution and therefore need to ensure fast compilation times. Consequently, runtime compiler designers are averse to implementing compile-time intensive optimization algorithms. Instead, they tend to select faster but less effective transformations. In this paper, we explore this trade-off for an important optimization - global register allocation. We present a graph-coloring register allocator that has been redesigned for runtime compilation. Compared to Chaitin- Briggs [7], a standard graph-coloring technique, the reformulated algorithm requires considerably less allocation time and produces allocations that are only marginally worse than those of Chaitin-Briggs. Our experimental results indicate that the allocator performs better than the linear-scan and Chaitin-Briggs allocators on most benchmarks in a runtime compilation environment. By increasing allocation efficiency and preserving optimization quality, the presented algorithm increases the suitability and profitability of a graph-coloring register allocation strategy for a runtime compiler.	algorithm;compile time;compiler;graph coloring;just-in-time compilation;mathematical optimization;register allocation	Keith D. Cooper;Anshuman Dasgupta	2006	International Symposium on Code Generation and Optimization (CGO'06)	10.1109/CGO.2006.35	allocative efficiency;parallel computing;real-time computing;resource allocation;computer science;operating system;just-in-time compilation;graph coloring;distributed computing;programming language;register allocation	PL	-3.833989961128507	52.01676491853311	141717
b9eb076df67e2396d5b8f9c237dbb8c5c46669ea	toward dynamic precision scaling		This article makes the case for dynamic precision scaling to improve power efficiency by tailoring arithmetic precision adaptively to temporal changes in algorithmic noise tolerance throughout the execution.	image scaling;performance per watt;significant figures	Serif Yesil;Ismail Akturk;Ulya R. Karpuzcu	2018	IEEE Micro	10.1109/MM.2018.043191123	parallel computing;floating point;approximation algorithm;electrical efficiency;approximate computing;scaling;benchmark (computing);computer science	Embedded	-4.156618946046098	55.17984345235129	141783
f14bfa09fccd8c47e4098b4d2eca97a9d4800de6	dynamic reconfigurable bus encoding scheme for reducing the energy consumption of deep sub-micron instruction bus	encoding;capacitance;embedded system;embedded systems;decoding;statistics	In very deep sub-micron designs, cross coupling capacitances become the dominant factor of the total bus loading and have a significant impact on the energy consumption. In this paper, we propose a re-configurable bus encoding scheme, which is based on the correlation among the bit lines, to reduce the energy consumption of the cross coupling capacitances of the instruction buses. The instruction is encoded by reordering the bit lines during compilation time to reduce the total cross coupling switching. A crossbar is used as a decoder to map back the data to the original instruction codeword before sending to the instruction decoder. The reordering can be reconfigured during run-time by using different configurations in the crossbar. Experimental results show that by using the proposed scheme, significant energy reduction, on average about 20%, can be achieved. Comparisons with existing fixed bit lines reordering encoding scheme have also been made and on average more than 15% reduction can be obtained using our method.	bus encoding;central processing unit;code word;compiler;crossbar switch;line code;run time (program lifecycle phase)	Siu-Kei Wong;Chi-Ying Tsui	2004	2004 IEEE International Symposium on Circuits and Systems (IEEE Cat. No.04CH37512)		embedded system;electronic engineering;parallel computing;real-time computing;computer science;capacitance;encoding	EDA	-2.0870299854776695	52.42307919346466	141904
d7782519a976b6aa5f37a4a56854b7728ca0e8d7	time and energy efficient matrix factorization using fpgas	block design;field programmable gate array;diseno circuito;matrix factorization;energy efficient;reconfigurable architectures;circuit design;energy dissipation;red puerta programable;reseau porte programmable;plan bloc;factorisation matricielle;factorizacion lu;plan bloque;conception circuit;factorizacion matricial;factorisation lu;architecture reconfigurable;lu factorization	In this paper, new algorithms and architectures for matrix factorization are presented. Two fully-parallel and block-based designs for LU decomposition on configurable devices are proposed. A linear array architecture is employed to minimize the usage of long interconnects, leading to lower energy dissipation. The designs are made scalable by using a fixed I/O bandwidth independent of the problem size. High level models for energy profiling are built and the energy performance of many possible designs is predicted. Through the analysis of design tradeoffs, the block size that minimizes the total energy dissipation is identified. A set of candidate designs was implemented on the Xilinx Virtex-II to verify the estimates. Also, the performance of our designs is compared with that of state-of-the-art DSP based designs and with the performance of designs obtained using a state-of-the-art commercial compilation tool such as Celoxica DK1. Our designs on the FPGAs are significantly more time and energy efficient in both cases.	algorithm;american and british english spelling differences;analysis of algorithms;block size (cryptography);charge-coupled device;compiler;digital signal processor;dynamic frequency scaling;dynamic voltage scaling;electrical connection;field-programmable gate array;image scaling;input/output;lu decomposition;scalability;simulation;vhdl;virtex (fpga);xilinx ise	Seonil B. Choi;Viktor K. Prasanna	2003		10.1007/978-3-540-45234-8_50	embedded system;block design;parallel computing;lu decomposition;computer science;dissipation;theoretical computer science;circuit design;efficient energy use;matrix decomposition;field-programmable gate array	EDA	-0.49797799262690146	53.236073450859294	141914
1a0f56439b7be926420b59d505dfa6135b28da51	system level online power management algorithms	on chip bus;online algorithm;experimental analysis;cache;intellectual property;software systems;analysis of algorithm;system on a chip;data distribution;embedded system;embedded systems;adaptive algorithm;low power;estimation;power dissipation;adaptive method;computer power supplies low power electronics embedded systems vlsi;low power electronics;power management;vlsi;vlsi system level online power management power management algorithms embedded system power dissipation algorithmic efficiency competitive analysis adaptive algorithms input data distribution disk drive power critical subsystems;competitive analysis;energy management power system management algorithm design and analysis embedded system power dissipation turning adaptive algorithm software algorithms hardware software systems;lower bound;computer power supplies;historical data	The problem of power management for an embedded system is to reduce system level power dissipation by shutting o parts of the system when they are not being used and turning them back on when they are required. Algorithms for this problem are online in nature where the algorithm must operate without access to the complete data set or its characteristics. In this paper, we present online algorithms to manage power for embedded systems and provide experimental analysis to back up the theoretical results. Speci cally, this paper makes four contributions. We propose an optimal online algorithm for power management. We present an analysis of algorithmic e ciency using a technique called competitive analysis which is particularly suitable for online algorithms. Using this analysis technique, we develop a lower bound for the non-adaptive version of the power management problem and show that our algorithm achieves this lower bound. Next, we explore adaptive algorithms that try to shut down the system based on historical data. We provide a lower bound for any algorithm that uses adaptive methods to manage power. We also propose an algorithm that is independent of the input data distribution, practical and usable in both hardware and software systems with guaranteed performance. Finally, we compare these algorithms with previously proposed heuristics both theoretically and experimentally. For the experiments, we model the disk drive of a laptop computer as an embedded system. The results show that the proposed algorithms perform well in practice with guaranteed bounds on their performance. Further, this paper conclusively demonstrates that to implement aggressive power management techniques for power critical subsystems, designers will have to commit greater resources such as dedicated registers and ALU units.	adaptive algorithm;arithmetic logic unit;backup;competitive analysis (online algorithm);disk storage;embedded system;experiment;heuristic (computer science);laptop;online algorithm;power management;software system	Dinesh Ramanathan;Rajesh K. Gupta	2000		10.1145/343647.343867	system on a chip;competitive analysis;embedded system;online algorithm;estimation;electronic engineering;parallel computing;real-time computing;telecommunications;cache;computer science;electrical engineering;dissipation;operating system;very-large-scale integration;upper and lower bounds;algorithm;intellectual property;low-power electronics;experimental analysis of behavior;software system	EDA	-1.532415938976166	57.13721183948976	142014
abfbd63939b979fa95ea3ba9b253de8436a8afd8	the benefits of low operating voltage devices to the energy efficiency of parallel systems		Programmable circuits such as general-purpose processors or FPGAs have their end-user energy efficiency strongly dependent on the program that they execute. Ultimately, it is the programmer's ability to code and, in the case of general purpose processors, the compiler's ability to translate source code into a sequence of native instructions that make the circuit deliver the expected performance to the end user. This way, the benefits of energy-efficient circuits build upon energy-efficient devices could be obfuscated by poorly written software. Clearly, having well-written software running on conventional circuits is no better in terms of energy efficiency than having poorly written software running on energy-efficient circuits. Therefore, to get the most out of the energy-saving capabilities of programmable circuits that support low voltage operating modes, it is necessary to address software issues that might work against the benefits of operating in such modes.	central processing unit;compiler;field-programmable gate array;processor register;programmer	Samuel Xavier de Souza;Eduardo André Neves;Alex F. A. Furtunato;Luiz Felipe Q. Silveira;Kyriakos Georgiou;Kerstin I. Eder	2017	2017 Fifth Berkeley Symposium on Energy Efficient Electronic Systems & Steep Transistors Workshop (E3S)		distributed computing;field-programmable gate array;software;end user;computer science;compiler;real-time computing;parallel computing;electronic circuit;programmer;source code;embedded system;low voltage	Arch	-4.205196754746083	53.48879409381996	142407
b1ac415f80c9ea45c481351c35905f0466249c2d	marginal performance: formalizing and quantifying power over/under provisioning in noc dvfs		In network-on-chip (NoC) based CMPs, DVFS is commonly used to co-optimize performance and power. To achieve optimal efficiency, it is important to gain proportional performance growth with power. However, power over/under provisioning often exists. To properly evaluate and guide NoC DVFS techniques, it is highly desirable to formalize and quantify power over/under provisioning. In this paper, we first show that application performance does not grow linearly with network power in an NoC-based CMP. Instead, their relationship is non-linear and can be captured using performance-power characteristics curve (PPCC) with three distinct regions: an inertial region, a linear region, and a saturation region. We note that conventional DVFS metrics such as Performance Per Watt (PPW) cannot accurately evaluate such non-linear relationship. Based on PPCC, we propose a new figure of merit called Marginal Performance (MP) which evaluates the incremental performance per power increment after the inertial region. The MP concept enables to formally define power over- and under-provisioning with reference to the linear region in which an efficient NoC DVFS should operate. Applying the PPCC and MP concepts in full-system simulations with PARSEC and SPEC OMP2012 benchmarks, we are able to identify power over/under provisioning occurrences, measure and compare their statistics in two latest NoC DVFS techniques. Moreover, we show evidences that MP can accurately and consistently evaluate the NoC DVFS techniques, avoiding the misjudgement and inconsistency of PPW-based evaluations.	dynamic voltage scaling;marginal model;network on a chip;nonlinear system;parsec;performance per watt;provisioning;simulation;spec#	Zhonghai Lu;Yuan Yao	2017	IEEE Transactions on Computers	10.1109/TC.2017.2715018	parallel computing;real-time computing;computer science;performance per watt;network on a chip;provisioning;figure of merit;benchmark (computing)	Arch	-4.367617683138291	56.047679104567834	142462
e56531638fedbc5089532e6cda5b63f0acd7b676	fastchart-a fast time deterministic cpu and hardware based real-time-kernel	deterministic behaviour;deterministic execution time;kernel;forth machine;fastchart;hardware based real time kernel;hardware structure;real time;reduced instruction set computing;delay effects;fast time deterministic cpu;computer architecture;realtime system;hardware central processing unit switches timing statistics real time systems delay effects kernel pipelines operating systems;pipelines;statistics;reduced instruction set computing computer architecture operating systems computers real time systems;switches;hard realtime systems;operating systems computers;hardware structure deterministic execution time forth machine fastchart fast time deterministic cpu hardware based real time kernel hard realtime systems deterministic behaviour timing;central processing unit;operating systems;hardware;real time systems;timing	The designer of hard realtime systems requires deterministic behaviour of the system. Today there are problems because of the hardware and the real-time kernel. So one gets only statistic statements regarding timing. The article describes a new hardware structure that is deterministic, fast and includes a real-time kernel in hardware. But this structure is limited to small real-time systems. >	central processing unit;kernel (operating system);real-time clock	Lennart Lindh	1991		10.1109/EMWRT.1991.144077	hardware compatibility list;embedded system;reduced instruction set computing;parallel computing;kernel;real-time computing;network switch;computer science;operating system;central processing unit;pipeline transport	Theory	-3.657603635954485	51.48913712008348	142564
ba4086088b94ed322c79bf08f10379db3b8901f5	mapping statechart models onto an fpga-based asip architecture		In this paper, we describe a system to map hardwaresoftware systems specified with statechart models on an ASIP architecture based on FPGAs. The architecture consists of a reusable CPU core with enhancements to execute the behavior of statecharts correctly. Our codesign system generates an application-specific hardware control block, an application-specific set of registers, and an instruction stream. The instruction stream consists of a static set of core instructions, and a set of custom instructions for performance enhancements. In contrast to previous approaches, the presented method supports extended statecharts. The system also assists designers during space/time tradeoff optimizations. The benefits of the approach are demonstrated with an industrial control application comparing two different timing schemes.	application-specific instruction set processor;central processing unit;field-programmable gate array;state diagram	Christian Veith;Klaus Buchenrieder;Andreas Pyttel	1996			embedded system;computer architecture;parallel computing;kernel;real-time computing;application-specific instruction-set processor;computer science;operating system;space time;instruction set;process control;application-specific integrated circuit;programming language;field-programmable gate array;software system	EDA	2.987922730905481	52.084379560071554	142573
ad4ce24ee41d982cca5a2fdf7bba852ebc86af84	asir: application-specific instruction-set router for noc-based mpsocs		The end of Dennard scaling led to the use of heterogeneous multi-processor systems-on-chip (MPSoCs). Heterogeneous MPSoCs provide a high efficiency in terms of energy and performance due to the fact that each processing element can be optimized for an application task. However, the evolution of MPSoCs shows a growing number of processing elements (PEs), which leads to tremendous communication costs, tending to become the performance bottleneck. Networks-on-chip (NoCs) are a promising and scalable intra-chip communication technology for MPSoCs. However, these technological advances require novel and effective programming methodologies to efficiently exploit them. This work presents a novel router architecture called application-specific instruction-set router (ASIR) for field-programmable-gate-arrays (FPGA)-based MPSoCs. It combines data transfers with application-specific processing by adding high-level synthesized processing units to routers of the NoC. The execution of application-specific operations during data exchange between PEs exploits efficiently the transmission time. Furthermore, the processing units can be programmed in C/C++ using high-level synthesis, and accordingly, they can be specifically optimized for an application. This approach enables transferred data to be processed by a processing element, such as a MicroBlaze processor, before the transmission or by a router during the transmission. Moreover, a static mapping algorithm for applications modeled by a Kahn process network-based graph is introduced that maps tasks to the MicroBlaze processors and processing units. The mapping algorithm optimizes the communication cost by allocating tasks to nearest neighboring PEs. This complete methodology significantly simplifies the design and programming of ASIR-based MPSoCs. Furthermore, it efficiently exploits the heterogeneity of processing capabilities inside the routers and MicroBlaze processors.	algorithm;c++;central processing unit;dennard scaling;field-programmability;field-programmable gate array;high- and low-level;high-level synthesis;image scaling;kahn process networks;microprocessor;multiprocessing;network on a chip;router (computing);scalability;system on a chip	Jens Rettkowski;Diana Göhringer	2018	Computers	10.3390/computers7030038	parallel computing;mpsoc;architecture;scalability;high-level synthesis;network on a chip;dennard scaling;microblaze;router;computer science	Arch	-0.4220288623718417	50.01703453464756	142736
593934eb743149fa7b0e7ae917e8dfdc32d90105	hw/sw partitioning for region-based dynamic partial reconfigurable fpgas	software engineering field programmable gate arrays integer programming linear programming;time step based approach hw sw partitioning region based dynamic partial reconfigurable fpgas dynamic partial reconfigurable capability software hardware partitioning approaches reconfiguration optimization advanced region based dpr design flow task graph representation mixed integer linear programming milp;hardware field programmable gate arrays schedules prefetching delays partitioning algorithms	With dynamic partial reconfigurable (DPR) capability, an FPGA fabric is no longer static; some of its regions can be dynamically reused for different tasks. Hence current software/hardware (HW/SW) partitioning approaches are no longer applicable to such reconfigurable hardware. This paper incorporates reconfiguration optimization into HW/SW partitioning targeting advanced region-based DPR design flow. Instead of analyzing the tasks for each time step, we introduce a new task graph representation with additional reconfiguration nodes so that the HW/SW partitioning for region-based DPR designs can be optimally solved efficiently by a novel mixed-integer linear programming (MILP) formulation. Experimental results show that our approach can obtain the optimal solutions 10 times faster than the current time-step based approach. For large designs which cannot be handled by the current approaches, our approach can achieve the optimum efficiently in reasonable runtime.	cluster analysis;field-programmable gate array;graph (abstract data type);high-level programming language;high-level synthesis;integer programming;linear programming formulation;mathematical optimization;overhead (computing);real-time clock;reconfigurable computing;run time (program lifecycle phase);shattered world;transform, clipping, and lighting	Yuchun Ma;Jinglan Liu;Chao Zhang;Wayne Luk	2014	2014 IEEE 32nd International Conference on Computer Design (ICCD)	10.1109/ICCD.2014.6974721	computer architecture;parallel computing;real-time computing;reconfigurable computing;computer science	EDA	-0.07696565367765906	52.12162202939092	142737
695b67a1333b2583719dd1c1d8234a9c11380c72	reliability of a softcore processor in a commercial sram-based fpga	mean time to failure;mttf;muitf;single event upset;avf;fault injection;softcore processors	Softcore processors are an attractive alternative to using radiation-hardened processors in space-based applications. Unlike traditional processors however, the logic and routing of a softcore processor are vulnerable to the effects of single-event upsets (SEUs). This paper applies two common SEU mitigation techniques, TMR with checkpointing and DWC with checkpointing, to the LEON3 softcore processor. The improvement in reliabilty over an unmitigated version of the processor is measured using three metrics: the architectural vulnerability factor (AVF), mean time to failure (MTTF), and mean useful instructions to failure (MuITF). Using configuration memory fault injection, we found that DWC with checkpointing improves the MTTF and MuITF by over 35x, and that TMR with triplicated input and outputs improves the MTTF and MITF by over 6000x.	application checkpointing;central processing unit;ddos mitigation;fault injection;field-programmable gate array;general protection fault;leon;mean time between failures;radiation hardening;routing;single event upset;static random-access memory;triple modular redundancy	Nathan Rollins;Michael J. Wirthlin	2012		10.1145/2145694.2145723	embedded system;parallel computing;real-time computing;mean time between failures;computer science;operating system	Arch	7.37615575699517	59.573541724134664	143192
480dedecf9c6524c95091d41487b218b2fcf2183	near-field communication transceiver system modeling and analysis using systemc/systemc-ams with the consideration of noise issues	gaussian random noise;mixed signal model;systemc ams;telecommunication computing;near field communication;c language;telecommunication computing c language near field communication radio transceivers;systemc ams failure rate gaussian random noise load modulation mixed signal model near field communication systemc;clocks voltage controlled oscillators antennas demodulation encoding phase locked loops;failure rate;load modulation;heterogeneous mixed signal system near field communication transceiver system modeling systemc systemc ams modeling c based hardware description language analog system level simulation near field communication system nfc transceiver system reader and card analog blocks digital blocks;radio transceivers;systemc	SystemC, as a C++-based hardware description language, is used for system architecture design, large digital hardware, software, and their interaction. Its extension, SystemC-AMS, provides the capability of abstract modeling to deliver analog system-level simulation of “real-time” application scenarios. SystemC and SystemC-AMS help designers to analyze a whole mixed-signal system and further guide the circuit design to reduce the design cost. This paper presents SystemC (2.2.0) and SystemC-AMS (1.0 Beta2) modeling of a near-field communication (NFC) system working in passive mode, based on the proximity contactless identification cards ISO/IEC 14443 international standard. The NFC transceiver system includes reader and card analog blocks, digital blocks, and antennas. Problems caused by realistic imperfections are considered, simulated, and then solved by modifying the design at a system level, which is significant to high-level modeling. Systematic simulation is given to prove SystemC/SystemC-AMS is an accurate and efficient tool to model a heterogeneous mixed-signal system in an early-design stage.	c++;circuit design;contactless smart card;digital electronics;hardware description language;high- and low-level;mixed-signal integrated circuit;near field communication;radio-frequency identification;real-time clock;system-level simulation;systemc ams;systems architecture;transceiver;vhdl-ams	Wei Li;Dian Zhou;Minghua Li;Binh P. Nguyen;Xuan Zeng	2013	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2012.2231443	embedded system;transaction-level modeling;electronic engineering;real-time computing;computer science;operating system;failure rate;near field communication;high-level synthesis	EDA	4.9183747415331664	52.906960551057644	143208
0ca33dbf89f3db64a65fcd3a88a385f9d956c600	architecture and synthesis issues in psoc	ioim;cpld;chip;logic synthesis;embedded system design;emi;vme	The technology and architecture of interconnections could be considered as the most important factor in determining the effectiveness of Microcontrollers, FPGA, PSoC (Program Silicon on chip), and ASIC etc. for a specific application. Due to their restricted connectivity, FPGAs represent a challenging technology for CAD tools developers. This paper presents theoretical and experimental aspects of the methodology for embedded system design with PSoC controllers. Also we will present an overview of PSoC architecture, available commercially and the approaches used in PSoC layout and logic synthesis. The presentation will cover these issues from both a user's prospective and CAD tools developers prospective.		Manoj Sankhe;Rajendra Kanphade	2010		10.1145/1741906.1742220	embedded system;computer architecture;psoc;engineering;computer engineering	EDA	9.460328313911603	51.81566285187008	143283
6a6cfadb4be52082f7d18cfb0bd7d2d2622571d1	design a hardware mechanism to utilize multiprocessors on a uni-processor operating system	design flow;fpga;embedded system;chip;operating system;multiprocessor architecture;software development tools;linux;inter processor communication;high speed;inter processor;xilinx ml310;multi core	The multiprocessor architecture for multimedia embedded systems becomes more popular, because of processor design and fabrication evolution. However the interprocessor communication is still an important problem in multiprocessor environments. In this paper, we propose a hardware interprocessor communication mechanism for a multi-core FPGA chip. Although the hardware/software develop tools do not support multi-core design in the target platform, we create a novel design flow to implement the multi-core under Linux with high speed communication mechanism. In the experiment results, the performance has at least 30% speedup when Dhrystone benchmark execute on the Xilinx ML310 platform which is redesign by our mechanism.	benchmark (computing);dhrystone;embedded system;field-programmable gate array;inter-process communication;linux;multi-core processor;multiprocessing;operating system;processor design;speedup	Slo-Li Chu;Chih-Chieh Hsiao;Pin-Hua Chiu	2009		10.1007/978-3-642-03095-6_79	chip;multi-core processor;computer architecture;parallel computing;real-time computing;computer science;design flow;operating system;linux kernel;field-programmable gate array	Arch	1.0528223627541984	49.43006923254589	143393
6164594701c47b1a6ae5cc1679ed8de13e99d2f7	socwire: a robust and fault tolerant network-on-chip approach for a dynamic reconfigurable system-on-chip in fpgas	socwire;dynamic reconfigurable system;fault tolerant;dynamic reconfiguration;network on chip;vmc;data processing;sytem on chip;channel capacity;system on chip;space missions;high performance;sram based fpga;space application	Individual Data Processing Units (DPUs) are commonly used for operational control and specific data processing of scientific space instruments. These instruments have to be suitable for the harsh space environment in terms of e.g. temperature and radiation. Thus they need to be robust and fault tolerant to achieve an adequate reliability. The Configurable System-on-Chip (SoC) solution based on FPGA has successfully demonstrated flexibility and reliability for scientific space applications like the Venus Express mission. Future space missions demand high-performance on board processing because of the discrepancy of extreme high data volume and low downlink channel capacity. Furthermore, in-flight reconfiguration ability and dynamic reconfigurable modules enhances the system with maintenance potential and at run-time adaptive functionality. To achieve these advanced design goals a flexible Network-on-Chip (NoC) is proposed for applications with high reliability, like space missions. The conditions for SRAM-based FPGA in space are outlined. Additionally, we present our newly developed NoC approach, System-on-Chip Wire (SoCWire) and outline its performance and suitability for robust dynamic reconfigurable systems.	field-programmable gate array;network on a chip;reconfigurable computing;system on a chip	Björn Osterloh;Harald Michalik;Björn Fiethe	2009		10.1007/978-3-642-00454-4_8	system on a chip;embedded system;fault tolerance;parallel computing;real-time computing;data processing;computer science;space exploration;channel capacity	EDA	4.608477771601613	57.59339924620429	143753
6a604b8c1ac40fb2b8f7597b7aea8cb13fb32381	a dual digital signal processor vme board for instrumentation and control applications	control application;digital signal processing;printed circuits;general and miscellaneous mathematics computing and information science;beam currents;continuous electron beam accelerator facility;computerized control systems;hardware architecture;fixed point;chip;modifications;texas instruments;open architecture;particle accelerators;cebaf accelerator;single channel;43 particle accelerators;analog to digital converters;digital signal processor;design;floating point;printed circuit board;beam monitors;99 general and miscellaneous mathematics computing and information science;signal conditioning	* This work was supported by the U.S. DOE Contract No. DE-AC05-84-ER40150 Abstract A Dual Digital Signal Processing VME Board was developed for the Continuous Electron Beam Accelerator Facility (CEBAF) Beam Current Monitor (BCM) system at Jefferson Lab. It is a versatile general-purpose digital signal processing board using an open architecture, which allows for adaptation to various applications. The base design uses two independent Texas Instrument (TI) TMS320C6711, which are 900 MFLOPS floating-point digital signal processors (DSP). Applications that require a fixed point DSP can be implemented by replacing the baseline DSP with the pin-for-pin compatible TMS320C6211. The design can be manufactured with a reduced chip set without redesigning the printed circuit board. For example it can be implemented as a single-channel DSP with no analog I/O.	asynchronous i/o;baseline (configuration management);central processing unit;chipset;digital signal processing;digital signal processor;electron;flops;fixed point (mathematics);general-purpose modeling;open architecture;pin compatibility;printed circuit board;printing;texas instruments tms320;vmebus	Hai Dong;Roger Flood;C. Hovater;J. Musson	2001	CoRR		embedded system;electronic engineering;computer hardware;computer science;engineering;electrical engineering;operating system;hardware architecture;printed circuit board	EDA	6.399779709042939	48.68826463369209	143950
995e81f3fce1d3075d854b2ef44956318f913c03	80960-next generation	branch prediction;microprocessor chips computer architecture;chip;computer architecture;next generation;it management;clocks read only memory decoding pipelines microarchitecture registers computer architecture microprocessors logic arrays radio frequency;functional unit;instructions per clock;high performance;embedded processor;microprocessor chips;performance features intel 80960 microprocessors next generation core embedded processor chips branch lookahead branch prediction parallelism multiple functional units resource scoreboarding microarchitecture	A discussion is presented of the next generation core for the 80960 family of embedded processor chips. It is shown that the next generation 960 core incorporates several features for high performance. It has wide and concurrent internal buses. It can decode and issue a sustained two instructions per clock even with loads and branches. It implements branch lookahead with branch prediction to minimize pipeline breaks. It manages the parallelism and multiple functional units by using resource scoreboarding techniques. It is a modular, high-performance core designed for the embedded processor market. The goals of the core design, the basic microarchitecture, the pipeline, and the key performance features are examined.<<ETX>>	branch predictor;bus (computing);embedded system;instructions per cycle;intel i960;microarchitecture;next-generation network;parallel computing;parsing;scoreboarding	Glenn Hinton	1989	Digest of Papers. COMPCON Spring 89. Thirty-Fourth IEEE Computer Society International Conference: Intellectual Leverage	10.1109/CMPCON.1989.301896	computer architecture;parallel computing;real-time computing;computer science	Arch	1.2495350937339678	48.21376781738982	144096
137f8767354f935bfb0756126a5612640ffd5db7	mapping and performance of dsp benchmarks on a medium-grain reconfigurable architecture		Reconfigurable hardware has become a wellaccepted option for implementing digital signal processing. Traditional devices such as field-programmable gate arrays offer good fine-grain flexibility. More recent coarse-grain reconfigurable architectures are optimized for word-length computations. We have developed a medium-grain reconfigurable architecture that combines the advantages of both approaches. Modules such as multipliers and adders are mapped onto blocks of 4-bit cells. Each cell contains a matrix of lookup tables that either implement mathematics functions or a random-access memory. A hierarchical interconnection network supports data transfer within and between modules. We recently created software tools that allow users to map algorithms onto the reconfigurable platform. This paper analyzes the implementation of several common benchmarks, ranging from simple floating-point arithmetic to a radix-4 Fast Fourier Transform. The results are compared to contemporary digital signal processing hardware.	4-bit;algorithm;benchmark (computing);computation;digital signal processing;fast fourier transform;field-programmability;field-programmable gate array;interconnection;lookup table;random access;random-access memory;reconfigurable computing	Mitchell J. Myjak;Jonathan Larson;José G. Delgado-Frias	2006			computer engineering;digital signal processing;architecture;computer architecture;computer science	Arch	6.568519659561079	46.584364269984256	144117
c326033fa501c4b5914e727e84cbb14dcbc8c3b2	evaluation of scheduling and allocation algorithms while mapping assembly code onto fpgas	resource utilization;processor architecture;register allocation;hardware synthesis;software systems;chaining;compilers;chip;texas instruments;optimal scheduling;binary translation;scheduling;optimizations;fpgas;embedded processor;hardware implementation	Migration of software from older general purpose embedded processors onto newer mixed hardware/software Systems-On-Chip (SOC) platforms is becoming an increasingly important topic. Automatic translation of general purpose software binaries and assembly code onto hardware implementations using FPGAs require sophisticated scheduling and allocation algorithms to maximize the resource utilization of such hardware devices. This paper describes the effects of scheduling and chaining of node operations in a CDFG onto an FPGA. The effects of register allocation on scheduled nodes are also discussed. The Texas Instruments C6000 DSP processor architecture was chosen as the DSP processor platform and assembly code, and the Xilinx Virtex II XC2V250 was chosen as the target FPGA. Results are reported on ten benchmarks, which show that scheduling with chaining operations produces the best results on FPGAs, while the addition of register allocation in fact generates poorer designs in terms of area and frequency.	algorithm;assembly language;benchmark (computing);binary file;central processing unit;digital signal processor;embedded system;field-programmable gate array;memory management;register allocation;scheduling (computing);software system;system on a chip;virtex (fpga)	David Zaretsky;Gaurav Mittal;Xiaoyong Tang;Prithviraj Banerjee	2004		10.1145/988952.989048	chip;embedded system;computer architecture;in situ resource utilization;compiler;parallel computing;real-time computing;microarchitecture;computer science;chaining;operating system;scheduling;register allocation;field-programmable gate array;software system	EDA	-0.09038280101148219	51.386706218797286	144240
be87215f2b744c4626e0eaf16e64f9578e999ab9	task allocation with algorithm transformation for reducing data-transfer bottlenecks in heterogeneous multi-core processors: a case study of hog descriptor computation	multi core processor;system on chip;data transfer;heterogeneous multi core processor;task allocation	Heterogeneous multi-core processors are attracted by the media processing applications due to their capability of drawing strengths of different cores to improve the overall performance. However, the data transfer bottlenecks and limitations in the task allocation due to the accelerator-incompatible operations prevents us from gaining full potential of the heterogeneous multi-core processors. This paper presents a task allocation method based on algorithm transformation to increase the freedom of task allocation. We use approximation methods such as CORDIC algorithms to map the accelerator-incompatible operations to accelerator cores. According to the experimental results using HOG descriptor computation, the proposed task allocation method reduces the data transfer time by more than 82% and the total processing time by more than 79% compared to the conventional task allocation method. key words: heterogeneous multi-core processor, task-allocation, systemon-chip	algorithm;approximation;cordic;central processing unit;computation;multi-core processor	Hasitha Muthumala Waidyasooriya;Daisuke Okumura;Masanori Hariyama;Michitaka Kameyama	2010	IEICE Transactions	10.1587/transfun.E93.A.2570	system on a chip;multi-core processor;parallel computing;real-time computing;telecommunications;computer science;distributed computing	Robotics	-4.234691072270349	50.369860855565214	144538
7efd11173212fc99e15bdb0372d084df2b8c102a	reliability through redundant parallelism for micro-satellite computing	fault tolerant;process capability;fpga;on board computer;cots components;commercial off the shelf;fault tolerance;micro satellite;parallel architecture;power consumption;parallel processing;hardware redundancy	Spacecraft typically employ rare and expensive radiation-tolerant, radiation-hardened, or at least military qualified parts for computational and other mission critical subsystems. Reasons include reliability in the harsh environment of space, and systems compatibility or heritage with previous missions. The overriding reliability concern leads most satellite computing systems to be rather conservative in design, avoiding novel or commercial-off-the-shelf components. This article describes an alternative approach: an FPGA-arbitrated parallel architecture that allows unqualified commercial devices to be incorporated into a computational device with aggregate reliability figures similar to those of traditional space-qualified alternatives. Apart from the obvious cost benefits in moving to commercial-off-the-shelf devices, these are attractive in situations where lower power consumption and/or higher processing performance are required. The latter argument is particularly of major importance at a time when the gap between required and available processing capability in satellites is widening. An analysis compares the proposed architecture to typical alternatives, maintaining risk of failure to within required levels, and discusses key applications for the parallel architecture.	aggregate data;computation;field-programmable gate array;mission critical;parallel computing;radiation hardening	Ian Vince McLoughlin;Timo Rolf Bretschneider	2010	ACM Trans. Embedded Comput. Syst.	10.1145/1698772.1698784	embedded system;parallel processing;fault tolerance;real-time computing;simulation;computer science;operating system	Arch	4.7078461037145	57.741606622090465	144686
6c466835c8f38b6b6096a0b4e1ac0f3ea335e1d2	smart phone power	smart phones displays energy consumption operating systems energy management power system management batteries voice mail marketing and sales mobile communication;cellular radio;smart phone;smart phones;industries;voice mail;smart phone power;mobile communication smart phone power;energy consumption;power system management;mobile handsets cellular radio;power dissipation;displays;batteries;mobile communication;mobile handsets;optimization;design issues;power demand;energy management;operating systems;marketing and sales;design issues smart phone power dissipation	There is considerable interest in the industry in smart phones today. Even in a down economy, sales of smart phones are rising and promising to shake up the future of mobile communication. The players involved are seeking ways to differentiate themselves in this market space, and improved energy management is a key area of focus. This paper focuses on smart phone power.	smartphone	Johnny John;Chris Riddle	2010	Design Automation Conference	10.1145/1837274.1837509	embedded system;electronic engineering;mobile telephony;computer science;engineering;dissipation;operating system;energy management	EDA	-1.400045265449498	58.462263210614026	144691
e4864d8ec329611944b3ff406bcaec60f816a6b7	orthogonal instruction encoding for a 16-bit embedded processor with dynamic implied addressing mode	microprocessors;risc style orthogonal architecture orthogonal instruction encoding 16 bit embedded processor dynamic implied addressing mode 32 bit architectures microprocessors low power consumption irregular instruction sets nonorthogonal architecture sophisticated compiler techniques optimal code generation compiler friendly processor;addressing mode embedded processor;high performance computing;compiler friendly processor;code generation;iron;orthogonal instruction encoding;16 bit embedded processor;scattering;data mining;computer architecture;embedded systems;high priority;risc style orthogonal architecture;registers;energy consumption;optimal code generation;irregular instruction sets;pipelines;dynamic implied addressing mode;code size;32 bit architectures;addressing mode;computer science;encoding registers instruction sets computer architecture microprocessors energy consumption hardware scattering high performance computing computer science;orthogonal codes;program compilers;sophisticated compiler techniques;encoding;embedded processor;low power consumption;nonorthogonal architecture;benchmark testing;microprocessor chips;program compilers embedded systems encoding instruction sets microprocessor chips orthogonal codes;instruction sets;hardware	Although 32-bit architectures are becoming the norm for modern microprocessors, 16-bit ones are still employed by many low-end processors, for which small size and low power consumption are of high priority. However, 16-bit architectures have a critical disadvantage for embedded processors that they do not provide enough encoding space to add special instructions coined for certain applications. To overcome this, many existing architectures adopt non-orthogonal, irregular instruction sets to accommodate a variety of unusual addressing modes thru which more opcodes and operands are densely encoded within the narrow instruction word. In general, these non-orthogonal architectures are regarded compiler-unfriendly as they tend to requires extremely sophisticated compiler techniques for optimal code generation. To address this issue, we propose a compiler-friendly processor with a new addressing mode, called the dynamic implied addressing mode (DIAM). In this paper, we will demonstrate that the DIAM provides more encoding space for our 16-bit processor so that we are able to support more instructions specially customized for our applications. And yet, the processor maintains a RISC-style orthogonal architecture, thereby allowing us to use traditional code generation algorithms. In our experiment, the architecture augmented with DIAMs shows 6.2% code size reduction and 3.5% performance increase on average, as compared to the basic architecture without DIAMs.	16-bit;32-bit;addressing mode;algorithm;central processing unit;code generation (compiler);compiler;digital signal processor;embedded system;microprocessor;opcode;operand	Jonghee M. Youn;Daeho Kim;Minwook Ahn;Yongjoo Kim;Yunheung Paek	2009	2009 11th IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2009.22	computer architecture;supercomputer;parallel computing;real-time computing;addressing mode;computer science;operating system;distributed computing;scattering;iron;code generation;encoding;orthogonal instruction set	Arch	0.10623918884113444	48.61405609125938	144887
00e97639f1afdcd8e709f68af071e5f66e094769	testing using a minimal number of instructions		A new technique to test microprocessor chips is presented in this article. The basic test philosophy adopted is to partition a microprocessor's instructions into several sets. Instructions which affect the identical modules inside the processor are grouped into the same set. The appropriate subset of each instruction set is exercised and if the results are satisfactory, then the microprocessor is assumed to be functioning correctly. The instruction subsets form a test sequence for the microprocessor. Stuck-at faults at the address lines, the data lines and at the output of some of the internal modules are also detected by the test sequence.		Parag K. Lala	1981	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(81)90401-4	parallel computing;real-time computing;computer hardware;in-target probe;computer science	EDA	6.518113977234283	56.753282488681705	144950
9ddadb10b7fa97fecd142d82edafdc97687ee7e7	improving a design methodology of synthesizable vhdl with formal verification	libraries;generators;integrated circuit modeling libraries model checking mathematical model design methodology companies generators;companies;model checking;integrated circuit modeling;mathematical model;hardware description languages formal verification;fiacre vhdl ovl model checking;memory controller application formal verification synthesizable vhdl design methodology product reliability ovl library vhdl code vhdl testbench;design methodology	In this paper we present a synthesizable VHDL design methodology that includes exhaustive verification of properties. The work was developed in a company environment with the goal of increasing reliability of products and reduce time of verification procedure. In this methodology the properties are represented using VHDL oriented patterns based on the OVL library and applied, with the VHDL code, into a verification environment (based on open source tools) that returns the results. Counterexamples are generated for properties that failed and returned as VHDL testbench, allowing the user to identify the faulty behavior with simulation. The methodology is illustrated with a simple memory controller application.	formal verification;logic synthesis;memory controller;open verification library;open-source software;simulation;test bench;vhdl	Luis Gustavo Perpetuo Costa Marques;Max Hering de Queiroz;Jean-Marie Farines	2016	2016 IEEE 7th Latin American Symposium on Circuits & Systems (LASCAS)	10.1109/LASCAS.2016.7451007	computer architecture;computer science;theoretical computer science;formal equivalence checking;programming language;intelligent verification	EDA	8.028108012859489	52.67737274980907	144976
39ea95c2b558e0318fa2b34462e51e6bc54a94c7	the reliable router: an architecture for fault tolerant interconnect	fault tolerant;fault tolerant routing;2 dimensional;virtual channel;wormhole routing;parallel computer;message passing	"""This thesis has four principal contributions in these areas: router architecture, fault-tolerant networks, interchip signalling, and data retiming (synchronization). The Reliable Router is a VLSI device implementing a message-passing communication substrate for parallel computers and other micro-area networks. The network topology is a 2-dimensional mesh. It uses wormhole routing, 5 virtual channels per physical channel, 2 priority levels, minimally adaptive and one-fault-tolerant routing. Bandwidth between routers is 3.2Gbit/s, each way. Router latency is about 70ns. The router has been fabricated on a 13.5mm by 15mm die, packaged in a 463-pin PGA, and partial testing performed. The design methodology is presented. The Unique Token Protocol is used to provide fault-tolerance in the network. The protocol keeps two copies of a message in the network, obviating the need for source buuering. A token is used to determine the unique (exactly-once) delivery of a message, nearly completely eliminating the need for duplicate detection at the receiver. Details of the protocol under wormhole-routing are also provided. Simultaneous Bidirectional Signalling is used for high-performance interchip communication , 200Mbit/s each way per wire. A single wire carries full duplex communication concurrently through the use of waveform superposition. Circuits, noise-reduction techniques, and analysis are given. Low-Latency Plesiochronous Data Retiming is used for interchip communication. Each router operates in a separate clock domain. The basic retiming method is fully described, along with circuits and constraints for correct operation. Average latency is one-half a clock period. Extensions for integral subrates and cascaded timing circuits are given. Acknowledgements \It's a long hard climb, but I'm gonna get there."""" from a Sesame Street song I've relied on many people over the years. I would like to say thank you to the following people: Bill Dally, for getting me started, for all his support along the way, and for bringing this project to a successful completion. My thesis readers, Gill Pratt and Tom Knight, who supplied energy to a rather tired student. Scott Furman, a great oocemate. The people who worked on the router over the years, for getting the no-cost extensions through. Tom Blackadar for understanding \I gotta get done."""" The CVA group, for all their moral support. Duke Xanthopoulos, oocemate, fellow router designer, and among the best of men. My father and mother, Daniel and Dolores, who encouraged me to succeed by teaching me not to fear failure. My father-in-law and mother-in-law, James and Lee Moses, for their support (and …"""	angelfire;arbiter (electronics);bidirectional search;bidirectional transformation;cmos;carpal tunnel syndrome;clock generator;clock rate;clock signal;code refactoring;compensating transaction;computation;computer;cray t3d;crossbar switch;datapath;deadlock;digital electronics;duplex (telecommunications);electronic circuit;end-to-end principle;fifo (computing and electronics);fault tolerance;gate array;glass;gon;hardware description language;ieee journal of solid-state circuits;ieee transactions on computers;ian cullimore;input/output;interconnection;international solid-state circuits conference;international symposium on computer architecture;jtag;jos stam;j–machine;knuth–morris–pratt algorithm;lam/mpi;larry laffer;mesh networking;message passing;moses;multiprocessing;network packet;network topology;newman's lemma;numerical aperture;olap cube;overhead (computing);parallel computing;pin grid array;pipeline (computing);plesiochronous system;polynomial;power cycling;power supply unit (computer);printed circuit board;quantum superposition;resultant;retiming;router (computing);routing;schematic;serial communication;sesame;signature block;simulation;smoke testing (software);software bug;solid-state drive;springer (tank);standard cell;swing (java);synchro;synchronizer (algorithm);systems design;telecommunications network;tom proulx;tridiagonal matrix algorithm;verification and validation;very-large-scale integration;virtual channel;waveform;wiring;woo–lam;wormhole switching	Larry R. Dennison	1996			static routing;parallel computing;multipath routing;distributed computing;computer network	Arch	8.17818672954602	49.79238593167587	145201
192427c9e0000d3b650584768a1f38e06e0ff113	a lightweight and open-source framework for the lifetime estimation of multicore systems	reliability program processors aging computer architecture computational modeling monte carlo methods estimation;open source framework mttf mean time to failure mathematical tools lifetime reliability monte carlo based framework multicore systems lifetime estimation;public domain software electronic engineering computing failure analysis integrated circuit reliability microprocessor chips monte carlo methods multiprocessing systems	This paper presents a Monte Carlo-based framework for the estimation of lifetime reliability of multicore systems. Existing mathematical tools either consider only the time to the first failure, or are limited by their intrinsic complexity and high computational time. The proposed framework allows to compute quasi-exact results with a reasonable computational time, without adopting typical (and possibly misleading) simplifications that characterize the existing tools for computing Mean Time To Failure (MTTF). The paper describes the framework with all its mathematical details, assumptions and simplifications; it proves the correctness of the obtained results, by comparing them against the exact ones, and underlines the differences with the simplistic approaches, also discussing time overhead improvements.	computation;correctness (computer science);mean time between failures;monte carlo method;multi-core processor;open-source software;overhead (computing);simulation;time complexity	Cristiana Bolchini;Matteo Carminati;Marco Gribaudo;Antonio Miele	2014	2014 IEEE 32nd International Conference on Computer Design (ICCD)	10.1109/ICCD.2014.6974677	embedded system;real-time computing;simulation;computer science;theoretical computer science;operating system	EDA	5.070876140857595	57.42106661116846	145339
f4f99a443d7558939225a583b07d62873d8635e3	larrabee: a many-core intel architecture for visual computing	programming model;computer architecture;vector processor;high performance;3d graphics	This talk will describe a many-core visual computing architecture code named Larrabee. Larrabee uses multiple in-order x86 CPU cores that are augmented by a wide vector processor unit, as well as some fixed function logic blocks. The talk will go into an overview of the Larrabee architecture and will cover the LRB Vector ISA in detail. We'll then cover the Larrabee programming model and finally close with how one would target Larrabee for high performance 3D graphics.	larrabee (microarchitecture);visual computing	Roger Espasa	2010		10.1007/978-3-642-11515-8_2	embedded system;computer architecture;vector processor;parallel computing;computer hardware;computer science;operating system;programming paradigm;programming language;3d computer graphics	HPC	-0.77875953846024	46.66034441522803	145372
ccc95d35d877d8a0669a52477fb0f672cf6ad444	feedback driven instruction-set extension	conception conjointe;compilateur;encryption;diseno conjunto;instruction set extensions;network processor;simulator generation;cifrado;generation simulateur;compiler;processeur reseau;program evaluation;automatic generation;simulator;instruction set extension;performance programme;simulador;programa aplicacion;cryptage;codesign;application program;criptografia;cryptography;programme application;simulateur;cryptographie;eficacia programa;program performance;compilador;generation compilateur;application specific instruction set processor;compiler generation	Application specific instruction-set processors combine an efficient general purpose core with special purpose functionality that is tailored to a particular application domain. Since the extension of an instruction set and its utilization are non-trivial tasks, sophisticated tools have to provide guidance and support during design. Feedback driven optimization allows for the highest level of specialization, but calls for a simulator that is aware of the newly proposed instructions, a compiler that makes use of these instructions without manual intervention, and an application program that is representative for the targeted application domain.In this paper we introduce an approach for the extension of instruction sets that is built around a concise yet powerful processor abstraction. The specification of a processor is well suited to automatically generate the important parts of a compiler backend and cycle-accurate simulator. A typical design cycle involves the execution of the representative application program, evaluation of performance statistics collected by the simulator, refinement of the processor specification guided by performance statistics, and update of the compiler and simulator according to the refined specification. We demonstrate the usefulness of our novel approach by example of an instruction set for symmetric ciphers.	application domain;central processing unit;cipher;compiler;computer architecture simulator;mathematical optimization;partial template specialization;refinement (computing);simulation;symmetric-key algorithm	Uwe Kastens;Dinh Khoi Le;Adrian Slowik;Michael Thies	2004		10.1145/997163.997182	computer architecture;compiler;parallel computing;real-time computing;computer architecture simulator;program evaluation;application-specific instruction-set processor;computer science;cryptography;programming language;encryption;network processor	Arch	3.2524906701762135	51.279104847444884	145382
30b14cb6b54302a872de45317f72ae736f290733	high-performance, high-capacity single-chip microcomputers	chip;high performance	The MC6801 Single-Chip Microcomputer has long been recognized as a highperformance microcomputer. This paper provides a brief look at the complete M6801 family and then discusses the enhancements made to the Timer and Serial Communications Interface circuitry of the basic MC6801 to develop the new MC6801U4 microcomputer.  The MC6801U4 strengthens the M6801 family position in the high-performance single-chip microcomputer marketplace.	electronic circuit;microcomputer;serial communication;timer	Ed Peatrowsky	1982		10.1145/1500774.1500783	embedded system;computer hardware;computer science;computer engineering	Arch	6.4764448638854395	49.469044240415876	145383
9a566fdf9e817ca48876ae13ad956584570cc251	the multi-dataflow composer tool: generation of on-the-fly reconfigurable platforms	coarse grained reconfigurable architectures;code generation;rvc cal;run time reconfiguration;dataflow	Dataflow specifications are suitable to describe both signal processing applications and the relative specialized hardware architectures, fostering the hardware–software development gap closure. They can be exploited for the development of automatic tools aimed at the integration of multiple applications on the same coarse-grained computational substrate. In this paper, the multi-dataflow composer (MDC) tool, a novel automatic platform builder exploiting dataflow specifications for the creation of run-time reconfigurable multi-application systems, is presented and evaluated. In order to prove the effectiveness of the adopted approach, a coprocessor for still image and video processing acceleration has been assembled and implemented on both FPGA and 90 nm ASIC technology. 60 % of savings for both area occupancy and power consumption can be achieved with the MDC generated coprocessor compared to an equivalent non-reconfigurable design, without performance losses. Thanks to the generality of high-level dataflow specification approach, this tool can be successfully applied in different application domains.	aggregate data;application domain;application-specific integrated circuit;clock rate;clock signal;code generation (compiler);codec;complexity;composer;coprocessor;dataflow;datapath;embedded system;fits;field-programmable gate array;high- and low-level;moving picture experts group;nl (complexity);overhead (computing);power management;programming paradigm;reconfigurable computing;run time (program lifecycle phase);scenario analysis;signal processing;software development;systems design;video processing	Francesca Palumbo;Nicola Carta;Danilo Pani;Paolo Meloni;Luigi Raffo	2012	Journal of Real-Time Image Processing	10.1007/s11554-012-0284-3	embedded system;computer architecture;parallel computing;real-time computing;computer science;dataflow;code generation	EDA	3.5303201284745733	52.49708576146854	145566
1d85d4cab5a64927085e0c30c843a8af55885189	concepts of building a design automation system	design automation;computer application	“....automation creates roles for people, which is to say depth of involvement in their work and human association that our preceding mechanical technology had destroyed.”  There are many terms used for the computer application to the aid of design and production, none of which truly state the case. We are concerned here with a system that will implement the ideas behind these terms. The tools used for this implementation are the computers and a large amount of software. This combination is ideally suited for the long, tedious, and dull tasks. The user, who is a human being is capable of working easily with abstractions. It is the combination of man and the machine which produces a system to perform our task. Thus it becomes clear that the engineer should be designing systems to design products, and not designing products.	automation;computer	D. Lyons;M. McLuhan	1966		10.1145/800267.810784	embedded system;simulation;electronic design automation;computer science;engineering;electrical engineering;algorithm;computer engineering	EDA	10.025433279843613	51.18818418638137	145822
7ad35953a515d82220217593e477254ee44b7302	a functional specification notation for co-design of mixed analog-digital systems	vlsi;circuit cad;formal specification;high level synthesis;integrated circuit design;mixed analogue-digital integrated circuits;programming language semantics;specification languages;soc;ablox;analog-digital interactions;analogue/digital codesign;declarative style;functional computational model;functional specification notation;functionality description;high-level synthesis;mixed analog-digital systems;mixed-signal system specification;performance issues;system on chip	This paper discusses aBlox - a specification notation forhigh-level synthesis of mixed-signal systems. aBlox addressesthree important aspects of mixed-signal system specification:(1) description of functionality and (2) performanceissues and (3) expression of analog-digital interactions.The semantics of aBlox embeds concepts and rulesof a functional computational model, and uses a declarativestyle to denote performance elements. The papershows some mixed-signal specifications that we developedin aBlox. Finally, we describe a high-level analog synthesisexperiment that used aBlox specifications as inputs.	computation;computational model;functional specification;high- and low-level;interaction;mixed-signal integrated circuit;top-down and bottom-up design;vhdl;vhdl-ams;verilog;verilog-a	Alex Doboli;Ranga Vemuri	2002			computer architecture;electronic engineering;real-time computing;computer science;formal specification;functional specification;very-large-scale integration;high-level synthesis;programming language;dram;integrated circuit design	EDA	5.166108908645933	52.08702552979407	145965
0e2176f393e531fd22ad2666fca52cb7e3753360	potential analysis of a superscalar core employing a reconfigurable array for improving instruction-level parallelism	x86;instruction-level parallelism;superscalar organizations;reconfigurable architectures	As technology scaling reduces pace and energy efficiency becomes a new important design constraint, superscalar processor designs are reaching their performance limits due to area and power restrictions. As a result, new microarchitectural paradigms need to be developed. This work proposes a new organization for x86 processors, based on a traditional superscalar design coupled to a reconfigurable array. The system exploits the fact that few basic blocks are responsible for most of the instructions that execute in the processor, and transforms these basic blocks into configurations for the reconfigurable array. Each configuration encodes the semantics and dependencies for all instructions in the block, so that the ones already mapped can execute bypassing the fetch, decode and dependency checks stages and improving instruction throughput. Our study on the potential of the architecture shows that performance gains of up to 2.5× with respect to a traditional superscalar can be achieved.	basic block;central processing unit;combinational logic;design closure;image scaling;instruction cycle;instruction-level parallelism;microarchitecture;parallel computing;reconfigurable computing;simulation;speculative execution;superscalar processor;throughput;traffic collision avoidance system;x86	Marcelo Brandalero;Antonio Carlos Schneider Beck	2016	Design Autom. for Emb. Sys.	10.1007/s10617-016-9174-4	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system	Arch	-1.0626899941608183	49.87315877930175	146044
92b38f49e057197efff7ca29c1203925f84f6672	isa wars: understanding the relevance of isa being risc or cisc to performance, power, and energy on modern architectures	energy efficiency;technology scaling;power	RISC versus CISC wars raged in the 1980s when chip area and processor design complexity were the primary constraints and desktops and servers exclusively dominated the computing landscape. Today, energy and power are the primary design constraints and the computing landscape is significantly different: Growth in tablets and smartphones running ARM (a RISC ISA) is surpassing that of desktops and laptops running x86 (a CISC ISA). Furthermore, the traditionally low-power ARM ISA is entering the high-performance server market, while the traditionally high-performance x86 ISA is entering the mobile low-power device market. Thus, the question of whether ISA plays an intrinsic role in performance or energy efficiency is becoming important again, and we seek to answer this question through a detailed measurement-based study on real hardware running real applications. We analyze measurements on seven platforms spanning three ISAs (MIPS, ARM, and x86) over workloads spanning mobile, desktop, and server computing. Our methodical investigation demonstrates the role of ISA in modern microprocessors’ performance and energy efficiency. We find that ARM, MIPS, and x86 processors are simply engineering design points optimized for different levels of performance, and there is nothing fundamentally more energy efficient in one ISA class or the other. The ISA being RISC or CISC seems irrelevant.	arm architecture;central processing unit;desktop computer;engineering design process;file spanning;ibm power microprocessors;industry standard architecture;laptop;low-power broadcasting;performance;power semiconductor device;processor design;relevance;server (computing);smartphone;x86	Emily R. Blem;Jai Menon;Thiruvengadam Vijayaraghavan;Karthikeyan Sankaralingam	2015	ACM Trans. Comput. Syst.	10.1145/2699682	embedded system;real-time computing;simulation;operating system;power;efficient energy use	Arch	-1.8987879659978781	57.71364719552967	146109
b3ea5b34c8b09d9d8051f4de968bb61bc50cfc30	a built-in self-testing framework for asynchronous bundled-data noc switches resilient to delay variations	circuit faults;built in self test;pipelines;ports computers;delays	Most multi- and many-core integrated systems are currently designed by following a globally asynchronous locally synchronous paradigm. Asynchronous interconnection networks are promising candidates to interconnect IP cores operating at potentially different frequencies. Nevertheless, post-fabrication testing is a big challenge to bring asynchronous NoCs to the market due to a lack of testing methodologies and support for them. In particular, the unpredictable delay variability introduced by the manufacturing process may differentiate the delay of nominally-balanced I/O timing paths, thus making the order of the input patterns unpredictable and precluding the correct behaviour of signature-based test compactors. This paper tackles this challenge by proposing a testing framework for asynchronous NoCs which works effectively despite delay variations in and across timing paths of the NoC under test. Moreover, in order to mitigate the growing test application costs in modern ICs, we come up with a built-in self-testing infrastructure which automatically controls and delivers the outcome of the testing process without the intervention of an external automatic test equipment (ATE).	built-in self-test;built-in test equipment;globally asynchronous locally synchronous;heart rate variability;input/output;interconnection;manycore processor;network on a chip;network packet;network switch;programming paradigm;responsiveness;routing	Gabriele Miorandi;Alberto Celin;Michele Favalli;Davide Bertozzi	2016	2016 Tenth IEEE/ACM International Symposium on Networks-on-Chip (NOCS)	10.1109/NOCS.2016.7579332	embedded system;real-time computing;operating system;pipeline transport;computer network	EDA	4.282755743826452	60.25640777333459	146253
5e78a1c63b81c9f1b72281c4841f9621bcb39eda	heuristic tradeoffs between latency and energy consumption in register assignment	energy consumption register assignment code generation embedded systems register allocation;cost based code motion;distributed power generation;hardware software codesign;register assignment;low energy;register allocation;hardware software codesign embedded systems program compilers;prefetching;code generation;response time;embedded system;vliw;embedded systems;low latency;permission;registers;energy consumption;scheduling;application specific processors;optimization;program compilers;delay energy consumption registers embedded system scheduling permission prefetching distributed power generation vliw application specific processors	One of the challenging tasks in code generation for embedded systems is register allocation and assignment, wherein one decides on the placement and lifetimes of variables in registers. When there are more live variables than registers, some variables need to be spilled to memory and restored later. In this paper we propose a policy that minimizes the number of spills — which is critical for portable embedded systems since it leads to a decrease in energy consumption. We argue however, that schedules with a minimal number of spills do not necessarily have minimum latency. Accordingly, we propose a class of policies that explore tradeoffs between assignments leading to schedules with low latency versus those leading to low energy consumption and show how to tune them to particular datapath characteristics. Based on experimental results we propose a criterion to select a register assignment policy that for 99% of the cases we considered minimizes both latency and energy consumption associated with spills to memory.	code generation (compiler);datapath;embedded system;heuristic;register allocation;schedule (computer science)	Rahul Anand;Margarida F. Jacome;Gustavo de Veciana	2000		10.1145/334012.334034	computer architecture;parallel computing;real-time computing;computer science	EDA	-4.133007148340283	53.22811627627335	146340
231f32a90807b2a992e969f82a08b560f36564ce	implementing ada.real_time.clock and absolute delays in real-time kernels	systeme temps reel;kernel;ada;real time;chip;noyau;multithread;real time system;sistema tiempo real;multitâche;ada language;real time systems;timing	A real-timekernelproviding multitaskingandtiming servicesisafundamentalcomponentof any real-timesystem.Timing services,whicharecrucial to the correctexecutionof this kind of applications,areusuallyprovided by a real-timeclock andtimer manager , which is part of the kernelandimplements the requiredfunctionality on top of the one or more hardware timers. Kernel timing servicesmustbe implementedcarefully in orderto avoid raceproblems andinconsistencieswhich may be causedby the fact that many commonhardwaretimer chipsarenot intendedat a direct implementationof softwaretiming services.This paperprovidesadviceon the implementationof two of the Ada timing services:Ada.Real_Time.Clock, andabsolutedelays(delay until). Theexampleimplementationof both servicesin theOpenRavenscarKernel,which is basedon theideaspresentedin thepaper , is alsodescribed.	ada;kernel (operating system);real-time transcription;timer	Juan Zamorano;José F. Ruiz;Juan Antonio de la Puente	2001		10.1007/3-540-45136-6_25	chip;embedded system;kernel;real-time computing;real-time operating system;ada;computer science;operating system;programming language	Embedded	4.470150577128316	48.033118254825304	146427
9447fbbc41632a8a3724ae2b0fb4a2c0c26900bf	java-based approach for high level openmp loops synthesis	manuals;generators;java packages;shared memory systems c language hardware description languages high level synthesis java parallel programming program compilers program control structures;standards;hardware java generators syntactics standards synchronization manuals;syntactics;synchronization;vhdl;openmp directives;shared memory parallel programs java based approach high level openmp loop synthesis automatic vhdl code generation method openmp parallel programming specification c code;java packages openmp directives vhdl c for loops;c for loops;hardware;java	This paper presents an automatic VHDL code generation method based on the OpenMP parallel programming specification. In order to synthesize C code for loops into hardware, we applied the directives of OpenMP, which specifies portable implementations of shared memory parallel programs. The proposed design flow using this method is described and its implementation details are provided. Experimental results show that the generated vhdl code from OpenMP is competitive with optimized code.	algorithm;automatic programming;code generation (compiler);compiler;design flow (eda);discrete cosine transform;field-programmable gate array;high-level programming language;java;java package;multi-core processor;openmp;parallel computing;prototype;scalability;shared memory;system on a chip;vhdl	Emna Kallel;Yassine Aoudni;Mohamed Abid	2016	2016 IEEE 14th International Conference on Software Engineering Research, Management and Applications (SERA)	10.1109/SERA.2016.7516142	synchronization;computer architecture;parallel computing;vhdl;computer science;operating system;real time java;programming language;java;java annotation	HPC	2.9505886004602937	50.68603726291184	146548
6bf24ff140a4cd2300ccd023ca803d2e17fae662	mixed-signal diverse redundant system for safety critical applications in fpga	digital signal processing;triple modular redundant;on chip redundancy mixed signal diverse redundant system safety critical applications fpga process automation sensor iec61508 standard modular redundancy fpaa;field programmable analog arrays;safety systems electric sensing devices field programmable analogue arrays field programmable gate arrays iec standards mixed analogue digital integrated circuits;mixed signal diverse redundant system;mixed signal;redundancy field programmable gate arrays safety digital signal processing field programmable analog arrays system on a chip current measurement;safety systems;fpga;iec61508 standard;process automation sensor;system on a chip;chip;field programmable analogue arrays;fpaa;redundancy fpga safety critical applications fpaa mixed signal;iec standards;redundancy;current measurement;safety;modular redundancy;mixed analogue digital integrated circuits;field programmable gate arrays;on chip redundancy;safety critical applications;electric sensing devices	The aim of this study is to design an architecture which increases the functional safety of a process automation sensor as defined by the standard IEC61508. Furthermore this architecture can increase the availability of the sensor. It is based on a triple modular redundancy with a combination of an FPGA and FPAAs, which means it is a mixed-signal diverse redundancy. The study also takes into account the latest development of the standard: the second edition which brings new requirements to on-chip redundancy in FPGAs. The paper exposes the advantages of this mixed-signal diverse redundancy and the progress of the realization.	field-programmable gate array;mixed-signal integrated circuit;requirement;triple modular redundancy	Romuald Girardey;Michael Hübner;Jürgen Becker	2010	2010 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2010.11	dual modular redundancy;embedded system;electronic engineering;active redundancy;engineering;redundancy;computer engineering	Embedded	5.610532827488551	55.24935645300074	146571
ffc8cc481ab68add9263867958e1bbce9831875c	static write buffer cache modeling to increase host-compiled simulation accuracy		Efficient design of complex multiprocessor embedded systems requires fast technologies for early system cosimulation and evaluation. Host-compiled simulation has been proposed as an option for this purpose, since it enables accurately timed modeling at high simulation speeds. To achieve high accuracy, simulation technology has to consider internal details of the processing system, such as the modeling of processor pipelines or theestimation of cache misses. However, the modeling of these details must involve low overhead to ensure simulation speed. This paper proposes adding the modeling of the impact of a write buffer in write-through policies. The scheme presented is oriented to maximizing the accuracy vs. speed balance, proposing a static solution that results in no additional simulation overhead.	3d modeling;cpu cache;cache (computing);compiler;embedded system;multiprocessing;overhead (computing);pipeline (computing);simulation;write buffer	Héctor Posadas;Luis Diaz;Eugenio Villar	2017	2017 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2017.84	real-time computing;cache;software;parallel computing;computer science;write buffer;multiprocessing	EDA	-2.188010386759645	55.685947744810285	146991
6b32cbc14128c1dcc87fcd2e03b29f29a5779bd6	an environment for design space exploration using gem5-mcpat	performance;embedded system;design space exploration;power consumption;cost	Design space exploration of embedded systems is an example of a multiobjective problem. At System Level, different solutions can be generated using different mappings of hardware and software. To fulfill the requirements like cost, energy consumption, performance, and power, these different solutions must be evaluated. This work presents an environment for design space exploration aiming to estimate area, energy, performance and power of different solutions in a multicore architecture. To obtain these measures, the simulators gem5 and McPAT are used. They are integrated into the environment VIPEX (Virtual Platform Exploration), a visual framework focused on simplifying the process of design space exploration	design space exploration;embedded system;multi-core processor;multi-objective optimization;requirement;simulation	Renan Tashiro;Márcio Oyamada	2016	2016 VI Brazilian Symposium on Computing Systems Engineering (SBESC)	10.1109/SBESC.2016.042	embedded system;real-time computing;simulation;engineering	EDA	1.33233143117485	54.87856472257458	147165
d406ddae4aedc2f9563e1839539d4fb068fd196c	low-power cache organization through selective tag translation for embedded processors with virtual memory support	virtual memory;probability;cache consistency;low energy;energy efficient;dynamic voltage scaling;hardware architecture;data cache;low power;indexation;multitasking;power reduction;embedded processor;hard real time	In this paper we present a novel cache architecture for energy-efficient data caches in embedded processors with virtual memory. Application knowledge regarding the nature of memory references is used to eliminate tag address translations for most of the cache accesses. We introduce a novel cache tagging scheme, where both virtual and physical tags co-exist in the cache tag arrays. Physical tags and special handling for the super-set cache index bits are used for references to shared data regions in order to avoid cache consistency problems. By eliminating the need for address translation on cache access for the majority of references, a significant power reduction is achieved. We outline an efficient hardware architecture for the proposed approach, where the application information is captured in a reprogrammable way and the cache architecture is minimally modified. Our experimental results show energy reductions for the address translation hardware in the range of 90%, while the reduction for the entire cache architecture is within the range of 25%-30%.	cache coherence;central processing unit;computer memory;embedded system;low-power broadcasting	Xiangrong Zhou;Peter Petrov	2006		10.1145/1127908.1127999	bus sniffing;embedded system;least frequently used;pipeline burst cache;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;real-time computing;cache coloring;page cache;cpu cache;human multitasking;tag ram;cache;computer science;virtual memory;write-once;cache invalidation;operating system;translation lookaside buffer;probability;hardware architecture;efficient energy use;smart cache;mesi protocol;cache algorithms;cache pollution;mesif protocol;statistics;cache-only memory architecture;non-uniform memory access	Arch	-3.054159250591989	53.57002920099103	147276
66e57501e834bcf60a094b1904b0bcdbb0efc0d2	design of parallel implementations by means of abstract dynamic critical path based profiling of complex sequential algorithms	optimal solution;evaluation performance;solution optimale;performance evaluation;concepcion sistema;implementation;data flow graphs;evaluacion prestacion;design flow;paralelisacion;sistema complejo;video codec;graphe flux donnee;algorithme;algorithm;parallel architectures;systeme complexe;architecture parallele;complex system;chemin critique;system design;critical path;solucion optima;parallelisation;system level design;performance analysis;parallelization;codec;development methodology;parallel implementation;data flow;implementacion;conception systeme;recorrido critico;algoritmo	This paper presents a methodology of parallel implementations design that starts with abstract sequential descriptions of complex systems when no any parallel solutions have been taken and solves dynamically at real input data very complex tasks that are typical for system-level design. Critical path and parallelization potential based profiling of large sequential algorithms on data flow execution graphs is the kernel of methodology that enables to search for optimal (sub-optimal) parallel implementation solutions at very abstract level of design flow. Experimental results obtained on the critical path and parallelization potential based profiling of MPEG4 video codec and subsequent performance analysis of possible parallel implementations prove usefulness and effectiveness of the developed methodology and tool.	algorithm	Anatoly Prihozhy;Daniel Mlynek	2006		10.1007/11847083_1	data flow diagram;complex systems;codec;parallel computing;real-time computing;computer science;design flow;theoretical computer science;operating system;critical path method;electronic system-level design and verification;implementation;algorithm;systems design	HPC	0.3365075889969656	53.8842001389567	147576
505f333a90afc278939919cced9099f1dc6f0d73	high-level description and synthesis of floating-point accumulators on fpga	db reduction method high level synthesis floating point accumulator fpga field programmable gate array high level hardware description c construct c construct parallel architecture pipelined architecture control flow dataflow graph hardware description language hdl data streaming source data sink data transfer synchronization interface high level communicating fsm programming model spatial parallelism temporal parallelism compiler combinatorial loop pipelined floating point accumulator delayed buffering reduction method;streaming data interface high level synthesis hardware description language control oriented architectures;hardware description languages;synchronisation;high level synthesis;c language;parallel architectures;synchronisation c language field programmable gate arrays fixed point arithmetic floating point arithmetic hardware description languages high level synthesis parallel architectures pipeline arithmetic;synchronization hardware data transfer adders hardware design languages computer architecture field programmable gate arrays;streaming data interface;fixed point arithmetic;floating point arithmetic;field programmable gate arrays;hardware description language;pipeline arithmetic;control oriented architectures	Decades of research in the field of high level hardware description now result in tools that are able to automatically transform C/C++ constructs into highly optimized parallel and pipelined architectures. Such approaches work fine when the control flow is a priory known since the computation results in a large dataflow graph that can be mapped into the available operators. Nevertheless, some applications have a control flow that is highly dependant on the data. This paper focuses on the hardware implementation of such applications and presents a high level synthesis methodology applied to a Hardware Description Language (HDL) in which assignments correspond to self-synchronized connections between predefined data streaming sources and sinks. A data transfer occurs over an established connection when both source and sink are ready, according to their synchronization interfaces. Founded on a high-level communicating FSM programming model, the language allows the user to describe and dynamically modify streaming architectures exploiting spatial and temporal parallelism. Our compiler attempts to maximize the number of transfers at each clock cycle and automatically fixes the potential combinatorial loops induced by the dynamic connection of dependant sources and sinks. The methodology is applied to the synthesis of a pipelined floating point accumulator using the Delayed-Buffering (DB) reduction method. The results we obtain are similar to state-of-the-art dedicated architectures but require much less design time and expertise.	abstraction layer;accumulator (computing);authorization;c++;clock signal;compiler;computation;control flow;data synchronization;data-flow analysis;dataflow;entity;field-programmable gate array;hardware description language;high- and low-level;high-level programming language;high-level synthesis;microsoft windows;parallel computing;performance;programming model;z-buffering	Marc-André Daigneault;Jean-Pierre David	2013	2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2013.37	embedded system;computer architecture;parallel computing;computer science;theoretical computer science;operating system;hardware description language;programming language	Arch	0.5122455168279958	51.098219330229426	147655
3e7704d5ec9ece0f4e703f03ed7912cc470b87ed	optimizing data intensive flows for networks on chips		Data flow analysis and optimization is considered for homogeneous rectangular mesh networks. We propose a flow matrix equation which allows a closed form characterization of the nature of the minimal time solution, speedup and a simple method to determine when and how much load to distribute to processors. We also propose a rigorous mathematical proof about the flow matrix optimal solution existence and that the solution is unique. The methodology introduced here is applicable to many interconnection networks and switching protocols (as an example we examine toroidal networks and hypercube networks in this paper). An important application is improving chip area and chip scalability for networks on chips processing divisible style loads.		Junwei Zhang;Yang Liu;Shi Li;Thomas G. Robertazzi	2018	CoRR			Arch	1.530305010866961	60.334687347905785	147707
345c89001c4f0abae3bf271b8171be9aaf2506d1	zero logic overhead integration of partially reconfigurable modules	reconfiguration;partial reconfiguration;fpga;design technique	Swapping just small fractions of the configuration of an FPGA can be very beneficial in many applications. This is in particular useful for reconfiguring the instruction set of embedded soft core processors. In this paper, we will sketch that present design techniques include a material overhead for integrating reconfigurable parts into the rest of the system. This overhead can cost more logic resources than the actual module implementations. For removing this overhead, we propose a novel technique to constrain the communication resources between the static system and the partial regions. We will demonstrate for a reconfigurable soft core processor that instructions can be integrated into the system without causing any additional logic overhead for the communication. In addition, we reveal how such systems can be easily implemented with our tool ReCoBus-Builder.	central processing unit;embedded system;field-programmable gate array;hot swapping;overhead (computing)	Dirk Koch;Christian Beckhoff;Jim Tørresen	2010		10.1145/1854153.1854181	embedded system;computer architecture;parallel computing;real-time computing;computer science;engineering;control reconfiguration;overhead;field-programmable gate array	EDA	6.679453408610271	57.241488854343466	148048
cbc14918d560754a454f07d5ae3cd2a016e1efb1	mpassign: a framework for solving the many-core platform mapping problem	many cores;mapping;evolutionary algorithm;mpsoc	Many-core platforms, providing large numbers of parallel execution resources, emerge as a response to the increasing computation needs of embedded applications. A major challenge raised by this trend is the efficient mapping of applications on parallel resources. This is a nontrivial problem because of the number of parameters to be considered for characterizing both the applications and the underlying platform architectures. Recently, several authors have proposed to use Multi-Objective Evolutionary Algorithm (MOEA) to solve this problem within the context of mapping applications on Network-on-Chips (NoC). However, these proposals have several limitations: (1) only few meta-heuristics are explored (mainly NSGAII and SPEA2), (2) only few cost functions are provided, and (3) they only deal with a small number of the application and architecture constraints. In this paper, we propose a new framework which avoids all of the problems cited above. Our framework allows designers to (1) explore several new meta-heuristics, (2) easily add a new cost function (or to use an existing one) and (3) take into account any number of architecture and application constraints. The paper also presents experiments illustrating how our framework is applied to the problem of mapping streaming applications on a NoC based many-core platform.	computation;embedded system;evolutionary algorithm;experiment;heuristic (computer science);loss function;moea framework;manycore processor;network on a chip	Youcef Bouchebaba;Ali Erdem Özcan;Pierre G. Paulin;Gabriela Nicolescu	2010	Proceedings of 2010 21st IEEE International Symposium on Rapid System Protyping	10.1002/spe.1157	differential evolution;mathematical optimization;computer science;theoretical computer science;evolutionary algorithm;algorithm	Embedded	-0.05183072528185758	55.46159238733664	148238
f3188782d3b62a68316d6e512070825e8107ed2c	performance benchmark of dsp and fpga implementations of low-level vision algorithms	digital signal processors;digital signal processing;field programmable gate array;digital signal processing field programmable gate arrays signal processing algorithms image processing computer vision hardware application software embedded system costs digital signal processors;image processing;application software;fpga high speed embedded platform;altera stratix field programmable gate array;embedded system;computer vision;fpga implementation;embedded systems;image processing digital signal processing chips embedded systems field programmable gate arrays;titms320c6414 dsp;performance benchmark;digital signal processor;low level vision algorithm;digital signal processing chips;field programmable gate arrays;image processing performance benchmark titms320c6414 dsp digital signal processor fpga high speed embedded platform altera stratix field programmable gate array low level vision algorithm;signal processing algorithms;high speed;hardware	Selecting an embedded hardware platform for image processing has a big influence on the achievable performance. This paper reports our work on a performance benchmark of different implementations of some low-level vision algorithms. The algorithms are implemented on both Digital Signal Processor (DSP) and Field Programmable Gate Array (FPGA) high-speed embedded platforms. The target platforms are a TITMS320C6414 DSP and an Altera Stratix FPGA. The implementations are evaluated, compared and discussed. The DSP implementations outperform the FPGA implementations, but at the cost of spending all its resources to these tasks. FPGAs, however, are well suited to algorithms, which benefit from parallel execution.	algorithm;bayer filter;benchmark (computing);digital signal processor;edge detection;embedded system;field-programmable gate array;gaussian blur;high- and low-level;image processing;image sensor;microsoft outlook for mac;parallel computing;sobel operator;stratix;system image	Daniel Baumgartner;Peter Rossler;Wilfried Kubinger	2007	2007 IEEE Conference on Computer Vision and Pattern Recognition	10.1109/CVPR.2007.383421	embedded system;digital signal processor;computer architecture;parallel computing;image processing;computer science;field-programmable gate array	Vision	2.753372202691356	46.6282469049284	148247
19130f14489a1881aa40d2345bc03577bddfe4ad	a modeling front-end for seamless design and generation of context-aware dynamically reconfigurable systems-on-chip		Abstract In this paper, we present a Model Driven Engineering (MDE) methodology for facilitating the modeling of the partial reconfiguration process, and for implementing Dynamic Reconfigurable System-on-Chip (DRSoC). The rationale for this approach is to provide a modeling front-end that enables to visually compose a hardware platform, containing heterogeneous components, using both static and context-management hardware wrappers. A model transformations engine (MTE) processes the high-level models to obtain the inputs for the Xilinx dynamic partial reconfiguration (DPR) design flow, with the benefit of better exploiting and reusing the designer intentions regarding the allocation of tasks into reconfigurable areas. Furthermore, the automatic synthesis of a reconfiguration controller (RecOS) with context-management and task relocation capabilities is supported in this version of our tool. The latter feature is possible due to the integration of relocation tool OORBIT into the design chain, but we point out at other avenues of research. We present a case study in which we assess the benefits of the methodology and present a thorough analysis of the reconfiguration costs associated with the context and relocation management, showing speedups of 1.5x over other solutions.	reconfigurability;reconfigurable computing;seamless3d;system on a chip	Gilberto Ochoa-Ruiz;Pamela Wattebled;Maamar Touiza;Florent de Lamotte;El-Bay Bourennane;Samy Meftali;Jean-Luc Dekeyser;Jean-Philippe Diguet	2018	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2017.09.011	parallel computing;distributed computing;control reconfiguration;chip;relocation;computer science;reuse;design flow;real-time computing;model-driven architecture;front and back ends;control theory	HCI	3.9407476812624487	52.409492975978026	148249
71824cf8a29a3a94f9ac92d685486fbd03e993db	splash 2 - fpgas in a custom computing machine			field-programmable gate array	Duncan A. Buell;Jeffrey M. Arnold;Walter Kleinfelder	1996				HPC	4.644181581795319	49.14826058090478	148318
3542f3edf161f363ab30afde30d0a5890a727db7	a coalescing-partitioning algorithm for optimizing processor specification and task allocation	optimising compilers;cp 2;partitioning algorithms benchmark testing runtime simulated annealing process design embedded computing design automation hardware costs automotive applications;design automation;hardware cost;heuristic approach coalescing partitioning algorithm processor specification optimisation task allocation design problems embedded computer systems graph partitioning based representation custom design automation algorithm cp 2 baseline algorithms hardware cost run time commercially developed automotive applications benchmark algorithms simulated annealing;formal specification;embedded computer systems;heuristic programming;design problems;simulated annealing;benchmark algorithms;runtime;process design;general solution;graph partitioning;coalescing partitioning algorithm;automotive applications;heuristic programming real time systems formal specification optimising compilers simulated annealing;custom design automation algorithm;processor specification optimisation;baseline algorithms;heuristic approach;figure of merit;run time;graph partitioning based representation;benchmark testing;embedded computing;commercially developed automotive applications;partitioning algorithms;task allocation;hardware;real time systems	This paper considers the design problems of processor speci3cation and task allocation for embedded computer system. A graph partitioning-based representation is proposed that allows these problems to be solved concurrently. A custom design automation algorithm based on this representation is then presented. This algorithm, named CP*-2, was benchmarked against two baseline algorithms on a combination of real and synthetic test cases with respect to two .figures of merit: hardware cost and run-time. The real test cases are based on commercially developed automotive applications and the benchmark algorithms consist of heuristic and simulated annealing approaches. On average, CP*-2 was found to generate solutions with quality comparable to simulated annealing with up to an order of magnitude improvement in run-time.	algorithm;baseline (configuration management);benchmark (computing);computer;embedded system;graph partition;heuristic;optimizing compiler;simulated annealing;synthetic intelligence;test case	James E. Beck;Daniel P. Siewiorek	1996		10.1109/ASAP.1996.542828	process design;embedded system;benchmark;figure of merit;computer architecture;parallel computing;real-time computing;simulated annealing;electronic design automation;computer science;graph partition;theoretical computer science;operating system;formal specification	EDA	0.6508819692725939	52.30868536132626	148377
775d27faab36242f90e1d92d0d72a6e72d0ed3b8	board and system test with soc dft	design for testability;chip manufacturing process board test system test soc dft;system testing design for testability software testing built in self test manufacturing processes design engineering embedded software software standards universal serial bus certification;chip manufacturing process;chip;soc dft;board test;system on chip;system on chip design for testability;system test	Testing an SoC in its chip manufacturing process is the simplest of the test problems that SoC and the DFT engineers working on its design will ever encounter. This article outlines the task and the challenges involved in DFT activities. It also tackled the implication of SoC DFT in later tests activities	system testing	Gordon D. Robinson	2005	IEEE International Conference on Test, 2005.	10.1109/TEST.2005.1584098	chip;system on a chip;embedded system;electronic engineering;telecommunications;computer science;engineering;design for testing;system testing;computer engineering	Robotics	9.657394115128715	54.19992338758321	148434
165c489f21098b3bacb51504bccbc299b5dd2f79	practical experiences in functional simulation. an integrated method from unit to co-simulation	project management;hardware software codesign;difference operator;specifications ibm compute servers verification firmware hardware layer system software layer project managing reusability;project management hardware software codesign firmware formal verification;firmware;interface design;software project management;chip;formal verification;operating system;complex system;hardware application software operating systems system software microprogramming computational modeling chip scale packaging very large scale integration application specific integrated circuits standardization;point of view	IBM, like other major companies, is developing large compute servers. These servers run typically various applications supported on a variety of different operating systems. To support all the user scenarios the systems consist of a hardware layer and a system software layer, that is hidden, from the application or operating system. The hardware consists of a set of chips where some are unique for a given series of compute server. The system software, also called firmware, can be viewed as an extension of the hardware, to enable additional features and to manage those complex systems.	co-simulation;complex systems;firmware;logic simulation;operating system;scenario (computing);server (computing);supercomputer	Klaus-Dieter Schubert	2002		10.1109/HLDVT.2002.1224426	chip;system on a chip;project management;hardware compatibility list;embedded system;firmware;verification and validation;formal methods;formal verification;software verification;software project management;computer science;interface design;operating system;software construction;hardware architecture;programming language;automated information system;hardware register;intelligent verification;software system	Arch	4.378281624395182	50.75736578719077	148532
bbd5af788db7fb5bbbb2829103909a53bd45866f	mixed-mode simulation of compiled vhdl programs	integrated approach;vlsi circuit cad digital integrated circuits digital simulation program compilers specification languages;digital integrated circuits;specification languages;digital systems;vhsic hardware description language;vlsi;mixed mode;data structuring very high speed ic compiled vhdl programs vhsic hardware description language hierarchical design documentation mixed mode simulation interface compilation;circuit cad;hierarchical design;program compilers;high performance;object oriented modeling switches computational modeling very high speed integrated circuits circuit simulation analytical models microelectronics hardware design languages documentation design automation;data structure;digital simulation	The VHSIC hardware description language (VHDL) supports the hierarchical design, documentation, and simulation of a wide range of digital system abstractions. VHDL, however, is often cited as difficult to use and inefficient for simulating designs below the gate level. The authors present the mixed-mode simulation facilities of a VHDL system that overcome this limitation by effectively merging gate and switch primitive evaluation routines with VHDL processes. A high-performance mixed-mode simulation capability is achieved through integrated approaches to interface compilation, data structuring, and implementation of the simulation cycle. Several experimental results serve as preliminary justification for this methodology. >	compiler;mixed-signal integrated circuit;simulation;vhdl	Ramón D. Acosta;Steven P. Smith;J. Larson	1989		10.1109/ICCAD.1989.76930	embedded system;computer architecture;electronic engineering;parallel computing;data structure;vhdl;computer science;operating system;very-large-scale integration;programming language;computer engineering	Arch	8.95911656843984	51.62882722385699	148708
268c8259b623e5420a76c69ad8f649cd0a0983fd	an integer programming approach to instruction implementation method selection problem	computer architecture;instruction sets;integer programming;cpu;cpu core architecture designs;application specific integrated processors;branch-and-bound algorithm;chip area;design automation;instruction implementation method selection problem;instruction set architecture;integer programming approach;performance;power consumption	This paper proposes a new algorithm for Instruction implementation Method Selection Problem (IMSP) in ASIP (Application Specific Integrated Processors) design automation. This problem is to be solved in the instruction set architecture and CPU core architecture designs. First, the IMSP is formalized as an integer progmmming problem, which is to maximize the performance of the CPU under the constraints of chip area and power consumption. Then, a branch-andbound algorithm to solve IiWSP is described. According to the experimental results, the proposed algorithm is quite effective and eficient in solving the IMSP. This algorithm will automate the complex parts of the ASIP chip design.	application-specific instruction set processor;central processing unit;integer programming;intel core (microarchitecture);selection algorithm	Masaharu Imai;Jun Sato;Alauddin Alomary;Nobuyuki Hikichi	1992			computer architecture;parallel computing;real-time computing;integer programming;electronic design automation;computer science;linear programming;operating system;central processing unit;instruction set	EDA	0.035421478618174865	52.19674794952523	148742
555b9f9502030c7ce10f3d2b53172050f16cea26	testing of core-based systems-on-a-chip	symbol manipulation;test application time reduction core based systems on a chip low overhead test architectures core access hardware additions finite state automata transparency modes hardware behavior symbolic test register transfer level circuit satisfiability based solution testability analysis system level average area overhead;common ground;satisfiability;system on a chip;system testing circuit testing system on a chip automatic testing hardware automata integrated circuit packaging costs visualization manufacturing;high level synthesis;logic testing application specific integrated circuits integrated circuit testing finite automata high level synthesis symbol manipulation;application specific integrated circuits;finite state automata;test coverage;logic testing;finite automata;integrated circuit testing;finite state automaton;model test;register transfer level	Available techniques for testing of core-based systems-on-a-chip (SOCs) do not provide a systematic means for synthesizing low-overhead test architectures and compact test solutions. In this paper, we provide a comprehensive framework that generates low-overhead compact test solutions for SOCs. First, we develop a common ground for addressing issues such as core test requirements, core access, and testing hardware additions. For this purpose, we introduce finite-state automata (FSA) for modeling tests, transparency modes, and testing hardware behavior. In many cases, the tests repeat a basic set of test actions for different test data that can again be modeled using FSA. While earlier work can derive a single symbolic test for a module in a register-transfer level (RTL) circuit as a finite-state automaton, this work extends the methodology to the system level and additionally contributes a satisfiability-based solution to the problem of applying a sequence of tests phased in time. This problem is known to be a bottleneck in testability analysis not only at the system level, but also at the RTL. Experimental results show that the system-level average area overhead for making SOCs testable with our method is only 4.5%, while achieving an average test application time reduction of 80% over recent approaches. At the same time, it provides 100% test coverage of the precomputed test sets/sequences of the embedded cores.	system on a chip	Srivaths Ravi;Ganesh Lakshminarayana;Niraj K. Jha	2001	IEEE Trans. on CAD of Integrated Circuits and Systems	10.1109/43.913760	system on a chip;embedded system;electronic engineering;real-time computing;computer science;automatic test pattern generation;test compression;application-specific integrated circuit;code coverage;finite-state machine;high-level synthesis;register-transfer level;algorithm;satisfiability	EDA	8.157051947799461	52.81671127109809	148893
9b9cd5b88e1b2d2fa3f19d055b19f1648ea052f3	constraint-driven system partitioning	on chip bus;hardware software partitioning process;convergence;hardware software codesign;cache;generality;cost function;intellectual property;optimization technique;co design problem;convergence rate;simulated annealing;design optimization;system on a chip;low power;estimation;hardware software partitioning;logic partitioning constraint handling hardware software codesign;cost functions;algorithm convergence rate;constraint handling;genetic algorithms;constraint driven system partitioning;software standards;logic partitioning;algorithm design and analysis;generality constraint driven system partitioning co design problem cost functions hardware software partitioning process algorithm convergence rate;partitioning algorithms cost function convergence software standards hardware simulated annealing genetic algorithms design optimization algorithm design and analysis;partitioning algorithms;hardware	Thispaperdescribeshowoptimizationtechniquescanbe appliedto efficientlysolvetheconstrainedco-designproblem. This is performedby the formulationof differentcost functionswhich will drive thehardware-softwar epartitioning process.Theuseof complex costfunctionsallowsusto capture more aspectsof thedesign.Besides,theappropriateformulationof this kind of functionshasa greatimpact on the resultsthat can be obtainedregarding both quality andalgorithmconvergencerate. A strongpoint of theproposedformulation is its generality. Therefore, it doesnot dependon theproblemandcanbeeasilyextendedfor consideringnew designconstraints.		Marisa López-Vallejo;Jesús Grajal;Juan Carlos López	2000		10.1145/343647.343811	system on a chip;algorithm design;estimation;parallel computing;real-time computing;multidisciplinary design optimization;genetic algorithm;convergence;simulated annealing;cache;computer science;theoretical computer science;rate of convergence;algorithm;intellectual property	Robotics	10.030200958465334	49.34447684682071	149008
dc526ec29683e835a44ddd63f0cb274e2917c32f	high level extraction of soc architectural information from generic c algorithmic descriptions	virtual machine;virtual memory;signal design;signal analysis;virtual machining;design flow;virtual memory architectures;data mining;fluid flow measurement;generic c algorithmic descriptions;data mining software algorithms algorithm design and analysis signal processing signal design signal analysis fluid flow measurement design methodology virtual machining memory architecture;high level synthesis;integrated circuit design;circuit simulation;c language;automatic analysis;lines of code;system on chip;processing algorithms;memory architecture;signal processing;high level extraction;code rewriting;data flow information;software specifications;software algorithms;data flow;soc architectural information;software specification;automatic tools;c virtual machine;algorithm design and analysis;c language system on chip integrated circuit design high level synthesis circuit simulation;design methodology;system on chip high level extraction soc architectural information generic c algorithmic descriptions processing algorithms software specifications design flow automatic tools code rewriting automatic analysis c virtual machine data flow information virtual memory architectures	"""The complexity of nowadays, algorithms in terms of number of lines of codes and cross-relations among processing algorithms that are activated by specific input signals, goes far beyond what the designer can reasonably grasp from the """"pencil and paper"""" analysis of the (software) specifications. Moreover, depending on the implementation goal different measures and metrics are required at different steps of the implementation methodology or design flow of SoC. The process of extracting the desired measures needs to be supported by appropriate automatic tools, since code rewriting, at each design stage, may result resource consuming and error prone. This paper presents an integrated tool for automatic analysis capable of producing complexity results based on rich and customizable metrics. The tool is based on a C virtual machine that allows extracting from any C program execution the operations and data-flow information, according to the defined metrics. The tool capabilities include the simulation of virtual memory architectures."""	algorithm;analysis of algorithms;code;cognitive dimensions of notations;critical path method;dataflow;high-level programming language;rewriting;simulation;source-to-source compiler;system on a chip;virtual machine	Marco Mattavelli;Massimo Ravasi	2005	Fifth International Workshop on System-on-Chip for Real-Time Applications (IWSOC'05)	10.1109/IWSOC.2005.71	computer architecture;parallel computing;computer science;theoretical computer science	EDA	2.5001076237885447	52.82129643825947	149031
9c7545705bc6f0aee3b4383c02e5b1f9ee3acfa0	functional verification of the superscalar sh-4 microprocessor	random test generators functional verification superscalar sh 4 microprocessor modern complex processors control logic design short processor design cycle sh4 processor dual issue superscalar risc architecture hardware support 3d graphics semi automated methodology automatic test program generation superscalar issue logic bypass multi bypass logic stall logic microarchitectural specification random test generation methodology;functional verification;microprocessors logic testing automatic logic units automatic control control systems logic design process design reduced instruction set computing hardware graphics;logic design;stall logic;reduced instruction set computing;random test generators;semi automated methodology;random testing;automatic programming;short processor design cycle;automatic generation;superscalar sh 4 microprocessor;microarchitectural specification;dual issue superscalar risc architecture;formal verification;computer testing;automatic programming computer testing integrated circuit testing logic design reduced instruction set computing formal verification;hardware support;automatic test program generation;integrated circuit testing;bypass multi bypass logic;random test generation methodology;modern complex processors;control logic design;sh4 processor;3d graphics;superscalar issue logic	Functional verification of modern complex processors is a formidable and time consuming task. In spite of substantial manual effort, it is extremely difficult to systematically cover the corner cases of the control logic design, within a short processor design cycle. The SH4 processor is a dual issue superscalar RISC architecture with extensive hardware support for 3D graphics. We present the development of a semi automated methodology for functional verification. In particular, we elaborate a scheme to automatically generate test programs to verify the superscalar issue logic, bypass/multi bypass logic and stall logic, starting from the microarchitectural specification. Finally, we present the Random Test Generation methodology and the specific Random Test Generators.	3d computer graphics;central processing unit;corner case;international symposium on computer architecture;microarchitecture;microprocessor;pipeline (computing);processor design;r4200;semiconductor industry;superh;superscalar processor;ueli maurer (cryptographer);wang tile	Prasenjit Biswas;Andy Freeman;Kouji Yamada;Norio Nakagawa;Kunio Uchiyama	1997	Proceedings IEEE COMPCON 97. Digest of Papers	10.1109/CMPCON.1997.584682	computer architecture;parallel computing;real-time computing;computer science;high-level verification;functional verification	Arch	7.765818050902019	52.18639503412751	149044
31819fb193b6af4c7e946105a631a98b71fcfaae	a survey oa survey on system-on-a-chip designn system-on-a-chip design	system level language system on a chip design language register level assembly language;hardware description languages;system on a chip;chip;hardware description languages system on chip circuit layout cad integrated circuit design;real time optimization;integrated circuit design;design technique;complex system;system on chip;system on a chip hardware design languages predictive models java computational modeling conferences real time systems system level design information analysis design automation;circuit layout cad	Advancement in the microelectronics era made it possible the integration of a complete yet complex system on a single chip. Over 10 million gates, integrated together and running a real-time optimized software red crossed classical design techniques. Register level will serve as an assembly language for the new design languages or so called system level languages. The problematic is to define a language that can allow the design of such a complex systems. In this paper, we explore different paradigms of state-of-the-art of the System-on-a-Chip (SoC) modeling and design. In particular, this paper presents the main proposals in defining a system level language and discusses their advantages and drawbacks.	system on a chip	Ali Habibi;Sofiène Tahar	2003		10.1109/IWSOC.2003.1213037	system on a chip;physical design;computer architecture;real-time computing;idef4;computer science;circuit design;design language;hardware description language;register-transfer level;computer engineering	EDA	6.728797217614366	52.363665561503126	149135
cbe5885b5a6ad1056ae2f6290517bcbece012f76	a bira using fault-free memory region for area reduction	random access memory;radiation detectors;maintenance engineering;computer architecture;redundancy;cams;algorithm design and analysis	As memory capacity and density grow, the test cost and yield improvement of embedded memories have become more crucial. For embedded memories, BIRA (built-in redundancy analysis) is widely used to improve yield. BIRA replaces faulty cells with spare cells. BIRA requires an extra hardware overhead since it needs to store and analyze faults. The most important factor in BIRA is the reduction of area overhead while keeping very high repair rate. Most of previous studies on BIRA utilize CAMs (content-addressable memories) for storing faulty memory addresses. The area overhead for CAM is not negligible, hence, a CAM is shared by many memory blocks to perform a repair process. With this BIRA architecture, each memory block needs to be analyzed one by one. This makes the CAM a bottleneck of improvement for area overhead and analysis speed in BIRA. In order to improve area overhead and analysis speed, we propose a BIRA architecture and RA (redundancy analysis) algorithm using a fault-free region in embedded memory instead of deploying extra CAMs. The proposed RA algorithm stores faulty cell addresses in the fault-free region and achieves a very high repair rate. The proposed BIRA brings a significant area reduction. In addition, it allows a parallel memory test by using its own memory instead of shared CAMs.	algorithm;content-addressable memory;embedded system;memory bank;memory tester;overhead (computing);system on a chip;traffic collision avoidance system	Chang-Hyun Oh;Sae-Eun Kim;Joon-Sung Yang	2016	2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2016.7804008	maintenance engineering;embedded system;algorithm design;parallel computing;real-time computing;computer science;engineering;operating system;redundancy;particle detector	EDA	7.437023325075171	60.247086439510085	149274
6c2122a24c8cb00be3b7cda1abccee779f34f1bc	fpga's middleware for software defined radio applications	field programmable gate array;field programmable gate arrays software radio middleware;software defined radio;building block;application program interface;software radio;hardware abstraction layer;middleware;field programmable gate arrays middleware software radio application software hardware energy consumption design optimization manufacturing irrigation high level languages;hardware abstraction layer middleware software defined radio applications high level languages application programming interfaces field programmable gate arrays;field programmable gate arrays;high level language	The division in several layers of the implementation of systems is a solution adopted to avoid complexity, provide flexibility and improve portability and code reusability through different hardware. Middleware (intermediate layer between two other layers) implementations are based on the use of increasingly high-level languages and application programming interfaces (API). The field programmable gate arrays (FPGA) world can also apply this approach to produce building blocks independent from hardware platforms and devices. This paper presents details of the implementation of a middleware, called platform and hardware abstraction layer (P-HAL) when applied to FPGA devices. It was specially designed for radio applications and allows designing specific functions independently of the hardware context where they are applied, thus providing flexibility to the so-called software radios employing FPGA devices.	abstraction layer;application programming interface;complexity;field-programmable gate array;hal;hardware abstraction;high- and low-level;middleware;software portability	Xavier Revés;Vuk Marojevic;Ramon Ferrús;Antoni Gelonch	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515794	embedded system;middleware;real-time computing;computer science;operating system;software-defined radio;middleware;hardware architecture;field-programmable gate array	EDA	3.2223013729314935	49.02048820085944	149366
5b25147a9b2b442fddcaa539e4619d58d6fd670e	synthesis of arithmetic expressions for the fixed-point arithmetic: the sardana approach	source coding fixed point arithmetic numerical analysis;image processing arithmetic expressions synthesis fixed point arithmetic sardana approach source codes numerical accuracy floating point expressions fixed point numbers digital filters;semantics;code synthesis;polynomials;compilers;compilers code synthesis fixed point formats abstract interpretation;numerical analysis;shape;abstracts;fixed point arithmetic;algorithm design and analysis abstracts shape polynomials semantics optimization;optimization;fixed point formats;abstract interpretation;algorithm design and analysis;source coding	Sardana is a tool which optimizes the arithmetic expressions present in source codes. The optimization is done by synthesizing automatically new mathematically equal expressions, given ranges of values for the variables. In previous work, Sardana has been used to optimize the numerical accuracy of floating-point expressions, by minimizing the worst roundoff error on the result of the evaluation. In this article, we show how our tool can be used to synthesize arithmetic expressions optimized for the fixed-point arithmetic. In this context, Sardana minimizes the number of bits required to represent without overflow the integer parts of the fixed-point numbers possibly occurring at any stage of the evaluation of an expression. We present experimental results showing how our tool optimizes the implementation of digital filters commonly used in image processing.	cobham's thesis;code;convolution;digital filter;fixed-point arithmetic;heuristic;image processing;local search (optimization);mathematical optimization;numerical analysis;round-off error;rounding;time complexity	Arnault Ioualalen;Matthieu Martel	2012	Proceedings of the 2012 Conference on Design and Architectures for Signal and Image Processing		arithmetic;discrete mathematics;theoretical computer science;mathematics	Visualization	6.871754229611678	46.94131566843253	149573
5d948dbef0907d928d901dafbf1193006cb0004b	cone of influence analysis at the electronic system level using machine learning	cone of influence;logic gates;machine learning;electronic engineering computing;learning artificial intelligence;source code cone of influence analysis electronic system level machine learning circuit signal design tasks register transfer level rtl gate level esl behavioral scheme system executions noninvasive cone of influence approximation;logic gates electronic engineering computing learning artificial intelligence;esl;logic gates approximation methods entropy libraries machine learning algorithms decision trees availability;cone of influence systemc esl machine learning;systemc	Cone of influence analysis, i.e. determining the parts of the circuit which are relevant to a considered circuit signal, is an established methodology applied in several design tasks. In abstractions like the Register Transfer Level (RTL) or the gate level, cone of influence analysis is simple. However, the introduction of higher levels of abstractions, particularly the Electronic System Level (ESL), made it significantly harder to reliably extract a cone of influence. In this paper, we propose a methodology that enables cone of influence analysis at the ESL. Instead of a structural analysis, a behavioral scheme is proposed, i.e. stimuli representing different system executions are analyzed. To this end, machine learning techniques are exploited. This enables a very good approximation of the desired cone of influence which is non-invasive, does not rely on the availability of the source code, and performs fast. Case studies confirm the applicability of the proposed approach.	approximation;cone;cone (formal languages);electronic system-level design and verification;machine learning;register-transfer level;structural analysis	Jannis Stoppe;Robert Wille;Rolf Drechsler	2013	2013 Euromicro Conference on Digital System Design	10.1109/DSD.2013.69	embedded system;logic gate;computer science;theoretical computer science;operating system;algorithm	EDA	4.6073135173673885	54.72907440178894	149662
7579db1b8083791a6f96fd3055e77a90a9508640	efficient embedded code generation with multiple load/store instructions		In a recent study, we discovered that many single load/store operations in embedded applications can be parallelized and thus encoded simultaneously in a single-instruction multiple-data instruction, called the multiple load/store (MLS) instruction. In this work, we investigate the problem of utilizing MLS instructions to produce optimized machine code, and propose an effective approach to the problem. Specifically, we formalize the MLS problem, that is, the problem of maximizing the use of MLS instructions with an unlimited register file size. Based on this analysis, we show that we can solve the problem efficiently by translating it into a variant of the problem finding a maximum weighted path cover in a dynamic weighted graph. To handle a more realistic case of the finite size of the register file, our solution is then extended to take into account the constraints of register sequencing in MLS instructions and the limited register resource available in the target processor. We demonstrate the effectiveness of our approach experimentally by using a set of benchmark programs. In summary, our approach can reduce the number of loads/stores by 13.3% on average, compared with the code generated from existing compilers. The total code size reduction is 3.6%. This code size reduction comes at almost no cost because the overall increase in compilation time as a result of our technique remains quite minimal. Copyright c © 2007 John Wiley & Sons, Ltd.	algorithm;benchmark (computing);code generation (compiler);embedded system;experiment;john d. wiley;load/store architecture;machine code;mathematical optimization;media processor;optimization problem;optimizing compiler;parallel computing;path cover;program optimization;register file;simd;time complexity	Yunheung Paek;Minwook Ahn;Doosan Cho;Taehwan Kim	2007	Softw., Pract. Exper.	10.1002/spe.801		PL	-0.9679485547641655	51.90624701244179	149711
8407bf47ed375c28a05bd2bb1a9f9c8f4d00cbe3	mlc nand flash memory	nand flash;non volatile memories;memory aging;fpga emulator	This work presents an FPGA-based emulator that can be used for emulating NAND Flash memories, either at the chip or at the channel level, along with the effect of aging on their performance. The emulator is based on a reconfigurable hardware-software architecture, which enables accurate representation of various NAND Flash technologies, focusing especially on MLC cases. The presented architecture can be used for emulating memories at the chip and channel level, while the proposed hardware platform can be used as a valuable tool for developing and evaluating memory-related algorithms and techniques. In this paper, we analyze the architecture of the NAND Flash memory emulator and we present details about its internal functionality. Using experimental results, we demonstrate the high accuracy achieved when it is used to emulate specific MLC and TLC NAND Flash chips and we describe how this custom hardware can be used to emulate a complete NAND Flash channel, which consists of multiple NAND Flash chips that share a common data path and support the execution of pipelined commands. 2015 Elsevier B.V. All rights reserved.	algorithm;bit error rate;field-programmable gate array;flash memory emulator;global variable;multi-level cell;real-time clock;software architecture;x.690	Antonios Prodromakis;Stelios Korkotsides;Theodore Antonakopoulos	2015	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2015.06.007	flash file system;embedded system;parallel computing;computer hardware;computer science;flash memory emulator	EDA	6.160711192216226	47.34491937890173	149808
a58bc02d1bde3350a7781b58b01ea4a8802201e5	trustworthy ics for secure embedded computing	integrated circuit;digital edition embedded systems security design test;embedded computing circuit testing embedded system integrated circuit testing communication system security power system security hardware communication system software embedded software software design;digital edition;communication system software;test;embedded system;embedded systems;system design;secure system;integrated circuit testing;design;circuit testing;security testing;software design;security;power system security;embedded computing;embedded software;communication system security;hardware	The design of secure and trusted embedded systems has recently drawn enormous attention from system-design practitioners. A secure system is only as strong as the weakest link. Therefore, any security functions implemented in an embedded system must be considered in both hardware and software, at all design abstraction levels, in communications between components, and in the manufacturing phase. In addition, these implementations are subject to typical power, performance, and cost constraints of consumer embedded systems. This issue of IEEE Design & Test features a special issue on Design and Test of Integrated Circuits for Secure Embedded Computing.	computer security;embedded system;integrated circuit;trustworthy computing;turing test	Kwang-Ting Cheng	2007	IEEE Design & Test of Computers	10.1109/MDT.2007.207	embedded system;design;real-time computing;embedded software;computer science;information security;software design;integrated circuit;security testing;computer engineering	EDA	9.165490347435322	54.49465894662757	149852
6ab1b5d56d652b8c500e02a1bfadd505c498709d	energy conscious simultaneous voltage scaling and on-chip communication bus synthesis	resource selection;on chip bus;nonlinear programming;energy efficient;interconnection network voltage scaling on chip communication bus technology scaling nonlinear programming;design space;chip;technology scaling;energy consumption;system design;integrated circuit interconnections;voltage scaling;voltage computer architecture topology system on a chip computational modeling energy consumption energy efficiency computational efficiency libraries embedded system;energy saving;nonlinear programming integrated circuit interconnections	Due to the ever increasing trend of system complexity and technology scaling, synthesizing on-chip communication architecture appears to be a challenging task for the system designers. The traditional approaches are mostly based on the simulation of an entire system. However, the resulting architecture may not fulfil the requirements such as the performance, energy, size etc. and the computational cost of simulation based techniques is infeasible when exploring a large design space. This paper presents a simultaneous on-chip communication bus synthesis and voltage scaling approach, which is modeled in NLP (nonlinear programming) and finds an energy efficient minimum number of bus(es) and an optimal size of bus width by simultaneously performing resource selection, scheduling, binding and voltage scaling of an on-chip bus. The voltages (supply and body bias) are scaled to reduce the total energy consumption of a bus by exploiting the slack of each on-chip module. The experimental results conducted on real-life examples, demonstrate the synthesis of an energy efficient communication bus with total energy saving up to 57.1% by scaling its supply and body bias voltages	algorithmic efficiency;bus (computing);computation;dynamic voltage scaling;image scaling;name binding;natural language processing;nonlinear programming;nonlinear system;real life;requirement;scheduling (computing);simulation;slack variable	Sujan Pandey;Tudor Murgan;Manfred Glesner	2006	2006 IFIP International Conference on Very Large Scale Integration	10.1109/VLSISOC.2006.313250	chip;embedded system;electronic engineering;real-time computing;slack bus;telecommunications;nonlinear programming;computer science;engineering;local bus;operating system;efficient energy use;systems design	EDA	-1.174996534451171	55.82725604538709	150024
eca779d4d67ca4ba84a2881985c130bd318adb13	retargetable graph-coloring register allocation for irregular architectures	irregularity;compilateur;information science;register allocation;implementation;graph coloring;systemvetenskap;optimizacion compiladora;compiler;registre;irregularite;prototipo;compiler optimization;irregularidad;implementacion;prototype;optimisation compilateur;registro;register;compilador;overlapping generations	Global register allocation is one of the most important optimizations in a compiler. Since the early 80’s, register allocation by graph coloring has been the dominant approach. The traditional formulation of graph-coloring register allocation implicitly assumes a single bank of non-overlapping general-purpose registers and does not handle irregular architectural features like overlapping register pairs, special purpose registers, and multiple register banks. We present a generalization of graph-coloring register allocation that can handle all such irregularities. The algorithm is parameterized on a formal target description, allowing fully automatic retargeting. We report on experiments conducted with a prototype implementation in a framework based on a commercial compiler.	algorithm;compiler;experiment;filter bank;general-purpose markup language;graph coloring;iar systems;interprocedural optimization;processor register;prototype;register allocation;retargeting	Johan Runeson;Sven-Olof Nyström	2003		10.1007/978-3-540-39920-9_17	compiler;parallel computing;information science;computer science;theoretical computer science;graph coloring;optimizing compiler;prototype;processor register;programming language;implementation;register allocation;overlapping generations model	PL	2.0099624383793806	51.170768391109284	150096
224da410b3a61dcf83b228c07c546819bdc02004	automapper: an automated tool for optimal hardware resource allocation for networking applications on fpga (abstract only)	resource allocation;networking;high level tool	It has now become imperative for routers to support complicated lookup schemes, based on the specific function of the networking hardware. It is no longer possible to ensure an optimal resource utilization using manual organization techniques due to the increasing complexity of lookup schemes, as well as the large number of potential implementation choices. We have developed an automated tool, AutoMapper, which can map lookup schemes onto a particular target architecture optimally, thereby providing a superior alternative to the time-consuming and resource inefficient technique of manual conversion. It is based on an Integer Linear Programming (ILP) formulation that is able to allocate the limited hardware resources for a single lookup scheme, while optimizing any of the three performance metrics of latency, throughput or power consumption. Accurate formulation of the objective function and the constraint equations guarantee optimality in terms of the chosen performance metric. We demonstrate the operation of the developed tool, by successfully mapping complex real world lookup schemes onto a state-of-the art FPGA device, with execution times being under a second on a dual-core computer with 4 GB of RAM, running at 2.40 GHz.	field-programmable gate array;imperative programming;integer programming;linear programming;lookup table;loss function;multi-core processor;networking hardware;optimization problem;random-access memory;router (computing);throughput	Swapnil Haria;Viktor K. Prasanna	2013		10.1145/2435264.2435335	embedded system;parallel computing;real-time computing;resource allocation;computer science;theoretical computer science;operating system;distributed computing	Networks	0.24625476379311023	50.49510823156023	150411
810ba25b7233de0ad48c5835c93eae2b1d6b43d0	a system-on-a-chip for pattern recognition architecture and design methodology	vlsi physical integration;formal specification;hardware software codesign;very large scale integration;specification;information filtering;vlsi characteristics system on a chip pattern recognition architecture design methodology specification heterogeneous architecture hardware software codesign system architecture vlsi physical integration;gabor filters;data mining;coprocessors;system on a chip;formal specification pattern recognition hardware software codesign vlsi;pattern recognition architecture;system on a chip pattern recognition design methodology gabor filters data mining frequency very large scale integration coprocessors information filtering information filters;pattern recognition;vlsi;heterogeneous architecture;vlsi characteristics;system architecture;frequency;information filters;design methodology	We address in this paper the design and specification of a heterogeneous architecture of a SOC (System-On-a-Chip) for pattern recognition. Once the algorithms involved presented, we investigate the hardware/software codesign methodology, the system architecture and finally the VLSI physical integration. We conclude by giving results on the performance of the system regarding recognition rate and VLSI characteristics.	pattern recognition;system on a chip	Mourad Aberbour;Habib Mehrez;François Durbin;Jacques Haussy;P. Lalande;André Tissot	2000		10.1109/CAMP.2000.875973	enterprise architecture framework;reference architecture;embedded system;software architecture;computer architecture;computer science;applications architecture;computer engineering	Vision	3.6813283857278445	51.98748191887004	150629
41ad95fe185928b2783ddeb59a3337ffbc913e8f	an adaptive and integrated low-power framework for multicore mobile computing		Employing multicore in mobile computing such as smartphone and IoT (Internet of Things) device is a double-edged sword. It provides ample computing capabilities required in recent intelligent mobile services including voice recognition, image processing, big data analysis, and deep learning. However, it requires a great deal of power consumption, which causes creating a thermal hot spot and putting pressure on the energy resource in amobile device. In this paper, we propose a novel framework that integrates two well-known low-power techniques, DPM (Dynamic PowerManagement) and DVFS (Dynamic Voltage and Frequency Scaling) for energy efficiency in multicore mobile systems.The key feature of the proposed framework is adaptability. By monitoring the online resource usage such as CPU utilization and power consumption, the framework can orchestrate diverse DPM and DVFS policies according to workload characteristics. Real implementation based experiments using three mobile devices have shown that it can reduce the power consumption ranging from 22% to 79%, while affecting negligibly the performance of workloads.	big data;central processing unit;deep learning;dynamic frequency scaling;dynamic voltage scaling;experiment;hotspot (wi-fi);image processing;internet of things;low-power broadcasting;mobile computing;mobile device;multi-core processor;smartphone;speech recognition	Jongmoo Choi;Bumjong Jung;Yongjae Choi;Seiil Son	2017	Mobile Information Systems	10.1155/2017/9642958	computer science;computer network;adaptability;frequency scaling;big data;distributed computing;multi-core processor;dynamic demand;real-time computing;ranging;mobile device;mobile computing	Mobile	-4.302598175408472	58.571017287889255	151021
3ddde0b1de7f56c38fb125e36a62b016a2ad37d8	synchroscalar: a multiple clock domain, power-aware, tile-based embedded processor	simd control;power efficiency ofasics;multiple clock domain;power efficiency;synchroscalar includingspice simulation;columnsof processor tile;forembedded processing;flexibilityof dsps;cycle-level simulation;power consumption;general-purpose microprocessor design;sequential consistency;computational modeling;parallel processing;low power electronics;digital signal processing;process design;low frequency;signal processing;embedded systems;embedded processor;application specific integrated circuits;energy efficient;message passing	We present Synchroscalar, a tile-based architecture forembedded processing that is designed to provide the flexibilityof DSPs while approaching the power efficiency ofASICs. We achieve this goal by providing high parallelismand voltage scaling while minimizing control and communicationcosts. Specifically, Synchroscalar uses columnsof processor tiles organized into statically-assignedfrequency-voltage domains to minimize power consumption.Furthermore, while columns use SIMD control to minimizeoverhead, data-dependent computations can besupported by extremely flexible statically-scheduled communicationbetween columns.We provide a detailed evaluation of Synchroscalar includingSPICE simulation, wire and device models, synthesisof key components, cycle-level simulation, andcompiler- and hand-optimized signal processing applications.We find that the goal of meeting, not exceeding, performancetargets with data-parallel applications leads todesigns that depart significantly from our intuitions derivedfrom general-purpose microprocessor design. Inparticular, synchronous design and substantial global interconnectare desirable in the low-frequency, low-powerdomain. This global interconnect supports parallelizationand reduces processor idle time, which are critical to energyefficient implementations of high bandwidth signalprocessing. Overall, Synchroscalar provides programmabilitywhile achieving power efficiencies within 8-30X ofknown ASIC implementations, which is 10-60X better thanconventional DSPs. In addition, frequency-voltage scalingin Synchroscalar provides between 3-32% power savingsin our application suite.	application-specific integrated circuit;clock signal;column (database);computation;data dependency;digital signal processor;dynamic voltage scaling;embedded system;general-purpose markup language;image scaling;performance per watt;power domains;processor design;simd;signal processing;simulation;synchronous circuit	John Oliver;Ravishankar Rao;Paul Sultana;Jedidiah R. Crandall;Erik Czernikowski;IV W. Jones LeslieW.Jones;Diana Franklin;Venkatesh Akella;Frederic T. Chong	2004	Proceedings. 31st Annual International Symposium on Computer Architecture, 2004.		process design;embedded system;parallel processing;computer architecture;parallel computing;message passing;real-time computing;electrical efficiency;computer science;operating system;digital signal processing;signal processing;efficient energy use;application-specific integrated circuit;low frequency;computational model;sequential consistency;low-power electronics	Arch	-1.418475424478415	54.912591691983366	151106
8b9d9ff7ccff9462ae3b784f634f765f1b3218a2	machine learning approach to generate pareto front for list-scheduling algorithms	machine learning;list scheduling;design space exploration	List Scheduling is one of the most widely used techniques for scheduling due to its simplicity and efficiency. In traditional list-based schedulers, a cost/priority function is used to compute the priority of tasks/jobs and put them in an ordered list. The cost function has been becoming more and more complex to cover increasing number of constraints in the system design. However, most of the existing list-based schedulers implement a static priority function that usually provides only one schedule for each task graph input. Therefore, they may not be able to satisfy the desire of system designers, who want to examine the trade-off between a number of design requirements (performance, power, energy, reliability ...). To address this problem, we propose a framework to utilize the Genetic Algorithm (GA) for exploring the design space and obtaining Pareto-optimal design points. Furthermore, multiple regression techniques are used to build predictive models for the Pareto fronts to limit the execution time of GA. The models are built using training task graph datasets and applied on incoming task graphs. The Pareto fronts for incoming task graphs are produced in time 2 orders of magnitude faster than the traditional GA, with only 4% degradation in the quality.	elegant degradation;genetic algorithm;job stream;list scheduling;loss function;machine learning;optimal design;pareto efficiency;predictive modelling;requirement;run time (program lifecycle phase);scheduling (computing);software release life cycle;systems design	Pham Nam Khanh;Akash Kumar;Khin Mi Mi Aung	2016		10.1145/2906363.2906380	list update problem;embedded system;mathematical optimization;real-time computing;computer science;theoretical computer science;operating system;self-organizing list	AI	0.7005590655281267	55.95806160223696	151132
953c26c4eaef31e91e374b86b06df1f6cd95d52e	optimising reconfigurable systems for real-time applications	tesla c2070;paper;thesis or dissertation;mixed precision;fpga;cuda;thesis;machine learning;package;nvidia;algorithms;computer science	This thesis addresses the problem of designing real-time reconfigurable systems. Our first contribution of this thesis is to propose novel data structures and memory architectures for accelerating real-time proximity queries, with potential application to robotic surgery. We optimise performance while maintaining accuracy by several techniques including mixed precision, function transformation and streaming data flow. Significant speedup is achieved using our reconfigurable system over double-precision CPU, GPU and FPGA designs. The second contribution of this thesis is an adaptation methodology for real-time sequential Monte Carlo methods. Adapting to workload over time, different configurations with various performance and power consumption trade-offs are loaded onto the FPGAs dynamically. Promising energy reduction has been achieved in addition to speedup over CPU and GPU designs. The approach is evaluated in an application to robot localisation. The third contribution of this thesis is a design flow for automated mapping and optimisation of real-time sequential Monte Carlo methods. Machine learning algorithms are used to search for an optimal parameter set to produce the highest solution quality while satisfying all timing and resource constraints. The approach is evaluated in an application to air traffic management.	algorithm;cartography;central processing unit;data structure;dataflow;double-precision floating-point format;field-programmable gate array;graphics processing unit;machine learning;mathematical optimization;memory architecture;monte carlo method;real-time clock;real-time transcription;reconfigurable computing;robot;speedup;stream (computing)	Thomas C. P. Chau	2014			computational science;parallel computing;computer science;theoretical computer science	EDA	0.17388249111845047	47.55275781169453	151137
467253431c269aae9c19df500501c89ebf55145d	an evolvable combinational unit for fpgas	field programmable gate array;evolutionary design;evolvable hardware;combinational circuit	A complete hardware implementation of an evolvable combinational unit for FPGAs is presented. The proposed combinational unit consisting of a virtual reconfigurable circuit and evolutionary algorithm was described in VHDL independently of a target platform, i.e. as a soft IP core, and realized in the COMBO6 card. In many cases the unit is able to evolve (i.e. to design) the required function automatically and autonomously, in a few seconds, only on the basis of interactions with an environment. A number of circuits were successfully evolved directly in the FPGA, in particular, 3-bit multipliers, adders, multiplexers and parity encoders. The evolvable unit was also tested in a simulated dynamic environment and used to design various circuits specified by randomly generated truth tables.	arithmetic logic unit;autonomous robot;combinational logic;encoder;evolutionary algorithm;evolvable hardware;field-programmable gate array;interaction;multiplexer;parity bit;pipeline (computing);procedural generation;randomness;vhdl	Lukás Sekanina;Stepan Friedl	2004	Computers and Artificial Intelligence		embedded system;parallel computing;real-time computing;computer science;combinational logic;algorithm;field-programmable gate array	EDA	9.655899390271337	48.1902000408766	151252
30b6b1e5f8f0ede95b053057732375303b14eb99	caas: core as a service realizing hardware sercices on reconfigurable mpsocs	image coding;data compression;reconfigurable architectures;hardware field programmable gate arrays programming transform coding service oriented architecture computer architecture program processors;logic circuits;system on chip data compression field programmable gate arrays image coding logic circuits microprocessor chips multiprocessing systems reconfigurable architectures service oriented architecture;system on chip;multiprocessing systems;field programmable gate arrays;service oriented architecture;microprocessor chips;dynamic reconfigurable technique caas hardware services reconfigurable mpsocs service oriented architecture soa core as a service high level programming paradigms reconfigurable multiprocessor system on chip platform mpsoc platform high level parallelization structural programming model jpeg application embedded processors ip cores	Service-oriented architecture (SOA) has been proved as an efficient way for high level programming paradigms. This paper realizes services into reconfigurable MPSoC to organize CaaS: a core as a service framework, which implements hardware services on state-of-the-art reconfigurable multi-processor system-on-chip (MPSoC) platform for high level parallelization. The integration of SOA concepts can provide structural programming models to ease the burden of high level programming. For demonstration, a prototype with JPEG application has been built on an FPGA, regarding embedded processors and IP cores as computing servants. The experimental results demonstrate the CaaS can achieve high flexibility with dynamic reconfigurable techniques.	central processing unit;embedded system;field-programmable gate array;high-level programming language;jpeg;mpsoc;multiprocessing;parallel computing;programming paradigm;prototype;service-oriented architecture;service-oriented device architecture;system on a chip	Chao Wang;Xi Li;Junneng Zhang;Peng Chen;Xuehai Zhou	2012	22nd International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2012.6339263	data compression;system on a chip;embedded system;computer architecture;parallel computing;real-time computing;logic gate;reconfigurable computing;computer science;service-oriented architecture;field-programmable gate array	EDA	3.6680696919111138	49.325307477396834	151281
b67ffa4485c7de0a9c8e32bda3be01583449d07a	a very fast trace-driven simulation platform for chip-multiprocessors architectural explorations		Simulation is the main tool for computer architects and parallel application developers for developing new architectures and parallel algorithms on many-core machines. Simulating a many-core architecture represent a challenge to software simulators even with parallelization of these SW on multi-cores. Field Programmable Gate Arrays offer an excellent implementation platform due to inherent parallelism. Existing FPGA-based simulators however, are mostly execution-driven which consumes too many FPGA resources. Hence, they still trade-off accuracy with simulation speed as SW simulators do. In this work, an application-level trace-driven FPGA-based many-core simulator is presented. A parameterized Verilog template was developed that can generate any number of simulator tiles. The input trace has an architecturally agnostic format that is directly interpreted by the FPGA-based timing model to re-construct the execution events of the original application with accurate timing. This allows fitting a large number of simulation tiles on a single FPGA without sacrificing simulation speed or accuracy. Experimental results show that the simulator's average accuracy is ∼14 percent with simulation speeds ranging from 100’s of MIPs to over 2,200 MIPS for a 16-core target architecture. Hence, with accuracy similar to SW simulators, its speed is higher than all other FPGA-based simulators.	clock rate;computer architecture simulator;field-programmable gate array;intel core (microarchitecture);kernel (operating system);manycore processor;operating system;parallel algorithm;parallel computing;protection ring;shattered world;simulation;thread (computing);verilog	Muhammad E. S. Elrabaa;Ayman Hroub;Muhamed F. Mudawar;Amran Al-Aghbari;Mohammed A. Al-Asli;Ahmad Khayyat	2017	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2017.2713782	real-time computing;computer science;verilog;field-programmable gate array;chip;multithreading;parallel algorithm;multi-core processor;software;ranging	HPC	-1.1513124517966744	46.74367651939545	151339
57c966d8033c35b51e0d57ae9dd34d0d1edb9f37	interconnect networks for resistive computing architectures	frequency modulation;multiplexing;computer architecture;micromechanical devices;adders;adders micromechanical devices computer architecture multiplexing integrated circuits frequency modulation;integrated circuits	Today's computing systems suffer from a memory/communication bottleneck, resulting in high energy consumption and saturated performance. This makes them inefficient in solving data-intensive applications at reasonable cost. Computation-In-Memory (CIM) architecture, based on the integration of storage and computation in the same physical location using non-volatile memristor crossbar technology, offers a potential solution to the memory bottleneck. An efficient interconnect network is essential to maximize CIM's architectural performance. This paper presents three interconnect network schemes for CIM architecture; these are (1) CMOS-based, (2) memristor-based and (3) hybrid cmos/memristor interconnect network scheme. To illustrate the feasibility of such schemes, a CIM parallel adder is used as a case study. The results show that the hybrid interconnect network scheme achieves a higher performance in comparison with the CMOS-based and memristor-based interconnect scheme in terms of delay, energy and area.	adder (electronics);bottleneck (engineering);cmos;computation;computer-integrated manufacturing;crossbar switch;data-intensive computing;memristor;non-volatile memory;von neumann architecture	Hoang Anh Du Nguyen;Lei Xie;Jintao Yu;Mottaqiallah Taouil;Said Hamdioui	2017	2017 12th International Conference on Design & Technology of Integrated Systems In Nanoscale Era (DTIS)	10.1109/DTIS.2017.7929872	embedded system;electronic engineering;parallel computing;computer science;interconnect bottleneck	EDA	3.852021635291336	58.411167390976836	151379
29f1859f5c991c4629f20765d642ebfc59d4a771	automatic generation of application-specific architectures for heterogeneous multiprocessor system-on-chip	vlsi layout editor;mipmapping;texture mapping;design flow;coprocessors;automatic generation;coprocessors application specific integrated circuits multiprocessing systems computer architecture high level synthesis integrated circuit design;computer architecture;high level synthesis;integrated circuit design;community networks;application specific integrated circuits;multiprocessor architecture;communication coprocessor application specific architectures heterogeneous multiprocessor system on chip design flow architectural parameters high level system specification memory modules communication networks;multiprocessor system on chip;multiprocessing systems;multiprocessing systems communication networks digital signal processing coprocessors application specific processors switching circuits packet switching communication switching design automation permission	We present a design flow for the generation of application-specific multiprocessor architectures. In the flow, architectural parameters are first extracted from a high-level system specification. Parameters are used to instantiate architectural components, such as processors, memory modules and communication networks. The flow includes the automatic generation of communication coprocessor that adapts the processor to the communication network in an application-specific way. Experiments with two system examples show the effectiveness of the presented design flow.	central processing unit;coprocessor;dimm;design flow (eda);experiment;high- and low-level;mpsoc;multiprocessing;system on a chip;telecommunications network	Damien Lyonnard;Sungjoo Yoo;Amer Baghdadi;Ahmed Amine Jerraya	2001		10.1145/378239.379015	texture mapping;embedded system;computer architecture;parallel computing;real-time computing;computer science;design flow;operating system;application-specific integrated circuit;high-level synthesis;mipmap;coprocessor;integrated circuit design	EDA	3.5770549866179393	52.10217796279925	151639
19805f70c6b6b194c19f76efb141ff3ca4100d31	speeding up processing with approximation circuits	pipeline processing microprocessor chips performance evaluation circuit simulation synchronisation;performance evaluation;synchronous system;synchronisation;circuit simulation;circuits delay frequency clocks pipelines microprocessors logic adders performance gain parallel processing;functional requirement;performance processing speedup approximation circuits microprocessors synchronization variable pipeline delays clock frequency simulations;data transfer;pipeline processing;microprocessor chips	Current microprocessors employ a global timing reference to synchronize data transfer. A synchronous system must know the maximum time needed to compute a function, but a circuit usually finishes computation earlier than the worst-case delay. The system nevertheless waits for the maximum time bound to guarantee a correct result. As a first step in achieving variable pipeline delays based on data values, approximation circuits can increase clock frequency by reducing the number of cycles a function requires. Instead of implementing the complete logic function, a simplified circuit mimics it using rough calculations to predict results. The results are correct most of the time, and simulations show improvements in overall performance in spite of the overhead needed to recover from mistakes.	approximation;best, worst and average case;boolean algebra;clock rate;computation;microprocessor;overhead (computing);simulation;synchronous circuit;waits	Shih-Lien Lu	2004	Computer	10.1109/MC.2004.1274006	synchronization;parallel computing;real-time computing;computer hardware;telecommunications;computer science;operating system;functional requirement	EDA	7.80689108896345	49.550698164712095	151847
9f5eabd438b278a3daf540b7d856c59342705911	portable, scalable, per-core power estimation for intelligent resource management	chip multiprocessor systems;power estimation;real time;power efficiency;resource manager;hidden markov models monitoring;per core power estimation;temperature sensor;chip;chip multiprocessor systems per core power estimation intelligent resource management power efficiency thermal constraints real time power consumption application independent models;power aware computing;first order;intelligent resource management;settore ing inf 05 sistemi di elaborazione delle informazioni;power aware computing microprocessor chips multiprocessing systems;multiprocessing systems;power consumption;application independent models;power modeling;microprocessor chips	Performance, power, and temperature are now all first-order design constraints. Balancing power efficiency, thermal constraints, and performance requires some means to convey data about real-time power consumption and temperature to intelligent resource managers. Resource managers can use this information to meet performance goals, maintain power budgets, and obey thermal constraints. Unfortunately, obtaining the required machine introspection is challenging. Most current chips provide no support for per-core power monitoring, and when support exists, it is not exposed to software. We present a methodology for deriving per-core power models using sampled performance counter values and temperature sensor readings. We develop application-independent models for four different (four- to eight-core) platforms, validate their accuracy, and show how they can be used to guide scheduling decisions in power-aware resource managers. Model overhead is negligible, and estimations exhibit 1.1%–5.2% per-suite median error on the NAS, SPEC OMP, and SPEC 2006 benchmarks (and 1.2%–4.4% overall).	first-order predicate;introspection;openmp;overhead (computing);performance per watt;real-time transcription;scalability;scheduling (computing);software development	Bhavishya Goel;Sally A. McKee;Roberto Gioiosa;Karan Singh;Major Bhadauria;Marco Cesati	2010	International Conference on Green Computing	10.1109/GREENCOMP.2010.5598313	embedded system;parallel computing;real-time computing;engineering	HPC	-3.5814454327254075	56.67480962229283	151866
f0bea103eff27db8ef315c352ca3ea44f2e7c7d1	dependability aspects in capacitive angular measurement systems	fault tolerant;automobile manufacture;closed loop systems;closed loop measurement system capacitive angular measurement systems smart sensors fault tolerance fault analysis capacitive angular sensor automotive applications system robustness self calibrating carrier frequency measurement principle drift effects electromagnetic disturbances data processing algorithms capacitive sensors assembly faults electromagnetic interference effects;capacitive sensor;measurement system;data processing;angular measurement;frequency measurement;smart sensor;robustness capacitive sensors semiconductor device measurement intelligent sensors production costs fault tolerance automotive applications degradation electromagnetic measurements;chip;stability;interference suppression;fault analysis;system design;fault tolerance;electromagnetic interference;production cost;computerised monitoring;effective degradability;calibration;computerised monitoring angular measurement capacitive sensors intelligent sensors fault tolerance automobile industry stability frequency measurement automobile manufacture closed loop systems calibration electromagnetic interference interference suppression;automobile industry;intelligent sensors;capacitive sensors	Today's smart sensors allow for the implementation of ample computing resources to improve their performance without increasing chip area and thus production costs significantly. What makes them more complex and at first sight less robust is in fact also a chance to improve their fault tolerance. In this paper we present a fault analysis for a capacitive angular sensor designed for automotive applications. Based on the assessment of potential faults and effects degrading the performance, we discuss constructive and operative measures and design decisions that have been taken to enhance the robustness of the system. A key component is a self-calibrating carrier frequency measurement principle, which is used to compensate common drift effects as well as to filter out electromagnetic disturbances. Electrical faults can be detected and lead to a fail stop behavior of the system. It is shown how these fault tolerance mechanisms are implemented in both a passive (i.e., by virtue of system design) and active way through appropriate data processing algorithms. The results of the case study are considered typical for capacitive sensors, but seem applicable also to similar problems.	angularjs;dependability;system of measurement	Thilo Sauter;Herbert Nachtnebel;Nikolaus Kerö	2003		10.1109/ETFA.2003.1248738	control engineering;embedded system;fault tolerance;electronic engineering;data processing;telecommunications;computer science;engineering;electrical engineering;capacitive sensing	Metrics	9.485958068626582	59.24786919062419	151944
afe1064847a23040161f2ff739127759b54e5956	a 144-configuration context mems optically reconfigurable gate array	s meu associated temporary failures 144 configuration context mems optically reconfigurable gate array fpga hardware update functions software repair space radiation environment high energy charged particles multievent latch up multievent upset s mel associated troubles;optical logic failure analysis field programmable gate arrays micro optomechanical devices optical arrays;micromechanical devices logic gates ultraviolet sources lighting;failure analysis;optical arrays;optical logic;field programmable gate arrays;micro optomechanical devices	Demand for space uses of FPGAs is increasing to support hardware repair and hardware update functions in addition to software repair and update functions in spacecraft, satellites, space stations, and other applications. However, under a space radiation environment, the incidence of high-energy charged particles causes single or multi-event latch-up (S/MEL)-associated troubles and single or multi-event upset (S/MEU)-associated temporary failures. Although an FPGA, because of its programmability, presents the advantageous capabilities of recovering from and updating after S/MEL-associated troubles, the FPGA can not guard itself completely from S/MEU-associated temporary failures that might arise on its configuration SRAM. This paper therefore presents a proposal for a 144-configuration context MEMS optically reconfigurable gate array that can support a remotely updatable hardware function, can quickly repair S/MEL-associated permanent failures, and can perfectly guard itself from S/MEU-associated temporary failures that can occur in a space radiation environment.	field-programmable gate array;incidence matrix;latch-up;microelectromechanical systems;static random-access memory	Yuichiro Yamaji;Minoru Watanabe	2011	2011 IEEE International SOC Conference	10.1109/SOCC.2011.6085083	embedded system;failure analysis;electronic engineering;real-time computing;computer science;engineering;electrical engineering;field-programmable gate array	EDA	8.955546001988571	59.370097513389595	152189
450bc1ca8d3f77b424895309d5625a12432b53b5	advances in adaptive signal processing: totally adaptive systems	real time;adaptive algorithm;a priori knowledge;adaptive signal processing;efficient implementation;application specific integrated circuit;signal processing;adaptive system;optimal design;genetic algorithm;hardware implementation;reconfigurable hardware	The design of traditional adaptive systems in signal processing applications has been strongly influenced by the need to provide an efficient implementation. At high bandwidths this has tended to be an ASIC (Application Specific Integrated Circuit). This, together with the need to make adaptation as reliable as possible has led to the use of as much a-priori knowledge as possible in the design. The result is often a non-optimal design. This paper examines an alternative strategy which removes many of the a-priori constraints and targets an implementation based on a real-time reconfigurable hardware	adaptive system;application-specific integrated circuit;field-programmable gate array;optimal design;real-time clock;real-time computing;signal processing	Colin Cowan;Roger F. Woods;Jean-Paul Heron;P. Power;F. J. Sweeney	1998	Annual Reviews in Control	10.1016/S1367-5788(01)00006-2	adaptive filter;control engineering;electronic engineering;real-time computing;a priori and a posteriori;genetic algorithm;reconfigurable computing;space-time adaptive processing;computer science;artificial intelligence;optimal design;adaptive system;signal processing;application-specific integrated circuit	EDA	5.702859193584704	47.48550665848268	152306
fd99fdfdf2f74f2b73f717778562579992c920f1	heuristic synthesis of microprogrammed computer architecture	manual tuning;dynamic microprogrming heuristic tuning manual tuning microprogram optimization;dynamic microprogrming;computer architecture;heuristic tuning;microprogram optimization;iteration method	This paper describes an algorithm for the synthesis of applications-oriented microcode for a dynamically microprogrammable computer. The synthesis algorithm provides an iterative method for generating specialized architectures. Current attempts at generating specialized architectures can be considered as manual tuning due to human generation of specialized microcode. Heuristic instruction synthesis is described as one phase of a heuristic tuning process which attempts to automate the manual tuning process.	algorithm;computer architecture;heuristic;iterative method;microcode;owner's manual	Abd-Elfattah Mohamed Abd-alla;David Carl Karlgaard	1974	IEEE Transactions on Computers	10.1109/T-C.1974.224036	computer architecture;parallel computing;computer science;theoretical computer science;iterative method	Arch	0.618660997817979	51.653408975623435	152406
256cd289a4644841c8b2c50fa3e69ee5259ef45a	targeting the motorola risc compiler for the powerpc architecture	c language reduced instruction set computing program compilers fortran;reduced instruction set computing optimal scheduling microprocessors optimizing compilers debugging tree data structures assembly systems power generation computer languages availability;fortran motorola risc compiler powerpc architecture c;reduced instruction set computing;c language;fortran;program compilers	Compiler technology has proven to be an integral part of overall performance in RISC systems. Motorola is providing highly optimizing C and FORTRAN compilers as part of the overall solution for the PowerPC and 88000 RISC architectures. This paper gives an introduction to the Motorola RISC compiler technology and describes how the technology was extended to support the PowerPC architecture in addition to the 88000. >	compiler;powerpc	Julie Shipnes	1994		10.1109/CMPCON.1994.282892	reduced instruction set computing;computer architecture;parallel computing;powerpc;computer science;compiler construction;optimizing compiler;programming language;power architecture	Arch	6.924113646634653	51.31134555555461	152437
cc2e9dc64eb2179dfb2585ae593579a25a9e6755	testing dsp cores based on self-test programs	integrated circuit testing automatic testing digital signal processing chips;automatic testing;circuit design;reconfigurable logic;system on a chip;emerging technology;chip;large scale;automatic testing digital signal processing built in self test system testing microprocessors semiconductor device testing system on a chip intellectual property data engineering systems engineering and theory;conservative parallel vhdl simulation;parallel discrete event simulation;pdes;integrated circuit testing;digital signal processing chips;fault coverage;fault coverage dsp core testing self test program datapath random pattern structural coverage testability metric;technological change	"""This paper presents a new method for the testing of the datapath of DSP cores based on self-test program. During the test, random patterns are loaded into the core, exercise different components of the core, and then are loaded out of the core for observation under the control of the self-test programs. We propose a systematic approach to generate the self-test program based on two metrics. One is the structural coverage and the other is the testability metric. Experimental results show the self-test program obtained by this approach can reach very high fault coverage in programmable core testing. A Systematic Analysis of Reuse Strategies for Design of Electronic Circuits83590292abs.htm Manfred Koegst, Dieter Garte Fraunhofer-Institut für Integrierte Schaltungen, EAS DresdenPeter Conradi, , Michael WahlUniversität-GH SiegenIn this paper a number of reuse approaches for circuit design are analysed. Based on this analysis an algebraic core model for discussion of a general reuse strategy is proposed. Using this model, the aim is to classify different reuse approaches for circuit design, to compare the applied terms and definitions, and to formulate classes of typical reuse tasks. In a practical application with focus on retrieval and parameterisation techniques, this model is on the way to being applied to DSP design issues. Reconfigurable Logic for Systems on a Chip83590340abs.htm W. Shields NeelyNational SemiconductorThe electronic systems of the future will be implemented in terms of multi-million gate """"systems on a chip"""". These systems will require an enormous investment in design and manufacturing; yet the pace of technological change (e.g., new algorithm development, new processor and memory designs) and ever changing requirements puts them in danger of obsolescence soon after they are created - applications always want to take advantage of new technical advances and must meet changed requirements. What is needed are single chip systems that are designed to be adaptable to a family of applications. The emerging technology of configurable logic offers the promise of large-scale silicon systems that are adaptive after manufacture, with little or no sacrifice in execution efficiency compared to hard-wired systems."""	algorithm;circuit design;code coverage;datapath;digital signal processor;fault coverage;linear algebra;reconfigurable computing;requirement;silicon systems;system on a chip	Wei Zhao;Christos A. Papachristou	1998		10.1109/DATE.1998.655852	chip;system on a chip;embedded system;technological change;electronic engineering;real-time computing;fault coverage;telecommunications;computer science;engineering;electrical engineering;operating system;circuit design;programming language;emerging technologies;algorithm	EDA	8.923273954209082	53.1409728122716	152727
2df31d915f8e6bb708652b0f8d478ff412ad89b9	training for optimizing transfer between word processors			microprocessor;optimizing compiler	Clare Pollock	1988				Arch	-4.400704815681047	52.35718828569481	152773
3b4adb6d6e29cc89016d57b1312f6291f67b06b0	educational use of field programmable gate arrays	field programmable gate array;logic design;digital logic	Traditionally, digital logic designs in undergraduate courses are described on paper and implemented with TTL SSI/MSI components. These standard logic devices have proven to be an inexpensive approach, but require “wirewrapping” and other similar means of circuit board assembly; as a result, a significant portion of the studentsu0027 effort is focused on completing and debugging the physical connections between devices. This paper describes an alternative approach using Field Programmable Gate Arrays that avoids these assembly issues, allowing the students to focus on the logic design process, with the added benefit of exposing the students to the use of modem CAE tools.	field-programmable gate array	David C.-L. Lam	1994		10.1007/3-540-58419-6_99	erasable programmable logic device;gate array;boolean algebra;embedded system;logic synthesis;macrocell array;logic gate;logic family;programmable logic array;computer science;gate equivalent;programmable logic device;nand gate;complex programmable logic device;simple programmable logic device;inverter;programmable array logic;field-programmable gate array;resistor–transistor logic	EDA	9.563816915077998	51.70466494748542	152860
bb86ec4c8150c40708cc705c005eed4ff0a460fb	an approach to design flow management in cad frameworks	verification;logic design;design flow;data management;silicon compilation;register transfer;verifiable silicon compiler;normalization;interchange format;extraction;flow graph	The approach described in this paper is based on a parallel that can be established between the design flow and the logic design domains. The paper describes how these mappings can be used to apply the existing techniques and algorithms of the logic design process to the problem of design flow management, thus providing solutions for design flow specification. representation, synthesis. flow data management and flow execution, in CAD frameworks. A 'design flow' view in EDIF has been proposed. It can be used as a standard interchange format for migrating flows across frameworks.	algorithm;computer-aided design;design flow (eda)	Mahesh Mehendale	1991			real-time computing;computer science;theoretical computer science;database	EDA	6.024632661211597	51.982933225127674	153001
1683f9ee046c4ffe3c944af70e1b5a19299ccc7f	cobra: a comprehensive bundle-based reliable architecture	hardware built in self test computer architecture reliability scheduling fault detection;parallel architectures;parallel architectures multiprocessing systems;multiprocessing systems;runtime fault detection techniques cobra comprehensive bundle based reliable architecture faulty processors computer architectures faulty transistors finegrained hardware reconfiguration fully decoupled control logic service based architecture multiprogrammed spec cpu2006 workloads	The declining robustness of transistors and their ever-denser integration threatens the dependability of future microprocessors. Classic multicores offer a simple solution to overcome hardware defects: faulty processors can be disabled without affecting the rest of the system. However, this approach becomes quickly an impractical solution at high fault rates. Recently, distributed computer architectures have been proposed to mitigate the effects of faulty transistors by utilizing finegrained hardware reconfiguration, managed by fully decoupled control logic. Unfortunately, such solutions trade flexibility for execution locality, and thus they do not scale to large systems. To overcome this issue we propose Cobra, a distributed, scalable, highly parallel reliable architecture. Cobra is a service-based architecture where groups of dynamic instructions flow independently through the system, making use of the available hardware resources. Cobra organizes the system's units dynamically using a novel resource assignment that preserves locality and limits communication overhead. Our experiments show that Cobra is extremely dependable, and outperforms classic multicores when subjected to 5 or more defects per 100 million transistors. We also show that Cobra operates 80% faster than previous state-of-the-art solutions on multi-programmed SPEC CPU2006 workloads and it improves cache hit rate by up to 62%. Our runtime fault detection techniques have a performance impact of only 3%.	algorithm;cache (computing);central processing unit;cobra;computer architecture;dependability;distributed computing;experiment;fault detection and isolation;locality of reference;memory organisation;microprocessor;overhead (computing);robustness (computer science);scalability;semiconductor research corporation;spec#;transistor;type system	Andrea Pellegrini;Valeria Bertacco	2013	2013 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS)	10.1109/SAMOS.2013.6621131	embedded system;parallel computing;real-time computing;computer science	Arch	5.627140532657844	59.231432105623334	153127
f44387794cfa00f460c46d6e881070b5cc9e052a	system level performance assessment of soc processors with systemc	performance evaluation;reduced instruction set computing;16 bit system level performance assessment systemc system on chip processors c c executable model finite state machines 16 bit reduced instruction set processor;performance of systems;performance metric;chip;16 bit reduced instruction set processor;finite state machines;system on chip processors;system on chip;16 bit;system level performance assessment;model of computation;power consumption;measurement computational modeling circuit simulation hardware process design application software energy consumption power system modeling very large scale integration switches;system on chip microprocessor chips performance evaluation reduced instruction set computing;finite state machine;c c executable model;performance assessment;microprocessor chips;systemc	This paper presents a system level methodology for modeling, and analyzing the performance of system-on-chip (SOC) processors. The solution adopted focuses on minimizing assessment time by modeling processors behavior only in terms of the performance metrics of interest. Formally, the desired behavior is captured through a C/C++ executable model, which uses finite state machines (FSM) as the underlying model of computation (MOC). To illustrate and validate our methodology we applied it to the design of a 16-bit reduced instruction set (RISC) processor. The performance metrics used to assess the quality of the design considered are power consumption and execution time. However, the methodology can be extended to any performance metric. The results obtained demonstrate the robustness of the proposed method both in terms of assessment time and accuracy	16-bit;c++;central processing unit;executable;finite-state machine;model of computation;run time (program lifecycle phase);system on a chip;systemc	Claudio Talarico;Min-Sung Koh;Esteban Rodriguez-Marek	2007	14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)	10.1109/ECBS.2007.69	model of computation;chip;system on a chip;embedded system;reduced instruction set computing;computer architecture;parallel computing;real-time computing;computer science;operating system;16-bit;finite-state machine	EDA	3.272896488196176	53.44641118562266	153438
6e6e6a11ef1cdea933b68f1f5781b4cd45ff5815	implementation of cell array neuro-processor by using fpga	field programmable gate arrays logic arrays principal component analysis circuits registers neural networks large scale integration computer architecture hardware design languages switches;hardware description languages field programmable gate arrays neural nets adders;neural nets;hardware description languages;satisfiability;adders;hardware description language cell array neuro processor fpga;field programmable gate arrays;hardware description language	We have been studying the neuro-processor that is reconfigured according to needs. We propose an architecture and examine the components of this processor. We implement this processor by using FPGAs. We design this processor by a hardware description language (HDL) and prepare the components to satisfy the architecture.	field-programmable gate array	Fumihiro Hatano;Noriko Akita;Kiyotaka Komoku;Takayuki Morishita;Iwao Teramoto	1999		10.1109/IJCNN.1999.833454	computer architecture;parallel computing;vhdl;reconfigurable computing;computer science;theoretical computer science;machine learning;hardware architecture;hardware description language;register-transfer level;artificial neural network	Robotics	9.206789618457726	48.676658200264704	153505
d2eced91001262f3a255c721bebe258e9a1976f2	sift: a low-overhead dynamic information flow tracking architecture for smt processors	microarchitecture;dynamic information flow tracking;chip;dynamic information;buffer overflow;critical path;code injection attack;simultaneous multithreading;security;high performance;hardware implementation	Dynamic Information Flow Tracking (DIFT) is a powerful technique that can protect unmodified binaries from a broad range of vulnerabilities such as buffer overflow and code injection attacks. Software DIFT implementations incur very high performance overhead, while comprehensive hardware implementations add substantial complexity to the microarchitecture, making it unlikely for chip manufacturers to adopt them. In this paper, we propose SIFT (SMT-based DIFT), where a separate thread performing taint propagation and policy checking is executed in a spare context of an SMT processor. However, the instructions for the checking thread are generated in hardware using self-contained off-the-critical path logic at the commit stage of the pipeline. We investigate several optimizations to the base design including: (1) Prefetching of the taint data from shadow memory when the corresponding data is accessed by the primary thread; (2) Optimizing the generation of the taint instructions to remove unneeded instructions. Together, these optimizations reduce the performance penalty of SIFT to 26% on SPEC CPU 2006 benchmarks--much lower than the overhead of previously proposed software-based DIFT schemes. To demonstrate the feasibility of SIFT, we design and synthesize a core with SIFT logic and show that the area overhead of SIFT is only 4.5% and that instruction generation can be performed in one additional cycle at commit time.	algorithm;binary file;buffer overflow;cpu cache;central processing unit;code injection;critical path method;information flow;microarchitecture;optimizing compiler;overhead (computing);scale-invariant feature transform;shadow memory;software propagation;taint checking	Meltem Ozsoy;Dmitry V. Ponomarev;Nael B. Abu-Ghazaleh;Tameesh Suri	2011		10.1145/2016604.2016650	parallel computing;real-time computing;computer science;operating system	Arch	7.746168906148725	59.567097742422014	153642
8043c45cdec9de389a065e3a3791dab88d60cfe3	compiling stream-language applications to a reconfigurable array processor	datorsystem;computer systems;engineering and technology;teknik och teknologier	New parallel architectures are emerging to meet the increased computational demands of streaming applications. This creates a need for high-level, architecture-independent languages. One such language is StreamIt, designed around the notions of streams and stream transformers, which allows efficient mapping to a variety of architectures. This paper presents our approach of compiling StreamIt applications to the XPP reconfigurable array architecture. We focus mainly on the compiler back end. Although StreamIt exposes the parallelism in the stream program, still a thorough analysis is needed to adapt it to the target architecture. A code generator has been designed for the XPP. It has been demonstrated that by applying optimizations, performance comparable to the low level NML implementation can be achieved. Moreover, the construction of the compiler makes it possible to port StreamIt applications to multiprocessor architectures by doing some architecture specific modifications in the back end.	code generation (compiler);compiler;computation;high- and low-level;multiprocessing;parallel computing;streams;transformers;vector processor	Zain-ul-Abdin;Bertil Svensson	2005			computer architecture;parallel computing;real-time computing;computer science	Arch	0.3481465169757605	49.50340145864478	153747
16e33cdf3aa7dcde3a64fcbbe5c2df7b860f918d	online placement and scheduling algorithm for reconfigurable cells in self-repairable field-programmable gate array systems		Abstract Most of the high-end very-large-scale integration- (VLSI-) based systems use a field-programmable gate array (FPGA) as their core component. As the complexity of systems increases, the chances of fault occurrence also increase. For systems that are part of safety-critical and mission-critical applications, even a single fault can result in entire mission failure. Fault tolerance techniques need to be installed in such systems to ensure reliable, prolonged operation in spite of fault occurrence. The level of fault tolerance exhibited in nature is very remarkable. Scientists are trying to reach that level of fault tolerance in the electronics world as well, which is not always completely acquirable. Basically, fault tolerance in an FPGA is achieved by the use of spare modules, which leads to high area overheads and routing complexity. The higher the number of faults to be handled, the greater the number of spares that will be required. Partial reconfiguration has always been a proven technique to improve the efficiency and flexibility of FPGAs. This paper discusses a multiple-fault repair algorithm for FPGA-based reconfigurable systems, using dynamic runtime partial reconfiguration, in order to relocate faulty modules. Three variants or cases of repair using the algorithm are discussed and demonstrated, namely, placement with the best resource utilization, placement with the least routing overhead, and a case to generate continuous free space for the relocation of a faulty module. The main advantage of the algorithm is its flexibility, which means that it can be used according to user demands and system lifetime requirements. Maximum care is taken to eliminate chances of an unrepaired fault or a repair that degrades the performance of the system. The algorithm can handle multiple permanent faults with the best resource utilization and the least overheads. The performance of the algorithm is analyzed quantitatively, while a comparison is made with some previous studies in the literature to justify the algorithm efficiency.	algorithm;field-programmability;field-programmable gate array;scheduling (computing)	C. Pradeep;Madhuri Elsa Eapen;P. P. Joby;Jubilant J. Kizhakkethottam	2018	Computers & Electrical Engineering	10.1016/j.compeleceng.2018.02.001	field-programmable gate array;very-large-scale integration;real-time computing;spite;control reconfiguration;fault tolerance;scheduling (computing);algorithmic efficiency;spare part;computer science	EDA	5.4838762257294045	58.64048412347218	153982
d8a320414fecfb331769bb67deab1a8082d4feca	test program optimization techniques for a high speed performance vlsi tester	program optimization		program optimization;rationalfunctionaltester;very-large-scale integration	Arthur L. Downey	1983			computer architecture;computer science;computer engineering;program optimization;very-large-scale integration	EDA	8.202648427652866	51.242760544757466	153983
79b8bcfc33c371d5be83c9a09fb14108637ef2c7	automated design of dsp array processor chips	digital signal processing;automated design;design automation;signal processing circuit analysis computing application specific integrated circuits circuit layout cad digital signal processing chips;very large scale integration;dsp asic compiler;finite impulse response filter;dsp asics;dsp array processor chips;chip;computer architecture;silicon compiler framework;high level specification automated design dsp array processor chips dac dsp asic compiler silicon compiler framework dsp asics;high level specification;application specific integrated circuits;signal processing;digital filters;circuit layout cad;digital signal processing chips;dac;silicon compiler;circuit analysis computing;digital signal processing chips digital signal processing design automation application specific integrated circuits silicon compiler very large scale integration computer architecture chip scale packaging digital filters finite impulse response filter;chip scale packaging	Details are presented of the DAC (DSP ASIC Compiler) silicon compiler framework. DAC allows a non-specialist to automatically design DSP ASICs and DSP ASIC cores directly form a high level specification. Typical designs take only several minutes and the resulting layouts are comparable in area and perjonnance to handcrafred designs.	application-specific integrated circuit;digital signal processor;digital-to-analog converter;high-level programming language;silicon compiler;vector processor	John V. McCanny;Yi Hu;Ming Yan	1994		10.1109/ASAP.1994.331818	chip;embedded system;chip-scale package;computer architecture;digital filter;computer science;electrical engineering;digital signal processing;finite impulse response;signal processing;application-specific integrated circuit;very-large-scale integration	EDA	6.0469688024867905	50.60333838852476	154069
fb95d99321a302333a142a769775c2985eac8a3a	automated hdl generation: comparative evaluation	hardware design languages;design automation;high level languages;reconfigurable computing;application software;hardware description languages;automated hardware generation automated hdl generation comparative evaluation reconfigurable computing systems general purpose processor;testing;design optimization;general purpose processor;automated hdl generation;microprocessor chips electronic design automation hardware description languages high level languages;sparks;hardware design;hardware design languages software design application software testing design optimization sparks design automation laboratories software tools high level languages;software tools;software design;comparative evaluation;high level language;reconfigurable computing systems;microprocessor chips;automated hardware generation;electronic design automation	Reconfigurable computing (RC) systems, coupling general purpose processor with reconfigurable components, offer a lot of advantages. Nevertheless, currently a designer needs both in-depth software and hardware design knowledge to develop applications for such platforms. The automated hardware generation addresses this problem. However, the success of such tools remains marginal. This paper discusses the reasons for the lack of success. It presents a quantitative and qualitative comparison of three hardware generators using the following criteria: quality of the hardware model, the supported HLL constructs, and the level of automation.	hardware description language;high-level programming language;marginal model;reconfigurable computing	Yana Yankova;Koen Bertels;Stamatis Vassiliadis;Roel Meeuws;Arcilio Virginia	2007	2007 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2007.378622	hardware compatibility list;embedded system;computer architecture;electronic design automation;computer science;operating system;software engineering;hardware architecture;high-level programming language;computer engineering	Arch	7.413332768703578	52.01946257324396	154123
09042f6f12668c2a94d1fb8fcecf400270093eb1	partial triplication of a sparc-v8 microprocessor using fault injection	radiation hardening electronics fault diagnosis flip flops integrated circuit reliability logic testing microprocessor chips;partial triplication microprocessor reliability stuck at faults single event upset sensitive flip flops pipelined microprocessor selective hardening fault injection sparc v8 microprocessor;tunneling magnetoresistance registers microprocessors circuit faults reliability sensitivity logic gates	We present in this paper the application of fault injection to selective hardening by triplication of a pipelined SPARC-V8 microprocessor. Fault injection is applied to identify the most sensitive flip-flops to single-event-upset and stuck-at faults of the design. The most sensitive flip-flops are replaced by triplicated ones during the physical implementation of the design. Selective triplication is applied to about 26% of the flip-flops of the design allowing to gain additional reliability with a low overhead and delay penalty.	embedded system;flops;fault injection;flip-flop (electronics);microprocessor;overhead (computing);reference design;sparc;single event upset	Cyril Bottoni;Benjamin Coeffic;Jean-Marc Daveau;Lirida A. B. Naviner;Philippe Roche	2015	2015 IEEE 6th Latin American Symposium on Circuits & Systems (LASCAS)	10.1109/LASCAS.2015.7250415	embedded system;electronic engineering;real-time computing;engineering	EDA	8.680026169597237	59.74050153371486	154148
7a4f61220268ad6609f3ccbb6f4b5fa0c1f05ab1	fast and accurate cache modeling in source-level simulation of embedded software	data models;indexes	Recently, source-level software models are increasingly used for software simulation in TLM (Transaction Level Modeling)-based virtual prototypes of multicore systems. A source-level model is generated by annotating timing information into application source code and allows for very fast software simulation.  Accurate cache simulation is a key issue in multicore systems design because the memory subsystem accounts for a large portion of system performance. However, cache simulation at source level faces two major problems: (1) as target data addresses cannot be statically resolved during source code instrumentation, accurate data cache simulation is very difficult at source level, and (2) cache simulation brings large overhead in simulation performance and therefore cancels the gain of source level simulation. In this paper, we present a novel approach for accurate data cache simulation at source level. In addition, we also propose a cache modeling method to accelerate both instruction and data cache simulation. Our experiments show that simulation with the fast cache model achieves 450.7 MIPS (million simulated instructions per second) on a standard x86 laptop, 2.3x speedup compared with a standard cache model. The source-level models with cache simulation achieve accuracy comparable to an Instruction Set Simulator (ISS). We also use a complex multimedia application to demonstrate the efficiency of the proposed approach for multicore systems design.	cpu cache;computer simulation;embedded software;embedded system;experiment;instruction set simulator;laptop;multi-core processor;overhead (computing);speedup;systems design;x86	Zhonglei Wang;Jörg Henkel	2013	2013 Design, Automation & Test in Europe Conference & Exhibition (DATE)		database index;embedded system;data modeling;pipeline burst cache;computer architecture;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;cpu cache;cache;computer science;cache invalidation;operating system;smart cache;programming language;cache algorithms;cache pollution	EDA	-3.2316346234793465	53.866563184102716	154150
1ef9517a850bf1cad23b05262e5710b9b0af01d5	a software framework for trace analysis targeting multicore platforms design	dynamically mapping multithreaded applications;software;cycle accurate network on chip architecture;trace analysis;design automation;multi threading;software design automation multicore processing usa councils measurement optimization routing;measurement;network on chip;routing;multicore platforms design;statistical analysis circuit optimisation electronic engineering computing integrated circuit design multiprocessing systems multi threading network on chip;usa councils;noc design;integrated circuit design;multicore;statistical analysis;multicore processing;noc optimization software framework trace analysis multicore platforms design dynamically mapping multithreaded applications cycle accurate network on chip architecture noc architecture statistical analysis network workloads noc design;software framework;noc architecture;optimization;electronic engineering computing;tools;multiprocessing systems;circuit optimisation;noc optimization;network workloads;multicore network on chip trace analysis	This demonstration presents a complete software framework for dynamically mapping multi-threaded applications on a cycle accurate Network-on-Chip (NoC) architecture, analyzing the statistics of network workloads and drawing general guidelines regarding the design and optimization of NoCs.	mathematical optimization;multi-core processor;network on a chip;software framework;thread (computing)	Guopeng Wei;Paul Bogdan;Radu Marculescu	2011	Proceedings of the Fifth ACM/IEEE International Symposium	10.1145/1999946.1999990	multi-core processor;computer architecture;parallel computing;real-time computing;electronic design automation;computer science;network on a chip;computer network	Arch	2.559448964604789	57.257209466716844	154165
70594c0222b8a5b15449de3efd44c0128aed0273	"""""""microprogramming - uses and tradeoffs"""""""	design automation;machine design	An investigation is made as to when, where and whether to use microprogramming in machine design. Various past and current Design Automation techniques for microcode development will be discussed.	automation;microcode	J. Michael Galey	1972		10.1145/800153.804959	embedded system;computer architecture;electronic design automation;computer science;systems engineering;computer-automated design;computer engineering	EDA	8.097030679909683	51.982450988151854	154236
80d8fcb6c4b85723a1cae6a36e338f994c38f962	a wrapper of pci express with fifo interfaces based on fpga	pcie solution;optimisation;pcie wrapper core;timing optimization method;xilinx inc. architecture;fifo interfaces;giga bit;key module;pci express;bit rate 1.8 gbit/s;fifo interface;vertex-5-fx70t fpga;on-chip interface technology;fifo based interface;pwrapper core;clock domain;on-chip interfaces technology;fpga;development board;peripheral interfaces;pci express wrapper;field programmable gate arrays;timing optimization methods	This paper proposes a PCI Express (PCIE) Wrapper core named PWrapper with FIFO interfaces. Compared with other PCIE solutions, PWrapper has several advantages such as flexibility, isolation of clock domain, etc. PWrapper is implemented and verified on Vertex-5-FX70T which is a development board provided by Xilinx Inc. Architecture of PWrapper and design of two key modules are illustrated, which timing optimization methods have been adopted. Then we explained the advantages and challenges of on-chip interfaces technology based on FIFOs. The verification results show that PWrapper can achieve the speed of 1.8Gbps (Giga bits per second).	central processing unit;clock signal;conventional pci;data rate units;direct memory access;fifo (computing and electronics);field-programmable gate array;mathematical optimization;microprocessor development board;pci express;semiconductor intellectual property core	Hu Li;Yuanan Liu;Dongming Yuan;Hefei Hu	2012	2012 International Conference on Industrial Control and Electronics Engineering		embedded system;real-time computing;pci configuration space;computer hardware;computer science;operating system;field-programmable gate array	EDA	5.728750552858907	49.127733025936934	154278
052cbf62301937712179dd40adba9aaf91b277f1	enhancements for variable n-point streaming fft/ifft on redefine, a runtime reconfigurable architecture	fast fourier transform fft;multiple channels;dynamic reconfiguration;fast fourier transforms variable n point streaming fft ifft enhancements redefine runtime reconfigurable architecture 3gpp long term evolution variable transmission bandwidths universal mobile telecommunications asic solutions idle resources multiple channels;reconfigurable architectures;runtime;fast fourier transform;reconfigurable platform;reconfigurable architecture;3g mobile communication;runtime reconfiguration;application specific integrated circuits;mobile telecommunication;mobile communication;fabrics;fast fourier transforms;cost effectiveness;tiles runtime fabrics hardware mobile communication throughput application specific integrated circuits;tiles;reconfigurable architectures 3g mobile communication fast fourier transforms;runtime reconfiguration reconfigurable platform fast fourier transform fft;throughput;hardware	3GPP LongTerm Evolution (LTE) is targeted towards variable transmission bandwidths to improve universal mobile telecommunications. This requires support for variable point FFT/IFFT (128 through 4096). ASIC solutions, one for a particular FFT is not cost-effective, while DSP solutions are not performance-effective. We hence provide a solution on REDEFINE, a dynamically reconfigurable platform, by appropriately characterizing the compute elements. We make use of the idle resources of REDEFINE to improve throughput by supporting multiple channels of FFT/IFFT.	application-specific integrated circuit;arithmetic logic unit;bandwidth (signal processing);compaq lte;computation;fast fourier transform;lookup table;norm (social);random-access memory;reconfigurability;system on a chip;throughput	N. Thambi Prashank;M. Prasadarao;Avinaba Dutta;Keshavan Varadarajan;Mythri Alle;S. K. Nandy;Ranjani Narayan	2010	2010 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation	10.1109/ICSAMOS.2010.5642067	embedded system;parallel computing;real-time computing;computer science	EDA	2.3373604833034	48.358639856807514	154303
481054c9d2f99aa417934d0aa6cb95396e3c4238	panel: future soc verification methodology: uvm evolution or revolution?	integrated circuit design;system-on-chip;soc verification methodology;uvm evolution;complexity system on chip verification design;universal verification methodology	With increasing design complexity System on Chip (SoC) verification is becoming a more and more important and challenging aspect of the overall development process. The Universal Verification Methodology (UVM) is thereby a common solution to this problem; although it still keeps some problems unsolved. In this panel leading experts from industry (both users and vendors) and academy will discuss the future of SoC verification methodology.	academy;system on a chip;universal verification methodology	Rolf Drechsler;Christophe Chevallaz;Franco Fummi;Alan J. Hu;Ronny Morad;Frank Schirrmeister;Alex Goryachev	2014			system on a chip;embedded system;electronic engineering;real-time computing;software verification;computer science;high-level verification;intelligent verification;functional verification;computer engineering;integrated circuit design	EDA	8.805160439668835	53.842049940519516	154323
cd76863ea92048b13fbfe6f638bd1589401ac426	eperf: energy-efficient execution of user-interactive event-driven applications: work in progress		Asynchronous or event-driven programming has rapidly gained importance due to its widespread use in web browsers, mobile OS, web servers, IoT and sensor networks. These applications typically place a high value on energy-efficiency without sacrificing user-perceived performance (quality of service, QoS). This programming modelu0027s natural discretization of the executed instruction stream into events offers novel optimization opportunities. Not all events are created equal. Their execution latencies have disproportionate effects on QoS. Some need the highest level of performance available, others can be slowed down to save energy. Leveraging this insight, we propose EPerf. Application developers are invited to label the events with the required performance level. EPerf uses this to execute the events on the right dynamic voltage and frequency scaling (DVFS) level of the processor in homogeneous systems or on the right core in heterogeneous systems.	event-driven programming	Gaurav Chadha	2018			wireless sensor network;work in process;real-time computing;asynchronous communication;frequency scaling;web server;quality of service;programming paradigm;efficient energy use;distributed computing;computer science	EDA	-4.3081427876449325	59.65577737292159	154370
75d0981617b89718501c5ec0767e5d8172bd0ca2	a multi-level design flow for incorporating ip cores: case study of 1d wavelet ip integration	c++ language;formal specification;formal verification;hardware-software codesign;industrial property;logic design;logic simulation;system-on-chip;wavelet transforms;1d wavelet transform ip integration;c++/systemc based simulation;eda co-verification tools;hw/sw co-design;ip block reuse;ip core incorporation;soc design;abstraction levels;algorithmic specification;co-simulation design;intellectual property;multilevel design flow;system verification;systems-on-chip;wavelet based compression system;wavelet transform ip core	The design of high performance multimedia systems in a short time force us to use IP's blocks in many designs. However, their correct integration in a design implies more complex verification problems. In this paper, we present a C++/SystemC based simulation flow at multiple levels of abstraction. Our approach is to use SystemC to describe both application and a set of algorithmic IP cores to be incorporated throughout the design flow. Our methodology supports design refinement through four main abstraction levels, offers verification techniques at each level and allows the use of EDA co-verification tools. The use of C++/SystemC to model all parts of the system provides great flexibility and enables faster simulation compared to existing methodologies. An illustrative case study for wavelet based compression system design shows that our methodology supports efficient algorithmic specification, where IP models can be easily incorporated, modified and simulated in order to quickly evaluate alternative system implementation .	design flow (eda);electronic design automation;level design;principle of abstraction;refinement (computing);semiconductor intellectual property core;simulation;systemc;systems design;wavelet	Adel Baganne;Imed Bennour;Mehrez Marzougui;Riadh Gaiech;Eric Martin	2003			system on a chip;embedded system;computer architecture;electronic engineering;logic synthesis;real-time computing;formal verification;computer science;design flow;theoretical computer science;operating system;logic simulation;formal specification;high-level synthesis;programming language;wavelet transform;systems design	EDA	4.637355425541312	52.76283172492618	154372
69867e8a43d69702acf98a79f19a0a88ffbceb86	development of a technology independent library	technology independent library	A technology-independentlibrary which is appropriate for fast turn-around ASIC design is introduced. This library is based on a set of technology-independentLeaf Cells and a set of high-level user programmable Modules built using them. Distinct design rules for developing the Modules have been adopted, from a set of manufacturers' most common design rules, thus resulting in the improvement of reliability and testability of designs. Using the set of Modules, the overall design time is reduced; furthermore, the structure of the library, enhances the aptitude of the designer to select a suitable IC manufacturer when most advantageous, independently of the progress of the design.	application-specific integrated circuit;high- and low-level;library (computing);software testability;aptitude	George-Peter K. Economou;Spiridon Nikolaidis;D. E. Metafas;Constantinos E. Goutis	1993	Microprocessing and Microprogramming	10.1016/0165-6074(93)90097-5	application-specific integrated circuit;real-time computing;aptitude;testability;computer science;computer architecture	EDA	4.2401134783438925	51.039149543969465	154937
8f4df878a1ce4c82734f7a4b4d1d87a8b5b8ba9c	reliability aware high-level embedded system design in presence of hard and soft errors			embedded system;high- and low-level;systems design	Adeel Israr	2012			embedded system;real-time computing;computer hardware	EDA	3.908806391332289	54.206140754353015	154981
eedf009172fc4ff3527774f4bface16aa877454c	modeling and predicting application performance on hardware accelerators	performance evaluation;integrated circuit;computer model;programming parallel machines performance evaluation;hardware accelerator;data model;computational modeling;predictive models computational modeling performance evaluation hardware data models integrated circuit modeling supercomputers;integrated circuit modeling;performance model;parallel machines;predictive models;prediction model;performance modeling framework hardware accelerator compute operation top500 supercomputer procurement cost programming porting effort;programming;supercomputers;data models;hardware	Systems with hardware accelerators speedup applications by offloading certain compute operations that can run faster on accelerators. Thus, it is not surprising that many of top500 supercomputers use accelerators. However, in addition to procurement cost, significant programming and porting effort is required to realize the potential benefit of such accelerators. Hence, before building such a system it is prudent to answer the question ‘what is the projected performance benefit from accelerators for workloads of interest?’ We address this question by way of a performance-modeling framework, which predicts realizable application performance on accelerators speedily and accurately without going to the considerable effort of porting and tuning.	hardware acceleration;procurement;speedup;supercomputer;top500	Mitesh R. Meswani;Laura Carrington;Didem Unat;Allan Snavely;Scott B. Baden;Stephen W. Poole	2011	2011 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2011.6114198	computer simulation;computer architecture;parallel computing;real-time computing;computer science;operating system;machine learning;predictive modelling	Arch	-3.143694124817596	46.41210729099303	155108
be70dd0d6e46d0888961a23d40184458b5bec157	agate - towards designing a low-power chip multithreading processor for mobile software defined radio systems	digital signal processing;software tool;multi threading;design process;power estimation;software defined radio;software radio;chip;general purpose processor;low power;design and implementation;cmos technology low power chip multithreading processor mobile software defined radio system low power consumption software generation programming tools chip multithreading general purpose processor core programmer friendly sdr processor platform software tools tightly coupled coprocessor extension digital signal processing agate processor system verilog language xilinx virtex 6 ml605 fpga evaluation board gate level simulation value change dump power estimation analysis;power demand hardware pipelines clocks instruction sets computer architecture registers;power consumption;software radio microprocessor chips multi threading;high throughput;low power consumption;programming tool;microprocessor chips	Providing low power consumption, high throughput and flexible solution is a challenge during designing process of a mobile software defined radio (SDR) system. The need for simple software generation using common programming tools becomes also a very significant factor. The paper presents the design and implementation of a chip multithreading general-purpose processor core (GPP), as the first step towards designing a flexible and programmer friendly SDR processor platform. Software tools developed for the hardware are described. The future work will be focused on designing tightly-coupled coprocessor extensions (TCC) for an application specific digital signal processing (DSP) purposes. AGATE processor system is described in form of a highly configurable library using Verilog language. The concept verification process was performed on the Xilinx Virtex-6 ML605 FPGA evaluation board. The maximum achieved frequency for the 8-thread processor is 190 MHz. Gate level simulation along with Value Change Dump (VCD) power estimation analysis were performed using three CMOS technologies: 130 nm, 90 nm and 65 nm. AGATE is capable of performing up to 0.72 DMIPS/MHz/thread with the maximum frequency of over 700 MHz and the power consumption of about 3 mW/core using 65 nm process.	application-specific integrated circuit;cmos;clock gating;combinational logic;coprocessor;dhrystone;digital signal processing;etsi satellite digital radio;field-programmable gate array;general-purpose macro processor;general-purpose markup language;graph partition;low-power broadcasting;microprocessor development board;multi-core processor;multithreading (computer architecture);programmer;programming tool;simulation;system on a chip;thread (computing);throughput;verilog	Krzysztof Marcinek;Witold A. Pleskacz	2012	2012 IEEE 15th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2012.6219018	embedded system;computer architecture;electronic engineering;parallel computing;computer science;electrical engineering;operating system;software-defined radio	Arch	5.64408632325565	48.905638518478064	155393
bae45454e97318d7c02144f4c9cc52d20baa730b	architecture of an asynchronous fpga for handshake-component-based design	fpga;asynchronous circuit	This paper presents a novel architecture of an asynchronous FPGA for handshake-component-based design. The handshake-component-based design is suitable for large-scale, complex asynchronous circuit because of its understandability. This paper proposes an area-efficient architecture of an FPGA that is suitable for handshakecomponent-based asynchronous circuit. Moreover, the FourPhase Dual-Rail encoding is employed to construct circuits robust to delay variation because the data paths are programmable in FPGA. The FPGA based on the proposed architecture is implemented in a 65nm process. Its evaluation results show that the proposed FPGA can implement handshake components efficiently.	asynchronous circuit;component-based software engineering;field-programmable gate array	Yoshiya Komatsu;Masanori Hariyama;Michitaka Kameyama	2013	IEICE Transactions		computer architecture;real-time computing;asynchronous circuit;reconfigurable computing;computer science;fpga prototype;field-programmable gate array	EDA	5.532229499215751	51.74586284384066	155401
941e11cec52821e04ebaf5d13500f46a7c0c2160	developing a design flow for embedded systems			design flow (eda);embedded system	Falko Guderian	2013				EDA	5.161930945939759	50.783041407107305	155467
91fd72c9177fbc19d29cbd5590e34c00cdfba7fe	design of the common framework for lin low level driver	lin bus;lin core api;lin low level driver	The LIN low level driver provided by the chip manufacturers cannot cover all the chips, in some cases users have to modify or design the LIN low level driver by themselves. To address this problem, a common framework of the driver was proposed and base on it research was carried out to realize the unconditional frame communication. The background knowledge of the LIN bus was introduced; it was pointed out that a common LIN driver framework should be based on UART/SCI, timer module and interrupt. The design of the driver was divided into configurable part and the fixed part, detail discuss of the latter one was carried out, especially on the basic types, the interrupt functions and the Core API. The feasibility of the framework was verified through the communication between the test circuit board and the Emulin. Analyses show that this method is applicable to many chips and can accelerate the development progress. © (2013) COPYRIGHT Society of Photo-Optical Instrumentation Engineers (SPIE). Downloading of the abstract is permitted for personal use only.		Feng Luo;Yupeng Lin	2013		10.1117/12.2031020	embedded system;real-time computing;simulation;engineering	Vision	6.834800826107777	48.96445454240452	155504
52b2bf1ba3a97dba22ceb0482f1594962c4b6f22	timing analysis in high-level synthesis	high-level synthesis;timing analysis;data flow;scheduling;scheduling algorithm;high level synthesis;logic synthesis;formal specification;control flow	This paper presents a comprehensive timing model for behavioral-level specifications and algorithms for timing analysis in high-level synthesis. It is based on a timing network which models the data flow as well as the control flow in the behavioral input specification. The delay values for the network modules are created by invoking the same logic synthesis procedure applied after behavioral synthesis. The timing network is built only once for a given behavioral description. Several parameters are used to explore different scheduling possibilities as well as different optimization modes (area, delay), without changing the network. The use of the timing model in conjunction with a path-based scheduling algorithm is presented. Results for several benchmarks attested the accuracy of this approach.	algorithm;benchmark (computing);control flow;data-flow analysis;dataflow;high- and low-level;high-level synthesis;logic synthesis;mathematical optimization;multiplexer;performance tuning;processor register;schedule (project management);scheduling (computing);static timing analysis	Andreas Kuehlmann;Reinaldo A. Bergamaschi	1992		10.1145/304032.304132	fair-share scheduling;computer architecture;parallel computing;real-time computing;logic optimization;computer science;operating system;programming language;scheduling;static timing analysis	EDA	1.4688348353557146	52.51798429932271	155531
2092d6e297d3c9dbf22f02577cfb2fd48658ddfa	building blocks to use in innovative non-volatile fpga architecture based on mtjs	magnetic tunnelling field programmable gate arrays integrated circuit design;integrated circuit design;transistors integrated circuit modeling programming field programmable gate arrays table lookup magnetic tunneling clocks;magnetic tunnelling;field programmable gate arrays;university of minnesota innovative nonvolatile fpga architecture magnetic tunnel junction devices reconfigurable field programmable logic array asic development transistor level circuit level designs electrically sound simulations cadance virtuoso spectre ibm p13 toolkit mtj spice model university of tohoku	This paper addresses the need for a non-volatile reconfigurable FPGA in order to allow for many current applications to transition away from costly ASIC development. It is assumed that an architecture has been selected and needs to be filled with blocks designed at the transistor level. These are to allow for non-volatility by means of magnetic tunnel junction devices (MTJs). Circuit level designs are presented, together with their successful simulations. The blocks are therefore assembled together and electrically sound simulations are presented for a fully functional FPGA of minimal size. Design and testing is carried out in Cadance Virtuoso and Spectre along with the IBM p13 toolkit. The typical parameters of a University of Tohoku MTJ are used in a SPICE model developed by University of Minnesota.	application-specific integrated circuit;field-programmable gate array;non-volatile memory;spice;simulation;transistor;volatility	Luca Montesi;Zeljko Zilic;Takahiro Hanyu;Daisuke Suzuki	2012	2012 IEEE Computer Society Annual Symposium on VLSI	10.1109/ISVLSI.2012.21	embedded system;electronic engineering;engineering;electrical engineering	EDA	9.813225330692758	52.514835646560755	155549
9839a473332c4ed7c359f4a643a705cb63499c30	bitwise optimised cam for network intrusion detection systems	network intrusion detection systems;optimisation;bitwise optimised cam;string pattern matching;tree based content addressable memory structure;boolean expression;optimisation content addressable storage security of data pattern matching field programmable gate arrays;snort rule set;computer aided manufacturing cadcam intrusion detection hardware payloads information security binary decision diagrams pattern matching engines boolean functions;pattern matching;xilinx xc2v80o0 fpga;hardware sharing;field programmable gate arrays;content addressable storage;network intrusion detection system;security of data;xilinx xc2v80o0 fpga bitwise optimised cam network intrusion detection systems string pattern matching tree based content addressable memory structure hardware sharing logic optimisations boolean expression snort rule set;logic optimisations	String pattern matching is a computationally expensive task, and when implemented in hardware, it can consume a large amount of resources for processing and storage. This paper presents a novel technique, based on a tree-based content addressable memory structure, for a pattern matching engine for use in a hardware-based network intrusion detection system. This technique involves hardware sharing at bit level in order to exploit powerful logic optimisations for multiple strings represented as a boolean expression. Our approach has been used to implement the entire SNORT rule set with around 12% of the area on a Xilinx XC2V80O0 FPGA. The design can run at a rate of approximately 2.5 Gigabits per second, and is approximately 30% smaller in area when compared with published results. The performance of our design can be improved further by having multiple designs operating in parallel.	algorithm;analysis of algorithms;bit-level parallelism;bitwise operation;boolean expression;computer hardware;content-addressable memory;field-programmable gate array;gigabit;intrusion detection system;parallel computing;pattern matching;snort	Sherif Yusuf;Wayne Luk	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515762	embedded system;parallel computing;real-time computing;boolean expression;computer science;theoretical computer science;operating system;pattern matching;field-programmable gate array	EDA	8.89992727795597	46.84915885486429	155602
8d21f9d8f9ae13ed01861581bf3c27393678decc	noc synthesis vs itrs predictions: the challenges of linear programming based synthesis	network on chip integrated circuit design linear programming network topology;network on chip;nocbench v 1 0 benchmarks itrs prediction linear programming based synthesis network on chip complex system on chip regular topology irregular topology custom topology area performance optimization noc synthesis flows noc topology application requirement coregraphs heuristic technique;network topology;integrated circuit design;linear programming;decision support systems hafnium	Network on chip are fundamental in the performance of complex system on chip. Numerous solutions have been proposed both for regular and irregular topologies. Irregular or custom topologies present numerous benefits with regard to area/performance optimizations and can be automatically generated through NOC synthesis flows. NOC synthesis generates NOC topologies from application requirements coregraphs. NOC synthesis techniques use either exact or heuristic techniques. So far NOC synthesis techniques have not been benchmarked against ITRS roadmaps. We propose in this paper to benchmark linear programming based NOC synthesis techniques using NOCBENCH v.1.0 benchmarks. The results show that new models and techniques are needed to overcome complexity of future manycore.	benchmark (computing);complex system;heuristic;linear programming;manycore processor;multi-core processor;network on a chip;requirement;system on a chip	Omar Hammami	2013	2013 8th IEEE Design and Test Symposium	10.1109/IDT.2013.6727135	computer architecture;parallel computing;real-time computing;computer science;linear programming;network on a chip;network topology;computer network;integrated circuit design	EDA	2.778803298157725	57.946698146462346	155774
4675c5682a6ea050bf6abcbe63132c4a792a6f8a	adaptive reliability for fault tolerant multicore systems		In an era of continuously shrinking technology and escalating power density, Multiprocessor System on Chips (MPSoCs) suffer from a growing prominence of device defects and increase of dependability-related issues. This paper tackles the dependability challenge by suggesting an adaptive reliability enhancement strategy for multicore systems. We dynamically adapt the reliability enhancement to the actual tasks requirements as well as cores runtime operating conditions. As reliability improvement may adversely affect the parameters of embedded systems, we suggest a runtime recovery method. In fact, we implement a 3-mode mapping technique to limit redundancy overheads through judicious task migrating and dropping. Our experiments show promising results in terms of error mitigation with controllable power and thermal overheads.	dependability;embedded system;experiment;floppy-disk controller;multi-core processor;multiprocessing;requirement;system on a chip	Ihsen Alouani;Thomas Wild;Andreas Herkersdorf;Smaïl Niar	2017	2017 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2017.78	real-time computing;parallel computing;redundancy (engineering);power density;fault tolerance;computer science;multi-core processor;overhead (business);multiprocessing;dependability	EDA	5.902608100234096	59.46511975355519	155874
0b14eb23ad4c3a34cfdc7b6eabe8f8ae7aa90376	a heterogeneous soc architecture with embedded virtual fpga cores and runtime core fusion	microprocessors;field programmable gate array;field programmable gate arrays hardware random access memory registers computer architecture microprocessors logic gates;random access memory;virtualization;hardware software codesign;partial reconfiguration;dynamic reconfiguration;fpga;hardware architecture;computer architecture;virtualisation field programmable gate arrays hardware software codesign multiprocessing systems system on chip;logic gates;system on chip;registers;dynamic and partial reconfiguration;multiprocessing systems;field programmable gate arrays;logic gate;virtualization dynamic and partial reconfiguration fpga heterogeneous co design platform;off the shelf;heterogeneous co design platform;virtualisation;dynamic reconfigurability heterogeneous soc architecture virtual fpga cores runtime core fusion hardware virtualization processor based hardware architectures field programmable gate arrays;hardware	Hardware virtualization is a well known technique in processor based hardware architectures for abstraction of the complexity of an underlying hardware from the programmer. Not only processor based hardware, especially Field Programmable Gate Arrays (FPGA), comes with a high complexity and the exploitation for developers suffer from this fact. Each change in the hardware e.g. through an introduction of a new series results in a re-design of the applications. Therefore, a novel concept for FPGA hardware virtualization is introduced in this paper. The advantage with this approach is that the specification of the virtual FPGA stays unchanged, independent from the underlying hardware and provides therefore features, which the exploited physical host FPGA cannot provide. A special feature of the presented virtual FPGA amongst others is the dynamic reconfigurability which is for example not available with all off the shelf FPGAs. Furthermore the concept of FPGA virtualization enables the re-use of hardware blocks on other physical FPGA devices. This paper presents the hardware platform, describes the tool chain for the virtual FPGA and introduces with Core Fusion a novel technique that improves the utilization of the virtual FPGA.	embedded system;field-programmable gate array;hardware virtualization;programmer;reconfigurability;semiconductor intellectual property core;system on a chip;toolchain	Peter Figuli;Michael Hübner;Romuald Girardey;Falco Bapp;Thomas Bruckschlögl;Florian Thoma;Jörg Henkel;Jürgen Becker	2011	2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2011.5963922	hardware compatibility list;embedded system;computer architecture;full virtualization;parallel computing;logic gate;reconfigurable computing;computer science;operating system;hardware architecture;fpga prototype;field-programmable gate array	EDA	-1.471024448886972	49.75402641213864	156311
3253b26a44e347c3b14577f12ae1945106476c71	a practical approach for combinatorial fuzzy logic control design	fuzzy logic control	This paper presents the architecture development of a Fuzzy Logic Controller (FLC), using combinatorial design implemented on a Field Programmable Gate Array (FPGA). This architecture is based on combinatorial basic modules that enable to increase and improve the entire system performance, by means of replication technique, which is widely used in computer architecture, and help to fit the particular application needs. Recent FPGA technology let us use fast combinatorial circuits for complex designs with parallelism for increasing the FLC performance and it is possible to take it up again as a practical way to build FLC for any process, approaching the fast prototyping advantages and easing the scaling to increase the control accuracy.	computer architecture;field-programmable gate array;fuzzy logic;image scaling;logic control;parallel computing;replication (computing)	Arturo V. Téllez;Luis A. Villa Vargas;Herón L. Molina;Oscar Camacho Nieto;Romeo P. Urbieta	2009			fuzzy logic;t-norm fuzzy logics;combs method;fuzzy electronics;logic optimization;defuzzification;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Arch	10.001834131594292	47.2355685048656	156446
059831f609c6897244cb5b3796935acee36cdbe6	power-aware video decoding using real-time event handlers	energy;event service;mobile device;resource allocation;interactive video;multimedia streaming;real time;multimedia application;mobile environment;power management;power consumption;quality of service;mobile devices;mobile user;time constraint	Multimedia applications have to receive sufficient resource allocations to maintain their desired levels of Quality of Service (QoS). On the other hand, in mobile environments, the devices on which these applications must run have to minimize power consumption to prolong battery life. Our work focuses on the QoS issues in the event-driven distribution of multimedia streams between mobile users, where a source provides interactive video in the form of streams of data events to multiple remote sinks. This paper addresses the power-aware execution of event handlers at such event sinks. In particular, an adaptive approach to the dynamic selection of a suitable CPU clock frequency of a mobile device is shown superior to non-adaptive power management. This approach (a) minimizes power consumption while also (b) guaranteeing that a given event handler finishes its execution within application-specific timing constraints. This is realized by dynamically measuring the progress of event handler functions and then using this information to re-adjust the clock frequency for the current event and to select appropriate clock frequencies for future events.	central processing unit;clock rate;emoticon;event (computing);event-driven programming;mobile device;power management;quality of service;real-time locating system;video decoder	Christian Poellabauer;Karsten Schwan	2002		10.1145/570790.570803	embedded system;real-time computing;mobile qos;simulation;computer science;operating system;mobile device;computer network	Mobile	-4.414313550058893	59.50685728718876	156573
1cfa27b0c41cedcdf82213ab3fa44300a9bb887d	a macroscopic view of self-replication	dna;digital circuit;electronic circuits;organisms;genomics;biocomputing;turing machines;programmable circuits self reproducing automata cellular automata biocomputing;self reproducing automata;emergencia;tom thumb algorithm;self replicating loop design;lindenmayer system;logic design;emergence;living cells;self replicating machines;digital logic;cellular array;dna double helix;thumb;tom thumb algorithm cellular automata emergence john von neumann lindenmayer system l system self replicating loop self replication;circuit numerique;langtons loop;automata;von neumanns universal interpreter copier;tom thumb loop;sistema l;programmable digital logic;programmable circuits;universal turing machine;von neumanns universal interpreter copier dna double helix cellular array universal turing machine living cells macroscopic analysis self replicating loop design langtons loop electronic circuits tom thumb loop programmable digital logic;automate cellulaire;circuito numerico;automata thumb organisms dna self replicating machines genomics bioinformatics turing machines electronic circuits logic design;l system;macroscopic analysis;self replicating loop;self replication;cellular automata;systeme l;lindenmayer system l system;cellular automaton;john von neumann;automata celular;bioinformatics;autoreplication	In 1953, Crick and Watson published their landmark paper revealing the detailed structure of the DNA double helix. Several years earlier, von Neumann embedded a very complex configuration, a universal interpreter-copier, into a cellular array. Astoundingly, the structure of this configuration, able to realize the self-replication of any computing machine, including a universal Turing machine, shares several common traits with the structure of living cells as defined by Crick and Watson's discovery. To commemorate the 100th anniversary of von Neumann's birth, this paper presents a macroscopic analysis of self-replication in computing machines using three examples. After describing self-replication in von Neumann's universal interpreter-copier, we will revisit the famous self-replicating loop designed by Langton in 1984. In order to overcome some of the major drawbacks of Langton's loop, namely, its lack of functionality and the fact that it is ill-adapted for a realization in electronic circuits, we present a novel self-replicating loop, the Tom Thumb loop. Endowed with the same capabilities as von Neumann's interpreter-copier, i.e., the possibility of replicating computing machines of any complexity, our loop is moreover specifically designed for the implementation of self-replicating structures in programmable digital logic.	boolean algebra;computer;electronic circuit;embedded system;langton's ant;photocopier;self-replication;tom;universal turing machine	Daniel Mange;André Stauffer;Leonardo Peparaolo;Gianluca Tempesti	2004	Proceedings of the IEEE	10.1109/JPROC.2004.837631	cellular automaton;telecommunications;computer science;engineering;electrical engineering;artificial intelligence;universal turing machine;l-system;mathematics;algorithm	Arch	9.018810181779244	48.94222920293636	157374
4de0b59aa46479bfe06fd96d53d8e7f78873de01	fault tolerance of multiple logic faults in sram-based fpga systems	configurable logic block;field programmable gate array;look up table;triple modular redundant;circuit faults;fault tolerant;triple modular redundancy sram based fpga configurable logic block look up table fault tolerance;semiconductor device reliability;triple modular redundancy;functional equivalence;indexing terms;chip;circuit faults field programmable gate arrays fault tolerance fault tolerant systems table lookup switches tiles;fault tolerant system;fault tolerant systems;vlsi fault tolerance field programmable gate arrays semiconductor device reliability sram chips table lookup;fault tolerance;tmr fault tolerance multiple logic faults sram based fpga systems submicron device vlsi technologies reconfiguration techniques chip reliability field reconfiguration triple modular redundancy voter output logic function master slave unit configuration memory look up tables;vlsi;tiles;field programmable gate arrays;switches;table lookup;sram based fpga;high power;sram chips	The very hight levels of integration and submicron device sizes used in current and emerging VLSI technologies for SRAM-based FPGAs lead to higher occurrences of defects and operational faults. Thus, there is a critical need for fault tolerance and reconfiguration techniques for SRAM-based FPGAs to increase chip reliability with field reconfiguration. We first propose a technique utilizing the principle of master slave to tolerate logic or cells in SRAM-based FPGAs. We show that this architectural technique can be used to build redundancy for defect and fault tolerance with limited area and performance overhead. Our algorithm improves reliability of the SRAM-based FPGAs by performing two operations: TMR (triple modular redundancy) (in which CLBs are used to triplicate a logic function whose value is obtained at the voter output) and partitioning (in which the design is partitioned into a set of MSUs (master-slave unit) to reduce the amount of configuration memory required). In response to a component failure, a functionality equivalent MSU that does not rely on the faulty component replaces the affected MSU. Our technique can handle a large numbers of faults (we show tolerance of 16 logic faults in look-up tables LUTs belonging to the same MSU). Experimental results conducted on a subset of the ITC'99 benchmarks demonstrate a high level of reliability in term of fault tolerance with low hardware overhead compared to TMR which has a 5x- 6x area overhead and high power consumption.	algorithm;benchmark (computing);boolean algebra;emergence;failure cause;fault tolerance;field-programmable gate array;high-level programming language;lookup table;msu lossless video codec;overhead (computing);physical design (electronics);software bug;static random-access memory;triple modular redundancy;very-large-scale integration	Farid Lahrach;Abderrahim Doumar;Eric Châtelet	2011	2011 14th Euromicro Conference on Digital System Design	10.1109/DSD.2011.33	embedded system;fault tolerance;parallel computing;real-time computing;computer science	EDA	8.225070807026302	58.77041756783317	157426
05f415aad254e4c79f8147a4de68d64ed8e20c08	on the energy efficiency of graphics processing units for scientific computing	energy efficiency;nongreen computing solution;energy efficiency scientific computing energy consumption computer graphics acceleration biology computing biological information theory central processing unit electrostatics space exploration;biology computing;nvidia geforce gtx 280;empirical study;kernel;electrostatics;paper;yarn;energy efficient;computer graphics;biological code;energy efficient computing;space exploration;power supply;acceleration;computational accelerator;cuda;power aware computing;servers;energy consumption;graphics processing units;energy delay product energy efficiency graphics processing units scientific computing computational accelerator high end computing parallel code execution time power consumption energy consumption nongreen computing solution biological code;scientific computing;nvidia;graphic processing unit;biological information theory;electric potential;computer science;power consumption;energy delay product;magnetic cores;high end computing;parallel code execution time;power consumption microprocessor chips parallel processing power aware computing;parallel processing;graphics;central processing unit;microprocessor chips	The graphics processing unit (GPU) has emerged as a computational accelerator that dramatically reduces the time to discovery in high-end computing (HEC). However, while today's state-of-the-art GPU can easily reduce the execution time of a parallel code by many orders of magnitude, it arguably comes at the expense of significant power and energy consumption. For example, the NVIDIA GTX 280 video card is rated at 236 watts, which is as much as the rest of a compute node, thus requiring a 500-W power supply. As a consequence, the GPU has been viewed as a “non-green” computing solution. This paper seeks to characterize, and perhaps debunk, the notion of a “power-hungry GPU” via an empirical study of the performance, power, and energy characteristics of GPUs for scientific computing. Specifically, we take an important biological code that runs in a traditional CPU environment and transform and map it to a hybrid CPU+GPU environment. The end result is that our hybrid CPU+GPU environment, hereafter referred to simply as GPU environment, delivers an energy-delay product that is multiple orders of magnitude better than a traditional CPU environment, whether unicore or multicore.	alexey chervonenkis;crc-based framing;central processing unit;computational science;computer graphics;data parallelism;geforce 200 series;general-purpose computing on graphics processing units;graphics processing unit;gyrokinetic electromagnetic;memory map;multi-core processor;parallel computing;power supply;run time (program lifecycle phase);tom;unicore;video card;watts humphrey	Song Huang;Shucai Xiao;Wu-chun Feng	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5160980	parallel processing;parallel computing;computer hardware;computer science;theoretical computer science;operating system;distributed computing;efficient energy use;electrostatics	HPC	-2.844319145979014	47.17611794059846	157688
cbcae797bb3f36987b878282b400b174399d1e19	uafea: unified analytical framework for ia/aa-based error analysis of fixed-point polynomial specifications	circuits and systems;time complexity;measurement uncertainty;error analysis;estimation;fixed point arithmetic data flow graphs error analysis;optimization;self validated numerical svn methods fixed point representation maximum mismatches mms polynomial specification precision analysis;correlation;error analysis correlation optimization measurement uncertainty estimation circuits and systems time complexity;error accuracy uafea unified analytical framework ia aa based error analysis fixed point polynomial specification computer arithmetic embedded systems error analysis fixed point polynomial data flow graphs ea framework error measure maximum mismatch feedforward fixed point polynomial dfg overestimation keeping efficiency self validated numerical method interval arithmetic affine arithmetic ia efficiency aa efficiency correlated arguments	A challenging verification and optimization problem in computer arithmetic and embedded systems is error analysis (EA) of fixed-point polynomial data-flow graphs (DFGs). This brief presents an EA framework to compute the error measure maximum mismatch of feedforward fixed-point polynomial DFGs. Our idea to reduce the overestimation keeping efficiency is introducing a unified analytical framework for the two most popular self-validated numerical methods, i.e., interval arithmetic (IA) and affine-arithmetic (AA), so that correlated arguments appear in suitable positions in the DFG. The results show that our methods can enhance error accuracy, in average, more than 24% in comparison with the state-of-the-art techniques while keeping the efficiency of IA and AA.	affine arithmetic;algorithm;arithmetic logic unit;dataflow;embedded system;enterprise architecture framework;error analysis (mathematics);feedforward neural network;fixed point (mathematics);high- and low-level;high-level synthesis;interval arithmetic;mathematical optimization;numerical method;optimization problem;polynomial;requirement	Mahdieh Grailoo;Bijan Alizadeh;Behjat Forouzandeh	2016	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2016.2539078	time complexity;mathematical optimization;estimation;discrete mathematics;affine arithmetic;mathematics;correlation;statistics;measurement uncertainty	EDA	2.9262429665783607	52.93535319695962	157692
57cbf73373f4ba6895af43cfa6d5cd72c6d7d1da	exploiting dynamic reconfiguration techniques: the 2d-vliw approach	reconfigurable architectures computer architecture vliw registers hardware runtime reconfigurable logic pattern recognition fabrics signal processing;cache storage;dynamic reconfiguration;reconfigurable architectures;pipelined reconfigurable multiple issue architecture dynamic reconfiguration 2d vliw very long instruction word architecture configuration cache;multiprocessing systems;configurable computing;functional unit;pipeline processing multiprocessing systems reconfigurable architectures cache storage;pipeline processing	Fast reconfiguration is a mandatory feature for re-configurable computing architectures. Research in this area has been increasingly focusing on new reconfiguration techniques that can sustain the architecture performance and to allow the simultaneous execution, at the same stage, of configuration and computation tasks. In this context, this paper presents a new dynamic reconfiguration technique, based on a configuration cache, that tackles this challenge by configuring and executing operations on functional units during the execution stage. This approach is implemented in a pipelined reconfigurable multiple-issue architecture called 2D-VLIW. Our dynamic reconfiguration technique takes advantage of the 2D-VLIW pipelined execution by starting reconfiguration concurrently to activities like reading operand registers and executing operations.	assembly language;compiler;computation;field-programmable gate array;operand;reconfigurable computing;routing	Ricardo Santos;Rodolfo Azevedo;Guido Araujo	2006	Proceedings 20th IEEE International Parallel & Distributed Processing Symposium	10.1109/IPDPS.2006.1639485	computer architecture;parallel computing;real-time computing;execution unit;computer science;operating system	Arch	-0.45646797289254654	50.34867439274578	157709
0e8a7da41297a92f878a09b3c35382ac44670200	functional abstraction driven design space exploration of heterogeneous programmable architectures	digital signal processing;space exploration vliw digital signal processing computer architecture computer science random access memory program processors permission computational modeling computer simulation;random access memory;functional abstraction;formal specification;hardware software codesign;superscalar;architecture description language;hardware description languages;space exploration;superscalar microprocessor;spectrum;vliw;computer architecture;computational modeling;permission;digital signal processing chips;superscalar processor;hardware description languages digital signal processing chips hardware software codesign computer architecture formal specification;design space exploration;computer science;programmable architecture;superscalar microprocessor functional abstraction design space exploration dsp vliw programmable architecture architecture description language;computer simulation;program processors;adl;dsp	Rapid Design Space Exploration (DSE) of a programmable architecture is feasible using an automatic toolkit (compiler, simulator, assembler) generation methodology driven by an Architecture Description Language (ADL). While many contemporary ADLs can effectively capture one class of architecture, they are typically unable to capture a wide spectrum of processor and memory features present in DSP, VLIW, EPIC and Superscalar processors. The main bottleneck has been the lack of an abstraction underlying the ADL (covering a diverse set of architectural features) that permits reuse of the abstraction primitives to compose the heterogeneous architectures. We present in this paper the functional abstraction needed to capture such wide variety of programmable architectures. We illustrate the usefulness of this approach by specifying two very different architectures using functional abstraction. Our DSE results demonstrate the power of reuse in composing heterogeneous architectures using functional abstraction primitives allowing for a reduction in the time for specification and exploration by at least an order of magnitude.	architecture description language;assembly language;capture one;central processing unit;compiler;design space exploration;digital signal processor;lambda calculus;superscalar processor;very long instruction word	Prabhat Mishra;Nikil D. Dutt;Alexandru Nicolau	2001		10.1145/500001.500061	computer architecture;parallel computing;real-time computing;computer science	Arch	2.140710458602159	50.84106571632751	157786
649e3aa11bbae8faa8f2a3d8aad29d9afa24ca74	access port protection for reconfigurable scan networks	hardware security;debug and diagnosis;secure dft;reconfigurable scan network;ijtag;ieee p1687	Scan infrastructures based on IEEE Std. 1149.1 (JTAG), 1500 (SECT), and P1687 (IJTAG) provide a cost-effective access mechanism for test, reconfiguration, and debugging purposes. The improved accessibility of on-chip instruments, however, poses a serious threat to system safety and security. While state-of-the-art protection methods for scan architectures compliant with JTAG and SECT are very effective, most of these techniques face scalability issues in reconfigurable scan networks allowed by the upcoming IJTAG standard. This paper describes a scalable solution for multi-level access management in reconfigurable scan networks. The access to protected instruments is restricted locally at the interface to the network. The access restriction is realized by a sequence filter that allows only a precomputed set of scan-in access sequences. This approach does not require any modification of the scan architecture and causes no access time penalty. Therefore, it is well suited for core-based designs with hard macros and 3D integrated circuits. Experimental results for complex reconfigurable scan networks show that the area overhead depends primarily on the number of allowed accesses, and is marginal even if this number exceeds the count of registers in the network.		Rafal Baranowski;Michael Andreas Kochte;Hans-Joachim Wunderlich	2014	J. Electronic Testing	10.1007/s10836-014-5484-2	embedded system;electronic engineering;parallel computing;real-time computing;telecommunications;computer science;engineering;boundary scan description language;hardware security module	EDA	8.959755386305597	58.676479505249425	157837
1a8b1ee1d9dffc2a2897b8ada5c3d754dd2a7a07	high-level architecture exploration for mpeg4 encoder with custom parameters	multiprocessors soc architecture;video encoder;architecture exploration;integrated circuit layout;performance estimation;design flow;customization;mpeg 4 standard computer architecture encoding costs change detection algorithms concurrent computing topology quantization partitioning algorithms delay;integrated circuit layout video codecs microprocessor chips system on chip;community networks;system on chip;video codecs;coarse grained;high level architecture;mpeg4;microprocessor chips;multiprocessors soc architecture high level architecture exploration mpeg4 video encoders architecture performances design flow rtl architecture customization parameters	this paper proposes the use of a high-level architecture exploration method for different MPEG4 video encoders using different customization parameters. The targeted architecture is a heterogeneous MP-SoC which may include up 2 coarse grain SIMD (task level SIMD) subsystems to perform the computations. The customization parameters are related to video resolution, frame rate, Communication Network, level of parallelism and CPU types. These parameters are determined during the high-level architecture exploration, by estimating the architecture performances at early stages of the design flow. Experiments shows that the error factor of these high-level performances estimations are less than 10% compared to those obtained with final manually implemented RTL architecture. This method was used successfully for exploration of different MPEG4 architecture configurations with different customization parameters. We consider these experiments a break-through because they show how a complex design can be mastered through a set of pragmatic choices.	algorithm;central processing unit;computation;design flow (eda);display resolution;encoder;experiment;high- and low-level;high-level architecture;parallel computing;performance;requirement;simd	Marius Bonaciu;Aimen Bouchhima;Mohamed-Wassim Youssef;Xi Chen;Wander O. Cesário;Ahmed Amine Jerraya	2006	Asia and South Pacific Conference on Design Automation, 2006.	10.1145/1118299.1118393	system on a chip;reference architecture;embedded system;space-based architecture;encoder;computer architecture;real-time computing;computer science;design flow;cellular architecture;electrical engineering;operating system;integrated circuit layout;mpeg-4	EDA	2.9572072719341396	52.20974812968813	157891
851084385895fa3396bd76241ebe6971aef74669	methodology for the evaluation and selection of microprocessors for specific applications	microprocessor;automatisation;methodologie;microprocesseur;methodology;algorithme selection;automation	Abstract   A methodology using a selection algorithm is specified by which engineers can select a microprocessor for a particular application. Such an automated procedure is of practical value because a wide variety of microprocessors are available in the market today. The practicality of the methodology is demonstrated by two applications (fast Fourier transform using the Cooley-Tukey algorithm, and cyclic redundancy code checking in protocols) with Intel 8085, 8086, Z80 and MC 6800 microprocessors.	microprocessor	Bhuvanesh Srinivasan	1983	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/0141-9331(83)90059-5	embedded system;computer architecture;parallel computing;computer science;electrical engineering;operating system;automation;methodology	EDA	7.66991731764364	51.653011829424926	157954
32314c6096c2a03951eb78b454a846841a679d2d	an application-centered design flow for self reconfigurable systems implementation	field programmable gate arrays;network synthesis;fpga;application-centered design flow;dynamic reconfigurable system development process;self reconfigurable systems implementation	Up to now every proposed methodology for implementing dynamic self reconfigurable systems is architecture-centered. In most cases the system development process is time consuming and requires a very specific technical background. Aim of this work is to provide a fast brain to bit design flow whose goal is to simplify the dynamic reconfigurable system development process by shifting the designer focus from the architecture point of view to the application point of view: designers will not need to possess Dynamic Reconfigurability expertise but just to be skilled with the application domain.	application domain;design flow (eda);point of view (computer hardware company);reconfigurability;reconfigurable computing;self-reconfiguring modular robot	Fabio Cancare;Marco D. Santambrogio;Donatella Sciuto	2009	2009 Asia and South Pacific Design Automation Conference		network synthesis filters;embedded system;electronic engineering;real-time computing;computer science;design flow;operating system;mathematical model;field-programmable gate array;3d computer graphics;computer engineering	EDA	4.910556583533968	50.94537834029649	157999
24c0bad738285c779656953c62630fd3aa2916dd	analog and digital collaborative design techniques for wireless socs			system on a chip	Ryuichi Fujimoto	2016	IEICE Transactions		system on a chip;embedded system;telecommunications;computer science;wireless	EDA	6.146268033796782	50.818437846320286	158074
d64aa1238c27c6b37931f0221ba17a95e3eda9d4	hardware-software co-design of resource constrained systems on a chip	hardware system on a chip delay communication system control system level design data communication resource management computer architecture embedded system partitioning algorithms;layout awarness;deep submicron process;hardware software codesign;hardware software co design;latency optimization;resource allocation;jpeg soc;resource management;hardware layout;data communication;system on a chip;embedded system;chip;system buses;data communication times;computer architecture;system on chip;bus architectures;resource sharing;system level design;medium grained resource sharing;trade offs;optimization;circuit optimisation;communication system control;systems on chip;data communication hardware software codesign system on chip system buses circuit optimisation resource allocation;constrained system;partitioning algorithms;hardware;bus architectures hardware software co design systems on chip deep submicron process hardware layout system level design latency optimization data communication times medium grained resource sharing jpeg soc	We present a hardware-software codesign methodology for resource constrained SoC fabricated in a deep submicron process. The novelty of the methodology consists in contemplating critical hardware and layout aspects during system level design for latency optimization. The effect of interconnect parasitic and delays is considered for characterizing bus speed and data communication times. The methodology permits coarse and medium grained resource sharing across tasks for execution speedup through superior usage of hardware. We offer experiments for the proposed codesign methodology, including a JPEG SoC.	algorithm;device driver synthesis and verification;electronic system-level design and verification;experiment;jpeg;level design;mathematical optimization;speedup;system on a chip;very-large-scale integration	Nattawut Thepayasuwan;Alex Doboli	2004	24th International Conference on Distributed Computing Systems Workshops, 2004. Proceedings.	10.1109/ICDCSW.2004.1284127	system on a chip;embedded system;parallel computing;real-time computing;computer science;resource management;operating system	EDA	1.6708809161174936	54.457970847781596	158085
fdaf37299c4324c9f9212775d5fd8027acf71f83	an on-line fault detection technique based on embedded debug features	microprocessors;microprocessor;detection probability;application code online fault detection technique embedded debug features electronic system microprocessor based systems hardware duplication system on chip debug features;circuit faults;application code;electronic system;on chip debug infrastructure;system on a chip;chip;system on chip debug features;system on chip;hardware duplication;fault detection;microprocessor based systems;system on chip fault diagnosis;on chip debug infrastructure fault detection microprocessor system on chip;online fault detection technique;program processors;embedded debug features;fault diagnosis;hardware;program processors microprocessors hardware circuit faults fault detection system on a chip	An increasing number of applications require being able to detect possible faults arising during the normal activity of the electronic system: for this reason, on-line fault detection is a hot topic today. This paper proposes a new technique which is suitable for microprocessor-based systems (no matter whether they are implemented in a single device or with discrete COTS) that exploit hardware duplication and combines it with the On-Chip Debug features existing in many processors. The new technique increases the observability of faults (thus increasing detection probability and reducing latency) and is characterized by a very reduced intrusiveness in terms of changes required in the application code.	central processing unit;debug;embedded system;fault detection and isolation;microprocessor;online and offline	Michelangelo Grosso;Matteo Sonza Reorda;Marta Portela-García;Mario García-Valderas;Celia López-Ongil;Luis Entrena	2010	2010 IEEE 16th International On-Line Testing Symposium	10.1109/IOLTS.2010.5560215	system on a chip;embedded system;parallel computing;real-time computing;telecommunications;computer science	Embedded	7.127356899215037	58.77159413905269	158145
c36a9e38c60c3c7246b370c1f914193ef61c0640	session 18 overview: socs for mobile vision, sensing, and communications: energy-efficient digital subcommittee		SoCs presented in this session highlight recent energy-efficient architectures and design techniques enabling new capabilities to be added to power-sensitive systems. Machine vision processing brings object recognition and visual analytics for human-machine interfaces and vehicular applications. Smart multi-dimensional sensing systems achieve highest reported efficiency for monitoring humans and our environment. New video and communication processors demonstrate energy-efficient and cost-effective designs that implement the newest standards.	system on a chip	Mike Polley;Paul Liang	2015		10.1109/ISSCC.2015.7063057	embedded system;real-time computing;computer hardware;telecommunications;computer science;electrical engineering;operating system	HCI	-0.48650869729339224	59.411539128549556	158371
e69aeeac91a2b57010a5beaeb056125dfb7d78d2	design and deployment of a generic ecc-based fault tolerance mechanism for embedded hw cores	hardware design languages;error correction codes;fault tolerant;hardware redundancy technique;logic design;information redundancy;fault tolerance fault tolerant systems hardware design languages redundancy nuclear magnetic resonance costs embedded system registers process design field programmable gate arrays;embedded hw cores;data mining;embedded system;error correcting codes generic ecc based fault tolerance embedded hw cores hardware redundancy technique metaprogramming information redundancy data storage element;embedded systems;data storage;error correcting codes;redundancy;fault tolerant systems;registers;fault tolerance;redundancy embedded systems error correction codes fault tolerance logic design program compilers;program compilers;magnetic cores;metaprogramming;data storage element;generic ecc based fault tolerance;hardware	Current practices for the design and deployment of hardware redundancy techniques in embedded systems remain in practice specific (defined on a case-per-case basis) and mostly manual. This paper addresses the challenging problems of engineering fault tolerance mechanisms in a generic way and providing suitable tools for coping with their deployment. This approach relies on metaprogramming to specify fault tolerance mechanisms and open compilers to automatically deploy such mechanisms on the selected hardware core. Our previous research has already shown the suitability of this approach for the generic design and automatic deployment of NMR strategies. In this paper, we explore the usefulness of the idea in the context of information redundancy. The main contribution is the development of a metaprogram for the provision of ECC-based fault tolerance in data storage elements (registers and memory modules). It is also shown to what extend such metaprogram can be useful for improving the reliability of communications between HW cores in embedded systems.	8-bit;compiler;computer data storage;dimm;dependability;embedded system;error detection and correction;fault injection;fault tolerance;interconnection;metaprogramming;personalization;redundancy (engineering);redundancy (information theory);software deployment;very-large-scale integration	Juan-Carlos Ruiz-Garcia;David de Andrés;Pedro J. Gil	2009	2009 IEEE Conference on Emerging Technologies & Factory Automation	10.1109/ETFA.2009.5347151	embedded system;fault tolerance;computer architecture;parallel computing;real-time computing;computer science;operating system;software fault tolerance	Embedded	8.066294921030872	57.78803332528622	158457
eff27409b97a4a8a1158236b03c71a7c6fafb73f	hardware/software co-design using hierarchical platform-based design method	virtual components level;software co-design;real components level;soc system design;soc system design flow;design planning;final design target;design level;level design template;soc high level design;hierarchical platform-based design method;system model level;system on chip;satisfiability;system design;system modeling;integrated circuit design	A Hierarchical Platform-Based Design (Hi-PBD) method is put forward for SoC system design. This method divides SoC system design flow into three levels (i.e. system model level, virtual components level and real components level) to achieve separation of function from structure and separation of computation from communication. HI-PBD defines two mapping processes (i.e. design planning and virtual-real synthesis) to go through all the three design levels. Hi-PBD supports reuse of both the three level design templates and the two mapping results, which increased reusing efficiency greatly. Besides, Hi-PBD boosts up design flexibility by means of supporting revision at all the three level and ensures the final design target satisfies performance requirements through a novel performance constraints transmission strategy. Experiments indicate Hi-PBD method improves SoC high level design efficiency by 30%-40%, and this method achieves platform template reuse ratio by 75%-90%.	platform-based design	Zhihui Xiong;Sikun Li;Jihua Chen	2005		10.1109/ASPDAC.2005.1466584	system on a chip;embedded system;computer architecture;electronic engineering;real-time computing;systems modeling;computer science;engineering;operating system;integrated circuit design;satisfiability;systems design	EDA	5.2936155872484605	53.059300319404244	158495
21e67d91292510717ed9452f622a316abdd69f19	capacity optimized noc for multi-mode soc	use cases system on chip network on chip routing;complexity theory;network on chip;routing;wires;use cases;simulated annealing procedure noc multimode soc network on chip system on chips optimization problem;simulated annealing;system on a chip;optimization problem;system on chip;routing system on a chip optimization bandwidth wires complexity theory switches;bandwidth;simulated annealing network on chip;optimization;switches;modes of operation;use case	Network-on-Chip (NoC) is an evolving interconnection architecture addressing the rising complexity of system-on-chips (SoCs). We present a model for the cost of a NoC for a multiple use-case SoC, i.e., a system with distinct modes of operation, each having a unique traffic pattern. Specifically, we formulate an optimization problem capturing the fact that different use-cases can share capacity. We evaluate the proposed scheme using synthetic and real-life traffic, showing a substantial reduction of up to 27% in the required NoC resources, both when using a new algorithm we present and when using (a somewhat heavier) simulated-annealing procedure.	algorithm;block cipher mode of operation;interconnection;mathematical optimization;network on a chip;optimization problem;real life;simulated annealing;synthetic intelligence;system on a chip	Isask'har Walter;Erez Kantor;Israel Cidon;Shay Kutten	2011	2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2024724.2024933	use case;system on a chip;embedded system;mathematical optimization;electronic engineering;parallel computing;real-time computing;computer science;engineering;network on a chip	EDA	2.6393980955117575	60.24011018613695	158768
6934846faa1e14481c9b53d89c0fe02fb92ae508	boundary-scan architecture, neutral testing models	boundary-scan architecture;neutral testing model		boundary scan		1992	IEEE Design & Test of Computers			SE	9.395041564412903	53.45244592475582	158927
2d98fc1f96e5bded2383f194f884d0865372e436	logca: a high-level performance model for hardware accelerators		With the end of Dennard scaling, architects have increasingly turned to special-purpose hardware accelerators to improve the performance and energy efficiency for some applications. Unfortunately, accelerators donu0027t always live up to their expectations and may under-perform in some situations. Understanding the factors which effect the performance of an accelerator is crucial for both architects and programmers early in the design stage. Detailed models can be highly accurate, but often require low-level details which are not available until late in the design cycle. In contrast, simple analytical models can provide useful insights by abstracting away low-level system details. In this paper, we propose LogCA---a high-level performance model for hardware accelerators. LogCA helps both programmers and architects identify performance bounds and design bottlenecks early in the design cycle, and provide insight into which optimizations may alleviate these bottlenecks. We validate our model across a variety of kernels, ranging from sub-linear to super-linear complexities on both on-chip and off-chip accelerators. We also describe the utility of LogCA using two retrospective case studies. First, we discuss the evolution of interface design in SUN/Oracleu0027s encryption accelerators. Second, we discuss the evolution of memory interface design in three different GPU architectures. In both cases, we show that the adopted design optimizations for these machines are similar to LogCAu0027s suggested optimizations. We argue that architects and programmers can use insights from these retrospective studies for improving future designs.	dennard scaling;encryption;graphics processing unit;hardware acceleration;high- and low-level;image scaling;programmer;software architecture	Muhammad Shoaib Bin Altaf;David A. Wood	2017	2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)	10.1145/3079856.3080216	real-time computing;encryption;computer architecture;interface design;computer science;parallel computing;oracle;dennard scaling;computer hardware;ranging	Arch	-4.372810161241748	47.247518842685864	158996
dfeec010317dc943d1afc0db1efb62f3b3df9afa	arco: a cost-effective and flexible hardware maze router	cost effectiveness	The ARCO architecture implements a cost-based version of Lee's routing algorithm. It supports two-layer routing and can be realized with the use of commercial memory chips and programmable logic devices. The architecture exploits the parallelism of the expansion and reset phases of Lee's algorithm with the use of two three-stage pipelines and a special organization of the memory which stores the board description. The algorithm retrace phase is implemented in software on the host computer. A speed-up factor over 27 in relation to an IBM-PC/486 running at 33 MHz has been achieved with an initial prototype of an accelerator based on the ARCO architecture in the routing of a two-layer printed circuit board.	algorithm;host (network);maze runner;parallel computing;pipeline (computing);printed circuit board;printing;programmable logic device;prototype;router (computing);routing	Júlio S. Aude;Ernesto P. Lopes;Mario F. Martins;S. B. Pinto	1993	Microprocessing and Microprogramming	10.1016/0165-6074(93)90138-B	embedded system;parallel computing;cost-effectiveness analysis;computer hardware;computer science;operating system	Arch	6.161085737901864	55.310311881480075	159095
0b7b1c46caf98d1a8f5b6025bbdf7d9317525fd0	efficient trace data compression using statically selected dictionary	silicon;static dictionary;dynamic dictionaries;data compression;pre silicon phase;system on chip data compression integrated circuit testing logic testing;compression algorithms;real time;trace signals;trace data compression;buffer storage;system on a chip;post silicon validation;error analysis;system on chip;dictionaries;dictionary size;logic testing;debug observation window;integrated circuit testing;hardware overhead;dictionaries data compression buffer storage compression algorithms error analysis silicon system on a chip;trace buffers;dictionary size trace data compression statically selected dictionary post silicon validation pre silicon phase internal signals post silicon debug signal states trace buffers debug observation window trace signals dictionary based compression techniques static dictionary dynamic dictionaries hardware overhead;dictionary based compression techniques;signal states;internal signals;statically selected dictionary;post silicon debug	Post-silicon validation and debug have gained importance in recent years to track down errors that have escaped the pre-silicon phase. Limited observability of internal signals during post-silicon debug necessitates the storage of signal states in real time. Trace buffers are used to store these states. To increase the debug observation window, it is essential to compress these trace signals, so that trace data over larger number of cycles can be stored in the trace buffer while keeping its size constant. In this paper, we propose several dictionary based compression techniques for trace data compression that takes account of the fact that the difference between golden and erroneous trace data is small. Therefore, the static dictionary selected based on golden trace data can provide notably better compression performance than the dynamic dictionaries selected in the current approaches. This will also significantly reduce the hardware overhead by reducing the dictionary size. Our experimental results demonstrate that our approach can provide up to 60% better compression compared to existing approaches, while reducing the architecture overhead by 84%.	algorithm;data compression;debugging;dictionary;overhead (computing);protocol buffers;tracing (software)	Kanad Basu;Prabhat Mishra	2011	29th VLSI Test Symposium	10.1109/VTS.2011.5783748	data compression;system on a chip;embedded system;electronic engineering;parallel computing;real-time computing;computer science;theoretical computer science;operating system;statistics	Metrics	7.134031934564914	59.443224461465505	159233
bc37cf59b9fa15f82d2889d6c7d5f9ddfef80525	an accurate cross-layer approach for online architectural vulnerability estimation	cross layer reliability;architectural vulnerability factor	Processor soft-error rates are projected to increase as feature sizes scale down, necessitating the adoption of reliability-enhancing techniques, but power and performance overhead remain a concern of such techniques. Dynamic cross-layer techniques are a promising way to improve the cost-effectiveness of resilient systems. As a foundation for making such a system, we propose a cross-layer approach for estimating the architectural vulnerability of a processor core online that works by combining information from software, compiler, and microarchitectural layers at runtime. The hardware layer combines the metadata from software and compiler layers with microarchitectural measurements to estimate architectural vulnerability online. We describe our design and evaluate it in detail on a set of SPEC CPU 2006 applications. We find that our online AVF estimate is highly accurate with respect to a postmortem AVF analysis, with only 0.46% average absolute error. Also, our design incurs negligible performance impact for SPEC2006 applications and about 1.2% for a Monte Carlo application, requires approximately 1.4% area overhead, and costs about 3.3% more power on average. We compare our technique against two prior online AVF estimation techniques, one using a linear regression to estimate AVF and another based on PVF-HVF; our evaluation finds that our approach, on average, is more accurate. Our case study of a Monte Carlo simulation shows that our AVF estimate can adapt to the inherent resiliency of the algorithm. Finally, we demonstrate the effectiveness of our approach using a dynamic protection scheme that limits vulnerability to soft errors while reducing the energy consumption by an average of 4.8%, and with a target normalized SER of 10%, compared to enabling a simple parity+ECC protection at all times.	aggregate data;algorithm;approximation error;central processing unit;clozure cl;compiler;core (optical fiber);microarchitecture;monte carlo method;multi-core processor;overhead (computing);run time (program lifecycle phase);specfp;simulation;soft error;vulnerability (computing)	Bagus Wibowo;Abhinav Agrawal;Thomas Stanton;James Tuck	2016	TACO	10.1145/2975588	parallel computing;real-time computing;computer science;theoretical computer science;operating system	Arch	-2.424074635966894	56.38530284503811	159249
c0ef82616a3f8fd5d1277734d22547b189d29de3	two effective methods to mitigate soft error effects in sram-based fpgas	resource utilization;field programmable gate array;evaluation performance;fonction booleenne;random access memory;deteriorizacion por irradiacion;deterioration par rayonnement;interconnection;static random access memory;memoria acceso directo;performance evaluation;fault simulation;integrated circuit;radiation damage;correction erreur;estudio comparativo;evaluacion prestacion;generateur fonction;error soft;boolean function;circuito integrado;memoire acces direct statique;basculement transitoire;red puerta programable;memoria no volatil;simulation defaut;carta de datos;reseau porte programmable;erreur soft;interconexion;etude comparative;memoire non volatile;funcion booliana;error correction;mappage;memoire acces direct;non volatile memory;interconnexion;comparative study;generador funcion;mapping;temps retard;single event upset;correccion error;delay time;fault injection;tiempo retardo;soft error;function generator;balanceo transitorio;circuit integre	This paper proposes an effective architecture that can mitigate Single Event Upset (SEU) effects in SRAM-based FPGAs. The architecture employs two different methods in both logic and interconnection resources. The logic resources utilize a new function generator that can tolerate 100% of single faults in its configuration memory while it can generate all the  k -input Boolean functions. In the interconnection resources, a kind of formation redundancy that can detect 94% of single faults in its configuration memory is applied. Both methods are based on an interesting relation in Boolean functions, identified as mapping. By this concept, a Boolean function is generated by modifying the inputs of other Boolean functions. The effectiveness of the proposed architecture is procured by a standard fault injection tool; moreover, different parameters such as required area, power, and delay are achieved by using synopsis® synthesis tool. The results show that the area, power, and delay overheads are respectively 179%, 94%, and 60% in comparison with the simple architecture.	field-programmable gate array;soft error;static random-access memory	Alireza Rohani;Hamid R. Zarandi	2010	Microelectronics Reliability	10.1016/j.microrel.2010.04.021	radiation damage;embedded system;boolean circuit;in situ resource utilization;electronic engineering;error detection and correction;non-volatile memory;static random-access memory;soft error;function generator;computer science;engineering;integrated circuit;interconnection;comparative research;boolean function;algorithm;field-programmable gate array	EDA	9.718589207420727	60.41705971506964	159433
60ed4026d40cb921e2c7837ccec6ed5686b26382	pact xpp architecture in adaptive system-on-chip integration	chip;adaptive system		adaptive system;system on a chip	Jürgen Becker;Martin Vorbach	2003			embedded system;parallel computing;adaptive system;chip;architecture;pact;computer science	Arch	5.077165810590889	49.77626662646553	159467
4ada05ecfa176768eee5d12faad1fcd87eeff400	a quantitative evaluation of a network on chip design flow for multi-core consumer multimedia applications	industrial case study;intellectual property;network on chip;design flow;multimedia application;satisfiability;soft real time;chip;large scale;low power;system on chip;user requirements;performance ratio;requirement specification;real time application;quantitative evaluation	A growing number of applications are integrated on the same System on Chip in the form of hardware and software Intellectual Property (IP). Many applications have firm or soft real-time requirements and require bounds on latency and throughput. To accommodate the growing number of application requirements, the on-chip interconnect must offer scalability on the physical, architectural and functional level. Networks on Chip (NoC) are proposed as a scalable communication architecture that is also able to deliver guaranteed performance. Traditionally, NoCs focus on delivering physical and architectural scalability. The functional scalability, i.e. the ability to satisfy an increasing number of increasingly demanding requirements with a constant cost/performance ratio, is often overlooked. The onus is on the interconnect design flow that translates user requirements to an interconnect instance. While mature tooling exists for many of the IPs, interconnect design flows are an active research area, with few concrete examples, and few large-scale case studies. As the main contribution of this work, we demonstrate a complete operational interconnect design flow for multiple real-time applications, and quantitatively evaluate the functional scalability on two large-scale industrial case studies. We illustrate the steps of the flow, going from requirement specification all the way to simulation of synthesised netlists in a 90 nm and 65 nm low-power standard-cell technology. We show that the interconnect and design flow offer scalability, on the physical, architectural as well as the functional level.	clock gating;design flow (eda);library (computing);logic synthesis;low-power broadcasting;mixed-signal integrated circuit;multi-core processor;network on a chip;real-time clock;real-time computing;requirement;scalability;simulation;spectral leakage;standard cell;system on a chip;throughput;user requirements document	Andreas Hansson;Kees G. W. Goossens	2011	Design Autom. for Emb. Sys.	10.1007/s10617-011-9073-7	chip;system on a chip;embedded system;real-time computing;telecommunications;computer science;design flow;user requirements document;operating system;non-functional testing;network on a chip;intellectual property;satisfiability	EDA	2.7186978582093784	58.749037441281	159535
12e9ae5e1541b017740ab3fc92b7c58286b59d2c	high level synthesis and generation fpgas with the bedroc system	high level synthesis		field-programmable gate array;high-level synthesis	Miriam Leeser	1993	VLSI Signal Processing	10.1007/BF01608541	computer science;high-level synthesis	EDA	5.5327004825408395	50.164593752888806	159665
e0f689df64ecd9a03ad044b6d43d70427600c002	exploiting parallelism of mpeg-4 decoder with dataflow programming on multicore processor	decoding transform coding multicore processing parallel processing resource management programming;synchronization mechanism mpeg 4 decoder dataflow programming multicore processor parallel programming flexible design methodology video codec cell processor mapping flow allocation algorithm;synchronization mechanism;decoding;resource allocation;component;mapping flow;resource management;cell processor;parallel programming;flexible design methodology;transform coding;allocation algorithm;mpeg 4 decoder;video codec;synchronisation;video coding;decoding component modeling pipeline processing parallel processing;multicore processing;middleware;multicore processors;multicore processor;dataflow programming;multiprocessing systems;parallel programs;modeling;programming;parallel processing;pipeline processing;video coding middleware multiprocessing systems parallel programming resource allocation synchronisation;task allocation;design methodology	Multicore processor provides large computation capability but also involves the complicate parallel programming. One of major considerations in parallel programming is the performance. Traditional design methodologies which usually start a design on a selected platform spend a lot of effort and time on tuning performance and debugging. When platform is changed even with different number of cores, considerable redesign effort is required. Hence a flexible design methodology is necessary. In this paper, a design methodology is presented for video codec, by using MPEG-4 SP decoder as an example, on multi coreprocessor. The parallelisms of MPEG-4 decoder are discussed and exposed with the dataflow model. The dataflow model provides a high-level abstraction of underlying hardware. Computation and communication of MPEG-4 decoder are separated and represented as modules and channels, respectively. It is possible to synthesize the model targeting to either dedicate hardware or software on multiprocessor. To map the high level dataflow model to Cell processor, the mapping flow, including offline profiling, task allocation and runtime libraries, are developed. According to the profiling results, the allocation algorithm could allocate tasks on multiprocessors as balanced as possible. An efficient synchronization mechanism on Cell processor is also proposed. We also discuss the impact of the model and the mapping flow corresponding to decoding speed. The results show that the proposed methodology gets considerable performance boost when the number of cores is increased.	algorithm;cell (microprocessor);codec;computation;dataflow programming;debugging;high- and low-level;high-level programming language;library (computing);multi-core processor;multiprocessing;online and offline;parallel computing;runtime library	Zhong-Ho Chen;Ta-Chun Chen;Jung-Yin Chien;Alvin Wen-Yu Su;Ce-Kuen Shieh	2010	International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2010.9	multi-core processor;parallel processing;computer architecture;parallel computing;real-time computing;computer science;resource management;operating system;signal programming;distributed computing	Arch	-2.9751898900136022	49.59436212242395	159676
20a2da53baed8c1ea5e17f4a91cfc635ab7412de	fft/ifft implementation using vivado™ hls		High level synthesis tools are an attractive option for rapid prototyping and implementation of hardware designs. In this paper we present a case study of using such a tool for the design and implementation of an FFT core for use in a wireless modem. The optimizations used for directing the conversion of C code to hardware are discussed and the impact of the different directives is analyzed. The resulting hardware architecture is competitive with the highly optimized IP core available from Xilinx for their FPGAs in terms of the hardware requirements while achieving a slightly better latency for the same configuration.	fast fourier transform;field-programmable gate array;high-level synthesis;mobile broadband modem;rapid prototyping;requirement;semiconductor intellectual property core	Amit Salaskar;Nitin Chandrachoodan	2016	2016 20th International Symposium on VLSI Design and Test (VDAT)	10.1109/ISVDAT.2016.8064896	latency (engineering);field-programmable gate array;fast fourier transform;real-time computing;mobile broadband modem;parallel computing;computer architecture;high-level synthesis;hardware compatibility list;hardware architecture;computer science;rapid prototyping	Arch	5.089126082480733	49.127680879882575	159723
e5783f1fb0be90ab309d6c82aa0164b606e1a94b	bringing c++ productivity to vhdl world: from language definition to a case study	programming language semantics;data compression;hardware syntactics adders semantics productivity field programmable gate arrays clocks;hardware description languages;xilinx toolchain c productivity language definition hardware description languages synthesizable hardware architectures vhdl 2008 vhdl semantics compile time lists thdl compiler synthesizable vhdl 87 back end c back end cycle accurate model hardware lzss zip compressor xilinx fpga ide;c language;programming language semantics c language data compression field programmable gate arrays hardware description languages program compilers;field programmable gate arrays;program compilers	During the last years the hardware description languages evolved providing a faster and a more generic way of describing synthesizable hardware architectures. E.g., VHDL 2008 extended the concept of generics from integral numbers to types, packages and subroutines. This paper presents a hardware description language based on the VHDL semantics — THDL++. It supports the extended generic concept and improves it further by supporting compile-time lists with “for each” semantics, inheritance, expression type derivation and late binding. We also present THDL++ compiler with 2 back-ends: a synthesizable VHDL-87 back-end makes it easy to integrate THDL++ into any existing design flow, and a C++ back-end that generates a cycle-accurate model for fast simulation. We illustrate how using THDL++ significantly reduces design effort compared to raw VHDL by making the code more readable and reusable. As a case study, we present a hardware LZSS (ZIP) compressor, targeting Xilinx FPGAs that's development was accelerated by using THDL++. We demonstrate how using THDL++ reduced the amount of code lines by a factor of 1.85 compared to VHDL and how using the C++ back-end increased simulation performance by a factor of 8 compared to ModelSim [1]. The THDL++ compiler and an IDE integrated with Xilinx toolchain is available online [2].	c++;capability maturity model;compile time;compiler;deflate;field-programmable gate array;generic programming;hardware description language;late binding;lempel–ziv–storer–szymanski;logic synthesis;modelsim;overhead (computing);scheduling (computing);simulation;speech synthesis;subroutine;systemverilog;toolchain;usability;vhdl;verilog	Ivan Shcherbakov;Christian Weis;Norbert Wehn	2011	FDL 2011 Proceedings		data compression;embedded system;computer architecture;parallel computing;c++;vhdl;computer science;operating system;hardware description language;programming language;field-programmable gate array	PL	3.4253755864790154	50.83777163596305	159773
d55615cde925e542333ae33ff1644adb525fd7db	power estimation for architectural exploration of hw/sw communication on system-level buses	switching activity;microprocessors;communication interface;memory communication;hardware software codesign;systems analysis hardware software codesign encoding;power estimation;power budget;video processing;chip;system buses;real microprocessor;systems analysis;ip;energy consumption;system design;power dissipation;system level buses;stream generator power estimation architectural exploration hw sw communication system level buses power budget switching activity power optimization techniques system design memory communication real microprocessor;power optimization;architectural exploration;bandwidth;power generation;power system modeling energy consumption power dissipation encoding power generation microprocessors system buses bandwidth capacitance costs;capacitance;memory hierarchy;power consumption;power system modeling;power optimization techniques;encoding;hw sw communication;stream generator	The power consumption due to the HW/SW communication on system-level buses represents one of the major contributions to the overall power budget. A model to estimate the switching activity of the on-chip and off-chip buses at the system-level has been defined to evaluate the power dissipation and to compare the effectiveness of power optimization techniques. The paper aims at providing a framework for architectural exploration of a system design, focusing on the power consumption estimation of memory communication. Experimental results, conducted on bus streams generated by a real microprocessor and a stream generator, show how the variation of cache parameters and the introduction of bus encoding at the different levels on the memory hierarchy can affect the system power dissipation. Therefore, the proposed model can be effectively adopted to appropriately configure the memory hierarchy and the system bus architecture from the power standpoint.	bus encoding;mathematical optimization;memory hierarchy;microprocessor;norm (social);shattered world;system bus;systems design	William Fornaciari;Donatella Sciuto;Cristina Silvano	1999		10.1145/301177.301516	embedded system;power-flow study;parallel computing;real-time computing;engineering;local bus;system bus	EDA	1.8300478872038752	54.94976869532915	159775
31408dff05a8723926c52eaa28b2b5b349c41b06	removing context memory from a multi-context dynamically reconfigurable processor	dynamically reconfigurable processors;cache storage;memory management;clocks;reconfigurable architectures;program control structures;reconfigurable architectures cache storage program compilers program control structures;loading;energy consumption context memory removal multicontext dynamically reconfigurable processor configuration cache multicontext dynamically reconfigurable processing array drpa loop separation for keeping datapath lskd loops compiler back ground configuration data loading time multicasting configuration data two dimensional bit map differential loading spare register semiconductor area;multi context memory;context registers switches loading clocks hardware memory management;registers;multi context memory dynamically reconfigurable processors;program compilers;switches;context;hardware	Although context memory or configuration cache is a key mechanism for quick dynamic reconfiguration of multi-context Dynamically Reconfigurable Processing Array (DRPA), it requires a large amount of area and energy. In order to save them, methods to remove the context memory from multi-context DRPA are proposed. In order to keep a context without switching, Loop Separation for Keeping Datapath (LSKD)is introduced. By separating loops by the compiler and some additional hardware, the same context can be used without switching in a certain clock cycles. The back-ground configuration data loading time can be reduced by multicasting configuration data with two dimensional bit-map. For further reduction, the differential loading and spare register are proposed. With combination of them, the increasing execution time is only up to 12-13% if the target application does not have loop-carried dependency. With the above overhead on the performance, the semiconductor area becomes 63%, and the energy consumption is reduced to 40%, thus, the performance per cost or energy is much improved.	bitmap;clock signal;compiler;datapath;multicast;nx bit;overhead (computing);reconfigurability;reconfigurable computing;run time (program lifecycle phase);semiconductor;switching loop	Hideharu Amano;Masayuki Kimura;Nobuaki Ozaki	2012	2012 IEEE 6th International Symposium on Embedded Multicore SoCs	10.1109/MCSoC.2012.35	computer architecture;parallel computing;real-time computing;computer science	Arch	-1.8313706349461427	52.26144391712911	160145
d90054af17304c88fbb4945a97767b8a257b0cae	applications acceleration through adaptive hardware components	high performance computing systems high level synthesis embedded systems;hybrid high performance architectures adaptive hardware components high level synthesis automatic flows hardware accelerators behavioral description digital signal processing centralized finite state machine parallelism exploitation instruction level parallelism single execution flow hardware architectures programming languages thread level parallelism adaptive accelerator design fsm execution paradigm dynamic parallel execution lightweight hardware modules execution completion execution flows embedded systems accelerators;signal processing embedded systems finite state machines high level synthesis parallel processing;embedded systems;high level synthesis;finite state machines;signal processing;high performance computing systems;hardware design automation computer architecture runtime registers processor scheduling;parallel processing	High Level Synthesis (HLS) provides automatic flows for the generation of hardware accelerators starting from their behavioral description. HLS guarantees results comparable to hand-written design for some applications domains such as Digital Signal Processing. However, it is not yet able to cope with performance requirements when scaling the application complexity. One of the biggest limitation is an execution paradigm still based on the construction of a centralized Finite State Machine (FSM). Parallelism exploitation is thus bound to Instruction Level Parallelism within a single execution flow. This is in contrast to the current trends for hardware architectures and programming languages, which are progressively moving towards execution paradigms dominated other type of parallelisms, such as Task or Thread Level Parallelism. This work proposes a novel adaptive accelerator design, not based on the FSM execution paradigm, which provides support to dynamic parallel execution. Execution is parallel, because no pre-computed scheduled is considered. Operations are directly managed by dedicated lightweight hardware modules, which directly communicate to notify execution completion and to start other dependent operations. Execution is parallel, because several execution flows may run concurrently. The proposed design targets different application domains, from Embedded Systems accelerators to hybrid high-performance architectures.	application domain;centralized computing;digital signal processing;embedded system;finite-state machine;hardware acceleration;high-level synthesis;image scaling;instruction-level parallelism;parallel computing;precomputation;programming language;programming paradigm;requirement;task parallelism	Vito Giovanni Castellana;Fabrizio Ferrandi	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.244	computer architecture;parallel computing;real-time computing;computer science;task parallelism	Arch	-0.5139880265144106	49.65739569723141	160231
9ba1044f3c3ab9554a5498d92b30e4d1ce4040c9	on-chip diagnosis for early-life and wear-out failures	circuit faults;integrated circuit testing failure analysis fault diagnosis;maintenance engineering;system on a chip;failure analysis;dictionaries;integrated circuit modeling;integrated circuit testing;robustness;trax dictionaries on chip diagnosis early life failure wear out failure integrated system robustness centers fault dictionaries fault location memory storage unspecified transition fault model transition x fault model hierarchical dictionary fault avoidance scalable architecture opensparc t2 processor;fault diagnosis;circuit faults dictionaries system on a chip integrated circuit modeling delay maintenance engineering robustness	One approach for achieving integrated-system robustness centers on performing test during runtime, identifying the location of any faults (or potential faults), and repairing or avoiding the affected portion of the system. Fault dictionaries can be used to locate faults but conventional approaches require significant memory storage and are therefore limited to simplistic fault types. To overcome these limitations, three contributions are made that include: (i) enhancement of an unspecified transition fault model (called here the transition-X fault model, or TRAX for short) for capturing the misbehaviors expected from scaled technologies, (ii) development of a new type of hierarchical dictionary that only localizes to the level of repair or fault avoidance, and (iii) the design of a scalable architecture for retrieving and using the hierarchical dictionary for performing on-chip diagnosis. Experiments involving various circuits, including the OpenSPARC T2 processor, demonstrate that early-life and wear-out failures can be accurately diagnosed with minimum overhead using TRAX dictionaries that are up to 2600x smaller than full-response dictionaries.	data compaction;dictionary;fault model;internationalization and localization;negative-bias temperature instability;opensparc;overhead (computing);scalability;software propagation;standard cell;ultrasparc t2	Matthew Beckler;R. D. Blanton	2012	2012 IEEE International Test Conference	10.1109/TEST.2012.6401580	maintenance engineering;system on a chip;reliability engineering;embedded system;failure analysis;electronic engineering;real-time computing;fault coverage;fault indicator;computer science;engineering;stuck-at fault;fault model;robustness	EDA	8.76876023287095	59.19263031309767	160295
0569b2521447937656f6049cbd6fd52c18da3c1b	integrated code generation for loops	modulo scheduling;register allocation;optimal method;code generation;engineering and technology;teknik och teknologier;clustered vliw architectures;integer linear program	Code generation in a compiler is commonly divided into several phases: instruction selection, scheduling, register allocation, spill code generation, and, in the case of clustered architectures, cluster assignment. These phases are interdependent; for instance, a decision in the instruction selection phase affects how an operation can be scheduled We examine the effect of this separation of phases on the quality of the generated code. To study this we have formulated optimal methods for code generation with integer linear programming; first for acyclic code and then we extend this method to modulo scheduling of loops. In our experiments we compare optimal modulo scheduling, where all phases are integrated, to modulo scheduling, where instruction selection and cluster assignment are done in a separate phase. The results show that, for an architecture with two clusters, the integrated method finds a better solution than the nonintegrated method for 27% of the instances.	assignment (computer science);code generation (compiler);compiler;directed acyclic graph;experiment;instruction selection;integer programming;interdependence;linear programming;modulo operation;register allocation;scheduling (computing)	Mattias V. Eriksson;Christoph W. Kessler	2012	ACM Trans. Embedded Comput. Syst.	10.1145/2180887.2180896	parallel computing;real-time computing;computer science;theoretical computer science;operating system;instruction scheduling;programming language;register allocation;code generation	PL	-0.8996020362809735	51.982645106429175	160346
3a277454cc72c9bea9f562206a3a5ec59c9d5dd0	introducing slambench, a performance and accuracy benchmarking methodology for slam	benchmarking;kernel;paper;performance;computer vision;cuda;conference paper;accuracy;three dimensional displays accuracy benchmark testing simultaneous localization and mapping graphics processing units kernel cameras;nvidia geforce gtx titan;three dimensional displays;graphics processing units;simultaneous localization and mapping;nvidia;algorithms;arm;simultaneous localisation and mapping slambench accuracy benchmarking methodology publicly available software framework rgb d slam system kinectfusion c openmp opencl cuda icl nuim dataset synthetic rgb d sequences energy consumption;computer science;opencl;benchmark testing;cameras;slam robots c language control system analysis computing image colour analysis image sequences parallel architectures public domain software robot vision	Real-time dense computer vision and SLAM offer great potential for a new level of scene modelling, tracking and real environmental interaction for many types of robot, but their high computational requirements mean that use on mass market embedded platforms is challenging. Meanwhile, trends in low-cost, low-power processing are towards massive parallelism and heterogeneity, making it difficult for robotics and vision researchers to implement their algorithms in a performance-portable way. In this paper we introduce SLAMBench, a publicly-available software framework which represents a starting point for quantitative, comparable and validatable experimental research to investigate trade-offs in performance, accuracy and energy consumption of a dense RGB-D SLAM system. SLAMBench provides a KinectFusion implementation in C++, OpenMP, OpenCL and CUDA, and harnesses the ICL-NUIM dataset of synthetic RGB-D sequences with trajectory and scene ground truth for reliable accuracy comparison of different implementation and algorithms. We present an analysis and breakdown of the constituent algorithmic elements of KinectFusion, and experimentally investigate their execution time on a variety of multicore and GPU-accelerated platforms. For a popular embedded platform, we also present an analysis of energy efficiency for different configuration alternatives.	algorithm;c++;cuda;computer vision;embedded system;experiment;graphics processing unit;ground truth;icl;low-power broadcasting;multi-core processor;opencl api;openmp;parallel computing;real-time transcription;requirement;robotics;run time (program lifecycle phase);simultaneous localization and mapping;software framework;synthetic intelligence	Luigi Nardi;Bruno Bodin;M. Zeeshan Zia;John Mawer;Andy Nisbet;Paul H. J. Kelly;Andrew J. Davison;Mikel Luján;Michael F. P. O'Boyle;Graham D. Riley;Nigel P. Topham;Stephen B. Furber	2015	2015 IEEE International Conference on Robotics and Automation (ICRA)	10.1109/ICRA.2015.7140009	embedded system;benchmark;computer vision;kernel;simulation;performance;computer science;accuracy and precision;arm architecture;benchmarking;computer graphics (images);simultaneous localization and mapping	Robotics	0.15121214896132978	46.960383704850976	160422
e6ea06f2766cb2c807a9cc5e8d8b4904bd23c52e	sunos on sparc	context switching sparc risc cpu cisu cpus port sunos unix operating system scalable processor architecture reduced instruction set computer microcode complex instruction set computer virtual memory management trap handling;parallel architectures operating systems computers;kernel operating systems hardware computer architecture sun memory management registers protection trademarks switches	A description is given of the port of SunOS, a derivative of the Unix operating system, to the scalable processor architecture (SPARC) RISC (reduced-instruction-set-computer) CPU. SPARC makes tradeoffs between hardware and software complexity. Operations implemented by microcode on CISC (complex-instruction-set computer) CPUs are supported by low-level operating system routines. The discussion covers virtual memory management, trap handling, and context switching.<<ETX>>	central processing unit;high- and low-level;memory management;microcode;operating system;programming complexity;sparc;scalability;sunos;trap (computing);unix	Steve R. Kleiman;Dock Williams	1988	Digest of Papers. COMPCON Spring 88 Thirty-Third IEEE Computer Society International Conference	10.1109/CMPCON.1988.4876	classic risc pipeline;reduced instruction set computing;computer architecture;parallel computing;register window;computer science;operating system;instruction set	Arch	-1.4143099580518699	47.31458367418979	160523
8edf22fe6de370d02b7d4792ff8808244a7b0376	tegas2 - anatomy of a general purpose test generation and simulation system for digital logic	digital logic;system design;test generation	This paper will attempt to consider requirements and problems encountered in the development of digital logic simulation and test generation systems. The procedure for doing this will be to first consider requirements and general considerations for a particular simulation system (TEGAS2<supscrpt>(1,2)</supscrpt><underline>TE</underline>ST <underline>G</underline>ENERATION <underline>A</underline>ND <underline>S</underline>IMULATION) and then to dissect the system into its major constituent parts with a discussion of adopted techniques and experiences. It is obvious that a detailed discussion of this sort would require far more space than permitted in these proceedings. Therefore, an attempt will be made to discuss the most important considerations for system design and development.	boolean algebra;experience;logic simulation;norsk data;requirement;systems design	Stephen A. Szygenda	1972		10.1145/800153.804937	boolean algebra;electronic engineering;computer science;engineering;electrical engineering;algorithm;systems design	EDA	9.88417766514898	51.60960972776123	160825
c1367b86a54040961822e0469f10225bb5fe555c	a methodology for process design simulators of mems based on vrml and java	electronic engineering education computer based training micromechanical devices electronic design automation java virtual reality languages;pedagogical simulation;mems;java programming;virtual reality languages;process design;micromechanical devices;process design micromechanical devices java animation microelectromechanical devices fabrication computational modeling education internet virtual reality;process design steps process design simulators mems vrml java animated 3d process design simulators students;computer based training;electronic engineering education;vrml;student learning;interactive computer based learning environment;java;electronic design automation	The paper presents an efficient way to produce pedagogical, fully interactive and animated 3D process design simulators for MEMS. The aim of such simulators is to make the student learn the logical suite of process design steps that have to be taken to produce correct MEMS. By means of VRML, the learner can visualize the studied MEMS device at any stage of its fabrication from any angle. The associated Java program checks that the actions performed by the student are done in the right order with respect to time.		François Pêcheux	2001		10.1109/MSE.2001.932401	process design;embedded system;simulation;vrml;electronic design automation;computer science;operating system;java;computer engineering;computer graphics (images)	EDA	8.637475263642635	51.12498225454129	160961
6b29c8f2547862d1853cd26244d9b2bee7f6208e	arm next generation 64bit processors for power efficient compute	computers;word length 64 bit arm next generation processors power efficient compute mobile computer power efficiency energy cost energy efficiency consumer products server arm cortex a50 series processors;power aware computing microcontrollers;program processors mobile communication next generation networking computers battery charge measurement power measurement;mobile communication;battery charge measurement;program processors;next generation networking;power measurement	Summary form only given. Since the first mobile computer, power efficiency was a key measure for success. As the need for performance ever increases, the energy cost of performance has metric well beyond just the life of the battery in mobile. Energy efficiency is now the driver in most consumer products, the compute density of a server, and has become the primary limit in the delivery of high performance. During this talk we will consider the various power related limitations of compute while discovering how the techniques and new capabilities introduced through the ARM Cortex A50 series processors into computing bring the flexibility to address the limitations of the traditional computing approach.	64-bit computing;arm architecture;central processing unit;mobile computing;next-generation network;performance per watt;server (computing)	John Goodacre	2013	2013 International Symposium onVLSI Design, Automation, and Test (VLSI-DAT)	10.1109/VLDI-DAT.2013.6533862	embedded system;real-time computing;computer hardware;computer science	Arch	-1.6855144465810747	57.94321942325492	161041
ba2eef7dcaa49af9b51fd954f06801f50c5d0bee	floorplan-driven high-level synthesis for distributed/shared-register architectures	behavioral synthesis;high level synthesis	In this paper, we propose a high-level synthesis method targeting distributed/shared-register architectures. Our method repeats (1) scheduling/FU binding, (2) register allocation, (3) register binding, and (4) module placement. By feeding back floorplan information from (4) to (1), our method obtains a distributed/shared-register architecture where its scheduling/binding as well as floorplaning are simultaneously optimized. Experimental results show that the area is decreased by 13.2% while maintaining the performance of the circuit equal with that using distributed-register architectures.	algorithm;high- and low-level;high-level programming language;high-level synthesis;processor register;register allocation;run time (program lifecycle phase);scheduling (computing)	Akira Ohchi;Shunitsu Kohara;Nozomu Togawa;Masao Yanagisawa;Tatsuo Ohtsuki	2008	IPSJ Trans. System LSI Design Methodology	10.2197/ipsjtsldm.1.78	computer architecture;parallel computing;real-time computing;computer science;high-level synthesis	EDA	-0.9602425289108945	51.99039681932533	161054
c641f5660bba45225656156e152352fa0d9fed53	design of boot loader with multiple communication port	software;embedded microprocessor system;communication interface;protocols;random access memory;linux operating system kernel;operating system kernels embedded systems linux;embedded system;embedded systems;operating system;registers;multiple communication port;operating systems microprocessors embedded system resource management hardware digital signal processing linux kernel communication system software embedded software;usb port boot loader multiple communication port embedded operating system booting program embedded microprocessor system linux operating system kernel software designing file downloading communication interface uart ethernet0;system development;linux;operating system kernels;software design;uart;boot loader;ethernet networks;file downloading;embedded operating system booting program;usb port;software designing;hardware;ethernet0	An embedded system with complex embedded microprocessor inner resources and plentiful peripheral devices cannot work efficiently without the management of operating system. How to develop Boot Loader on the basis of the specific hardware platform is a key point and difficulty without question. The concept and function of embedded operating system booting program-Boot Loader which has multiple communication ports is introduced. In the ARM & DSP dual-core embedded microprocessor system development platform, by loading Linux operating system kernel, the authors expatiate on software designing methods of file downloading via three common communication interfaces (UART, Ethernet0, USB port).	arm architecture;booting;download;embedded operating system;embedded system;kernel (operating system);linux;microprocessor;multi-core processor;peripheral;usb	Pei Ke;Zhang Gang;Li Fu-jiang	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1285	embedded system;communications protocol;embedded operating system;real-time computing;universal asynchronous receiver/transmitter;computer science;software design;operating system;processor register;linux kernel	Embedded	3.621139541781405	48.21101679861373	161096
2cac6e84d3d7fed13ec9a5d39fd2bd6e75423578	fine-grained power modeling for smartphones using system call tracing	energy;mobile;mobile device;energy consumption;power optimization;smartphone;energy estimate;mobile application;device driver;power modeling	Accurate, fine-grained online energy estimation and accounting of mobile devices such as smartphones is of critical importance to understanding and debugging the energy consumption of mobile applications. We observe that state-of-the-art, utilization-based power modeling correlates the (actual) utilization of a hardware component with its power state, and hence is insufficient in capturing several power behavior not directly related to the component utilization in modern smartphones. Such behavior arise due to various low level power optimizations programmed in the device drivers. We propose a new, system-call-based power modeling approach which gracefully encompasses both utilization-based and non-utilization-based power behavior. We present the detailed design of such a power modeling scheme and its implementation on Android and Windows Mobile. Our experimental results using a diverse set of applications confirm that the new model significantly improves the fine-grained as well as whole-application energy consumption accuracy. We further demonstrate fine-grained energy accounting enabled by such a fined-grained power model, via amanually implemented eprof, the energy counterpart of the classic gprof tool, for profiling application energy drain.	android;debugging;device driver;gprof;graceful exit;microsoft windows;mobile app;mobile device;smartphone;system call;windows mobile	Abhinav Pathak;Y. Charlie Hu;Ming Zhang;Paramvir Bahl;Yi-Min Wang	2011		10.1145/1966445.1966460	embedded system;real-time computing;energy;computer hardware;computer science;operating system;mobile technology;mobile device;power optimization	OS	-3.243857758098798	56.48427873946643	161211
0bb3503fd05a6dd1304171503120518f2795bb1b	voltage-frequency island partitioning for gals-based networks-on-chip	networks on chip algorithms design voltage frequency island gals multi processor systems;voltage frequency island partitioning;logic design;network on chip;voltage frequency island;gals based network on chip;chip;fine grain system level power management;low power;energy consumption;threshold voltage;energy consumption network on a chip clocks signal design energy management power system management design methodology threshold voltage field programmable gate arrays prototypes;power management;multicore soc design;modular design;algorithms;design;networks on chip;network on chip energy consumption logic design;power consumption;gals;globally asynchronous locally synchronous;low power consumption;multi processor systems;fine grain system level power management gals based network on chip voltage frequency island partitioning multicore soc design energy consumption power consumption;design methodology	Due to high levels of integration and complexity, the design of multi-core SoCs has become increasingly challenging. In particular, energy consumption and distributing a single global clock signal throughout a chip have become major design bottlenecks. To deal with these issues, a globally asynchronous, locally synchronous (GALS) design is considered for achieving low power consumption and modular design. Such a design style fits nicely with the concept of voltage-frequency islands (VFIs) which has been recently introduced for achieving fine-grain system-level power management. This paper proposes a design methodology for partitioning an NoC architecture into multiple VFIs and assigning supply and threshold voltage levels to each VFI. Simulation results show about 40% savings for a real video application and demonstrate the effectiveness of our approach in reducing the overall system energy consumption. The results and functional correctness are validated using an FPGA prototype for an NoC with multiple VFIs.	asynchronous circuit;bottleneck (software);clock signal;correctness (computer science);fits;field-programmable gate array;globally asynchronous locally synchronous;modular design;multi-core processor;network on a chip;power management;prototype;simulation;system on a chip	Ümit Y. Ogras;Radu Marculescu;Puru Choudhary;Diana Marculescu	2007	2007 44th ACM/IEEE Design Automation Conference	10.1145/1278480.1278509	chip;embedded system;design;electronic engineering;real-time computing;telecommunications;computer science;engineering;operating system;network on a chip;threshold voltage;modular design;algorithm	EDA	5.130647776820142	54.23239822225645	161311
3ba9bebaff69571dc4e0e372fcbf238e5ad3ee4c	technologies for low latency interconnection switches	engineering design;fault tolerant;interconnection network;low latency;high performance;parallel processing	This paper presents an engineering design for a low latency high bandwidth interconnection network which will form the switching substrate for a multi-model parallel processing system. The performance is enhanced with a variety of approaches covering interconnection protocols, routing, fault tolerance, advanced packaging, and electrical interconnection techniques. The synergistic application of these technologies leads to a high performance design.	computer architecture;engineering design process;fault tolerance;interconnection;network switch;parallel computing;programming model;prototype;routing;synergy	Thomas F. Knight	1989	SIGARCH Computer Architecture News	10.1145/121956.121963	embedded system;parallel processing;fault tolerance;parallel computing;real-time computing;computer science;operating system;low latency	Arch	-2.926143502653786	48.15570488652877	161326
f6d2893ee0690fafc8107a3b9dc50e68d851403e	programming a microcoded processor for speech waveform generation	hardware realization;terminal analog model;hardware cost;human vocal tract;specialized task;microcoded processor;speech waveform generation;vocal tract	One effective way to exploit decreasing hardware costs is to build processors tailored to highly specialized tasks. Our experience with the specification, design and programming of a hardware realization of a terminal analog model of the human vocal tract are discussed.	central processing unit;microcode;tract (literature);waveform	Gary E. Kopec;Glen S. Miranker	1978			vocal tract;speech recognition;computer science	Arch	5.320814080769965	47.19153651134795	161363
07dbdd015d37c5efcc6b8b82041116378e984536	optimal general offset assignment	address code generation;digital signal processors;datorsystem;computer systems;embedded systems;integer programming;offset assignment;compiler optimization;application specific processors;inbaddad systemteknik;branch and cut	We present an exact approach to the General Offset Assignment problem arising in the domain of address code generation for application specific and digital signal processors. General Offset Assignment is composed of two subproblems, namely to find a permutation of variables in memory and to select a responsible address register for each access to one of these variables. Our method is a combination of established techniques to solve both subproblems using integer linear programming. To the best of our knowledge, it is the first approach capable of solving almost all instances of the established OffsetStone benchmark set to global optimality within reasonable time. We provide a first comprehensive evaluation of the quality of several state-of-the-art heuristics relative to the optimal solutions.	assignment problem;benchmark (computing);central processing unit;code generation (compiler);digital signal processor;heuristic (computer science);integer programming;linear programming;memory address register	Sven Mallach;Roberto Castañeda Lozano	2014		10.1145/2609248.2609251	mathematical optimization;digital signal processor;parallel computing;integer programming;computer science;theoretical computer science;optimizing compiler;weapon target assignment problem;branch and cut	PL	-0.4678663924908749	52.14320348579399	161470
558f1b5f20f6bd61baf988101ef573285d1bb7c9	run-time partial reconfiguration for removal, placement and routing on the virtex-ii pro	dynamic partial reconfiguration;partial reconfiguration;reconfigurable computing;routing;reconfigurable computing run time partial reconfiguration fpga routing;reconfigurable architectures;xilinx tools;fpga;run time partial reconfiguration;network routing;chip;general purpose processor;reconflgurable computing;xilinx virtex ii pro;runtime routing hardware field programmable gate arrays delay laboratories logic joining processes shape filters;reconfigurable architectures field programmable gate arrays network routing;reconflgurable hardware structure;field programmable gate arrays;xilinx virtex ii pro run time partial reconfiguration reconflgurable computing general purpose processor reconflgurable hardware structure fpga dynamic partial reconfiguration hardware modules xilinx tools;hardware modules;reconfigurable hardware	Reconfigurable computing entails the utilization of a general-purpose processor augmented with a reconfigurable hardware structure (usually an FPGA). Normally, a complete reconfiguration is needed to change the functionality of the FPGA even when the change is minor. Moreover, the complete chip needs to be halted to perform the reconfiguration. Dynamic partial reconfiguration (DPR) provides the possibility to change certain parts of the hardware while other parts of the FPGA remain in use. In this paper, we propose a solution using dynamic partial reconfiguration which provides a methodology to generate bitstreams for removal of 'old' hardware modules, and placement and routing of new hardware modules within an FPGA. Hardware modules may reside at any location and our solution can connect the additional functionality to the remaining running parts of the chip. In addition, bus macros are no longer necessary and we use the Xilinx tools only for generating the modules. We implemented our solution on a Xilinx Virtex-II Pro series FPGA, specifically the XC2VP30 on the XUP board, and demonstrated that the solution is fully functional.	extensible user interface protocol;field-programmable gate array;general-purpose markup language;place and route;reconfigurable computing;routing;virtex (fpga)	Stefan Raaijmakers;Stephan Wong	2007	2007 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2007.4380744	embedded system;routing;parallel computing;real-time computing;reconfigurable computing;computer science;field-programmable gate array	EDA	6.783517241377224	55.29383962798208	161513
30c9325d592372d3651a0cf6b93ac6443f34f106	esl design methodology for architecture exploration	high level languages;architecture exploration;programming language;transaction level model;test data compression;sockets;architecture tlm esl methodology memory;transaction level modeling;high level synthesis;integrated circuit design;memory architecture;digital systems;digital design;tlm;high level programming languages;abstraction level time to market register transfer level transaction level modeling system level design test data compression high level programming languages c systemc tlm 2 0 standard electronic system level architecture exploration digital design;system level design;abstraction level;sockets hardware switches design methodology memory architecture timing;circuit cad;time to market;methodology;tlm 2 0 standard;esl;c;switches;register transfer level;integrated circuit design circuit cad high level languages high level synthesis;architecture;memory;systemc;hardware;design methodology;electronic system level;timing	With the increasing complexity of digital systems, digital design techniques have evolved and got to a higher abstraction level which is Electronic System Level (ESL). Designing in this abstraction level needs new languages and methods to describe, synthesize, test, and verify systems. ESL design methodology is a way to handle the complexity of designing digital systems, and decrease time-to-market by starting the design from higher abstraction level than Register Transfer Level (RTL). Transaction Level Modeling (TLM) has emerged in the direction of system level design. TLM is originally based on high level programming languages such as C++ and SystemC. In this paper a hardware-aware ESL design methodology is proposed. The proposed methodology includes several levels containing various architecture models and guidelines for design at each level. A Test Data Compression (TDC) system is implemented as a case study using TLM-2.0 standard.	abstraction layer;c++;concurrency (computer science);data compression;digital electronics;electronic system-level design and verification;high-level programming language;level design;logic synthesis;message passing;pipeline (computing);register-transfer level;systemc;test data;transaction-level modeling	Fatemeh Javaheri;Zainalabedin Navabi	2010	2010 East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2010.5742064	computer architecture;parallel computing;real-time computing;computer science;electronic system-level design and verification	EDA	6.4061845855943815	52.360913917571615	161517
05220e235846467c8db08fb2c52ff4a4423b7b0a	run-time resources management on coarse grained, packet-switching reconfigurable architecture: a case study through the apaches’ platform	parallelisme;arquitectura red;posicionamiento;image coding;image processing;integrated circuit;execution time;red conmutacion por paquete;network on chip;flexibilidad;reconfigurable architectures;estudio comparativo;concepcion optimal;resource manager;resource management;conception optimale;procesamiento imagen;telecommunication network;circuito integrado;packet switched;packet switched network;architecture reseau;carta de datos;traitement image;system on a chip;interconnection network;reseau commutation paquet;algorithme;etude comparative;algorithm;codage image;gestion recursos;positioning;reconfigurable architecture;parallelism;compression image;paralelismo;image compression;estudio caso;sistema sobre pastilla;matriz formadora;red telecomunicacion;mappage;comparative study;reseau telecommunication;etude cas;optimal design;die;gestion ressources;temps execution;flexibilite;computer application;mapping;network architecture;systeme sur puce;task graphs;coarse grained;tiempo ejecucion;matrice formage;architecture reconfigurable;red interconexion;circuit integre;flexibility;positionnement;algoritmo;compresion imagen;reseau interconnexion	The increasing number of cores used on a single die in response to the power-computing applications tends to orient SoCs more and more toward communication-centric concept. Networks-on-chip (NoC) are good candidates providing both parallelism and flexibility. Nevertheless they imply to consider the notion of locality when distributing the computation among a set of cores. Defining an optimal placement at compile-time is difficult since other applications may temporarily make use of some of the processing resources. This paper explores the opportunity of dynamically mapping task graphs through using different placement algorithms, experiments and comparisons are conducted on a homogeneous coarse-grain reconfigurable architecture running JPEG applications. Results show that run-time task mapping is possible and brings interesting benefits over a random or static placement, especially when contention effects stemming from the communication medium are taken into account.	network switch;packet switching	Alex Ngouanga;Gilles Sassatelli;Lionel Torres;Thierry Gil;André Borin Soares;Altamiro Amadeu Susin	2006		10.1007/11802839_19	system on a chip;embedded system;simulation;network architecture;telecommunications;image compression;computer science;optimal design;resource management;integrated circuit;comparative research;die;packet switching;telecommunications network	HPC	-0.5482548966346762	54.43758429371692	161731
3978d8c546d128d8c02bc5f262da12587aa72950	the energy efficiency of cmp vs. smt for multimedia workloads	dynamic voltage frequency scaling;energy efficiency;processor architecture;multimedia;energy efficient;real time;multimedia application;clock gating;out of order;chip;mathematical model;smt;capacitance voltage;design space exploration;simultaneous multithreading;cmp;adaptive architecture	This paper compares the energy efficiency of chip multiprocessing (CMP) and simultaneous multithreading (SMT) on modern out-of-order processors for the increasingly important multimedia applications. Since performance is an important metric for real-time multimedia applications, we compare configurations at equal performance. We perform this comparison for a large number of performance points derived using different processor architectures and frequencies/voltages.We find that for the design space explored, for each workload, at each performance point, CMP is more energy efficient than SMT. The difference is small for two thread systems, but large (18% to 44%) for four thread systems. We also find that the best SMT and the best CMP configuration for a given performance target have different architecture and frequency/voltage. Therefore, their relative energy efficiency depends on a subtle interplay between various factors such as capacitance, voltage, IPC, frequency, and the level of clock gating, as well as workload features. We perform a detailed analysis considering these factors and develop a mathematical model to explain these results.Although CMP shows a clear energy advantage for four-thread (and higher) workloads, it comes at the cost of increased silicon area. We therefore investigate a hybrid solution where a CMP is built out of SMT cores, and find it to be an effective compromise. Finally, we find that we can reduce energy further for CMP with a straightforward application of previously proposed techniques of adaptive architectures and dynamic voltage/frequency scaling.	central processing unit;clock gating;frequency scaling;image scaling;inter-process communication;mathematical model;multiprocessing;multithreading (computer architecture);real-time clock;simultaneous multithreading	Ruchira Sasanka;Sarita V. Adve;Yen-Kuang Chen;Eric Debes	2004		10.1145/1006209.1006238	parallel computing;real-time computing;computer hardware;computer science;operating system;efficient energy use	HPC	-3.892509934474489	55.44685446508826	161806
5387fc4d9ae5f206e2c92de7c222ffc05f6decd8	a versatile emulator for the aging effect of non-volatile memories: the case of nand flash	phase change materials;microprocessors;nand flash;non volatile memories;memory aging;emulation;aging;computer architecture;nonvolatile memory computer architecture microprocessors flash memories aging phase change materials emulation;nonvolatile memory;phase change memory;fpga emulator;fpga emulator non volatile memories nand flash phase change memory memory aging;nand flash chip fpga based platform aging effect nonvolatile memories reconfigurable hardware software architecture pcm memory related algorithms nonvolatile memory emulator;random access storage ageing field programmable gate arrays flash memories hardware software codesign nand circuits;flash memories	This work presents a versatile and flexible FPGA-based platform, designed for accurate emulation of the aging effect of non-volatile memories. The emulator is based on a reconfigurable hardware-software architecture which enables the accurate representation of various non-volatile memory technologies, like NAND Flash and PCM. The proposed architecture can be used for emulating memories at the cell, chip and system level, while the proposed hardware platform can be used as a valuable tool for the development and evaluation of memory-related algorithms and techniques. In this paper, we analyze the architecture of the non-volatile memory emulator, focusing mainly on the NAND Flash case, we present details about its internal functionality and, using experimental results, we demonstrate the high accuracy achieved when it is used to emulate a specific NAND Flash chip.	algorithm;bit error rate;computer data storage;emulator;experiment;field-programmable gate array;flash memory;multi-level cell;non-volatile memory;real-time clock;software architecture;volatile memory	Antonios Prodromakis;Stelios Korkotsides;Theodore Antonakopoulos	2014	2014 17th Euromicro Conference on Digital System Design	10.1109/DSD.2014.56	embedded system;emulation;parallel computing;non-volatile memory;phase-change memory;computer hardware;computer science;flash memory emulator	EDA	6.157501988704193	47.36181617600173	161948
b1add5445fe28bd64665cc90a7d1880c2729bef0	meta-assemblers	hardware design languages;computer languages;large scale integration;assembly systems;software tools;microprogramming;microcomputers	Meta-assemblers automate the construction of assemblers. The availability of microcomputers spurred the development of meta-assemblers, and many now exist. They are described and classified here.	assembly language;meta-process modeling;microcomputer	Emmanuel Skordalakis	1983	IEEE Micro	10.1109/MM.1983.291064	computer architecture;parallel computing;computer science;operating system;microcomputer;microcode;programming language	Visualization	7.23097206367439	51.47342491672536	161994
5554e64e278816cb980fea76959f6abe11ba5b4f	register binding for fpgas with embedded memory	minimisation;memory binding technique;registers field programmable gate arrays high level synthesis data structures costs embedded computing application software circuits logic design latches;minimization;total chip area;logic design;application software;resource bag;integrated memory circuits;fpga;chip;resource bag register binding technique fpga onchip embedded memory block high level synthesis minimization memory binding technique total chip area;input output;high level synthesis;registers;data structures;minimisation field programmable gate arrays high level synthesis integrated memory circuits;onchip embedded memory block;circuits;latches;field programmable gate arrays;embedded computing;register binding technique	The trend in new state-of-the-art FPGAs is to have large amounts of on-chip embedded memory blocks. These memory blocks are used to hold the input/output data for various applications. Existing register binding techniques in high-level synthesis aim at minimizing the storage requirements of circuits by sharing variables among registers and thus minimizing the required number of registers for a specific design. In this paper, a new technique is proposed that makes use of the existing embedded memory blocks and maps variables to these blocks. The proposed memory binding approach gives considerable performance increase over the existing register binding techniques. The memory binding technique resulted in up to 57% savings in the total chip area (number of logic cells/elements occupied on the FPGA) over the old register binding techniques for a small resource bag and up to 6% savings for a large resource bag.	algorithm;edram;embedded system;field-programmable gate array;high- and low-level;high-level synthesis;input/output;level of detail;map;memory bank;memory management;multiplexer;network congestion;place and route;processor register;random-access memory;register file;requirement;routing	Hassan Al Atat;Iyad Ouaiss	2004	12th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2004.49	embedded system;interleaved memory;computer architecture;semiconductor memory;parallel computing;real-time computing;data structure;computer science;processor register;registered memory;field-programmable gate array;memory management	EDA	-1.3637754736790118	53.017554543633025	162022
fce8f685b8081241a2088d99f4578345a3bed977	metric based multi-timescale control for reducing power in embedded systems	voltage control;low power requirements;ultra low power;computer aided design;optimisation;parallel linear systems;power saving;frequency control;error tolerance;ultra low power digital control architectures;satisfiability;linear system;embedded system;digital controller;control problem;computer architecture;micro electro mechanical system;embedded systems;lithography;control system;feedback;computational modeling;low power;low latency;power dissipation;multitimescale control;control systems embedded system digital control design automation real time systems embedded computing bandwidth delay micromechanical devices electromechanical sensors;low power electronics;process control;bandwidth;computer aided design multitimescale control embedded systems control loop bandwidth low power requirements micro electro mechanical systems sensors ultra low power digital control architectures lithography microstructures parallel linear systems error tolerance linear feedback control systems system optimization;optimisation digital control electronic design automation embedded systems feedback lithography low power electronics;digital control;control loop bandwidth;microstructures;feedback control;linear feedback control systems;system optimization;hard real time;noise;micro electro mechanical systems sensors;electronic design automation	Digital control for embedded systems often requires low-power, hard real-time computation to satisfy high control-loop bandwidth, low latency, and low-power requirements. In particular, the emerging applications of Micro Electro-Mechanical Systems (MEMS) sensors, and their increasing integration, presents a challenging requirement to embed ultra-low power digital control architectures for these lithographically formed micro-structures. Controlling electromechanical structures of such a small scale, using naive digital controllers, can be prohibitively expensive (both in power and cost for portable or battery operated applications.) In this paper, we describe the potential for control systems to be transformed into a set of co-operating parallel linear systems and demonstrate, for the first time, that this parallelization can reduce the total number of instructions executed, thereby reducing power, at the expense of controlled loss in control fidelity. Since the error tolerance of linear feedback control systems is mathematically well-posed, this technique opens up a new, independent dimension for system optimization. We present a novel Computer-Aided Design (CAD) method to evaluate control fidelity, with varying timescales on the controller, and analyze the trade-off between performance and power dissipation. A CAD Metric for control fidelity is proposed and we demonstrate the potential for power savings using this decomposition on two different control problems.	computation;computer-aided design;control system;embedded system;error-tolerant design;feedback;high- and low-level;high-level synthesis;intel core (microarchitecture);linear system;low-power broadcasting;mathematical optimization;microelectromechanical systems;multi-core processor;parallel computing;program optimization;real-time clock;real-time computing;requirement;sensor;well-posed problem	Nitin Kataria;Forrest Brewer;João Pedro Hespanha;Timothy Sherwood	2009	2009 22nd International Conference on VLSI Design	10.1109/VLSI.Design.2009.16	lithography;control engineering;embedded system;electronic engineering;real-time computing;digital control;electronic design automation;computer science;engineering;control system;electrical engineering;operating system;process control;feedback	EDA	2.281558605607548	54.24256761018189	162379
198ebeb7fd8d005214728ace772734eac34d0707	fast processor core selection for wlan modem using mappability estimation	processor architecture;digital signal processing;processor architecture evaluation;wireless lan modems algorithm design and analysis computer architecture signal processing algorithms application specific integrated circuits resource management costs multimedia communication multimedia databases;hardware software codesign;cost function;wlan modem;performance;resource management;computer architecture;codesign;system synthesis;application specific integrated circuits;multimedia communication;multimedia databases;fast processor core selection;digital signal processing fast processor core selection wlan modem mappability estimation mappability metric performance cost metrics codesign system synthesis design space exploration baseband algorithms;hardware software codesign modems wireless lan;modems;wireless lan;design space exploration;signal processing algorithms;mappability metric;baseband algorithms;mappability estimation;algorithm design and analysis;cost metrics	Mappability metric and a novel method for evaluating the goodness of processor core and algorithm combinations are introduced. The new mappability concept is an addition to performance and cost metrics used in existing codesign and system synthesis approaches. The mappability estimation is based on the analysis of the correlation or similarity of algorithm and core architecture characteristics. It allows fast design space exploration of core architectures and mappings with little modeling effort. The method is demonstrated by analyzing suitable processor core architectures for baseband algorithms of the WLAN modem. 140400 architecture-algorithm pairs were analyzed in total and the estimated results were similar to the results of more detailed evaluations. The method is not, however, limited to the WLAN modem, but is applicable for digital signal processing in general.	algorithm;baseband;design space exploration;digital signal processing;intel core (microarchitecture);modem;multi-core processor	Juha-Pekka Soininen;Jari Kreku;Yang Qu;Martti Forsell	2002		10.1145/774789.774803	embedded system;parallel computing;real-time computing;computer science	EDA	2.3828951888393153	52.01087913716188	162435
2c9d05206dc03b01ab4b99d5424b3e024b605d7e	a codesign virtual machine for hierarchical, balanced hardware/software system modeling	verification;virtual machine;bottom up;time sharing computer systems;system modeling;routing;top down;processor scheduling;virtual machining;software systems;physical design;physics computing;multiterminal signal nets;computer networks;semantic model;computational modeling;permission;virtual machining hardware software systems processor scheduling computational modeling physics computing computer networks time sharing computer systems unified modeling language permission;unified modeling language;next generation;iterative design;electromigration;model of computation;steiner tree;hardware;current density;design methodology	The Codesign Virtual Machine (CVM) is introduced as a next generation system modeling semantic. The CVM permits unrestricted system-wide software and hardware behaviors to be designed to a single scheduling semantic by resolving time-based (resource) and time-independent (state-interleaved) models of computation. CVM hierarchical relationships of bus and clock state domains provide a means of exploring hardware/software scheduling trade-offs to a consistent semantic model using top-down, bottom-up and iterative design approaches from a high system level to the machine implementation. State domain partitionings permit run-time software schedulers to be resolved with design time physical scheduling as peer- and hierarchically-related architectural abstractions which cut across functional boundaries. The resultant abstraction provides “component-less” paths to physical design with greater accommodation of shared resource modeling. A simulation example is included.	bottom-up proteomics;iterative design;iterative method;model of computation;physical design (electronics);resultant;scheduling (computing);simulation;software system;systems modeling;top-down and bottom-up design;virtual machine	JoAnn M. Paul;Simon N. Peffers;Donald E. Thomas	2000		10.1145/337292.337506	embedded system;electronic engineering;parallel computing;real-time computing;computer science;theoretical computer science;operating system;top-down and bottom-up design;programming language;algorithm;computer network	Arch	1.7020907968039087	53.56704662750368	162451
239281abda9bc7b37190b2c9a46a24af1b7d9533	nrepo: normal basis recomputing with permuted operands	aes implementation nrepo normal basis recomputing with permuted operands hardware implementation cryptographic algorithm concurrent error detection ced redundant computational resources low cost advanced encryption standard implementation;polynomials;redundancy;logic gates;registers;magnetic resonance;redundancy cryptography;magnetic resonance hardware robustness redundancy logic gates registers polynomials;robustness;hardware	Hardware implementations of cryptographic algorithms are vulnerable to natural and malicious faults. Concurrent Error Detection (CED) can be used to detect these faults. We present NREPO, a CED which does not require redundant computational resources in the design. Therefore, one can integrate it when computational resources are scarce or when the redundant resources are difficult to harness for CED. We integrate NREPO in a low-cost Advanced Encryption Standard (AES) implementation with 8-bit datapath. We show that NREPO has 25 and 50 times lower fault miss rate than robust code and parity, respectively. The area, throughput, and power are compared with other CEDs on 45nm ASIC. The hardware overhead of NREPO is 34.9%. The throughput and power are 271.6Mbps and 1579.3μW, respectively. One can also implement NREPO in other cryptographic algorithms.	8-bit;algorithm;application-specific integrated circuit;computation;computational resource;cryptography;datapath;encryption;normal basis;operand;overhead (computing);parity bit;throughput	Xiaofei Guo;Debdeep Mukhopadhyay;Chenglu Jin;Ramesh Karri	2014	2014 IEEE International Symposium on Hardware-Oriented Security and Trust (HOST)	10.1109/HST.2014.6855581	parallel computing;real-time computing;logic gate;computer science;theoretical computer science;magnetic resonance imaging;processor register;redundancy;robustness;polynomial	Arch	8.723026589233799	60.25879494362036	162521
887816113a19e55dde351fe7eeea9390d0d9e6e9	closing the performance gap with modern c++		On the way to Exascale, programmers face the increasing challenge of having to support multiple hardware architectures from the same code base. At the same time, portability of code and performance are increasingly difficult to achieve as hardware architectures are becoming more and more diverse. Today’s heterogeneous systems often include two or more completely distinct and incompatible hardware execution models, such as GPGPU’s, SIMD vector units, and general purpose cores which conventionally have to be programmed using separate tool chains representing non-overlapping programming models. The recent revival of interest in the industry and the wider community for the C++ language has spurred a remarkable amount of standardization proposals and technical specifications in the arena of concurrency and parallelism. This recently includes an increasing amount of discussion around the need for a uniform, higher-level abstraction and programming model for parallelism in the C++ standard targeting heterogeneous and distributed computing. Such an abstraction should perfectly blend with existing, already standardized language and library features, but should also be generic enough to support future hardware developments. In this paper, we present the results from developing such a higher-level programming abstraction for parallelism in C++ which aims at enabling code and performance portability over a wide range of architectures and for various types of parallelism. We present and compare performance data obtained from running the well-known STREAM benchmark ported to our higher level C++ abstraction with the corresponding results from running it natively. We show that our abstractions enable performance at least as good as the comparable base-line benchmarks while providing a uniform programming API on all compared target architectures. c © Springer International Publishing AG 2016 M. Taufer et al. (Eds.): ISC High Performance Workshops 2016, LNCS 9945, pp. 18–31, 2016. DOI: 10.1007/978-3-319-46079-6 2 Closing the Performance Gap with Modern C++ 19	application programming interface;benchmark (computing);c++;c++17;closing (morphology);coherence (physics);concurrency (computer science);distributed computing;general-purpose computing on graphics processing units;human-centered computing;ibm notes;knights;lecture notes in computer science;locality of reference;memory hierarchy;parallel computing;programmer;programming model;simd;springer (tank);whole earth 'lectronic link	Thomas Heller;Hartmut Kaiser;Patrick Diehl;Dietmar Fey;Marc Alexander Schweitzer	2016		10.1007/978-3-319-46079-6_2		Arch	-1.5010429513328036	46.526511676817286	162551
f001876895997ba6bde1d118b5e9ec46f53829bb	switch and logic-level modeling in edif 200: limitations and proposed solutions	possible extension;edif writer;gate-level description;intermediate format;proposed solution;logicmodel view;industry standard;logic-level modeling;simulation environment	In order to transfer switch and gate-level descriptions from one simulation environment to another, information like connectivity, delay, and strength needs to be maintained. Since EDIF is rapidly becoming an industry standard, it can be used as an intermediate format for such transfers. This paper illustrates the effectiveness of EDIF 200 netlist and logicModel views for this purpose and identifies its limitations. Several EDIF writers have been implemented using these concepts. In addition, this paper discusses possible extensions to improve the expressiveness of EDIF.	and gate;logic level;netlist;simulation;technical standard	Shankar R. Mukherjee;Maqsoodul Mannan	1991				EDA	9.253389192685429	51.84491899753033	162737
85f4dc973c782e6bea17b9112efd3606eeba7692	the case for application specific compilers	processor architecture;simulation;resource aware programming;mpsoc;embedded computing	We believe it makes sense to develop compilers that are specific for particular application domains. In many areas of embedded computing, processor architectures are designed specifically to run a narrow band of application code very well. These architectures are unlike any the world has seen before and to program them is a challenge to say the least. CoSy's flexible compiler technology thrives in this area. By viewing the compiler as a means and not as a goal, it is possible to achieve spectacular results in a very short time-frame.	application domain;cosy (computer conferencing system);compiler;embedded system	Marcel Beemster	2011		10.1145/1988932.1988943	embedded system;computer architecture;parallel computing;real-time computing;microarchitecture;computer science;operating system;compiler construction;programming language	PL	0.4137975311298264	49.51092147975834	162807
