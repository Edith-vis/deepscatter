id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
779ab3f3e2b858a36104753050d6caaddc869815	celerity hardware implementation of the aes with data parallel and pipelining architecture inside the round function	des;data parallel;data pipelining architecture;security constrained applications;slice registers;high throughput aes;cipertext data;hardware clocks registers throughput pipeline processing encryption;plaintext data;encryption;clocks;slice lut celerity hardware implementation aes data parallel architecture data pipelining architecture round function data encryption standard des security constrained applications software applications hardware applications plaintext data cipertext data clock cycles s boxes look up tables altera fpga realization modelsim simulation slice registers gate level netlists;aes;software applications;modelsim simulation;look up tables;table lookup clocks cryptography field programmable gate arrays parallel architectures pipeline processing power aware computing;power aware computing;parallel architectures;data parallel architecture;registers;cryptography;hardware applications;data encryption standard;altera fpga realization;data parallel round function pipelining;round function;gate level netlists;field programmable gate arrays;high throughput;table lookup;round function pipelining;clock cycles;celerity hardware implementation;slice lut;pipeline processing;s boxes;throughput;hardware	Since it was accepted as the replacement of the Data Encryption Standard (DES) and 3 DES by NIST in 2001, the AES has played a major role in various security-constrained applications. Many applications are power-saving, resource constrained and require high-speed. AES is precisely suitable for being implemented in both software and hardware applications. Hardware implementation of AES has the advantage of increased throughput and better security. On the basis of those facts, in this paper, different hardware architectures of AES have been presented and surveyed. And we present a way of data parallel to deal with the bulk data of plaintext or cipertext, which brings lower clock cycles and increased working frequencies. Pipelining architecture inside the round function also contributed a lot to the promotion of data throughput. Moreover, the key schedule algorithm is pipelined to get the speedup, and the S-boxes which are based on look-up tables (LUTs) could be area-efficient. By conducting ALTERA FPGA realization and ModelSim simulation, we analyze the performance relating to the Slice registers, gate-level netlists, memory, Slice LUTs and so on.	algorithm;celerity computing;clock signal;computer architecture;computer hardware;data parallelism;encryption;field-programmable gate array;key schedule;lookup table;modelsim;pipeline (computing);plaintext;requirement;s-box;simulation;speedup;throughput	Shouwen Yang;Hui Li;Xiaotao Zhang;Gang Zhao	2013	2013 12th IEEE International Conference on Trust, Security and Privacy in Computing and Communications	10.1109/TrustCom.2013.210	computer architecture;parallel computing;computer science;theoretical computer science	EDA	8.981096305885597	45.106824884163686	46920
cf8b06a47aec465c394e068809acccbe5c5d882f	a versatile pipelined hardware implementation for encryption and decryption using advanced encryption standard	advanced encryption standard;hardware implementation	The Advanced Encryption System – AES is now used in almost all network-based applications to ensure security. In this paper, we propose a very efficient pipelined hardware implementation of AES128. The design is versatile as it allows both encryption and decryption. The core computation of AES, which is performed on data blocks of 128 bits, is iterated for several rounds, depending on the key size. The security strength of AES has been proven proportional to the number of rounds applied. we show that if the required number of rounds must increase to defeat attackers, the proposed implementation stays efficient.	algorithm;cipher;computation;cryptography;encryption;handy board;hardware restriction;iteration;key size;throughput	Nadia Nedjah;Luiza de Macedo Mourelle	2006		10.1007/978-3-540-71351-7_20	multiple encryption;advanced encryption standard;embedded system;encryption software;disk encryption theory;40-bit encryption;disk encryption;advanced encryption standard process;client-side encryption;computer science;theoretical computer science;operating system;on-the-fly encryption;disk encryption hardware;aes instruction set;aes implementations;computer security;encryption;probabilistic encryption;56-bit encryption	Crypto	8.3859901878452	45.24190523994515	47045
ef843ec44a90a09adc6ccebcc16bedb6210bbc1b	accelerating multiple sequence alignment with the cell be processor	cell broadband engine;technology and engineering;parallelization;bio informatics;multiple sequence alignment	The Cell Broadband Engine (BE) Architecture is a new heterogeneous multi-core architecture targeted at compute-intensive workloads. The architecture of the Cell BE has several features that are unique in high-performance general-purpose processors, most notably the extensive support for vectorization, scratch pad memories and explicit programming of direct memory accesses (DMAs) and mailbox communication. While these features strongly increase programming complexity, it is generally claimed that significant speedups can be obtained by using Cell BE processors. This paper presents our experiences with using the Cell BE architecture to accelerate Clustal W, a bio-informatics program for multiple sequence alignment. We report on how we apply the unique features of the Cell BE to Clustal W and how important each is in obtaining high performance. By making extensive use of vectorization and by parallelizing the application across all cores, we demonstrate a speedup of 24.4 times when using 16 synergistic processor units on a QS21 Cell Blade compared to single-thread execution on the power processing unit. As the Cell BE exploits a large number of slim cores, our highly optimized implementation is just 3.8 times faster than a 3-thread version running on an Intel Core2 Duo, as the latter processor exploits a small number of fat cores.	automatic vectorization;bioinformatics;british informatics olympiad;cell (microprocessor);central processing unit;clustalw/clustalx;design of the fat file system;general-purpose markup language;intel core (microarchitecture);multi-core processor;multiple sequence alignment;parallel computing;programming complexity;speedup;synergy;thread (computing)	Hans Vandierendonck;Sean Rul;Koen De Bosschere	2010	Comput. J.	10.1093/comjnl/bxp086	parallel computing;real-time computing;computer hardware;multiple sequence alignment;computer science;operating system;database	HPC	-0.9489514129722937	43.774243894847366	47051
7de4c361843671dcf11cb1a411ea8e9fa1337fcf	a migrating data driven architecture for signal processing	traitement signal;architecture systeme;multiprocessor;red petri;flot donnee;flujo datos;signal processing;arquitectura sistema;systeme parallele;parallel system;multiprocesador;data flow;system architecture;petri net;procesamiento senal;reseau petri;sistema paralelo;multiprocesseur	The paper presents a migrating operations data-driven multiprocessor architecture suited for digital signal processing. Three types of basic operations are first defined; this allows a simplified and efficient representation of algorithm execution in terms of Petri Nets. Data-flow primitives, packets and operators, allow the implementation of the algorithm into an architecture, which is a generalization of the well known data driven architecture. Operand packets allow migration of operation computing from one processor to the neighbouring ones in case of nonuniform computation load within the system. The architecture of the data driven processor is then examined in its basic blocks and an example of operation migration control strategy is shown. Different architectures suited for specific trade-off are proposed.	algorithm;basic block;computation;control theory;digital signal processing;multiprocessing;operand;petri net;whole earth 'lectronic link	Sergio Cesare Brofferio;Giuseppe Mastronardi	1990	European Transactions on Telecommunications	10.1002/ett.4460010207	reference architecture;embedded system;data flow diagram;parallel computing;real-time computing;multiprocessing;computer science;electrical engineering;signal processing;petri net;data architecture	Arch	2.0046977833641133	44.506729657101594	47078
a757eeb305eec4b66f53ed37393f0d6e2648bce3	evaluating matrix representations for error-tolerant computing		We propose a methodology to determine the suitability of different data representations in terms of their error-tolerance for a given application with accelerator-based computing. This methodology helps match the characteristics of a representation to the data access patterns in an application. For this, we first identify a benchmark of key kernels from linear algebra that can be used to construct applications of interest using any of several widely used data representations. This is then used in an experimental framework for studying the error tolerance of a specific data format for an application. As case studies, we evaluate the error-tolerance of seven dataformats on sparse matrix to vector multiplication, diagonal add, and two machine learning applications i) principal component analysis (PCA), which is a statistical technique widely used in data analysis and ii) movie recommendation system with Restricted Boltzmann Machine (RBM) as the core. We observe that the Dense format behaves well for complicated data accesses such as diagonal accessing but is poor in utilizing local memory. Sparse formats with simpler addressing methods and a careful selection of stored information, e.g., CRS and ELLPACK, demonstrate a better error-tolerance for most of our target applications.	benchmark (computing);data access;error-tolerant design;linear algebra;machine learning;principal component analysis;recommender system;restricted boltzmann machine;sparse matrix	Pareesa Ameneh Golnari;Sharad Malik	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		benchmark;sparse pca;kernel;sparse matrix;computer science;theoretical computer science;linear algebra;operating system;machine learning;data mining;computational model;algorithm;statistics;principal component analysis	HPC	-3.2087144830679475	44.509401444379264	47254
b93ad3ae242e5cea92d7b6d2fd1eff0e5cafe1a6	on-board feature extraction from acceleration data for activity recognition		Modern wearable devices are equipped with increasingly powerful microcontrollers and therefore are increasingly capable of doing computationally heavy operations, such as feature extraction from sensor data. This paper quantifies the time and energy costs required for on-board computation of features on acceleration data, the reduction achieved in subsequent communication load compared with transmission of the raw data, and the impact on daily activity recognition in terms of classification accuracy. The results show that platforms based on modern 32-bit ARM Cortex-M microcontrollers significantly benefit from on-board extraction of time-domain features. On the other hand, efficiency gains from computation of frequency domain features at the moment largely remain out of their reach.	32-bit;arm cortex-m;arm architecture;activity recognition;computation;feature extraction;microcontroller;on-board data handling;wearable technology	Atis Elsts;Ryan McConville;Xenofon Fafoutis;Niall Twomey;Robert J. Piechocki;Raúl Santos-Rodriguez;Ian Craddock	2018			real-time computing;microcontroller;frequency domain;acceleration;activity recognition;feature extraction;wearable technology;wearable computer;computer science;arm architecture	AI	2.9361118053955506	34.70749416652373	47443
a59826bc15679fb0f2283bb04db26a77789538c4	a flexible-length-arithmetic processor based on fdfm approach in fpgas	rsa;fpga;random access storage digital signal processing chips field programmable gate arrays public key cryptography;block rams;flexible length arithmetic processor processor cores software debugging software development rsa encryption software assembly program processor architecture arithmetic instructions xilinx virtex 6 fpga few dsp slices and few memory blocks approach fpga fdfm approach;montgomery modular multiplication multiple length arithmetic fpga dsp slices block rams rsa;dsp slices;digital signal processing random access memory field programmable gate arrays hardware computer architecture software debugging;multiple length arithmetic;montgomery modular multiplication	The main contribution of this paper is to present an intermediate approach of software and hardware using FPGAs. More specifically, we present a processor based on FDFM (Few DSP slices and Few Memory blocks) approach that supports arithmetic operations with flexibly many bits, and implement it in the Xilinx Virtex-6 FPGA. Arithmetic instructions of our processor architecture include addition, subtraction, and multiplication for numbers with variable size longer than 64 bits. To show the potentiality of our processor, we have implemented 2048-bit RSA encryption/decryption by software written by assembly program. The resulting processor uses only one DSP48E1 slice and two block RAMs, and RSA encryption software on it runs in 613.71ms. It has been shown that the direct hardware implementation of RSA encryption runs in 277.26ms. Although our intermediate approach is slower, it has several advantages. Since programs for the proposed processor can be written by software, the development and the debugging are easy. We have also succeeded in implementing 306 processor cores in one Xilinx Virtex-6 FPGA which work in parallel to improve the throughput greatly.	64-bit computing;central processing unit;debugging;encryption software;field-programmable gate array;rsa (cryptosystem);throughput	Tatsuya Kawamoto;Yasuaki Ito;Koji Nakano	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.12	computer architecture;parallel computing;computer hardware;computer science	Arch	8.841031623849606	44.5238463151665	47474
8c1661101107b084429b2b8f843e015359969e27	a mixed hardware-software approach to flexible artificial neural network training on fpga	system flexibility;flexible artificial neural network training;cost function;system on chip artificial intelligence coprocessors field programmable gate arrays learning artificial intelligence;fpga;data representation;coprocessors;system on a chip;chip;network topology;mixed hardware software approach;artificial neural networks;low precision data representation;system on chip;software algorithms;neural network hardware;artificial intelligence;application specific coprocessor architecture;parallel processing elements;neural coprocessor;floating point arithmetic;field programmable gate arrays;learning artificial intelligence;development time;artificial neural networks field programmable gate arrays neural network hardware system on a chip coprocessors parallel processing cost function throughput network topology software algorithms;high efficiency;system flexibility mixed hardware software approach flexible artificial neural network training fpga network topology low precision data representation system on chip neural coprocessor parallel processing elements on chip software control floating point arithmetic network parallelism application specific coprocessor architecture;parallel processing;training algorithm;network parallelism;software implementation;artificial neural network;throughput;on chip software control	FPGAs offer a promising platform for the implementation of Artificial Neural Networks (ANNs) and their training, combining the use of custom optimized hardware with low cost and fast development time. However, purely hardware realizations tend to focus on throughput, resorting to restrictions on applicable network topology or low-precision data representation, whereas flexible solutions allowing a wide variation of network parameters and training algorithms are usually restricted to software implementations. This paper proposes a mixed approach, introducing a system-on-chip (SoC) implementation where computations are carried out by a high efficiency neural coprocessor with a large number of parallel processing elements. System flexibility is provided by on-chip software control and the use of floating-point arithmetic, and network parallelism is exploited through replicated logic and application-specific coprocessor architecture, leading to fast training time. Performance results and design limitations and trade-offs are discussed.	algorithm;artificial neural network;backpropagation;computation;coprocessor;data (computing);field-programmable gate array;maximal set;microprocessor development board;network topology;parallel computing;system on a chip;throughput	Ramón José Aliaga;Rafael Gadea Gironés;Ricardo José Colom-Palero;Joaquín Cerdá;Néstor Ferrando;Vicente Herrero	2009	2009 International Symposium on Systems, Architectures, Modeling, and Simulation	10.1109/ICSAMOS.2009.5289235	system on a chip;embedded system;computer architecture;parallel computing;computer science;operating system;machine learning;artificial neural network;field-programmable gate array	Arch	5.18895011362923	44.1402206787451	47500
bac01702aeb5cb8d5884ac27e9f8c84041bb0bf2	optimized architecture for aes		This paper presents a highly optimized architecture for Advanced Encryption Standard (AES) by dividing and merging (combining) different sub operations in AES algorithm. The proposed architecture uses ten levels of pipelining to achieve higher throughput and uses Block-RAM utility to reduce slice utilization which subsequently increases the efficiency. It achieves the data stream of 57 Gbps at 451 MHz working frequency and obtains 36% improvement in efficiency to the best known similar design throughput per area (Throughput/Area) and 35% smaller in slice area. This architecture can easily be embedded with other modules because of significantly reduced slice utilization.	algorithm;data rate units;embedded system;encryption;pipeline (computing);random-access memory;throughput	S. AbhijithP.;Manish Goswami;S. Tadi;Kamal Pandey	2014	IACR Cryptology ePrint Archive		architecture;throughput;data stream;parallel computing;merge (version control);advanced encryption standard;pipeline (computing);computer science	Arch	9.423153143601775	45.19299354376825	47705
09a706889d394e423d95e704c045d4e26bb98462	holotrap: interactive hologram design for multiple dynamic optical trapping	optical tweezer;real time;digital holography;spatial light modulator;optical trapping;spatial light modulators;optical tweezers;42 40 jv;87 80 cc;87 80 y;42 79 kr;interactive manipulation	This work presents an application that generates real-time holograms to be displayed on a holographic optical tweezers setup; a technique that allows the manipulation of particles in the range from micrometres to nanometres. The software is written in Java, and uses random binary masks to generate the holograms. It allows customization of several parameters that are dependent on the experimental setup, such as the specific characteristics of the device displaying the hologram, or the presence of aberrations. We evaluate the software's performance and conclude that real-time interaction is achieved. We give our experimental results from manipulating 5 µm microspheres using the program. PROGRAM SUMMARY Title of program: HoloTrap Computer for which the program is designed and others on which it has been tested: General computer Operating systems or monitors under which the program has been tested: Windows, Linux Programming language used: Java Memory required to execute with typical data: up to 34 Mb including the Java Virtual Machine No. of bits in a word:8 bits No. of processors used: 1 Has the code been vectorized or parallelized?: No No. of bytes in distributed program, including test data, etc.: 1118 KB Distribution format: jar file Nature of physical problem: To calculate and display holograms for generating multiple and dynamic optical tweezers to be reconfigured interactively Method of solution: Fast random binary mask for the simultaneous codification of multiple phase functions into a phase modulation device Typical running time: Up to 10 frames per second Unusual features of the program: None References: The method for calculating holograms can be found in [M.	byte;central processing unit;general computer corporation;holography;interactivity;java virtual machine;linux;microsoft windows;modulation;operating system;parallel computing;programming language;real-time clock;test data;time complexity;variable shadowing	E. Pleguezuelos;Artur Carnicer;J. Andilla;Estela Martín-Badosa;M. Montes-Usategui	2007	Computer Physics Communications	10.1016/j.cpc.2007.03.003	optical tweezers;computer hardware;computer science;theoretical computer science;physics;computer graphics (images)	Visualization	5.660953482110333	37.80417504220336	47813
53589b748a07ac6a5bf7a4bb7ec1251507e6f3c3	optimal mapping of systolic algorithms by regular instruction shifts	nonlinear allocation technique;optimisation;distributed memory systems;concurrent computing;optimal mapping;systolic algorithms;processor scheduling;integral equations;systolic arrays;computer aided instruction;computational geometry;distributed computing;systolic array;data processing;matrix algebra;regular arrays;cholesky factorization;geometric modelling optimal mapping systolic algorithms regular instruction shifts affine recurrence equations regular arrays nonlinear allocation technique instruction shift planar regular array synthesis linear schedule cholesky factorization distributed memory multiprocessors;geometric modelling;regular instruction shifts;difference equations;distributed memory multiprocessors;signal processing;affine recurrence equations;planar regular array synthesis;scalability;linear schedule;instruction shift;computational geometry parallel algorithms systolic arrays equations optimisation distributed memory systems matrix algebra;difference equations computer aided instruction concurrent computing distributed computing integral equations processor scheduling scalability signal processing data processing systolic arrays;parallel algorithms	This paper addresses the problem of determining efficient mappings of systems of affine recurrence equations into regular arrays, in a nearly space-optimal fashion. A new nonlinear allocation technique is presented: the Instruction Shift. It allows to synthesize planar regular arrays without increasing the initial linear schedule. This technique is illustrated with the LL/sup t/ Cholesky factorization. >	algorithm	Philippe Clauss;Guy-René Perrin	1994		10.1109/ASAP.1994.331801	parallel computing;scalability;recurrence relation;concurrent computing;data processing;systolic array;computational geometry;computer science;theoretical computer science;signal processing;distributed computing;parallel algorithm;cholesky decomposition;integral equation	Arch	4.378806723715978	38.327845229334976	47905
c41515b0ebfe0f66edc62783a523e1a1595be632	an energy-efficient accelerator for hybrid bit-width dnns		This paper proposed an energy-efficient accelerator for hybrid bit-width DNNs. To speedup the DNNs and make it energy-efficient, we first propose a hybrid bit-width compression method that can save the memory storage of the state-of-the-art network LeNet and AlexNet by 7x and 8x with negligible accuracy loss. Also we propose an energy-efficient iterative logarithmic multiplier with approximate computing to process the multiply-accumulate operations for DNNs. The computational ability and adaptability of the FPGA greatly expands by using the LUT-based multiplier with improved iterative logarithmic rather than a DSP-based multiplier. The accelerator power is reduced from 24.1 W to 14.3 W. The accelerator proposed performs 1.53 times better in power efficiency and can achieve 33.98 GOP/s/W comparing with state-of-the-art architecture.	approximate computing;approximation algorithm;computation;digital signal processor;field-programmable gate array;iterative method;multiply–accumulate operation;performance per watt;speedup	Bo Liu;Xing Ruan;Mengwen Xia;Yu Gong;Jinjiang Yang;Wei Ge;Jun Yang	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8280940	field-programmable gate array;parallel computing;artificial neural network;speedup;architecture;adaptability;digital signal processing;electrical efficiency;computer science;lookup table	EDA	3.994473020125566	42.745376292131745	47972
6a1fd7ff7f3e548f9414237dd705058eefecf9c0	algorithm 311: prime number generator 2	prime number	i n t e g e r p r o c e d u r e sieve2(m, p); v a l u e m; i n t e g e r m; i n t e g e r a r r a y p; c o m m e n t sieve2 is a faster version of sievel. Two changes were made to obtain higher speed. (1) The multiples q[i] are sorted, smallest first, so tha t each value of n does not need to be compared with every q[i]. The sorted order of the q[i] is indicated by an index array r. The i th sorted element of q is q[r[i]]. I t was found empirically tha t greater speed is obtained when the q[r[i]] are not kept cons tant ly sorted, but are re-sorted only at the time a new prime is discovered. The i n teger j j indicates wtfieh of the q[r [i]] are sorted: q[r[3]] througtl q[r[jj-1]] are out of order, whereas q[r[jj]] through q[r[j]] are in order. Sorting is performed in two stages. A sif t sort first rearranges r[3] through r[jj-1] into rr[3] through rr[jj-1]. Then a single merge sort combines rr[3] through rr[jj-1] and r[jj] through r[j] into ri l l through r[j]. (2) All multiples of 3 are at t tomatically excluded from consideration by stepping n a l ternate ly by 2 and 4, and, in a similar way, by stepping q[i] al ternately by 2 X p[i] and 4 × p[i].; b e g i n i n t e g e r a r r a y q, dq, sq, r, rr[2: 2.TXsqrt(m)/ln(m) ]; i n t e g e r i , j , j j , k, n, ir, j r , dn; B o o l e a n t; p[1] : = d n := 2; p[2] : = j : = j j : = k := r[3] := 3; p[3] := 5; q[3] := 25; dq[3] := 10; sq[3] := 30; for n := 7 s t ep dn u n t i l m do beg in t := t r u e ; dn := 6 dn; for i := 3 s t ep 1 u n t i l j j do beg in ir := r[i]; i f n = q[ir] t h e n b e g i n q[ir] := n + dq[ir]; dq[ir] := sq[ir] dq[ir]; t := fa l se ; i f i = j j t h e n beg in j j := j j + l; i f i r = j t h e n b e g i n j := j + l; r[j] := j; q[J] := P[J] T 2; sq[j] := 6 X p[j]; dq[j] := sq[j] × ( l+(p[ j ] + 3)) 2X q[j] end e n d e n d e n d ; i f t t h e n b e g i n k : = k + l ; p[k] := n; A: i f j j = 3 t h e n go to F ; j j : = j j 1 ; i f q[r[jj]] < q[r[2+l]] t h e n go to A ; c o m m e n t sift sort; rr[3] := r[3]; for ir := 4 s t e p 1 u n t i l j j d o b e g i n i : = i r 1 ; B: i f q[r[ir]] < q[rr[i]] t h e n b e g i n rr[i+l] := rr[i]; i := i -1; i £ i > 3 t h e n g o t o B e n d ; r s [ i+ l ] := r[ir] e , d ;	algorithm;goto;merge sort;sorting;stepping level	Bruce A. Chartres	1967	Commun. ACM	10.1145/363566.363692	provable prime;prime power;computer science;safe prime;prime factor;prime number	Theory	8.057477656995296	42.01284195068338	48258
6df31e481004a64425accc362e4f344e3839543a	using the reconfigurable massively parallel architecture copacobana 5000 for applications in bioinformatics	energy efficient;massively parallel computer;cost effectiveness;parallel architecture;high throughput;hardware implementation;reconfigurable hardware;parallel processing	Currently several computational problems require high processing power to handle huge amounts of data, although underlying core algorithms appear to be rather simple. Especially in the area of bioinformatics, algorithms implemented in PCs do not utilize all hardware functionalities provided by standard CPUs. As the demand for efficient utilization and speed up increases, this leads to a boost in the trend of implementing dedicated hardware. Hardware implementations can be done very fast and are cost effective on reconfigurable devices such as FPGAs. With 128 lowcost FPGAs residing on the COPACOBANA 5000 and in combination with a high-throughput systolic bus system, this machine therefore provides a dynamic solution for massively parallel computations with reconfigurable capabilities. This paper describes the advantages of this architecture based on the implementation of efficient solutions designed for two well-known algorithmic problems in bioinformatics: Smith-Waterman Alignment and DNA Motif Finding.	assembly language;bioinformatics;block-matching algorithm;central processing unit;computation;computational problem;cryptanalysis;custom hardware attack;field-programmable gate array;high-throughput computing;microsoft outlook for mac;motif;parallel computing;quality of results;smith–waterman algorithm;speedup;throughput;xfig	Lars Wienbrandt;Stefan Baumgart;Jost Bissel;Carol May Yen Yeo;Manfred Schimmler	2010		10.1016/j.procs.2010.04.114	high-throughput screening;parallel processing;computer architecture;parallel computing;cost-effectiveness analysis;reconfigurable computing;computer science;massively parallel;distributed computing;efficient energy use	Arch	0.3420882684820796	43.23056625512928	48358
46e6aa9c60a3d357e7df78bca8a0962b6d0d8447	light speed labeling for risc architectures	connected component labeling;image segmentation;binary image;reduced instruction set computing;wu algorithm risc architectures light speed labeling connected component labeling binary images line relative labeling risc computers;labeling reduced instruction set computing algorithm design and analysis pipelines image segmentation computer architecture filtering algorithms testing geophysics computing image processing;real time implementation connected component labeling run length coding;indexing terms;optical character recognition software;computer architecture;reduced instruction set computing image segmentation;run length coding;fast algorithm;pipelines;real time implementation;connected component;algorithm design and analysis;benchmark testing;labeling	This article introduces a fast algorithm for Connected Component Labeling of binary images called Light Speed Labeling. It is segment-based and a line-relative labeling that was especially thought for RISC computers. An extensive benchmark on both structured and unstructured images substanciates that the algorithm, the way it is designed, is faster and more runtime predictable than Wu's algorithm claimed to be the world fastest in 2007.	algorithm;benchmark (computing);binary image;computer;connected-component labeling;fastest;speed of light (cellular automaton)	Lionel Lacassagne;Bertrand Zavidovique	2009	2009 16th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2009.5414352	algorithm design;benchmark;computer vision;reduced instruction set computing;labeling theory;parallel computing;real-time computing;connected component;index term;binary image;computer science;theoretical computer science;pipeline transport;image segmentation;connected-component labeling	Robotics	9.19918410474508	39.177648422640345	48425
1f836d4182e617c5fd5372c998f1702f07d6220c	heterogeneous computation of rainbow option prices using fourier cosine series expansion under a mixed cpu-gpu computation framework	exotic options;gpgpu;distributed computation;finanzas y contabilidad;economia y empresa	This paper focuses on comparing different heterogeneous computational designs for the calculation of rainbow options prices using the Fourier-cosine series expansion COS method. We also propose a simple enough way to automatically decide the ratio of load balancing at runtime. A general-purpose computing on graphic processing unit implementation of the two-dimensional composite Simpson rule free of conditional statements with some degree of loop unrolling is also introduced. We will also show how to reduce the integration domain of coefficients appearing in the option pricing and by doing so achieve a substantial speed-up and improve accuracy when compared with a straightforward implementation. Copyright © 2014 John Wiley & Sons, Ltd.	central processing unit;computation;graphics processing unit	Aurelien Cassagnes;Yu Chen;Hirotada Ohashi	2014	Int. Syst. in Accounting, Finance and Management	10.1002/isaf.1349	mathematical optimization;exotic option;computer science;artificial intelligence;theoretical computer science;finance;general-purpose computing on graphics processing units;algorithm	DB	-3.7105273166604333	40.84177958965224	48435
0e1797fcbf8294f1bcfe6ec3918ca585756fb4f1	application composition and communication optimization in iterative solvers using fpgas	spmv;eigenvalues and eigenfunctions;microprocessor chips eigenvalues and eigenfunctions field programmable gate arrays floating point arithmetic graphics processing units iterative methods mathematics computing;gpus communication avoiding iterative solvers spmv matrix powers fpgas;gpus;drntu engineering computer science and engineering computing methodologies;mathematics computing;matrix powers;application composition peak single precision floating point performance off chip memory bandwidth gpu algorithm architecture interaction lanczos method computation cost communication cost dependency storage resource constrained framework time multiplexed fpga architecture on chip communication bandwidth on chip memory capacity on chip data sharing linear systems of equations large scale eigenvalue problems multiple linear algebra kernels communication minimization problem iterative solvers communication optimization;conference paper;iterative methods;graphics processing units;kernel field programmable gate arrays system on chip graphics processing units vectors bandwidth computer architecture;fpgas;floating point arithmetic;field programmable gate arrays;microprocessor chips;communication avoiding iterative solvers	We consider the problem of minimizing communication with off-chip memory and composition of multiple linear algebra kernels in iterative solvers for solving large-scale eigenvalue problems and linear systems of equations. While GPUs may offer higher throughput for individual kernels, overall application performance is limited by the inability to support on-chip sharing of data across kernels. In this paper, we show that higher on-chip memory capacity and superior on-chip communication bandwidth enables FPGAs to better support the composition of a sequence of kernels within these iterative solvers. We present a time-multiplexed FPGA architecture which exploits the on-chip capacity to store dependencies between kernels and high communication bandwidth to move data. We propose a resource-constrained framework to select the optimal value of an algorithmic parameter which provides the tradeoff between communication and computation cost for a particular FPGA. Using the Lanczos Method as a case study, we show how to minimize communication on FPGAs by this tight algorithm-architecture interaction and get superior performance over GPU despite of its ~5x larger off-chip memory bandwidth and ~2x greater peak singleprecision floating-point performance.	communication-avoiding algorithms;computation;computer memory;field-programmable gate array;graphics processing unit;iterative method;lanczos algorithm;linear algebra;linear system;memory bandwidth;multiplexing;optimization problem;speedup;throughput	Abid Rafique;Nachiket Kapre;George A. Constantinides	2013	2013 IEEE 21st Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2013.16	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;field-programmable gate array	Arch	-1.7416842252504252	40.26761238220148	48468
150fc0c63f3e97260fe99c705ffb94fc71a77442	a systolic array for cyclic-by-rows jacobi algorithms	time dependent;general and miscellaneous mathematics computing and information science;time complexity;performance;systolic array;mathematical logic;iterative methods;computer architecture;time dependence;array processors;algorithms;jacobian function;logic programs;programming 990210 supercomputers 1987 1989;iteration method;functions;parallel processing	The major concern of this paper is the systolic realization of Jacobi algorithms with a cyclic-by-rows iteration scheme. This aim can be achieved by construction of an algorithm which is well suited for parallel processing and is essentially equivalent to the above-mentioned type of algorithm. Moreover, a large variety of applications for Jacobi algorithms is presented as well as a comparison to other parallel schemes for the same problem. Finally, a systolic array is derived which requires (n + I)‘/4 processing cells and has a time complexity of O(n) for each sweep. Q 1987 Academic Press, Inc.	algorithm;iteration;iterative method;jacobi method;parallel computing;systolic array;time complexity	Uwe Schwiegelshohn;Lothar Thiele	1987	J. Parallel Distrib. Comput.	10.1016/0743-7315(87)90041-4	parallel processing;mathematical optimization;combinatorics;parallel computing;computer science;theoretical computer science;distributed computing;iterative method;algorithm	HPC	-1.9616620379760892	36.85382732778791	48587
f0c22ea139367f287858e87a100f3ecc4175804e	a fast parallel matrix multiplication reconfigurable unit utilized in face recognitions systems	digital signal processing;clocks;face recognition signal processing algorithms acceleration databases algorithm design and analysis field programmable gate arrays digital signal processing statistical analysis principal component analysis bayesian methods;matrix multiplication digital signal processing chips face recognition field programmable gate arrays fixed point arithmetic;digital signal processing unit;fixed point;general purpose processor;face recognition;face recognition system;parallel blocked matrix multiplication reconfigurable unit;pipelines;computational intensive function;classification algorithms;state of the art fpga device;face identification scheme;cpu intensive part;fixed point arithmetic;digital signal processing chips;face;matrix multiplication;fixed point matrix multiplication;field programmable gate arrays;fixed point arithmetic parallel blocked matrix multiplication reconfigurable unit face recognition system computational intensive function dsp digital signal processing unit state of the art fpga device fixed point 32 bit number fixed point matrix multiplication cpu intensive part face identification scheme;algorithm design and analysis;fixed point 32 bit number;dsp	In this paper we present a reconfigurable device which significantly improves the execution time of the most computational intensive functions of three of the most widely used face recognition algorithms; those tasks multiply very large dense matrices. The presented architecture utilizes numerous digital signal processing units (DSPs) organized in a parallel manner within a state-of-the-art FPGA device. In order to accelerate those functions we have implemented a ldquoblockedrdquo matrix multiplication algorithm which multiplies certain sub-matrices of fixed-point 32-bit numbers; the size of the sub-matrices has been selected so as to fully exploit the resources of the underlying reconfigurable device. Our system is up to 550 times faster than a conventional general purpose processor when implementing the most CPU intensive parts of a number of very widely used face identification schemes, whereas it is more than 40 times faster than the similar schemes implemented in reconfigurable devices. Moreover, our system is general enough so as to be efficiently utilized in any application incorporating fixed-point matrix multiplications.	32-bit;central processing unit;digital signal processing;facial recognition system;field-programmable gate array;fixed point (mathematics);matrix multiplication algorithm;reconfigurable computing;run time (program lifecycle phase);sparse matrix	Joannis Sotiropoulos;Ioannis Papaefstathiou	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272287	facial recognition system;statistical classification;embedded system;parallel computing;computer hardware;computer science;theoretical computer science;digital signal processing	EDA	5.007054492093128	43.41542969059368	48808
c5622bf1f5385db0651d29212abe107336a25cf8	rmac: runtime configurable floating point multiplier for approximate computing		Approximate computing is a way to build fast and energy efficient systems, which provides responses of good enough quality tailored for different purposes. In this paper, we propose a novel approximate floating point multiplier which efficiently multiplies two floating numbers and yields a high precision product. RMAC approximates the costly mantissa multiplication to a simple addition between the mantissa of input operands. To tune the level of accuracy, RMAC looks at the first bit of the input mantissas as well as the first N bits of the result of addition to dynamically estimate the maximum multiplication error rate. Then, RMAC decides to either accept the approximate result or re-execute the exact multiplication. Depending on the value of N, the proposed RMAC can be configured to achieve different levels of accuracy. We integrate the proposed RMAC in AMD southern Island GPU, by replacing RMAC with the existing floating point units. We test the efficiency and accuracy of the enhanced GPU on a wide range of applications including multimedia and machine learning applications. Our evaluations show that a GPU enhanced by the proposed RMAC can achieve 5.2x energydelay product improvement as opposed to GPU using conventional FPUs while ensuring less than 2% quality loss. Comparing our approach with other state-of-the-art approximate multipliers shows that RMAC can achieve 3.1x faster and 1.8x more energy efficient computations while providing the same quality of service.	approximate computing;approximation algorithm;computation;floating-point unit;graphics processing unit;machine learning;operand;principle of good enough;quality of service;significand	Mohsen Imani;Ricardo Omar Chávez García;Saransh Gupta;Tajana Simunic	2018		10.1145/3218603.3218621	operand;significand;real-time computing;computation;floating point;quality of service;word error rate;computer science;multiplication;multiplier (economics)	Arch	5.999667545939485	44.127510367733	49133
c91defad4a1e3df0e6d028266c1180a0104fdedf	a parallel algorithm for solving hard tsume-shogi problems			parallel algorithm	Yasuichi Nakayama;Tadafumi Akazawa;Kohei Noshita	1996	ICGA Journal	10.3233/ICG-1996-19204	artificial intelligence;computer science;theoretical computer science;parallel algorithm;algorithm design	Crypto	0.3826252491674602	36.94396435131618	49297
f5647111d6a9b9ca7089220af3cf4e2c89f00b9e	an approach of finding maximal submeshes for task allocation algorithms in mesh structures	efficiency;simulation;mesh structure;maximal submeshes;task allocation algorithm	This paper concerns the problem of finding efficient task allocation algorithms in mesh structures. An allocation algorithm, called Window-Based-Best-Fit with Validated-Submeshes WBBFVS, has been created. The core of this algorithm lays in the way of finding maximal submeshes what is an important part of the task allocation algorithms based on the stack approach. The new way of finding maximal submeshes avoids post-checking submeshes' redundancy thanks to the proposed initial validation. The elimination of redundancy may increase the speed of the algorithms' performance. The three considered algorithms have been evaluated on the basis of the results of simulation experiments made using the designed and implemented experimentation system. The obtained results of investigation show that the WBBFVS algorithm seems to be very promising.	algorithm;maximal set;memory management	Radoslaw J. Jarecki;Iwona Pozniak-Koszalka;Leszek Koszalka;Andrzej Kasprzak	2014		10.1007/978-3-319-05476-6_34	mathematical optimization;computer science;theoretical computer science;distributed computing;efficiency	Theory	3.2352037115723773	36.799163342095824	49498
2bb42f9310356c812f5e2cbad03c72ab1f2a4a3d	protection of superconducting industrial machinery using rnn-based anomaly detection for implementation in smart sensor †	lhc;anomaly detection;neural networks compression;recurrent neural networks	Sensing the voltage developed over a superconducting object is very important in order to make superconducting installation safe. An increase in the resistive part of this voltage (quench) can lead to significant deterioration or even to the destruction of the superconducting device. Therefore, detection of anomalies in time series of this voltage is mandatory for reliable operation of superconducting machines. The largest superconducting installation in the world is the main subsystem of the Large Hadron Collider (LHC) accelerator. Therefore a protection system was built around superconducting magnets. Currently, the solutions used in protection equipment at the LHC are based on a set of hand-crafted custom rules. They were proved to work effectively in a range of applications such as quench detection. However, these approaches lack scalability and require laborious manual adjustment of working parameters. The presented work explores the possibility of using the embedded Recurrent Neural Network as a part of a protection device. Such an approach can scale with the number of devices and signals in the system, and potentially can be automatically configured to given superconducting magnet working conditions and available data. In the course of the experiments, it was shown that the model using Gated Recurrent Units (GRU) comprising of two layers with 64 and 32 cells achieves 0.93 accuracy for anomaly/non-anomaly classification, when employing custom data compression scheme. Furthermore, the compression of proposed module was tested, and showed that the memory footprint can be reduced four times with almost no performance loss, making it suitable for hardware implementation.		Maciej Wielgosz;Andrzej Skoczen;Ernesto De Matteis	2018		10.3390/s18113933		Mobile	3.5934815551590944	40.65878561768595	49594
1f27e210540367a66061956e284b1b2c476560b8	a perspective on parallel processing	parallel processing	Computing requirements in several application areas are unlikely to be satisfied solely by uniprocessors in the future. In addition to providing very high absolute computing performance, parallel processing offers the advantages of improved cost performance and upwardly scalable performance. To realize these benefits, researchers at IBM have been experimenting with parallel processing for several years. The initial focus was on highly parallel systems for specific design automation problems. Encouraged by successes in these projects and a growing conviction that parallel processing is very important, our research has expanded into more general application areas and systems approaches. We find it useful to discuss our work in two categories: specialized parallel processors and multipurpose parallel processors.	central processing unit;electronic design automation;experiment;parallel computing;requirement;scalability;uniprocessor system	Tilak Agerwala;Stephen L. Harvey	1987		10.1007/3-540-18991-2_2	parallel processing;parallel computing;computer science	HPC	-2.025925480971853	46.254374978631624	49888
05370a6cc820ffe5393fcc948d7d600b5949a217	goffish: a sub-graph centric framework for large-scale graph analytics		Large scale graph processing is a major research area for Big Data exploration. Vertex centric programming models like Pregel are gaining traction due to their simple abstraction that allows for scalable execution on distributed systems naturally. However, there are limitations to this approach which cause vertex centric algorithms to under-perform due to poor compute to communication overhead ratio and slow convergence of iterative superstep. In this paper we introduce GoFFish a scalable sub-graph centric framework co-designed with a distributed persistent graph storage for large scale graph analytics on commodity clusters. We introduce a sub-graph centric programming abstraction that combines the scalability of a vertex centric approach with the flexibility of shared memory sub-graph computation. We map Connected Components, SSSP and PageRank algorithms to this model to illustrate its flexibility. Further, we empirically analyze GoFFish using several real world graphs and demonstrate its significant performance improvement, orders of magnitude in some cases, compared to Apache Giraph, the leading open source vertex centric implementation.	algorithm;apache giraph;big data;bulk synchronous parallel;circuit complexity;computation;computational complexity theory;connected component (graph theory);data parallelism;distributed computing;graph (abstract data type);graph theory;iterative method;open-source software;overhead (computing);pagerank;parallel computing;programming model;prototype;scalability;sequence diagram;shared memory;shortest path problem;software prototyping;traction teampage	Yogesh L. Simmhan;Alok Gautam Kumbhare;Charith Wickramaarachchi;Soonil Nagarkar;Santosh Ravi;Cauligi S. Raghavendra;Viktor K. Prasanna	2014		10.1007/978-3-319-09873-9_38	parallel computing;computer science;theoretical computer science;operating system;database;distributed computing	HPC	-3.3148703104419575	42.84309631125949	50232
0434e3422e2fdcaf20a8b8b234592903a2a17bec				code;matrix multiplication	Yuan Cao;Yonglin Cao;Fang-Wei Fu	2018	CoRR			NLP	5.625630705545854	36.735383093154084	50258
7f2ecaf4d73a4e8987b2723dae1ec958126a117e	improving the throughput of the aes algorithm with multicore processors	parallel programming cryptography linux multiprocessing systems;high throughput aes;parallel programming;throughput program processors multicore processing encryption standards;cryptography;posix threads advanced encryption standard aes algorithm multicore processors high throughput aes parallel processing fork system call;fork system call;parallel programming aes algorithm multicore processors advanced encryption standard symmetric key encryption standard software implementation multicore architectures linux posix standard sequential program;linux;multicore processors;multiprocessing systems;advanced encryption standard;posix threads;parallel processing;aes algorithm	AES, Advanced Encryption Standard, can be considered the most widely used modern symmetric key encryption standard. To encrypt/decrypt a file using the AES algorithm, the file must undergo a set of complex computational steps. Therefore a software implementation of AES algorithm would be slow and consume large amount of time to complete. The immense increase of both stored and transferred data in the recent years had made this problem even more daunting when the need to encrypt/decrypt such data arises. As a solution to this problem, in this paper, we present an extensive study of enhancing the throughput of AES encryption algorithm by utilizing the state of the art multicore architectures. We take a sequential program that implements the AES algorithm and convert the same to run on multicore architectures with minimum effort. We implement two different parallel programmes, one with the fork system call in Linux and the other with the pthreads, the POSIX standard for threads. Later, we ran both the versions of the parallel programs on different multicore architectures and compared and analysed the throughputs between the implementations and among different architectures. The pthreads implementation outperformed in all the experiments we conducted and the best throughput obtained is around 7Gbps on a 32-core processor (the largest number of cores we had) with the pthreads implementation.	automatic parallelization;central processing unit;encryption;experiment;fork (software development);fork (system call);graphics processing unit;linux;multi-core processor;posix threads;parallel computing;symmetric-key algorithm;system call;thread (computing);throughput	Arthur Barnes;R. Fernando;K. Mettananda;Roshan G. Ragel	2012	2012 IEEE 7th International Conference on Industrial and Information Systems (ICIIS)	10.1109/ICIInfS.2012.6304791	advanced encryption standard;parallel processing;computer architecture;parallel computing;encryption software;computer science;cryptography;posix threads;operating system;distributed computing;aes implementations;linux kernel	HPC	-2.2596858464363865	43.94878259605689	50363
dfb705abaf499d77f63ddbf3351305fdecbfefa4	efficient algorithms and architectures for field multiplication using gaussian normal bases	look up table;finite field multiplication;multiplying circuits;gaussian processes;efficient algorithm;digital signatures;inner product;low complexity;ecdsa;indexing terms;index terms finite field multiplication;digital arithmetic gaussian processes multiplying circuits;finite field;normal basis;multiplying circuits digital signatures digital arithmetic gaussian processes;xor gates gaussian normal bases extended binary field ieee standards nist standards elliptic curve digital signature algorithm bit wise inner product operation vector level software algorithm normal basis multiplication algorithm look up table based counterpart digit level multiplier architecture;gaussian normal basis;software algorithms;digital arithmetic;ecdsa index terms finite field multiplication normal basis gaussian normal basis software algorithms;elliptic curve digital signature algorithm;software implementation	Recently, implementations of normal basis multiplication over the extended binary field GF(2/sup m/) have received considerable attention. A class of low complexity normal bases called Gaussian normal bases has been included in a number of standards, such as IEEE and NIST for an elliptic curve digital signature algorithm. The multiplication algorithms presented there are slow in software since they rely on bit-wise inner product operations. In this paper, we present two vector-level software algorithms which essentially eliminate such bit-wise operations for Gaussian normal bases. Our analysis and timing results show that the software implementation of the proposed algorithm is faster than previously reported normal basis multiplication algorithms. The proposed algorithm is also more memory efficient compared with its look-up table-based counterpart. Moreover, two new digit-level multiplier architectures are proposed and it is shown that they outperform the existing normal basis multiplier structures. As compared with similar digit-level normal basis multipliers, the proposed multiplier with serial output requires the fewest number of XOR gates and the one with parallel output is the fastest multiplier.	digital signature;exclusive or;fastest;gaussian blur;grammatical framework;lookup table;multiplication algorithm;normal basis;polynomial;serial communication;time complexity	Arash Reyhani-Masoleh	2006	IEEE Transactions on Computers	10.1109/TC.2006.10	discrete mathematics;parallel computing;multiplication algorithm;elliptic curve digital signature algorithm;computer science;theoretical computer science;mathematics;algorithm;algebra	Visualization	9.969342987506954	43.80434765133511	50426
94f4cfdb7a5b2f00eca2a488203c749f4c06c754	a parallel digital vlsi architecture for integrated support vector machine training and classification	kernel;support vector machines;very large scale integration;training;computer architecture;support vector machines training computer architecture very large scale integration hardware kernel optimization;vlsi cmos digital integrated circuits electronic engineering computing multiprocessing systems parallel processing support vector machines;optimization;size 90 nm parallel digital vlsi architecture combined support vector machine training combined svm classification hardware based svm training training workload multiple svm processing units hardware friendly implementation cascade algorithm parallel cascade architecture multilayer system bus multiple distributed memories hardware parallel processing parallel cascade svm processors commercial 90 nm cmos technology;system buses digital integrated circuits multicore processing parallel architectures support vector machines;hardware	This paper presents a parallel digital VLSI architecture for combined support vector machine (SVM) training and classification. For the first time, cascade SVM, a powerful training algorithm, is leveraged to significantly improve the scalability of hardware-based SVM training and develop an efficient parallel VLSI architecture. The presented architecture achieves excellent scalability by spreading the training workload of a given data set over multiple SVM processing units with minimal communication overhead. Hardware-friendly implementation of the cascade algorithm is employed to achieve low hardware overhead and allow for training over data sets of variable size. In the proposed parallel cascade architecture, a multilayer system bus and multiple distributed memories are used to fully exploit parallelism. In addition, the proposed architecture is rather flexible and can be tailored to realize hybrid use of hardware parallel processing and temporal reuse of processing resources, leading to good tradeoffs between throughput, silicon overhead and power dissipation. Several parallel cascade SVM processors have been designed with a commercial 90-nm CMOS technology, which provide up to a 561× training time speedup and a significant estimated 21 859× energy reduction compared with the software SVM algorithm running on a 45-nm commercial general-purpose CPU.	cmos;cascade algorithm;central processing unit;digital electronics;general-purpose modeling;image scaling;input/output;overhead (computing);parallel computing;processor register;scalability;speedup;static random-access memory;support vector machine;system bus;throughput;very-large-scale integration	Qian Wang;Peng Li;Yongtae Kim	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2343231	embedded system;support vector machine;computer architecture;electronic engineering;parallel computing;kernel;real-time computing;computer science;operating system;very-large-scale integration	Arch	4.211682615127892	43.88295713390048	50480
776ad28abe62cccd8aaafeddf6b9e5dc8a9b1da2	towards scaling parallel seismic raytracing		Marine geologists use seismic tomography techniques to determine the 3D geophysical structure of the ocean floor. At the heart of seismic tomography methods is a forward solver used to compute minimum travel times from all locations in a earth model to sensors used in seismic experiments. The Stingray seismic raytracer was originally based on an adaption of Dijkstra's single-source shortest-path algorithm. Unfortunately, the algorithm's inherent sequential nature limits its scalability. Our new parallel Stingray implementations are based on an iterative data parallel algorithm and demonstrates scalable performance in both time and problem size. Results are presented for OpenMP and MPI experiments on seismic models of significantly larger size than Stingray has processed before.	analysis of algorithms;bus functional model;data parallelism;distributed memory;experiment;graphics processing unit;image scaling;iterative method;mathematical optimization;message passing interface;openmp;parallel algorithm;performance tuning;ray tracing (graphics);scalability;sensor;shortest path problem;solver;stingray phone tracker;supercomputer;tomography	Allen D. Malony;Mohammad Alaul Haque Monil;Craig Rasmusen;Kevin A. Huck;Joseph S Byrnes;Douglas Toomey	2016	2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)	10.1109/CSE-EUC-DCABES.2016.189	theoretical computer science;scalability;parallel algorithm;seismic tomography;shortest path problem;computer science;ray tracing (graphics);solver;solid modeling;dijkstra's algorithm	HPC	-2.9341666478961064	41.12306729991193	50596
ecad20e2f891246466fc71914dbe9e03751438b6	blocked-based sparse matrix-vector multiplication on distributed memory parallel computers	distributed memory;parallel computer;sparse matrix	The present paper discusses the implementations of sparse matrix-vector products, which are crucial for high performance solutions of large-scale linear equations, on a PC-Cluster. Three storage formats for sparse matrices compressed row storage, block compressed row storage and sparse block compressed row storage are evaluated. Although using BCRS format reduces the execution time but the improvement may be limited because of the extra work from filled-in zeros. We show that the use of SBCRS not only improves the performance significantly but reduces matrix storage also.	block size (cryptography);blocking (computing);computer cluster;distributed memory;iterative method;linear equation;locality of reference;matrix multiplication;parallel computing;run time (program lifecycle phase);sparse matrix;the matrix	Rukhsana Shahnaz;Anila Usman	2011	Int. Arab J. Inf. Technol.		parallel computing;distributed memory;sparse matrix;computer science;theoretical computer science;sparse approximation;distributed computing;skyline matrix;sparse array	HPC	-2.148233967950811	38.848528083395735	50661
ff4bb7fce973c58619d033978dbc18936ae24103	a general neural network hardware architecture on fpga		Field Programmable Gate Arrays (FPGAs) plays an increasingly important role in data sampling and processing industries due to its highly parallel architecture, low power consumption, and flexibility in custom algorithms. Especially, in the artificial intelligence field, for training and implement the neural networks and machine learning algorithms, high energy efficiency hardware implement and massively parallel computing capacity are heavily demanded. Therefore, many global companies have applied FPGAs into AI and Machine learning fields such as autonomous driving and Automatic Spoken Language Recognition (Baidu) [1] [2] and Bing search (Microsoft) [3]. Considering the FPGAs great potential in these fields, we tend to implement a general neural network hardware architecture on XILINX ZU9CG System On Chip (SOC) platform [4], which contains abundant hardware resource and powerful processing capacity. The general neural network architecture on the FPGA SOC platform can perform forward and backward algorithms in deep neural networks (DNN) with high performance and easily be adjusted according to the type and scale of the neural networks.	algorithm;artificial intelligence;artificial neural network;autonomous car;deep learning;field-programmable gate array;machine learning;network architecture;networking hardware;parallel computing;sampling (signal processing)	Yufeng Hao	2017	CoRR		system on a chip;artificial neural network;massively parallel;artificial intelligence;field-programmable gate array;machine learning;fpga prototype;hardware architecture;computer science;reconfigurable computing;efficient energy use;computer architecture	AI	3.842738777845066	42.59266219360261	50726
704ba4f8974fa3f470841a91a2a68db03062917c	building single fault survivable parallel algorithms for matrix operations using redundant parallel computation	parallel algorithms fault tolerant computing gaussian processes matrix algebra;parallel algorithm;fault tolerant;gaussian processes;parallel dense matrix matrix multiplication;large scale system;matrix algebra;parallel algorithms concurrent computing fault tolerance high performance computing distributed computing scalability hardware information technology laboratories distributed processing;fault tolerant computing;matrix operations;redundant parallel computation;high performance computer;parallel computer;gaussian elimination;fault tolerance approach;gaussian elimination fault survivable parallel algorithms matrix operations redundant parallel computation fault tolerance approach parallel dense matrix matrix multiplication;matrix multiplication;fault survivable parallel algorithms;parallel algorithms	As the size of today's high performance computers continue to grow, node failures in these computers are becoming frequent events. Although checkpoint is the typical technique to tolerate such failures, it often introduces a considerable overhead and has shown poor scalability on today's large scale systems. In this paper we defined a new term called fault tolerant parallel algorithm which means that the algorithm gets the correct answer despite the failure of nodes. The fault tolerance approach in which the data of failed processes is recovered by modifying applications to recompute on all surviving processes is checkpoint-free. In particular, if no failure occurs, the fault tolerant parallel algorithms are the same as the original algorithms. We show the practicality of this technique by applying it to parallel dense matrix-matrix multiplication and Gaussian elimination to tolerate single process failure. Experimental results demonstrate that a process failure can be tolerated with a good scalability for the two fault tolerant parallel algorithms and the proposed fault tolerant parallel dense matrix-matrix multiplication is able to survive process failure with a very low performance overhead. The main drawback of this approach is non-transparent and algorithm-dependent.	central processing unit;computation;computer;effective method;fault tolerance;gaussian elimination;matrix multiplication;overhead (computing);parallel algorithm;scalability;sparse matrix;supercomputer;transaction processing system	Yunfei Du;Panfeng Wang;Hongyi Fu;J. J. Jia;Haifang Zhou;Xuejun Yang	2007	7th IEEE International Conference on Computer and Information Technology (CIT 2007)	10.1109/CIT.2007.27	parallel computing;matrix multiplication;computer science;theoretical computer science;distributed computing;parallel algorithm	HPC	0.1559042939665232	38.11483529589056	50950
87fa4282bc9a7e901dd998d83249d9978ab37538	parallel on-chip power distribution network analysis on multi-core-multi-gpu platforms	parallel computing;power grid simulation;approximation method;smoothing method;distributed networks;multigrid method;preconditioner circuit simulation graphics processing units gpus interconnect modeling multigrid method parallel computing power grid simulation;computer graphic equipment;coprocessors;circuit complexity;chip;circuit simulation;large scale;conjugate gradient;smoothing methods;preconditioner;multiprocessing systems computer graphic equipment conjugate gradient methods coprocessors;parallel computer;multigrid methods;graphic processing unit;power grid;approximation methods;power grids;multiprocessing systems;interconnect modeling;graphics processing units gpus;graphics processing unit power grids multigrid methods smoothing methods sparse matrices algorithm design and analysis approximation methods;graphics processing unit;conjugate gradient methods;algorithm design;sparse matrices;algorithm design and analysis;performance tuning;power distribution network;onchip power distribution network multicore multigpu platform graphics processing unit parallel single instruction multiple thread simt based gpu gpu speciflc algorithm design circuit topology transformation workload partitioning performance tuning gpu accelerated hybrid multigrid algorithm conjugate gradient solver dc power grid analysis four core four gpu system	The challenging task of analyzing on-chip power (ground) distribution networks with multimillion node complexity and beyond is key to today's large chip designs. For the first time, we show how to exploit recent massively parallel single-instruction multiple-thread (SIMT)-based graphics processing unit (GPU) platforms to tackle large-scale power grid analysis with promising performance. Several key enablers including GPU-speciflc algorithm design, circuit topology transformation, workload partitioning, performance tuning are embodied in our GPU-accelerated hybrid multigrid (HMD) algorithm (GpuHMD) and its implementation. We also demonstrate that using the HMD solver as a preconditioner, the conjugate gradient solver can converge much faster to the true solution with good robustness. Extensive experiments on industrial and synthetic benchmarks have shown that for DC power grid analysis using one GPU, the proposed simulation engine achieves up to 100× runtime speedup over a state-of-the-art direct solver and more than 50× speedup over the CPU based multigrid implementation, while utilizing a four-core-four-GPU system, a grid with eight million nodes can be solved within about 1 s. It is observed that the proposed approach scales favorably with the circuit complexity, at a rate about 1 s per two million nodes on a single GPU card.	algorithm design;central processing unit;circuit complexity;circuit topology;computer graphics;conjugate gradient method;converge;experiment;graphics processing unit;head-mounted display;multi-core processor;multigrid method;performance tuning;preconditioner;simulation;solver;speedup;synthetic intelligence	Zhuo Feng;Zhiyu Zeng;Peng Li	2011	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2010.2059718	computational science;algorithm design;parallel computing;computer science;theoretical computer science;algorithm	HPC	-1.143238139907894	40.067614604493386	51045
31fec54bd00fd613f8df3c383e4f282f3c44274a	high-performance integer factoring with reconfigurable devices	public key cryptography;copacobana factorization elliptic curve method reconfigurable hardware;low level arithmetic computation;digital signal processing;field programmable gate array;copacobana;composite integer factorization;elliptic curves;clocks;integer factorization;reconfigurable architectures;fpga;computer architecture;factorization;field programmable gate array reconfigurable device fpga elliptic curve method composite integer factorization prime factor xilinx virtex 4 sx35 rsa cryptosystem number field sieve dsp block low level arithmetic computation copacobana;reconfigurable device;dsp block;reconfigurable architectures digital arithmetic digital signal processing chips field programmable gate arrays public key cryptography;rsa cryptosystem;xilinx virtex 4 sx35;digital signal processing chips;digital arithmetic;prime factor;field programmable gate arrays;performance ratio;high performance;number field sieve;elliptic curve method;reconfigurable hardware;electronic countermeasures;electronic countermeasures digital signal processing field programmable gate arrays elliptic curves hardware clocks computer architecture;hardware	We present a novel FPGA-based implementation of the Elliptic Curve Method (ECM) for the factorization of medium-sized composite integers. More precisely, we demonstrate an ECM implementation capable to determine prime factors of up to 2,424 151-bit integers per second using a single Xilinx Virtex-4 SX35 FPGA. Using this implementation on a cluster like the COPACOBANA is beneficial for attacking cryptographic primitives like the well-known RSA cryptosystem with advanced methods such as the Number Field Sieve (NFS). To provide this vast number of integer factorizations per FPGA, we make use of the available DSP blocks on each Virtex-4 device to accelerate low-level arithmetic computations. This methodology allows the development of a time-area efficient design that runs 24 ECM cores in parallel, implementing both phase 1 and phase 2 of the ECM. Moreover, our design is fully scalable and supports composite integers in the range from 66 to 236 bits without any significant modifications to the hardware. Compared to the implementation by Gaj et al., who reported an ECM design for the same Virtex-4 platform, our improved architecture provides an advanced cost-performance ratio which is better by a factor of 37.	broadcast signal intrusion;computation;cryptography;cryptosystem;custom hardware attack;ecrypt;field-programmable gate array;general number field sieve;high- and low-level;information security;input/output;integer factorization;lenstra elliptic curve factorization;memory management;population;rsa (cryptosystem);scalability;whole earth 'lectronic link	Ralf Zimmermann;Tim Güneysu;Christof Paar	2010	2010 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2010.26	embedded system;parallel computing;computer science;theoretical computer science;field-programmable gate array	EDA	8.836319963981323	44.55539418796401	51195
57112b1a3e6147162abb8e877111668722ee05dd	cache friendly sparse matrix-vector multiplication	conjugate gradient method;scientific computing;sparse matrix;numerical experiment;sparse matrices	Sparse matrix-vector multiplication or SpMxV is an important kernel in scientific computing. For example, in the conjugate gradient method, where SpMxV is the main computational step. Though the total number of arithmetic operations in SpMxV is fixed, reducing the probability of cache misses per operation is still a challenging area of research. In this work, we present a new column ordering algorithm for sparse matrices. We analyze the cache complexity of SpMxV when A is ordered by our technique. The numerical experiments, with very large test matrices, clearly demonstrate the performance gains rendered by our proposed technique.	algorithm;cache (computing);computational science;conjugate gradient method;experiment;matrix multiplication;numerical analysis;sparse matrix	Sardar Anisul Haque;Shahadat Hossain;Marc Moreno Maza	2010	ACM Comm. Computer Algebra	10.1145/1940475.1940490	sparse matrix;theoretical computer science;mathematics;algebra	HPC	-2.2487287085193395	38.90825818089127	51604
34f8e87ff76120931d6a59b71a43430d8e4bd882	designing and evaluating high speed elliptic curve point multipliers	hardware delay effects polynomials field programmable gate arrays multiplexing elliptic curve cryptography table lookup;gf 2k multiplication algorithm elliptic curve cryptography ecc point multiplication pm;delay effects;finite field computation;vlsi design;multiplexing;polynomials;elliptic curve cryptography;vlsi design elliptic curve cryptography finite field computation;field programmable gate arrays;table lookup;hardware;public key cryptography digital arithmetic	Point Multiplication (PM) is considered the most computationally complex and resource hungry Elliptic Curve Cryptography (ECC) related mathematic operation. The design of PM hardware accelerators follows approaches that have a trade off between utilized hardware resources and computation speed. In this paper, the above trade-off and its relation with the operations of the GF(2k) defining the Elliptic Curve (EC) is highlighted and investigated. Following this direction, a point operation design methodology based on the parallelization and scheduling of GF(2k) operations is proposed. This design approach is adapted to the PM employed GF(2k) multiplication algorithm and associated implementation in an effort to increase PM accelerator speed with an acceptable cost on chip covered area (hardware resources). Using the proposed methodology, two PM accelerator hardware architectures were proposed based on bit serial and bit parallel GF(2k) multipliers that, when implemented in FPGA technology, proved to be very fast in comparison to other similar works.	computation;elliptic curve cryptography;field-programmable gate array;hardware acceleration;multiplication algorithm;parallel computing;scheduling (computing);serial communication	Apostolos P. Fournaris;John Zafeirakis;Odysseas G. Koufopavlou	2014	2014 17th Euromicro Conference on Digital System Design	10.1109/DSD.2014.104	embedded system;parallel computing;elliptic curve digital signature algorithm;computer science;theoretical computer science;counting points on elliptic curves;elliptic curve cryptography;very-large-scale integration;multiplexing;field-programmable gate array;polynomial	EDA	9.736441846674442	44.66195070350618	51643
ee1c87bd879505da33af1b568059313954cdda5b	dynamic energy-aware sensor configuration in multi-application monitoring systems		A typical pervasive monitoring system like a smart building depends on an infrastructure composed of hundreds of heterogeneous wireless sensor devices. Managing the energy consumption of these devices poses a challenging problem that affects the overall efficiency and usability. Existing approaches for sensor energy consumption typically assume a single monitoring application to consume sensor data and a static configuration for sensor devices. In this paper, we focus on a multi-application context with dynamic requirements and multi-modal sensor devices. We present 3SoSM, an approach to optimize interactions between application requirements and wireless sensor environment in real-time. It relies on an energy-aware dynamic configuration of sensor devices to lower energy consumption while fulfilling application requirements. To bind together sensor configuration and dynamic management of data streams, we design a sustainable multi-application monitoring system architecture for pervasive environments that collects application requirements for sensor data streams and optimizes them into sensor configurations. To demonstrate the effectiveness of our approach, a set of experiments are designed in the context of smart buildings. We comparatively evaluate our approach to show how dynamic sensor configuration for multiple monitoring applications indeed outperforms the mainstream duty-cycling method.	context (computing);duty cycle;experiment;interaction;modal logic;real-time clock;requirement;sensor;systems architecture;usability	Ozgun Pinarer;Yann Gripay;Sylvie Servigne;Atay Ozgovde;Atilla Baskurt	2017	Pervasive and Mobile Computing	10.1016/j.pmcj.2017.08.005	building automation;visual sensor network;wireless sensor network;embedded system;real-time computing;computer science;sensor web;sensor node;key distribution in wireless sensor networks;mobile wireless sensor network;intelligent sensor	Mobile	2.723730271399316	33.381069530858625	51699
38c92d89b71c4e54b024b7bd07155a5f74991e54	new modular multiplication algorithms for fast modular exponentiation	modular exponentiation;modular multiplication	A modular exponentiation is one of the most important operations in public-key cryptography. However, it takcs much time because the modular exponentiation deals with very large operands as 512-bit integers. The modular exponentiation is composed of repetition of modular multiplications. Therefore, we can rcducc the execution time of it by reducing thc execution time of each modular multiplication. In this paper, we propose two fast modular multiplication algorithms. One is for modular multiplications between different integers, and the other is for modular squarings. These proposed algorithms require single-precision multiplications fewer than those of Montgomery modular multiplication algorithms by 1/2 and 1/3 times, respectively. Implementing on PC, proposed algorithms reducc execution times by 50% and 30% compared with Montgomery algorithms, respectively.	modular exponentiation;montgomery modular multiplication;multiplication algorithm;operand;public-key cryptography;run time (program lifecycle phase);single-precision floating-point format	Seong-Min Hong;Sang-Yeop Oh;Hyunsoo Yoon	1996		10.1007/3-540-68339-9_15	arithmetic;exponentiation by squaring;modular arithmetic;parallel computing;knuth's up-arrow notation;computer science;mathematics;algorithm;modular exponentiation;algebra	Crypto	9.22758082292236	43.84483039045448	52229
d1626ef06805b4168d162c17a8dfcce2d26b715a	abstract only: an empirical study of parallel big number arithmetic	empirical study;parallel processing	To capitalize on multi-core processing, it would be good to perform big number arithmetic in parallel. While a performance increase equal to the number of processors is theoretically possible, our experiments show that inpractice the likelihood of gaining any performance increase for big number arithmetic through parallel processing is low. A speed-up approaching the number of processors was not achieved for multiplication until the operands had at least 215 bits. No performance gain was realized for addition. The base algorithm used for multiplication was O(n2) and, therefore, suboptimal. The expected performance gain achieved when parallelizing a more efficient base algorithm should be even smalle.	algorithm;automatic parallelization;central processing unit;experiment;multi-core processor;operand;parallel computing	Deborah Mathews	2010	ACM Comm. Computer Algebra	10.1145/1838599.1838615	arithmetic;parallel processing;theoretical computer science;mathematics;empirical research	DB	-0.6112998299856404	39.37653185570543	52558
6c50058425a848c758f4a9671f32f6dcd90af253	a compact fpga-based montgomery multiplier over prime fields	fpga;coprocessor;montgomery multiplication;finite field arithmetic;prime fields	This work describes a compact FPGA hardware architecture for computing modular multiplications over GF(p) using the Montgomery method, suitable for public key cryptography for embedded or mobile systems. The multiplier is parameterizable, allowing to evaluate the hardware design for different prime fields using different radix of the form β = 2k. The design uses only three k x k multipliers and three 2k-bit adders. The hardware organization of the multiplier maximizes the use of the multipliers processing iteratively the multiplicand, multiplier and modulus. The parametric design allows to study area-performance trade offs, in order to meet system requirements such as available resources, throughput, and efficiency. The proposed multiplier achieves a 1024-bit modular multiplication in 15.63 μs using k = 32. Compared to the most compact FPGA implementation previously reported, our proposed design uses 79% less FPGA resources with better efficiency expressed as Mbps/Slice.	bit slicing;data rate units;embedded system;field-programmable gate array;modulus of continuity;montgomery modular multiplication;parametric design;public-key cryptography;requirement;system requirements;throughput	Miguel Morales-Sandoval;Arturo Diaz-Perez	2013		10.1145/2483028.2483102	arithmetic;embedded system;finite field arithmetic;parallel computing;montgomery reduction;computer science;theoretical computer science;mathematics;coprocessor;field-programmable gate array	Mobile	9.817082455440246	44.697359644969936	52712
1006da3e40a66356668355c7694a7f709fc3d881	parallel cyk membership test on gpus	parallel computing;paper;nvidia geforce gtx 560 ti;gpu programming;cuda;nvidia;algorithms;computer science;context free language membership test;cyk algorithm	Nowadays general-purpose computing on graphics processing units (GPGPUs) performs computations what were formerly handled by the CPU using hundreds of cores on GPUs. It often improves the performance of sequential computation when the running program is wellstructured and formulated for massive threading. The CYK algorithm is a well-known algorithm for the context-free language membership test and has been used in many applications including grammar inferences, compilers and natural language processing. We revisit the CYK algorithm and its structural properties suitable for parallelization. Based on the discovered properties, we then parallelize the algorithm using different combinations of memory types and data allocation schemes using a GPU. We evaluate the algorithm based on real-world data and herein demonstrate the performance improvement compared with CPU-based computations.	benchmark (computing);cpu cache;cyk algorithm;central processing unit;compiler;computation;computer graphics;context-free language;data access;dummy variable (statistics);fastest;general-purpose computing on graphics processing units;general-purpose markup language;graphics processing unit;linux virtual server;memory access pattern;natural language processing;overhead (computing);parallel computing;shared memory;sorting;test case;thread (computing);zero-copy	Kyoung-Hwan Kim;Sang-Min Choi;Hyein Lee;Ka Lok Man;Yo-Sub Han	2014		10.1007/978-3-662-44917-2_14	parallel computing;computer science;theoretical computer science;operating system;programming language;general-purpose computing on graphics processing units	PL	-1.876427833997409	43.15143801626286	52938
22c7c647045337265cce7f2b4d4acf52ed7b8237	a multifunctional rf remote control for ultralow standby power home appliances		In spite of many benefits, since a target RF should be able to react to real time user commands even during system power-off, RF remote controls generally require more standby energy than IR manner. Therefore, in this paper a multifunctional RF remote control (MRRC), which is capable of providing larger coverage and various services, is introduced, and an ultralow standby power operation method for target RFs, utilizing an extended preamble transmission and a variable length periodic preamble sensing according to system power states, is proposed. In addition, a prototype and implementation details are also described. In order to evaluate the proposed MRRC, several experiments are conducted, and each performance of MRRC is also compared with ZigBee RF4CE no power saving and power saving mode. The experimental results demonstrate that the MRRC system enables not only ultralow standby power operation during system power-off but also low power operation even in system power-on state. In spite of ultralow standby power operation, the experimental result also shows that the MRRC provides reasonable response time to user command.	multi-function printer;radio frequency;remote control	Kwang-il Hwang;Sung-Hyun Yoon	2014	IJDSN	10.1155/2014/381430	embedded system;real-time computing;standby power	HCI	2.4331478353496303	33.839908133737104	52945
cda54ab1f47dfd4cadd183fb9f3a0ff95a38ab46	the communication performance of the cray t3d and its effect on iterative solvers	distributed memory;iterative solver;linear system of equations;multigrid algorithm;distributed memory system;computation fluid dynamics;turbulent combustion;linear system;direct numerical simulation;computational fluid dynamics;conjugate gradient;low latency;communication requirements;cray t3d;linear equations;iterative solvers;workstation cluster	On many distributed memory systems, such as workstation clusters or the Intel iPSC/860, the multigrid algorithm suffers from having extensive communication requirements and, in general, it is not very competitive in comparison to the conjugate gradient algorithm. This is in contrast to the sequential problem whereby the multigrid algorithm is very effective in reducing the global residual, particularly for very large linear systems of equations. These two algorithms are now compared on the Cray T3D for solving very large systems of linear equations (resulted from grids of the order 2563 cells). The communication performance of the Cray T3D is first measured by the standard ping-pong tests and also by practical communication tasks that are found frequently in CPD calculations. It is found that the Cray T3D has a low latency (= 6 ps) and a high bandwidth interprocessor communication (120 MB/s) when the low-level intrinsic communication routines are used. As a result, the multigrid algorithm is found to be very competitive when compared with the conjugate gradient algorithm for solving the very large linear systems arising from the Direct Numerical Simulation of turbulent Combustion (DNSC). Results are contrasted by those on the Intel iPSC/%O.	algorithm;central processing unit;collaborative product development;computation;conjugate gradient method;cray t3d;direct numerical simulation;distributed memory;high- and low-level;intel ipsc;inter-process communication;iteration;linear equation;linear system;mebibyte;multigrid method;numerical linear algebra;overhead (computing);ping-pong scheme;preconditioner;requirement;roland gs;set packing;stationary process;system of linear equations;the machine;turbulence;uptime;workstation	Y. F. Hu;D. R. Emerson;R. J. Blake	1996	Parallel Computing	10.1016/0167-8191(96)00035-X	system of linear equations;computational science;parallel computing;distributed memory;computational fluid dynamics;computer science;theoretical computer science;conjugate gradient method;linear equation;direct numerical simulation;linear system;algebra;low latency	HPC	-3.4103808662580413	38.31534444985089	53838
5b5c3e8c9ef8fcb3880e5a00a458e570ea00ca48	stdcbench: a benchmark for small systems		Benchmark programs are useful for measuring performance. Benchmarks written in C effectively benchmark the performance of a C implementation consisting of hardware, compiler and standard library. For small systems (i.e. systems with just a few KB of memory) three well-known and widely used benchmarks are Whetstone, Dhrystone and Coremark. However, all three have their shortcomings. Whetstone scores depend heavily on the performance of floating-point functions from the standard library. Dhrystone scores depend heavily on the performance of just a few string processing functions from the standard library. Coremark intentionally avoids using the standard library and the scores heavily depend on the performance of matrix multiplications. All three thus highly depend on a single aspect of the C implementation each, so that optimizations targeting that aspect have a huge effect on scores.  stdcbench is a benchmark for small systems that tries to give a more balanced reflection of performance. It is intended to be usable for a wide range of C implementations for small systems. We present the design of stdcbench, and discuss a few benchmark results also in comparison to Dhrystone and Coremark.	benchmark (computing);coremark;dhrystone;silicon compiler;standard library;string (computer science);whetstone (benchmark)	Philipp Klaus Krause	2018		10.1145/3207719.3207726	parallel computing;microcontroller;implementation;compiler;computer science;dhrystone;matrix multiplication;usable;coremark;whetstone	PL	-3.754608139768329	45.70146336158249	53896
2d64cb96180191711a6773baf5733e4eec0ce271	vlsi architectures for the restricted boltzmann machine		Neural network (NN) systems are widely used in many important applications ranging from computer vision to speech recognition. To date, most NN systems are processed by general processing units like CPUs or GPUs. However, as the sizes of dataset and network rapidly increase, the original software implementations suffer from long training time. To overcome this problem, specialized hardware accelerators are needed to design high-speed NN systems. This article presents an efficient hardware architecture of restricted Boltzmann machine (RBM) that is an important category of NN systems. Various optimization approaches at the hardware level are performed to improve the training speed. As-soon-as-possible and overlapped-scheduling approaches are used to reduce the latency. It is shown that, compared with the flat design, the proposed RBM architecture can achieve 50% reduction in training time. In addition, an on-the-fly computation scheme is also used to reduce the storage requirement of binary and stochastic states by several hundreds of times. Then, based on the proposed approach, a 784-2252 RBM design example is developed for MNIST handwritten digit recognition dataset. Analysis shows that the VLSI design of RBM achieves significant improvement in training speed and energy efficiency as compared to CPU/GPU-based solution.	artificial neural network;bitwise operation;central processing unit;computation;computer vision;flat design;graphics processing unit;hardware acceleration;mnist database;mathematical optimization;restricted boltzmann machine;scheduling (computing);speech recognition;speedup;very-large-scale integration	Bo Yuan;Keshab K. Parhi	2017	JETC	10.1145/3007193	real-time computing;computer science;latency (engineering);very-large-scale integration;parallel computing;hardware architecture;architecture;artificial neural network;theoretical computer science;ranging;mnist database;restricted boltzmann machine	Arch	4.0540693597455295	43.0398893866822	54030
366efb131efb6bc6fc2c64bc7c90c9b057197a71	fpga accelerated parallel sparse matrix factorization for circuit simulations	processing element;fpga;circuit simulation;parallelism;sparse matrix;lu factorization;sparse matrices	Sparse matrix factorization is a critical step for the circuit simulation problem, since it is time consuming and computed repeatedly in the flow of circuit simulation. To accelerate the factorization of sparse matrices, a parallel CPU+FPGA based architecture is proposed in this paper. While the preprocessing of the matrix is implemented on CPU, the parallelism of numeric factorization is explored by processing several columns of the sparse matrix simultaneously on a set of processing elements (PE) in FPGA. To cater for the requirements of circuit simulation, we also modified the Gilbert/Peierls (G/P) algorithm and considered the scalability of our architecture. Experimental results on circuit matrices from the University of Florida Sparse Matrix Collection show that our architecture achieves speedup of 0.5x-5.36x compared with the CPU KLU results.	central processing unit;column (database);computation;electronic circuit simulation;entity;field-programmable gate array;gilbert cell;lu decomposition;liu hui's π algorithm;microprocessor;parallel computing;preprocessor;requirement;scalability;solver;sparse matrix;speedup;the matrix	Wei Wu;Yi Shan;Yu Wang;Huazhong Yang	2011		10.1007/978-3-642-19475-7_33	parallel computing;sparse matrix;computer science;theoretical computer science;sparse approximation;distributed computing	HPC	-2.4863272864837573	39.2751564877011	54223
a4cd92d3177af21607841fe38ff732b5776ca917	fast and efficient implementation of aes via instruction set extensions	fixed field constant multiplication;cryptography hardware public key acceleration galois fields information security data security application software throughput application specific integrated circuits;acceleration module;intellectual property;information security;application software;matrix multiplication coprocessors cryptography galois fields industrial property instruction sets;block cipher;galois field cryptography aes block cipher;aes;coprocessors;acceleration;instruction set extension;public key;efficient implementation;application specific integrated circuits;cryptography;ip processor;32 bit sparc v8 compatible processor;matrix multiplication;industrial property;advanced encryption standard;fixed field constant multiplication aes block cipher data security advanced encryption standard intellectual property ip processor acceleration module general purpose instruction set extension 32 bit sparc v8 compatible processor galois field;galois field;high speed;galois fields;software implementation;general purpose instruction set extension;instruction sets;throughput;hardware;data security	Efficient implementation of block ciphers is critical towards achieving both high security and high-speed processing. Numerous block ciphers, including the Advanced Encryption Standard (AES), have been proposed and implemented, using a wide and varied range of functional operations. Existing microprocessor architectures do not provide this broad range of support. However, the advent of intellectual property (IP) processor cores presents the opportunity to augment existing datapaths with instruction set extensions to add acceleration modules in the form of new instructions. We will present a general purpose instruction set extension to a 32-bit SPARC V8 compatible processor core that accelerates the performance of Galois Field fixed field constant multiplication, a core element of the AES algorithm. This extension will be shown to accelerate AES encryption versus pure software implementations at a small hardware cost. This matches the improvement demonstrated in previously proposed AES-specific instruction set extensions while maintaining a generalized implementation format capable of supporting other algorithms that use Galois Field fixed field constant multiplication.	32-bit;algorithm;assembly language;block cipher;cycle count;encryption;gate equivalent;information processing;microprocessor;multi-core processor;profiling (computer programming);sparc;the matrix	Adam J. Elbirt	2007	21st International Conference on Advanced Information Networking and Applications Workshops (AINAW'07)	10.1109/AINAW.2007.182	advanced encryption standard;computer architecture;parallel computing;computer science;information security;theoretical computer science;operating system;aes instruction set;computer security	EDA	8.650212541709625	45.00406663912584	54505
009f96ce9878f8ebc9ee250415c7aef53426fe6c	rram fabric for neuromorphic and reconfigurable compute-in-memory systems		We are on the cusp of a new computing era with increased reliance on artificial intelligence, cognitive processing, and big-data analysis. These tasks demand faster and more power-efficient hardware, which may, in turn, be driven by architecture and device innovations. RRAM is widely viewed as a promising candidate that can meet future storage and computing needs. In this paper, we discuss potential computing applications enabled by RRAM devices within both conventional and emerging computing paradigms. A concept of RRAM-based Memory Processing Unit (MPU), which allows the hardware resources to be dynamically allocated to optimally process a broad range of tasks, will be introduced.	artificial intelligence;big data;cognition;in-memory database;mpu-401;neuromorphic engineering;reconfigurable computing;resistive random-access memory	Mohammed Affan Zidan;Wei D. Lu	2018	2018 IEEE Custom Integrated Circuits Conference (CICC)	10.1109/CICC.2018.8357066	electronic engineering;memristor;architecture;artificial neural network;computer science;control reconfiguration;computer architecture;memory processing;resistive random-access memory;neuromorphic engineering	EDA	3.0738453639465955	43.724882667962426	54530
bdba998f27d4f6932684c3092454195dd4814d50	parallel louvain community detection optimized for gpus		Community detection now is an important operation in numerous graph based applications. It is used to reveal groups that exist within real world networks without imposing prior size or cardinality constraints on the set of communities. Despite its potential, the support for parallel computers is rather limited. The cause is largely the irregularity of the algorithm and the underlying heuristics imply a sequential nature. In this paper a GPU based parallelized version of the Louvain method is presented. The Louvain method is a multi-phase, iterative heuristic for modularity optimization. It was originally developed by Blondel et al. (2008), the method has become increasingly popular owing to its ability to detect high modularity community partitions in a fast and memory-efficient manner. The parallel heuristics used, were first introduced by Hao Lu et al. (2015). As the Louvain method is inherently sequential, it limits the possibility of scalable usage. Thanks to the proposed parallel heuristics, I observe how this method can behave on GPUs. For evaluation I implemented the heuristics using CUDA on a GeForce GTX 980 GPU and for testing Ive used organization landscapes from the CERN developed Collaboration Spotting project that involves patents and publications to visualize the connections in technologies among its collaborators. Compared to the parallel Louvain implementation running on 8 threads on the same machine that has the used GPU, the CUDA implementation is able to produce community outputs comparable to the CPU generated results, while providing absolute speedups of up to 30 using the GeForce GTX 980 consumer grade GPU.	cuda;central processing unit;computation;computer;dendrogram;geforce 200 series;geforce 900 series;graphics processing unit;heuristic (computer science);iterative method;lu decomposition;louvain modularity;mathematical optimization;parallel algorithm;parallel computing;performance;scalability;speedup;test case;time complexity	Richard Forster	2018	CoRR		cardinality;distributed computing;parallel computing;scalability;heuristic;computer science;cuda;heuristics;thread (computing);central processing unit;modularity	HPC	-2.797082288146001	42.19850948614287	54669
90e9a4037316cd5b1cee46ae765a010e3c3e8d5f	randomized qr with column pivoting		The dominant contribution to communication complexity in factorizing a matrix using QR with column pivoting is due to column-norm updates that are required to process pivot decisions. We use randomized sampling to approximate this process which dramatically reduces communication in column selection. We also introduce a sample update formula to reduce the cost of sampling trailing matrices. Using our column selection mechanism, we observe results that are comparable in quality to those obtained from the QRCP algorithm, but with performance near unpivoted QR. We also demonstrate strong parallel scalability on shared-memory multiple core systems using an implementation in Fortran with OpenMP. This work immediately extends to produce lowrank truncated approximations of large matrices. We propose a truncated QR factorization with column pivoting that avoids trailing matrix updates which are used in current implementations of level-3 BLAS QR and QRCP. Provided the truncation rank is small, avoiding trailing matrix updates reduces approximation time by nearly half. By using these techniques and employing a variation on Stewart’s QLP algorithm, we develop an approximate truncated SVD that runs nearly as fast as truncated QR.	approximation algorithm;blas;column-oriented dbms;communication complexity;computation;distributed memory;fortran;machine learning;openmp;qr decomposition;randomized algorithm;sampling (signal processing);scalability;selection (user interface);shared memory;singular value decomposition;time complexity;truncation error	Jed A. Duersch;Ming Gu	2017	SIAM J. Scientific Computing	10.1137/15M1044680	mathematical optimization;parallel computing;mathematics;fortran;qr decomposition;low-rank approximation;sampling (statistics);scalability;truncation;communication complexity;matrix (mathematics)	HPC	-1.4660770086419777	38.861024585727456	54695
d5a83763157eac567647c8f88c71f2142a989ba1	a sparse scf algorithm and its parallel implementation: application to dftb		We present an algorithm and its parallel implementation for solving a self-consistent problem as encountered in Hartree-Fock or density functional theory. The algorithm takes advantage of the sparsity of matrices through the use of local molecular orbitals. The implementation allows one to exploit efficiently modern symmetric multiprocessing (SMP) computer architectures. As a first application, the algorithm is used within the density-functional-based tight binding method, for which most of the computational time is spent in the linear algebra routines (diagonalization of the Fock/Kohn-Sham matrix). We show that with this algorithm (i) single point calculations on very large systems (millions of atoms) can be performed on large SMP machines, (ii) calculations involving intermediate size systems (1000-100 000 atoms) are also strongly accelerated and can run efficiently on standard servers, and (iii) the error on the total energy due to the use of a cutoff in the molecular orbital coefficients can be controlled such that it remains smaller than the SCF convergence criterion.	algorithm;architecture as topic;coefficient;computer architecture;density functional theory;fock space;functional theories of grammar;hartree–fock method;kohn–sham equations;linear algebra;molecular orbital;one thousand;small;sparse matrix;symmetric multiprocessing;tight binding;time complexity	Anthony Scemama;Nicolas Renon;Mathias Rapacioli	2014	Journal of chemical theory and computation	10.1021/ct500115v	parallel computing;theoretical computer science;computational chemistry	HPC	-4.260208806656895	37.6567539435216	54782
a5935430b8dc50db2081c4b0e70bf3760fa678b3	optimization of linked list prefix computations on multithreaded gpus using cuda	parallel computing;prefix sums;data parallel;multi threading coprocessors;multi threading;paper;linked list prefix computations;concurrent computing;list ranking;optimization technique;tesla c1060 optimization linked list prefix computations multithreaded gpus cuda prefix sums fine grain memory accesses extremely high bandwidth memory accesses data parallel computations randomization process cell processor mta nvidia geforce 200 series;cell processor;gpu;multithreaded gpus;parallel computing prefix computation gpu cuda;phase change random access memory;coprocessors;tesla c1060;cuda;memory access;tree graphs;extremely high bandwidth memory accesses;random process;nvidia geforce 200 series;multicore processing;parallel computer;nvidia;concurrent computing parallel algorithms multicore processing parallel processing graphics processing unit scalability hardware tree graphs educational institutions phase change random access memory;randomization process;optimization;technical report;parallel implementation;scalability;computer science;graphics processing unit;mta;data parallel computations;prefix computation;parallel processing;hardware;fine grain memory accesses;parallel algorithms	We present a number of optimization techniques to compute prefix sums on linked lists and implement them on multithreaded GPUs using CUDA. Prefix computations on linked structures involve in general highly irregular fine grain memory accesses that are typical of many computations on linked lists, trees, and graphs. While the current generation of GPUs provides substantial computational power and extremely high bandwidth memory accesses, they may appear at first to be primarily geared toward streamed, highly data parallel computations. In this paper, we introduce an optimized multithreaded GPU algorithm for prefix computations through a randomization process that reduces the problem to a large number of fine-grain computations. We map these fine-grain computations onto multithreaded GPUs in such a way that the processing cost per element is shown to be close to the best possible. Our experimental results show scalability for list sizes ranging from 1M nodes to 256M nodes, and significantly improve on the recently published parallel implementations of list ranking, including implementations on the Cell Processor, the MTA-8, and the NVIDIA GeForce 200 series. They also compare favorably to the performance of the best known CUDA algorithm for the scan operation on the Tesla C1060.	cuda;computation;graphics processing unit;linked list;thread (computing)	Zheng Wei;Joseph JáJá	2010		10.1109/IPDPS.2010.5470455	multi-core processor;parallel processing;computer architecture;parallel computing;scalability;multithreading;concurrent computing;computer science;technical report;theoretical computer science;operating system;distributed computing;parallel algorithm;tree;coprocessor	HPC	-0.021263859650397197	41.34271379016187	54985
b66d2ab0085045d2b272b6a44cdd5a904cbf89ba	latest advances on design and implementation of dsp systems		New challenges for implementations and design methodologies have been introduced over the years and the trend seems to continue. More and more complex algorithms need to be implementedwith higher performance but with strict constraints on power consumption. There is need for improving the implementation techniques, design methodologies, and algorithmarchitecture optimizations to tackle the challenges in present signal and information processing systems. This special issue contains a selection of papers reporting the latest advances on design and implementation of signal processing systems. The topics span from circuit level architectures in memories and arithmetic to dynamic code generation and from application implementations on mobile devices or heterogeneous platforms. In their paper Improving the Reliability of MLC NAND Flash Memories through Adaptive Data Refresh and Error Control Coding, Yang, Chen, Mudge, and Chakrabarti propose a combination of data refresh policies and low cost error control coding (ECC) schemes to address the errors in multilevel cell (MLC) NAND Flash memories given application characteristics. It is shown that an appropriate choice of refresh interval and BCH based ECC scheme can minimize memory energy while satisfying the reliability constraint. Hardware implementation of associative memories based on message passing algorithms on sparse graphs is described in Algorithm and Architecture of Fully-Parallel Associative Memories Based on Sparse Clustered Networks by Jarollahi, Onizawa, Gripon, and Gross. Architectures are derived that eliminates the need for computationally-complex winner-take all circuits. This results in improvement in clock frequency by about a factor of 2 and a reduction in circuit size. A design space exploration is provided and hardware complexity of the FPGA implementations are described. Antao and Sousa propose an implementation of an arithmetic accelerator for modular arithmetic based on the residue number system (RNS) in their paper A Flexible Architecture for Modular Arithmetic Hardware Accelerators based on RNS. An architecture of processing elements connected as a ring is proposed. The architecture is fully-parallel and is scalable to different algorithms and operand sizes. Implementations on FPGAs are provided. A Real-time Scalable Object Detection System using LowPower HOG Accelerator VLSI by Takagi, Tanaka, Izumi, Kawaguchi, and Yoshimoto proposes a real-time object detection system using a Histogram of Oriented Gradients (HOG) feature extraction accelerator and reconfigurable multiplyaccumulate array for supporting processing objects of different shapes. The proposed approach uses support vector machine for early classification. The system has been implemented on a 65 nmCMOS technology and the characteristics of the chip are reported in the paper. Blake and Hunter propose dynamically generated code for fast Fourier transforms. The fastest Fourier Transform in the South (FFTS) is a discrete Fourier transform library for x86 and ARM based devices, which was shown to be faster than FFTW, Intel IPP and Apple vDSP partly due to the use of program specialization and dynamic code generation. In this work, FFTS has been modified to dynamically exploit streaming store instructions on x86 machines, resulting in speedups of over 10 %, Also, when dynamic code generation is prohibited on some mobile platforms, FFTS has been altered to avoid it, while maximizing the performance. In paper Computer Vision Accelerators for Mobile Systems based on OpenCL GPGPU Co-Processing, Wang, Xiong, W. J. Gross (*) Department of Electrical and Computer Engineering, McGill University, Montreal, Quebec, Canada e-mail: warren.gross@mcgill.ca		Warren J. Gross;Zhiyuan Yan	2014	Signal Processing Systems	10.1007/s11265-014-0930-z	error detection and correction;architecture;parallel computing;digital signal processing;code generation;signal processing;very-large-scale integration;design space exploration;computer science;general-purpose computing on graphics processing units	EDA	4.948978998948121	44.08217304760886	55011
fa380ebf3b6e034090f6a7f4b2b81c9d6a57b344	intelligent energy management system for smart offices	power meters;electric connectors;energy management systems;zigbee electric connectors energy management systems power measurement power meters;sockets power demand energy management zigbee power measurement integrated circuits artificial intelligence;zigbee;appliances intelligent energy management system smart offices iems smart socket device zigbee controlled outlets power metering device power energy ic power consumption measurement power outlet probe multiplexer;power consumption;energy management system;power measurement	This paper presents an intelligent energy management system (IEMS) for smart offices. Our system includes the smart socket device that integrates ZigBee controlled outlets and a power metering device. We propose a new smart socket which has only one power/energy IC to measure the power consumption of every power-outlet by the probe multiplexer. The IEMS can control the power consumption of various appliances intellectually.	intellect;multiplexer	Jiho Kim;Wan-Hee Cho;Yongjin Jeong;Ohyoung Song	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6162020	embedded system;electronic engineering;real-time computing;engineering;smart grid	Robotics	1.93059243937506	32.80217444353014	55290
a99eb91bf5759566c528116e9c4db4912e71cbde	delivering parallel programmability to the masses via the intel mic ecosystem: a case study	manycore;programmability;data access patterns parallel programmability intel mic ecosystem moore law microprocessor clock speed intel xeon phi coprocessor many core architecture inter core parallelism intra core parallelism simd width data intensive applications bandwidth constraint michinders computational resources massive parallelism big data sets architectural expertise big data applications floyd warshall algorithm graph applications compiler optimizations intel xeon phi platform intel xeon phi ecosystem;coprocessors;programmability intel xeon phi mic graph floyd warshall manycore;multicore processing;graph;parallel processing coprocessors optimization hardware bandwidth multicore processing instruction sets;bandwidth;floyd warshall;optimization;mic;intel xeon phi;parallel processing;instruction sets;hardware;program compilers big data microprocessor chips parallel programming	"""Moore's Law effectively doubles the compute power of a microprocessor every 24 months. Over the past decade, however, this doubling in performance has been due to the doubling of the number of cores in a microprocessor rather than clock speed increases. Perhaps nowhere is this more evident than with the Intel Xeon Phi coprocessor. This many core architecture exhibits not only massive inter-core parallelism but also intra-core parallelism via a wider SIMD width. However, for data-intensive applications, the bandwidth constraint of MIChinders the full utilization of computational resources, especiallywhen massive parallelism is required to process big data sets. Furthermore, the process of optimizing the performance on suchplatforms is complex and requires architectural expertise. To evaluate the efficacy of the Intel MIC ecosystem for """"big data"""" applications, we use the Floyd-Warshall algorithmas a representative case study for graph applications. Ourstudy offers evidence that traditional compiler optimizations candeliver parallel programmability to the masses on the Intel XeonPhi platform. That is, developers can straightforwardly createmanycore codes in the Intel Xeon Phi ecosystem that deliversignificant speedup. The optimizations include reordering data-access patterns, adjusting loop structures, vectorizing branches, and using OpenMP directives. We start from the default serialalgorithm and apply the above optimizations one by one. Overall, we achieve a 281.7-fold speedup over the default serial version. When compared with the default OpenMP Floyd-Warshall parallel implementation, we still achieve a 6.4-fold speedup. We also observe that the identically optimized code on MIC can outperform its CPU counterpart by up to 3.2-fold."""	avx-512;automatic vectorization;big data;block (data storage);blocking (computing);breadth-first search;cpu cache;central processing unit;clock rate;code;computation;computational resource;coprocessor;data-intensive computing;directive (programming);ecosystem;experiment;floyd–warshall algorithm;graph (abstract data type);intel core (microarchitecture);intrinsic function;list of intel core i5 microprocessors;locality of reference;loop unrolling;manycore processor;mathematical optimization;microprocessor;moore's law;multi-core processor;norm (social);openmp;optimizing compiler;parallel computing;period-doubling bifurcation;programming complexity;simd;speedup;xeon phi	Kaixi Hou;Hao Wang;Wu-chun Feng	2014	2014 43rd International Conference on Parallel Processing Workshops	10.1109/ICPPW.2014.44	multi-core processor;parallel processing;computer architecture;parallel computing;floyd–warshall algorithm;computer science;operating system;instruction set;xeon phi;graph;bandwidth;coprocessor	HPC	-3.5960664804803764	43.791856544120066	55330
b5a4b0af698c8aea9fdd8dff6b0b11d1ade839d3	complexity analysis in heterogeneous system		Complexity analysis in heterogeneous system is one of the more complicated topics in the subject mathematics for computing. It involves an unusual concept and some tricky algebra. This paper shows how various algorithms take part in heterogeneous system and how they affect the overall system complexity? All analysis and methods are situation specific. Here we show a comparative study of various method and techniques of algorithm analysis giving their specific advantages and disadvantages. We will specify general guidelines for calculating the complexity of heterogeneous system which still gives sleepless nights to the researchers.	accounting method;algorithm;amortized analysis;analysis of algorithms;authentication;communications protocol;complex systems;dspace;encryption;fault tolerance;heterogeneous computing;information and computer science;information science;potential method;routing table	Kuldeep Sharma;Deepak Garg	2009	Computer and Information Science		mathematical optimization;computer science;artificial intelligence;theoretical computer science;mathematics;algorithm	DB	0.5316286713719619	35.683707034230984	55492
78e0f0f76d1706a345eda2c4b2e3c4ee6d5a0dd0	blocked algorithms for robust solution of triangular linear systems		We consider the problem of computing a scaling (alpha ) such that the solution ({varvec{x}}) of the scaled linear system ({varvec{Tx}} = alpha {varvec{b}}) can be computed without exceeding an overflow threshold (varOmega ). Here ({varvec{T}}) is a non-singular upper triangular matrix and ({varvec{b}}) is a single vector, and (varOmega ) is less than the largest representable number. This problem is central to the computation of eigenvectors from Schur forms. We show how to protect individual arithmetic operations against overflow and we present a robust scalar algorithm for the complete problem. Our algorithm is very similar to xLATRS in LAPACK. We explain why it is impractical to parallelize these algorithms. We then derive a robust blocked algorithm which can be executed in parallel using a task-based run-time system such as StarPU. The parallel overhead is increased marginally compared with regular blocked backward substitution.	algorithm;linear system	Carl Christian Kjelgaard Mikkelsen;Lars Karlsson	2017		10.1007/978-3-319-78024-5_7	scalar (physics);eigenvalues and eigenvectors;triangular matrix;scaling;algorithm;computer science;linear system	EDA	-1.9946697883085998	38.46457120322363	55651
4e9e3308bf7a4d09fe3bb855341311c49b3d740a	modular exponent realization on fpgas	field programmable gate array;concepcion circuito;integrated circuit;circuit design;circuito integrado;red puerta programable;reseau porte programmable;key exchange;conception circuit;circuit integre;modular exponent	The article describes modular exponent calculations used widely in cryptographic key exchange protocols. The measures for hardware consumption and execution speed based on argument bit width and algorithm rank are created. The partitioning of calculations is analyzed with respect to interconnect signal numbers and added delay. The partitioned blocks are used for implementation approximations of two different multiplier architectures. Examples are provided for 3 families of FPGAs: XC4000, XC6200 and FLEX10k	1-bit architecture;algorithm;approximation;clock rate;computer;field-programmable gate array;key (cryptography);key exchange;motherboard;routing	Juri Põldre;Kalle Tammemäe;Marek Mandre	1998		10.1007/BFb0055261	embedded system;real-time computing;key exchange;computer science;operating system;integrated circuit;circuit design;mathematics;distributed computing;algorithm;field-programmable gate array	EDA	9.76371050379849	44.459188507357766	55896
6db0cdd55421571e4551403ff48bec2ee1f5ed04	thundervolt: enabling aggressive voltage underscaling and timing error resilience for energy efficient deep learning accelerators		Hardware accelerators are being increasingly deployed to boost the performance and energy efficiency of deep neural network (DNN) inference. In this paper we propose Thundervolt, a new framework that enables aggressive voltage underscaling of high-performance DNN accelerators without compromising classification accuracy even in the presence of high timing error rates. Using post-synthesis timing simulations of a DNN accelerator modeled on the Google TPU, we show that Thundervolt enables between 34%-57% energy savings on state-of-the-art speech and image recognition benchmarks with less than 1% loss in classification accuracy and no performance loss. Further, we show that Thundervolt is synergistic with and can further increase the energy efficiency of commonly used run-time DNN pruning techniques like Zero-Skip.	deep learning	Jeff Zhang;Kartheek Rangineni;Zahra Ghodsi;Siddharth Garg	2018		10.1109/DAC.2018.8465918	machine learning;real-time computing;voltage;psychological resilience;deep learning;artificial neural network;artificial intelligence;efficient energy use;inference;computer science;data flow diagram	EDA	3.6208240433702255	42.33107196578105	55996
e9737a118b8d316d8584c9eec5aa88d5f32992d0	design and implementation of a floating-point quasi-systolic general purpose cordic rotator for high-rate parallel data and signal processing	floating point extension quasi systolic cordic rotator rotations pipelined architecture;systolic arrays;systolic arrays computerised signal processing matrix algebra parallel architectures;matrix algebra;parallel architectures;design and implementation;signal processing;signal processing algorithms hardware application software algorithm design and analysis computer applications computer graphics image processing radar applications array signal processing radar signal processing;scientific computing;floating point;high throughput;computerised signal processing	The authors describe the design and implementation of an algorithm and a processor which can be used to accelerate computations in which large amounts of rotations (circular as well as hyperbolic) are involved. The processor is a low-cost high-throughput VLSI implementation of the algorithm. With 10/sup 7/ rotations per second, many real-time and interaction-time applications in scientific computation become feasible. The required storage and/or silicon area is low and the execution time is independent of the particular operation performed. Another feature of this CORDIC design is its pipelined architecture and floating point extension. It is angle-pipelinable at the bit-level and has an execution time which is independent of any possible operation that can be executed. >	cordic;signal processing	A. A. de Lange;Ed F. Deprettere	1991		10.1109/ARITH.1991.145571	multidimensional signal processing;high-throughput screening;computer architecture;parallel computing;computer science;floating point;theoretical computer science;operating system;signal processing	ML	4.732326711047388	45.142410296285405	56178
3be5d51aa16127c8051c94be4954a2645b7024b1	power efficient special processor design for burrows-wheeler-transform-based short read sequence alignment		Because of the advancement of Next Generation Sequencing technology, DNA short reads can be generated in parallel at very high-throughput rates. One of the new challenges in bioinformatics is how to map these high volume short reads to the target reference accurately and efficiently. Besides the conventional software approaches, using a special processor for sequence alignment is a promising alternative. In this work, we present a Burrows-Wheeler-Transform-based hardware architecture and implement it on a special processor using 90 nm TSMC technology. With techniques including Forward String Sorting, Depth-First Search, and Extension Modules, our hardware is very efficient in terms of time and power. When compared to the hundred-Watt-level software approach, the proposed solution needs only less than 600 mW to achieve the same speed performance.	bioinformatics;burrows–wheeler transform;communications satellite;depth-first search;high-throughput computing;processor design;sequence alignment;sorting;throughput	Nae-Chyun Chen;Tai-Yin Chiu;Yu-Cheng Li;Yu-Chun Chien;Yi-Chang Lu	2015	2015 IEEE Biomedical Circuits and Systems Conference (BioCAS)	10.1109/BioCAS.2015.7348380	parallel computing;real-time computing;computer science;theoretical computer science	Arch	0.3190227584599418	43.110231291978174	56186
72d8587018e9aee30f7656c14e8265ada24bcf83	comparative study of one-sided factorizations with multiple software packages on multi-core hardware	mathematics computing;matrix decomposition;multiprocessing systems;parallel architectures;parallel programming;software packages;software portability;cholesky factorization;ibm power6;intel xeon emt64;mkl;pessl;plasma performance;parallel linear algebra for scalable multicore architecture;scalapack;tblas;commercial software offerings;linear algebra package;matrix algebra;multicore architectures;multicore hardware;multiple software package;one-sided linear algebra factorization;parallel execution;parallelism;portability;task based linear algebra subroutines	The emergence and continuing use of multi-core architectures require changes in the existing software and sometimes even a redesign of the established algorithms in order to take advantage of now prevailing parallelism. The Parallel Linear Algebra for Scalable Multi-core Architectures (PLASMA) is a project that aims to achieve both high performance and portability across a wide range of multi-core architectures. We present in this paper a comparative study of PLASMA's performance against established linear algebra packages (LAPACK and ScaLAPACK), against new approaches at parallel execution (Task Based Linear Algebra Subroutines -- TBLAS), and against equivalent commercial software offerings (MKL, ESSL and PESSL). Our experiments were conducted on one-sided linear algebra factorizations (LU, QR and Cholesky) and used multi-core architectures (based on Intel Xeon EMT64 and IBM Power6). A performance improvement of 67% was for instance obtained on the Cholesky factorization of a matrix of order 4000, using 32 cores.	algorithm;cholesky decomposition;commercial software;emergence;experiment;lapack;linear algebra;math kernel library;multi-core processor;parallel computing;scalapack;subroutine;weatherstar;x86-64	Emmanuel Agullo;Bilel Hadri;Hatem Ltaief;Jack J. Dongarra	2009	Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis	10.1145/1654059.1654080	parallel computing;xeon;linear algebra;multi-core processor;cholesky decomposition;scalapack;power6;computer architecture;distributed computing;numerical linear algebra;software portability;computer science	HPC	-3.7470288928668745	40.543554815764296	56225
66f3c3ca8b236b394b3fa5d9c2a176bdfa4159ac	a massively parallel digital learning processor		We present a new, massively parallel architecture for accelerating machine learning algorithms, based on arrays of vector processing elements (VPEs) with variable-resolution arithmetic. Groups of VPEs operate in SIMD (single instruction multiple data) mode, and each group is connected to an independent memory bank. The memory bandwidth thus scales with the number of VPEs, while the main data flows are local, keeping power dissipation low. With 256 VPEs, implemented on two FPGAs (field programmable gate array) chips, we obtain a sustained speed of 19 GMACS (billion multiplyaccumulate per sec.) for SVM training, and 86 GMACS for SVM classification. This performance is more than an order of magnitude higher than that of any FPGA implementation reported so far. The speed on one FPGA is similar to the fastest speeds published on a Graphics Processor for the MNIST problem, despite a clock rate that is an order of magnitude lower. Tests with Convolutional Neural Networks show similar compute performances. This massively parallel architecture is particularly attractive for embedded applications, where low power dissipation is critical.	algorithm;clock rate;computation;convolutional neural network;embedded system;fastest;field-programmable gate array;goodyear mpp;gosling emacs;graphics processing unit;low-power broadcasting;mnist database;machine learning;memory bandwidth;memory bank;parallel computing;performance;routing;simd;set packing;vector processor	Hans Peter Graf;Srihari Cadambi;Igor Durdanovic;Venkata Jakkula;Murugan Sankaradass;Eric Cosatto;Srimat T. Chakradhar	2008			parallel computing;real-time computing;computer hardware;computer science	Arch	3.7346824619750323	43.69460840735043	56428
ccb61816a08aed1e513a519d329e0bf04c204faf	a scalable and high performance elliptic curve processor with resistance to timing attacks	multiplying circuits;elliptic curve;reconfigurable architectures;cosic;hardware architecture;data flow graph;elliptic curve cryptography;cryptography;side channel attacks;fixed point arithmetic;reconfigurable architectures cryptography fixed point arithmetic microprocessor chips multiplying circuits galois fields;clock cycles elliptic curve processor timing attack resistance point multiplication algorithm double add subtract bit pattern key pattern extraction data flow graph galois field operators processor architecture bit serial field multiplier bit serial field squarer configurable processor;elliptic curves timing delay galois fields clocks process design data mining flow graphs scheduling algorithm processor scheduling;security;galois field;high performance;galois fields;microprocessor chips;timing attack	This paper presents a high performance and scalable elliptic curve processor which is designed to be resistant against timing attacks. The point multiplication algorithm (double-add-subtract) is modified so that the processor performs the same operations for every 3 bits of the scalar k independent of the bit pattern of the 3 bits. Therefore, it is not possible to extract the key pattern using a timing attack. The data flow graph of the modified algorithm is derived and the underlying Galois field operators are scheduled so that the point multiplication delay is minimized. The architecture of this processor is based on the Galois field of GF(2n) and the bit-serial field multiplier and squarer are designed. The processor is configurable for any value of n and the delay of point multiplication is [18(n+3) + (n+3)/2 + 1]/spl times/(n/3) clock cycles. For the case of GF(2/sup 163/) the point multiplication delay is 165888 clock cycles.	canonical quantization;clock signal;dataflow;multiplication algorithm;scalability;serial communication	Alireza Hodjat;David Hwang;Ingrid Verbauwhede	2005	International Conference on Information Technology: Coding and Computing (ITCC'05) - Volume II	10.1109/ITCC.2005.32	parallel computing;real-time computing;theoretical computer science;mathematics	EDA	9.807317503815323	44.552908204734536	56479
d0105df7879ae848d0f13a8f446ae123b01f715b	a study on the use of smartphones under realistic settings to estimate road roughness condition	signal image and speech processing;information systems applications incl internet;communications engineering networks	Almost every today's smartphone is integrated with many useful sensors. The sensors are originally designed to make the smartphones' user interface and applications more convenient and appealing. These sensors, moreover, are potentially useful for many other applications in different fields. Using smartphone sensors to estimate road roughness condition has a great potential, since many similar sensors are already in use in many sophisticated road roughness profilers. This study explores the use of data, collected by sensors from smartphones under realistic settings, in which the smartphones are placed at more realistic locations and under realistic manner inside a moving vehicle, to evaluate its relationship with the actual road pavement roughness. An experiment has been conducted to collect data from smartphone acceleration and Global Positioning System (GPS) sensors; frequency domain analysis is also carried out. It has been revealed that the data from smartphone accelerometers has a linear relationship with road roughness condition, whereas the strength of the relationship varies at different frequency ranges. The results of this paper also confirm that smartphone sensors have a great potential to be used for estimating the current status of the road pavement condition.		Viengnam Douangphachanh;Hiroyuki Oneyama	2014	EURASIP J. Wireless Comm. and Networking	10.1186/1687-1499-2014-114	embedded system;simulation;telecommunications	HCI	5.009854908276586	34.82299955085192	56520
7b1bf1d95a7b961a5fb14b7611ef0045939c27ea	a constant array multiplier core generator with dynamic partial evaluation architecture selection (abstract only)	digital signal processing;control system;parallelism;speculation;partial evaluation;networking;hardware description language;reconfigurable hardware	Numerous applications in Digital Signal Processing (DSP), telecommunications, graphics, cryptography and control systems have computations that involve a large number of multiplications of one variable with one or several constants. In this paper, we present a constant array multiplier core generator using dynamic partial evaluation. The proposed constant array multiplier core generator combines a new partial evaluation method named Full Complement Recoding with Booth's recoding and the straightforward partial evaluation method. Based on the number of 0s, the number of runs that have more than two consecutive 1s and the total number of 1s in all the runs in the constant operand, the proposed multiplier core generator selects one of the three partial evaluation methods to construct a partial evaluation architecture and generate an efficient Hardware Description Language (HDL) code that can be used as a design component. The constant multiplier core generated by the Xilinx CORE GeneratorTM system does not provide the optimized constant multipliers for a large number of cases. When implemented using Xilinx FPGA Virtex II device, the average area saving and delay improvement of the constant multiplier generated by proposed core generator is 70% and 36% compared to the 55% and 15% of constant multipliers generated by Xilinx CORE GeneratorTM system.	booth's multiplication algorithm;computation;control system;cryptography;digital signal processing;field-programmable gate array;graphics;hardware description language;operand;partial evaluation;virtex (fpga)	Bo Yang;Nikhil Joshi;Ramesh Karri	2005		10.1145/1046192.1046267	embedded system;speculation;parallel computing;real-time computing;computer hardware;reconfigurable computing;computer science;control system;operating system;digital signal processing;hardware description language;partial evaluation	EDA	7.0508978501092185	45.8532615427126	56597
f42fe8b6f8d3e145807bbe0ed01cfd3fb18c2e4f	scaling of parallel multiple sequence alignment on the supercomputer juqueen	biology computing;optimisation;performance evaluation;parallel program implementation parallel multiple sequence alignment juqueen supercomputer optimization performance evaluation clustalw algorithm bluegene q supercomputer influenza virus sequences parallel io interface;parallel programming;supercomputers influenza program processors performance evaluation optimization algorithm design and analysis;parallel machines;parallel performance biocomputing clustalw algorithm high performance computing multiple sequence alignment;performance evaluation biology computing optimisation parallel machines parallel programming	In this paper is proposed optimization, scaling, performance evaluation and profiling of parallel multiple sequence alignment based on ClustalW algorithm on the supercomputer BlueGene/Q, so-called JUQUEEN, for the case study of the influenza virus sequences. For this purpose a parallel I/O interface for simultaneous and independent access to single file collectively has been designed and verified on the basis of parallel program implementation on the supercomputer JUQUEEN.	algorithm;blue gene;clustalw/clustalx;input/output;mathematical optimization;multiple sequence alignment;parallel i/o;performance evaluation;supercomputer	Plamenka Borovska;Veska Gancheva;Soon-Heum Ko	2013	2013 IEEE 7th International Conference on Intelligent Data Acquisition and Advanced Computing Systems (IDAACS)	10.1109/IDAACS.2013.6663013	computer architecture;parallel computing;computer science;theoretical computer science;massively parallel	HPC	-0.7883872971125989	42.61805915582011	56765
4c91a2707802755361e19e9c5aa57d4554ef2f01	on how to design dataflow fpga-based accelerators for convolutional neural networks		In the past few years we have experienced an extremely rapid growth of modern applications based on deep learning algorithms such as Convolutional Neural Network (CNN), and consequently, an intensification of academic and industrial research focused on the optimization of their imple- mentation. Among the different alternatives that have been ex- plored, FPGAs seems to be one of the most attractive, as they are able to deliver high performance and energy-efficiency, thanks to their inherent parallelism and direct hardware execution, while retaining extreme flexibility due to their reconfigurability.In this paper we present a design methodology of a dataflow accelerator for the implementation of CNNs on FPGAs, that ensures scalability – and achieve a higher degree of parallelism as the size of the CNN increases – and an efficient exploitation of the available resources. Furthermore, we analyze resource consumption of the layers of the CNN as well as latency in relation to the implementation's hyperparameters. Finally, we show that the proposed design implements a high-level pipeline between the different network layers, and as a result, we can improve the latency to process an image by feeding the CNN with batches of multiple images.	algorithm;computer memory;convolutional neural network;dataflow;deep learning;degree of parallelism;energy (psychological);field-programmable gate array;high- and low-level;image scaling;machine learning;mathematical optimization;memory bandwidth;parallel computing;pipeline (computing);reconfigurability;scalability;throughput	Giuseppe Natale;Marco Bacis;Marco D. Santambrogio	2017	2017 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)	10.1109/ISVLSI.2017.126	convolutional neural network;resource consumption;scalability;parallel computing;field-programmable gate array;latency (engineering);deep learning;real-time computing;degree of parallelism;dataflow;computer science;artificial intelligence	Arch	2.811291412995663	43.83125529159189	56879
f383bfc04f4796a7ff522e4a05c34e4546841ef9	localization method for low-power wireless sensor networks	centroid;localization;rssi;wsn;ami;fuzzy system	Context awareness is an important issue in ambient intelligence to anticipate the desire of the user and, in consequence, to adapt the system. In context awareness, localization is very important to enable a responsive environment for the users. Focusing on this issue, this paper presents a localization system based on the use of Wireless Sensor Networks devices. In contrast to a traditional RFID, these devices offer the possibility of a collaborative sensing and processing of environmental information. The proposed system is a range-free localization algorithm that uses fuzzy inference to process the RSSI measurement and to estimate the position of mobile devices. The main goal of the algorithm is to reduce the power consumption and the cost of the devices, especially for the mobiles ones, maintaining the accuracy of the inferred position.	algorithm;ambient intelligence;context awareness;fuzzy logic;internationalization and localization;low-power broadcasting;mobile device;radio-frequency identification	Diego Francisco Larios Marín;Julio Barbancho;Francisco Javier Molina;Carlos León	2013	JNW	10.4304/jnw.8.1.45-58	embedded system;internationalization and localization;centroid;computer science;computer security;fuzzy control system	Mobile	-0.07042465561903151	32.735520663870226	56966
aa8502f2d78bbf9df9c30c454b3b0f911756f9f0	efficient parallel modular exponentiation algorithm	public key cryptography;algoritmo paralelo;cryptographie cle publique;parallel algorithm;algorithm complexity;complejidad algoritmo;algorithme parallele;complexite algorithme;modular exponentiation	Modular exponentiation is fundamental to several public-key cryptography systems such as the RSA encryption system, as well as the most dominant part of the computation performed. The operation is time consuming for large operands. This paper analyses and compares the complexity of a variety of algorithms proposed to compute the modular exponentiation of a relatively large binary number, and proposes a new parallel modular exponentiation method.	algorithm;binary number;computation;encryption;modular exponentiation;operand;public-key cryptography;rsa (cryptosystem)	Nadia Nedjah;Luiza de Macedo Mourelle	2002		10.1007/3-540-36077-8_43	parallel computing;computer science;theoretical computer science;mathematics;parallel algorithm;public-key cryptography;algorithm;modular exponentiation	Crypto	8.95109114912216	43.606426452207536	57054
0543e354bd38deb4bc11ab565c60ea1e2f071492	efficient backprojection-based synthetic aperture radar computation with many-core processors	efficient backprojection-based synthetic aperture;synthetic aperture radar;power aware computing;radar computation;software architecture support;computation cost reduction;strength reduction approximation;backprojection-based synthetic aperture radar computation;software locality optimization;advanced architecture;xeon phi;energy conservation;algorithmic innovation;parallelism exploitation;computation cost;approximate strength reduction;code transformation technique;multiprocessing systems;image reconstruction;single node;software architecture;many-core processor;intel xeon phi coprocessors;advanced architecture support;coprocessors;billion backprojections;energy-efficient computing;image reconstruction method;data movement cost;medical imaging;radar imaging;intel xeon processor e5-2670-based cluster;gpgpu;gpu computing	Tackling computationally challenging problems with high efficiency often requires the combination of algorithmic innovation, advanced architecture, and thorough exploitation of parallelism. We demonstrate this synergy through synthetic aperture radar (SAR) via backprojection, an image reconstruction method that can require hundreds of TFLOPS. Computation cost is significantly reduced by our new algorithm of approximate strength reduction; data movement cost is economized by software locality optimizations facilitated by advanced architecture support; parallelism is fully harnessed in various patterns and granularities. We deliver over 35 billion backprojections per second throughput per compute node on an Intel® Xeon® E5-2670-based cluster, equipped with Intel® Xeon Phi#8482; coprocessors. This corresponds to processing a 3K x 3K image within a second using a single node. Our study can be extended to other settings: backprojection is applicable elsewhere including medical imaging, approximate strength reduction is a general code transformation technique, and many-core processors are emerging as a solution to energy-efficient computing.	aperture (software);approximation algorithm;central processing unit;computation;coprocessor;flops;iterative reconstruction;locality of reference;manycore processor;medical imaging;parallel computing;strength reduction;synergy;synthetic data;synthetic intelligence;throughput	Jongsoo Park;Ping Tak Peter Tang;Mikhail Smelyanskiy;Daehyun Kim;Thomas Benson	2012	2012 International Conference for High Performance Computing, Networking, Storage and Analysis	10.3233/SPR-130372	computer architecture;parallel computing;computer hardware;computer science;operating system;general-purpose computing on graphics processing units	HPC	-1.7387409527080813	44.59445583222023	57094
99ef1e68ac0182bb9b26a812a3de554020192ad7	approximate inverse preconditioning in the parallel solution of sparse eigenproblems		A preconditioned scheme for solving sparse symmetric eigenproblems is proposed. The solution strategy relies upon the DACG algorithm, which is a Preconditioned Conjug ate Gradient algorithm for minimizing the Rayleigh Quotient. A comparison with the well established ARPACK code, shows that when a small number of the leftmost eigenpairs is to be c omputed, DACG is more efficient than ARPACK. Effective convergence accelera tion of DACG is shown to be performed by a suitable approximate inverse preconditioner (AINV). The per formance of such a preconditioner is shown to be safe, i.e. not highly dependent on a drop tolerance parameter. On sequential machines, AINV preconditioning proves a practicabl e alternative to the effective incomplete Cholesky factorization, and is more efficient tha Block Jacobi. Due to its parallelizability, the AINV preconditioner is exploited for a parall el implementation of the DACG algorithm. Numerical tests account for the high degree of paralleliza tion ttainable on a Cray T3E machine and confirm the satisfactory scalability properties of t he algorithm. A final comparison with PARPACK shows the (relative) higher efficiency of AI NV-DACG.	arpack;approximation algorithm;central processing unit;cholesky decomposition;condition number;cray t3e;freedos;gradient;incomplete cholesky factorization;jacobi method;lanczos algorithm;numerical method;nv network;parallel algorithm;parallel computing;parallel programming model;preconditioner;rayleigh–ritz method;requirement;scalability;series acceleration;sparse matrix;speedup;supercomputer;whole earth 'lectronic link	Luca Bergamaschi;Giorgio Pini;Flavio Sartoretto	2000	Numerical Lin. Alg. with Applic.	10.1002/(SICI)1099-1506(200004/05)7:3%3C99::AID-NLA188%3E3.0.CO;2-5	mathematical optimization;combinatorics;theoretical computer science;mathematics;parallel algorithm	HPC	-2.4810193628931354	38.24068332034879	57434
ff110a229e0b2ee425370ec4c36b596f205fd549	reconfigurable accelerators for combinatorial problems	hardware computer architecture software prototyping prototypes field programmable gate arrays boolean functions logic parallel processing combinational circuits clocks;boolean constraint propagation;mathematics computing;performance evaluation;reconfigurable architectures;computability;combinatorial problems;special purpose computers;parallel architectures;normal form;special purpose computers reconfigurable architectures computability combinatorial mathematics parallel architectures mathematics computing parallel algorithms performance evaluation;execution time reconfigurable accelerators combinatorial problems process time fine grained parallelism logical operations simultaneous evaluation instance specific reconfiguration hardware circuit boolean satisfiability sat problem speedup;combinatorial mathematics;parallel algorithms	Reconfigurable accelerators can improve process time on combinatorial problems with fine-grained parallelism. Such problems contain a huge number of logical operations (NOT, AND and OR) that can evaluate simultaneously, a characteristic that varies considerably from problem to problem. Because of this variability, such combinatorial problems are approached using instance-specific reconfiguration-hardware tailored to a specific algorithm and a specific set of input data. Boolean satisfiability (SAT for short) is a common combinatorial problem that exhibits fine-grained parallelism. SAT varies considerably based on the situation. Its solution is thus an ideal candidate for improvements based on instance-specific reconfiguration. In fact, simulation of an instance-specific accelerator show potential speed-ups by a factor of up to 140,000 in execution time over the solution by a software solver. The authors detail the results of their prototype that results an order-of-magnitude speed-up in the execution of difficult satisfiability problems.		Marco Platzner	2000	IEEE Computer	10.1109/2.839322	optimization problem;parallel computing;computer science;theoretical computer science;operating system;parallel algorithm;computability;programming language;algorithm	Robotics	2.1908474031457614	40.72079089433546	57453
69929295c2a12d7c03fb9c31050f35b87ad17790	hardware design and analysisof block cipher components	block ciphering;algoritmo paralelo;complejidad espacio;parallel algorithm;encryption;time complexity;cryptanalyse;cifrado;reseau permutation substitution;algorithme parallele;cryptanalysis;criptoanalisis;complexite temps;complexity measure;cryptage;chiffrement bloc;mesure complexite;space complexity;cifrado en bloque;complexite espace;complejidad tiempo;medida complexidad	This paper describes the efficient implementation of Maximum Distance Separable (MDS) mappings and Substitution-boxes (S-boxes) in gate-level hardware for application to Substitution-Permutation Network (SPN) block cipher design. Different implementations of parameterized MDS mappings and S-boxes are evaluated using gate count as the space complexity measure and gate levels traversed as the time complexity measure. On this basis, a method to optimize MDS codes for hardware is introduced by considering the complexity analysis of bit parallel multipliers. We also provide a general architecture to implement any invertible S-box which has low space and time complexities. As an example, two efficient implementations of Rijndael, the Advanced Encryption Standard (AES), are considered to examine the different tradeoffs between speed and time.	and gate;analysis of algorithms;block cipher;blum axioms;circuit minimization for boolean functions;code;dspace;encryption;gate count;mds matrix;mathematical optimization;s-box;substitution-permutation network;time complexity	Lu Xiao;Howard M. Heys	2002		10.1007/3-540-36552-4_12	time complexity;cryptanalysis;discrete mathematics;computer science;theoretical computer science;mathematics;parallel algorithm;dspace;encryption;algorithm	Crypto	8.695199500350588	43.27409500323194	57465
36d692449a288c03fceebb25e06d6b86001be18c	balancing maximal independent sets using hadoop	parallel computing;balanced mis;hadoop	Given a graph G = (V, E), an independent set I is a subset of vertices, such that no two vertex in I share a common edge. A maximal independent set (MIS) is an independent set such that there is no way we can add any other vertex such that the property of independent set remains intact. Different MIS heuristics are used in parallel applications for identifying the subset of independent tasks. Traditional MIS heuristics aim to generate minimum number of MIS i.e. number of parallel steps for the particular applications. However, if MIS produced have skewed sizes, hardware resource utilization becomes inefficient. Parallel computing has become a key feature for improving performance in these days. A task is broken down into various subtasks and each sub-task is executed in parallel on various processors. This is where generating and balancing the maximal independent sets come into play. In this paper, in the context of parallel application, we have proposed a heuristic for balancing the maximal independent sets using Hadoop after dividing the graph into various maximal independent sets. We have verified through experiment that our heuristic balances MIS' without much increasing the number of MIS generated.	algorithm;apache hadoop;central processing unit;expect;graph (discrete mathematics);heuristic (computer science);independent set (graph theory);maximal independent set;maximal set;parallel computing;time complexity;vertex (graph theory)	Ashish Bhuker;Rajiv Misra;Bhanu Pratap Singh	2017		10.1145/3007748.3018282	real-time computing;computer science;theoretical computer science;distributed computing;maximal independent set	DB	4.103918968675314	36.14741336673037	57518
16dcc0a66a6a9424e344e97c793709e682113dbd	co-gps: energy efficient gps sensing with cloud offloading	sensors;earth;satellites global positioning system receivers mobile computing earth sensors synchronization;location;receivers;coarse time navigation location assisted gps cloud offloading;global positioning system;synchronization;cloud offloading;satellites;duty cycle co gps energy efficient gps sensing mobile computing gps receivers cloud offloaded gps solution raw gps signal post processing gnss satellite ephemeris earth elevation database cloud service portable sensing device platform cleon;coarse time navigation;telecommunication power management cloud computing energy conservation global positioning system mobile computing sensors signal processing telecommunication computing;mobile computing;assisted gps	Location is a fundamental service for mobile computing. Typical GPS receivers, although widely available for navigation purposes, may consume too much energy to be useful for many applications. Observing that in many sensing scenarios, the location information can be post-processed when the data is uploaded to a server, we design a cloud-offloaded GPS (CO-GPS) solution that allows a sensing device to aggressively duty-cycle its GPS receiver and log just enough raw GPS signal for post-processing. Leveraging publicly available information such as GNSS satellite ephemeris and an Earth elevation database, a cloud service can derive good quality GPS locations from a few milliseconds of raw data. Using our design of a portable sensing device platform called CLEON, we evaluate the accuracy and efficiency of the solution. Compared to more than 30 seconds of heavy signal processing on standalone GPS receivers, we can achieve three orders of magnitude lower energy consumption per location tagging.	assisted gps;cloud computing;duty cycle;gps signals;global positioning system;mobile computing;satellite navigation;server (computing);signal processing;video post-processing	Jie Liu;Bodhi Priyantha;Ted Hart;Yuzhe Jin;Woo Suk Lee;Vijay Raghunathan;Heitor S. Ramos;Qiang Wang	2016	IEEE Transactions on Mobile Computing	10.1109/TMC.2015.2446461	embedded system;synchronization;s-gps;secure user plane location;satellite navigation;time to first fix;global positioning system;computer science;precision lightweight gps receiver;sensor;operating system;gps disciplined oscillator;u-tdoa;assisted gps;mobile phone tracking;gnss applications;earth;location;mobile computing;gps tracking server;satellite	Mobile	0.9493765504951193	34.464450978697315	57765
64879ae2307c83b3b6b3d8323d1423fcee84dd2f	cnndroid: gpu-accelerated execution of trained deep convolutional neural networks on android	low energy consumption;android;deep convolutional neural network cnn;renderscript;performance optimization;mobile gpu;open source software	Many mobile applications running on smartphones and wearable devices would potentially benefit from the accuracy and scalability of deep CNN-based machine learning algorithms. However, performance and energy consumption limitations make the execution of such computationally intensive algorithms on mobile devices prohibitive. We present a GPU-accelerated library, dubbed CNNdroid [1], for execution of trained deep CNNs on Android-based mobile devices. Empirical evaluations show that CNNdroid achieves up to 60X speedup and 130X energy saving on current mobile devices. The CNNdroid open source library is available for download at https://github.com/ENCP/CNNdroid	algorithm;android;convolutional neural network;download;graphics processing unit;machine learning;mobile app;mobile device;open-source software;scalability;smartphone;speedup;wearable technology	Seyyed Salar Latifi Oskouei;Hossein Golestani;Matin Hashemi;Soheil Ghiasi	2016		10.1145/2964284.2973801	embedded system;parallel computing;real-time computing;computer science;operating system;android	Mobile	2.086426043382298	42.94161288485727	57888
4a01e1c908eb97a498f3fd9d8a6b8272f82e06b3	pose: design of hardware-friendly particle-based observation selection phd filter	hardware friendly particle based observation selection phd filter multitarget tracking problem field programmable gate array hardware circuit xilinx virtex ii pro fpga platform hardware design issues circuit budget approximation error pose phd filter algorithm hardware implementation difficulty computational complexity probability hypothesis density filter;systems;target tracking computational complexity field programmable gate arrays particle filtering numerical methods;real time performance field programmable gate array fpga platform hardware design multitarget tracking mtt particle probability hypothesis density phd filter;networks;surveillance;multitarget tracking mtt;particle probability hypothesis density phd filter;journal;multiple target tracking;efficient;algorithm;computational complexity;期刊论文;hardware design;field programmable gate array fpga platform;field programmable gate arrays;target tracking;architecture;article;particle filtering numerical methods;real time performance	Particle probability hypothesis density (PHD) filtering is a promising technology for the multitarget-tracking problem. Traditional particle PHD filter solutions usually have high computational complexity, and the lack of dedicated hardware has seriously limited their usages in real-time industrial applications. The hardware implementation difficulty of the particle PHD filtering in field-programmable gate array (FPGA) platforms lies in that the number of observations for filtering is time varying while the number of parallel processing units in circuit is fixed. To overcome this challenge, we propose a novel particle-based observation selection (POSE) PHD filter algorithm and its hardware implementation in this paper. Specifically, we opportunistically select a fixed number of observations out of a varying number of observations for filtering, where the approximation error is proved to be negligible by adapting the circuit budget to the environment accordingly. To implement the proposed POSE PHD filter, the hardware design issues are addressed in depth. Extensive simulations demonstrate that the POSE PHD filter has a comparable performance with the traditional one while its hardware implementation challenge is overcome. The hardware experiment results of the POSE PHD filter on a Xilinx Virtex-II Pro FPGA platform match the simulation ones well. Furthermore, the execution time of the implemented hardware circuit is evaluated, and the results show that it can achieve a processing rate of 6.892 kHz with a 50-MHz system clock.	approximation error;computational complexity theory;electronic circuit;field-programmability;field-programmable gate array;parallel computing;peterson's algorithm;real-time computing;real-time transcription;run time (program lifecycle phase);simulation;virtex (fpga)	Zhiguo Shi;Yongkang Liu;Shaohua Hong;Jiming Chen;Xuemin Shen	2014	IEEE Transactions on Industrial Electronics	10.1109/TIE.2013.2262753	electronic engineering;real-time computing;simulation;computer science;engineering;architecture	EDA	8.715852846948055	41.03472023368595	58564
1ad17be079e6d31585478dc819fdd49dd14e33dd	a robust gf(p) parallel arithmetic unit for public key cryptography	field programmable gate arrays;public key cryptography;modular multiplication;modular division;elliptic curve cryptography;timing attack	This paper presents the architecture and FPGA implementation of a robust GF(p) parallel arithmetic unit. The most efficient modular multiplication, inversion and division units greatly reduce the clock cycles requirement for point operations applicable to elliptic curve cryptography. The parallel arithmetic unit helps to achieve a high speed up in cryptographic applications. The architecture can resist the cryptographic timing attack. Integrated input and output interface units provide lower bandwidth requirement to plug in the architecture with automated cryptographic systems. The design exhibits its elegance among competitive architecture with respect to throughput and robustness.	arithmetic logic unit;binary number;bit-length;clock signal;cryptographic hash function;ecc memory;elliptic curve cryptography;field-programmable gate array;grammatical framework;input/output;period-doubling bifurcation;public-key cryptography;robustness (computer science);speedup;throughput;virtex (fpga);warez	Santosh Ghosh;Monjur Alam;Indranil Sengupta;Dipanwita Roy Chowdhury	2007	10th Euromicro Conference on Digital System Design Architectures, Methods and Tools (DSD 2007)	10.1109/DSD.2007.4341457	modular arithmetic;parallel computing;computer science;theoretical computer science;distributed computing;algorithm	EDA	9.054234431041172	44.55488898787953	58700
22cf3ead61485597ee51863ea041aef9218343d5	methodology for refinement and optimisation of dynamic memory management for embedded systems in multimedia applications	storage allocation;optimisation;dynamic memory management;memory management;multimedia;3d imaging;dynamic data types;multimedia applications;storage management;real time;energy dissipation;multimedia application;multimedia systems;embedded system;memory access;embedded systems;dynamic data;imaging system;low power;digital storage multimedia systems storage management optimisation embedded systems image reconstruction storage allocation;data structures;image reconstruction;storage capacity;optimization methods memory management embedded system multimedia systems power system management runtime information retrieval bandwidth image reconstruction energy dissipation;refinement methodology;static memory dynamic memory management embedded systems multimedia applications run time memory management data storage capacity memory bandwidth 3d image reconstruction system memory accesses normalized memory use energy estimation memory footprint;system level exploration;memory hierarchy;digital storage;memory footprint;memory bandwidth;high speed;energy estimate;3d reconstruction;large data;type system	In multimedia applications, run-time memory management support has to allow real-time memory de/allocation, retrieving and processing of data. Thus, its implementation must be designed to combine high speed, low power, large data storage capacity and a high memory bandwidth. In this paper, we assess the performance of our new system-level exploration methodology to optimise the memory management of typical multimedia applications in an extensively used 3D reconstruction image system [22, 8]. This methodology is based on an analysis of the number of memory accesses, normalised memory footprint 1 and energy estimations for the system studied. This results in an improvement of normalised memory footprint up to 44.2% and the estimated energy dissipation up to 22.6% over conventional static memory implementations in an optimised version of the driver application. Finally, our final version is able to scale perfectly the memory consumed in the system for a wide range of input parameters whereas the statically optimised version is unable to do this. ∗ The original version of this paper first appeared in the Proceedings of Signal Processing Systems 2003 1 The sum of the memory used at a time slice, multiplied by the time. This amount is then divided over one run of the algorithm c © 2004Kluwer Academic Publishers. Printed in the Netherlands. journalvlsi2003.tex; 15/01/2004; 8:59; p.1 2 Marc Leeman, David Atienza et al.	3d reconstruction;algorithm;computer data storage;embedded system;high memory;mathematical optimization;memory bandwidth;memory footprint;memory management;preemption (computing);real-time transcription;signal processing	Marc Leeman;David Atienza;Geert Deconinck;Vincenzo De Florio;Jose Manuel Mendias;Chantal Ykman-Couvreur;Francky Catthoor;Rudy Lauwereins	2005	VLSI Signal Processing	10.1007/s11265-005-5272-4	3d reconstruction;iterative reconstruction;uniform memory access;memory footprint;stereoscopy;embedded system;interleaved memory;parallel computing;real-time computing;dynamic data;type system;distributed memory;data structure;computer science;dissipation;operating system;static memory allocation;overlay;extended memory;flat memory model;programming language;memory bandwidth;computing with memory;memory management	HPC	2.6455587111908887	46.376657938019136	58945
2052249c502599e519425f002bd39b9a67999747	parameter-free tree style pipeline in asynchronous parallel game-tree search		Asynchronous parallel game-tree search methods are effective in improving playing strength by using many computers connected through relatively slow networks. In game position parallelization, the master program manages a game-tree and distributes positions in the tree to workers. Then, each worker asynchronously searches the best move and evaluation for its assigned position. We present a new method for constructing an appropriate master tree that provides more important moves with more workers on their sub-trees to improve playing strength. Our contribution introduces two advantages: (1) being parameter free in that users do not need to tune parameters through trial and error, and (2) efficiency suitable even for short-time matches, such as one second per move. We implemented our method in chess with a top-level chess program (Stockfish) and evaluated playing strength through self-plays. We confirmed that playing strength improves with up to sixty workers.	chess engine;computer;graph partition;monte carlo tree search;parallel computing;scalability;stockfish;tree traversal	Shu Yokoyama;Tomoyuki Kaneko;Tetsuro Tanaka	2015		10.1007/978-3-319-27992-3_19	parallel computing;theoretical computer science;distributed computing;pipeline	ML	0.2016554222599789	40.82354800626753	59007
ff123563273b30e41038e7c511e386763b334b9d	degree of scalability: scalable reconfigurable mesh algorithms for multiple addition and matrix-vector multiplication	parallel algorithm;reconfigurable mesh;arithmetic algorithms;parallel models;reconfigurable models;floating point;scalability;parallel algorithms	The usual concern when scaling an algorithm on a parallel model of computation is preserving efficiency while increasing or decreasing the number of processors. Many algorithms for reconfigurable models, however, attain constant time at the expense of an inefficient algorithm. For these algorithms, scaling down the number of processors while preserving inefficiency is no benefit once constant time execution is lost. In fact, one can often accelerate the efficiency of these algorithms while reducing the number of processors. To quantify this improvement in efficiency, this paper introduces the measure of degree of scalability to complement the insight obtained from efficiency for such algorithms. Demonstrating the utility of this measure, we present new reconfigurable mesh (R-Mesh) algorithms for multiple addition and matrix-vector multiplication, improving both the number of processors and the degree of scalability compared to previous algorithms. We also extend these results to floating point number operands, which have previously received little attention on the R-Mesh.	algorithm;matrix multiplication;scalability	Ramachandran Vaidyanathan;Jerry L. Trahan;Chun-ming Lu	2003	Parallel Computing	10.1016/S0167-8191(02)00164-3	parallel computing;computer science;theoretical computer science;analysis of parallel algorithms;distributed computing;parallel algorithm	HPC	-0.5018206433180391	38.80137053751163	59237
d3865ee28853fae20533b7b408cffa47e8b73b55	parallel bees swarm optimization for association rules mining using gpu architecture		This paper addresses the problem of association rules mining with large scale data sets using bees behaviors. The bees swarm optimization method have been successfully running on small and medium data size. Nevertheless, when dealing with large benchmark, it is bluntly blocked. Additionally, graphic processor units are massively threaded providing highly intensive computing and very usable by the optimization research community. The parallelization of such method on GPU architecture can be deal large data sets as the case of WebDocs in real time. In this paper, the evaluation process of the solutions is parallelized. Experimental results reveal that the suggested method outperforms the sequential version at the order of ×70 in most data sets, furthermore, the WebDocs benchmark is handled with less than forty hours.	graphics processing unit;swarm	Youcef Djenouri;Habiba Drias	2014		10.1007/978-3-319-11897-0_7	artificial intelligence;theoretical computer science;machine learning;bees algorithm	DB	-2.0711292011206544	43.34271605119185	59328
d0b599d8b2a56163054e940fae0e536fd89ff028	cluster computers and grid processing in the first radio-telescope of a new generation	distributed application;astronomy computing radiotelescopes workstation clusters distributed processing image processing digital signal processing chips programmable logic arrays general purpose computers microcomputers antenna arrays calibration;cluster computing;radiotelescopes;600 tbyte cluster computers grid processing lofar radio telescope low frequency array telescope data processing machine antennas data stream astronomical images data products distributed applications heterogeneous system digital signal processors programmable logic general purpose microcomputers processing power iterative calibration online storage 2 gbit s 40 tflops;image processing;heterogeneous systems;antenna arrays;low frequency;data stream;distributed processing;data processing;programmable logic arrays;astronomy computing;general purpose computers;digital signal processor;digital signal processing chips;workstation clusters;grid computing telescopes data processing streaming media distributed computing image converters application software process design signal design digital signal processors;programmable logic;calibration;radio telescope;microcomputers	The LOFAR telescope is being developed as a giant dataprocessing machine. A total of 13,000 antennas produce a datastream of 2 Gbit/sec each. This data is processed by a massive cluster computer and converted to astronomical images and other data-products using distributed applications. The LOFAR Data Processor will be designed as a heterogeneous system containing digital signal processors, programmable logic and general purpose microcomputers with a total processing power of 40 TFlops. To allow for iterative calibration over 600 TByte on-line storage is envisaged. LOFAR will be developed in a collaboration between the Netherlands (ASTRON) and the USA (NRL and MIT Haystack Observatory). Initial funding has been secured for a feasibility phase. Initial operations are planned for 2004, full capacity science operations for 2007.	byte;central processing unit;computer cluster;digital signal processor;distributed computing;flops;gigabit;iterative method;microcomputer;online and offline	C. M. de Vos;K. Van der Schaaf;J. D. Bregman	2001		10.1109/CCGRID.2001.923188	embedded system;digital signal processor;radio telescope;calibration;data processing;computer hardware;image processing;computer cluster;computer science;operating system;programmable logic device;microcomputer;distributed computing;low frequency	HPC	3.288069422818831	45.283792845576706	59380
4dd6da40354e506df459585f35501bbf70b84f40	an easy and efficient method for synthesizing two-dimensional finite impulse response filters with improved selectivity [tips & tricks]		It is hard to imagine what the world would look like without the modern technologies using digital signal processing. The developments in this technical field provide an opportunity for building technical devices that implement mathematical methods unattainable by analog technology. Many modern technical devices work with two-dimensional (2-D) signals in a process called digital image processing, and 2-D digital finite impulse response (FIR) filters are basic technical tools in image processing. FIR filters are extensively used in digital television, radio astronomy, radio location, biomedicine, and so on.	digital image processing;digital signal processing;finite impulse response;selectivity (electronic)	Peter S. Apostolov;Borislav Yurukov;Alexey K. Stefanov	2017	IEEE Signal Processing Magazine	10.1109/MSP.2017.2717498	computer science;image processing;multidimensional signal processing;theoretical computer science;digital image processing;digital signal processing;signal processing;finite impulse response;digital television;digital filter	Graphics	7.935371057869557	40.78981156694566	59630
8459f25634ddb54846eb5219f6beb9de613d0eb5	sic - a system for stochastic simulation in c++			c++;simplified instructional computer;simulation	Bernd Kluth	1990			mathematical optimization;stochastic simulation;mathematics	Robotics	8.001047830368538	33.008696685249426	59661
871f5112ca61767f96ad0c5b2e582a5a421aa3f7	multi-dimensional montgomery ladders for elliptic curves		Montgomery’s ladder algorithm for elliptic curve scalar multiplication uses only the xcoordinates of points. Avoiding calculation of the y-coordinates saves time for certain curves. Montgomery introduced his method to accelerate Lenstra’s elliptic curve method for integer factoring. Bernstein extended Montgomery’s ladder algorithm by computing integer combinations of two points, thus accelerating signature verification over certain curves. This paper modifies and extends Bernstein’s algorithm to integer combinations of two or more points.	algorithm;arjen lenstra;integer factorization;lenstra elliptic curve factorization;montgomery modular multiplication;phil bernstein	Daniel R. L. Brown	2006	IACR Cryptology ePrint Archive		discrete mathematics;lenstra elliptic curve factorization;elliptic curve digital signature algorithm;theoretical computer science;mathematics;elliptic curve cryptography;elliptic curve point multiplication;algorithm;schoof's algorithm	Crypto	9.234901245288924	42.6710058996183	59783
c100bcb676058859e29384b3182d57e943ed7d36	accelerating dna sequence analysis using intel(r) xeon phi(tm)		Genetic information is increasing exponentially, doubling every 18 months. Analyzing this information within a reasonable amount of time requires parallel computing resources. While considerable research has addressed DNA analysis using GPUs, so far not much attention has been paid to the Intel Xeon Phi coprocessor. In this paper we present an algorithm for large-scale DNA analysis that exploits thread-level and the SIMD parallelism of the Intel Xeon Phi. We evaluate our approach for various numbers of cores and thread allocation affinities in the context of real-world DNA sequences of mouse, cat, dog, chicken, human and turkey. The experimental results on Intel Xeon Phi show speed-ups of up to 10× compared to a sequential implementation running on an Intel Xeon processor E5.	automata theory;coprocessor;dna barcoding;experiment;finite-state machine;graphics processing unit;k-mer;knights;parallel algorithm;parallel computing;period-doubling bifurcation;simd;scalability;sequence analysis;xeon phi	Suejb Memeti;Sabri Pllana	2015	2015 IEEE Trustcom/BigDataSE/ISPA	10.1109/Trustcom.2015.636	computer architecture;parallel computing;performance;computer science;x86;operating system;xeon phi;hyper-threading;pentium;mmx	HPC	-0.47317291921663196	42.10970706945714	59934
07d47fbf51efccdd759190df9036c47e177e48c6	improved des on heterogeneous multi-core architecture		DES (Data Encryption Standard) is one of the most classical algorithms of cryptography and its higher security makes it hard to be broke for a very long time. However, along with the constant development of computer technology, especially in the 21st century, DES cannot be applied widely because of its low efficiency. Recently, the novel heterogeneous multi-core architecture represented by APU (Accelerated Processing Unit), provides a new solution for the above problems. APU integrates CPU and GPU in a groundbreaking manner and makes the algorithm to make full use of the performance advantage of heterogeneous multi-core system by realizing the HSA (Heterogeneous System Architecture) standard. This paper realizes DES on the fresh APU processor. By analyzing the performance, two kinds of improved schemes are proposed. The experimental results show that the running efficiency of algorithm can be greatly improved by using APU with reasonable optimization. In the same way, the other DES-like algorithm would also be optimized on these heterogeneous multi-core architecture.	intel core (microarchitecture);multi-core processor	Zhenshan Bao;Chong Chen;Wenbo Zhang	2018		10.1007/978-981-13-2203-7_34	architecture;heterogeneous system architecture;symmetric multiprocessor system;parallel computing;cryptography;encryption;multi-core processor;accelerated processing unit;computer science	Arch	7.392640184674124	44.681291314254636	60062
2a4e4796d4cd2696ed26baea28b3b3671808bc6d	quality of life context influence factors improvement using houseplants and internet of things	temperature sensors;monitoring;humidity;temperature measurement;context	Modern research directions in smart environment include analysis of factors that influence overall human Quality of Life (QoL). Context influence factors (CIF) are unavoidable within smart environment research. In this paper, we address indoor air quality (IAQ) as one of the main CIF. The major contaminants of indoor air are CO2 and volatile organic compounds (VOCs). There are still no satisfactory methods for air pollutants monitoring and removal. The ability of plants to detoxify these compounds makes biological systems the promising technologies for this purpose. Although precise mechanisms how plants remove VOC are unknown and current knowledge is based on experiments conducted under controlled conditions, it does not prevent us not to use biological air purifiers. Our measurements under uncontrolled conditions in environments where people live, showed that houseplants' ability to reduce VOC differs from one to another environment. Instead of attempt to design the unique botanical system for air purification, we proposed internet of things (IoT) based methodology for continuous monitoring and objective assessment of IAQ aiming to improve the designing and adaptation of botanical purifiers to specific environment, user's health, living habits and requirements.	biological system;experiment;internet of things;non-volatile memory;purification of quantum state;requirement;server message block;smart environment;uncontrolled format string	Milos Ljubojevic;Mitar Simic;Zdenka Babic;Martina Zoric	2016	2016 IEEE International Black Sea Conference on Communications and Networking (BlackSeaCom)	10.1109/BlackSeaCom.2016.7901574	simulation;environmental engineering;engineering	Robotics	3.877844754431308	33.081888176781874	60187
d1c9476855a53bb90f6a1036820ee08a3d8ca038	mapping a class of run-time dependencies onto regular arrays	time dependent;systolic arrays parallel algorithms;run time dependencies;processor scheduling;very large scale integration;systolic arrays;systolic array;runtime timing computer architecture very large scale integration difference equations processor scheduling control system synthesis production systolic arrays solid modeling;algorithmic engineering;regular arrays;runtime;computer architecture;knapsack problem;difference equations;algorithm engineering;control system synthesis;regular computations;solid modeling;production;massively parallel;mapping theorem;vlsi processor arrays;systolic array run time dependencies regular arrays regular computations algorithmic engineering massively parallel vlsi processor arrays mapping theorem uniform recurrences knapsack problem;uniform recurrences;parallel algorithms;timing	The production of regular computations using algorithmic engineering techniques is beginning to play an important role in the synthesis of massively parallel and VLSI processor arrays. The author widens the class of algorithms that can be formally synthesized by introducing a mapping theorem for a class of algorithms with run-time dependencies. The technique is illustrated by deriving uniform recurrences for the so-called knapsack problem, the resulting systolic array is known to be optimal. >		Graham M. Megson	1993		10.1109/IPPS.1993.262863	parallel computing;computer science;theoretical computer science;distributed computing	HPC	4.194555278951314	38.42867776989328	60625
2789e3a861bec62df94dcf231216a120c6a2cad0	a time-parallel multigrid-extrapolation method for parabolic partial differential equations	equation derivee partielle;tratamiento paralelo;algoritmo paralelo;partial differential equation;ecuacion derivada parcial;extrapolateur;parallel algorithm;traitement parallele;multiprocessor;parabolic equation;parabolic partial differential equation;multigrille;ecuacion parabolica;algorithme parallele;extrapolador;equation parabolique;calculateur mimd;methode extrapolation;multigrid;extrapolator;multigrilla;methode temps parallele;systeme parallele;parallel system;multiprocesador;parallel processing;sistema paralelo;mimd computer;multiprocesseur	Abstract   We consider the problem of solving unsteady partial differential equations on an MIMD machine. Conventional parallel methods use a data partitioning type approach in which the solution grid at each time-step is divided amongst the available processors. The sequential nature of the time integration is, however, retained. The algorithm presented in this paper makes use of a time-parallel approach, whreby several processors may be employed to solve at several time-steps simultaneously. The time-parallel method enables the inherent parallelism of the extrapolation scheme to be efficiently exploited, allowing a significant increase both in accuracy and in the degree of parallelism. The efficiencies obtained by an implementation on a message-passing multiprocessor demonstrate the suitability of the time-parallel extrapolation method for this type of equation.	extrapolation;multigrid method;parabolic antenna	Graham Horton;Ralf Knirsch	1992	Parallel Computing	10.1016/0167-8191(92)90108-J	parallel processing;mathematical optimization;parallel computing;multiprocessing;parabola;computer science;parabolic partial differential equation;mathematics;parallel algorithm;partial differential equation;algorithm;multigrid method	HPC	-3.231766767448114	36.75578773047594	60849
c426c31b335e8359a9702a3bae64cff01e4fb116	efficient parabolic solvers scalable across multi-architectural levels	numerical stability;cache storage;partial differential equation;parallel algorithm;mathematics computing;numerical solution;resource allocation;controllability;resource allocation cache storage controllability mathematics computing numerical stability parabolic equations parallel algorithms parallel architectures;cache performance parallel algorithm numerical solution partial differential equation;parallel architectures;cache performance;equations accuracy mathematical model parallel processing numerical stability power system stability algorithm design and analysis;fast memory access multiarchitectural level scalability high end computing hardware uniprocessor performance memory access speed stable explicit implicit domain decomposition algorithm seidd algorithm numerical algorithm parabolic equation solvers parallel computers flexible controllability load balancing minimal communication cost stability parallel processing cache memories;parabolic equations;parallel algorithms	High end computing hardware has been growing fast in both uniprocessor performance and parallel system scales. Steadily advancing but somewhat lagging behind is the speed of memory accesses. Thus, needed are software and algorithms behind software that adapt well with architectural features of high end computing hardware. Stable explicit implicit domain decomposition (SEIDD) is a class of numerical algorithms originally introduced for solving parabolic equations on parallel computers, which has adequately high parallelism, flexible controllability for load balancing, minimal communication cost, and good stability and efficiency. In this paper, we study the effectiveness of SEIDD in harnessing the computing power at the inter-processor level for parallel processing as well as the level of cache memories for fast memory accesses.	algorithm;cpu cache;computer hardware;domain decomposition methods;load balancing (computing);numerical analysis;overhead (computing);parabolic antenna;parallel computing;simulation;uniprocessor system	Yu Zhuang;Heng Wu	2012	2012 IEEE 10th International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2012.23	parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm	HPC	-4.227847177255485	39.13183253321243	60894
a8bb563c714f0a4324e5dea6e0130f79548f89e5	conflict-free scheduling of nested loop algorithms on lower dimensional processor arrays	multiprocessor interconnection networks;total execution time;2 dimensional bit level processor array;conflict free scheduling;nested loops;parallel programming;nested loop algorithms;1 dimensional;5 dimensional bit level matrix multiplication algorithm;2 dimensional;2 dimensional bit level processor array conflict free scheduling nested loop algorithms lower dimensional processor arrays computational conflicts lower bound total execution time integer programming 5 dimensional bit level matrix multiplication algorithm;integer programming;computational complexity;scheduling;computational conflicts;matrix multiplication;processor scheduling scheduling algorithm vectors matrix decomposition testing convolution linear programming contracts signal processing image processing;integer program;lower bound;scheduling computational complexity integer programming multiprocessor interconnection networks parallel algorithms parallel programming;lower dimensional processor arrays;parallel algorithms	In practice, it is interesting to map n-dimensional algorithms, or algorithms with n nested loops, onto (k-1)-dimensional arrays where k u003e		Zhenhui Yang;Weijia Shang;José A. B. Fortes	1992		10.1109/IPPS.1992.223054	parallel computing;nested loop join;computer science;theoretical computer science;distributed computing	Arch	4.183261230544899	38.805789157444096	61027
13b0c5d0b844eb17d8bd28eab5fc00fca89aaaaa	a 34.7-mw quad-core miqp solver processor for robot control	real time control;real time;power 34 7 mw quad core miqp solver processor robot control quad core mixed integer quadric programming solver processor hybrid control system real time control robotics multi core architecture fixed point calculations branch and bound method high dispersion performance;hybrid control;random access memory robots field programmable gate arrays multicore processing frequency measurement calculators control systems;fixed point;hybrid control system;branch and bound method;robot control;robots fixed point arithmetic integer programming microprocessor chips;integer programming;robots;real time hybrid control miqp multi core;fixed point arithmetic;microprocessor chips	We propose a quad-core mixed integer quadric programming (MIQP) solver processor. The MIQP solver is applicable to hybrid control systems including real-time control robotics. Using multi-core architecture, fixed-point calculations, and branch-and-bound method with high-dispersion performance while processing a 50-variable problem, our design achieves 34.7-mW operation at a frequency of 52 MHz in measurement results, although a core 2 duo PC requires 3.16 GHz to solve it as rapidly.	branch and bound;control system;intel core (microarchitecture);multi-core processor;real-time clock;robot control;robotics;solver	Hiroki Noguchi;Junichi Tani;Yusuke Shimai;Masanori Nishino;Shintaro Izumi;Hiroshi Kawaguchi;Masahiko Yoshimoto	2010	IEEE Custom Integrated Circuits Conference 2010	10.1109/CICC.2010.5617467	robot;embedded system;parallel computing;real-time computing;integer programming;real-time control system;computer science;operating system;fixed point;robot control;fixed-point arithmetic	Robotics	5.329800661029327	45.00459193463637	61072
c740d39bcd99bb8c7c38a569532653c90d615bf8	session 31 overview: computation in memory for machine learning: technology directions and memory subcommittees		Many state-of-the-art systems for machine learning are limited by memory in terms of the energy they require and the performance they can achieve. This session explores how this bottleneck can be overcome by emerging architectures that perform computation inside the memory array. This necessitates unconventional, typically mixed-signal, circuits for computation, which exploit the statistical nature of machine-learning applications to achieve high algorithmic performance with substantial energy and throughput gains. Further, the architectures serve as a driver for emerging memory technologies, exploiting the high-density and nonvolatility these offer towards increased scale and efficiency of computation. The innovative papers in this session provide concrete demonstrations of this promise, by going beyond conventional architectures.	computation;machine learning	Naveen Verma;Fatih Hamzaoglu;Makoto Nagata;Leland Chang	2018		10.1109/ISSCC.2018.8310396	throughput;electronic circuit;computer science;computation;exploit;machine learning;artificial intelligence;bottleneck	Logic	4.828753426415771	41.76795696907639	61291
c8c7eb2d1670d1794ff13a9cddf3ce48e5d557ce	minimum assignment of test links for hypercubes with lower fault bounds	modelizacion;distributed system;algoritmo paralelo;eficacia sistema;hypercube;systeme reparti;parallel algorithm;automatic proving;systeme multiprocesseur memoire repartie;complexite calcul;multiprocessor systems;performance systeme;demostracion automatica;system performance;algorithme parallele;theorem proving;modelisation;demonstration automatique;demonstration theoreme;complejidad computacion;sistema repartido;computational complexity;sistema multiprocesador memoria distribuida;distributed memory multiprocessor system;demostracion teorema;modeling;hipercubo	0743-7315/97 $25.00 Copyright  1997 by Academic Press All rights of reproduction in any form reserved. 185 In an n-dimensional hypercube multiprocessor system, to correctly diagnose faulty processors among themselves, the maximum allowed number of faulty processors is n under the well-known PMC diagnostic model. When the n fault bound is adopted, all links between processors will be used in the diagnosis. However, if the fault bound is lower than n, many links can be freed from the task of performing diagnosis. In this paper, we show that each drop of the fault bound by 1 will free 2 links from diagnosis. We will present an algorithm that selects, in a symmetric manner, the to-be-freed links, so that only a minimum number of links will be used to perform diagnosis. A rigorous proof for the algorithm’s correctness is given. The freed links will never be used for the purpose of diagnosis, so that the diagnosis and some conventional computations may be carried out simultaneously, improving the performance of the system as a whole.  1997 Academic Press	algorithm;central processing unit;computation;correctness (computer science);multiprocessing;whole earth 'lectronic link	Dajin Wang;Zhongxian Wang	1997	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.1279	parallel computing;systems modeling;computer science;mathematics;distributed computing;parallel algorithm;computer performance;automated theorem proving;computational complexity theory;algorithm;hypercube	PL	9.989820630684587	33.96799363925575	61505
5c3befe5cdc010adb15e183eb724631551c6af9d	a survey and discussion of memcomputing machines		This paper serves as a review and discussion of the recent works on memcomputing. In particular, the universal memcomputing machine (UMM) and the digital memcomputing machine (DMM) are discussed. We review the memcomputing concept in the dynamical systems framework and assess the algorithms offered for computing NP problems in the UMM and DMM paradigms. We argue that the UMM is a physically implausible machine, and that the DMM model, as described by numerical simulations, is no more powerful than Turing-complete computation. We claim that the evidence for the resolution of P vs. NP is therefore inconclusive, and conclude that the memcomputing machine paradigm constitutes an energy efficient, special-purpose class of models of dynamical systems computation.	algorithm;analog computer;best, worst and average case;boolean satisfiability problem;correctness (computer science);digital molecular matter (dmm);digital electronics;dynamical system;experiment;integer factorization;limit cycle;memristor;model of computation;numerical analysis;overhead (computing);p versus np problem;plausibility structure;polynomial;programming paradigm;simulation;solver;thyristor;time complexity;turing completeness;turing machine	Daniel Saunders	2017	CoRR		theoretical computer science;dynamical systems theory;computation;computer science;machine learning;artificial intelligence	Logic	5.442119794541128	40.32703326542462	61517
136115270557470e5f7c94b2cbd9e2383e2dc186	parallelizing simplex within smt solvers	smt solving;simplex parallelization within smt;parallel smt portfolio	The usual approach in parallelizing SAT and SMT solvers is either to explore different parts of the search space in parallel (divide-and-conquer approach) or to run multiple instances of the same solver with suitably altered parameters in parallel, possibly exchanging some information during the solving process (parallel portfolio approach). Quite a different approach is to parallelize the execution of time-consuming algorithms that check for satisfiability and propagations during the search space exploration. Since most of the execution time is spent in these procedures, their efficient parallelization might be a promising research direction. In this paper we present our experience in parallelizing the simplex algorithm which is typically used in the SMT context to check the satisfiability of linear arithmetic constraints. We provide a detailed description of this approach and present experimental results that evaluate the potential of the approach compared to the parallel portfolio approach. We also consider the combination of the two approaches.	amdahl's law;arbitrary-precision arithmetic;automatic parallelization;boolean satisfiability problem;central processing unit;coefficient;experiment;long division;parallel computing;run time (program lifecycle phase);satisfiability modulo theories;simplex algorithm;simultaneous multithreading;solver	Milan Bankovic	2016	Artificial Intelligence Review	10.1007/s10462-016-9495-5	mathematical optimization;parallel computing;computer science;theoretical computer science	AI	0.9747281368130177	41.14217065454965	61617
5d17673a1657064ad654be79164697b89695dcf2	dynamic prediction of minimal trees in large-scale parallel game tree search	distributed computing;game tree search;performance modeling	Parallelization of the alpha-beta algorithm on distributed computing environments is a promising way of improving the playing strength of computer game programs. Search programs should predict and concentrate the effort on the subtrees that will not be pruned. Unlike in sequential search, when subtrees are explored in parallel, their results are obtained asynchronously. Using such information dynamically should allow better prediction of subtrees that are never pruned. We have implemented a parallel game tree search algorithm performing such dynamic updates on the prediction. Two kinds of game trees were used in performance evaluation: synthetic game trees and game trees generated by a state-of-the-art computer player of shogi (Japanese chess). On a computer cluster with 1,536 cores, dynamic updates actually show significant performance improvements, which are more apparent in game trees generated by the shogi program for which the initial prediction is less accurate. The speedup nevertheless remains sublinear. A performance model built through analyses of the results reasonably explains the results.	alpha–beta pruning;computer cluster;distributed computing;information processing;iterative deepening depth-first search;iterative method;linear search;microsoft windows;monte carlo tree search;pc game;parallel computing;performance evaluation;principal variation search;search algorithm;speedup;synthetic intelligence;time complexity;transposition table;tree (data structure);tree traversal;variation (game tree)	Akira Ura;Yoshimasa Tsuruoka;Takashi Chikayama	2015	JIP	10.2197/ipsjjip.23.9	optimal binary search tree;red–black tree;game tree;expectiminimax tree;computer science;theoretical computer science;order statistic tree;machine learning;k-d tree;interval tree;distributed computing;iterative deepening depth-first search;search tree;monte carlo tree search;ternary search tree;k-d-b-tree;tree traversal;game complexity	HPC	0.6451932905184793	40.45393540710656	61659
36947f53576fb41b6195fde7a8f92b4e6019e7e4	a study and implementation of the huffman algorithm based on condensed huffman table	decoding huffman encoding algorithm condensed canonical huffman table data compression;huffman coding decoding costs binary sequences magnetic heads computer science software engineering software algorithms laboratories computer architecture;pediatrics;image coding;new condensed huffman table;data compression;decoding;huffman coding;huffman codes data compression decoding;huffman codes;arrays;condensed canonical huffman table;binary sequences;compressing ratio;compression ratio;signal processing algorithms;huffman encoding algorithm;algorithm design and analysis;compressing ratio canonical huffman tree huffman codes new condensed huffman table;canonical huffman tree	Huffman codes are being widely used as a very efficient technique for compressing data. To achieve high compressing ratio, some properties of encoding and decoding for canonical Huffman table are discussed. A study and implementation of the Huffman algorithm based on condensed Huffman table is studied. New condensed Huffman table could reduce the cost of the Huffman coding table. Compared with traditional Huffman coding table and other improved tables, the best advantages of new condensed Huffman table is that the space requirement is reduced significantly.	algorithm;code;huffman coding	Ergude Bao;Weisheng Li;Dongrui Fan;Xiaoyu Ma	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1432	arithmetic;shannon–fano coding;canonical huffman code;computer science;theoretical computer science;modified huffman coding;tunstall coding;algorithm;deflate;statistics;huffman coding;range encoding	DB	9.741872093836458	37.707164487986056	61981
5788722a792a07383ea87dc86b8bd0f2a8708039	gpurir: a python library for room impulse response simulation with gpu acceleration		The Image Source Method (ISM) is one of the most employed techniques to calculate acoustic Room Impulse Responses (RIRs), however, its computational complexity grows fast with the reverberation time of the room and its computation time can be prohibitive for some applications where a huge number of RIRs are needed. In this paper, we present a new implementation that dramatically improves the computation speed of the ISM by using Graphic Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and the computation of the images inside each RIR. We provide a Python library under GNU license that can be easily used without any knowledge about GPU programming and we show that it is about 100 times faster than other state of the art CPU libraries.	acoustic cryptanalysis;central processing unit;computation;computational complexity theory;gnu;graphics processing unit;implicit shape model;library (computing);python;simulation;time complexity	David Diaz-Guerra;Antonio Miguel;José Ramón Beltrán	2018	CoRR		acceleration;parallel computing;impulse (physics);computational complexity theory;computation;impulse response;reverberation;general-purpose computing on graphics processing units;python (programming language);computer science	HPC	-2.4098350151137464	40.049802402310355	62019
2e7924d3cab5b3f6b050703870a9633c4bb64ab7	how well do cpu, gpu and hybrid graph processing frameworks perform?		The importance of high-performance graph processing to solve big data problems targeting high-impact applications is greater than ever before. Recent graph processing frameworks target different hardware platforms (e.g., shared memory systems, accelerators such as GPUs, and distributed systems) and differ with respect to the programming model they adopt (e.g., based on linear algebra formulations of graph algorithms or enabling direct access to the graph structure). To better understand the impact of these choices, this paper, presents a comparative study of five state-of-the-art graph processing frameworks: two CPU-only frameworks - GraphMat and Galois, two GPU-based frameworks - Nvgraph and Gunrock; and Totem, a hybrid (CPU+GPU) framework. We use three popular graph algorithms (PageRank, Single Source Shortest Path, and Breadth-First Search), and massive scale graphs with up to billions of edges. Our evaluation focuses on three performance metrics: (i) execution time, (ii) scalability and (iii) energy consumption.	algorithm;big data;breadth-first search;central processing unit;distributed computing;graph (abstract data type);graph theory;graphics processing unit;linear algebra;list of algorithms;pagerank;pathfinding;programming model;random access;run time (program lifecycle phase);scalability;shared memory;shortest path problem	Tanuj kr Aasawat;Tahsin Reza;Matei Ripeanu	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00082	parallel computing;computer science;programming paradigm;scalability;sparse matrix;pagerank;big data;shortest path problem;instruction set;shared memory	Arch	-3.3082432653024463	42.96675439479693	62171
272d11a627ee4b22b42faaa063362e60e0f35598	using state-of-the-art sparse matrix optimizations for accelerating the performance of multiphysics simulations	performance problem;state-of-the-art sparse matrix optimizations;multiphysics simulation;multiple call;byte ratio;memory hierarchy;low flop;finite element methods;large linear system;modern computer;costly sparse matrix-vector multiplication	Multiphysics simulations are at the core of modern Computer Aided Engineering (CAE) allowing the analysis of multiple, simultaneously acting physical phenomena. These simulations often rely on Finite Element Methods (FEM) and the solution of large linear systems which, in turn, end up in multiple calls of the costly Sparse Matrix-Vector Multiplication (SpM×V) kernel. The major—and mostly inherent—performance problem of the this kernel is its very low flop:byte ratio, meaning that the algorithm must retrieve a significant amount of data from the memory hierarchy in order to perform a useful operation. In modern hardware, where the processor speed has far overwhelmed that of the memory subsystem, this characteristic becomes an overkill [1]. Indeed, our preliminary experiments with the Elmer multiphysics package [3] showed that 60–90% of the total execution time of the solver was spent in the SpM×V routine. Despite being relatively compact, the widely adopted Compressed Sparse Row (CSR) storage format for sparse matrices cannot compensate for the very low flop:byte ratio of the SpM×V kernel, since it itself has a lot of redundant information. We have recently proposed the Compressed Sparse eXtended (CSX) format [2], which applies aggressive compression to the column indexing structure of CSR. Instead of storing the column index of every non-zero element of the matrix, CSX detects dense substructures of non-zero elements and stores only the initial column index of each substructure (encoded as a delta distance from the previous one) and a two-byte descriptor of the substructure. The greatest advantage of CSX over similar attempts in the past [4,5] is that it incorporates a variety of different dense substructures (incl. horizontal, vertical, diagonal and 2-D blocks) in a single storage format representation allowing high compression ratios, while its baseline performance, i.e., when no substructure is detected, is still higher than CSR’s. The considerable reduction of the sparse matrix memory footprint achieved by CSX alleviates the memory subsystem significantly, especially for shared memory architectures, where an average performance improvement of more than 40% over multithreaded CSR implementations can be observed. In this paper, we integrate CSX into the Elmer [3] multiphysics simulation software and evaluate its impact on the total execution time of the solver. Elmer employs iterative Krylov subspace methods for treating large problems using the Bi-Conjugate Gradient Stabilized (BiCGStab) method for the solution of	algorithm;baseline (configuration management);best, worst and average case;biconjugate gradient stabilized method;byte;clock rate;computer simulation;conjugate gradient method;elmer guidelines;experiment;flops;finite element method;iterative method;krylov subspace;linear system;memory footprint;memory hierarchy;multiphysics;run time (program lifecycle phase);shared memory;simulation software;solver;sparse matrix;the matrix;thread (computing)	Vasileios Karakasis;Georgios I. Goumas;Konstantinos Nikas;Nectarios Koziris;Juha Ruokolainen;Peter Råback	2012		10.1007/978-3-642-36803-5_40	computational science;parallel computing;computer science;theoretical computer science;operating system;algorithm	HPC	-3.168341141456971	39.424544939357716	62834
240e602f741dce3d534a959a924961fab038085d	multi-complex attributes analysis for optimum gps baseband receiver tracking channels selection		The Global Positioning System (GPS) passed a long way of development, starting from an advanced specialized tool, to a general purpose gadget used every day in our life. There are numerous presences of GPS in new technologies, applications and consumer products especially in Smartphone's and tablets. In GPS receiver design, power consumption and localization accuracy act as critical factors that affect the GPS receiver system outcome. Theoretically, increasing the Number of Required Tracking Channels (NRTC) in the GPS baseband receiver will increase the design complexity and size. Hence, the power consumption would significantly increase. Furthermore, to improve the location accuracy of a position, more satellites should be acquired and tracked by the receiver. This requires higher number of tracking channels in the receiver. Thus, optimizing the number of tracking channels to balance the conflict among performance parameters is a difficult and challenging task. The objective of this study is to highlight the need for an effective strategy to balance the tradeoff between conflicted GPS design parameters. A conceptual framework is proposed for determining the optimum GPS baseband receiver tracking channels in terms of power consumption and localization accuracy. Nine different operation modes of GPS receiver are evaluated by each design parameters, namely, power consumption, localization accuracy, and time with no position available for static and dynamic positioning. Multi-criteria analysis is a good strategy to visualize the trade-off between GPS design parameters, and to provide a dynamic power consumption planning.	baseband;global positioning system;smartphone	B. Rahmatullah;A. A. Zaidan;F. Mohamed;A. Sali	2017	2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2017.8102743	real-time computing;dynamic demand;global positioning system;time to first fix;receiver autonomous integrity monitoring;gps disciplined oscillator;precision lightweight gps receiver;assisted gps;electronic engineering;engineering;baseband	Mobile	4.4695327785439325	34.99791992584611	62843
744b718f88622adc882077c1abd14fb1753a32e6	improving particle filter performance using sse instructions	robot sensing systems;monte carlo localization;particle filter performance;run time gain;sse2 instruction sets;real time;data mining;implementation level optimization;robots control engineering computing instruction sets monte carlo methods particle filtering numerical methods program processors;particle filter;robots;particle filters intelligent robots monte carlo methods robot sensing systems runtime instruction sets computer aided instruction concurrent computing libraries usa councils;monte carlo localization particle filter performance implementation level optimization run time gain extended instruction sets sse2 instruction sets sse1 instruction sets;optimization;control engineering computing;extended instruction sets;program processors;monte carlo methods;particle filtering numerical methods;instruction sets;sse1 instruction sets	Robotics researchers are often faced with real-time constraints, and for that reason algorithmic and implementation-level optimization can dramatically increase the overall performance of a robot. In this paper we illustrate how a substantial run-time gain can be achieved by taking advantage of the extended instruction sets found in modern processors, in particular the SSE1 and SSE2 instruction sets. We present an SSE version of Monte Carlo Localization that results in an impressive 9x speedup over an optimized scalar implementation. In the process, we discuss SSE implementations of atan, atan2 and exp that achieve up to a 4x speedup in these mathematical operations alone.	algorithm;central processing unit;exptime;emoticon;internationalization and localization;katherine albrecht;mathematical optimization;monte carlo localization;monte carlo method;open-source software;particle filter;real-time computing;real-time transcription;robotics;sse2;speedup;streaming simd extensions;warren abstract machine	Peter Djeu;Michael Quinlan;Peter Stone	2009	2009 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2009.5354190	robot;monte carlo localization;parallel computing;real-time computing;particle filter;computer science;artificial intelligence;theoretical computer science;instruction set;statistics;monte carlo method	Robotics	0.5573182104025255	39.71184387344445	63062
b3052fe2e65981a6c45e7bdca999fb56b313f0c2	a wrapper around parallel mumps solver to reduce its memory usage and execution time for finite element method computations		Abstract In this paper, we present a wrapper around MUMPS solver, called Hierarchical Solver Wrapper (HSW), that is tailored to domain decomposition-based parallel finite element method computations on distributed memory systems. It offers the same interface as parallel MUMPS with matrix entries in coordinate format provided in a distributed fashion among multiple processors. The algorithm implemented by the wrapper utilizes multiple sequential instances of MUMPS solver to compute Schur complements over subdomains. Next, it deallocates sequential MUMPS solvers and LU factors, and it calls the parallel MUMPS solver feeded with the Schur complements, stored in distributed manner. In the backward substitution stage it recomputes the local LU factors before solving the local problems. The wrapper has been tested with three-dimensional isogeometric analysis computations, and we show it reduces both the memory usage and the execution time, in comparison with a single parallel MUMPS call.	computation;finite element method;mumps;run time (program lifecycle phase);solver	Maciej Paszynski;Antônio Tadeu A. Gomes	2017		10.1016/j.procs.2017.05.096	isogeometric analysis;parallel computing;computation;domain decomposition methods;theoretical computer science;finite element method;computer science;matrix (mathematics);distributed memory;solver	HPC	-3.2419357598718723	38.29752436545719	63397
54cbec83687db56f2ab01f6683d658b1d5298be2	performance and toolchain of a combined gpu/fpga desktop (abstract only)	hls compilers;accelerators;performance;gpu;fpga;programming toolchain;opencl	Low-power, high-performance computing nowadays relies on accelerator cards to speed up the calculations. Combining the power of GPUs with the flexibility of FPGAs enlarges the scope of problems that can be accelerated [2, 3]. We describe the performance analysis of a desktop equipped with a GPU Tesla 2050 and an FPGA Virtex-6 LX240T. First, the balance between the I/O and the raw peak performance is depicted using the roofline model [4]. Next, the performance of a number of image processing algorithms is measured and the results are mapped onto the roofline graph. This allows to compare the GPU and the FPGA and also to optimize the algorithms for both accelerators. A programming toolchain is implemented, consisting of OpenCL for the GPU and several High-Level Synthesis compilers for the FPGA. Our results show that the HLS compilers outperform handwritten code and offer a performance comparable to the GPU. In addition the FPGA compilers reduce the development time by an order of magnitude, at the expense of an increased resource consumption. The roofline model also shows that both accelerators are equally limited by the input/output bandwidth to the host. A well-tuned accelerator-based codesign, identifying the parallelism, the computation and data patterns of different classes of algorithms, will enable to maximize the performance of the combined GPU/FPGA system [1].	algorithm;compiler;computation;desktop computer;field-programmable gate array;graphics processing unit;high-level synthesis;image processing;input/output;opencl api;parallel computing;roofline model;supercomputer;toolchain	Bruno da Silva;An Braeken;Erik H. D'Hollander;Abdellah Touhafi;Jan G. Cornelis;Jan Lemeire	2013		10.1145/2435264.2435336	embedded system;computer architecture;parallel computing;real-time computing;performance;computer science;operating system;field-programmable gate array	HPC	-0.7982869672256622	46.21107245912089	63521
c480fc76bce0e102c73060818d70d833ec28d410	unified real time software decoder for hevc extensions	3d hevc real time decoder hevc shvc mv hevc;enhancement layers unified real time software decoder hevc extensions high efficiency video coding extensions 3d rendering range extension open source project openhevc decoder end to end video streaming gpac player gpac server hevc base layer;real time decoder;3d hevc;video streaming image enhancement public domain software rendering computer graphics video codecs video coding;mv hevc;hevc;decoding video coding standards software three dimensional displays high definition video encoding;shvc	There are several High Efficiency Video Coding (HEVC) extensions providing new tools on the top of a common HEVC base layer. These HEVC extensions enable higher temporal, spatial or quality of the video, 3-D rendering and range extension. In addition to the conforming HEVC decoder, the end-user need to support the decoding of the HEVC extension to benefits from its tools. In this paper we propose an unified software decoder enabling to decode all HEVC extensions. This solution is based on the open source project OpenHEVC which implements a conforming HEVC decoder. The new tools defined in HEVC extension are implemented and integrated into the OpenHEVC decoder. We show the first end-to-end video streaming demonstration of the HEVC extensions with the OpenHEVC decoder and the GPAC player. The GPAC server streams one HEVC base layer and two enhancement layers. At the client side, GPAC player uses the OpenHEVC to decode the base layer for HD resolution and can also decode whether the first EL for 4K resolution or the second EL for 3-D rendering.	4k resolution;client-side;end-to-end principle;gpac;high efficiency video coding;information needs;open-source software;server (computing);streaming media	Benoît Martin;Wassim Hamidouche;Jean Le Feuvre;Mickaël Raulet	2014	2014 IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2014.7025442	real-time computing;computer science;theoretical computer science;multimedia	Robotics	6.236266632402929	33.605424843578554	63641
23fc02e6f4244a9109f20d132bef14b2bd64d722	evaluation of hierarchical mesh reorderings	mesh optimization;first order;cache performance;scientific computing;memory systems;graph model;sparse matrix;tetrahedral mesh	Irregular and sparse scientific computing programs frequently experience performance losses because of inefficient use of the memory system in most machines. Previous work has shown that, for a graph model, performing a partitioning and then reordering within each partition (hierarchical reordering) improves performance. More recent work has shown that reordering heuristics based on a hypergraph model result in better reorderings than those based on a graph model. This paper studies the effects of hierarchical reordering strategies within the hypergraph model. In our experiments, the reorderings are applied to the nodes and elements of tetrahedral meshes, which are used as input to a mesh optimization application. This application includes computations such as loops over the elements in the mesh and sparse matrix multiplication based on the structure of the mesh. We show that cache performance degrades over time with consecutive packing, but not with breadth-first ordering, and that hierarchical reorderings involving hypergraph partitioning followed by consecutive packing or breadth-first orderings in each partition improve overall execution time.	breadth-first search;computation;computational science;computer performance;elegant degradation;energy citations database;execution unit;experiment;graph partition;hardware performance counter;heuristic (computer science);locality of reference;mathematical optimization;matrix multiplication;run time (program lifecycle phase);set packing;sparse matrix	Michelle Mills Strout;Nissa Osheim;Dave Rostron;Paul D. Hovland;Alex Pothen	2009		10.1007/978-3-642-01970-8_53	parallel computing;sparse matrix;computer science;theoretical computer science;first-order logic;distributed computing	HPC	-3.459214939250285	41.73133163925806	63665
1fe0f150c4dbb3b998b2e52257851358a03d417c	a cross-layer perspective for energy efficient processing: - from beyond-cmos devices to deep learning		As Moore's Law based device scaling and accompanying performance scaling trends are slowing down, there is increasing interest in new technologies and computational models for fast and more energy-efficient information processing. Meanwhile, there is growing evidence that, with respect to traditional Boolean circuits and von Neumann processors, it will be challenging for beyond-CMOS devices to compete with the CMOS technology. Nevertheless, some beyond-CMOS devices demonstrate other unique characteristics such as ambipolarity, negative differential resistance, hysteresis, and oscillatory behavior. Exploiting such unique characteristics, especially in the context of alternative circuit and architectural paradigms, has the potential to offer orders of magnitude improvement in terms of power, performance and capability. In order to take full advantage of beyond-CMOS devices, however, it is no longer sufficient to develop algorithms, architectures and circuits independent of one another. Cross-layer efforts spanning from devices to circuits to architectures to algorithms are indispensable. This talk will examine energy-efficient neural network accelerators for embedded applications in this context. Several deep neural network accelerator designs based on alternative device technologies, circuit styles and architectures will be highlighted. A comprehensive application-level benchmarking study for the MNIST dataset will be presented. The discussions will demonstrate that cross-layer efforts indeed can lead to orders of magnitude gain towards achieving extreme scale energy-efficient processing.	algorithm;artificial neural network;beyond cmos;boolean circuit;central processing unit;computation;computational model;computer performance;deep learning;embedded system;file spanning;hysteresis;ibm websphere extreme scale;image scaling;information processing;mnist database;moore's law;von neumann architecture	Xiaobo Sharon Hu	2018		10.1145/3194554.3200204	real-time computing;cellular neural network;beyond cmos;computational model;electronic circuit;computer science;boolean circuit;information processing;mnist database;cmos	EDA	4.59975056412984	41.824745491683906	63808
81c6b0d943d9491020a10bcd94a2cacb6422f413	load-balanced sparse matrix--vector multiplication on parallel computers	calcul matriciel;eficacia sistema;ordinateur parallele;etude experimentale;equilibrio de carga;equilibrage charge;performance systeme;transmission message;sparse matrix vector;system performance;message transmission;parallel computation;algorithme;algorithm;calculo paralelo;matrice creuse;greedy allocation;ordenador paralelo;load balancing;parallel computer;message passing;load balance;matrix calculus;sparse matrix;optimized message passing;parallel computations;multiplication;calcul parallele;estudio experimental;calculo de matrices;sparse matrix vector multiplication;matriz dispersa;transmision mensaje;algoritmo	We considered the load-balanced multiplication of a large sparse matrix with a large sequence of vectors on parallel computers. We propose a method that combines fast load-balancing with efficient message-passing techniques to alleviate computational and inter-node communications challenges. The performance of the proposed method was evaluated on benchmark as well as on synthetically generated matrices and compared with the current work. It is shown that, by using our approach, a tangible improvement over prior work can be obtained, particularly for very sparse and skewed matrices. Moreover, it is also shown that I/O overhead for this problem can be efficiently amortized through I/O latency hiding and overall load-balancing. © 1997 Academic Press	amortized analysis;benchmark (computing);computer;input/output;load balancing (computing);message passing;overhead (computing);parallel computing;sparse matrix	Sorin G. Nastea;Ophir Frieder;Tarek A. El-Ghazawi	1997	J. Parallel Distrib. Comput.	10.1006/jpdc.1997.1361	parallel computing;computer science;load balancing;theoretical computer science;sparse approximation;computer performance;algorithm	HPC	-2.3147643326285174	36.680643127007905	63897
03527ecd0708d33f118f335fb691ba3c89716725	design and implementation of parallel fft on cuda	cuda technology memory hierarchy thread thread block;parallelism optimization parallel fft algorithm fast fourier transform cuda technology compute unified device architecture graphics processing units gpus;memory hierarchy;parallel architectures fast fourier transforms graphics processing units optimisation parallel algorithms;cuda technology;thread thread block;instruction sets graphics processing units algorithm design and analysis optimization parallel processing educational institutions discrete fourier transforms	Fast Fourier Transform (FFT) algorithm has an important role in the image processing and scientific computing, and it's a highly parallel divide-and-conquer algorithm. In this paper, we exploited the Compute Unified Device Architecture CUDA technology and contemporary graphics processing units (GPUs) to achieve higher performance. We focused on two aspects to optimize the ordinary FFT algorithm, multi-threaded parallelism and memory hierarchy. We also proposed parallelism optimization strategies when the data volume occurs and predicted the possible situation when the amount of data increased further.it can be seen from the results that Parallel FFT algorithm is more efficient than the ordinary FFT algorithm.	algorithm;cuda;computational science;computer graphics;fast fourier transform;graphics processing unit;image processing;mathematical optimization;memory hierarchy;parallel computing;thread (computing)	Xueqin Zhang;Kai Shen;Chengguang Xu;Kaifang Wang	2013	2013 IEEE 11th International Conference on Dependable, Autonomic and Secure Computing	10.1109/DASC.2013.130	cuda pinned memory;computer architecture;parallel computing;computer science;theoretical computer science;general-purpose computing on graphics processing units	HPC	-1.0712818614752717	41.00322229340098	64309
b2a1bf9c5fbe0fcb0a4ea924fe01d5db8a63e9cc	opac: a floating-point coprocessor dedicated to compute-bound kernels		In many applications, the main part of the computations may be encapsulated in compute-bounds kernels. Achieving high performance on compute-bound primitives at a low hardware cost has became an important challenge. OPAC was designed as the basic cell of a floating-point coprocessor dedicated to the execution of compute-bound kernels. Due to efficient hardware mechanisms for controlling and sequencing a pipeline performance close to a floating-point multiply-add per cycle per cell is reached on applications such as solving linear systems, FFTs or correlations in a microprocessor environment.	coprocessor;online public access catalog	André Seznec;Karl Courtel	1992		10.1145/146628.140408	embedded system;computer architecture;parallel computing;kernel;computer hardware;computer science;floating point;control system;operating system;knowledge-based systems;pipeline transport;linear system;coprocessor	EDA	3.984700419889646	45.485613598680544	64373
b3ec06586447b4d497c0b95b10770c05745e475f	accelerating the explicitly restarted arnoldi method with gpus using an autotuned matrix vector product	parallel computing;15a18;arnoldi iteration;multicore;general purpose graphics processing unit;eigenproblem	This paper presents a parallelized hybrid single-vector Arnoldi algorithm for computing approximations to eigenpairs of a nonsymmetric matrix. We are interested in the use of accelerators and multicore units to speed up the Arnoldi process. The main goal is to propose a parallel version of the Arnoldi solver, which can efficiently use multiple multicore processors or multiple graphics processing units (GPUs) in a mixed coarse and fine grain fashion. In the proposed algorithms, this is achieved by an autotuning of the matrix vector product before starting the Arnoldi eigensolver as well as the reorganization of the data and global communications so that communication time is reduced. The execution time, performance, and scalability are assessed with well-known dense and sparse test matrices on multiple Nehalems, GT200 NVidia Tesla, and next generation Fermi Tesla. With one processor, we see a performance speedup of 2 to 3x when using all the physical cores, and a total speedup of 2 to 8x when adding a GPU to this multicore unit, and hence a speedup of 4 to 24x compared to the sequential solver.	arnoldi iteration;graphics processing unit	Jérôme Dubois;Christophe Calvin;Serge G. Petiton	2011	SIAM J. Scientific Computing	10.1137/10079906X	multi-core processor;computational science;parallel computing;computer science;theoretical computer science;mathematics;algebra;arnoldi iteration	HPC	-3.861798103212847	39.44109227427184	64500
f3acce9fa569bead6912985872b20344cefc6777	elliptic curve gf (p) point multiplier by dual arithmetic cores		Elliptic curve point multiplication (ECPM) over prime fields has received much attention these years due to growing applications of public key cryptography. In this work we propose a parallel architecture to perform ECPM by Montgomery ladder algorithm. It consists of dual arithmetic cores carrying out pipelined modular multiplications, two-port block RAMs, the modular divider and the control unit. It can compute a 384-bit ECPM by 1.03 ms at 276 MHz in Xilinx Virtex-4 FPGA, costing 11883 slices, 26 DSPs, and 26 block RAMs(32×12).	algorithm;control unit;cost per mille;digital signal processor;field-programmable gate array;grammatical framework;montgomery modular multiplication;parallel computing;public-key cryptography	Tao Wu	2015	2015 IEEE 11th International Conference on ASIC (ASICON)	10.1109/ASICON.2015.7516997	arithmetic;parallel computing;mathematics;algebra	Vision	9.516146024083525	44.35576915339483	64731
59f5ef901fd99867d00d44e0c174268c59c5cee3	towards energy efficient sensor nodes for online activity recognition		In sensor-based activity recognition often huge amounts of data have to be acquired from multiple sensors, which need to be communicated for further processing. When using wireless sensor nodes, energy efficiency is of outstanding importance, since it directly influences the time until the battery needs to be recharged. However, communicating a huge amount of sensor data over wireless interfaces causes high energy consumption as well. Furthermore, the host controller receiving the sensor data is hindered of using its low power modes, as it needs to be woken up more frequently as well. In the paper at hand, we introduce our approach of reducing energy consumption of sensor nodes in online activity recognition scenarios by calculating the feature extraction on the sensor subsystem itself. By doing so, we can reduce the output rate of the sensor which enables the host controller to stay in its low power modes for longer periods. Additionally, this approach drastically reduces the amount of data to be transmitted over wireless interfaces, which further improves energy consumption. In our experiments, the proposed approach reduces the energy consumption of a sensor node by up to 33 %.	atm adaptation layer;activity recognition;assistive technology;computation;controllers;experiment;feature extraction;host adapter;interface device component;node - plant part;power glove;sensor node;smart environment;uncompressed video;disease transmission	Florian Gr&#x00FC;tzmacher;Johann-Peter Wolff;Albert Hein;Polichronis Lepidis;Rainer Dorsch;Thomas Kirste;Christian Haubelt	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8217455	control engineering;wireless sensor network;battery (electricity);engineering;activity recognition;control theory;energy consumption;feature extraction;efficient energy use;real-time computing;sensor node	Mobile	2.54166036791309	33.72541264925641	64737
e6d360ca0ca02c6a543242011488c564d039ec31	parallel algorithms and subcube embedding on a hypercube	algoritmo paralelo;hypercube;parallel algorithm;multiprocessor;65f05;implementation;factorisation qr;hypercube multiprocessors;methode gauss;methode pivotage;parallel computation;algorithme parallele;ejecucion;calculo paralelo;gauss method;pivoting method;68r10;metodo gauss;65f50;plongement souscube;multiprocesador;calcul parallele;metodo pivotaje;multiprocesseur;hipercubo	It is well known that the connection in a hypercube multiprocessor is rich enough to allow the embedding of a variety of topologies within it. For a given problem, the best choice of topology is naturally the one that incurs the least amount of communication and allows parallel execution of as many tasks as possible. In a previous paper we proposed efficient parallel algorithms for performing QR factorization on a hypercube multiprocessor, where the hypercube network is configured as a two-dimensional subcube-grid with an aspect ratio optimally chosen for each problem. In view of the very substantial net saving in execution time and storage usage obtained in performing QR factorization on an optimally configured subcube-grid, similar strategies are developed in this work to provide highly efficient implementations for three fundamental numerical algorithms: Gaussian elimination with partial pivoting, QR factorization with column pivoting, and multiple least squares updating. Timing results on Intel iPSC/2...	parallel algorithm	Eleanor Chu;Alan George	1993	SIAM J. Scientific Computing	10.1137/0914006	parallel computing;multiprocessing;theoretical computer science;mathematics;parallel algorithm;implementation;algorithm;hypercube;gauss–seidel method	HPC	-2.2177870131532984	37.04420175496091	64900
8a8353cf1ce0d55d5aa3e0c0b9e927d5f0bd470a	inexact arithmetic circuits for energy efficient iot sensors data processing		Developing portable autonomous systems is highly requested for numerous application domains such as Internet of Things (IoT), wearable devices, and biomedical applications. Portable systems usually contain autonomous and networked sensors; each sensor hosts multiple input channels (e.g. tactile) closely coupled to embedded computing unit and power supply. The embedded computing unit should locally extract meaningful information by employing sophisticated methods. This imposes challenges on real time operation and adds a burden regarding power consumption. Approximate or inexact computing represents a promising solution for energy efficient data processing; it tunes the accuracy of computation on the specific application requirements in order to reduce power consumption. In this paper, inexact arithmetic circuits have been employed to improve the energy efficiency for sensors digital data processing. The proposed inexact circuits achieve up to 80% power saving when compared to the exact one, and similar solutions presented in literature with a maximum loss of 1.39 dB in output SNR when applied to FIR filters.	approximation algorithm;arithmetic circuit complexity;autonomous robot;autonomous system (internet);computation;decibel;digital data;discounted maximum loss;embedded system;finite impulse response;internet of things;power supply;real-time computing;requirement;sensor;signal-to-noise ratio;wearable technology	Mario Osta;Ali Ibrahim;Hussein Chible;Maurizio Valle	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351839	digital data;computation;wearable technology;efficient energy use;electronic circuit;arithmetic;computer science;adder;communication channel;signal-to-noise ratio	Embedded	3.7149311249328942	35.093170599072785	64938
27dc10a54b88f8571657e4a1618917e0ddd502f1	static mapping of parallelizable tasks under deadline constraints				Yining Xu;Ittetsu Taniguchi;Hiroyuki Tomiyama	2017	IEICE Transactions		theoretical computer science;parallelizable manifold;mathematics;distributed computing	Embedded	7.006813578867347	32.56502675552566	65152
dbe940fd48215524cbc9188f7677eb0d0d5bb45e	utilization of vlsi for creating an active data base of 3-d geometric models	geometric model	Parallelism of geometric computation can be achieved by distri­ buting the computation efforts according to essentially three different strategies, based on functional, spatial and struc­ tural division, respectively (Mantyla 1983). The conventional and already commercialized way to introduce parallel computa­ tion for viewing 3-D geometric models is employing functional parallelism as a pipeline for performing different sequential transformation phases of the 3-D viewing operation (Clark 1981). This approach limits the number of parallel activities to the number of separable functional computational modules. A second approach for parallelism is the division of the modeling space into separable volume elements, which can be processed independently using a suitable data structure like an octree (Kronlof 1985). The logical component structure of a model gives a third distribution strategy. Then each processor answers only to the computational needs of its assigned ob­ jects.	computation;computational geometry;data structure;database;octree;parallel computing;very-large-scale integration	Jorma Skyttä;Tapio Takala	1986		10.2312/EGGH/EGGH86/083-093	computer science;theoretical computer science;geometry;data parallelism;algorithm;task parallelism	Graphics	-0.3911225886654081	35.94087620377037	65442
0d1d8624763d0e5973d4c424dacab3e8c25f8741	parallel algorithms for triangular sylvester equations: design, scheduling and saclability issues	calcul matriciel;algoritmo paralelo;sylvester equation;parallel algorithm;matematicas aplicadas;algorithm performance;algorithm analysis;mathematiques appliquees;algorithme parallele;resultado algoritmo;algorithme reparti;performance algorithme;performance model;algoritmo repartido;analyse algorithme;matrix calculus;applied mathematics;distributed algorithm;analisis algoritmo;calculo de matrices	A new scalable algorithm for solving (quasi)triangular Sylvester equations on a logical 2D-toroidal processor grid is presented. Based on a performance model of the algorithm, a static scheduler closing the optimal processor grid and block sizes for a rectangular block scatter (RBS) mapping of matrices is incorporated in the algorithm. Scalability properties implying good scalability of the algorithm are also derived from the performance model. Performance results from our ScaLAPACK-like implementations on an 64 processor IBM SP verify the scalability potential of the algorithm.	parallel algorithm;scheduling (computing)	Peter Poromaa	1998		10.1007/BFb0095366	distributed algorithm;mathematical optimization;parallel computing;matrix calculus;computer science;theoretical computer science;sylvester equation;mathematics;distributed computing;parallel algorithm;algorithm	Robotics	-2.483196284508038	36.95188406350477	65529
b07c44cb7a1c002f019b51afb1c7b5bcec97d767	an efficient gpu implementation of bulk computation of the eigenvalue problem for many small real non-symmetric matrices		The main contribution of this paper is to present an efficient GPU implementation of bulk computation of eigenvalues for many small, non-symmetric, real matrices. This work is motivated by the necessity of such bulk computation in designing of control systems, which requires to compute the eigenvalues of hundreds of thousands non-symmetric real matrices of size up to 30x30. Several efforts have been devoted to accelerating the eigenvalue computation including computer languages, systems, environments supporting matrix manipulation offering specific libraries/function calls. Some of them are optimized for computing the eigenvalues of a very large matrix by parallel processing. However, such libraries/function calls are not aimed at accelerating the eigenvalues computation for a lot of small matrices. In our GPU implementation, we considered programming issues of the GPU architecture including warp divergence, coalesced access of the global memory, utilization of the shared memory, and so forth. In particular, we present two types of assignments of GPU threads to matrices and introduce three memory arrangements in the global memory. Furthermore, to hide CPU-GPU data transfer latency, overlapping computation on the GPU with the transfer is employed. Experimental results on NVIDIA TITAN~X show that our GPU implementation attains a speed-up factor of up to 83.50 and 17.67 over the sequential CPU implementation and the parallel CPU implementation with eight threads on Intel Core i7-6700K, respectively.	computation;graphics processing unit	Hiroki Tokura;Takumi Honda;Yasuaki Ito;Koji Nakano;Mitsuya Nishino;Yushiro Hirota;Masami Saeki	2017	IJNC		parallel computing;eigenvalues and eigenvectors;architecture;computation;theoretical computer science;cuda;thread (computing);matrix (mathematics);general-purpose computing on graphics processing units;shared memory;computer science	Theory	-4.168532517788237	40.020067984474736	65603
0632b7068a796502df04b05f478171c0a6d2bedc	performance models for the spike banded linear system solver	linear systems;iterative solver;design process;performance prediction spike banded linear system solver large scale parallel platform scalable parallel sparse linear system solver design process cost model iterative solvers performance banded preconditioner convergence property ilu preconditioner family truncated spike algorithm deep memory hierarchy pseudoanalytical parallel performance model heterogeneous multiclusters platforms;performance evaluation;costing;convergence of numerical methods;performance;performance evaluation convergence of numerical methods costing iterative methods linear systems parallel algorithms;linear system;performance characterization;iterative methods;linear systems predictive models scalability availability large scale systems process design costs robustness algorithm design and analysis performance analysis;large scale;banded parallel scalability performance prediction model;performance model;parallel;model;performance prediction;scalability;memory hierarchy;prediction;sparse linear system;cost model;analytical model;banded;parallel algorithms	With availability of large-scale parallel platforms comprised of tens-of-thousands of processors and beyond, there is significant impetus for the development of scalable parallel sparse linear system solvers and preconditioners. An integral part of this design process, is the development of performance models capable of predicting performance and providing accurate cost models for the solvers and preconditioners. There has been some work in the past on characterizing performance of the iterative solvers themselves. In this paper, we investigate the problem of characterizing performance and scalability of banded preconditioners. Recent work has demonstrated the superior convergence properties and robustness of banded preconditioners, compared to state-of-the-art ILU family of preconditioners. Furthermore, when used in conjunction with efficient banded solvers, banded preconditioners are capable of significantly faster time-to solution. Our banded solver, the Truncated Spike algorithm is specifically designed for parallel performance and tolerance to deep memory hierarchies. Its regular structure is also highly amenable to accurate performance characterization. Using these characteristics, we derive the following results in this paper: (i) we develop parallel formulations of the Truncated Spike solver, (ii) we develop a highly accurate pseudo-analytical parallel performance model for our solver, (iii) we show excellent predication capabilities of our model – based on which we argue the high scalability of our solver. Our pseudo-analytical performance model is based on analytical performance characterization of each phase of our solver. These analytical models are then parameterized using actual runtime information on target platforms. An important consequence of our performance models is that they reveal underlying performance bottlenecks in both serial and parallel formulations. All of our results are validated on diverse heterogeneous multiclusters – platforms for which performance prediction is particularly challenging.	linear system;solver;the spike (1997)	Murat Manguoglu;Faisal Saied;Ahmed H. Sameh;Ananth Grama	2010		10.1109/ISPDC.2010.8	mathematical optimization;parallel computing;computer science;theoretical computer science;linear system	ML	-3.54109475000898	40.22910961596755	65639
3d020d71f3b21c82d9ba00063e02cb9012f75f3f	multi-objective differential evolution on the gpu with c-cuda		In some applications, evolutionary algorithms may require high computational resources and high processing power, sometimes not producing a satisfactory solution after running for a considerable amount of time. One possible improvement is a parallel approach to reduce the response time. This work proposes to study a parallel multi-objective algorithm, the multi-objective version of Differential Evolution (DE). The generation of trial individuals can be done in parallel, greatly reducing the overall processing time of the algorithm. A novel approach to parallelize this algorithm is the implementation on the Graphic Processing Units (GPU). These units present high degree of parallelism and they were initially developed for image rendering. However, NVIDIA has released a framework, named CUDA, which allows developers to use GPU for general-purpose computing (GPGPU). This work studies the implementation of Multi-Objective DE (MODE) on the GPU with C-CUDA, evaluating the gain in processing time against the sequential version. Benchmark functions are used to validate the implementation and to confirm the efficiency of MODE on the GPU. The results show that the approach achieves an expressive speed up and a highly efficient processing power.	cuda;differential evolution;graphics processing unit	Fernando Bernardes de Oliveira;Donald Davendra;Frederico G. Guimarães	2012		10.1007/978-3-642-32922-7_13	differential evolution;parallel computing;computer science;theoretical computer science;rendering (computer graphics);degree of parallelism;speedup;evolutionary algorithm;general-purpose computing on graphics processing units;cuda;response time	Vision	-1.928199875294386	43.42618341248176	65870
8ef7f89454b775dd89ec5ae5da60ab769b664e12	nngine: ultra-efficient nearest neighbor accelerator based on in-memory computing		The nearest neighbor (NN) algorithm has been used in a broad range of applications including pattern recognition, classification, computer vision, databases, etc. The NN algorithm tests data points to find the nearest data to a query data point. With the Internet of Things the amount of data to search through grows exponentially, so we need to have more efficient NN design. Running NN on multicore processors or on general purpose GPUs has significant energy and performance overhead due to small available cache sizes resulting in moving a lot of data via limited bandwidth busses from memory. In this paper, we propose a nearest neighbor accelerator, called NNgine, consisting of ternary content addressable memory (TCAM) blocks which enable near-data computing. The proposed NNgine overcomes energy and performance bottleneck of traditional computing systems by utilizing multiple non-volatile TCAMs which search for nearest neighbor data in parallel. We evaluate the efficiency of our NNgine design by comparing to existing processor-based approaches. Our results show that NNgine can achieve 5590x higher energy efficiency and 510x speed up compared to the state-of-the-art techniques with a negligible accuracy loss of 0.5%.	central processing unit;computation;computer vision;content-addressable memory;data point;dynamic voltage scaling;electronic circuit;graphics processing unit;image scaling;in-memory database;internet of things;k-nearest neighbors algorithm;multi-core processor;nearest neighbor search;nearest neighbour algorithm;non-volatile memory;overhead (computing);php accelerator;pattern recognition;speedup;statistical classification;telecommunications access method	Mohsen Imani;Yeseong Kim;Tajana Simunic	2017	2017 IEEE International Conference on Rebooting Computing (ICRC)	10.1109/ICRC.2017.8123666	content-addressable memory;cache;parallel computing;data point;speedup;non-volatile memory;in-memory processing;multi-core processor;k-nearest neighbors algorithm;computer science	EDA	2.927101245290042	42.730533923000564	66150
7c72f7c3e733372e63d6e72cb17122c8dab4050b	parallelization of a level set method for simulating dendritic growth	distributed memory;distributed system;algoritmo paralelo;virtual machine;systeme reparti;parallel algorithm;deposition process;image segmentation;processor virtualization;memoria compartida;communicating process;solidification;level set;distributed memory machine;cache memory;paralelisacion;machine virtuelle;algorithme parallele;antememoria;proceso comunicante;resolucion problema;antememoire;sistema repartido;envoi message;parallelisation;ampi;processus communicant;segmentation image;dendritic growth;cache performance;courbe niveau;parallelization;message passing;level set methods;procedimiento revestimiento;mpi;curva nivel;memoire repartie;level set method;maquina virtual;procede depot;parallel applications;contour line;problem solving;resolution probleme	Processor virtualization is a parallelization technique that may be used to enhance the performance of parallel applications through the improvement of cache performance, overlapping of communication and computation. In this study, we use the processor virtualization technique to parallelize the level set method for solving solidification problems. Numerical results on a distributed memory machine are reported to show the performance of the resulting level set solver, and demonstrate the advantages of using processor virtualization. © 2006 Elsevier Inc. All rights reserved.	adaptive mesh refinement;automatic parallelization;cpu cache;central processing unit;computation;distributed memory;load balancing (computing);numerical linear algebra;overhead (computing);parallel computing;simulation;solver	Kai Wang;Anthony Chang;Laxmikant V. Kalé;Jonathan A. Dantzig	2006	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2006.02.005	dendrite;parallel computing;message passing;distributed memory;cpu cache;computer science;virtual machine;level set;message passing interface;theoretical computer science;operating system;distributed computing;parallel algorithm;image segmentation;level set method;algorithm;contour line	HPC	-4.009285467348064	35.588619695522006	66275
30bd6ff36a841aa923430e8d4cc2c1c6bfe7fac1	enloc: energy-efficient localization for mobile phones	energy efficient;emergency service;real time;energy efficiency mobile handsets global positioning system gsm batteries energy measurement wireless sensor networks costs poles and towers communications society;mobile phone;global positioning system;gsm enloc energy efficient localization mobile phones location based applications gps wifi;energy budget;energy cost;wireless lan;wireless lan global positioning system mobile computing;mobile computing	A growing number of mobile phone applications utilize physical location to express the context of information. Most of these location-based applications assume GPS capabilities. Unfortunately, GPS incurs an unacceptable energy cost that can reduce the phone’s battery life to less than nine hours. Alternate localization technologies, based on WiFi or GSM, improve battery life at the expense of localization accuracy. This paper quantifies this important tradeoff that underlies a range of emerging services. Driven by measurements from Nokia N95 phones, we develop an energy-efficient localization framework called EnLoc. The framework characterizes the optimal localization accuracy for a given energy budget, and develops predictionbased heuristics for real-time use. Evaluation on traces from real users demonstrates the possibility of achieving good localization accuracy for a realistic energy budget.	global positioning system;heuristic (computer science);internationalization and localization;location-based service;mobile app;mobile phone;real-time transcription;tracing (software)	Ionut Constandache;Shravan Gaonkar;Matt Sayler;Romit Roy Choudhury;Landon P. Cox	2009	IEEE INFOCOM 2009	10.1109/INFCOM.2009.5062218	embedded system;mobile identification number;mobile search;global positioning system;telecommunications;gsm services;computer science;operating system;location-based service;mobile phone tracking;efficient energy use;mobile station;timing advance;mobile computing;energy budget;computer network	Mobile	1.4200240642947564	34.36071155563109	66293
66dddfbad0e89894f904425320e970a2a0d0b1dd	aep: an error-bearing neural network accelerator for energy efficiency and model protection		Neural Networks (NNs) have recently gained popularity in a wide range of modern application domains due to its superior inference accuracy. With growing problem size and complexity, modern NNs, e.g., CNNs (Convolutional NNs) and DNNs (Deep NNs), contain a large number of weights, which require tremendous efforts not only to prepare representative training datasets but also to train the network. There is an increasing demand to protect the NN weight matrices, an emerging Intellectual Property (IP) in NN field. Unfortunately, adopting conventional encryption method faces significant performance and energy consumption overheads. In this paper, we propose AEP, a DianNao based NN accelerator design for IP protection. AEP aggressively reduces DRAM timing to generate a device dependent error mask, i.e., a set of erroneous cells while the distribution of these cells are device dependent due to process variations. AEP incorporates the error mask in the NN training process so that the trained weights are device dependent, which effectively defects IP piracy as exporting the weights to other devices cannot produce satisfactory inference accuracy. In addition, AEP speeds up NN inference and achieves significant energy reduction due to the fact that main memory dominates the energy consumption in DianNao accelerator. Our evaluation results show that by injecting 0.1% to 5% memory errors, AEP has negligible inference accuracy loss on the target device while exhibiting unacceptable accuracy degradation on other devices. In addition, AEP achieves an average of 72% performance improvement and 44% energy reduction over the DianNao baseline.	analysis of algorithms;application domain;asymptotic equipartition property;baseline (configuration management);computer data storage;dynamic random-access memory;elegant degradation;encryption;human body weight;it baseline protection;neural networks	Lei Zhao;Youtao Zhang;Jun Yang	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203854	artificial neural network;real-time computing;computer science;energy consumption;encryption;efficient energy use;side channel attack;overhead (business);inference;countermeasure	EDA	3.5376435470462146	41.67518381706608	66329
2325e476fd4a04d68039cbe84994759cc28707e4	an intelligent cloud-based energy management system using machine to machine communications in future energy environments	cloud computing renewable energy resources monitoring energy consumption artificial intelligence mobile handsets;energy management systems;test bed;power engineering computing;power engineering computing cloud computing energy management systems power consumption;power consumption;icems intelligent cloud based energy management system machine to machine communication future energy environment power consumption reduction;energy management system;cloud computing	This paper proposes an intelligent cloud-based energy management system (iCEMS) using the M2M communications, considering the requirements of the future energy environment. We designed and implemented the iCEMS in the test bed. The proposed system reduces the power consumption of the test bed up to 22.5%.	cloud computing;machine to machine;requirement;testbed	Jinsung Byun;Youngil Kim;Zion Hwang;Sehyun Park	2012	2012 IEEE International Conference on Consumer Electronics (ICCE)	10.1109/ICCE.2012.6162018	embedded system;real-time computing;simulation;cloud computing;computer science;engineering;operating system;testbed	Robotics	1.0889176794289839	32.471268403941714	66336
7e45c8a63324f14131f0c5cd2f980b03bc001ca4	gpu-accelerated verification of the collatz conjecture		The main contribution of this paper is to present an implementation that performs the exhaustive search to verify the Collatz conjecture using a GPU. Consider the following operation on an arbitrary positive number: if the number is even, divide it by two, and if the number is odd, triple it and add one. The Collatz conjecture asserts that, starting from any positive number m, repeated iteration of the operations eventually produces the value 1. We have implemented it on NVIDIA GeForce GTX TITAN and evaluated the performance. The experimental results show that, our GPU implementation can verify 5.01×10 64-bit numbers per second, while the CPU implementation on Intel Xeon X7460 can verify 1.80 × 10 64-bit numbers per second. Thus, our implementation on the GPU attains a speed-up factor of 278 over the single CPU implementation.	64-bit computing;brute-force search;central processing unit;geforce 200 series;geforce 6 series;geforce 700 series;geforce 900 series;graphics processing unit;iteration;memory bank;multi-core processor;shared memory;speedup	Takumi Honda;Yasuaki Ito;Koji Nakano	2014		10.1007/978-3-319-11197-1_37	collatz conjecture	DB	8.153609837681925	42.43091727223605	66513
3f6c4a777cdcb073257e6a7d4e645cf88904fab2	comparison of parallel scheduling for triangular system resolution on multi-core processors		In this paper, we present two parallel scheduling resolving linear triangular system: on the one hand Column-Oriented-Scheduling (COS) and Critical Path Algorithm (CPA) on the other hand through which, theoretically, their execution time reaches the lower bound of makespan without communication costs. These algorithms are implemented on the GRID'5000 and some experimental results are presented for comparing experimentally makespans and efficiencies with that of the appropriate routine belonging to library called PLASMA (Parallel Linear Algebra Software for Multi-core Architectures).	algorithm;cos;central processing unit;critical path method;experiment;linear algebra;makespan;multi-core processor;run time (program lifecycle phase);scheduling (computing)	Mounira Belmabrouk;Mounir Marrakchi	2017	2017 4th International Conference on Control, Decision and Information Technologies (CoDIT)	10.1109/CoDIT.2017.8102668	job shop scheduling;critical path method;linear algebra;multi-core processor;scheduling (computing);software;upper and lower bounds;computer science;distributed computing	HPC	0.1781448867246452	40.25604145956872	66548
5e47e1a5f61f54323408a21d62ad67f725d11bbe	a parallel inverse kinematics solution for robot manipulators based on multiprocessing and linear extrapolation	time of execution;robot manipulators;parallel processing;parallel inverse kinematics solution;robots;parallel inverse kinematics;inverse problems;extrapolation;multiprocessing;parallel algorithms;concurrency;closed-form solution;kinematics;linear extrapolation;joint velocity;computational latency;inverse kinematics;parallel robots;robot control;closed form solution;robot kinematics;concurrent computing	A method of computing inverse kinematics in parallel for robots with a closed-form solution is presented. The computational task of computing each inverse kinematics solution is partitioned with one subtask per joint, and all subtasks are computed concurrently. The intrinsic dependency among subtasks is removed by linear extrapolation through the gradient of inverse kinematic functions and joint velocity information. The high degree of concurrency and naturally balanced concurrent subtasks of the system significantly reduce the latency of the inverse kinematics evaluation. Compared with a serial solution, the algorithm results in a reduction of the time of execution by a factor proportional to the number of joints when implemented on a multiprocessor system. Its simplicity makes it easily applicable to any robot manipulators with closed-form solutions. Examples are used to illustrate the effectiveness and the efficiency of the algorithm. Implementation of the algorithm on a multiprocessor system is also discussed		Hong Zhang;Richard P. Paul	1990		10.1109/ROBOT.1990.126022	mathematical optimization;real-time computing;computer science;inverse kinematics;321 kinematic structure;control theory	Robotics	2.5754832471758218	38.273622324830356	66552
0c1fb179f7dc33b9df37db0e1e273c9a3abfc2ff	efficient matrix multiplication in hadoop		In a typical MapReduce job, each map task processing one piece of the input file. If two input matrices are stored in separate HDFS files, one map task would not be able to access the two input matrices at the same time. To deal with this problem, we propose a efficient matrix multiplication in Hadoop. For dense matrices, we use plain row major order to store the matrices on HDFS; For sparse matrices, we use the row-major-like strategy. So, a mapper can get the rows and columns by only scannig through a consecutive part of a file. We modify the Hadoop MapReduce input format, add two file paths to the two input matrices and store the input matrices in row major order. With the new file split structure, all data are distributed properly to the mappers. Finally, we propose a user feedback method to avoid the overheads of starting multiple map waves. A number of comparative experiments are conducted, the result show that our method observably improve the performance of dense matrix multiplication in MapReduce.	apache hadoop;column (database);experiment;mapper;mapreduce;matrix multiplication;overhead (computing);preprocessor;set packing;sparse matrix	Song Deng;Wenhua Wu	2016	IJCSA		machine learning;artificial intelligence;theoretical computer science;computer science;matrix multiplication	HPC	-0.7622618760957274	40.43418410781177	66614
91f8691fb209811de4b22f14c43e5806257781d4	a generalized vision of some parallel bidiagonal systems solvers	r cyclic reduction;linear recurrences;vector uniprocessors;gaussian elimination;vector processor;system of equations;divide and conquer;cyclic reduction	In this paper, a review of methods for the solution of general bidiagonal systems of equations is done. Gaussian Elimination, the r-Cyclic Reduction family of algorithms and the Divide and Conquer algorithm are analyzed. A unified view of the three types of methods is proposed. The work is focussed on two basic aspects of the methods: parallelism and grain. The influence of the architecture of the target computer on the parallelism and grain of the methods is evaluated. In particular, vector processors are analyzed as target architecture and one vector processor of the Convex C-3480 is taken as a case study. For the special case of Divide and Conquer, a model is made in order to tune parallelism and grain for its optimal execution. Two conclusions can be outlined from the analysis of the methods. First, the execution time of the r-Cyclic Reduction family of algorithms is lower as r grows reaching a lower significative bound in r=9. This means that the classic use of Cyclic Reduction on vector computers is outdated from now on. Second, the higher rank versions of the r-Cyclic Reduction family of algorithms and the optimized version of Divide and Conquer behave similarly on vector computers.	algorithm;bidiagonal matrix;central processing unit;computer;cyclic reduction;gaussian elimination;parallel computing;run time (program lifecycle phase);vector processor	Josep-Lluís Larriba-Pey;Juan J. Navarro;Oriol Roig;Angel Jorba	1994		10.1145/181181.181573	system of linear equations;gaussian elimination;vector processor;parallel computing;divide and conquer algorithms;computer science;akra–bazzi method;theoretical computer science;mathematics;algorithm;algebra	HPC	-4.159822902965742	40.46972548146069	66780
1e166504b47629451fa91ebd1ee14fb5ae945ddd	highly parallel architectures for solving linear equations	linear systems;architectural design;parallel architectures equations signal processing algorithms computer architecture computer networks data flow computing linear systems very large scale integration parallel processing algorithm design and analysis;very large scale integration;linear system;computer networks;computer architecture;parallel architectures;signal processing;least square;data flow computing;parallel architecture;linear equations;signal processing algorithms;data flow;algorithm design and analysis;parallel processing	"""An important impact of the fast growing VLSI device technology will be the massive capability of parallel processing which will in turn greatly affect the trend of modern signal processing technology. For the new trend it will be a necessity to have revolutionary architectural design concepts such as topological mapping between algorithm and architecture, simple and regular data flow etc [1]. In this paper, based on a natural topology of the computing structure, a novel """"computational wave-front"""" notion is introduced for describing and and validating data flow in locally connected networks. The parallel architectures include the linear system, with and without pivoting, and the least-square solver using Givens method. We believe that this set of linear system architectures will play a central role in modern signal processing."""	linear equation	Sun-Yuan Kung;D. V. Bhaskar Rao	1981		10.1109/ICASSP.1981.1171299	parallel processing;computer architecture;parallel computing;computer science;theoretical computer science;signal processing;linear system	HPC	7.498304301314357	38.75724038173872	67082
692eb2483240b0819827a0b61191fd1b5f9ab7fb	dual-mode floating-point multiplier architectures with parallel operations	double-extended precision floating-point multiplication;quadruple precision multiplication;computer arithmetic;parallel quadruple precision multiplier;dual-mode double precision floating-point;parallel operation;single precision;dual-mode quadruple precision multiplier;dual-mode double precision multiplier;parallel double precision multiplication;double precision;normalization;double precision multiplication;dual-mode floating-point multiplier architecture;floating-point;quadruple precision;conventional double precision multiplier;rounding;multiplier;scientific computing;floating point	Although most modern processors have hardware support for double precision or double-extended precision floatingpoint multiplication, this support is inadequate for many scientific computations. This paper presents the architecture of a quadruple precision floating-point multiplier that also supports two parallel double precision multiplications. Since hardware support for quadruple precision arithmetic is expensive, a new technique is presented that requires much less hardware than a fully parallel quadruple precision multiplier. With this architecture, quadruple precision multiplication has a latency of three cycles and two parallel double precision multiplications have latencies of only two cycles. The multiplier is pipelined so that two double precision multiplications can begin every cycle or a quadruple precision multiplication can begin every other cycle. The technique used for the dual-mode quadruple precision multiplier is also applied to the design of a dual-mode double precision floating-point multiplier that performs a double precision multiplication or two single precision multiplications in parallel. Synthesis results show that the dual-mode double precision multiplier requires 43% less area than a conventional double precision multiplier. The correctness of all the multipliers presented in this paper is tested and verified through extensive simulation. 2006 Elsevier B.V. All rights reserved.		Ahmet Akkas;Michael J. Schulte	2006	Journal of Systems Architecture	10.1016/j.sysarc.2006.03.002		SE	6.597657516025598	43.59309925256472	67333
b21078269a560fe92788ed7fa6a575c25dd62dab	automatic tuning of compiler optimizations and analysis of their impact	compiler optimization	Modern compilers can work on many platforms and implement a lot of optimizations, which are not always tuned well for every target platform. In the paper we present the Tool for Automatic Compiler Tuning (TACT), which helps to identify such underperforming compiler optimizations. Using GCC for ARM, we show how this tool can be used to improve performance of several popular applications, and how the results can be further analyzed to find places for improvement in the GCC compiler itself. c © 2013 The Authors. Published by Elsevier B.V. Selection and/or peer-review under responsibility of the organizers of the 2013 International Conference on Computational Science.	arm cortex-a9;arm architecture;apple a9;common subexpression elimination;computation;computational science;computer hardware;enlightenment foundation libraries;gnu compiler collection;mathematical optimization;multi-objective optimization;open-source software;optimizing compiler;pareto efficiency;requirement;speedup;x264;zlib	Dmitry Plotnikov;Dmitry Melnik;Mamikon Vardanyan;Ruben Buchatskiy;Roman Zhuykov;Je-Hyung Lee	2013		10.1016/j.procs.2013.05.298	manifest expression;parallel computing;real-time computing;compiler correctness;interprocedural optimization;computer science;superoptimization;operating system;programming language;intrinsic function;functional compiler	HPC	-2.4328244601689777	45.77216955993239	67337
d7b415998599565033dc08894eecfa48ac677767	an efficient data compression method for artificial vision systems and its low energy implementation using asip technology	visualization data compression machine vision encoding registers energy consumption retina;neurophysiology biomedical communication computer vision data compression energy consumption;specific instruction set processor asip technology efficient data compression method artificial vision systems visual cortex stimulation wireless communication human body data transmission energy consumption power consuming data compression method data pattern energy reduction ratios	This paper proposes an efficient data compression method for artificial vision systems based on visual cortex stimulation. These systems require wireless communication between inside and outside of human body, which consumes large amount of energy for data transmission. In order to reduce the energy consumption, efficient and less power consuming data compression method is required. The proposed method takes advantage of the appearance frequency of stimulation data pattern. Experimental results show that the proposed compression method achieves more than 81% of data compression rate, and energy reduction ratios by an application specific instruction-set processor implementation is 85% in compression and 36% in decompression compared to its base processor, respectively.	application-specific instruction set processor;computer vision;data compression	Tomoki Sugiura;Shoko Nakatsuka;Jaehoon Yu;Yoshinori Takeuchi;Masaharu Imai	2014	2014 IEEE Biomedical Circuits and Systems Conference (BioCAS) Proceedings	10.1109/BioCAS.2014.6981650	embedded system;computer vision;computer hardware;computer science	EDA	6.520980084357467	42.3814954913779	67415
c7216d7096c296ef8347c0ac245eb6b7d2e58f6d	instruction set extensions for pairing-based cryptography	pairing based cryptography;instruction set extension	A series of recent algorithmic advances has delivered highly effective methods for pairing evaluation and parameter generation. However, the resulting multitude of options means many different variations of base field must ideally be supported on the target platform. Typical hardware accelerators in the form of co-processors possess neither the flexibility nor the scalability to support fields of different characteristic and order. On the other hand, extending the instruction set of a generalpurpose processor by custom instructions for field arithmetic allows to combine the performance of hardware with the flexibility of software. To this end, we investigate the integration of a tri-field multiply-accumulate (MAC) unit into a SPARC V8 processor core to support arithmetic in Fp, F2n and F3n . Besides integer multiplication, the MAC unit can also execute dedicated multiply and MAC instructions for binary and ternary polynomials. Our results show that the tri-field MAC unit adds only a small size overhead while significantly accelerating arithmetic in F2n and F3n , which sheds new light on the relative performance of Fp, F2n and F3n in the context of pairing-based cryptography.	algorithm;bitwise operation;central processing unit;hardware acceleration;multi-core processor;multiply–accumulate operation;overhead (computing);pairing-based cryptography;polynomial;sparc;scalability;triangular function	Tobias Vejda;Dan Page;Johann Großschädl	2007		10.1007/978-3-540-73489-5_11	parallel computing;real-time computing;computer science;theoretical computer science	Arch	8.957421724975491	44.69391219191891	67521
b2d42f750b8f03e3f1608e5c220ce75573d2b51a	design and implementation of can data compression algorithm	time 0 16 ms controller area network multiplexing communication electronic control units ecus high level industrial control system design can bus low priority can messages error probability data transmission can frame length can message compression method embedded test board ems can data compression algorithm;protocols controller area networks data communication data compression error statistics multiplexing;data compression can embedded system;data compression algorithm design and analysis torque acceleration protocols automobiles engines	Controller area network (CAN) was designed for multiplexing communication between electronic control units (ECUs) in vehicles and many high-level industrial control applications. Its cost, performance and reliability provide for tremendous flexibility in system design. When a CAN bus is overloaded by the increased number of ECUs connected to the CAN bus, both the waiting time for low priority CAN messages and the error probability of data transmission are increased. Thus, it is desirable to reduce the frame length since the time duration for the data transmission is proportional to CAN frame length. In this paper, we present a CAN message compression method to reduce the CAN frame length. Experimental results show that the CAN transmission data can be compressed up to 76% by the proposed method. By using an embedded test board, it is shown that 64-bit EMS CAN data compression can be performed within 0.16ms and consequently the proposed algorithm can be used in automobile applications without any problem.	64-bit computing;algorithm;can bus;data compression;embedded system;high- and low-level;multiplexing;selection algorithm;simulation;systems design	Yujing Wu;Jin-Gyun Chung;Myung Hoon Sunwoo	2014	2014 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2014.6865202	data compression;embedded system;electronic engineering;real-time computing;telecommunications;image compression;computer science;operating system	Embedded	9.720301341458228	40.37042929590362	67738
520d97afca066856972117db02fc3b80814be9de	the enabling power of graph coloring algorithms in automatic differentiation and parallel processing	004;graph coloring sparse derivative computation automatic differentiation parallel computing	Combinatorial scientific computing (CSC) is founded rnon the recognition of the enabling power of combinatorial algorithms rnin scientific and engineering computation and in high-performance computing.rnThe domain of CSC extends beyond traditional scientific computing---the rnthree major branches of which are numerical linear algebra, rnnumerical solution of differential equations, and rnnumerical optimization---to include a range of emerging and rnrapidly evolving computational and information science disciplines.rnOrthogonally, CSC problems could also emanate fromrninfrastructural technologies for supporting high-performance computing.rnDespite the apparent disparity in their origins, rnCSC problems and scenarios are unified by the following common features:rn(A) The overarching goal is often to make computation efficient---by minimizing overall execution time, memory usage, and/or storage space---or to facilitate knowledge discovery or analysis.rn(B) Identifying the most accurate combinatorial abstractions thatrn help achieve this goal is usually a part of the challenge.rn(C) The abstractions are often expressed, with advantage, as graphrn or hypergraph problems.rn(D) The identified combinatorial problems are typically NP-hard torn solve optimally. Thus, fast, often linear-time, approximation (orrn heuristic) algorithms are the methods of choice.rn(E) The combinatorial algorithms themselves often need to bern parallelized, to avoid their being bottlenecks within a largerrn parallel computation.rn(F) Implementing the algorithms and deploying them via softwarern toolkits is critical.rnrnThis talk attempts to illustrate the aforementioned features of CSCrnthrough an example: we consider the enabling role graph coloring rnmodels and their algorithms play in efficient computation of rnsparse derivative matrices via automatic differentiation (AD). rnThe talk focuses on efforts being made on this topic within rnthe SciDAC Institute for Combinatorial Scientific Computing rnand Petascale Simulations (CSCAPES). rnAiming at providing overview than details, we discuss rnthe various coloring models used in sparse Jacobian and Hessian computation,rnthe serial and parallel algorithms developed in CSCAPES rnfor solving the coloring problems, and arncase study that demonstrate the efficacy of the coloring techniques rnin the context of an optimization problem in a Simulated Moving Bed process. rnImplementations of our serial algorithms for the coloring rnand related problems in derivative computation are assembled rnand made publicly available in a package called ColPack. rnImplementations of our parallel coloring algorithms are rnincorporated into and deployed via the load-balancing toolkit Zoltan. rnColPack has been interfaced with ADOL-C, an operator overloading-based rnAD tool that has recently acquired improved capabilities for rnautomatic detection of sparsity patterns of Jacobians and Hessiansrn(sparsity pattern detection is the first step in derivative matrix rncomputation via coloring-based compression). rnFurther information on ColPack and Zoltan is available rnat their respective websites, which can be accessed via rnhttp://www.cscapes.org	algorithm;automatic differentiation;computation;graph coloring;graph power;sparse matrix	Assefaw Hadish Gebremedhin	2009			computer science;theoretical computer science;distributed computing;algorithm	HPC	-2.608260432905033	40.5675130498004	67786
165239c3ec03b26b5f0a0812f927c144ad82c214	optimizing boolean embedding matrix for compressive sensing in rram crossbar	complexity theory;cmos asic platform resistive random access memory crossbar rram crossbar optimal boolean embedding matrix compressive sensing matrix vector multiplication power efficient linear embedding hardware random boolean embedding learning based embedding matrices real valued embedding matrix embedding distortion image recovery error;resistive ram application specific integrated circuits boolean algebra cmos integrated circuits compressed sensing;quantization signal;hardware;hardware quantization signal complexity theory	The emerging resistive random-access-memory (RRAM) crossbar provides an intrinsic fabric for matrix-vector multiplication, which can be leveraged as power efficient linear embedding hardware for data analytics such as compressive sensing. As the matrix elements are represented by resistance of RRAM cells, it imposes constraints for the embedding matrix due to limited RRAM programming resolution. A random Boolean embedding can be efficiently mapped to the RRAM crossbar but suffers from poor performance. Learning-based embedding matrices can deliver optimized performance but are continuous-valued which prevents it from being mapped to RRAM crossbar structure directly. In this paper, we have proposed one algorithm that can find an optimal Boolean embedding matrix for a given learned real-valued embedding matrix, so that it can be effectively mapped to the RRAM crossbar structure while high performance is preserved. The numerical experiments demonstrate that the proposed optimized Boolean embedding can reduce the embedding distortion by 2.7x, and image recovery error by 2.5x compared to the random Boolean embedding, both mapped on RRAM crossbar. In addition, optimized Boolean embedding on RRAM crossbar exhibits 10x faster speed, 17x better energy efficiency, and three orders of magnitude smaller area with slight accuracy penalty, when compared to the optimized real-valued embedding on CMOS ASIC platform.	algorithm;application-specific integrated circuit;arc diagram;cmos;compressed sensing;crossbar switch;data acquisition;distortion;experiment;isometric projection;matrix multiplication;numerical analysis;optimizing compiler;random access;resistive random-access memory;the matrix	Yuhao Wang;Xin Li;Hao Yu;Leibin Ni;Wei Yang;Chuliang Weng;Junfeng Zhao	2015	2015 IEEE/ACM International Symposium on Low Power Electronics and Design (ISLPED)	10.1109/ISLPED.2015.7273483	electronic engineering;parallel computing;computer science;theoretical computer science	EDA	4.0642353533821	42.74464700657774	67946
57fe0acf075d31cfb756a2a03a00fbf178763f6d	acceleration of othello computer game using an fpga tablet	mobile device othello computer game fpga tablet artificial intelligence field programmable gate array android os cpu;androids;mobile device;smart phones android operating system computer games field programmable gate arrays microprocessor chips mobile computing notebook computers;logic circuits;android;fpga;othello hpc fpga android mobile device;acceleration;hpc;humanoid robots;othello;games;artificial intelligence;field programmable gate arrays games acceleration artificial intelligence logic circuits androids humanoid robots;field programmable gate arrays	In this research, we accelerate the AI (Artificial Intelligence) portion of the Othello computer game using an FPGA (Field Programmable Gate Arrays). In computer-based board games such as Go and Othello, the calculation time for the AI part increases according to the number of states of a game and the level of look-ahead. We propose the use of an FPGA tablet running Android OS to accelerate these games. Generally, the CPUs in mobile devices are relatively slow, and a dedicated circuit with an FPGA can easily overtake. Using the FPGA allows us to reconfigure the circuit for different applications while the operating system is running. We achieve 2.5 times acceleration compared with a CPU by developing a pattern matching circuit.	artificial intelligence;central processing unit;computer othello;field-programmable gate array;leased line;mobile device;operating system;pc game;pattern matching;reversi;tablet computer	Tomoya Sato;Tetsu Narumi	2015	2015 Third International Symposium on Computing and Networking (CANDAR)	10.1109/CANDAR.2015.69	embedded system;simulation;computer hardware;computer science	Arch	1.4391158608187997	46.34531731928052	68047
2771299029e644595b49cf47a63a1fed126c7513	significance-driven logic compression for energy-efficient multiplier design		Approximate arithmetic has recently emerged as a promising paradigm for many imprecision-tolerant applications. It can offer substantial reductions in circuit complexity, delay, and energy consumption by relaxing accuracy requirements. In this paper, we propose a novel energy-efficient approximate multiplier design using a significance-driven logic compression (SDLC) approach. Fundamental to this approach is an algorithmic and configurable lossy compression of the partial product rows based on their progressive bit significance. This is followed by the commutative remapping of the resulting product terms to reduce the number of product rows. As such, the complexity of the multiplier in terms of logic cell counts and lengths of critical paths is drastically reduced. A number of multipliers with different bit-widths (4-bit to 128-bit) are designed in SystemVerilog and synthesized using Synopsys Design Compiler. Post-synthesis experiments showed that up to an order of magnitude energy savings, and reductions of 65% in critical delay, and almost 45% in silicon area can be achieved for an 128-bit multiplier, compared with an accurate equivalent. These gains are achieved with low accuracy losses estimated at less than 0.0028 mean relative error. Additionally, we demonstrate the performance-energy-quality tradeoffs for different degrees of compression, achieved through configurable logic clustering. While evaluating the effectiveness of the proposed approach three case studies were set up. First, a Gaussian blur filter was designed, which demonstrated up to 80% energy reduction with a meagre loss of image quality. Second, we evaluate our approach in machine learning application using perceptron classifier, showed up to 74% energy reduction with negligible error rate. Third, the proposed multiplier designs were used in a power-constrained image processing application. We showed that SDLC can achieve  $60\times $  improvement in computation capability, with potential to be employed in ubiquitous systems.	128-bit;4-bit;approximation algorithm;approximation error;box blur;circuit complexity;cluster analysis;compiler;computation;elasticity (data store);everquest;experiment;gaussian blur;image processing;image quality;lossy compression;low-power broadcasting;machine learning;most significant bit;perceptron;programming paradigm;requirement;run time (program lifecycle phase);synchronous data link control;systemverilog	Issa Qiqieh;Rishad A. Shafik;Ghaith Tarawneh;Danil Sokolov;Shidhartha Das;Alexandre Yakovlev	2018	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2018.2846410	systemverilog;lossy compression;real-time computing;perceptron;cluster analysis;circuit complexity;approximation error;adder;computer science;gaussian blur	EDA	4.480055144622997	42.92112478378739	68325
7a5b400ad25b0617a4b7a4be6c0687683dbcc7ac	assessing the effects of data compression in simulations using physically motivated metrics	data compression;digital simulation;hydrodynamics;parallel processing;physics computing;turbulence;eulerian coupled laser-plasma interaction code;eulerian higher-order hydrodynamics turbulence modeling code;lagrangian shock-hydrodynamics code;compression algorithms;compression impact assessment;compression rates;data compression effects;disk-bandwidth reduction;error characteristics;high performance computing architectures;lossy compression;physically motivated metrics;physics simulations;physics-based metrics;simulation codes;tightly coupled compression;data compression;high performance computing	This paper examines whether lossy compression can be used effectively in physics simulations as a possible strategy to combat the expected data-movement bottleneck in future high performance computing architectures. We show that, for the codes and simulations we tested, compression levels of 3-5X can be applied without causing significant changes to important physical quantities. Rather than applying signal processing error metrics, we utilize physics-based metrics appropriate for each code to assess the impact of compression. We evaluate three different simulation codes: a Lagrangian shock-hydrodynamics code, an Eulerian higher-order hydrodynamics turbulence modeling code, and an Eulerian coupled laser-plasma interaction code. We compress relevant quantities after each time-step to approximate the effects of tightly coupled compression and study the compression rates to estimate memory and disk-bandwidth reduction. We find that the error characteristics of compression algorithms must be carefully considered in the context of the underlying physics being modeled.	approximation algorithm;code;data compression;lossy compression;plasma active;signal processing;simulation;supercomputer;turbulence modeling	Daniel E. Laney;Steven Langer;Christopher Weber;Peter Lindstrom;Al Wegener	2013	2013 SC - International Conference for High Performance Computing, Networking, Storage and Analysis (SC)	10.3233/SPR-140386	simulation;computer science;theoretical computer science	HPC	-3.946998084862109	35.05295011070122	68366
8c27ed23bf974cfb87e93143ffc0556d1e04fb55	iteration space tiling for memory hierarchies	memory hierarchy		iteration;memory hierarchy;tiling window manager	Michael Wolfe	1987			discrete mathematics;hierarchy;memory hierarchy;computer science	HPC	5.123486825258182	36.69470876992455	68547
2f3ffae083aaba04f55bd564c53e14b6a57bd1f7	parallel matrix algorithms and applications (pmaa '02)	parallel matrix algorithm;guest editorial	This issue of the journal contains seven articles selected from invited and contributed presentations made at the Workshop on Parallel Matrix Algorithms and Applications (PMAA’02), which took place in Neuchâtel, Switzerland, on 9–10 November 2002. This workshop was organized by Erricos J. Kontoghiorghes and his group at the University of Neuchâtel. The workshop was attended by more than 100 participants from all over Europe, Israel, Korea, Japan, and the United States. PMAA’02 is the second PMAA Workshop––the previous one was held in August 2000. The workshop covered a wide range of topics in numerical linear algebra, including direct and iterative methods for solving systems of equations, singular value decomposition algorithms, total least squares problems, optimization and estimation, and algebraic multigrid methods. Issues concerning implementation, performance, and optimization of memory requirements were also discussed. Based on the enthusiasm of the attendees, several of whom traveled thousands of miles to attend the PMAA meeting for a second time, one may suggest that we are witnessing the emergence, in Europe, of a significant biannual event for researchers in the above themes. We hope this is indeed the case. The third edition of this meeting, PMAA’04, is due to take place in France in the Fall of 2004. This special issue of Parallel Computing consists of a selected set of papers presented at the workshop which are concerned with algorithms related to parallel computation. Not surprisingly, several of the papers deal with Numerical Linear Algebra. The first paper, by Beaumont, Legrand and Robert, describes algorithms for scheduling divisible workloads on heterogeneous systems. In addition to new optimality results, the paper describes an asymptotically optimal multiround algorithm. This multiround algorithm automatically performs resource selection, a difficult task that was previously left to the user. The algorithm is simpler to implement than previous algorithms, is more robust to changes in the speeds of the processors and/or communication links and appears to outperform existing solutions on a large variety of platforms, especially when the communication-to-computation ratio is not very high. Be cka and Ok sa present a case study of their two-sided block Jacobi algorithm with dynamic ordering for computing the SVD of a n-by-n matrix. They investigate the behavior of the algorithm as they vary the block size. The authors find that the	asymptotically optimal algorithm;block size (cryptography);central processing unit;computation;emergence;estimation theory;iterative method;jacobi method;mathematical optimization;multigrid method;numerical analysis;numerical linear algebra;parallel computing;requirement;scheduling (computing);singular value decomposition;switzerland;total least squares	Peter Arbenz;Efstratios Gallopoulos;Bernard Philippe;Yousef Saad	2003	Parallel Computing	10.1016/S0167-8191(03)00094-2	theoretical computer science;matrix (mathematics);computer science	HPC	-0.9742727688368091	38.0337356232659	69073
1898f77164463ddeee35b3a5ed197390435df3e6	energy-efficient cmos memristive synapses for mixed-signal neuromorphic system-on-a-chip		Emerging non-volatile memory (NVM), or memristive, devices promise energy-efficient realization of deep learning, when efficiently integrated with mixed-signal integrated circuits on a CMOS substrate. Even though several algorithmic challenges need to be addressed to turn the vision of memristive Neuromorphic Systems-on-a-Chip (NeuSoCs) into reality, issues at the device and circuit interface need immediate attention from the community. In this work, we perform energy-estimation of a NeuSoC system and predict the desirable circuit and device parameters for energy-efficiency optimization. Also, CMOS synapse circuits based on the concept of CMOS memristor emulator are presented as a system prototyping methodology, while practical memristor devices are being developed and integrated with general-purpose CMOS. The proposed mixed-signal memristive synapse can be designed and fabricated using standard CMOS technologies and open doors to interesting applications in cognitive computing circuits.	cmos;capability maturity model;cognitive computing;computer vision;deep learning;emulator;general-purpose modeling;mathematical optimization;memristor;mixed-signal integrated circuit;neuromorphic engineering;non-volatile memory;recurrence relation;simulation;synapse;system on a chip;volatile memory	Vishal Saxena;Xinyu Wu;Kehan Zhu	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351766	system on a chip;mixed-signal integrated circuit;memristor;electronic engineering;electronic circuit;computer science;integrated circuit;cognitive computing;neuromorphic engineering;cmos	EDA	4.911744947641948	41.44659612610359	69204
0010d3c2d597e760263ed06f820511dbd1ec6384	fault-detection by result-checking for the eigenproblem	developpement logiciel;eigenvalue problem;confidence level;tolerance aux pannes logiciel;sistema informatico;software fault tolerance;probleme valeur propre;computer system;eigenvalues;eigenvalues and eigenvectors;rounding errors;desarrollo logicial;fault detection;software development;numerical algorithm;fault coverage;systeme informatique;fiabilite logiciel;fiabilidad logicial;fault injection;software reliability;problema valor propio	This paper proposes a new fault detection mechanism for the computation of eigenvalues and eigenvectors, the so called eigenproblem, for which no such scheme existed before, to the best of our knowledge. It consists of a number of assertions that can be executed on the results of the computation to determine their correctness. The proposed scheme follows the Result Checking principle, since it does not depend on the particular numerical algorithm used. It can handle both real and complex matrices, symmetric or not. Many practical issues are handled, like rounding errors and eigenvalue ordering, and a practical implementation was built on top of unmodified routines of the well-known LAPACK library. The proposed scheme is simultaneously very efficient, with less than 2% performance overhead for medium to large matrices, very effective, since it exhibited a fault coverage greater than 99.7% with a confidence level of 99%, when subjected to extensive fault-injection experiments, and very easy to adapt to other libraries of mathematical routines besides LAPACK. 1 This work was partially supported by the Portuguese Ministério da Ciência e Tecnologia, the European Union through the R&D Unit 326/94 (CISUC) and the project PRAXIS XXI 2/2.1/TIT/1625/95 (PARQUANTUM).	algorithm;computation;correctness (computer science);experiment;fault coverage;fault detection and isolation;fault injection;lapack;library (computing);numerical analysis;overhead (computing);round-off error;rounding;whole earth 'lectronic link	Paula Prata;João Gabriel Silva	1999		10.1007/3-540-48254-7_28	reliability engineering;eigenvalues and eigenvectors;computer science;theoretical computer science;operating system;software engineering;mathematics;distributed computing;algorithm;statistics	HPC	-2.4633658420292295	35.02471408579838	69259
6342d20ec15a75b8cc3eef9e67b5c72e7da218c5	integer ray tracing	floating point unit;fixed point;general purpose processor;material model;ray tracing;floating point;source code	Despite nearly universal support for the IEEE 754 floating-point standard on modern generalpurpose processors, a wide variety of more specialized processors do not provide hardware floating-point units and rely instead on integer-only pipelines. Ray tracing on these platforms thus requires an integer rendering process. Toward this end, we clarify the details of an existing fixed-point ray/triangle intersection method, provide an annotated implementation of that method in C++, introduce two refinements that lead to greater flexibility and improved accuracy, and highlight the issues necessary to implement common material models in an integer-only context. Finally, we provide the source code for a template-based integer/floating-point ray tracer to serve as a testbed for additional experimentation with integer ray tracing methods.	c++;central processing unit;pipeline (computing);ray tracing (graphics);testbed	Jared Heinly;Shawn Recker;Kevin Bensema;Jesse Porch;Christiaan P. Gribble	2009	J. Graphics, GPU, & Game Tools	10.1080/2151237X.2009.10129289	floating-point unit;ray tracing;real-time computing;computer hardware;computer science;floating point;theoretical computer science;operating system;fixed point;source code	Visualization	-0.07358609631664212	46.052314816706634	69618
202f76c909b6de022dc37e1e0ba76fcd6866a731	the gpu-based string matching system in advanced ac algorithm	paper;memory management;information retrieval;computer graphic equipment;kernel algorithm;gpu;string matching system;graphics processing unit instruction sets automata pattern matching algorithm design and analysis memory management data structures;coprocessors;gpu string matching system adavanced ac algorithm kernel algorithm information retrieval computational biology webpage matching system;cuda;automata;data structures;pattern matching;web services;web services computer graphic equipment coprocessors information retrieval string matching;parallel multi string matching;nvidia;adavanced ac algorithm;ac algorithm;algorithms;ac algorithm gpu cuda parallel multi string matching webpage matching system;webpage matching system;computer science;string matching;graphics processing unit;computational biology;algorithm design and analysis;instruction sets	As one of the most pervasive problems in computer science, string matching is the kernel algorithm in many applications,which especially within the communities of information retrieval and computational biology. Meanwhile, the CPU+GPU heterogeneous parallel platform becomes more and more popular in solving computing intensive applications. This paper implements the webpage matching system with GPU-based advanced AC algorithm, G-AC, which is almost 28 times peak performance to the original AC algorithm which is referred from Snort [1].	central processing unit;computational biology;computer science;graphics processing unit;information retrieval;snort;speedup;string searching algorithm;web page	Jiang-Feng Peng;Hu Chen;Shaohuai Shi	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.210	web service;algorithm design;parallel computing;commentz-walter algorithm;data structure;computer science;theoretical computer science;operating system;machine learning;pattern matching;instruction set;automaton;programming language;coprocessor;string searching algorithm;memory management	Theory	0.018833707491691207	42.36279107124869	69696
0c68710056aa57df75179744247a2b8b8ef581c3	high-performance energy-efficient encryption in the sub-45nm cmos era	public key cryptography;energy efficiency;ultra low voltage circuit design encryption accelerators advanced encryption standard aes;encryption accelerators;cmos integrated circuits;encryption engines energy efficiency cmos technology usa councils algorithm design and analysis;random number generator;cmos technology;ultra low voltage;encryption;secure hash algorithm;energy efficient;power efficiency;circuit design;random number generation;hardware accelerator;usa councils;design optimization;ultra low voltage circuit design;power 10 mw high performance energy efficient encryption engines sub 45nm cmos era high performance energy efficient decryption engines transistor integration single processor core processor platform security power performance unprecedented improvements hardware accelerator engines compute intensive cryptography algorithms arithmetic technologies data path technologies processor security secure hashing algorithm compute engines sha galois field multipliers public key cryptography acceleration secure on die key generation fully digital random number generators circuit optimizations design optimizations dynamic operating voltage range aes encryption aes decryption mobile devices hand held devices wearable devices;metastable state;technology scaling;engines;transistors;digital arithmetic;transistors cmos integrated circuits digital arithmetic microprocessor chips public key cryptography random number generation;advanced encryption standard aes;advanced encryption standard;galois field;high performance;algorithm design;algorithm design and analysis;microprocessor chips	With technology scaling enabling integration of billions of transistors on a single processor core, deploying high-performance and energy-efficient encryption/decryption engines on-die has become a reality in the sub-45nm CMOS era. Not only does this enable increased security of processor platforms, but also achieves unprecedented improvements in power-performance through the adoption of specialized hardware accelerator engines for various compute-intensive cryptography algorithms. In this presentation, we describe novel arithmetic and data-path technologies to enable highspeed on-die AES encryption/decryption accelerators for processor security, Secure Hashing Algorithm (SHA) compute engines, Galois-Field multipliers for public-key cryptography acceleration and secure on-die key generation using fully-digital random number generators based on metastable state elements. Circuit and design optimizations to enable ultra-low voltage operation of these accelerators are discussed to achieve up to a 10X higher energy-efficiency and a wide dynamic operating voltage range that enables scalable AES encryption/decryption down to sub-10mW per round, enabling power-efficient security to permeate into future mobile/hand-held/wearable devices.	algorithm;cmos;cryptographic hash function;encryption;hardware acceleration;image scaling;key generation;mobile device;multi-core processor;public-key cryptography;random number generation;scalability;transistor;wearable technology	Ram Krishnamurthy;Sanu K. Mathew;Farhana Sheikh	2011	2011 48th ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2024724.2024804	embedded system;algorithm design;electronic engineering;40-bit encryption;computer science;theoretical computer science;operating system;efficient energy use;cmos;algorithm;56-bit encryption	EDA	7.135707449758666	43.93117041804203	69962
b5ea7001786676fd0cce40055defada78878e21c	a grid-based system for the multi-reservoir optimal scheduling in huaihe river basin	distributed system;optimisation;haute performance;systeme reparti;protocole transmission;flood;rivers;optimizacion;sequential computation;hydrologie;concepcion optimal;conception optimale;inundacion;distributed computing;calcul sequentiel;sistema complejo;grid;inondation;protocolo transmision;hidrologia;sistema repartido;systeme complexe;complex system;optimal scheduling;rejilla;scheduling;rio;flood control;riviere;alto rendimiento;hydrology;optimal design;grille;calculo repartido;optimization;calculo secuencial;river basin;algoritmo optimo;algorithme optimal;optimal algorithm;grid computing;high performance;calcul reparti;ordonnancement;reglamento;transmission protocol	The up- and mid-stream of Huaihe River Basin is a complex system of reservoirs and river-ways. It is difficult for flood control and reservoir scheduling. It is ineffective to perform sequential computations for optimal scheduling of multi-reservoir due to the system complexity. In this paper, we implemented the multi-reservoir optimal scheduling algorithm in a Grid environment. Key components as multiple Protocols were developed within the layers of Grid architecture. The proposed Grid computing architecture provides an innovative design of multi-reservoir optimal scheduling system for increasing the accuracy of flood control and speedup of computing.		Bing Liu;Huaping Chen;Guoyi Zhang;Shijin Xu	2006		10.1007/11610496_90	fair-share scheduling;complex systems;simulation;drainage basin;dynamic priority scheduling;computer science;rate-monotonic scheduling;optimal design;distributed computing;flood control;round-robin scheduling;grid;scheduling;grid computing	Robotics	-3.284702353062773	33.84099526405376	70162
dc92a7f023019648450e250dbb7d5c78c48f69d8	parallel data acquisition for visualization of very large sparse matrices	performance evaluation parallel visualization data acquisition very large sparse matrices massively parallel computer systems matrix down sampling personal computer;sparse matrices data visualization memory management microcomputers arrays educational institutions runtime;data acquisition visualization sparse matrices parallel algorith;visualization;parallel algorith;sparse matrices data acquisition data visualisation mathematics computing parallel processing;data acquisition;sparse matrices	The problem of visualization of very large sparse matrices emerging on massively parallel computer systems is identified and a new method along with an accompanying algorithm for parallel acquisition of visualization data for such matrices are presented. The proposed method is based on downsampling a matrix into blocks for which the desired visualization data are saved into a file. This file is then supposed to be downloaded and processed into a final image on a personal computer. Experimental results for the evaluation of the performance and scalability of the proposed algorithm are further provided and discussed.	algorithm;central processing unit;code;data acquisition;decimation (signal processing);experiment;graphical user interface;mathematical optimization;overhead (computing);parallel computing;performance tuning;personal computer;pixel;programmer;scalability;sparse matrix	Daniel Langr;Ivan Simecek;Pavel Tvrdík;Tomás Dytrych	2013	2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing	10.1109/SYNASC.2013.51	parallel computing;visualization;sparse matrix;computer science;theoretical computer science;sparse approximation;distributed computing;data acquisition;algebra	HPC	-2.1880359416072626	40.7508469676603	70346
cfeaf5df93cb15cc1aa21f41e1c974894d5cfd0d	insertion, deletion robust audio watermarking: a set theoretic, dynamic programming approach	digital watermarking;computer programming	Desynchronization vulnerabilities have limited audio watermarking’s success in applications such as digital rights management (DRM). Our work extends (blind-detection) spread spectrum (SS) watermarking to withstand time scale desynchronization (insertion/deletions) by applying dynamic programming (DP). Detection uses short SS watermark blocks with a novel O(N log N) correlation algorithm that provides robustness to time shifts and the resulting offsets to the watermarking domain transform. To withstand insertion/deletion, DP techniques then search for sequences of blocks instead of detecting SS watermarks individually. This allows DP techniques to govern the tradeoff between long/short SS blocks for non-desynchronization/desynchronization robustness. However, high dimensional searches and short SS blocks both increase false detection rates. Consequently, we verify detections between multiple, simultaneously embedded watermarks. Embedding multiple watermarks while considering host interference, compression robustness, and perceptual degradation to the host audio is a complex problem, solved using a set theoretic embedding framework. Proposed techniques improve performance by multiple orders of magnitude compared with naive SS schemes. Results also demonstrate the tradeoff between non-desynchronization/desynchronization robustness.	algorithm;digital rights management;digital watermarking;distortion;dynamic programming;elegant degradation;embedded system;insertion sort;interference (communication);mp3;robustness (computer science);sensor;set theory	Andrew Nadeau;Gaurav Sharma	2013		10.1117/12.2005462	telecommunications;digital watermarking;computer science;theoretical computer science;computer programming;computer security;physics	Vision	8.224665947200124	33.560239449564435	70530
1a05e56a4377635b70373042e12719e761f98301	exploring multiple dimensions of parallelism in junction tree message passing	parallel computing;gpus;paper;bepress selected works;data mining;cuda;junction trees;belief propagation;belief propagation parallel computing gpus junction trees bayesian networks;nvidia;artificial intelligence;computer science;nvidia geforce gtx 460;bayesian networks	Belief propagation over junction trees is known to be computationally challenging in the general case. One way of addressing this computational challenge is to use node-level parallel computing, and parallelize the computation associated with each separator potential table cell. However, this approach is not efficient for junction trees that mainly contain small separators. In this paper, we analyze this problem, and address it by studying a new dimension of node-level parallelism, namely arithmetic parallelism. In addition, on the graph level, we use a clique merging technique to further adapt junction trees to parallel computing platforms. We apply our parallel approach to both marginal and most probable explanation (MPE) inference in junction trees. In experiments with a Graphics Processing Unit (GPU), we obtain for marginal inference an average speedup of 5.54x and a maximum speedup of 11.94x; speedups for MPE inference are similar.	belief propagation;cuda;clique (graph theory);computation;experiment;geforce;graphics processing unit;machine learning;marginal model;mathematical optimization;message passing;parallel computing;planar separator theorem;software propagation;speedup;table cell;tree decomposition	Lu Zheng;Ole J. Mengshoel	2013			parallel computing;computer science;theoretical computer science;machine learning	ML	0.6579239060174237	41.41243694855578	70802
7254f519946ceffe56325f37b3017714d8a8f0c6	accelerating residue-to-binary conversion of very high cardinality moduli set for fully homomorphic encryption	encryption;acceleration;adders;graphics processing units;dynamic range;field programmable gate arrays;hardware	Recent hardware implementations of fully homomorphic encryption (FHE) exploit very high cardinality arbitrary moduli sets to parallelize large integer arithmetic. However, the benefit they gained are heavily offset by the slow residue-to-binary conversion due to the large modulo operations and limited number theoretic properties of arbitrary moduli. This paper presents a fast residue-to-binary (R2B) conversion method by transforming a large number of residues of arbitrary moduli into only three residues before the conversion. It exploits the smaller and parallel operations of base extension for the mapping to {2n−1, 2n, 2n +1} and the highly efficient adder-based R2B converter of the latter to achieve 2.25 ∼ 6.8 times speedup over recent R2B converters, which translates to approximately 10.2% to 32.08% of overall speed improvement for their FHE implementations.	adder (electronics);computation;dynamic range;homomorphic encryption;like button;modulo operation;residue number system;speedup;theory	Truong Phu Truan Ho;Chip Hong Chang	2016	2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2016.7803882	acceleration;embedded system;computer vision;dynamic range;electronic engineering;discrete mathematics;computer science;theoretical computer science;mathematics;encryption;algorithm;adder;field-programmable gate array	Arch	9.021060017433372	43.91822376959029	70822
6955d45674a17238abff3cf48c4867706378416d	modular exponentiation of matrices on fpga-s		We describe an efficient FPGA implementation for the exponentiation of large matrices. The research is related to an algorithm for constructing uniformly distributed linear recurring sequences. The design utilizes the special properties of both the FPGA and the used matrices to achieve a very significant speedup compared to traditional architectures.	algorithm;field-programmable gate array;modular exponentiation;speedup	T. Herendi;R. Major	2011	CoRR		parallel computing;computer science;theoretical computer science;distributed computing	ML	9.12199762553506	44.1647939102437	70868
1ebecb6480f43f61b10bd10d3e158f0ee569f09f	efficient multimedia data encryption based on flexible qm coder	software;multimedia;encryption;information security;multimedia encryption;cryptanalyse;real time processing;cifrado;securite donnee;arithmetic code;random coding;cryptanalysis;codigo aritmetico;codage aleatoire;criptoanalisis;cryptage;criptografia;cryptography;multimedia data;random sequence;cryptographie;algorithms;code arithmetique;computer hardware;security of data;codeur qm;codificacion aleatoria	Efficient multimedia encryption algorithms are fundamental to multimedia data security because of the massive data size and the need of real-time processing. In this research, we present an efficient encryption scheme, called the RCI-QM coder, which achieves the objective of encryption with a regular QM coder by applying different coding conventions to encode individual symbols according to a statistically random sequence. One main advantage of our scheme is the negligible computation cost associated with encryption. We also demonstrate with cryptanalysis that the security level of our scheme is high enough to thwart most common attacks. The proposed RCI-QM coder is easy to implement in both software and hardware, while providing backward compatibility to the standard QM coder.	encryption	Dahua Xie;C.-C. Jay Kuo	2004		10.1117/12.528059	multiple encryption;dictionary coder;real-time computing;40-bit encryption;computer science;theoretical computer science;disk encryption hardware;computer security;encryption;probabilistic encryption;56-bit encryption	EDA	8.640872071277734	36.6780412463866	70880
72678c3561b6111b0632b154c7ee9c88f2b87da1	multilevel task partition algorithm for parallel simulation of power system dynamics	graph partition;power system dynamics;satisfiability;task partition;objective function;large scale;graph partitioning;power system;parallel computer;power grid;dynamic simulation;parallel simulation	Nowadays task partition for parallel computing is becoming more and more important. Particular in power system dynamic simulation, it is critical to design an efficient partition algorithm to reduce the communication and balance the computation load [1]. This paper presents a novel multilevel partition scheme based on the graph partition algorithm. By introducing regional characteristic into the partition, improving the weights of nodes and edges, proposing an objective function to evaluate the partition results and some other schemes, we can efficiently improve the defects in the traditional partition methods. With 12 CPUs for a large scale power system with 10188 nodes, the parallel efficiency with our new algorithm was 63% higher than that with METIS, a well-known program used for partitioning graphs. The proposed algorithm will satisfy the requirement for large scale power grid dynamic simulation.	algorithm;central processing unit;computation;dynamic simulation;graph partition;loss function;metis;optimization problem;parallel computing;speedup;system dynamics	Wei Xue;Shanxiang Qi	2007		10.1007/978-3-540-72584-8_70	dynamic simulation;mathematical optimization;parallel computing;partition refinement;computer science;graph partition;theoretical computer science	HPC	-3.2600337041680625	38.88586669626294	71191
a0e91cbb949daa657ac188a4de3f5df76e130097	aiscale — a coarse grained reconfigurable cnn hardware accelerator		In this paper we propose a novel CNN hardware accelerator, called AlScale, capable of accelerating convolutional, pooling, fully-connected and adding CNN layers. In contrast to most existing solutions, AIScale offers a complete solution to the full CNN acceleration. AIScale is designed as a coarse-grained reconfigurable architecture, which uses rapid, dynamic reconfiguration during the CNN layer processing. Furthermore, a novel algorithm for mapping computations to the available computing resources enables AIScale to achieve higher utilization ratios than some of the previously proposed solutions. Results of the experiments indicate that the AIScale architecture is 1.16 to 2.73 times faster and consumes from 25% to 45% less energy on DRAM data transfers than the previously proposed MIT's Eyeriss CNN accelerator while using an identical number of computing units and having almost identical on-chip RAM memory size.	algorithm;computation;dynamic random-access memory;experiment;hardware acceleration;memory management	Rastislav J. R. Struharik;Bogdan Vukobratovic	2017	2017 IEEE East-West Design & Test Symposium (EWDTS)	10.1109/EWDTS.2017.8110048	architecture;computer science;memory management;parallel computing;control reconfiguration;kernel (linear algebra);computation;pooling;hardware acceleration	EDA	3.300772122142421	43.344295261323936	71305
40a4218ca6a688fc743d3840cfc06d0daeb38e52	adaptive bacterial foraging driven datapath optimization: exploring power-performance tradeoff in high level synthesis	hls;bacterial foraging;chemotaxis;elimination dispersal;power	An automated exploration of datapath for power-delay tradeoff in high level synthesis (HLS) driven by bacterial foraging optimization algorithm (BFOA) is proposed in this paper. The proposed exploration approach is simulated to operate in the feasible temperature range of an actual Escherichia coli (E. coli) bacterium in order to mimic its biological lifecycle. The proposed work transforms a regular BFOA into an adaptive DSE framework that is capable to explore power-performance tradeoffs during HLS. The key sub-contributions of the proposed methodology are as follows: (a) Novel chemotaxis driven exploration drift algorithm; (b) Novel multi-dimensional bacterium encoding scheme to handle the DSE problem; (c) A novel replication algorithm customized to the DSE problem for manipulating the position of the bacterium by keeping the resource information constant (useful for inducing exploitative ability in the algorithm); (d) A novel elimination-dispersal (ED) algorithm to introduce diversity during the exploration process; (e) Adaptive mechanisms such as resource clamping and step size clamping to handle boundary outreach problem during exploration. Finally, results indicated an average improvement in QoR of >?35% and reduction in runtime of >?4% compared to recent approaches.	datapath;high-level programming language;high-level synthesis;mathematical optimization	Saumya Bhadauria;Anirban Sengupta	2015	Applied Mathematics and Computation	10.1016/j.amc.2015.07.042	simulation;power;chemotaxis	EDA	6.815951494250503	37.653763062189235	71997
78b8b0498deb90bf5bcea10f9ba560494e751f86	high-performance direct pairwise comparison of large genomic sequences	databases;microprocessors;biology computing;genomics;data parallel;sequences;pairwise comparison;high performance direct pairwise comparison;application software;high performance computing;data parallel algorithm;genomics algorithm;comparative genomics;sequences biology computing genetics parallel algorithms;genomics bioinformatics application software microprocessors vector processors open systems computer science databases filtering algorithms parallel processing;altivec;genetics;comparative genomics dot plot data parallel pairwise comparison sequence alignment vector processor altivec high performance computing;filtering algorithms;dot plot algorithm;genome comparison;altivec high performance direct pairwise comparison large genomic sequences data parallel algorithm genomics algorithm dot plot algorithm sequence alignment vector processor;dot plot;high performance computer;experimental model;sequence alignment;computer science;large genomic sequences;vector processor;high throughput;open systems;high performance;vector processors;parallel processing;genome sequence;bioinformatics;parallel algorithms	Many applications in comparative genomics lend themselves to implementations that take advantage of common high-performance features in modern microprocessors. However, the common suggestion that a data-parallel, multithreaded, or high-throughput implementation is possible often ignores the complexity of actually creating such software. In this paper, we present two parallel algorithms for a classic comparative genomics algorithm, the dot plot. First, we describe a data-parallel algorithm that achieves speedups of up to 14.4x over the sequential version for large genomic comparisons. Then, we use the new algorithm as the base for a coarse-grained parallel version, suitable for multiprocessor and cluster environments, that scales linearly with the number of processors. These speedups introduce the opportunity to perform full pairwise comparisons on entire genomes on a much larger scale than previously possible. We also present the experimental, model-driven approach used to develop the algorithm that allowed us to carefully study and evaluate implementation options and to fully understand the parameters affecting its performance	central processing unit;computer cluster;dot plot (bioinformatics);high-throughput computing;microprocessor;model-driven architecture;multiprocessing;parallel algorithm;thread (computing);throughput	Christopher Mueller;Mehmet M. Dalkilic;Andrew Lumsdaine	2005	IEEE Transactions on Parallel and Distributed Systems	10.1109/IPDPS.2005.246	high-throughput screening;pairwise comparison;parallel processing;altivec;genomics;application software;vector processor;parallel computing;whole genome sequencing;computer science;bioinformatics;theoretical computer science;dot plot;sequence alignment;sequence;parallel algorithm;open system;comparative genomics	HPC	-0.7749508372940488	42.921686087302675	72116
a2d08e953b655c24c647da1b78a13fff517c5b8f	performance of the hough transform on a distributed memory multiprocessor	distributed memory;parallel algorithm;performance evaluation;distributed memory machine;parallel machines;hough transform	The Hough transform is a projection-based transform which can be used to detect shapes in images. One of the disadvantages of the transform is its requirement for large amounts of computing power. Parallel machines have given programmers the potential for incredible computing power. To obtain maximum performance from parallel machines, parallel algorithms should be designed to reflect the architecture of the parallel machine. The work reported in this paper compares the performance obtained in running several parallel versions of the Hough transform on a Fujitsu AP1000 distributed memory multiprocessor. q 1999 Published by Elsevier Science B.V.	accumulator (computing);central processing unit;distributed memory;hough transform;information source;multicast;multiprocessing;parallel algorithm;parallel computing;programmer;speedup;system administrator;the australian	Austin Underhill;Mohammed Atiquzzaman;John Ophel	1999	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/S0141-9331(98)00093-3	hough transform;parallel computing;distributed memory;computer science;theoretical computer science;parallel algorithm;computer graphics (images)	HPC	-3.281125235402783	39.42385901772925	72262
2cfd764d311939ae92b36301204d25955e5df759	ultrasonic signal acquisition and processing platform based on zynq soc	ultrasonic signal processing ultrasonic signal acquisition high voltage pulse generation xilinx zynq soc sampling control unit digital signal processing module arm processor field programmable gate array fpga multilevel voltage power supply usap platform ultrasonic testing systems;signal acquisition and processing;fpga;ultrasonic system;zynq soc;signal acquisition and processing ultrasonic system zynq soc fpga;acoustics field programmable gate arrays hardware signal processing power supplies testing transducers;ultrasonics acoustic signal detection acoustic signal processing field programmable gate arrays	Ultrasonic testing systems have been widely used in medical and industrial fields, demanding strict requirements for processing speed and re-configurability of the hardware system. This paper presents a prototype of a reconfigurable, high performance, low cost and real-time Ultrasonic Signal Acquisition and Processing (USAP) platform based on Zynq System-on-Chip (SoC). The USAP platform consists of five major components: Multi-level voltage power supply; High voltage pulse generation; Ultrasonic signal acquisition; Sampling control unit and Digital signal processing module. The system uses Xilinx Zynq SoC as main processor and controller. It combines both ARM processor and field-programmable gate array (FPGA) on the same chip, which makes the system capable of doing complex system configuration and performing high speed data processing. With this arrangement, the developed system is highly reconfigurable, where software and hardware configuration can be changed on both ARM and FPGA. This paper presents a system design flow of a complete ultrasonic testing system which is capable of acquiring ultrasonic data and performing advanced signal processing algorithms in real-time.	arm architecture;algorithm;central processing unit;complex system;control unit;digital signal processing;field-programmability;field-programmable gate array;mpsoc;multi-level governance;power supply;prototype;real-time clock;real-time transcription;requirement;sampling (signal processing);system configuration;system on a chip;systems design	Boyang Wang;Jafar Saniie	2016	2016 IEEE International Conference on Electro Information Technology (EIT)	10.1109/EIT.2016.7535282	embedded system;electronic engineering;computer hardware;computer science;engineering;field-programmable gate array	Robotics	4.1020157662634285	46.05306948268559	72352
667ca60ed5da22119f6d83aa9a3ddf00cf9fcf44	task scheduling for gpu accelerated hybrid olap systems with multi-core support and text-to-integer translation	shared memory systems application program interfaces competitive intelligence data mining data structures graphics processing units parallel processing query processing scheduling;query processing;processor scheduling;scheduling olap gpu acceleration openmp;olap;data mining;arrays;shared memory systems;performance measurement task scheduling gpu accelerated hybrid olap system multicore support online analytical processing business intelligence application multidimensional data structure multifaceted analytical query answering molap cube openmp text to integer translation method string processing parallel cpu implementation multicore processor shared memory system preprocessing partition;estimation;gpu acceleration;data structures;scheduling;graphics processing units;application program interfaces;graphics processing unit instruction sets estimation bandwidth dictionaries processor scheduling arrays;dictionaries;openmp;competitive intelligence;bandwidth;graphics processing unit;parallel processing;instruction sets	OLAP (On-Line Analytical Processing) is a powerful method for analyzing the excessive amount of data related to business intelligence applications. OLAP utilizes the efficient multidimensional data structure referred to as the OLAP cube to answer multi-faceted analytical queries. As queries become more complex and the dimensionality and size of the cube grows, the processing time required to aggregate queries increases. In this paper, we are proposing: (1) a parallel implementation of MOLAP cube using OpenMP, (2) a text-to-integer translation method to allow effective string processing on GPU, and (3) a new scheduling algorithm that support these new features. To be able to process string queries on the GPU, we are introducing a text-to-integer translation method which works with multiple dictionaries. The translation is necessary only for the GPU side of the system. To support the translation and parallel CPU implementation, a new scheduling algorithm is proposed. The scheduler divides multi-core processor(s) of a shared memory system into a processing partition and a preprocessing (or translation) partition. The performance of the new system is evaluated. The text-to-integer translation adds a new vital functionality to our system, however it also slows down the GPU processing by 7% when compare to original implementation without string support. The performance measurements indicate that due to the parallel implementation, the processing rate of the CPU partition improves from 12 to 110 queries per second. Moreover, the CPU partition is now able to process OLAP cubes of size 32 GB at rate of 11 queries per second. The total performance of the entire hybrid system (CPU + GPU) increased from 102 to 228 queries per second.	aggregate data;algorithm;central processing unit;column (database);data structure;dictionary;empty string;faceted classification;graphics processing unit;hybrid system;multi-core processor;olap cube;online analytical processing;openmp;precomputation;preprocessor;random-access memory;requirement;scheduling (computing);shared memory;string (computer science);thread (computing);windows task scheduler	Maria Malik;Lubomir Riha;Colin Shea;Tarek A. El-Ghazawi	2012	2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum	10.1109/IPDPSW.2012.259	parallel processing;estimation;parallel computing;competitive intelligence;online analytical processing;computer science;theoretical computer science;operating system;instruction set;database;distributed computing;programming language;scheduling;bandwidth	DB	-0.7530724663479553	42.136126434754566	72815
c0038a450dc0b0bd9fd448d8922d8ae6381dac39	cross-environment comparison of a bioinformatics pipeline: perspectives for hybrid computations		In this work a previously published bioinformatics pipeline was reimplemented across various computational platforms, and the performances of its steps evaluated. The tested environments were: (I) dedicated bioinformatics-specific server (II) low-power single node (III) HPC single node (IV) virtual machine. The pipeline was tested on a use case of the analysis of a single patient to assess single-use performances, using the same configuration of the pipeline to be able to perform meaningful comparison and search the optimal environment/hybrid system configuration for biomedical analysis. Performances were evaluated in terms of execution wall time, memory usage and energy consumption per patient. Our results show that, albeit slower, low power single nodes are comparable with other environments for most of the steps, but with an energy consumption two to four times lower. These results indicate that these environments are viable candidates for bioinformatics clusters where long term efficiency is a factor.		Nico Curti;Enrico Giampieri;Andrea Ferraro;Maria Cristina Vistoli;Elisabetta Ronchieri;Daniele Cesini;Barbara Martelli;Cristina Duma Doina;Gastone C. Castellani	2018		10.1007/978-3-030-10549-5_50	parallel computing;energy consumption;computation;hybrid system;virtual machine;computer science;bioinformatics;distributed computing	HPC	-3.3627845946520356	44.50100857056695	73000
f2302cfd4b0848eef42033acdd46e74eb32b8732	session wa6a: computational aspects in array processing	array processing		array processing;computation	Christ D. Richmond	2011		10.1109/ACSSC.2011.6190382	theoretical computer science;electronic engineering;computer science;array processing	ML	7.476701674481799	38.993377390704076	73074
491c7585c839cd5217b85db6beadf1bb6f7e36b8	arithmetic on a distributed-memory quantum multicomputer	quantum computer architecture;distributed memory;quantum teleportation;network topology;quantum computer;quantum physics;quantum computing	We evaluate the performance of quantum arithmetic algorithms run on a distributed quantum computer (a quantum multicomputer). We vary the node capacity and I/O capabilities, and the network topology. The tradeoff of choosing between gates executed remotely, through “teleported gates” on entangled pairs of qubits (telegate), versus exchanging the relevant qubits via quantum teleportation, then executing the algorithm using local gates (teledata), is examined. We show that the teledata approach performs better, and that carry-ripple adders perform well when the teleportation block is decomposed so that the key quantum operations can be parallelized. A node size of only a few logical qubits performs adequately provided that the nodes have two transceiver qubits. A linear network topology performs acceptably for a broad range of system sizes and performance parameters. We therefore recommend pursuing small, high-I/O bandwidth nodes and a simple network. Such a machine will run Shor's algorithm for factoring large numbers efficiently.	distributed memory;input/output;integer factorization;network topology;parallel computing;quantum computing;quantum teleportation;qubit;ripple effect;shor's algorithm;transceiver	Rodney Van Meter;William J. Munro;Kae Nemoto;Kohei M. Itoh	2008	JETC	10.1145/1324177.1324179	parallel computing;quantum information;quantum teleportation;computer science;theoretical computer science;quantum network;loss–divincenzo quantum computer;quantum capacity;distributed computing;quantum convolutional code;quantum channel;quantum computer;quantum algorithm;algorithm;one-way quantum computer;quantum mechanics;quantum phase estimation algorithm;quantum sort;quantum error correction	Arch	0.6847315578290717	37.93011554553542	73125
4948f1908079c295f48c948eeea12879c4716075	sipe: small integer plus exponent	libraries;mini library;computers;subtraction;ieee standards;standards;radix 2;c header file;low precision;arithmetic operations;correct rounding;libraries standards timing hardware context computers algorithm design and analysis;ieee standards floating point arithmetic;sipe;small integer plus exponent;gnu mpfr;mini library sipe small integer plus exponent c header file floating point computations radix 2 addition subtraction multiplication fma timing comparisons hardware ieee 754 floating point gnu mpfr;c language;floating point computations;hardware ieee 754 floating point;fma;addition;timing comparisons;floating point arithmetic;multiplication;context;algorithm design and analysis;correct rounding low precision arithmetic operations;hardware;timing	SIPE (Small Integer Plus Exponent) is a mini-library in the form of a C header file, to perform floating-point computations in very low precisions with correct rounding to nearest in radix 2. The goal of such a tool is to do proofs of algorithms/properties or computations of tight error bounds in these precisions by exhaustive tests, in order to try to generalize them to higher precisions. The currently supported operations are addition, subtraction, multiplication (possibly with the error term), FMA, and miscellaneous comparisons and conversions. Timing comparisons have been done with hardware IEEE-754 floating point and with GNU MPFR.	algorithm;computation;fma instruction set;gnu mpfr;include directive;rounding	Vincent Lefèvre	2013	2013 IEEE 21st Symposium on Computer Arithmetic	10.1109/ARITH.2013.22	algorithm design;parallel computing;subtraction;include directive;computer hardware;computer science;floating point;theoretical computer science;operating system;mathematics;addition;multiplication;algorithm;algebra;first-mover advantage	Theory	7.792383093671005	43.036568789093224	73292
3ed9b038b01a4102f45ed04cb22b2d88db286318	a hybrid chaotic and number theoretic approach for securing dicom images		The advancements in telecommunication and networking technologies have led to the increased popularity and widespread usage of telemedicine. Telemedicine involves storage and exchange of large volume of medical records for remote diagnosis and improved health care services. Images in medical records are characterized by huge volume, high redundancy, and strong correlation among adjacent pixels. This research work proposes a novel idea of integrating number theoretic approach with Henon map for secure and efficient encryption. Modular exponentiation of the primitive roots of the chosen prime in the range of its residual set is employed in the generation of two-dimensional array of keys.The keymatrix is permuted and chaotically controlled byHenonmap to decide the encryption keys for every pixel of DICOM image. The proposed system is highly secure because of the randomness introduced due to the application of modular exponentiation key generation and application of Henon maps for permutation of keys. Experiments have been conducted to analyze key space, key sensitivity, avalanche effect, correlation distribution, entropy, and histograms. The corresponding results confirm the strength of the proposed design towards statistical and differential crypt analysis.The computational requirements for encryption/decryption have been reduced significantly owing to the reduced number of computations in the process of encryption/decryption.	array data structure;avalanche effect;computation;dicom;dm-crypt;encryption;hénon map;key generation;key space (cryptography);modular exponentiation;pixel;randomness;requirement;theory	Jeyamala Chandrasekaran;S. J. Thiruvengadam	2017	Security and Communication Networks	10.1155/2017/6729896	40-bit encryption;theoretical computer science;data mining;computer security;probabilistic encryption	Security	7.059552765427284	34.95457327368942	73338
5b3aa65922bc90b6a166ec81a81c8171b47caa37	bridging the semantic gaps of gpu acceleration for scale-out cnn-based big data processing: think big, see small	libraries;distributed system;paper;neural networks;distributed computing;semantics;gpu;acceleration;cuda;computer architecture;big data;deep learning;graphics processing units;nvidia;tesla k20;computer science;analytical model	Convolutional Neural Networks (CNNs) have substantially advanced the state-of-the-art accuracies of object recognition, which is the core function of a myriad of modern multimedia processing techniques such as image/video processing, speech recognition, and natural language processing. GPU-based accelerators gained increasing attention because a large amount of highly parallel neurons in CNN naturally matches the GPU computation pattern. In this work, we perform comprehensive experiments to investigate the performance bottlenecks and overheads of current GPU acceleration platform for scale-out CNN-based big data processing.  In our characterization, we observe two significant semantic gaps: framework gap that lies between CNN-based data processing workflow and data processing manner in distributed framework; and the standalone gap that lies between the uneven computation loads at different CNN layers and fixed computing capacity provisioning of current GPU acceleration library. To bridge these gaps, we propose D3NN, a Distributed, Decoupled, and Dynamically tuned GPU acceleration framework for modern CNN architectures. In particular, D3NN features a novel analytical model that enables accurate time estimation of GPU accelerated CNN processing with only 5-10% error. Our evaluation results show the throughput of standalone processing node using D3NN gains up to 3.7X performance improvement over current standalone GPU acceleration platform. Our CNN-oriented GPU acceleration library with built-in dynamic batching scheme achieves up to 1.5X performance improvement over the non-batching scheme and outperforms the state-of-the-art deep learning library by up to 28% (performance mode) ~ 67% (memory-efficient mode).	big data;bridging (networking);computation;convolutional neural network;deep learning;experiment;graphics processing unit;natural language processing;neural networks;outline of object recognition;provisioning;scalability;speech recognition;throughput;video processing	Mingcong Song;Yang Hu;Yunlong Xu;Chao Li;Huixiang Chen;Jingling Yuan;Tao Li	2016	2016 International Conference on Parallel Architecture and Compilation Techniques (PACT)	10.1145/2967938.2967944	acceleration;parallel computing;big data;computer science;theoretical computer science;operating system;semantics;deep learning;programming language;computer graphics (images)	DB	3.032667635710035	43.250510626838256	73898
59d0a239d54fb122032443184c7169039920411e	analyzing graphics processor unit (gpu) instruction set architectures	instruction sets computer architecture graphics processing units;processor design graphics processor unit analysis massively parallel architectures generous purpose computing gpu instruction set architectures isa;会议论文;assembly;computer architecture;registers;graphics processing units;process control;graphics processing units registers computer architecture benchmark testing process control hardware assembly;benchmark testing;hardware	Because of their high throughput and power efficiency, massively parallel architectures like graphics processing units (GPUs) become a popular platform for generous purpose computing. However, there are few studies and analyses on GPU instruction set architectures (ISAs) although it is wellknown that the ISA is a fundamental design issue of all modern processors including GPUs.	central processing unit;computer graphics;graphics processing unit;performance per watt;throughput	Kothiya Mayank;Hongwen Dai;Jizeng Wei;Huiyang Zhou	2015	2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)	10.1109/ISPASS.2015.7095794	benchmark;computer architecture;graphics pipeline;parallel computing;computer hardware;computer science;operating system;instruction set;process control;real-time computer graphics;assembly;processor register;graphics hardware;general-purpose computing on graphics processing units	Arch	-2.7894943136125554	45.24838975703393	73953
c16c4fa113cd2c93f7557e05831039ed1436735a	drisa: a dram-based reconfigurable in-situ accelerator		Data movement between the processing units and the memory in traditional von Neumann architecture is creating the “memory wall” problem. To bridge the gap, two approaches, the memory-rich processor (more on-chip memory) and the compute-capable memory (processing-in-memory) have been studied. However, the first one has strong computing capability but limited memory capacity/bandwidth, whereas the second one is the exact the opposite.To address the challenge, we propose DRISA, a DRAM-based Reconfigurable In-Situ Accelerator architecture, to provide both powerful computing capability and large memory capacity/bandwidth. DRISA is primarily composed of DRAM memory arrays, in which every memory bitline can perform bitwise Boolean logic operations (such as NOR). DRISA can be reconfigured to compute various functions with the combination of the functionally complete Boolean logic operations and the proposed hierarchical internal data movement designs. We further optimize DRISA to achieve high performance by simultaneously activating multiple rows and subarrays to provide massive parallelism, unblocking the internal data movement bottlenecks, and optimizing activation latency and energy. We explore four design options and present a comprehensive case study to demonstrate significant acceleration of convolutional neural networks. The experimental results show that DRISA can achieve 8.8× speedup and 1.2× better energy efficiency compared with ASICs, and 7.7× speedup and 15× better energy efficiency over GPUs with integer operations.CCS CONCEPTS• Hardware → Dynamic memory; • Computer systems organization → reconfigurable computing; Neural networks;	application-specific integrated circuit;artificial neural network;bandwidth (signal processing);bitwise operation;convolutional neural network;dynamic random-access memory;functional completeness;graphics processing unit;in-memory database;logical connective;parallel computing;speedup;von neumann architecture	Shuangchen Li;Dimin Niu;Krishna T. Malladi;Hongzhong Zheng;Bob Brennan;Yuan Xie	2017		10.1145/3123939.3123977	parallel computing;memory controller;computer science;real-time computing;semiconductor memory;registered memory;flat memory model;sense amplifier;interleaved memory;computer memory;computing with memory	Arch	3.127287923829471	43.94298136889399	74164
ffb215bff105fd476659d6f7b7f5f098fbf964e0	parallel processing videos in very large digital libraries	replayme;dspace;conference contribution;vldl;open mpi;greenstone;parallel processing	Nowhere are the ‘growing pains’ of Very Large-scale Digital Libraries more pronounced than in collections containing multimedia data. Not only do such collections contain large numbers of items, but they also push the boundaries of scale in terms of storage space and processing expense. In this paper we explore how applying parallel processing opensource libraries and techniques—previously developed for and applied to textual content—can be of benefit to multimedia digital libraries. We provide a real-world use case of ingesting video into the ReplayMe! system, an extension of the Greenstone digital library software, that simultaneously records and ingests all of the free-to-air television channels available in New Zealand. Current ingest of video in ReplayMe! is intentionally light due to processing time constraints on the single processor architecture it was developed on. The work reported here investigates how this system can be scaled up to include the conversion of the broadcast video transport format to a suitable a streaming format (MP4) and to automatically extract content analysis based keyframes, while still performing within real-time. By applying parallel processing, and utilizing a cluster of sixteen desktop computers, the paper shows how this processing time can be significantly reduced compared to the equivalent computation if conducted serially. We then generalize the work, and show how the same basic techniques can be applied to other common digital library software such as DSpace to provide similar advantages when dealing with processor intensive content.	central processing unit;computation;dspace;desktop computer;digital library;digital video;key frame;library (computing);multi-core processor;open-source software;parallel computing;preprocessor;real-time clock;requirement;television channel	John Thompson;David Bainbridge;Maxime Roüast	2012		10.1007/978-3-642-34752-8_28	very low-density lipoprotein;parallel processing;parallel computing;computer science;theoretical computer science;dspace;computer graphics (images)	OS	-0.09562468906761261	42.31105248734742	74275
41cd06ee86e85a988d3393ee0d59c45a80e68cf0	memory-based scheduling for a parallel multifrontal solver	dynamic scheduling parallel memories;memory management;sparse direct solvers;mumps parallel multifrontal solver;processor scheduling;dependence graph;assembly;iterative methods;memory based scheduling;tasks dependency graph memory based scheduling mumps parallel multifrontal solver sparse direct solvers parallel direct solver;robustness;scalability;parallel direct solver;dynamic scheduling assembly memory management large scale systems sparse matrices robustness equations iterative methods scalability processor scheduling;sparse matrices;parallel memories;large scale problem;tasks dependency graph;large scale systems;dynamic scheduling	Summary form only given. The memory usage of sparse direct solvers can be the bottleneck to solve large-scale problems. We describe dynamic scheduling strategies that aim at reducing the memory usage of a parallel direct solver. Combined to static modifications of the tasks dependency graph, experiments show that such techniques have a good potential to improve the memory usage of a parallel multifrontal solver, MUMPS.	experiment;frontal solver;input/output;mumps;out-of-core algorithm;random-access memory;run time (program lifecycle phase);scalability;scheduling (computing);sparse matrix;tree (data structure)	Abdou Guermouche;Jean-Yves L'Excellent	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303001	parallel computing;scalability;sparse matrix;dynamic priority scheduling;computer science;theoretical computer science;operating system;assembly;distributed computing;iterative method;programming language;robustness;memory management	HPC	-3.5514649983937794	41.38545996008995	74305
edd801c7e42e344169b27ffed5bb07f2fe521ec8	the implementation and optimization of bitonic sort algorithm based on cuda	paper;sorting;cuda;nvidia;algorithms;computer science	This paper describes in detail the bitonic sort algorithm,and implements the bitonic sort algorithm based on cuda architecture.At the same time,we conduct two effective optimization of implementation details according to the characteristics of the GPU,which greatly improve the efficiency. Finally,we survey the optimized Bitonic sort algorithm on the GPU with the speedup of quick sort algorithm on the CPU.Since Quick Sort is not suitable to be implemented in parallel,but it is more efficient than other sorting algorithms on CPU to some extend.Hence,to see the speedup and performance,we compare bitonic sort on GPU with quick Sort on CPU. For a series of 32-bit random integer,the experimental results show that the acceleration of our work is nearly 20 times.When array size is about 2,the speedup ratio is even up to 30.	32-bit;64-bit computing;bitonic sorter;cuda;central processing unit;graphics processing unit;kernal;mathematical optimization;multi-core processor;quicksort;sorting algorithm;speedup	Qi Mu;Liqing Cui;Yufei Song	2015	CoRR		merge sort;adaptive sort;insertion sort;computer architecture;parallel computing;bucket sort;selection sort;hybrid algorithm;computer science;sorting;theoretical computer science;bitonic sorter;sorting algorithm;in-place algorithm;integer sorting;algorithm;stooge sort	HPC	-0.1161361589502134	41.05667600253844	74365
50f2e946cb0e17e64635c516c8fc3fbdbe51840d	hashing into jacobi quartic curves	field inversion;encodings;finite fields;timing attacks;会议论文;deterministic encoding;random oracle;hash function;jacobi quartic curves;s function	Jacobi quartic curves are well known for efficient arithmetics in regard to their group law and immunity to timing attacks. Two deterministic encodings from a finite field $$\mathbb {F}_q$$Fq to Jacobi quartic curves are constructed. When $$q\equiv 3\pmod 4$$qi¾ź3mod4, the first deterministic encoding based on Skalba's equality saves two field squarings compared with birational equivalence composed with Fouque and Tibouchi's brief version of Ulas' function. When $$q\equiv 2\pmod 3$$qi¾ź2mod3, the second deterministic encoding based on computing cube root costs one field inversion less than birational equivalence composed with Icart's function at the cost of four field multiplications and one field squaring. It costs one field inversion less than Alasha's encoding at the cost of one field multiplication and two field squarings. With these two deterministic encodings, two hash functions from messages directly into Jacobi quartic curves are constructed. Additionally, we construct two types of new efficient functions indifferentiable from a random oracle.	hash function;jacobi method;quartic function	Wei Yu;Kunpeng Wang;Bao Li;Xiaoyang He;Song Tian	2015		10.1007/978-3-319-23318-5_20	combinatorics;discrete mathematics;mathematics;algebra	Crypto	9.534030979373355	43.01509154369884	74575
1b0464f8dc0815bf91b7cbf6a8927c3cb4727c3b	closed-form mapping conditions for the synthesis of linear processor arrays	condition dependence;space time;data dependence	This paper addresses the problem of mapping algorithms with constant data dependences to linear processor arrays. The closed-form necessary and suucient mapping conditions are derived to identify mappings without computational connicts and data link collisions. These mapping conditions depend on the space-time mapping matrix and the problem size parameters only. Their correctness can be veriied in constant time that is independent of problem size. The design of optimal linear processor arrays is formulated as a mathematic programming problem, which can be solved eeciently by a systematic enumeration of a polynomial search space.	algorithm;analysis of algorithms;correctness (computer science);polynomial;time complexity	Jingling Xue	1995	VLSI Signal Processing	10.1007/BF02407035	mathematical optimization;combinatorics;discrete mathematics;space time;mathematics	Theory	4.433609461879482	37.70800302668081	74799
659afb284b6f9b073f740a1aad316791d4b3fba5	linking parallel algorithmic thinking to many-core memory systems and speedups for boosted decision trees		The current focus of research on parallel computing takes current commercial hardware for granted. Here, we consider an alternative approach: start with a time-tested algorithmic theory and develop a supporting computer architecture and toolchain. This paper focuses on the hybrid memory architecture of this computer platform, which is designed to efficiently support execution of both serial and parallel code and switching between the two. A key part of this architecture is a flexible all-to-all interconnection network that connects processors to shared memory modules. To understand some recent advances in GPU memory architecture and how they relate to this hybrid memory architecture, we use microbenchmarks including list ranking.  A second part of this work contrasts the scalability of applications with that of routines. In particular, regardless of the scalability needs of full applications, some routines may involve smaller problem sizes, and in particular smaller levels of parallelism, perhaps even serial. To see how a hybrid memory architecture can benefit such applications, we simulate a computer with such an architecture and demonstrate the potential for a speedup of 3.3X over NVIDIA's most powerful GPU to date for XGBoost, an implementation of boosted decision trees, a timely machine learning approach.		James Alexander Edwards;Uzi Vishkin	2018		10.1145/3240302.3240321	memory architecture;architecture;speedup;toolchain;scalability;computer architecture;list ranking;shared memory;computer science;alternating decision tree	Arch	-3.371393303047094	43.562435816210844	74834
8a5b3e11848ce58a4532e87f1df2917b2d370c0b	collusion-traceable secure multimedia distribution based on controllable modulation	filigranage numerique;protection information;collusion;digital watermarking;demodulacion;security of data multimedia communication multimedia computing;watermarking;digital fingerprint;gestion de derechos de autor digitales;image recognition;reconocimiento imagen;collusion resistant fingerprint code;collusion attack;multimedia;encryption;fourniture information;biometrie;random sequences;authentication;biometrics;pseudorandom sequences;biometria;cifrado;modulacion;securite donnee;information delivery;digital rights management;multimedia computing;demodulation process;sucesion seudo aleatoria;protection;codificacion;entrega informacion;automatic recognition;suite pseudoaleatoire;colusion;demodulation;internet;content distribution;cryptage;proteccion informacion;gestion des droits numeriques;secure multimedia content distribution collusion traceable secure multimedia distribution controllable modulation pseudorandom sequences server side unintelligible multimedia content demodulation process collusion resistant fingerprint code;fingerprint recognition;cryptography;digital fingerprinting;unintelligible multimedia content;information protection;robustesse;dactyloscopie;filigrana digital;modulation coding;video encryption;multimedia communication;coding;reconnaissance image;pattern recognition;secure multimedia content distribution;pseudorandom sequence;robustness;collusion traceable secure multimedia distribution;reconnaissance forme;controllable modulation;reconocimiento patron;communication multimedia;fingerprint recognition protection cryptography watermarking modulation coding random sequences demodulation robustness internet authentication;secure multimedia distribution;server side;security of data;watermarking collusion attack digital fingerprint digital rights management secure multimedia distribution video encryption;digital right management;fingerprint identification;reconocimiento automatico;codage;reconnaissance automatique;robustez;modulation	In this paper, a secure multimedia distribution scheme resistant to collusion attacks is proposed. In this scheme, the multimedia content is modulated by n pseudorandom sequences at the server side, which generates the unintelligible multimedia content, and then demodulated under the control of the fingerprint code at the customer side, which produces the multimedia content contains a unique code. The demodulation process adopts collusion-resistant fingerprint codes to determine which sequences will be removed from the received multimedia content. Since the collusion-resistant fingerprint code is used, the colluders who combine different copies together can be detected. Compared with existing schemes, the collusion-resistant code is used in the proposed scheme, which confirms the robustness against collusion attacks. This scheme provides a good choice for secure multimedia content distribution.	digital distribution;encryption;experiment;fingerprint;mpeg-2;modulation;performance;pseudorandomness;qr code;server (computing);server-side;traceability;utility functions on indivisible goods	Shiguo Lian;Zhiquan Wang	2008	IEEE Transactions on Circuits and Systems for Video Technology	10.1109/TCSVT.2008.2002829	computer vision;telecommunications;digital watermarking;computer science;electrical engineering;internet privacy;computer security	Security	8.089356030260442	34.70336537957651	74866
03bd700053c3f654156138e55984cd15ac85670f	parallel orthogonal factorizations of large sparse matrices on distributed -memory multiprocessors	sparse matrices;distributed memory		distributed memory;sparse matrix	Thomas F. Coleman;Chunguang Sun	1993			parallel computing;theoretical computer science;distributed memory;sparse matrix;sparse approximation;computer science	HPC	-2.5977229594476188	38.47324224242766	75057
28e0ff40da62e7443064642a0212eff80489209c	parallelizing partial digest problem on multicore system		The partial digest problem, PDP, is one of the methods used in restriction mapping to characterize a fragment of DNA. The main challenge of PDP is the exponential time for the best exact sequential algorithm in the worst case. In this paper, we reduce the running time for generating the solution of PDP by designing an efficient parallel algorithm. The algorithm is based on parallelizing the fastest sequential algorithm for PDP. The experimental study on a multicore system shows that the running time of the proposed algorithm decreases with the number of processors increases. Also, the speedup achieved good scales with increase in the number of processors.	automatic parallelization;digest access authentication;multi-core processor;symmetric multiprocessing	Hazem M. Bahig;Mostafa M. Abbas;M. M. Mohie-Eldin	2017		10.1007/978-3-319-56154-7_10	artificial intelligence;computer vision;parallel computing;engineering;multi-core processor;scalability;speedup;sequential algorithm;parallel algorithm;restriction map	Logic	0.955362606151649	40.55269663175787	75119
42cb500244bfab454f1f01e41e2f910bb6fab9e0	jparent: parallel entropy decoding for jpeg decompression on heterogeneous multicore architectures		SummaryrnThe JPEG format employs Huffman codes to compress the entropy data of an image. Huffman codewords are of variable length, which makes parallel entropy decoding a difficult problem. To determine the start position of a codeword in the bitstream, the previous codeword must be decoded first. We present JParEnt, a new approach to parallel entropy decoding for JPEG decompression on heterogeneous multicores. JParEnt conducts JPEG decompression in two steps: (1) an efficient sequential scan of the entropy data on the CPU to determine the start-positions (boundaries) of coefficient blocks in the bitstream, followed by (2) a parallel entropy decoding step on the graphics processing unit (GPU). The block boundary scan constitutes a reinterpretation of the Huffman-coded entropy data to determine codeword boundaries in the bitstream. We introduce a dynamic workload partitioning scheme to account for GPUs of low compute power relative to the CPU. This configuration has become common with the advent of SoCs with integrated graphics processors (IGPs). We leverage additional parallelism through pipelined execution across CPU and GPU. For systems providing a unified address space between CPU and GPU, we employ zero-copy to completely eliminate the data transfer overhead.rnrnOur experimental evaluation of JParEnt was conducted on six heterogeneous multicore systems: one server and two desktops with dedicated GPUs, one desktop with an IGP, and two embedded systems. For a selection of more than 1000 JPEG images, JParEnt outperforms the SIMD–implementation of the libjpeg-turbo library by up to a factor of 4.3×, and the previously fastest JPEG decompression method for heterogeneous multicores by up to a factor of 2.2×. JParEntu0027s entropy data scan consumes 45% of the entropy decoding time of libjpeg-turbo on average. Given this new ratio for the sequential part of JPEG decompression, JParEnt achieves up to 97% of the maximum attainable speedup (95% on average).rnrnOn the IGP-based desktop platform, JParEnt achieves energy savings of up to 45% compared to libjpeg-turbou0027s SIMD-implementation.	data compression;jpeg;multi-core processor	Wasuwee Sodsong;Minyoung Jung;Jinwoo Park;Bernd Burgstaller	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4111	parallel computing;huffman coding;computer science;speedup;graphics processing unit;real-time computing;decoding methods;multi-core processor;lossless jpeg;general-purpose computing on graphics processing units;jpeg	HPC	0.11928369396831252	44.161158846856914	75213
ab85a92a0e20b8ab167928cb361257cbe0c61d34	a new approach for computing multidimensional dft's on parallel machines and its implementation on the ipsc/860 hypercube	espacio n dimensiones;tratamiento paralelo;multidimensional space;traitement signal;hypercube;espace n dimensions;estacion trabajo;traitement parallele;multiprocessor;parallel architectures parallel algorithms hypercube networks discrete fourier transforms group theory;station travail;transformacion fourier discreta;discrete fourier transformation;data processing;cluster of workstations;supercomputer;group theory;supercomputador;transformation fourier discrete;workstation;calculateur mimd;parallel architectures;efficient implementation;concurrent computing multidimensional systems clustering algorithms supercomputers workstations data processing hypercubes timing optimization methods assembly;signal processing;indexation;parallel machines;prime number;discrete fourier transform multidimensional dft parallel machines implementation intel ipsc i860 hypercube interprocessor communications multiprocessor platforms mimd supercomputers clusters of workstation group theoretic concepts computational strategy reduced transform algorithm good thomas factorization non power of two sizes rta algorithm bookkeeping tool subproblems timing results nonoptimized realizations;multiprocesador;discrete fourier transforms;zero padding;procesamiento senal;parallel processing;mimd computer;superordinateur;hypercube networks;parallel algorithms;multiprocesseur;hipercubo	In this paper, we propose a new approach for computing multidimensional DFT’s that reduces interprocessor communications and is therefore suitable for efficient implementation on a variety of multiprocessor platforms including MIMD supercomputers and clusters of workstations. Group theoretic concepts are used to formulate a flexible computational strategy that hybrids the reduced transform algorithm (RTA) with the Good-Thomas factorization and can deal efficiently with nonpower-of-two sizes without resorting to zero-padding. The RTA algorithm is employed not as a data processing but rather as a bookkeeping tool in order to decompose the problem into many smaller size subproblems (lines) that can be solved independently by the processors. Implementation issues on an Intel iPSCli860 hypercube are discussed and timing results for large 2-D and 3-D DFTs with index sets in Z/.l[P x Z l I i P and Z / S x Z/lZfP x 1/ I i P respectively are provided, where -I-. .U. I< are powersof-two and P is a small prime number such as 3, 5, or 7. The nonoptimized realizations of the new hybrid RTA approach are shown to outperform by as much as 70% the optimized assembly coded realizations of the traditional row-column method on the iPSC/860. I . INTRODUCTION-MOTIVATION ARALLEL computing presents a new environment for P algorithm design and implementation, along with new challenges to the computational scientist. The performance of any given program depends on an increased number of parameters compared to the serial case, widening this way the difference between theoretical modeling and practical experience. In this paper, we present a strategy for computing a multidimensional DFT that hybrids a relatively new algorithm Manuscript received August 13, 1993; revised May 23, 1994. This work was supported by the Advanced Research Projects Agency of the U.S. Department of Defense and was monitored by the Air Force Office of Scientific Research under Contract number F49620-9 1-0098. The associatc editor coordinating the review of this paper and approving it for publication was Prof. Russell M. Mersereau. G. I . K.echriotis and M. An are with AWARE Inc., Cambridge, MA 02142 USA. M. Bletsaa and R. Tolimieri arc with Aware Inc.. Cambridge, MA 02142 USA, ancl with Communications and Digital Signal Processing (CDSP), Center for Research and Graduate Studies, Electrical and Computer Engineering Department, Northeastern Univmity. Boston, MA 02 I 15 USA. E. S. Manolakos is with Communications and Digital Signal Processing (CDSP). Center for Research and Graduate Studies, Electrical and Computer Engineering Department, Northeastern University, Boston, MA 021 15 USA. IEEE Log Number 9406900. (reduced transform algorithm) with already implemented single processor kernel routines. We will use the reduced transform algorithm to address the reduction and optimization of interprocessor communications. Our work has been mainly motivated from the distributed memory parallel computing paradigm, which is arguably the most difficult to harness due to its exposed interprocessor communication to the programmer. Most parallel computers require sophisticated algorithms and programming techniques for their optimum utilization. In this discussion, we will make use of algebraic facts in presenting the algorithms. The parameters in algebraic formulas give us the important implementation parameters. Thus, the flexibility to address the variables in implementations is equated with flexibility in manipulating algebraic formalism. Initial investment in familiarity with some amount of algebra may be necessary, but the payoff is immediate. Most of the relevant algebra, not in its most rigorous form but its usage, can be found in [ I ] . In its most general form, the reduced transform algorithm (RT,4) is a full utilization of the duality between periodic and decimated data in the Fourier transform. This duality was used partially in some algorithms and implementations for restricted cases [2]-[SI. A description of a generalization in a unified setting is found in [6] and [7], along with the work of M. Rofheart [8]. In this paper, we will consider the application of RTA to the case Z I P x Z I P , for a prime number P. Tensor product formulation of DFT computation on Z I P x Z / P x Z I P is interleaved with the periodization step in RTA for Z I P x Z I P to produce P + 1 independent data of size N P . We will use the RTA to address the imbalance between computation and communication rates in current distributed memory parallel machines by reducing communication between processors to collective patterns only (broadcast and combine) instead of the all-to-all communication patterns required in the global matrix transpose needed by the rowcolumn (RC) implementations of multidimensional DFT’s. In addition, since fast algorithms for prime size I-D DFT’s exist [ I ] and the case Z I P x Z I P of the RTA is very efficient because its computation requires only P + 1 I D transforms (versus 2P for the row column method), our approach addresses the issue of storage reduction by providing additional transform size options. For example, the ability to perform a 181 x 181-point 2-D DFT means potential storage I 053-587X/95$(l4.00	algorithm design;broadcasting (networking);central processing unit;computation;computational scientist;computer engineering;decimation (signal processing);digital signal processing;discrete fourier transform;distributed memory;formal system;inter-process communication;linear algebra;mimd;mathematical optimization;multiprocessing;parallel computing;programmer;programming paradigm;supercomputer;theory;time complexity;tridiagonal matrix algorithm;workstation	George Kechriotis;Myoung An;Michail Bletsas;Richard Tolimieri;Elias S. Manolakos	1995	IEEE Trans. Signal Processing	10.1109/78.365307	parallel processing;supercomputer;parallel computing;multiprocessing;workstation;data processing;computer science;theoretical computer science;signal processing;distributed computing;parallel algorithm;group theory;prime number;hypercube	HPC	-1.1673404256918638	36.67989870053621	75253
eabdfd6352587ed415ff0949b7e4397fab4e5272	fast evaluation of multivariate quadratic polynomials over gf(2^32) using grahpics processing units		QUAD stream cipher is a symmetric cipher based on multivariate public-key cryptography(MPKC), which uses multivariate polynomials as encryption keys. It holds the provable security property based on the computational hardness assumption. More specifically, the security of QUAD depends on the hardness of solving non-linear multivariate quadratic systems over a finite field, which is known as an NP-complete problem. However, QUAD is slower than other stream ciphers, and an efficient implementation, which has a reduced computational cost, is required. In this paper, we propose some implementations of QUAD over GF(232) on Graphics Processing Units(GPU) and compare them. Moreover, we provide fast multiplications over GF(232), the core operation of QUAD. Our implementation gives the fastest throughput of QUAD as 24.827 Mbps. We propose an efficient implementation for computing with multivariate polynomials in multivariate cryptography on GPU and evaluate the efficiency of the proposal. GPU is considered to be a commodity parallel arithmetic unit. Our proposal parallelizes an algorithm coming from multivariate cryptography, and makes it efficient by optimizing the algorithm with GPU.	algorithmic efficiency;arithmetic logic unit;computation;computational hardness assumption;data rate units;encryption;fastest;graphics processing unit;multivariate cryptography;np-completeness;nonlinear system;parallel computing;polynomial;provable security;public-key cryptography;stream cipher;symmetric-key algorithm;throughput	Satoshi Tanaka;Takanori Yasuda;Kouichi Sakurai	2014	J. Internet Serv. Inf. Secur.		discrete mathematics;quadratic equation;gf(2);multivariate statistics;polynomial;arithmetic;mathematics	Crypto	8.935622845908064	43.65848244158677	75329
6d01eb214cdb8b41ca25446b1ee58a4258621899	efficient analog circuits for boolean satisfiability		"""Efficient solutions to nonpolynomial (NP)-complete problems would significantly benefit both science and industry. However, such problems are intractable on digital computers based on the von Neumann architecture, thus creating the need for alternative solutions to tackle such problems. Recently, a deterministic, continuous-time dynamical system (CTDS) was proposed <xref ref-type=""""bibr"""" rid=""""ref1"""">[1]</xref> to solve a representative NP-complete problem, Boolean Satisfiability (SAT). This solver shows polynomial analog time-complexity on even the hardest benchmark <inline-formula> <tex-math notation=""""LaTeX"""">$k$ </tex-math></inline-formula>-SAT (<inline-formula> <tex-math notation=""""LaTeX"""">$k \geq 3$ </tex-math></inline-formula>) formulas, but at an energy cost through exponentially driven auxiliary variables. This paper presents a novel analog hardware SAT solver, <monospace>AC-SAT</monospace>, implementing the CTDS via incorporating novel, analog circuit design ideas. <monospace>AC-SAT</monospace> is intended to be used as a coprocessor and is programmable for handling different problem specifications. It is especially effective for solving hard <inline-formula> <tex-math notation=""""LaTeX"""">$k$ </tex-math></inline-formula>-SAT problem instances that are challenging for algorithms running on digital machines. Furthermore, with its modular design, <monospace>AC-SAT</monospace> can readily be extended to solve larger size problems, while the size of the circuit grows linearly with the product of the number of variables and the number of clauses. The circuit is designed and simulated based on a 32-nm CMOS technology. Simulation Program with Integrated Circuit Emphasis (SPICE) simulation results show speedup factors of ~10<sup>4</sup> on even the hardest 3-SAT problems, when compared with a state-of-the-art SAT solver on digital computers. As an example, for hard problems with <inline-formula> <tex-math notation=""""LaTeX"""">$N=50$ </tex-math></inline-formula> variables and <inline-formula> <tex-math notation=""""LaTeX"""">$M=212$ </tex-math></inline-formula> clauses, solutions are found within from a few nanoseconds to a few hundred nanoseconds."""	algorithm;analogue electronics;analysis of algorithms;application-specific integrated circuit;benchmark (computing);boolean satisfiability problem;cmos;central processing unit;circuit design;computer;coprocessor;cross-reference;discrete optimization;dynamical system;image noise;mathematical optimization;modular design;np-completeness;polynomial;spice;simulation;solver;speedup;time complexity;von neumann architecture	Xunzhao Yin;Behnam Sedighi;Melinda Varga;Mária Ercsey-Ravasz;Zoltán Toroczkai;Xiaobo Sharon Hu	2018	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2017.2754192	von neumann architecture;computer science;electronic engineering;theoretical computer science;speedup;integrated circuit;modular design;polynomial;circuit design;boolean satisfiability problem;solver	EDA	4.807570441072422	41.1309524661326	75390
3a3aef0a0547a2bf9e40f594b5db422ef8b279f9	metaoptimization of the in-lining priority function for a compiler targeting a polymorphous computing architecture	evolutionary computation;compiler optimization;polymorphous computing architectures	Leading polymorphous computing architecture (PCA) efforts include the Raw Architecture Workstation (Raw) and the Tera-op Reliable and Intelligently Adaptive Processing System (TRIPS), both of which are tile-based. The Raw toolchain places responsibility for program decomposition on the programmer, but the TRIPS toolchain automatically generates hyperblocks and allocates them to processing elements. This report identifies evolutionary computation (EC) techniques that enable and that are enabled by PCA technology, focusing on application of EC in enhancing the effectiveness of the TRIPS toolchain, including the Scale compiler. In particular, computational experiments are described that investigate the application of genetic programming to the meta-optimization of the priority function used to increase the number of instructions per hyperblock in the in-lining optimization phase of Scale. Results suggest continued experimentation with larger population sizes and more generations.	compiler;computer architecture;evolutionary computation;experiment;genetic programming;inline expansion;mathematical optimization;meta-optimization;programmer;toolchain;workstation	Laurence D. Merkle	2008		10.1145/1388969.1389000	parallel computing;real-time computing;simulation;computer science;artificial intelligence;machine learning;optimizing compiler;evolutionary computation	Arch	-2.372260085660879	45.663931316234944	76409
b11388b6011ce5959593762341c72e25cb9abe80	a novel controller design approach based on object-oriented virtual prototyping	control systems;zinc oxide;design process;object oriented methods;computer model;fuzzy control;niobium;control system cad;pulse width modulated;vitural prototyping;virtual prototyping;control system;niobium control systems pulse width modulation zinc oxide computational modeling mechatronics;computational modeling;collaborative simulation;multi domain;object oriented;system design;system level design;controller design;time to market;fuzzy control vitural prototyping collaborative simulation control system object oriented;mechatronics;virtual prototyping control system cad mechatronics object oriented methods time to market;mechatronic systems;pulse width modulation;systemized requirements controller design object oriented virtual prototyping multidomain mechatronics products functional digital prototyping fdp oo method controller tuning system level design three loop control system x y table time to market reduction virtualized requirements concurrent requirements	Nowadays, with the ever increasing complexity of multi-domain mechatronics products and the demand of reducing time-to-market, the system design approach which meets the requirements of virtualized, concurrent, and systemized is needed. Based on the practical problems arise in the controller design for mechatronics system, this paper proposes a novel controller design approach based on functional digital prototyping (FDP) and object oriented (OO) method. This approach enables to design and tune the controller on virtual platform in the early stage of system level design. The feasibility of our approach is proved by a case study in the design process of a three-loop control system for X-Y table.	control system;level design;matlab;mechatronics;pid;reconfigurable computing;requirement;systems design;velocity (software development);virtual machine;virtual reality	Yi Xiong;Guangyou Yang	2012	Proceedings of the 2012 IEEE 16th International Conference on Computer Supported Cooperative Work in Design (CSCWD)	10.1109/CSCWD.2012.6221810	iterative design;embedded system;niobium;real-time computing;design process;computer science;control system;operating system;zinc;pulse-width modulation;electronic system-level design and verification;object-oriented programming;computational model;systems design	EDA	5.685041813777414	38.459918775617204	76564
15eb17bc680ae54fd8812b17ffe2460f3acbe5ea	parallel depth first search. part i. implementation	algoritmo paralelo;data transmission;processing;general and miscellaneous mathematics computing and information science;parallel algorithm;algoritmo busqueda;communications;multiprocessor;implementation;algorithme recherche;efficiency;recherche profondeur d abord;performance;search algorithm;data processing;intelligence artificielle;mathematical logic;algorithme parallele;iterative methods;ejecucion;computer architecture;bbn butterfly;probleme combinatoire;problema combinatorio;intel hypercube;state space;array processors;computer codes;artificial intelligence;algorithms;depth first search;inteligencia artificial;combinatory problem;task scheduling;multiprocesador;programming 990210 supercomputers 1987 1989;busqueda profundidad primer;memory devices;parallel processing;sequent balance 21000;multiprocesseur	This paper presents a parallel formulation of depth-first search which retains the storage efficiency of sequential depth-first search and can be mapped on to anyMIMD architecture. To study its effectiveness it has been implemented to solve the 15-puzzle problem on three commercially available multiprocessors—Sequent Balance 21000, the Intel Hypercube and BBN Butterfly. We have been able to achieve fairly linear speedup on Sequent up to 30 processors (the maximum configuration available) and on the Intel Hypercube andBBN Butterfly up to 128 processors (the maximum configurations available). Many researchers considered the ring architecture to be quite suitable for parallel depth-first search. Our experimental results show that hypercube and sharedmemory architectures are significantly better. At the heart of our parallel formulation is a dynamic work distribution scheme that divides the work between different processors. The effectiveness of the parallel formulation is strongly influenced by the work distribution scheme and architectural features such as presence/absence of shared memory, the diameter of the network, relative speed of the communication network, etc. In a companion paper,(1) we analyze the effectiveness of different load-balancing schemes and architectures, and also present new improved work distribution schemes.	15 puzzle;bbn butterfly;central processing unit;connection machine;depth-first search;load balancing (computing);shared memory;speedup;storage efficiency;telecommunications network	V. Nageshwara Rao;Vipin Kumar	1987	International Journal of Parallel Programming	10.1007/BF01389000	parallel processing;mathematical logic;parallel computing;multiprocessing;mimd;data processing;breadth-first search;performance;computer science;state space;processing;theoretical computer science;operating system;parallel algorithm;iterative method;efficiency;intel ipsc;programming language;implementation;algorithm;data transmission;search algorithm	HPC	-2.3579145607010745	35.58171163166029	76616
2df181d748027453d552006ae69a6a15b1c68284	structured adaptive mesh refinement on the connection machine	structured adaptive mesh refinement;compressible flow;general and miscellaneous mathematics computing and information science;mathematics;classical and quantum mechanics general physics;adaptive mesh refinement;other aspects of physical science 1992;fluid flow;informing science;mathematical logic;mathematics 990200;quantum mechanics;other aspects of physical science;computer calculations;compressible fluid flow;programming 990200 mathematics computers;algorithms;parallel machines;3 dimensional;data layout;logic programs;mesh generation;parallel processing;mathematics and computers	Adaptive mesh refinement has proven itself to be a useful tool in a large collection of applications. By refining only a small portion of the computational domain, computational savings of up to a factor of 80 in 3 dimensional calculations have been obtained on serial machines. A natural question is, can this algorithm be used on massively parallel machines and still achieve the same efficiencies We have designed a data layout scheme for mapping grid points to processors that preserves locality and minimizes global communication for the CM-200. The effect of the data layout scheme is that at the finest level nearby grid points from adjacent grids in physical space are in adjacent memory locations. Furthermore, coarse grid points are arranged in memory to be near their associated fine grid points. We show applications of the algorithm to inviscid compressible fluid flow in two space dimensions.	adaptive mesh refinement;connection machine;refinement (computing)	Jeffrey Saltzman;Marsha J. Berger	1993			computational science;grid file;computer science;theoretical computer science	HPC	-3.7273042780224994	36.687790740457366	76774
07b7e754b5e8395f283817b2f269d35a7bd163b2	an improved architecture of a hardware accelerator for factoring integers with elliptic curve method		Elliptic Curve Method (ECM) is a well-known method for factoring integers, which is usually used in the Number Field Sieve algorithm as a subroutine for factoring smaller integers than the targeted one. ECM is called many times and can be executed in parallel for different inputs. This method mainly consist of simple operations on elliptic curves. Thus, ECM is suitable for hardware implementations that can efficiently reduce computational time. This work describes a new, improved FPGA-based hardware accelerator for ECM, designed for large scale computations. Our accelerator can operate with an on board ARM processor or with an external host computer. This design can factor several numbers at once and can be easily ported to various FPGA boards. Different methods for improving results (e.g. the use of DSP blocks, cache-registers, reorganizing instruction order) are described and their performance is analyzed. As a result, one of the fastest hardware ECM units is achieved.	arm architecture;computation;digital signal processor;fastest;field-programmable gate array;general number field sieve;hardware acceleration;host (network);integer factorization;lenstra elliptic curve factorization;map (parallel pattern);multiplication algorithm;simulation;stratix;subroutine;time complexity	Michal Andrzejczak	2018	2018 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2018F197	parallel computing;architecture;general number field sieve;field-programmable gate array;machine learning;artificial intelligence;elliptic curve;subroutine;arm architecture;computer science;hardware acceleration;integer	EDA	8.693536219371124	44.52793716061827	77204
0357718ec787f91c6559f8c4b709b57cb9cdd14e	glb: lifeline-based global load balancing library in x10	work stealing;glb;performance;x10;scalability	We present GLB, a programming model and an associated implementation that can handle a wide range of irregular parallel programming problems running over large-scale distributed systems. GLB is applicable both to problems that are easily load-balanced via static scheduling and to problems that are hard to statically load balance. GLB hides the intricate synchronizations (e.g., inter-node communication, initialization and startup, load balancing, termination and result collection) from the users. GLB internally uses a version of the lifeline graph based work-stealing algorithm proposed by Saraswat et al [25]. Users of GLB are simply required to write several pieces of sequential code that comply with the GLB interface. GLB then schedules and orchestrates the parallel execution of the code correctly and efficiently at scale.  We have applied GLB to two representative benchmarks: Betweenness Centrality (BC) and Unbalanced Tree Search (UTS). Among them, BC can be statically load-balanced whereas UTS cannot. In either case, GLB scales well -- achieving nearly linear speedup on different computer architectures (Power, Blue Gene/Q, and K) -- up to 16K cores.	algorithm;betweenness centrality;blue gene;computer architecture;distributed computing;load balancing (computing);parallel computing;programming model;scheduling (computing);speedup;uts;work stealing;x10	Wei Zhang;Olivier Tardieu;David Grove;Benjamin Herta;Tomio Kamada;Vijay A. Saraswat;Mikio Takeuchi	2014		10.1145/2567634.2567639	parallel computing;real-time computing;scalability;performance;computer science;operating system;distributed computing;programming language;algorithm	HPC	-4.371704754257872	45.969242703922696	77359
4b60399984dbc2db66e51a2791fe65e968fb3488	room occupancy detection: combining rss analysis and fuzzy logic		In this paper we focus our attention on the world of Internet of Things (IoT) objects and their potential for human indoor localization. Our aim is to investigate how Received Signal Strength (RSS) can be effectively used for identifying the position of a person at home, by exploiting common IoT communication networks. We propose a plug and play solution where the Anchor Nodes (ANs) are represented by smart objects located in the house, while the Unknown Node (UN) can be any smart object held by the user. The proposed solution automatically identifies the rooms where the smart objects are placed, by comparing a fuzzy weighted distance matrix derived from the anchor signals, with a threshold weighted distance matrix derived from the distances between rooms. The information can be easily integrated in any IoT environment to provide the estimation of the user position, without requiring the a priori knowledge of the positions of the anchor nodes.	distance matrix;fuzzy logic;internet of things;plug and play;rss;random number generation;smart objects;telecommunications network	Alessandro Baldini;Lucio Ciabattoni;Riccardo Felicetti;Francesco Ferracuti;Sauro Longhi;Andrea Monteriù;Alessandro Freddi	2016	2016 IEEE 6th International Conference on Consumer Electronics - Berlin (ICCE-Berlin)	10.1109/ICCE-Berlin.2016.7684720	simulation;engineering;world wide web;computer security	Robotics	-0.2596232397625535	32.41277886531502	77457
24530b573a17c41ae8d5de22b91417cdce9a521c	multigrain parallelism for eigenvalue computations on networks of clusters	eigenvalues and eigenfunctions;latency effects multigrain parallelism eigenvalues workstation clusters grids multigrain mode parallel algorithm block iterative methods granularity jacobi davidson eigensolver;scientific application;latency effects;impedance;jacobi davidson eigensolver;parallel algorithm;mathematics computing;concurrent computing;block iterative methods;iterative algorithms;parallel processing eigenvalues and eigenfunctions computer networks concurrent computing delay iterative methods iterative algorithms workstations resource management impedance;resource management;network performance;cluster of workstations;eigenvalues;computer networks;multigrain mode;iterative methods;multigrain parallelism;granularity;workstations;resource sharing;scientific computing;cost effectiveness;workstation clusters;coarse grained;iteration method;grids;mathematics computing parallel processing iterative methods eigenvalues and eigenfunctions parallel algorithms workstation clusters;parallel processing;parallel algorithms	Clusters of workstations have become a cost-effective means of performing scientific computations. However, large network latencies, resource sharing, and heterogeneity found in networks of clusters and Grids can impede the performance of applications not specifically tailored for use in such environments. A typical example is the traditional fine grain implementations of Krylov-like iterative methods, a central component in many scientific applications. To exploit the potential of these environments, advances in networking technology must be complemented by advances in parallel algorithmic design. In this paper, we present an algorithmic technique that increases the granularity of parallel, block iterative methods by inducing additional work during the preconditioning (inexact solution) phase of the iteration. During this phase, each vector in the block is preconditioned by a different subgroup of processors, yielding a much coarser granularity. The rest of the method comprises a small portion of the total time and is still implemented in fine grain. We call this combination of fine and coarse grain parallelism multigrain. We apply this idea to the block Jacobi-Davidson eigensolver, and present experimental data that shows the significant reduction of latency effects on networks of clusters of roughly equal capacity and size. We conclude with a discussion on how multigrain can be applied dynamically based on runtime network performance monitoring.	block size (cryptography);central processing unit;computation;computational model;computer cluster;eigenvalue algorithm;grid computing;iteration;iterative method;jacobi method;krylov subspace;network performance;overhead (computing);parallel computing;preconditioner;run time (program lifecycle phase);synthetic intelligence;workstation	James R. McCombs;Andreas T Stathopoulos	2002		10.1109/HPDC.2002.1029912	parallel processing;parallel computing;concurrent computing;computer science;resource management;theoretical computer science;distributed computing;parallel algorithm;iterative method	HPC	-3.5360788885909207	39.95944641097613	77590
55558423857d2a2ee6d18c32877703d0de1a0672	digital real-time spectral analysis	spectral analysis equations concurrent computing algorithm design and analysis shift registers arithmetic sampling methods fourier transforms real time systems speech analysis;concurrent computing;fourier transform;real time;speech analysis;real time correlation;special purpose machine organizations;fast fourier transform;shift registers;fourier transforms;telecommunication;spectral analysis fast fourier transform algorithm fourier transforms real time correlation special purpose machine organizations;time compression;parallel computer;fast fourier transform algorithm;arithmetic;spectral analysis;sampling methods;algorithm design and analysis;real time systems	The fast Fourier transform algorithm, reported by Cooley and Tukey, results in substantial computational savings and permits a considerable amount of parallel computation. By making use of these features, estimates of the spectral components of a time function can be calculated by a special-purpose digital machine while the function is being sampled. In this paper, two digital machine organizations are suggested which use the algorithm for the case of N (the number of samples analyzed) being a power of 2 and the case of N being the product of two integers. The first machine consists of shift registers and arithmetic units organized in stages which perform calculations in parallel. It can be used when N is a power of 2 and can accept signals being sampled at a rate exceeding 500 000 samples per second. The second machine requires fewer shift registers and only one arithmetic unit but cannot operate in a continuous manner. This means that either a dead time between adjacent records of data must be allowed or a time compression unit must be used. In the first case the obtainable sampling rate depends upon the dead time which can be allowed between adjacent records of data. In the second case sampling rates up to 8000 samples per second are feasible. For this analyzer, N is required to be expressible as the product of two integers.	real-time transcription	Glenn David Bergland;Harry W. Hale	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264814	arithmetic;fourier transform;concurrent computing;computer science;theoretical computer science;algorithm	Embedded	8.701925623244698	38.8683750777996	78150
bab493772e7964330c17d994eddbdd529c10eade	midpoint cell method for hybrid (mpi+openmp) parallelization of molecular dynamics simulations	hybrid parallelization;domain decomposition;molecular dynamics;midpoint cell method	We have developed a new hybrid (MPI+OpenMP) parallelization scheme for molecular dynamics (MD) simulations by combining a cell-wise version of the midpoint method with pair-wise Verlet lists. In this scheme, which we call the midpoint cell method, simulation space is divided into subdomains, each of which is assigned to a MPI processor. Each subdomain is further divided into small cells. The interaction between two particles existing in different cells is computed in the subdomain containing the midpoint cell of the two cells where the particles reside. In each MPI processor, cell pairs are distributed over OpenMP threads for shared memory parallelization. The midpoint cell method keeps the advantages of the original midpoint method, while filtering out unnecessary calculations of midpoint checking for all the particle pairs by single midpoint cell determination prior to MD simulations. Distributing cell pairs over OpenMP threads allows for more efficient shared memory parallelization compared with distributing atom indices over threads. Furthermore, cell grouping of particle data makes better memory access, reducing the number of cache misses. The parallel performance of the midpoint cell method on the K computer showed scalability up to 512 and 32,768 cores for systems of 20,000 and 1 million atoms, respectively. One MD time step for long-range interactions could be calculated within 4.5 ms even for a 1 million atoms system with particle-mesh Ewald electrostatics.		Jaewoon Jung;Takaharu Mori;Yuji Sugita	2014	Journal of computational chemistry	10.1002/jcc.23591	computational science;molecular dynamics;chemistry;theoretical computer science;domain decomposition methods;physics;quantum mechanics	HPC	-4.2054484974435224	37.47983529757398	78212
00bdb8545b2162c6d55fad4b5b2a0e5ecc5aac1b	architectural support for 3d graphics in the complex streamed instruction set.	processor architecture;floating point unit;perforation;superscalar processor;streaming simd extensions;floating point;3d graphics	In this paper we extend the previously proposed Complex Streamed Instruction Set (CSI) architecture to provide for floating-point computations and conditional execution in order to efficiently support 3D graphics applications. The CSI extension is evaluated using an industry standard 3D benchmark, and compared to the Intel’s Streaming SIMD Extension (SSE). Compared to a 4-way issue superscalar processor extended with SSE and capable of processing 8 single-precision floating-point operations in parallel, the same processor extended with CSI attains the speedups of 2.8 and 2.13 on the transform and lighting kernels and the speedup of 1.61 on the geometry computations in whole. We also study how performance scales with the number of floating-point units and observe that CSI extension allows to utilize them more efficiently then SSE. Finally, the performance bottlenecks of the SSEenhanced superscalar CPUs on the 3D graphics workload are identified. Results show that performance of the 4-way issue machines is limited by the issue width and that of the 8-way machines is limited by the number of the cache ports.	3d computer graphics;benchmark (computing);central processing unit;computation;single-precision floating-point format;speedup;streaming simd extensions;streaming media;superscalar processor;technical standard;transform, clipping, and lighting	Dmitry Cheresiz;Ben H. H. Juurlink;Stamatis Vassiliadis;Harry A. G. Wijshoff	2002			floating-point unit;computer architecture;parallel computing;computer hardware;microarchitecture;computer science;floating point;operating system;3d computer graphics	Arch	-4.4201771737230695	45.686934871491395	78243
ab045b247579ed319bbbc344a0f9a76826f4d1df	parallel algorithm for swfft using 3d data structure		Sliding-Window Fast Fourier Transform (SWFFT) is a very important and widely used time-frequency representation of a signal. In the paper, we mainly focus on the problem of how to implement non-recursive SWFFT in parallel programming. To avoid repeated calculations, non-recursive SWFFT algorithms always save the calculated results and use for later calculations. So the current calculations need the results from the earlier calculations, and this leads to the main obstacle of implementing SWFFT in parallel programming. By assuming that all the data have been present at the outset, a new parallel algorithm for non-recursive SWFFT is proposed in the paper. In our algorithm, 3D data structure is utilized to represent the computation of SWFFT while the calculations are carried out along level axis instead of time axis. Since the current calculations do not anymore depend on the results from the earlier calculations, our algorithm can be implemented by parallel programming easily. Finally, the algorithm is programmed in C++ based on OpenMP API and is evaluated on a 4-processor desktop computer. Threads up to four are created and each of them is performed by one processor core. A memory-shared parallel programming model is devised to achieve data accessing and results exchanging between threads. Compared with employing one thread, our algorithm reduces the computational time of a signal of length L = 1016 (the window length N = 1010) to 32% by employing four threads.	apache axis;application programming interface;c++;central processing unit;computation;computational complexity theory;data structure;desktop computer;fast fourier transform;multi-core processor;openmp;parallel algorithm;parallel computing;parallel programming model;recursion;time complexity;time–frequency representation	Jian-Ming Wang;William F. Eddy	2012	CSSP	10.1007/s00034-011-9342-5	mathematical optimization;computer science;theoretical computer science;algorithm	HPC	-0.920836060941384	39.72385311701092	78271
46b87c4a407bab3f6520b0e104bdbcba89701101	a flexible class of parallel matrix multiplication algorithms	mathematics computing;program control structures;nested loops;software performance evaluation;spectrum;matrix shape parallel matrix multiplication algorithms parallel implementation nested loops performance;matrix multiplication;parallel implementation;program control structures parallel algorithms matrix multiplication mathematics computing software performance evaluation;broadcasting shape high performance computing ice algorithm design and analysis memory architecture nasa geoscience councils mathematics;hybrid algorithm;parallel algorithms	This paper explains why parallel implementation of matrix multiplication—a seemingly simple algorithm that can be expressed as one statement and three nested loops—is complex: Practical algorithms that use matrix multiplication tend to use matrices of disparate shapes, and the shape of the matrices can significantly impact the performance of matrix multiplication. We provide a class of algorithms tha t covers the spectrum of shapes encountered and demonstrate that good performance can be attained if the right algorithm is chosen. These observations set the stage for hybrid algorithms which choose between the algorithms based on the shapes of the matrices involved. While the paper resolves a number of issues, it concludes with discussion of a number of directions yet to be pursued.	blas;hybrid algorithm;matrix multiplication;multiplication algorithm	John A. Gunnels;Calvin Lin;Greg Morrow;Robert A. van de Geijn	1998		10.1109/IPPS.1998.669898	parallel computing;multiplication algorithm;computer science;matrix chain multiplication;theoretical computer science;distributed computing	HPC	-1.2638967392489537	38.44652728834667	78680
c489e224b5bb8950282787b3f54786dcfc911cfd	scalable b-matching on gpus		We present a new greedy b-MATCHING algorithm suitable for running on a GPU. Our algorithm differs from previous efforts at designing parallel algorithms for this problem in that it does not use software locks and that it also exploits substantially more of the available concurrency. We achieve this by allowing the same vertex to concurrently match with several other vertices and also by letting multiple vertices simultaneously match with the same target vertex. We have compared our algorithm using a Pascal P100 GPU with the previous best shared memory algorithm for this problem both when running on a 16 core Xeon E5 and on a Xeon Phi. On average our algorithm outperforms the Xeon E5 by a factor of 4.6 and the Xeon Phi by a factor of 2.3. We also show that our algorithm using an NVIDIA DGX-1 multi-GPU system is highly competitive compared to a distributed memory implementation running on one of the top ten computers from the current TOP500 list.	computer;concurrency (computer science);distributed memory;experiment;graphics processing unit;greedy algorithm;lock (computer science);multitier architecture;mutual exclusion;non-blocking algorithm;nvidia dgx-1;parallel algorithm;shared memory;top500;thread block;vertex (geometry);xeon phi	Md. Naim;Fredrik Manne	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00104	xeon;parallel computing;computer science;distributed memory;xeon phi;approximation algorithm;distributed computing;top500;parallel algorithm;greedy algorithm;shared memory	HPC	-3.975965801911294	42.23861404780681	78969
6d10a32b6cb3b95b33570a0055f8cdf17facbdfe	a new code compression method for fota	nand flash memory;flash memory;compression algorithm;nand;data compression;decoding;delta;compression algorithms;fota;over the air;firmware;firmware over the air;program code;nand circuits;mobile handsets;encoding decoding microprogramming data compression mobile handsets flash memory compression algorithms;compression ratio;microprogramming;encoding;copy information;flash memories;nand circuits data compression decoding encoding firmware flash memories;code compression;copy information firmware over the air fota code compression nand flash memory encoding decoding program code	This paper presents a new compression method for compressed Firmware-over-the-air (FOTA) that reconstructs a new compressed version from an old compressed version of firmware stored in NAND flash memory. The compressed FOTA requires a small delta that minimizes the amount of update data and an efficient compression method that provides fast encoding and decoding to achieve fast updating and extraction. A fast dictionary-based compression algorithm for the program code is introduced. With a high compression ratio, our method provides relatively fast encoding and very fast decoding. We demonstrate that the compression time during the update process can be reduced by utilizing the copy information stored in the delta without increasing the size of the delta. We present some experimental results to show the impact of this method in terms of the compression ratio, extraction time and updating time.	algorithm;bitwise operation;data compression;dictionary;firmware;flash memory;lz77 and lz78;terrestrial television;zlib	Youngcheul Wee;Taehwa Kim	2010	IEEE Transactions on Consumer Electronics	10.1109/TCE.2010.5681111	data compression;data compression ratio;parallel computing;real-time computing;computer hardware;computer science;operating system;lossless compression;incremental encoding;statistics	EDA	9.441561021140195	38.11287411252396	79032
76d18c43e935dc34f834277b4cc4582f31f62faf	a parallel block implementation of level-3 blas for mimd vector processors	algorithm performance;matrix matrix kernels;vectorisation;paralelisacion;vectorization;vectorisacion;calculateur mimd;resultado algoritmo;parallelisation;basic linear algebra subprograms;performance algorithme;parallelization;level 3 blas;matrix multiplication;fortran;memory hierarchy;vector processor;mimd computer	We describe an implementation of Level-3 BLAS (Basic Linear Algebra Subprograms) based on the use of the matrix-matrix multiplication kernel (GEMM). Blocking techniques are used to express the BLAS in terms of operations involving triangular blocks and calls to GEMM. A principal advantage of this approach is that most manufacturers provide at least an efficient serial version of GEMM so that our implementation can capture a significant percentage of the computer performance. A parameter which controls the blocking allows an efficient exploitation of the memory hierarchy of the various target computers. Furthermore, this blocked version of Level-3 BLAS is naturally parallel. We present results on the ALLIANT FX/80, the CONVEX C220, the CRAY-2, and the IBM 3090/VF. For GEMM, we always use the manufacturer-supplied versions. For the operations dealing with triangular blocks, we use assembler or tuned Fortran (using loop-unrolling) codes, depending on the efficiency of the available libraries.	alliant computer systems;assembly language;blas;blocking (computing);central processing unit;code;computer performance;cray-2;fortran;library (computing);linear algebra;mimd;matrix multiplication;memory hierarchy;subroutine;the matrix;vector processor	Michel J. Daydé;Iain S. Duff;Antoine Petitet	1994	ACM Trans. Math. Softw.	10.1145/178365.174413	computer architecture;vector processor;parallel computing;matrix multiplication;computer science;theoretical computer science;basic linear algebra subprograms;vectorization;mathematics;algebra	HPC	-4.325269075503213	40.833883627192385	79315
4cd598d9b0fef4f4773d4314a0433df895d48ddd	she: smart home energy management system for appliance identification and personalized scheduling	smart home;non intrusive monitoring;ubiquitous computing;energy saving;personalized scheduling	The home appliance scheduling is a promising energy saving technique that has significant commercial potential. In this demo, Smart Home Energy Management System (SHE) is developed on Android platform to schedule users' appliances. SHE monitors the power consumption to identify the operations of home appliances using a privacy preserving technique called Non-Intrusion Load Monitoring (NILM). The operations are integrated with dynamic electric price and environment data to mine users' personal demand and preference on appliance operation. Finally, SHE generates personalized scheduling strategies to meet the different users' demands at the minimal cost.	android;home automation;management system;personalization;scheduling (computing)	Ting Liu;Siyun Chen;Yuqi Liu;Zhanbo Xu;Yulin Che;Yufei Duan	2014		10.1145/2638728.2638732	embedded system;home automation;real-time computing;human–computer interaction;computer science;computer security;ubiquitous computing	HCI	1.110686428398816	32.865978177873515	79549
f863462955341fd29e48688321ccddd5fcc403ba	distributed and shared memory algorithm for parallel mining of association rules	frequent pattern;shared memory;data mining;frequent itemset;association rule	The search for frequent patterns in transactional databases is considered one of the most important data mining problems. Several parallel and sequential algorithms have been proposed in the literature to solve this problem. Almost all of these algorithms make repeated passes over the dataset to determine the set of frequent itemsets, thus implying high I/O overhead. In the parallel case, most algorithms perform a sum-reduction at the end of each pass to construct the global counts, also implying high synchronization cost. We present a novel algorithm that exploits efficiently the trade-offs between computation, communication, memory usage and synchronization. The algorithm was implemented over a cluster of SMP nodes combining distributed and shared memory paradigms. This paper presents the results of our algorithm on different data sizes experimented on different numbers of processors, and studies the effect of these variations on the overall performance.	algorithm;shared memory	José Hernández Palancar;O. Fraxedas Tormo;J. Festón Cárdenas;Raudel Hernández-León	2007		10.1007/978-3-540-73499-4_27	distributed shared memory;shared memory;parallel computing;association rule learning;distributed memory;computer science;data mining;distributed computing	ML	-0.5997967295117642	41.55335170451209	79887
bdf127654ddbb9cb9b335316c7e51a435409c59d	a falsification prevention method for face authentication using light	face authentication;object recognition;certification;biometrics access control;authorisation;face recognition authorisation biometrics access control;falsification prevention method;dynamic images;still images;falsification prevention;brightness;physical characteristic;authentication face detection certification skin colored noise brightness multimedia systems signal processing microcomputers fingerprint recognition;color shades;face recognition;shape;dynamic images falsification prevention method face authentication personal identification systems color shades still images;image color analysis;pixel;face;face detection;personal identification systems;face detection face authentication falsification prevention	Fuzzy commitment of Juels and Wattenberg 1999 is a popular technique for designing secure systems based on noisy data. The scheme is easy to implement using standard error-correcting codes. However, secrecy of this scheme is only guaranteed when input data are generated by uniform i.i.d. sources, while typical input data (PUFs and biometrics) are not uniform. In this paper we address the problem of extracting robust independent uniformly distributed bits out of noisy data that can be used as entries to fuzzy commitment. The proposed techniques can serve as a building block of secure fuzzy commitment systems.	authentication;biometrics;code;forward error correction;image;signal-to-noise ratio	Yosuke Horiuchi;Osamu Uchida	2009	2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing	10.1109/IIH-MSP.2009.311	facial recognition system;face;computer vision;face detection;shape;computer science;cognitive neuroscience of visual object recognition;authorization;internet privacy;certification;computer security;brightness;pixel	Robotics	7.519726784007899	34.21853776377004	80075
6dd781c48f1018b9f7f321f1917197eed988594a	an exact matching approach for high throughput sequencing based on bwt and gpus	dna;multi threading;exact sequence matching;bwt;computer graphic equipment;gpu;general purpose parallel computing exact matching approach high throughput sequencing model bwt gpu burrows wheeler transformation sequence alignment method bioinformatics applications multithreaded program bowtie bwa soap2 dna sequencing cpu nvidia fermi architecture;coprocessors;exact sequence matching bwt hts gpu;parallel processing bioinformatics computer graphic equipment coprocessors dna multi threading;parallel computer;hts;burrows wheeler transform;sequence alignment;high throughput;dna sequence;parallel processing;graphics processing unit indexing random access memory high temperature superconductors throughput complexity theory;bioinformatics	In recent years, Burrows-Wheeler Transformation (BWT) has become a popular method for sequence alignment in bioinformatics applications. Several multithreaded programs such Bowtie, BWA and SOAP2 are developed to make DNA sequencing fast. However, as the requests from patients grow extremely fast, current machines for analysis, most of which are CPUs, becomes more incapable in providing sufficient computational powers. In 2010, the graphics card company NVIDIA released its new Fermi architecture and several series of brands to meet the need for general-purpose parallel computing, which provide dozens to hundreds time of computation power increase compared to single CPU. In this paper, Burrows-Wheeler Transformation (BTW) is analyzed thoroughly and several optimizations are proposed for exact sequence matching. Also, efficient High Throughput Sequencing (HTS) models on both CPU and GPU are developed for practical applications. Experimental results have demonstrated the effectiveness of our algorithm and the great potentials of GPUs in the sequence alignment area.	algorithm;bioinformatics;bowtie (sequence analysis);burrows–wheeler transform;cuda;central processing unit;computation;fermi (microarchitecture);general-purpose modeling;graphics processing unit;high-throughput satellite;input/output;list of sequence alignment software;parallel computing;programming language;thread (computing);throughput;video card	Su Chen;Hai Jiang	2011	2011 14th IEEE International Conference on Computational Science and Engineering	10.1109/CSE.2011.41	parallel processing;parallel computing;computer science;bioinformatics;theoretical computer science;operating system;burrows–wheeler transform;database	HPC	-0.11997344688636223	42.6994675877305	80090
11e1e64434d2aebab4f74f08e8b5d95d13f5feda	the symmetric tridigonal eigenproblem on a shared memory multiprocessor: part i	a shared memory encore multimax multiprocessor;lenguaje programacion;algoritmo paralelo;analisis numerico;parallel algorithm;matematicas aplicadas;shared memory;mathematiques appliquees;langage c;programming language;multiprocessor;implementation;memoria compartida;the symmetric tridiagonal eigenproblem;parallel computation;analyse numerique;algorithme parallele;methode matricielle;the c programming language;c language;calculo paralelo;methode cuppen;numerical analysis;object oriented;object oriented programming languages;matrix method;cuppen method;parallel computer;metodo matriz;langage programmation;oriente objet;c programming language;experimental evaluation;40c05;multiprocesador;implementacion;applied mathematics;cuppen s method;orientado objeto;calcul parallele;divide and conquer;60k30;memoire partagee;lenguaje c;shared memory multiprocessor;multiprocesseur	The article presents an experimental evaluation of the effect of deflation on accuracy. A number of test matrices were used to test Cuppen s algorithm since the amount of deflation in this method depends on the test matrix. The motivation for a non-recursive version was to compare this with the recursive version and to investigate the relationship between the recursive and non-recursive implementations of Cuppen s divide-and-conquer method. We have concentrated in particular on algorithms written in the C++ programming language. C++ is an object-oriented programming language, which can provide various types of matrix classes. The use of C++ implies some storage organisation for array elements. The primitive arrays provided in C++ use storage by rows. As far as we are aware there is little previous work on parallelising these algorithms using C++. Consequently in this work we have considered implementation of the serial using the objectoriented programming language C++. 2003 Elsevier Inc. All rights reserved.	actionscript;computation;multiprocessing;numerical analysis;point of view (computer hardware company);recursion;root-finding algorithm;shared memory;the c++ programming language	Dogan Kaya	2004	Applied Mathematics and Computation	10.1016/j.amc.2003.07.009	parallel computing;computer science;programming language;object-oriented programming;algorithm	PL	-2.259722346609578	36.96697534498606	80139
51eccb42dcbf33162d86a717eaf644cc78070825	a fast algorithm for concurrent lu decomposition and matrix inversion.	general and miscellaneous mathematics computing and information science;mathematics;linear system of equations;efficient algorithm;systolic array;matrix inversion;mathematical logic;matrices;algebra;mathematical programming;fast algorithm;programming 990200 mathematics computers;algorithms;parallel processing	The authors present an efficient algorithm for Lu decomposition and matrix inversion based on the concurrent data-loading array architecture. The algorithm performs the Lu decomposition of a strongly nonsingular matrix initially loaded in the array, in parallel with computing the inverse matrices l/sup -1/, u/sup -1/, and a/sup -1/: the l/sup -1/ and u/sup -1/ can be taken out together with l and u; and a/sup -1/ will appear in the array at the end of the computation. For an n*n matrix executed on the array of the same size, the total time required for the above computation is n(t/sub a/+t/sub m/+t/sub d/); where t/sub a/, t/sub m/, and t/sub d/ represent the time for addition, multiplication, and division, respectively. A simple augmentation of this algorithm can lead to the solution of a linear system of equations without additional time. The performance of this algorithm is analyzed and compared favorably with an improved version of systolic arrays. 12 references.		Ming-Yang Chern;Tadao Murata	1983			system of linear equations;parallel processing;mathematical logic;parallel computing;systolic array;computer science;theoretical computer science;algorithm;matrix	HPC	-1.444401245146397	36.847673466179906	80393
5044aa8f4107745e0c638148bcbdf740804ca7db	optimizing dense linear algebra algorithms on heterogeneous machines		This paper addresses the execution of inherently sequential linear algebra algorithms namely LU factorization, tridiagonal reduction and the symmetric QR factorization algorithm used for eigenvector computation, which are significant building blocks for applications in our target image processing and analysis domain. These algorithms present additional difficulties to optimize the processing time due to the fact that the computational load for data matrix columns increases with their index, requiring a fine tuned load assignment and distribution. We present an efficient methodology to determine the optimal number of processors to be used in a computation, as well as a new static load distribution strategy that achieves better results than other algorithms developed for the same purpose.	algorithm;application domain;array dbms;bottleneck (engineering);central processing unit;column (database);computation;computational model;computer;fastest;general-purpose modeling;heterogeneous computing;image analysis;image processing;interactivity;lu decomposition;linear algebra;load balancing (computing);mathematical optimization;medical imaging;optimizing compiler;parallel computing;qr decomposition;response time (technology);run time (program lifecycle phase);speedup;structural load;virtual machine	Jorge G. Barbosa	2002	Scalable Computing: Practice and Experience		computer science;theoretical computer science;distributed computing;algorithm	HPC	-3.333847388182168	38.958902236403844	80411
da977559c35ee9256250b5f0fce24c69a2467788	improving i/o performance with adaptive data compression for big data applications	analytical models;data compression;data compression big data data analysis;compression algorithms;in situ analytics;computational modeling;high end computing i o bottlenecks in situ analytics compression big data;big data;data compression compression algorithms data models computational modeling analytical models bandwidth data transfer;data transition bandwidth i o performance adaptive data compression technology big data applications high end computing machines data analysis data movement cost petascale applications adaptive data compression algorithm adaptive compression service;bandwidth;compression;high end computing;data transfer;data models;i o bottlenecks	Increasingly larger scale simulations are generating an unprecedented amount of data. However, the increasing gap between computation and I/O capacity on High End Computing machines makes a severe bottleneck for data analysis. As a solution, in-situ analytics processes output data while simulations are running and before placing data on disk. Data movement between simulation and analytics, however, incurs overheads of in-situ analytics at scale. This paper tries to answer the following question: can we use compression technology to reduce the data movement cost and improve the performance of in-situ analytics for peta-scale applications? In particular, we explore when, where, how to use the compression techniques to reduce data movement cost between simulation and analytics. To find out the best algorithm and place to compress data in given situation, we introduce an adaptive data compression algorithm in this paper. The adaptive compression service is developed and analyzed for the in-situ analytics middleware. Experimental results demonstrate that compression service increases data transition bandwidth and improve the application End-to-End transfer performance.	adaptive compression;algorithm;big data;crc-based framing;central processing unit;computation;data compression;end-to-end principle;experiment;input/output;mathematical optimization;middleware;real-time clock;real-time computing;simulation	Hongbo Zou;Yongen Yu;Wei Tang;Hsuan-Wei Michelle Chen	2014	2014 IEEE International Parallel & Distributed Processing Symposium Workshops	10.1109/IPDPSW.2014.138	data compression;big data;computer science;data science;theoretical computer science;data mining;statistics	HPC	1.5577028531198889	38.9128991689203	80522
a4507e3eb6aed4ec9e9e4be15f5907da90e3ce5e	vlock: lock virtualization mechanism for exploiting fine-grained parallelism in graph traversal algorithms	graph theory;cache storage;vlock intel westemere six core processors smp system pagerank cc sssp bfs pthreads like library legacy graph programs cache miss memory cost program life cycle physical lock space logical lock space fine grained locks fine synchronization graph traversal algorithms fine grained parallelism lock virtualization mechanism;virtualization parallel processing synchronization programming algorithm design and analysis instruction sets usability;fine synchronization;virtualisation cache storage graph theory multiprocessing systems parallel processing;multiprocessing systems;vlock;graph algorithms;vlock graph algorithms fine synchronization;parallel processing;virtualisation	For graph traversal applications, fine synchronization is required to exploit massive fine parallelism. However, in the conventional solution using fine-grained locks, locks themselves suffer huge memory cost as well as poor locality for inherent irregular access to vertices. In this paper, we propose a novel fine lock solution-vLock. The key idea is lock virtualization that maps the huge logical lock space to a much smaller physical lock space that can reside in cache during the program life cycle. Lock virtualization effectively reduces lock incurred overheads of both memory cost and cache misses. It also achieves high usability in legacy graph programs, as from users's view vLock is the same as lock methods in Pthreads. We implement vLock as a Pthreads-like library and evaluate its performance in four classical graph algorithms (BFS, SSSP, CC, PageRank). Experiments on a SMP system with two Intel Westemere six-core processors show that, compared to conventional fine locks, vLock significantly reduces locks' cache misses and has competitive performance. Particularly, PageRank with vLock has about 20% performance improvement.	algorithm;breadth-first search;cpu cache;central processing unit;graph theory;graph traversal;locality of reference;lock (computer science);posix threads;pagerank;parallel computing;shortest path problem;symmetric multiprocessing;tree traversal;usability;x86 virtualization	Jie Yan;Guangming Tan;Xiuxia Zhang;Erlin Yao;Ninghui Sun	2013	Proceedings of the 2013 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)	10.1109/CGO.2013.6494991	giant lock;lock;parallel processing;parallel computing;real-time computing;virtualization;computer science;graph theory;operating system;distributed computing;programming language	Arch	-3.0559880238317465	42.95361825380375	80531
333ebf5d1ac16bb311eceaf3e9c4a8ae14f8f08b	constructing a gpu cluster platform based on multiple nvidia jetson tk1	noise measurement;servers;graphics processing units;mobile communication;xml;master slave	High-end graphics processing units (GPUs), such as NVIDIA Fermi/Tesla series cards, are widely applied to the high performance computing fields in a decade. NVIDIA releases Tegra K1, called Jetson TK1, which contains 4 ARM Cortex-A15 CPUs and 192 CUDA cores (Kepler GPU) is an embedded board with low cost, low power consumption, and high applicability advantages for several specific applications. In the previous work, we have constructed a bioinformatics platform based on single NVIDIA Jetson TK1, and ClustalWtk tool for multiple sequence alignments was designed on this platform. With more and more biological sequences generated and high computing power requirement, it is necessary to construct a cluster platform based on multiple NVIDIA Jetson TK1. In this paper, a bioinformatics platform based on a cluster of multiple NVIDIA Jetson TK1 is constructed. Besides, two job assignment modes are proposed in this platform: one is a “user selected” mode and another is an “automatic assigned” mode. ClustalWtk tool also is ported on this platform with these two modes, respectively.	arm cortex-a15;arm cortex-m;arm architecture;bioinformatics;cuda;central processing unit;computer graphics;embedded system;fermi (microarchitecture);gpu cluster;graphics processing unit;kepler (microarchitecture);multiple sequence alignment;nvidia tesla;supercomputer;tegra	Kuan-Yu Yeh;Hui-Jun Cheng;Jin Ye;Jyh-Da Wei;Chun-Yuan Lin	2016	2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2016.7822647	embedded system;master/slave;parallel computing;xml;mobile telephony;computer hardware;computer science;noise measurement;server	HPC	-1.031213446516692	43.97531264631488	80538
853936c719ee98105eb7e91f8494fc52addcd431	area-efficient evaluation of a class of arithmetic expressions using deeply pipelined floating-point cores	indexing terms;graph theory;floating point arithmetic;floating point;scientific computing;hardware accelerator	Due to technological advances,it has becomepossible to implement floating-point coreson FPGAs in an effort to provide hardware acceleration for the myriad applications that require high performance floating-point arithmetic. However, in order to achieve a high clock rate, these floating-point cores must be deeply pipelined. Due to this deep pipelining and the complexity of floating-point arithmetic, floating-point cores use a great deal of the FPGA’s area. It is thus important to use as few floating-point cores in an architecture as possible.However, the deep pipelining makes it difficult to reusethe samefloatingpoint core for a series of floating-point computations that are dependent upon one another. In this paper, we describe an area-efficient architecture and algorithm for the evaluation of arithmetic expressions.This design effectively hides the pipeline latencyof the floating-point coresand usesonly onefloating-point core for each type of operator in the expression.The design is applicable to a wide variety of fields suchasscientific computing, cognition, and graph theory. We analyzethe performance of this design when implemented on a Xilinx Virtex-II Pro FPGA.	algorithm;clock rate;clock signal;cognition;computation;computational science;field-programmable gate array;graph theory;hardware acceleration;input/output;instruction pipelining;pipeline (computing);virtex (fpga)	Ronald Scrofano;Ling Zhuo;Viktor K. Prasanna	2005			parallel computing;double-precision floating-point format;machine epsilon;floating point;binary scaling;arbitrary-precision arithmetic;saturation arithmetic;floating-point unit;minifloat;computer science	PL	4.897141105632064	45.52251917800659	80893
c54264f1cac929db704c151c4b94466127c0fdee	an accelerated recursive doubling algorithm for block tridiagonal systems	parallel solver block tridiagonal matrix cyclic reduction prefix computation;parallel solver;block tridiagonal matrix;parallel algorithms computational complexity mathematics computing matrix algebra;program processors algorithm design and analysis complexity theory acceleration bismuth runtime computer architecture;parallel solver linear equations scientific applications engineering applications prefix computation based numerical algorithm block tridiagonal system tridiagonal matrix sub optimal algorithm accelerated recursive doubling algorithm complexity analysis runtime improvements;prefix computation;cyclic reduction	Block tridiagonal systems of linear equations arise in a wide variety of scientific and engineering applications. Recursive doubling algorithm is a well-known prefix computation-based numerical algorithm that requires O(M3(N/P + log P)) work to compute the solution of a block tridiagonal system with N block rows and block size M on F processors. In real-world applications, solutions of tridiagonal systems are most often sought with multiple, often hundreds and thousands, of different right hand sides but with the same tridiagonal matrix. Here, we show that a recursive doubling algorithm is sub-optimal when computing solutions of block tridiagonal systems with multiple right hand sides and present a novel algorithm, called the accelerated recursive doubling algorithm, that delivers O(R) improvement when solving block tridiagonal systems with R distinct right hand sides. Since R is typically ~102 - 104, this improvement translates to very significant speedups in practice. Detailed complexity analyses of the new algorithm with empirical confirmation of runtime improvements are presented. To the best of our knowledge, this algorithm has not been reported before in the literature.	algorithm;analysis of algorithms;block size (cryptography);central processing unit;computation;computational complexity theory;extrapolation;linear equation;linear system;numerical analysis;numerical stability;parallel computing;period-doubling bifurcation;pointer jumping;recursion (computer science);remote database access;speedup;system of linear equations;world wide web	Sudip K. Seal	2014	2014 IEEE 28th International Parallel and Distributed Processing Symposium	10.1109/IPDPS.2014.107	tridiagonal matrix;parallel computing;theoretical computer science;block matrix;tridiagonal matrix algorithm	HPC	-1.2452933212049364	39.120659799482084	80949
6ee80d46ae6f75742dfaaf77e0e4cf276f8a7740	motion control firmware for high-speed robotic systems	motion control;software platform;real time;multiaxis motion;multimodal sensing;numerical processor chip architecture;software engineering;motion control microprogramming robot control programming robot sensing systems hardware multimodal sensors software algorithms signal processing algorithms computer architecture;chip;digital signal processor dsp;general purpose processor;software engineering control engineering computing digital signal processing chips industrial robots motion control;robot control;industrial robots;command shaping;software development;robot control command shaping digital signal processor dsp motion control;digital signal processor;digital signal processing chips;control engineering computing;high speed industrial robotic system;high speed;real time slewing control;real time slewing control motion control high speed industrial robotic system multimodal sensing multiaxis motion numerical processor chip architecture software development digital signal processor	This paper addresses the hardware and software platforms of high-speed robot control systems, which are usually integrated with multimodal sensing, multiaxis motion, and complex algorithmic capabilities. In this paper, robot control is considered from numerical processor chip architecture, board-level features, to software development cycles. Various numerical processors ranging from general purpose processors to digital signal processors are examined. This is followed by a review of the common motion control boards. Cross platform and upward compatible software development are addressed. Finally, an example application of real-time slewing control of a high-speed industrial robot is provided	central processing unit;control system;digital signal processor;firmware;forward compatibility;industrial robot;multimodal interaction;numerical analysis;real-time transcription;robot control;software development	Timothy N. Chang;Biao Cheng;Paiboon Sriwilaijaroen	2004	IEEE Transactions on Industrial Electronics	10.1109/TIE.2006.881957	chip;control engineering;motion control;embedded system;digital signal processor;real-time computing;computer science;engineering;software development;robot control	Robotics	5.367780379569141	45.09759366503634	81251
ba0a51196408045f120a623abf80e18f3f04caca	a parallel cordic algorithm for sine and cosine generation.			algorithm;cordic	Hao-Yung Lo	1997			parallel computing;discrete cosine transform	NLP	8.493671761850782	39.54699516069008	81313
4113329fa25fa7a15b8afb803ae3c078f0fb031d	the greedy approach to dictionary-based static text compression on a distributed system	distributed algorithms;robustness;scalability;static lossless compression	The greedy approach to dictionary-based static text compression can be executed by a finite-state machine. When it is applied in parallel to different blocks of data independently, there is no lack of robustness even on standard large scale distributed systems with input files of arbitrary size. Beyond standard large scale, a negative effect on the compression effectiveness is caused by the very small size of the data blocks. A robust approach for extreme distributed systems is presented in this paper, where this problem is fixed by overlapping adjacent blocks and preprocessing the neighborhoods of the boundaries.	data compression;dictionary;distributed computing;greedy algorithm	Sergio De Agostino	2015	J. Discrete Algorithms	10.1016/j.jda.2015.05.001	distributed algorithm;scalability;computer science;theoretical computer science;machine learning;distributed computing;programming language;robustness	Theory	2.322456674437124	36.22305258317342	81609
3ee2e26092bd5c573345843f4033615fda9ca3ff	data sensing and analysis: challenges for wearables	sensors mobile handsets micromechanical devices biomedical monitoring batteries energy consumption quaternions;power consumptions data analysis internet of things magnitude form factor reduction ubiquitous personalized services end users aggressive reduction battery capacity accelerometers gyroscopes economical sensors mobile phones energy consumptions ultra compact wearables wearable sensing technologies wearable sensors mems based inertial measurement units sports wearables fitness wearables energy characteristics mems imu data sensing wireless communication;wearable computers accelerometers data analysis gyroscopes microsensors power aware computing power consumption	Wearables are a leading category in the Internet of Things. Compared with mainstream mobile phones, wearables target one order of magnitude form factor reduction, and offer the potential of providing ubiquitous, personalized services to end users. Aggressive reduction in size imposes serious limits on battery capacity. Wearables are equipped with a range of sensors, such as accelerometers and gyroscopes. Most economical sensors were developed for mobile phones, with energy consumptions more appropriate for phones than for ultra-compact wearables. This article describes the energy challenges for wearable sensing technologies, with a primary focus on the most widely used wearable sensors: MEMS-based inertial measurement units. Using sports and fitness wearables as the pilot application, we analyze the energy characteristics of MEMS IMU data sensing, analysis, and wireless communication. We then discuss the technologies needed to solve the power and energy consumptions challenges for wearables.	internet of things;microelectromechanical systems;mobile phone;personalization;sensor;wearable computer	James Williamson;Qi Liu;Fenglong Lu;Wyatt Mohrman;Kun Li;Robert P. Dick;Li Shang	2015	The 20th Asia and South Pacific Design Automation Conference	10.1109/ASPDAC.2015.7058994	embedded system;electronic engineering;telecommunications;engineering	Mobile	2.725470807833052	34.59737831006241	81906
e4adbd8ec8087496c21f572b03bb898614388367	parallel vsipl++: an open standard software library for high-performance parallel signal processing	tratamiento paralelo;standard embedded computing object oriented software parallel processing self optimization signal processing;traitement signal;self optimization;programa paralelo;signal processing software libraries c language parallel programming;haute performance;traitement parallele;software libraries;software standards software libraries signal processing radar signal processing hardware video signal processing application software wireless communication biomedical imaging image coding;real time;standard;high performance embedded computing vendors;object oriented software;real time processing;adaptive optimization;parallel programming;standard software library;high performance embedded computing vendors parallel vsipl standard software library high performance parallel signal processing adaptive optimization runtime optimization automatic hardware specialization compiler communication operations parallel mapping mechanism;compiler;parallel mapping mechanism;program optimization;fast fourier transform;tratamiento tiempo real;parallel vsipl;traitement temps reel;c language;object oriented;signal processing;runtime optimization;high performance parallel signal processing;parallel computer;signal and image processing;alto rendimiento;oriente objet;optimisation programme;bibliotheque logiciel;software specification;automatic hardware specialization;procesamiento senal;orientado objeto;high performance;parallel program;open standard;parallel processing;embedded computing;optimizacion programa;communication operations;programme parallele	Real-time signal processing consumes the majority of the world's computing power. Increasingly, programmable parallel processors are used to address a wide variety of signal processing applications (e.g., scientific, video, wireless, medical, communication, encoding, radar, sonar, and imaging). In programmable systems, the major challenge is no longer hardware but software. Specifically, the key technical hurdle lies in allowing the user to write programs at high level, while still achieving performance and preserving the portability of the code across parallel computing hardware platforms. The Parallel Vector, Signal, and Image Processing Library (Parallel VSIPL++) addresses this hurdle by providing high-level C++ array constructs, a simple mechanism for mapping data and functions onto parallel hardware, and a community-defined portable interface. This paper presents an overview of the Parallel VSIPL++ standard as well as a deeper description of the technical foundations and expected performance of the library. Parallel VSIPL++ supports adaptive optimization at many levels. The C++ arrays are designed to support automatic hardware specialization by the compiler. The computation objects (e.g., fast Fourier transforms) are built with explicit setup and run stages to allow for runtime optimization. Parallel arrays and functions in Parallel VSIPL++ also support explicit setup and run stages, which are used to accelerate communication operations. The parallel mapping mechanism provides an external interface that allows optimal mappings to be generated offline and read into the system at runtime. Finally, the standard has been developed in collaboration with high performance embedded computing vendors and is compatible with their proprietary approaches to achieving performance.	adaptive optimization;analysis of algorithms;c++;central processing unit;compiler;computation;computer hardware;data parallelism;embedded system;expression templates;fast fourier transform;finite impulse response;high- and low-level;high-level programming language;image processing;library (computing);mathematical optimization;moving target indication;online and offline;parallel computing;partial template specialization;pointer (computer programming);qr code;radar;real-time transcription;requirement;run time (program lifecycle phase);sonar (symantec);signal processing;software portability;throughput	James M. Lebak;Jeremy Kepner;Henry Hoffmann;Edward Rutledge	2005	Proceedings of the IEEE	10.1109/JPROC.2004.840303	adaptive optimization;parallel processing;fast fourier transform;software requirements specification;compiler;parallel computing;real-time computing;open standard;computer science;theoretical computer science;program optimization;signal processing;object-oriented programming	HPC	2.4099750204095045	46.21007535524094	82050
e538239c415704c0f076b3b7900ec88ccafaa311	fault table computation on gpus	hardware acceleration;nvidia quadro fx 5800;thread level parallelism;paper;fault simulation;testing and debugging;hardware accelerator;critical path;fault detection;nvidia;graphic processing unit;computer science;fault table;memory bandwidth;fault diagnosis	In this paper, we explore the implementation of fault table generation on a Graphics Processing Unit (GPU). A fault table is essential for fault diagnosis and fault detection in VLSI testing and debug. Generating a fault table requires extensive fault simulation, with no fault dropping, and is extremely expensive from a computational standpoint. Fault simulation is inherently parallelizable, and the large number of threads that a GPU can operate on in parallel can be employed to accelerate fault simulation, and thereby accelerate fault table generation. Our approach, called GFTABLE, employs a pattern parallel approach which utilizes both bit-parallelism and thread-level parallelism. Our implementation is a significantly modified version of FSIM, which is pattern parallel fault simulation approach for single core processors. Like FSIM, GFTABLE utilizes critical path tracing and the dominator concept to reduce runtime. Further modifications to FSIM allow us to maximally harness the GPU’s huge memory bandwidth and high computational power. Our approach does not store the circuit (or any part of the circuit) on the GPU. Efficient parallel reduction operations are implemented in our implementation of GFTABLE. We Responsible Editor: P. Mishra K. Gulati (B) · S. P. Khatri Department of ECE, Texas A&M University, College Station, TX 77843, USA e-mail: kgulati@tamu.edu S. P. Khatri e-mail: sunilkhatri@tamu.edu compare our performance to FSIM∗, which is FSIM modified to generate a fault table on a single core processor. Our experiments indicate that GFTABLE, implemented on a single NVIDIA Quadro FX 5800 GPU card, can generate a fault table for 0.5 million test patterns on average 15.68× faster when compared with FSIM∗. With the NVIDIA Tesla server, our approach would be potentially 89.57× faster.	analysis of algorithms;bit-level parallelism;central processing unit;computation;computational resource;critical path method;dominator (graph theory);electrical engineering;email;experiment;fault detection and isolation;geforce fx series;graphics processing unit;lookup table;memory bandwidth;nokia 5800 xpressmusic;nvidia quadro;nvidia tesla;parallel computing;path tracing;server (computing);simulation;task parallelism;test card	Kanupriya Gulati;Sunil P. Khatri	2010	J. Electronic Testing	10.1007/s10836-010-5147-x	embedded system;computer architecture;parallel computing;real-time computing;hardware acceleration;computer science;stuck-at fault;segmentation fault;general protection fault;software fault tolerance	HPC	1.6171089828972531	40.9916738571819	82132
a7e91078f1c3a6b486d90025194fd6952f3ec983	efficient interleaved montgomery modular multiplication for lattice-based cryptography	interleaved montgomery modular multiplication;lattice based cryptography;ntruencrypt;gpu implementation		lattice-based cryptography;montgomery modular multiplication	Sedat Akleylek;Zaliha Yüce Tok	2014	IEICE Electronic Express	10.1587/elex.11.20140960	arithmetic;lattice-based cryptography;kochanski multiplication;computer science;theoretical computer science;mathematics;ntruencrypt;algebra	Crypto	9.050391232904984	43.60596773487987	82187
da5fbc534be38bf7eebcd71f41a8ccfe2da34950	the importance and characteristics of communication in high performance data analytics	instruments;tree searching data analysis graph theory parallel algorithms parallel machines;electric breakdown;runtime;data analysis;computational modeling;distributed databases;network design optimization high performance data analytics social networks business analytics large scale distributed computing systems scalable parallel algorithms graph based analytics graph 500 benchmark supercomputing systems breadth first search graph traversal problem bfs graph traversal problem graph 500 mpi network data motion analysis large scale high performance computing system marenostrum iii supercomputer node to node communication;benchmark testing;runtime instruments benchmark testing electric breakdown data analysis computational modeling distributed databases	Social networks and business analytics typically need to process vast amounts of data that are often modeled as graphs. The scale of the data that such applications have to handle requires large-scale distributed computing systems, together with scalable parallel algorithms, to efficiently process the graphs. Representative of the graph-based analytics class of applications is the Graph 500 benchmark (Murphy, et.al., 2010), which is designed to assess the performance of supercomputing systems by solving the Breadth-First Search (BFS) graph traversal problem. In this work, we analyze the network data motion of a Graph 500 MPI version of the graph traversal problem, using a large-scale high-performance computing system, i.e., the MareNostrum III supercomputer (http://www.bsc.es/marenostrum-support-services/mn3). We focus our analysis on the node-to-node communication and show that the application runtime is communication-bound, the communication making up as much as 80% of the execution time of each BFS iteration. We also show that the dominating communication pattern is an overall all-to-all exchange (every process communicates to every other process and roughly the same amount of data is exchanged between any two processes), thus providing preliminary guidance for future application or network design optimization efforts.	benchmark (computing);breadth-first search;business analytics;distributed computing;graph traversal;iteration;mathematical optimization;network planning and design;parallel algorithm;run time (program lifecycle phase);scalability;supercomputer;tree traversal	Andreea Anghel;Germán Rodríguez;Bogdan Prisacari	2014	2014 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2014.6983044	benchmark;computer architecture;parallel computing;wait-for graph;power graph analysis;computer science;theoretical computer science;operating system;machine learning;distributed computing;data analysis;programming language;computational model;distributed database;graph database	HPC	-2.7978439911298447	42.68209824238622	82212
f402ee49e534a08545ec21e9de6003cd8c827fe5	a survey on design and implementation of floating point adder in fpga		FPGAs are widely being used in high performance and scientific computing applications. FPGAs are suitable for such applications when conventional processors implementations do not satisfy the realtime and high performance requirements. Most of these applications demand high numerical stability and accuracy, and wide range of numbers. Hence, they are usually based on floating point. Implementing floating point unit in FPGA is one of the crucial issues of hardware implementations. Floating point addition is the most frequent floating point operation and takes most of the scientific operation. It is a costly operation in terms of time and hardware. The main objectives of implementing floating point in FPGA are to achieve high performance with less area, increase the throughput, and decrease the latency. This review paper presents a survey on floating point addition algorithms and techniques, and the implementation of floating point adder in FPGA.	adder (electronics);field-programmable gate array	Luka Daoud;Dawid Zydek;Henry Selvaraj	2014		10.1007/978-3-319-08422-0_129	adder	EDA	5.701892554877038	45.24640412065755	82377
7febc7204004cfa2df9af2ddc00cc7034da1ec27	on-chip network-enabled multicore platforms targeting maximum likelihood phylogeny reconstruction	network on chip noc;optimisation;kernel;multi threading;performance evaluation;phylogeny;phylogeny reconstruction;maximum likelihood;network on chip;bayesian inference;resource manager;resource management;inference mechanisms;hardware accelerator;presentation;search problems bioinformatics inference mechanisms multiprocessing systems multi threading network on chip optimisation;phylogeny resource management switches multicore processing kernel hardware;end to end runtime reductions on chip network enabled multicore platforms maximum likelihood phylogeny reconstruction phylogenetic inference phylogenetic tree evolutionary relationship statistical estimation approaches bayesian inference multidimensional real continuous space search trees cpu hours node level likelihood vector computation branch length optimization network on chip noc architectures raxml ml software suites state of the art multithreaded software;chip;search trees;multicore;phylogenetic tree;multicore processing;search problems;multiprocessing systems;switches;statistical estimation;phylogenetic inference;phylogeny reconstruction hardware accelerator multicore network on chip noc;hardware;bioinformatics	In phylogenetic inference, which aims at finding a phylogenetic tree that best explains the evolutionary relationship among a given set of species, statistical estimation approaches such as maximum likelihood (ML) and Bayesian inference provide more accurate estimates than other nonstatistical approaches. However, the improved quality comes at a higher computational cost, as these approaches, even though heuristic driven, involve optimization over multidimensional real continuous space. The number of possible search trees in ML is at least exponential, thereby making runtimes on even modest-sized datasets to clock up to several million CPU hours. Evaluation of these trees, involving node-level likelihood vector computation and branch-length optimization, can be partitioned into tasks (or kernels), providing the application with the potential to benefit from hardware acceleration. The range of hardware acceleration architectures tried so far offer limited degree of fine-grain parallelism. Network-on-chip (NoC) is an emerging paradigm that can efficiently support integration of massive number of cores on a chip. In this paper, we explore the design and performance evaluation of 2-D and 3-D NoC architectures for RAxML, which is one of the most widely used ML software suites. Specifically, we implement the computation kernels of the top three functions consuming more than 85% of the total software runtime. Simulations show that through appropriate choice of NoC architecture, and novel core design, allocation and placement strategies, our NoC-based implementation can achieve individual function-level speedups of 390x to 847x, speed up the targeted kernels in excess of 6500x, and provide end-to-end runtime reductions up to 5x over state-of-the-art multithreaded software.	aggregate data;algorithm;algorithmic efficiency;bioinformatics;central processing unit;computation;computational phylogenetics;computer simulation;end-to-end principle;estimation theory;experiment;function-level programming;hardware acceleration;heuristic;mathematical optimization;multi-core processor;network on a chip;offset binary;parallel computing;performance evaluation;phylogenetic tree;programming paradigm;routing;software suite;space-filling curve;speedup;thread (computing);time complexity	Turbo Majumder;Michael Edward Borgens;Partha Pratim Pande;Anantharaman Kalyanaraman	2012	IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems	10.1109/TCAD.2012.2188401	multi-core processor;embedded system;parallel computing;real-time computing;computer science;resource management;theoretical computer science;operating system;statistics	EDA	-0.10531068332881971	44.1576273076061	82555
e890824cdce45a543479e3d186ec3867877feda9	reaping the processing potential of fpga on double-precision floating-point operations: an eigenvalue solver case study	eigenvalues and eigenfunctions;scientific application;mathematics computing;performance evaluation;clocks;floating point processing;pipeline arithmetic eigenvalues and eigenfunctions field programmable gate arrays floating point arithmetic mathematics computing;field programmable gate arrays eigenvalues and eigenfunctions hardware computer science usa councils application software pipeline processing electromagnetics coprocessors image processing;data processing;fpga;electromagnetic field;eigenvalues;performance improvement;adders;eigenvalue solver;numerical linear algebra fpga double precision floating point operations eigenvalue solver electromagnetics floating point processing;blades;electromagnetics;electromagnetics double precision floating point operations eigenvalue solver field programmable gate arrays fpgas;floating point;numerical linear algebra;floating point arithmetic;field programmable gate arrays;double precision floating point operations;pipeline arithmetic;field programmable gate arrays fpgas;hardware	Many scientific applications such as electromagnatics require their operations carried out in double-precision floating-point format. The efficiency of these applications is mainly subject to the floating-point processing performance on the target processors. In this work, we use an eigenvalue solver application as a case study to demonstrate the processing potential of an FPGA device when dealing with floating-point operations. Relying on deep pipelines and large local memory directly accessible within the FPGA device, more than 20$\times$ performance improvement has been achieved for this particular case whose computation is intrinsically sequential. The methodology described in this paper can be conveniently applied to other domain applications that share similar data processing characteristics, demonstrating an impact on a broad range of scientific fields, e.g., numerical linear algebra.	american and british english spelling differences;cpu cache;central processing unit;computation;coprocessor;data access;double-precision floating-point format;experiment;field-programmable gate array;general-purpose computing on graphics processing units;memory bank;microprocessor;numerical linear algebra;pipeline (computing);solver;speedup;xilinx ise	Miaoqing Huang;Özlem Kilic	2010	2010 18th IEEE Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2010.23	embedded system;parallel computing;data processing;computer science;floating point;theoretical computer science;field-programmable gate array	Arch	-3.50702582316694	39.925122779588044	82678
4fe97fbfba3a32dd07b5ce0b77e2550c899815da	a hybrid parallel algorithm for the auction algorithm in multicore systems	computer vision;heuristic algorithms;multicore processing;clustering algorithms;bipartite graph;algorithm design and analysis	The bipartite graph matching problem is based on finding a point that maximizes the chances of similarity with another one, and it is explored in several areas such as Bioinformatics and Computer Vision. To solve that matching problem the auction algorithm has been widely used and its parallel implementation is employed to find matching solutions in a reasonable computational time. For example, image analysis may require a large amount of processing, as dense images can have thousands of points to be considered. Furthermore, to exploit the benefits of multicore architectures, a hybrid implementation can be used to deal with communication in both distributed and shared memory. The main goal of this paper is to implement and evaluate the performance of an hybrid parallel auction algorithm for multicore clusters. The experiments carried out analyzes the problem size, the number of iterations to solve the matching and the impact of these parameters in the communication costs and how it affects the execution times.	analysis of algorithms;auction algorithm;bioinformatics;computation;computer vision;experiment;image analysis;image scaling;iteration;loop scheduling;matching (graph theory);multi-core processor;overhead (computing);parallel algorithm;scalability;scheduling (computing);shared memory;time complexity;unbalanced circuit	Aline de P. Nascimento;C. N. Vasconcelos;F. S. Jamel;Alexandre da Costa Sena	2016	2016 International Symposium on Computer Architecture and High Performance Computing Workshops (SBAC-PADW)	10.1109/SBAC-PADW.2016.21	multi-core processor;auction algorithm;algorithm design;parallel computing;bipartite graph;computer science;3-dimensional matching;theoretical computer science;machine learning;distributed computing;assignment problem;cluster analysis	Arch	-1.162950684237486	42.12834021702682	82715
e6eb7af765185d6e87664ebee0ac2b5294ef4732	implementation of µnacl on 32-bit arm cortex-m0		By the deployment of Internet of Things, embedded systems using microcontroller are nowadays under threats through the network and incorporating security measure to the systems is highly required. Unfortunately, microcontrollers are not so powerful enough to execute standard security programs and need light-weight, high-speed and secure cryptographic libraries. In this paper, we port NaCl cryptographic library to ARM Cortex-M0(M0+) Microcontroller, where we put much effort in fast and secure implementation. Through the evaluation we show that the implementation achieves about 3 times faster than AVR NaCl result and reduce half of the code size. key words: Cortex-M0, crypto library, embedded, microcontroller, IoT	32-bit;arm cortex-m;arm architecture;atmel avr;cryptography;embedded system;internet of things;library (computing);microcontroller;software deployment	Toshifumi Nishinaga;Masahiro Mambo	2016	IEICE Transactions		microcontroller;embedded system;computer hardware;computer science;operating system;internet of things	Security	7.852773401425536	45.115865599319	82797
54bb24d6ca0e93e4e4b131ff19206f2a8f9c1743	adaptive duty cycling for place-centric mobility monitoring using zero-cost information in smartphone	adaptive duty cycling;poles and towers;sensors poles and towers batteries ieee 802 11 standards hidden markov models monitoring global positioning system;sensors;mobility learning and prediciton;smart phones hidden markov models mobility management mobile radio;hidden markov models;monitoring;global positioning system;batteries;ieee 802 11 standards;real time operation adaptive duty cycling place centric mobility monitoring zero cost information smartphone mobility data battery capacity energy issues mobility learning optimal performance energy consumption cellular connection patterns battery charging behaviors human mobility hidden markov model;smartphone;smartphone mobility learning and prediciton adaptive duty cycling	Smartphones enable the collection of mobility data using various sensors. The key challenge in the collection of continuous data is to overcome the limited battery capacity of the device. While extensive research has been conducted to solve energy issues in continuous mobility learning, we argue that previous works have not reached optimal performance. In this paper, we propose an energy-efficient mobility monitoring system, FreeTrack, to collect place-centric mobility data with minimum energy consumption in everyday life. We first analyzed the regularity of life patterns, cellular connection patterns, and battery charging behaviors of 94 smartphone users to examine important features related to human mobility. Based on our findings, we design an adaptive duty cycling scheme that uses zero-cost information (i.e., regular mobility, cell connection, and battery state) as low-level sensing to infer location change without the need to activate sensors. We model the location inference on the Hidden Markov Model and optimize the sensing schedule of individual smartphones for real-time operation. Our extensive experiment with 48 smartphone users shows that the proposed system achieves an energy saving of about 68% over previous works, yet still correctly traces 97% of mobility with 0.2±0.5 places misses in a day.	battery charger;cell (microprocessor);duty cycle;experiment;freetrack;hidden markov model;high- and low-level;location inference;markov chain;real-time clock;scheduling (computing);sensor;smartphone;tracing (software)	Yohan Chon;Yungeun Kim;Hyojeong Shin;Hojung Cha	2014	IEEE Transactions on Mobile Computing	10.1109/TMC.2013.151	embedded system;simulation;global positioning system;telecommunications;computer science;sensor;mobility model;hidden markov model	Mobile	1.4447273561338658	34.14058777964865	82953
4156ca32cf3232a69d93c688aaab0ce8bf3dbaf6	a comparison study of heuristics for mapping parallel algorithms to message-passing multiprocessors	parallel algorithm;graph clustering;task flow graphs;f 1 2;message passing interface;clustering;taylor and francis;original articles;message passing;communication cost;c 1 2;mapping;task graphs;communication channels	This paper presents a comparison study of popular clustering and mapping heuristics which are used to map taskow graphs to message-passing multiprocessors. To this end, we use task-graphs which are representative of important scienti c algorithms running on data-sets of practical interest. The annotation which assigns weights to nodes and edges of the task-graphs is realistic. It re ects current trends in processor, communication channel, and message-passing interface technology and takes into consideration hardware characteristics of state-of-the-art multiprocessors. Our experiments show that applying realistic models for task-graph annotation a ects the e ectiveness and functionality of clustering and mapping techniques. Therefore, new heuristics are necessary that will take into account more practical models of communication costs. We present modi cations to existing clustering and mapping algorithms which improve their e ciency and running-time for the practical models adopted.	admissible heuristic;analysis of algorithms;blocking (computing);central processing unit;channel (communications);cluster analysis;computation;computer cluster;experiment;fast multipole method;graph dynamical system;heuristic (computer science);ibm notes;list scheduling;load balancing (computing);message passing interface;n-body problem;non-blocking algorithm;overhead (computing);parallel algorithm;run time (program lifecycle phase);sap netweaver;schedule (computer science);speedup;time complexity	Marios D. Dikaiakos;Anne Rogers;Kenneth Steiglitz	1995	Parallel Algorithms Appl.	10.1080/10637199508915537	parallel computing;message passing;computer science;message passing interface;theoretical computer science;clustering coefficient;distributed computing;parallel algorithm;cluster analysis;algorithm;channel	HPC	-3.594683628014699	41.6403606526885	82978
8dfcf92e09755975cd9128eeb82630a5d73f3f6d	using phipac to speed error back-propagation learning	libraries;software portability;learning algorithm;libraries signal processing algorithms workstations convergence speech recognition production registers guidelines space exploration computer errors;convergence;error back propagation;software libraries;neural nets;convergence of numerical methods;performance;standard speech recognition data set;coding method;space exploration;bunch size;training speed;ansi c;standard speech recognition data set error backpropagation learning phipac coding method portable high performance numerical libraries ansi c optimized matrix multiply routines performance workstations bunch mode backpropagation algorithm bunch size convergence rate training speed;convergence rate;backpropagation;software portability speech recognition neural nets backpropagation software libraries matrix multiplication digital arithmetic convergence of numerical methods;error backpropagation learning;optimized matrix multiply routines;bunch mode backpropagation algorithm;guidelines;registers;workstations;back propagation algorithm;production;speech recognition;digital arithmetic;matrix multiplication;signal processing algorithms;high performance;back propagation;portable high performance numerical libraries;computer errors;phipac	We i n troduce PHiPAC, a coding methodology for developing portable high-performance numerical libraries in ANSI C. Using this methodology, w e h a v e developed code for optimized matrix multiply routines. These routines can achieve o v er 90% of peak performance on a variety of current w orkstations, and are often faster than vendor-supplied optimized libraries. We then describe the bunch-mode back-propagation algorithm and how it can use the PHiPAC derived matrix multiply routines. Using a set of plots, we investigate the tradeos between bunch size, convergence rate, and training speed using a standard speech recognition data set and show h o w use of the PHiPAC routines can lead to a signicantly faster back-propagation learning algorithm.	algorithm;backpropagation;convolution;cross-correlation;infinite impulse response;library (computing);matrix multiplication;numerical analysis;rate of convergence;signal processing;software propagation;speech recognition	Jeff A. Bilmes;Krste Asanovic;Chee-Whye Chin;James Demmel	1997		10.1109/ICASSP.1997.604861	parallel computing;speech recognition;computer science;backpropagation;theoretical computer science;machine learning;artificial neural network	ML	7.205103754907967	40.525053909204125	83057
4bb765736f5b390683bc9fec2ea16c1caed15d9b	communication performance models in prism : a spectral element-fourier parallel navier-stokes solver	computational mechanics;numerical method;benchmark;navier stokes equations;computer performance;incompressible navier stokes equation;navier stokes;testing;supercomputer;three dimensional;finite element;direct numerical simulation;pattern analysis performance analysis numerical simulation navier stokes equations couplings costs delay predictive models testing timing;performance analysis;performance model;model;communication cost;predictive models;pattern analysis;prediction model;couplings;multiply connected domain;communication pattern;numerical simulation;timing	In this paper we analyze communication patterns in the parallel three-dimensional Navier-Stokes solver Prism, and present performance results on the IBM SP2, the Cray T3D and the SGI Power Challenge XL. Prism is used for direct numerical simulation of turbulence in non-separable and multiply-connected domains. The numerical method used in the solver is based on mixed spectral element-Fourier expansions in (x-y) planes and z-direction, respectively. Each (or a group) of Fourier modes is computed on a separate processor as the linear contributions (Helmholtz solves) are completely uncoupled in the incompressible Navier-Stokes equations; coupling is obtained via the nonlinear contributions (convective terms). The transfer of data between physical and Fourier space requires a series of complete exchange operations, which dominate the communication cost for small number of processors. As the number of processors increases, global reduction and gather operations become important while complete exchange becomes more latency dominated. Predictive models for these communication operations are proposed and tested against measurements. A relatively large variation in communication timings per iteration is observed in simulations and quantified in terms of specific operations. A number of improvements are proposed that could significantly reduce the communications overhead with increasing numbers of processors, and {\em generic} predictive maps are developed for the complete exchange operation, which remains the fundamental communication in Prism. Results presented in this paper are representative of a wider class of parallel spectral and finite element codes for computational mechanics which require similar communication operations.	central processing unit;code;computational mechanics;computer simulation;cray t3d;direct numerical simulation;finite element method;iteration;map;navier–stokes equations;nonlinear system;numerical method;overhead (computing);solver;turbulence	Constantinos Evangelinos;George Em Karniadakis	1996	Proceedings of the 1996 ACM/IEEE Conference on Supercomputing	10.1145/369028.369068	mathematical optimization;supercomputer;parallel computing;simulation;computer science;computational mechanics;theoretical computer science;computer performance;predictive modelling	HPC	-3.9627912515281802	38.32091978203516	83116
1d3a495ba7cb9714bee125a28e43628ca666a917	an iteration-based hybrid parallel algorithm for tridiagonal systems of equations on multi-core architectures	multi threading;tridiagonal system of equations;multi core architecture;hybrid parallel algorithm	An optimized parallel algorithm is proposed to solve the problem occurred in the process of complicated backward substitution of cyclic reduction during solving tridiagonal linear systems. Adopting a hybrid parallel model, this algorithm combines the cyclic reduction method and the partition method. This hybrid algorithm has simple backward substitution on parallel computers comparing with the cyclic reduction method. In this paper, the operation count and execution time are obtained to evaluate and make comparison for these methods. On the basis of results of these measured parameters, the hybrid algorithm using the hybrid approach with a multi-threading implementation achieves better efficiency than the other parallel methods, that is, the cyclic reduction and the partition methods. In particular, the approach involved in this paper has the least scalar operation count and the shortest execution time on a multi-core computer when the size of equations meets some dimension threshold. The hybrid parallel algorithm improves the performance of the cyclic reduction and partition methods by 19.2% and 13.2%, respectively. In addition, by comparing the single-iteration and multi-iteration hybrid parallel algorithms, it is found that increasing iteration steps of the cyclic reduction method does not affect the performance of the hybrid parallel algorithm very much. Copyright © 2015 John Wiley & Sons, Ltd.	computer;cyclic reduction;hybrid algorithm;iteration;john d. wiley;linear system;multi-core processor;parallel algorithm;parallel computing;run time (program lifecycle phase);thread (computing)	Guangping Tang;Wangdong Yang;Keqin Li;Yu Ye;Guoqing Xiao;Keqin Li	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3511	parallel computing;multithreading;computer science;theoretical computer science;distributed computing	HPC	-3.767146037275088	38.77145093787155	83129
a3b4133fb1a65f35b9b7950da9786d23fe5723b4	an efficient multi-core implementation of a novel hss-structured multifrontal solver using randomized sampling	multifrontal method;sparse gaussian elimination;parallel algorithm;65f05;hss matrices;97n80;65f50	We present a sparse linear system solver that is based on a multifrontal variant of Gaussian elimination, and exploits low-rank approximation of the resulting dense frontal matrices. We use hierarchically semiseparable (HSS) matrices, which have low-rank off-diagonal blocks, to approximate the frontal matrices. For HSS matrix construction, a randomized sampling algorithm is used together with interpolative decompositions. The combination of the randomized compression with a fast ULV HSS factorization leads to a solver with lower computational complexity than the standard multifrontal method for many applications, resulting in speedups up to 7 fold for problems in our test suite. The implementation targets many-core systems by using task parallelism with dynamic runtime scheduling. Numerical experiments show performance improvements over state-of-the-art sparse direct solvers. The implementation achieves high performance and good scalability on a range of modern shared memory parallel systems, including the Intel © Xeon Phi (MIC). The code is part of a software package called STRUMPACK – STRUctured Matrices PACKage, which also has a distributed memory component for dense rank-structured matrices.	approximation algorithm;computational complexity theory;distributed memory;experiment;frontal solver;gaussian elimination;high-speed serial interface;linear system;low-rank approximation;manycore processor;multi-core processor;parallel computing;randomized algorithm;sampling (signal processing);scalability;scheduling (computing);shared memory;singular value decomposition;sparse matrix;task parallelism;test suite;uml state machine;xeon phi	Pieter Ghysels;Xiaoye S. Li;François-Henry Rouet;Samuel Williams;Artem Napov	2016	SIAM J. Scientific Computing	10.1137/15M1010117	mathematical optimization;parallel computing;computer science;theoretical computer science;parallel algorithm	HPC	-2.657167806645944	38.817824502109225	83256
d51a66f791f2ac143060f34762e844060266dab1	fast, general parallel computation for machine learning		Today the terms machine learning (ML) and Big Data are closely correlated. This, and the complexity of many ML algorithms, motivates a search for fast parallel computation methods. A further motivating factor is a need to deal with memory size limitations, especially for the moderately-sized machines common in many ML applications. In addition, it is desirable to develop generally applicable methods, rather than needing to develop a different parallel approach for every ML algorithm. In this work, we apply a technique we call Software Alchemy to ML. We are particularly interested in ML for recommender systems, and explore the feasibility of SA in that context.	algorithm;big data;computation;machine learning;parallel computing;recommender system	Robin Elizabeth Yancey;Norman Matloff	2018		10.1145/3229710.3229716	recommender system;distributed computing;parallel computing;software;big data;computer science;machine learning;artificial intelligence	PL	-1.77951338429044	41.98379545647281	83337
395dad219a90e99d5f0cdd26f28facb31912d485	zedwulf: power-performance tradeoffs of a 32-node zynq soc cluster	field programmable gate array;random access memory;power performance tradeoff;system on chip application program interfaces field programmable gate arrays message passing power aware computing;programmable fpga fabric;system on chip field programmable gate arrays bandwidth throughput neurons random access memory computational modeling;conference paper;power aware computing;zynq soc cluster;computational modeling;message passing interface;computer science and engineering;system on chip;communication bound sparse graph oriented applications;application program interfaces;message passing;arm processor;bandwidth;neurons;neural network simulations;field programmable gate arrays;mpi routines;arm processor zedwulf power performance tradeoff zynq soc cluster system on chip programmable fpga fabric field programmable gate array communication bound sparse graph oriented applications neural network simulations mpi routines message passing interface;throughput;zedwulf	Commodity SoCs with hybrid architectures that combine CPUs with programmable FPGA fabric such as the Xilinx Zynq SoC have become a competitive energy-efficient platform for addressing irregular parallelism in graph problems. In this paper, we prototype a 32-node cluster composed from these Zynq SoC chips to accelerate communication-bound sparse graph-oriented applications such as neural network simulations. We develop specialized MPI routines specifically developed for irregular accelerator-to-accelerator communication of small message traffic. We use the ARM processor for handling the MPI stack while offloading compute-intensive calculations to the FPGA. For graphs with 32M nodes and 32M edges, Zedwulf delivers the highest 94 MTEPS (Million Traversed Edges Per Second)throughput over other x86 multi-threaded platforms in our study by 1.2 -- 1.4×. For this experiment, Zedwulf operates at an efficiency of 0.49 MTEPS/W when using ARM+FPGA which is1.2× better than using ARMv7 CPUs alone, and within 8% of the Intel Core i7-4770k platform.	arm architecture;artificial neural network;central processing unit;field-programmable gate array;graph (abstract data type);haswell (microarchitecture);message passing interface;parallel computing;prototype;simulation;sparse matrix;system on a chip;thread (computing);throughput;traversed edges per second;x86	Pradeep Moorthy;Nachiket Kapre	2015	2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2015.37	embedded system;computer architecture;parallel computing;computer science;operating system;field-programmable gate array	Arch	-1.3268064446795877	46.149391344437674	83356
1f7df75135aca59bb4229218902061de6cac485e	parallelization of digital wavelet transformation of ecg signals		The advances in electronics and ICT industry for biomedical use have initiated a lot of new possibilities. However, these IoT solutions face the big data challenge where data comes with a certain velocity and huge quantities. In this paper, we analyze a situation where wearable ECG sensors stream continuous data to the servers. A server needs to receive these streams from a lot of sensors and needs to star various digital signal processing techniques initiating huge processing demands. Our focus in this paper is on optimizing the sequential Wavelet Transform filter. Due to the highly dependent structure of the transformation procedure we propose several optimization techniques for efficient parallelization. We set a hypothesis that optimizing the DWT initialization and processing part can yield a faster code. In this paper, we have provided several experiments to test the validity of this hypothesis by using OpenMP for parallelization. Our analysis shows that proposed techniques can optimize the sequential version of the code.	automatic parallelization;barrier (computer science);baseline (configuration management);big data;dataflow architecture;digital signal processing;discrete wavelet transform;experiment;graphics processing unit;information;iteration;mathematical optimization;openmp;parallel algorithm;parallel computing;scalability;sensor;server (computing);speedup;velocity (software development);wearable computer	Ervin Domazet;Marjan Gusev	2017	2017 40th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)	10.23919/MIPRO.2017.7973442	streams;parallel computing;wavelet;theoretical computer science;wavelet transform;big data;initialization;real-time computing;computer science;digital signal processing;server;automatic parallelization	DB	6.670270291252773	40.72521672852752	83468
561bf25b01b9ce9b3b2d997c8abcb3431b9b97a2	accelerating the resolution of generalized lyapunov matrix equations on hybrid architectures	generalized lyapunov matrix equations;matrix sign function;matrix inverse;nvidia gpu generalized lyapunov matrix equations hybrid architectures bilinear model order reduction linear stochastic differential equations generalized riccati equations stochastic control generalized lyapunov equation solver matrix sign function method multicore general purpose processor multicore cpu hardware accelerator graphics processing unit;mathematical model graphics processing units acceleration stochastic processes hardware memory management central processing unit;graphics processors;multi core processors;multiprocessing systems computer architecture graphics processing units lyapunov matrix equations;matrix inverse generalized lyapunov matrix equations matrix sign function graphics processors multi core processors	Generalized Lyapunov equations play an important role in bilinear model order reduction and linear stochastic differential equations. They also arise solving generalized Riccati equations from stochastic control. In this paper we present a high-performance implementation of a Generalized Lyapunov equation solver based on the matrix sign function method for heterogeneous platforms, composed by a multi-core general purpose processor (multi-core CPU) and a hardware accelerator (graphics processing unit - GPU). Our CPU proposal shows runtime reductions of up to 5.2x over the original method, while a GPU-enabled version offers an extra speed-up near to 2.2x over the CPU counterpart, in a platform equipped with an NVIDIA GPU. Additionally, both variants show a good scalability on the problem dimension.	bilinear filtering;central processing unit;computer graphics;graphics processing unit;hardware acceleration;lyapunov fractal;model order reduction;multi-core processor;navier–stokes equations;scalability;solver;stochastic control;the matrix	Rodrigo Baya;Ignacio Decia;Pablo Ezzatti;Hermann Mena	2016	2016 International Conference on High Performance Computing & Simulation (HPCS)	10.1109/HPCSim.2016.7568397	multi-core processor;mathematical optimization;parallel computing;computer science;theoretical computer science;operating system;generalized linear array model	HPC	-3.989031226028483	39.40990131180187	83789
2e0016235b7ea370b6f2cbb556c8b16b42ccebc5	a study on the efficient parallel block lanczos method	methode a pas;block lanczos method;shared memory;storage access;ordinateur parallele;factorisation qr;memoria compartida;message passing system;methode lanczos bloc;qr factorization;metodo lanczos;memory access;factorizacion qr;computer architecture;step method;parallel computer architecture;architecture ordinateur;lanczos method;envoi message;methode lanczos;acces memoire;ordenador paralelo;parallel computer;message passing;acceso memoria;arquitectura ordenador;shared memory system;memoire partagee;metodo a paso	In order to use parallel computers in specific applications, algorithms need to be developed and mapped onto parallel computer architectures. Main memory access for shared memory system or global communication in message passing system deteriorate the computation speed. In this paper, it is found that the m-step generalization of the block Lanczos method enhances parallel properties by forming m simultaneous search direction vector blocks. QR factorization, which lowers the speed on parallel computers, is not necessary in the m-step block Lanczos method. The m-step method has the minimized synchronization points, which resulted in the minimized global communications and main memory accesses compared to the standard method.	lanczos algorithm	Sun Kyung Kim;Taehee Kim	2004		10.1007/978-3-540-30497-5_37	shared memory;parallel computing;message passing;computer science;theoretical computer science;operating system;programming language;qr decomposition;algorithm	HPC	-2.1692314415144947	37.30785313301458	83906
01d562712cb3dd4edca991d064369a1d8442512e	belief propagation by message passing in junction trees: computing each message faster using gpu parallelization	parallel computing;gpus;paper;bepress selected works;cuda;bayesian;junction trees;nvidia;algorithms;data structures and algorithms;computer science;nvidia geforce gtx 460;bayesian networks junction trees parallel computing gpus cuda;bayesian networks	Compiling Bayesian networks (BNs) to junction trees and performing belief propagation over them is among the most prominent approaches to computing posteriors in BNs. However, belief propagation over junction tree is known to be computationally intensive in the general case. Its complexity may increase dramatically with the connectivity and state space cardinality of Bayesian network nodes. In this paper, we address this computational challenge using GPU parallelization. We develop data structures and algorithms that extend existing junction tree techniques, and specifically develop a novel approach to computing each belief propagation message in parallel. We implement our approach on an NVIDIA GPU and test it using BNs from several applications. Experimentally, we study how junction tree parameters affect parallelization opportunities and hence the performance of our algorithm. We achieve speedups ranging from 0.68 to 9.18 for the BNs studied.	algorithm;automatic parallelization;bayesian network;belief propagation;cas latency;cuda;computation;data structure;experiment;geforce;graphics processing unit;message passing;overhead (computing);parallel computing;planar separator theorem;software propagation;speedup;state space;theory;tree decomposition	Lu Zheng;Ole J. Mengshoel;Jike Chong	2011			parallel computing;data structure;bayesian probability;computer science;theoretical computer science;machine learning;bayesian network;distributed computing	ML	0.7598837636116882	41.449333466161384	83937
e5d6decdd8bfd54494cc594e33c568b047420474	scalable and unified digit-serial processor array architecture for multiplication and inversion over gf( $2^{m}$ )		"""This paper proposes a scalable and unified digit-serial structure, with low space complexity to perform multiplication and inversion operations in <inline-formula> <tex-math notation=""""LaTeX"""">$GF(2^{m})$ </tex-math></inline-formula>, based on the bit serial multiplication algorithm and the previously modified extended Euclidean inversion algorithm. In this structure, the multiplier and inverter shares the data-path and thus saves more area resources and power than the case of using separate data-path for each operation. Also, this structure is suitable for fixed size processor that only reuse the core and does not require to modulate the core size when the field size <inline-formula> <tex-math notation=""""LaTeX"""">$m$ </tex-math></inline-formula> is modified. This structure is extracted by applying a nonlinear methodology that gives the designer more flexibility to control the processing element workload. Implementation results for of the proposed scalable and unified digit-serial design and previously reported efficient designs show that the proposed scalable structure achieves a significant reduction in area ranging from 64.3% to 85.5% and also achieves a significant saving in energy ranging from 21.9% to 92.5% over them, but it has lower throughput compared with them. This makes the proposed design more suitable for constrained implementations of cryptographic primitives in ultra-low power devices, such as wireless sensor nodes and radio frequency identification devices."""	cryptographic primitive;cryptography;dspace;grammatical framework;multiplication algorithm;nonlinear system;overhead (computing);power inverter;power semiconductor device;processor array;radio frequency;radio-frequency identification;scalability;serial communication;throughput	Atef Ibrahim;Fayez Gebali	2017	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2017.2691353	parallel computing;throughput;processor array;scalability;architecture;real-time computing;ranging;gf(2);multiplication algorithm;multiplication;computer science	Arch	9.833680180528615	44.701800484312336	84178
37fe3b73ffe86cb8e2fa1961aa17f95915520da6	on longest repeat queries using gpu	paper;biology;cuda;nvidia;computer science;computational biology;nvidia geforce gtx 660 ti	Repeat finding in strings has important applications in subfields such as computational biology. The challenge of finding the longest repeats covering particular string positions was recently proposed and solved by İleri et al., using a total of the optimal O(n) time and space, where n is the string size. However, their solution can only find the leftmost longest repeat for each of the n string position. It is also not known how to parallelize their solution. In this paper, we propose a new solution for longest repeat finding, which although is theoretically suboptimal in time but is conceptually simpler and works faster and uses less memory space in practice than the optimal solution. Further, our solution can find all longest repeats of every string position, while still maintaining a faster processing speed and less memory space usage. Moreover, our solution is parallelizable in the shared memory architecture (SMA), enabling it to take advantage of the modern multi-processor computing platforms such as the general-purpose graphics processing units (GPU). We have implemented both the sequential and parallel versions of our solution. Experiments with both biological and non-biological data show that our sequential and parallel solutions are faster than the optimal solution by a factor of 2–3.5 and 6–14, respectively, and use less memory space.	computational biology;computer graphics;dspace;experiment;general-purpose markup language;graphics processing unit;jian xu;lr parser;multiprocessing;shared memory;space–time tradeoff;speedup	Yun Tian;Bojian Xu	2015		10.1007/978-3-319-18120-2_19	parallel computing;computer science;theoretical computer science;distributed computing;algorithm	Metrics	-0.28786345181054374	41.92300162820826	84188
ec06c73aa267efe903825ccc75ae42ead1ec1ffc	a knowledge-based operator for a genetic algorithm which optimizes the distribution of sparse matrix data		We present the Hogs and Slackers genetic algorithm (GA) which addresses the problem of improving the parallelization efficiency of sparse matrix computations by optimally distributing blocks of matrices data. The performance of a distribution is sensitive to the non-zero patterns in the data, the algorithm, and the hardware architecture. In a candidate distributions the Hogs and Slackers GA identifies processors with many operations – hogs, and processors with fewer operations – slackers. Its intelligent operation-balancing mutation operator then swaps data blocks between hogs and slackers to explore a new data distribution.We show that the Hogs and Slackers GA performs better than a baseline GA. We demonstrate Hogs and Slackers GA’s optimization capability with an architecture study of varied network and memory bandwidth and latency.	genetic algorithm;sparse matrix	Una-May O'Reilly;Nadya T. Bliss;Sanjeev Mohindra;Julie Mullen;Eric Robinson	2012		10.1007/978-3-642-28789-3_10	cuthill–mckee algorithm;mathematical optimization;sparse matrix;machine learning;pattern recognition;sparse approximation	AI	-1.184345486367717	41.13865824108211	84205
fe19cd767ae07f78b2029f9f72c783408968611a	wi-fi fingerprinting in the real world - rtls@um at the evaal competition	databases;benchmarking;indoor navigation;competition;training;benchmarking indoor positioning wi fi fingerprinting competition;ipin 2015 conference wi fi fingerprinting rtls um evaal competition indoor positioning indoor navigation wlan infrastructures software only positioning application software only tracking application software only navigation application;wi fi fingerprinting;fingerprint recognition;ieee 802 11 standard;fingerprint recognition training ieee 802 11 standard databases floors indoor navigation;wireless lan indoor navigation;indoor positioning;floors	Research and development around indoor positioning and navigation is capturing the attention of an increasing number of research groups and labs around the world. Among the several techniques being proposed for indoor positioning, solutions based on Wi-Fi fingerprinting are the most popular since they exploit existing WLAN infrastructures to support software-only positioning, tracking and navigation applications. Despite the enormous research efforts in this domain, and despite the existence of some commercial products based on Wi-Fi fingerprinting, it is still difficult to compare the performance, in the real world, of the several existing solutions. The EvAAL competition, hosted by the IPIN 2015 conference, contributed to fill this gap. This paper describes the experience of the RTLS@UM team in participating in track 3 of that competition.	fingerprint (computing)	Adriano J. C. Moreira;Maria João Nicolau;Filipe Meneses;António Costa	2015	2015 International Conference on Indoor Positioning and Indoor Navigation (IPIN)	10.1109/IPIN.2015.7346967	simulation;telecommunications;engineering;computer security	Mobile	6.614421004315035	36.24315251823975	84253
f5de5cad5ac0c63cc8f65f671bc6e145909c5b11	self-reconfigurable smart camera networks	surveillance networks;data sharing;self reconfigurable smart camera networks;self reconfigurable networks;fov;fields of view;heterogeneous devices coordination self reconfigurable smart camera networks concurrent task allocation data sharing fields of view fov;pervasive computing;resource management;smart camera networks;concurrent task allocation;network topology;embedded systems;cameras smart cameras resource management network topology network topology target tracking calibration self organizing networks;pervasive computing self reconfigurable networks smart camera networks surveillance networks detection and tracking embedded systems hardware distributed systems;smart cameras;target tracking;distributed systems;heterogeneous devices coordination;calibration;cameras;detection and tracking;hardware;self organizing networks	Camera networks that reconfigure while performing multiple tasks have unique requirements, such as concurrent task allocation with limited resources, the sharing of data among fields of view across the network, and coordination among heterogeneous devices.	requirement;self-reconfiguring modular robot;smart camera	Juan Carlos SanMiguel;Christian Micheloni;Karen Shoop;Gian Luca Foresti;Andrea Cavallaro	2014	Computer	10.1109/MC.2014.133	smart camera;embedded system;real-time computing;calibration;field of view;computer science;resource management;distributed computing;self-organizing network;network topology	Mobile	6.096705670114756	37.49981548712439	84350
6eccd1c766432f6823b3d6d46ba90ad30bf2c90c	synthesis of activation-parallel convolution structures for neuromorphic architectures		Convolutional neural networks have demonstrated continued success in various visual recognition challenges. The convolutional layers are implemented in the activation-serial or fully parallel manner on neuromorphic computing systems. This paper presents an unrolling method that generates parallel structures for the convolutional layers depending on a required level of parallel processing. We analyze the resource requirements for the unrolling of the two-dimensional filters, and propose methods to deal with practical considerations such as stride, borders, and alignment. We apply the propose methods to practical convolutional neural networks including AlexNet and the generated structures are mapped onto a recent neuromorphic computing system. This demonstrates that the proposed methods can improve the performance or reduce the power consumption significantly even without area penalty.	artificial neural network;convolution;convolutional neural network;neuromorphic engineering;parallel computing;requirement	Seban Kim;Jaeyong Chung	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		embedded system;parallel processing;electronic engineering;parallel computing;real-time computing;computer science;theoretical computer science;operating system;convolution;algorithm	EDA	3.8338122607225853	43.10056964846942	84365
dd3ed1bdf960db72a1cb3b497d885bb1f69235d6	an optimized gpu-based 2d convolution implementation	convolution;gpu;filter	With the increasing sophistication of image processing algorithms, and due to its low computation complexity, convolution should fully benefit from the ever-increasing capacities of state-of-the-art GPUS, such as Nvidia’s Kepler and Maxwell family cards. Currently, it tends to be used as a preprocessing stage within more intricate image manipulations and has recently been implemented quite efficiently by several teams. However, either their implementations do not come near hardware’s peak performance or are unable to process large mask sizes. Such limitations are overrun by our original PCRF (Parallel Register-only Convolution Filter) implementation of 2D convolution filters that can process 32-bit floating-point images on an NVidia K40 card using mask sizes up to 127x127 and at the same time achieving pixel throughputs over 29GPs, which is, as far as we know, the highest rate known to date. Such results were obtained by using registers sparingly and by designing memory access patterns that cancel both load and store replays at warp levels, along with optimizing cache use. Copyright c © 2015 John Wiley & Sons, Ltd.	32-bit;algorithm;cloud computing;coalescing (computer science);computation;concurrency control;convolution;fast fourier transform;graphics processing unit;image processing;instruction-level parallelism;intel hd and iris graphics;john d. wiley;kepler (microarchitecture);kernel (operating system);maxwell (microarchitecture);median filter;on the fly;parallel computing;pixel;policy and charging rules function;preprocessor;processor register;usability	Gilles Perrot;Stéphane Domas;Raphaël Couturier	2016	Concurrency and Computation: Practice and Experience	10.1002/cpe.3752	parallel computing;filter;computer science;theoretical computer science;operating system;database;distributed computing;convolution;computer graphics (images)	HPC	-1.9389398593523546	44.66014280650497	84711
1e749c09bbdaad9525dbec0067321da490a346ee	a gmp-based implementation of schönhage-strassen's large integer multiplication algorithm	multiprecision arithmetic;integer multiplication	Schönhage-Strassen's algorithm is one of the best known algorithms for multiplying large integers. Implementing it ef?ciently is of utmost importance, since many other algorithms rely on it as a subroutine. We present here an improved implementation, based on the one distributed within the GMP library. The following ideas and techniques were used or tried: faster arithmetic modulo 2n + 1, improved cache locality, Mersenne transforms, Chinese Remainder Reconstruction, the √2 trick, Harley's and Granlund's tricks, improved tuning.	gnu multiple precision arithmetic library;harley's humongous adventure;locality of reference;modulo operation;multiplication algorithm;schönhage–strassen algorithm;strassen algorithm;subroutine	Pierrick Gaudry;Alexander Kruppa;Paul Zimmermann	2007		10.1145/1277548.1277572	arithmetic;theoretical computer science;mathematics;algorithm;algebra	Theory	8.612720249360677	42.18272268722785	85116
a857c4a27988afc6328d3021b16f4aafe9e78871	parallel resolution of the 3d helmholtz equation based on multi-graphics processing unit clusters	multi gpu;mpi paradigm;helmholtz equation;biconjugate gradient method	The resolution of the 3D Helmholtz equation is required in the development of models related to a wide range of scientific and technological applications. For solving this equation in complex arithmetic, the biconjugate gradient (BCG) method is one of the most relevant solvers. However, this iterative method has a high computational cost because of the large sparse matrix and the vector operations involved. In this paper, a specific BCG method, adapted for the regularities of the Helmholtz equation is presented. This BCG is based on the implementation of a novel format (named ‘Regular Format’) that allows the storage of the large sparse matrix involved in the sparse matrix vector product in a compact form. The contribution of this work is twofold: (1) decreasing the memory requirements of the 3D Helmholtz equation using the ‘Regular Format’ and (2) speeding up the resolution of the equation using high performance computing resources. A hybrid Message Passing Interface (MPI)-graphics processing unit CUDA GPU parallelization that is capable of solving complex problems in short time has carried out (Fast-Helmholtz). Fast-Helmholtz combines optimizations at Message Passing Interface and GPU levels to reduce communications costs and to improve the exploitation of GPU architecture. This strategy makes it possible to extend the dimension of the Helmholtz problem to be solved, thanks to the relevant reduction of memory requirements and runtime. Copyright © 2014 John Wiley & Sons, Ltd.	cuda;computation;computational complexity theory;computer graphics;conjugate gradient method;convex conjugate;graphics processing unit;iterative method;john d. wiley;message passing interface;parallel computing;regular expression;requirement;sparse matrix;supercomputer	Gloria Ortega López;Julia Lobera;Inmaculada García;María del Pilar Arroyo;Ester M. Garzón	2015	Concurrency and Computation: Practice and Experience	10.1002/cpe.3212	computational science;parallel computing;computer science;theoretical computer science;distributed computing;helmholtz equation;biconjugate gradient method	HPC	-3.2570142023379187	39.086470494348994	85144
f3d6cd48bc10bcfd464b46e46aa19a5d24154428	a vlsi design for the parallel finite state automaton and its performance evaluation as a hardware scanner	parallelisme;evaluation performance;performance evaluation;design and development;circuit vlsi;automate etat fini;vlsi design;parallelism;vlsi circuit;system design;pattern matching;text retrieval;finite state automaton;finite automaton;conception systeme	Recent developments in VLSI technology have made it possible to design and implement smaller, cheaper, faster, and more reliable computer components. In addition, these advances have been a major drive for the design and development of the so-called “suitable algorithm” for VLSI implementation. The finite state automaton, its concept, computational power, and applications have been under investigation since the early 60's. Most of the studies have addressed the deterministic model of the finite state automaton with no degree of parallelism in the operations. This paper introduces a hardware architecture and VLSI design of the parallel finite state model. In addition, it addresses the time and space complexities of the proposed organization. Moreover, the paper represents the performance evaluation of the proposed architecture as a special purpose hardware recognizer device capable of performing pattern matching and text retrieval operations. The proposed model simulates a parallel finite state automaton by utilization of a number of identical and independent units called “CELLs,” which have associative capability. Each cell represents a state and transitions out of that state.	algorithm;automaton;computation;computer hardware;degree of parallelism;document retrieval;finite-state machine;parallel computing;pattern matching;performance evaluation;very-large-scale integration	Ali R. Hurson	1984	International Journal of Computer & Information Sciences	10.1007/BF00985824	parallel computing;computer science;theoretical computer science;pattern matching;very-large-scale integration;finite-state machine;programming language;algorithm;systems design	Arch	5.530495484866051	45.47777331919412	85327
19cadcb4e7439bc525c604771ab4872ec93a5b53	leopard: lightweight edge-oriented partitioning and replication for dynamic graphs		This paper introduces a dynamic graph partitioning algorithm, designed for large, constantly changing graphs. We propose a partitioning framework that adjusts on the fly as the graph structure changes. We also introduce a replication algorithm that is tightly integrated with the partitioning algorithm, which further reduces the number of edges cut by the partitioning algorithm. Even though the proposed approach is handicapped by only taking into consideration local parts of the graph when reassigning vertices, extensive evaluation shows that the proposed approach maintains a quality partitioning over time, which is comparable at any point in time to performing a full partitioning from scratch using a state-the-art static graph partitioning algorithm such as METIS. Furthermore, when vertex replication is turned on, edge-cut can improve by an order of magnitude.	algorithm;graph partition;metis;on the fly	Jiewen Huang;Daniel J. Abadi	2016	PVLDB	10.14778/2904483.2904486	combinatorics;computer science;graph partition;space partitioning;theoretical computer science;distributed computing;set partitioning in hierarchical trees	DB	2.255746233378617	36.35735958936634	85808
d6d1541b77cd3178b4801012ee6fd470027b3736	resampling algorithms and architectures for distributed particle filters	estensibilidad;multiprocessor interconnection networks;processing element;distributed algorithms;filtre particule;algoritmo paralelo;field programmable gate array;filtering;evaluation performance;unite centrale;arquitectura red;sampling importance resampling;parallel algorithm;performance evaluation;frecuencia muestreo;signal sampling;central unit;implementation;evaluacion prestacion;sampling frequency;filtro particulas;distributed resampling;red puerta programable;architecture reseau;reseau porte programmable;approche deterministe;interconnection network;algorithme parallele;nonproportional allocation;deterministic approach;fpga implementation;mouvement particule;frequence echantillonnage;rna;parallel architectures;particle motion;architecture parallele;particle filter;parallel architectures signal sampling distributed algorithms field programmable gate arrays importance sampling;algorithms for parallel implementation;movimiento particula;algorithme reparti;echantillonnage importance;poursuite cible;enfoque determinista;algoritmo repartido;unidad central;distributed resampling algorithm;methode reechantillonnage;network architecture;parallel implementation;extensibilite;scalability;parallel architecture;network structure;resampling method;distributed particle filter;field programmable gate arrays;particle filters;target tracking;importance sampling;sampling methods;implementacion;particle filters field programmable gate arrays sampling methods parallel architectures multiprocessor interconnection networks parallel processing filtering scalability rna frequency;frequency;distributed algorithm;distributed resampling distributed particle filter distributed resampling algorithm proportional allocation nonproportional allocation processing element interconnection network field programmable gate array sampling importance resampling parallel implementation;proportional allocation;parallel processing;red interconexion	In this paper, we propose novel resampling algorithms with architectures for efficient distributed implementation of particle filters. The proposed algorithms improve the scalability of the filter architectures affected by the resampling process. Problems in the particle filter implementation due to resampling are described, and appropriate modifications of the resampling algorithms are proposed so that distributed implementations are developed and studied. Distributed resampling algorithms with proportional allocation (RPA) and nonproportional allocation (RNA) of particles are considered. The components of the filter architectures are the processing elements (PEs), a central unit (CU), and an interconnection network. One of the main advantages of the new resampling algorithms is that communication through the interconnection network is reduced and made deterministic, which results in simpler network structure and increased sampling frequency. Particle filter performances are estimated for the bearings-only tracking applications. In the architectural part of the analysis, the area and speed of the particle filter implementation are estimated for a different number of particles and a different level of parallelism with field programmable gate array (FPGA) implementation. In this paper, only sampling importance resampling (SIR) particle filters are considered, but the analysis can be extended to any particle filters with resampling.	algorithm;cuda;central processing unit;centralized computing;complexity;control unit;field-programmable gate array;interconnection;microprocessor;parallel computing;particle filter;performance;point-to-point protocol;powerset construction;real-time transcription;resampling (statistics);routing;run time (program lifecycle phase);sampling (signal processing);scalability;scheduling (computing);telephone exchange;tip (unix utility)	Miodrag Bolic;Petar M. Djuric;Sangjin Hong	2005	IEEE Transactions on Signal Processing	10.1109/TSP.2005.849185	distributed algorithm;parallel computing;particle filter;auxiliary particle filter;computer science;theoretical computer science;field-programmable gate array;statistics	Visualization	-2.9231262294399034	33.49640441635837	86253
b01e479df3c6216fbd4c37d37d647217076569eb	design of a distributed compressor for astronomy ssd	run length;field programmable gate arrays astronomy computing data compression;astronomy bandwidth radiation detectors image coding table lookup memory data compression;fpga;astronomy ssd;run length compression fpga astronomy ssd huffman;huffman;length limited huffman algorithm distributed compressor solid state device astronomy data storage data compression fpga based astronomy ssd field programmable gate array data driven compressor compression algorithm run length algorithm;compression	SSD (solid state device) has shown a great potential in astronomy data storage. Data compression is an essential task to obtain higher storage density and bandwidth. This paper proposes a distributed compressor customized for FPGA-based astronomy SSD. Our data-driven compressor cope with astronomy data in the unit of byte, two compression algorithms, run length and length-limited huffman are utilized, a distributed length-limited huffman encoder for SSD is further developed to reduce the latency. Experimental results indicate that our proposed compressor achieves a 1GB/s bandwidth with less than 2500 LUTs utilized while the compression ratio is only 10% lower than Gzip level9.	algorithm;areal density (computer storage);byte;computer data storage;data compression;encoder;field-programmable gate array;huffman coding;netbsd gzip / freebsd gzip;run-length encoding;solid-state drive;solid-state electronics	Bo Peng;Xi Jin;Tianqi Wang;Xueliang Du	2015	2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2015.29	embedded system;parallel computing;computer hardware;computer science;theoretical computer science;compression;field-programmable gate array;huffman coding	Arch	2.3996180187892078	41.579303195963604	86310
786700d2e0e4aed858501d87907136b6cc8ccc33	a synthesis method of lsgp partitioning for given-shape regular arrays	school of no longer in use;electronics and computer science;theoretical framework;locally sequential globally parallel;timing vector;integral equations;systolic arrays;systolic array;timing integral equations design methodology shape systolic arrays difference equations;computational polytope;activity matrix;parallel algorithms systolic arrays data structures;difference equations;shape;data structures;locally sequential globally parallel lsgp partitioning given shape regular arrays computational polytope processor arrays activity matrix timing vector;given shape regular arrays;lsgp partitioning;processor arrays;design methodology;parallel algorithms;timing	This paper presents a method to partition and map a computational polytope onto processor arrays. Based on the theoretical framework of an existing LSGP method, a systematic design procedure is proposed which constructs an activity matrix, proposed by Darte, according to the shapes of the computational polytope and the processor array and derives a valid timing vector. By this method the given-shape mapping can be achieved with neither difficulty nor exhausted computations by removing the need to compute HNFs. >	binary space partitioning	Graham M. Megson;Xian Chen	1995		10.1109/IPPS.1995.395938	mathematical optimization;combinatorics;theoretical computer science;mathematics	EDA	4.404547724266239	38.322043684078494	86355
9f7bef91b1cc3578ec59671dd833b778e8175761	time-domain bem for the wave equation on distributed-heterogeneous architectures: a blocking approach	hybrid parallelization;multi gpu;gpu;cuda;bem;multi gpus;time domain;parallel algorithms	The problem of time-domain BEM for the wave equation in acoustics and electromagnetism can be expressed as a sparse linear system composed of multiple interaction/convolution matrices. It can be solved by using sparse matrix-vector products which are inefficient to achieve high Flop-rate neither on CPUs nor GPUs. In this paper we extend the approach proposed in a previous work [1] in which we re-order the computation to get a special matrix structure with one dense vector per row. This new structure is called a slice matrix and is computed with a custom matrix/vector product operator. In this study, we present an optimized implementation of this operator on Nvidia GPUs based on two blocking strategies. We explain how we can obtain multiple block-values from a slice and how these can be computed efficiently on GPUs since we target heterogeneous nodes composed of CPUs and GPUs. In order to deal with different efficiencies of the processing units we use a greedy heuristic that dynamically balances work among the workers. We demonstrate the performance of our system by studying the quality of the balancing heuristic and the sequential Flop-rate of the blocked implementations. Finally, we validate our implementation with an industrial test case on 8 heterogeneous nodes, each composed of 12 CPUs and 3 GPUs. © 2015 Elsevier B.V. All rights reserved.	blocking (computing);boundary element method;central processing unit;computation;convolution;flip-flop (electronics);graphics processing unit;greedy algorithm;heuristic;linear system;sparse matrix;test case	Bérenger Bramas;Olivier Coulaud;Guillaume Sylvand	2015	Parallel Computing	10.1016/j.parco.2015.07.005	mathematical optimization;parallel computing;time domain;computer science;theoretical computer science;operating system;parallel algorithm	HPC	-3.555595870863631	39.26906583267458	86379
988522d55a514a90705d809fe729887243ed95b9	comparison of the parallel fast marching method, the fast iterative method, and the parallel semi-ordered fast iterative method		Solving the eikonal equation allows to compute a monotone front propagation of anisotropic nature and is thus a widely applied technique in different areas of science and engineering. Various methods are available out of which only a subset is suitable for shared-memory parallelization, which is the key focus of this analysis. We evaluate three different approaches, those being the recently developed parallel fast marching method based on domain decompositioning, the inherently parallel fast iterative method, and a parallel approach of the semi-ordered fast iterative method, which offers increased stability for variations in the front velocity as compared to established iterative methods. We introduce the individual algorithms, evaluate the accuracy, and show benchmark results based on a dual socket Intel Ivy Bridge-EP cluster node using C++/OpenMP implementations. Our investigations show that the parallel fast marching method performs best in terms of accuracy and single thread performance and reasonably well with respect to parallel efficiency for up to 8-16 threads.	algorithm;benchmark (computing);computer performance;domain decomposition methods;expectation propagation;fast marching method;fast multipole method;iterative method;ivy bridge (microarchitecture);parallel computing;semiconductor industry;shared memory;simulation;software propagation;solver;speedup;synthetic intelligence;velocity (software development);monotone	Josef Weinbub;Andreas Hössinger	2016		10.1016/j.procs.2016.05.408	mathematical optimization;parallel computing;computer science;theoretical computer science	HPC	-4.489904418363512	38.738697112543676	86625
04edd3f14a13f4836d4c8c9a776adf385a8902a4	evaluation of k-means data clustering algorithm on intel xeon phi	k means;accelerator;open area test sites;data clustering;computer architecture;xeon phi;graphics processing units;parallelization;clustering algorithms;parallel processing;instruction sets	Intel Xeon Phi is a processor based on MIC architecture that contains a large number of compute cores with a high local memory bandwidth and 512-bit vector processing units. To achieve high performance on Xeon Phi, it is important for programmers to explore all the software features provided by the Intel compiler and libraries to fully utilize the new hardware resources. In this paper, we use the K-Means algorithm to study the performance of various Intel software settings available for Xeon Phi and their impacts to the performance of K-means. At first we examine different memory layouts for storing data points using Intel compiler-intrinsic functions. During distance calculation, the computational kernel of K-means, when the size of individual input data points is not vector-friendly, we pad the data points to align with the VPU width. At last, we implement a parallel reduction to increase memory access parallelism and cache hits. These techniques enable us to successfully take advantage of thread-level parallelism and data-level parallelism on Xeon Phi. Experimental results demonstrate large performance gains over the default auto-vectorization approach. The K-Means implemented with the proposed techniques achieves up to 68.65% and 56.14% performance improvements for aligned datasets and unaligned datasets, respectively. For high-dimensional aligned datasets, we achieved up to 53.49% performance improvement on a large-scale parallel computer.	algorithm;align (company);automatic vectorization;byte;central processing unit;cluster analysis;compiler;coprocessor;data parallelism;data point;dimension 3;graphics processing unit;intel developer zone;intrinsic function;k-means clustering;library (computing);machine learning;memory bandwidth;parallel computing;programmer;supercomputer;task parallelism;vector processor;xeon phi	Sunwoo Lee;Wei-keng Liao;Ankit Agrawal;Nikolaos Hardavellas;Alok N. Choudhary	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840856	computer architecture;parallel computing;computer science;operating system;hyper-threading	HPC	-4.457298683430709	44.36944875367163	86711
073ef969c42c3a079f6d30b0ab043331026cb2fd	level-3 cholesky factorization routines improve performance of many cholesky algorithms	cholesky factorization and solution;mathematics;lapack;positive definite matrices;inplace transposition;software engineering;complex hermitian matrices;novel blocked packed matrix data structures;data structures;datavetenskap datalogi;computer software;computer science;cache blocking;real symmetric matrices;matematik;blas	Four routines called DPOTF3i, i = a,b,c,d, are presented. DPOTF3i are a novel type of level-3 BLAS for use by BPF (Blocked Packed Format) Cholesky factorization and LAPACK routine DPOTRF. Performance of routines DPOTF3i are still increasing when the performance of Level-2 routine DPOTF2 of LAPACK starts decreasing. This is our main result and it implies, due to the use of larger block size nb, that DGEMM, DSYRK, and DTRSM performance also increases! The four DPOTF3i routines use simple register blocking. Different platforms have different numbers of registers. Thus, our four routines have different register blocking sizes.  BPF is introduced. LAPACK routines for POTRF and PPTRF using BPF instead of full and packed format are shown to be trivial modifications of LAPACK POTRF source codes. We call these codes BPTRF. There are two variants of BPF: lower and upper. Upper BPF is “identical” to Square Block Packed Format (SBPF). “LAPACK” implementations on multicore processors use SBPF. Lower BPF is less efficient than upper BPF. Vector inplace transposition converts lower BPF to upper BPF very efficiently. Corroborating performance results for DPOTF3i versus DPOTF2 on a variety of common platforms are given for n ≈ nb as well as results for large n comparing DBPTRF versus DPOTRF.	blas;block size (cryptography);blocking (computing);central processing unit;cholesky decomposition;code;in-place algorithm;lapack;multi-core processor;processor register	Fred G. Gustavson;Jerzy Wasniewski;Jack J. Dongarra;José R. Herrero;Julien Langou	2013	ACM Trans. Math. Softw.	10.1145/2427023.2427026	parallel computing;data structure;computer science;loop nest optimization;theoretical computer science;basic linear algebra subprograms;programming language;algorithm	HPC	0.6800445076727232	40.05605289969195	86897
fd0b8fc16892faf8a6392d9bcc1c7c0001fa376c	a communication reduction approach to iteratively solve large sparse linear systems on a gpgpu cluster	iterative solver;communication reduction;sparse linear systems;gpgpu cluster	Finite Element Methods (FEM) are widely used in academia and industry, especially in the fields of mechanical engineering, civil engineering, aerospace, and electrical engineering. These methods usually convert partial difference equations into large sparse linear systems. For complex problems, solving these large sparse linear systems is a time consuming process. This paper presents a parallelized iterative solver for large sparse linear systems implemented on a GPGPU cluster. Traditionally, these problems do not scale well on GPGPU clusters. This paper presents an approach to reduce the communications between cluster compute nodes for these solvers. Additionally, computation and communication are overlapped to reduce the impact of data exchange. The parallelized system achieved a speedup of up to 15.3 times on 16 NVIDIA Tesla GPUs, compared to a single GPU. An analytical evaluation of the algorithm is conducted in this paper, and the analytical equations for predicting the performance are presented and validated.	algorithm;computation;conjugate gradient method;correctness (computer science);electrical engineering;finite element method;general-purpose computing on graphics processing units;graph partition;graphics processing unit;iterative method;linear system;nvidia tesla;parallel computing;recurrence relation;reliability-centered maintenance;scalability;solver;sparse matrix;speedup	Chong Chen;Tarek M. Taha	2013	Cluster Computing	10.1007/s10586-013-0279-2	computational science;parallel computing;computer science;theoretical computer science;sparse approximation	HPC	-3.242498587009736	38.8659873787472	86925
e8c8cf2528d274182e040c48f05c1d0b5d58dd5d	a hardware unit for fast sah-optimised bvh construction	bounding volume hierarchy;ray tracing hardware;sah;ray tracing;bvh	Ray-tracing algorithms are known for producing highly realistic images, but at a significant computational cost. For this reason, a large body of research exists on various techniques for accelerating these costly algorithms. One approach to achieving superior performance which has received comparatively little attention is the design of specialised ray-tracing hardware. The research that does exist on this topic has consistently demonstrated that significant performance and efficiency gains can be achieved with dedicated microarchitectures. However, previous work on hardware ray-tracing has focused almost entirely on the traversal and intersection aspects of the pipeline. As a result, the critical aspect of the management and construction of acceleration data-structures remains largely absent from the hardware literature.  We propose that a specialised microarchitecture for this purpose could achieve considerable performance and efficiency improvements over programmable platforms. To this end, we have developed the first dedicated microarchitecture for the construction of binned SAH BVHs. Cycle-accurate simulations show that our design achieves significant improvements in raw performance and in the bandwidth required for construction, as well as large efficiency gains in terms of performance per clock and die area compared to manycore implementations. We conclude that such a design would be useful in the context of a heterogeneous graphics processor, and may help future graphics processor designs to reduce predicted technology-imposed utilisation limits.	algorithm;algorithmic efficiency;bounding volume hierarchy;computation;graphics processing unit;manycore processor;microarchitecture;multi-core processor;ray tracing (graphics);ray-tracing hardware;simulation	Michael J. Doyle;Colin Fowler;Michael Manzke	2013	ACM Trans. Graph.	10.1145/2461912.2462025	ray tracing;mathematical optimization;parallel computing;real-time computing;simulation;computer science;geometry;bounding volume hierarchy;algorithm;ray tracing hardware;computer graphics (images)	Graphics	-0.2585880030923788	45.03499544934846	87194
99691fe8c73b1409f921fd86540b6836d1edc490	on the security of block scrambling-based image encryption including jpeg distorsion against jigsaw puzzle solver attacks		Encryption-then-Compression (EtC) systems have been considered for the user-controllable privacy protection of social media like Twitter and Facebook. The aim of this paper is to evaluate the security of block scrambling-based encryption schemes, which have been proposed to construct EtC systems. Even though this scheme has enough key spaces for protecting brute-force attacks, each block in encrypted images has almost the same correlation as that of original images. Therefore, it is required to consider the security from different viewpoints from number theory-based encryption methods with provable security such as RSA and DES. In this paper, we evaluate the security of encrypted images including JPEG distorsion by using automatic jigsaw puzzle solvers.	brute-force attack;distortion;encryption;jpeg;key space (cryptography);performance;privacy;provable security;simulation;social media;solver	Tatsuya Chuman;Hitoshi Kiya	2017	2017 Eighth International Workshop on Signal Design and Its Applications in Communications (IWSDA)	10.1109/IWSDA.2017.8095737	56-bit encryption;40-bit encryption;on-the-fly encryption;theoretical computer science;encryption;provable security;filesystem-level encryption;jpeg;link encryption;mathematics	Security	8.365768934600098	35.44060733764159	87283
be0bb623838f707a644681b17c311288d7c4213e	optimal scheduling of unit-time tasks on two uniform processors under tree-like precedence constraints	optimal scheduling;precedence constraint	An O(n3/(b + 1)) time algorithm to obtain a minimum finish time schedule subject to tree-like precedence constraints for unit-time tasks on two uniform processors is obtained. It is assumed that the slower processor takes b time units for each one taken by the speedier one, for some integer b. It is also noted that a slight modification of this schedule yields a minimum mean flow time schedule. Zusammenfassung: Es wird ein Algorithmus der Ordnung O(n3/(b + 1)) zur Ermittlung der friihesten Fertigstellungszeit f'fir Sehedulingprobleme mit 2 Prozessoren und baumartigen Prgzendenzbeziehungen angegeben. Dabei wird angenommen, dag die Durchfiihrung jeder einzlnen Aufgabe gleich lange dauert, der langsamere Prozessor jedoch b Zeiteinheiten (b ganzzahlig) zur Durchfiihrung einer Aufgabe bentitigt. Ferner wird gezeigt, dag eine leichte Modifikation der L6sung eine Reihenfolge mit minimaler mittlerer Durchlaufzeit ergibt.	algorithm;central processing unit;directed acyclic graph;eine and zwei;scheduling (computing)	Wieslaw Kubiak	1989	ZOR - Meth. & Mod. of OR	10.1007/BF01415940	parallel computing;real-time computing;computer science;mathematics;distributed computing	Theory	9.16830736129483	33.12476147356076	87389
05904e107e5b9f732682689ee535058418f252c9	spinning fast iterative data flows	novel dataflow framework;iterative data flow;bulk iterative algorithm;specialized system;incremental iteration;parallel dataflow system;iterative algorithm;unified dataflow abstraction;dataflow system;iterative nature;improved dataflow system	Parallel dataflow systems are a central part of most analytic pipelines for big data. The iterative nature of many analysis and machine learning algorithms, however, is still a challenge for current systems. While certain types of bulk iterative algorithms are supported by novel dataflow frameworks, these systems cannot exploit computational dependencies present in many algorithms, such as graph algorithms. As a result, these algorithms are inefficiently executed and have led to specialized systems based on other paradigms, such as message passing or shared memory. We propose a method to integrate incremental iterations, a form of workset iterations, with parallel dataflows. After showing how to integrate bulk iterations into a dataflow system and its optimizer, we present an extension to the programming model for incremental iterations. The extension alleviates for the lack of mutable state in dataflows and allows for exploiting the sparse computational dependencies inherent in many iterative algorithms. The evaluation of a prototypical implementation shows that those aspects lead to up to two orders of magnitude speedup in algorithm runtime, when exploited. In our experiments, the improved dataflow system is highly competitive with specialized systems while maintaining a transparent and unified dataflow abstraction.	algorithm;approximation algorithm;big data;compiler;dataflow;dynamic problem (algorithms);experiment;fault tolerance;fixed point (mathematics);graph theory;immutable object;iteration;machine learning;mathematical optimization;message passing;microsoft outlook for mac;pipeline (computing);programming model;shared memory;sparse matrix;speedup	Stephan Ewen;Kostas Tzoumas;Moritz Kaufmann;Volker Markl	2012	PVLDB	10.14778/2350229.2350245	parallel computing;computer science;theoretical computer science;database;distributed computing;programming language	DB	-4.16369735507873	42.50390667189989	87417
8544ed521b9a23f97536af591bbc6ba9acb4ff68	a fast algorithm for constructing inverted files on heterogeneous platforms	indexer;pipelined and parallel parsing and indexing;gpu;inverted files;multicore	Given a collection of documents residing on a disk, we develop a new strategy for processing these documents and building the inverted files extremely quickly. Our approach is tailored for a heterogeneous platform consisting of multicore CPUs and highly multithreaded GPUs. Our algorithm is based on a number of novel techniques, including a high-throughput pipelined strategy, a hybrid trie and B-tree dictionary data structure, dynamic work allocation to CPU and GPU threads, and optimized CUDA indexer implementation. We have performed extensive tests of our algorithm on a single node (two Intel Xeon X5560 Quad-core CPUs) with two NVIDIA Tesla C1060 GPUs attached to it, and were able to achieve a throughput of more than 262 MB/s on the ClueWeb09 dataset. Similar results were obtained for widely different datasets. The throughput of our algorithm is superior to the best known algorithms reported in the literature even when compared to those run on large clusters.	algorithm;inverted index	Zheng Wei;Joseph JáJá	2012	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2012.02.005	multi-core processor;computer architecture;indexer;parallel computing;computer science;theoretical computer science;operating system;programming language	HPC	-1.1727226484324127	42.46329348945486	87709
748065e884739ad50c0b8982422f963336c13d8d	hierarchical dependency graphs: abstraction and methodology for mapping systolic array designs to multicore processors	high performance computing;dependence graph;systolic array;parallel programming;systolic array designs;dependency graphs;multicore;compiler optimization;high performance computer;scientific computing;transitive closure;multicore processors;parallel programs	Systolic array designs and dependency graphs are some of the most important class of algorithms in several scientific computing areas. In this paper, we first propose an abstraction based on the fundamental principles behind designing systolic arrays. Then, based on the abstraction, we propose a methodology to map a dependency graph to a generic multicore processor. Then we present two case studies: Convolution and Transitive Closure, on two state of the art multicore architectures: Intel Xeon and Cell multicore processors, illustrating the ideas in the paper. We achieved scalable results and higher performance compared to standard compiler optimizations and other recent implementations in the case studies. We comment on the performance of the algorithms by taking into consideration the architectural features of the two multicore platforms.	algorithm;cell (microprocessor);central processing unit;computation;computational science;convolution;coppersmith–winograd algorithm;discrete-time signal;multi-core processor;optimizing compiler;recurrence relation;scalability;signal processing;systolic array;transitive closure	Sudhir Vinjamuri;Viktor K. Prasanna	2009		10.1007/978-3-642-03275-2_28	multi-core processor;computer architecture;supercomputer;parallel computing;computer science;theoretical computer science	HPC	-4.419122809581765	44.02968320551987	87830
87c8a3b89fd8773d5969f3f1359b5c553f8d2ae0	deep learning on fpgas: past, present, and future	paper;overview;fpga;machine learning;deep learning;artificial intelligence;computer science;opencl	The rapid growth of data size and accessibility in recent years has instigated a shift of philosophy in algorithm design for artificial intelligence. Instead of engineering algorithms by hand, the ability to learn composable systems automatically from massive amounts of data has led to groundbreaking performance in important domains such as computer vision, speech recognition, and natural language processing. The most popular class of techniques used in these domains is called deep learning , and is seeing significant attention from industry. However, these models require incredible amounts of data and compute power to train, and are limited by the need for better hardware acceleration to accommodate scaling beyond current data and model sizes. While the current solution has been to use clusters of graphics processing units (GPU) as general purpose processors (GPGPU), the use of field programmable gate arrays (FPGA) provide an interesting alternative. Current trends in design tools for FPGAs have made them more compatible with the high-level software practices typically practiced in the deep learning community, making FPGAs more accessible to those who build and deploy models. Since FPGA architectures are flexible, this could also allow researchers the ability to explore model-level optimizations beyond what is possible on fixed architectures such as GPUs. As well, FPGAs tend to provide high performance per watt of power consumption, which is of particular importance for application scientists interested in large scale server-based deployment or resource-limited embedded applications. This review takes a look at deep learning and FPGAs from a hardware acceleration perspective, identifying trends and innovations that make these technologies a natural fit, and motivates a discussion on how FPGAs may best serve the needs of the deep learning community moving forward.	accessibility;algorithm design;artificial intelligence;central processing unit;computer graphics;computer vision;deep learning;embedded system;field-programmable gate array;general-purpose computing on graphics processing units;graphics processing unit;hardware acceleration;high- and low-level;image scaling;natural language processing;performance per watt;server (computing);software deployment;speech recognition	Griffin Lacey;Graham W. Taylor;Shawki Areibi	2016	CoRR		parallel computing;real-time computing;simulation;computer science;theoretical computer science;operating system;distributed computing;deep learning;programming language;algorithm;field-programmable gate array	Arch	-0.11927897640042182	44.900532389881185	87834
158e6784644a5b4ba6d06bf007aef57a0eab6ae1	optimization-based computation with spiking neurons		Considerable effort is currently being spent designing neuromorphic hardware for addressing challenging problems in a variety of pattern-matching applications. These neuromorphic systems offer low power architectures with intrinsically parallel and simple spiking neuron processing elements. Unfortunately, these new hardware architectures have been largely developed without a clear justification for using spiking neurons to compute quantities for problems of interest. Specifically, the use of spiking for encoding information in time has not been explored theoretically with complexity analysis to examine the operating conditions under which neuromorphic computing provides a computational advantage (time, space, power, etc.) In this paper, we present and formally analyze the use of temporal coding in a neural-inspired algorithm for optimization-based computation in neural spiking architectures.	algorithm;analysis of algorithms;artificial neural network;computation;logic gate;logical connective;low insertion force;mathematical optimization;neural coding;neuromorphic engineering;neuron;optimization problem;parallel algorithm;parallel computing;pattern matching;spiking neural network;synaptic weight;turing completeness;utm theorem	Stephen J. Verzi;Craig M. Vineyard;Eric D. Vugrin;Meghan Galiardi;Conrad D. James;James B. Aimone	2017	2017 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2017.7966098	artificial intelligence;machine learning;theoretical computer science;parallel algorithm;algorithm design;computer science;computation;linear programming;neuromorphic engineering	HPC	4.736438762443628	40.949174544612156	87853
3e3038155313141a972687e16a043b305010b3a1	ieeecc754++ - an advanced set of tools to check ieee 754-2008 conformity		The „IEEE Standard for Binary Floating-Point Arithmetic“ IEEE 754 has arguably been one of the most influential standards for the broad area of scientic computing. Floating point arithmetic conforming to the standard implies that numerical algorithms behave identically across the various computing platforms (or at least in a predictable manner) and that developers of numerical algorithms can rely on the standard when addressing stability issues that might otherwise depend on the floating-point environment.rnrnWith this thesis, we provide a contribution to evaluating the conformity of a given floating-point environment to IEEE 754-2008, the current version of the standard. In particular, we extended the well known testing tool IeeeCC754 with a large number of features, such as support for IEEE 754-2008, new analysis facilities, additional floating-point operators, and new test vectors to verify that these operators are implemented in a conforming manner. Furthermore, we heavily expanded the selection of supported floating-point environments and provide facilities to easily extend our new tool IeeeCC754++ with new ports targeted at future floating-point platforms. Additionally, we developed a testing framework that enables large-scale evaluation of floating-point environments and a variation of the testing framework tailored to studying the influence of compiler options on the behaviour of numerical applications regarding floating-point accuracy as well as application performance.	conformity	Matthias Hüsken	2018			conformity;operator (computer programming);compiler;computer engineering;floating point;ieee floating point;binary number;computer science	Visualization	-0.37151838819182254	46.23072495537782	88105
0acd9738dc6df6f48b99897b6162c6958a437ce8	performance evaluation of the sparse matrix-vector multiplication on modern architectures	calcul scientifique;calcul matriciel;scientific application;evaluation performance;optimisation;microarchitecture;multicore architectures;performance evaluation;optimizacion;producto matriz;evaluacion prestacion;processeur multicoeur;procesador multinucleo;supercomputer;preparacion serie fabricacion;supercomputador;computacion cientifica;matrice creuse;microarquitectura;multithread;scientific applications;optimization;multicore processor;multitâche;matrix calculus;process planning;sparse matrix;vector processor;scientific computation;produit matrice;preparation gamme fabrication;multitarea;calculo de matrices;matrix product;superordinateur;sparse matrix vector multiplication;matriz dispersa;processeur vectoriel	In this paper, we revisit the performance issues of the widely used sparse matrix-vector multiplication (SpMxV) kernel on modern microarchitectures. Previous scientific work reports a number of different factors that may significantly reduce performance. However, the interaction of these factors with the underlying architectural characteristics is not clearly understood, a fact that may lead to misguided, and thus unsuccessful attempts for optimization. In order to gain an insight into the details of SpMxV performance, we conduct a suite of experiments on a rich set of matrices for three different commodity hardware platforms. In addition, we investigate the parallel version of the kernel and report on the corresponding performance results and their relation to each architecture’s specific multithreaded configuration. Based on our experiments, we extract useful conclusions that can serve as guidelines for the optimization process of both single and multithreaded versions of the kernel.	commodity computing;computation;computer data storage;datapath;experiment;flops;kernel (operating system);mathematical optimization;matrix multiplication;memory controller;microprocessor;multithreading (computer architecture);performance evaluation;quantifier (logic);scientific literature;sparse matrix;thread (computing);von neumann architecture	Georgios I. Goumas;Kornilios Kourtis;Nikos Anastopoulos;Vasileios Karakasis;Nectarios Koziris	2008	The Journal of Supercomputing	10.1007/s11227-008-0251-8	multi-core processor;supercomputer;vector processor;parallel computing;sparse matrix;matrix calculus;microarchitecture;matrix multiplication;computer science;theoretical computer science;operating system;algorithm	HPC	-4.278331142907146	40.96083299600221	88170
5f6c1bcb193d625b36f7101fe6dd10f4a5c875c9	design and implementation of a cost-optimal parallel tridiagonal system solver using skeletons	algoritmo paralelo;programa paralelo;proceso concepcion;secuencial;metodo paso a paso;step by step method;design process;parallel algorithm;sequential;esqueleto;communicating process;machine parallele;distributed computing;skeleton;parallel computation;tridiagonal matrix;algorithme parallele;preparacion serie fabricacion;proceso comunicante;sequentiel;calculo paralelo;message passing interface;efficient implementation;cost optimization;design and implementation;envoi message;processus communicant;matriz tridiagonal;methode pas a pas;coste;message passing;squelette;calculo repartido;parallel machines;mpi;process planning;parallel programs;preparation gamme fabrication;calcul parallele;parallel program;calcul reparti;matrice tridiagonale;processus conception;cout;programme parallele	We address the problem of systematically designing correct parallel programs and developing their efficient implementations on parallel machines. The design process starts with an intuitive, sequential algorithm and proceeds by expressing it in terms of well-defined, pre-implemented parallel components called skeletons. We demonstrate the skeleton-based design process using the tridiagonal system solver as our example application. We develop step by step three provably correct, parallel versions of our application, and finally arrive at a cost-optimal implementation in MPI (Message Passing Interface). The performance of our solutions is demonstrated experimentally on a Cray T3E machine.	cost efficiency;solver	Holger Bischof;Sergei Gorlatch;Emanuel Kitzelmann	2003		10.1007/978-3-540-45145-7_39	parallel computing;computer science;message passing interface;distributed computing;programming language;algorithm	HPC	-2.470401879598333	36.178320572813654	88238
ad689e653feec1b15018e62c4bd9c39df7b19b2a	the quantum structure of matter grand challenge project: large-scale 3-d solutions in relativistic quantum dynamics	dirac equation;matrix algebra;parallel algorithms;physics computing;splines (mathematics);intel ipsc/860 hypercube;matter grand challenge project;basis-spline functions;coordinate-space operators;discretisation;distributed-memory;large-scale 3d solutions;lattice basis-spline collocation method;limited node memory;matrix-vector operations;multiple-data stream parallel computers;multiple-instruction;node-to-node communication overhead;numerical methods;parallelism;quantum structure;quantum-state vectors;relativistic quantum dynamics;spatial lattice;three-dimensional cartesian lattice;time-dependent dirac equation	The numerical methods used to solve the time-dependent Dirac equation on a three-dimensional Cartesian lattice are described. Efficient algorithms are required for computationally intensive studies of nonperturbative relativistic quantum dynamics. Discretization is achieved through the lattice basis-spline collocation method, in which quantum-state vectors and coordinate-space operators are expressed in terms of basis-spline functions on a spatial lattice. All numerical procedures reduce to a series of matrix-vector operations which are performed on the Intel iPSC/860 hypercube, making full use of parallelism. Solutions to the problems of limited node memory and node-to-node communication overhead inherent in using distributed-memory, multiple-instruction, multiple-data stream parallel computers are discussed.	grand challenges;quantum dynamics	J. C. Wells;V. E. Oberacker;A. S. Umar;C. Bottcher;M. R. Strayer;John B. Drake;R. Flanery	1993		10.1109/SUPERC.1993.1263466	parallel processing;parallel computing;quantum field theory;bound state;lattice model;numerical analysis;computer science;theoretical computer science;quantum dynamics;quantum mechanics	Vision	-3.4749635371441423	37.840733162772224	88367
44ab0a2c3213279927bdb4ca08b4869d3c67cb7e	medusa: a scalable interconnect for many-port dnn accelerators and wide dram controller interfaces		To cope with the increasing demand and computational intensity of deep neural networks (DNNs), industry and academia have turned to accelerator technologies. In particular, FPGAs have been shown to provide a good balance between performance and energy efficiency for accelerating DNNs. While significant research has focused on how to build efficient layer processors, the computational building blocks of DNN accelerators, relatively little attention has been paid to the on-chip interconnects that sit between the layer processors and the FPGA's DRAM controller. We observe a disparity between DNN accelerator interfaces, which tend to comprise many narrow ports, and FPGA DRAM controller interfaces, which tend to be wide buses. This mismatch causes traditional interconnects to consume significant FPGA resources. To address this problem, we designed Medusa: an optimized FPGA memory interconnect which transposes data in the interconnect fabric, tailoring the interconnect to the needs of DNN layer processors. Compared to a traditional FPGA interconnect, our design can reduce LUT and FF use by 4.7x and 6.0x, and improves frequency by 1.8x.		Yongming Shen;Tianchu Ji;Michael Ferdman;Peter A. Milder	2018	2018 28th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2018.00026	port (computer networking);scalability;field-programmable gate array;parallel computing;artificial neural network;computer science;control theory;data transmission;lookup table;multiplexing	EDA	3.213679010748453	43.26061462128473	88393
ff6cefb8c311cdaaa7b1b9d8ce3186b5fc348f0d	accelerating recurrent neural networks in analytics servers: comparison of fpga, cpu, gpu, and asic	random access memory;field programmable gate arrays recurrent neural networks graphics processing units random access memory logic gates classification algorithms runtime;runtime;graphics processing unit recurrent neural networks analytics servers fpga gpu asic rnn gated recurrent unit gru memoization optimization dense matrix vector multiplications sgemv multicore cpu on chip bram dsp reconfigurable fabric fine grained parallelisms field programmable gate array;recurrent neural nets application specific integrated circuits field programmable gate arrays graphics processing units matrix multiplication neural chips;logic gates;graphics processing units;classification algorithms;recurrent neural networks;field programmable gate arrays	Recurrent neural networks (RNNs) provide state-of-the-art accuracy for performing analytics on datasets with sequence (e.g., language model). This paper studied a state-of-the-art RNN variant, Gated Recurrent Unit (GRU). We first proposed memoization optimization to avoid 3 out of the 6 dense matrix vector multiplications (SGEMVs) that are the majority of the computation in GRU. Then, we study the opportunities to accelerate the remaining SGEMVs using FPGAs, in comparison to 14-nm ASIC, GPU, and multi-core CPU. Results show that FPGA provides superior performance/Watt over CPU and GPU because FPGA's on-chip BRAMs, hard DSPs, and reconfigurable fabric allow for efficiently extracting fine-grained parallelisms from small/medium size matrices used by GRU. Moreover, newer FPGAs with more DSPs, on-chip BRAMs, and higher frequency have the potential to narrow the FPGA-ASIC efficiency gap.	application-specific integrated circuit;artificial neural network;central processing unit;computation;digital signal processor;field-programmable gate array;graphics processing unit;language model;mathematical optimization;memoization;multi-core processor;performance per watt;random neural network;recurrent neural network;sparse matrix;stratix	Eriko Nurvitadhi;Jaewoong Sim;David Sheffield;Asit K. Mishra;Krishnan Srivatsan;Debbie Marr	2016	2016 26th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2016.7577314	statistical classification;embedded system;parallel computing;real-time computing;logic gate;computer science;recurrent neural network;theoretical computer science;operating system;field-programmable gate array	EDA	3.6717636589304177	43.117102565728665	88431
7754ba5fb60293f613e6c6edf80934a9798b5034	gpu implementation of krylov solvers for block-tridiagonal eigenvalue problems	gpu computing;krylov methods;eigenvalue computation;comunicacion en congreso;capitulo de libro;block tridiagonal linear solvers	In an eigenvalue problem defined by one or two matrices with block-tridiagonal structure, if only a few eigenpairs are required it is interesting to consider iterative methods based on Krylov subspaces, even if matrix blocks are dense. In this context, using the GPU for the associated dense linear algebra may provide high performance. We analyze this in an implementation done in the context of SLEPc, the Scalable Library for Eigenvalue Problem Computations. In the case of a generalized eigenproblem or when interior eigenvalues are computed with shift-andinvert, the main computational kernel is the solution of linear systems with a block-tridiagonal matrix. We explore possible implementations of this operation on the GPU, including a block cyclic reduction algorithm.		Alejandro Lamas Daviña;José E. Román	2015		10.1007/978-3-319-32149-3_18	computational science;mathematical optimization;parallel computing;computer science;theoretical computer science;general-purpose computing on graphics processing units	HPC	-2.8123687920822102	38.7032584880836	88477
37cf8058b107cce4887b7e7c1c89ca231e3da3e9	profiling transactional memory applications	software;histograms;genomics;application software genomics bioinformatics java scalability computer languages computer science testing parallel programming programming profession;programming language;profiling;routing;storage management;vacation;software requirement;software requirements;lee tm;storage management profiling;kmeans;dstm2 transactional memory scalable parallel programs software requirement genome kmeans vacation lee tm;genome;dstm2;scalability;transactional memory;parallel programs;scalable parallel programs;profiling transactional memory;hardware;bioinformatics	Transactional Memory (TM) has become an active research area as it promises to simplify the development of highly scalable parallel programs. Scalability is quickly becoming an essential software requirement as successive commodity processors integrate ever larger numbers of cores. Non-trivial TM applications to test TM implementations have only recently begun to emerge, but have been written in different programming languages, using different TM implementations, making analysis difficult.We ported the popular non-trivial TM applications from the STAMP suite (Genome, KMeans, and Vacation), and Lee-TM to DSTM2, a software TM implementation, and built into it a framework to profile their execution. This paper investigates which profiling information is most relevant to understanding the performance of these non-trivial TM applications using up to 8 processors. We report commonly used transactional execution metrics and introduce two new metrics that can be used to profile TM applications.	bulldozer (microarchitecture);central processing unit;html;intelligent character recognition;k-means clustering;parallel computing;programming language;requirement;run time (program lifecycle phase);scalability;software transactional memory	Mohammad Ansari;Kim Jarvis;Christos Kotselidis;Mikel Luján;Chris C. Kirkham;Ian Watson	2009	2009 17th Euromicro International Conference on Parallel, Distributed and Network-based Processing	10.1109/PDP.2009.35	genomics;parallel computing;real-time computing;computer science;operating system;database;distributed computing;programming language;software requirements	HPC	-1.1183532008061243	43.06491654024641	88551
b253d14f74c064e39451e1d34d9a326fb10ada83	a prototype processing-in-memory (pim) chip for the data-intensive architecture (diva) system	memory wall;multimedia application;chip;floating point;sparse matrix computation;memory bandwidth;processing in memory	The Data-Intensive Architecture (DIVA) system employs Processing-In-Memory (PIM) chips as smartmemory coprocessors. This architecture exploits inherent memory bandwidth both on chip and across the system to target several classes of bandwidth-limited applications, including multimedia applications and pointer-based and sparse-matrix computations. The DIVA project has built a prototype development system using PIM chips in place of standard DRAMs to demonstrate these concepts. We have recently ported several demonstration kernels to this platform and have exhibited a speedup of 35X on a matrix transpose operation. This paper focuses on the 32-bit scalar and 256-bit WideWord integer processing components of the first DIVA prototype PIM chip, which was fabricated in TSMC 0.18 μm technology. In conjunction with other publications, this paper demonstrates that impressive gains can be achieved with very little “smart” logic added to memory devices. A second PIM prototype that includes WideWord floating-point capability is scheduled to tape out in August 2003.	32-bit;computation;coprocessor;dimm;data-intensive computing;memory bandwidth;memory bus;personal handy-phone system;pointer (computer programming);processor design;prototype;random-access memory;server (computing);single-precision floating-point format;software architecture;sparse matrix;speedup;system integration;systems architecture;tape-out;workstation	Jeffrey T. Draper;Tim Barrett;Jeff Sondeen;Sumit D. Mediratta;Chang Woo Kang;Ihn Kim;Gokhan Daglikoca	2005	VLSI Signal Processing	10.1007/s11265-005-4939-1	chip;embedded system;parallel computing;computer hardware;telecommunications;computer science;floating point;operating system;memory bandwidth	HPC	-1.2302606666286462	45.516622796035364	88899
1da81b1430a4300f36b6eaf199e6281ab12510fa	distributed-memory breadth-first search on massive graphs		In this chapter, we study the problem of traversing large graphs. A traversal, a systematic method of exploring all the vertices and edges in a graph, can be done in many different orders. A traversal in “breadth-first” order, a breadth-first search (BFS), is important because it serves as a building block for many graph algorithms. Parallel graph algorithms increasingly rely on BFS for exploring all vertices in a graph because depth-first search is inherently sequential. Fast parallel graph algorithms often use BFS, even when the optimal sequential algorithm for solving the same problem relies on depth-first search. Strongly connected component decomposition of a graph [27, 32] is an example of such an computation. Given a distinguished source vertex s, BFS systematically explores the graph G to discover every vertex that is reachable from s. In the worst case, BFS has to explore all of the edges in the connected component that s belongs to in order to reach every vertex in the connected component. A simple level-synchronous traversal that explores all of the outgoing edges of the current frontier (the set of vertices discovered in this level) is therefore considered optimal in the worst-case analysis. This level-synchronous algorithm exposes lots of parallelism for low-diameter (small-world) graphs [35]. Many real-world graphs, such as those representing social interactions and brain anatomy, are known to have small-world characteristics. Parallel BFS on distributed-memory systems is challenging due to its low computational intensity and irregular data access patterns. Recently, a large body of optimization strategies have been designed to improve the performance of parallel BFS in distributed-memory systems. Among those, two major techniques stand out in terms of their success and general applicability. The first one is the direction-optimization by Beamer et al.[4] that optimistically reduces the number of edge examinations by integrating a bottom-up algorithm into the traversal. The second one is the use of two-dimensional (2D) decomposition of the sparse adjacency matrix of the graph [6, 37]. In this article, we build on our prior distributed-memory BFS work [5, 6] by expanding our performance optimizations. We generalize our 2D approach to arbitrary pr × pc rectangular processor grids (which subsumes the 1D cases in its extremes of pr = 1 or pc = 1). We evaluate the effects of in-node multithreading for performance and scalability. We run our experiments on three different platforms: a Cray XE6, a Cray XK7 (without using co-processors), and a Cray XC30. We compare the effects of using a scalable hypersparse representation called Doubly Compressed Sparse Column (DCSC) for storing local subgraphs versus a simpler but less memory-efficient Compressed Sparse Row (CSR) representation. Finally, we validate our implementation by using it to efficiently traverse a real-world graph.	adjacency matrix;best, worst and average case;breadth-first search;central processing unit;computation;connected component (graph theory);cray xc30;cray xe6;cray xk7;data access;depth-first search;distributed memory;experiment;graph (discrete mathematics);graph theory;graph traversal;interaction;list of algorithms;mathematical optimization;multithreading (computer architecture);parallel computing;scalability;sequential algorithm;sparse matrix;strongly connected component;thread (computing);tree traversal;vertex (geometry);vertex (graph theory)	Aydin Buluç;Scott Beamer;Kamesh Madduri;Krste Asanovic;David A. Patterson	2015	CoRR		parallel computing;computer science;theoretical computer science;distributed computing	ML	-2.941507859106031	42.89869608240237	89621
12f18385d58f4fa0065bca5333ebad65d7c2ec6b	the row/column pivoting strategy on multicomputers	parallelisme;numerical stability;facteur croissance;strategie orientale;growth factor;distribucion carga;estabilidad numerica;pivoting strategies;factor crecimiento;pivoting strategy;parallelism;paralelismo;load balancing;distribution charge;load distribution;stabilite numerique;pipelining schemes	On multicomputers the partial pivoting phase of the LU factorization has a peculiar load unbalancing due to the presence of idle processors in most matrix decompositions. Moreover, intrinsic synchronization barriers do not allow a complete masking of this overhead by means of pipelining techniques. We propose to reduce load unbalancing by 'assigning extra work to idle processors'; this leads to a new pivoting strategy, named row/column pivoting, which is mainly attractive to 2D decompositions. Row/column pivoting furnishes an LU factorization algorithm that guarantees better numerical stability at the same cost of partial pivoting in case of square decomposition. A further improvement is achieved by adding pipelining schemes to the naive form. In the design of the algorithms and in their evaluation we have adopted a new environment that allows a decomposition-independent parallel programming.	algorithm;central processing unit;distributed computing;lu decomposition;numerical stability;overhead (computing);parallel computing;pipeline (computing);pivot element;qr decomposition	Michele Angelaccio;Michele Colajanni	1994	Parallel Computing	10.1016/0167-8191(94)90081-7	parallel computing;computer science;weight distribution;load balancing;mathematics;distributed computing;numerical stability;algorithm	HPC	-2.2525097390676545	37.23683507957983	89733
5dcbbe4bba5b2a5afb07985f8cdbb7a0aea70efc	fast data manipulation in multiprocessors using parallel pipelined memories	file transfer;tratamiento datos;distributed system;systeme reparti;shared memory;multiprocessor;memoria compartida;transferencia fichero;data processing;traitement donnee;manipulacion;sistema repartido;transfert fichier;manipulation;multiprocesador;memoire partagee;multiprocesseur	We present a new approach to structured data movement in shared-memory multiprocessors using 1 D and 2D interleaved memory organizations. We introduce the concept of the interleaved-read-modify-write (IRW) cycle for atomic vector access in shared memory. An on-the-fly index manipulation scheme provides the functionality of a switch network capable of realizing the generalized interconnections characterized by C. D. Thompson (Generalized connection networks for parallel processor interconnections. IEEE Trans. Comput. C-27, 12, Dec. 1978, 1119-l 125). The effectiveness of this scheme is evaluated on two different multiprocessor organizations: a crossbar-connected multiprocessor with 1D memory interleaving and an orthogonal multiprocessor (OMP) with 2D memory interleaving, each with n processors and n* memory modules. With n* data elements and O(n) data buffers associated with each processor, the crossbar-connected multiprocessor is shown to be O(n) and U(n*) times slower than the OMP to realize permutations and generalized mappings, respectively. The OMP realizes any generalized mapping of n * data elements in at most five IRW steps and any permutation in at most three IRW steps. Using only n processors, this data movement capability of an OMP is comparable to that of a mesh-connected computer with n* processors. For certain classes of data movement, a reduction framework is presented to reduce multiple row and column operations of a mesh-connected computer into fewer equivalent row-column operations in an OMP. The proposed scheme demonstrates the potential for significant speedup in large-scale vector/ matrix manipulations and in real-time image/signal processing on a sharedmemory multiprocessor.	central processing unit;crossbar switch;dimm;forward error correction;interleaved memory;multiprocessing;openmp;parallel computing;read-modify-write;real-time clock;shared memory;signal processing;speedup	Dhabaleswar K. Panda;Kai Hwang	1991	J. Parallel Distrib. Comput.	10.1016/0743-7315(91)90017-4	shared memory;parallel computing;real-time computing;multiprocessing;distributed memory;data processing;computer science;theoretical computer science;operating system;distributed computing;algorithm	Arch	-1.5269250725747774	37.37502027928872	89760
b513711621e81d0abd042e0877ca751581a993f5	graphmat: high performance graph analytics made productive		Given the growing importance of large-scale graph analytics, there is a need to improve the performance of graph analysis frameworks without compromising on productivity. GraphMat is our solution to bridge this gap between a user-friendly graph analytics framework and native, hand-optimized code. GraphMat functions by taking vertex programs and mapping them to high performance sparse matrix operations in the backend. We thus get the productivity benefits of a vertex programming framework without sacrificing performance. GraphMat is a single-node multicore graph framework written in C++ which has enabled us to write a diverse set of graph algorithms with the same effort compared to other vertex programming frameworks. GraphMat performs 1.1-7X faster than high performance frameworks such as GraphLab, CombBLAS and Galois. GraphMat also matches the performance of MapGraph, a GPU-based graph framework, despite running on a CPU platform with significantly lower compute and bandwidth resources. It achieves better multicore scalability (13-15X on 24 cores) than other frameworks and is 1.2X off native, hand-optimized code on a variety of graph algorithms. Since GraphMat performance depends mainly on a few scalable and well-understood sparse matrix operations, GraphMat can naturally benefit from the trend of increasing parallelism in future hardware.	algorithm;c++;central processing unit;graph theory;graphics processing unit;list of algorithms;multi-core processor;parallel computing;scalability;sparse matrix;usability	Narayanan Sundaram;Nadathur Satish;Md. Mostofa Ali Patwary;Subramanya Dulloor;Michael J. Anderson;Satya Gautam Vadlamudi;Dipankar Das;Pradeep Dubey	2015	PVLDB	10.14778/2809974.2809983	parallel computing;computer science;theoretical computer science;database;distributed computing	DB	-3.509232562120198	43.24710675882782	90185
16a3e7dcd42c2f50a5a34591bf33729979bb9c6f	equalisation of time-variant communications channels via channel estimation based approaches	channel estimator;kalman filter;adaptive equaliser;time-variant channel;channel estimation;gradient algorithm;time-variant communications channel;linear filtering;communication channels;least square;second order;computer simulation	Dans le but de résoudre le problème de l'égalisation dans un environnement variant dans le temps, deux nouvelles approches sont développées, dans lesquelles on tente de séparer le processus d'estimation du canal de celui de l'égalisation. Deux filtres linéaires, estimateur de canal et filtre d'égalisation, sont utilisés pour reconstruire la séquence transmise. L'algorithme du gradient avec une prédiction à mémoire progressive aux moindres carrés de degré 1 est adopté pour l'estimateur de canal. Les coefficients du filtre d'égalisation sont indirectement mis à jour à l'aide des résultats produits par l'estimateur de canal. Les simulations informatiques montrent que l'estimation à deux canaux basée sur les égalisateurs adaptatifs représente une amélioration significative par rapport au cas d'un canal de communication modélisé par un modèle de Markov du deuxième ordre.	channel state information	Tetsuya Shimamura;Shahram Semnani;Colin Cowan	1997	Signal Processing	10.1016/S0165-1684(97)80005-9	computer simulation;kalman filter;electronic engineering;telecommunications;computer science;engineering;linear filter;control theory;mathematics;least squares;second-order logic;statistics;channel	EDA	8.570883790859803	34.127913614585324	90319
253402be4173c31f09b74007c3024518fa1c06fe	summa: scalable universal matrix multiplication algorithm	matrix multiplication	In this paper, we give a straight forward, highly e cient, scalable implementation of common matrix multiplication operations. The algorithms are much simpler than previously published methods, yield better performance, and require less work space. MPI implementations are given, as are performance results on the Intel Paragon system.	intel paragon;matrix multiplication algorithm;scalability	Robert A. van de Geijn;Jerrell Watts	1997	Concurrency - Practice and Experience	10.1002/(SICI)1096-9128(199704)9:4%3C255::AID-CPE250%3E3.0.CO;2-2	parallel computing;multiplication algorithm;computer science;theoretical computer science;distributed computing	HPC	-1.4813644427349542	38.50341900074447	90383
3ea4e800101b48675ce30ac8fed135bd0f36eb4d	an improved hardware implementation of the grain stream cipher	galois configuration;fibonacci configuration;pseudo random bit sequence;nlfsr;shift registers cryptography random sequences;clocks;random sequences;galois configuration grain stream cipher pseudorandom bit sequence nonlinear feedback shift register fibonacci configuration;linear feedback shift register;datorsystem;computer systems;other electrical engineering electronic engineering information engineering;indexes;stream cipher;logic gates;grain;nonlinear feedback shift register;grain stream cipher;cryptography;hardware implementation grain stream ciphers galois nlfsr;shift registers;annan elektroteknik och elektronik;galois;clocks shift registers delay throughput indexes hardware logic gates;stream ciphers;pseudorandom bit sequence;hardware implementation;throughput;hardware	A common approach to protect confidential information is to use a stream cipher which combines plain text bits with apseudo-random bit sequence. Among the existing stream ciphers, Non-Linear Feedback Shift Register (NLFSR)-based ones provide the best trade-off between cryptographic security and hardware efficiency. In this paper, we show how to further improve the hardware efficiency of the Grain stream cipher. By transforming the NLFSR of Grain from its original Fibonacci configuration to the Galois configuration and by introducing new hardware solutions, we double the throughput of the 80 and 128-bit key 1 bit/cycle architectures of Grain with no area and power penalty.	1-bit architecture;128-bit;confidentiality;nonlinear feedback shift register;parallel computing;pseudorandomness;stream cipher;strong cryptography;throughput	Shohreh Sharif Mansouri;Elena Dubrova	2010	2010 13th Euromicro Conference on Digital System Design: Architectures, Methods and Tools	10.1109/DSD.2010.49	parallel computing;real-time computing;computer science;theoretical computer science;stream cipher attack;stream cipher;algorithm	Arch	9.276876771512908	45.04856806305113	90453
b2421095c6225b330f1c66cba42b8ce4792435ea	high performance fft on multicore processors	cognitive radio;fast fourier transforms;microprocessor chips;multiprocessing systems;telecommunication computing;enlight digital optical core device;ibm cell multisimd processor;nvidia tesla simt processor;ofdm modulators;cognitive radio applications;fast fourier transforms;high performance fft implementation;multicore processors;orthogonal frequency-division multiplexing;fft;cognitive radio;multicore procesors;optical core processors;transverse vectorization	Importance of achieving high performance Fourier transforms for Cognitive Radio applications can not be over-emphasized. This includes signal detection in the presence of noise power uncertainty, multi-resolution spectrum sensing, minimization of subcarriers' side lobes in OFDM modulators, multi-stream processing, or spectrum loading, for example. With the emergence of advanced multicore processors, there is a remarkable opportunity to develop novel, massively parallel implementations of the FFT. This paper reviews recent advances in the area, and presents results for three classes of devices: the IBM Cell multi-SIMD processor, the Nvidia Tesla SIMT processor, and the EnLight digital optical core device.	algorithm;cuda;cell (microprocessor);central processing unit;cognitive radio;coherent;detection theory;direct memory access;emergence;flops;fast fourier transform;fortran;general-purpose computing on graphics processing units;graphics processing unit;input/output;mathematical optimization;multi-core processor;noise power;numerical analysis;nvidia tesla;optimizing compiler;processor technology;simd;single instruction, multiple threads;stream processing;subcarrier;toslink;tesla (microarchitecture);throughput	Jacob Barhen;Charlotte Kotas;Travis S. Humble;Pramita Mitra;Neena Imam;Mark A. Buckner;Michael R. Moore	2010	2010 Proceedings of the Fifth International Conference on Cognitive Radio Oriented Wireless Networks and Communications		embedded system;parallel computing;computer hardware;computer science	HPC	1.572561220968848	44.63194400822056	90518
1a6145895dc7c762ad3c1ad97ab0da961f61b97e	solving the symmetric eigenvalue problem on distributed memory systems	distributed memory		distributed memory	Domingo Giménez;M. J. Majado;Isidro Verdú	1997			computer science;mathematical optimization;divide-and-conquer eigenvalue algorithm;distributed memory;distributed computing;eigenvalues and eigenvectors;inverse iteration;eigenvalue perturbation	HPC	-2.9900333266938266	37.86197164702283	90534
01b4e8fe9c081a24fa904783153f9675281d7b9f	exploiting the power of gpus for asymmetric cryptography	asymmetric cryptosystems;dsa;paper;ecc;rsa;cuda;elliptic curve cryptography;nvidia geforce 8800 gts;nvidia;graphic processing unit;algorithms;computer science;graphics processing unit;security;off the shelf;central processing unit	Modern Graphics Processing Units (GPU) have reached a dimension with respect to performance and gate count exceeding conventional Central Processing Units (CPU) by far. Many modern computer systems include – beside a CPU – such a powerful GPU which runs idle most of the time and might be used as cheap and instantly available co-processor for general purpose applications. In this contribution, we focus on the efficient realisation of the computationally expensive operations in asymmetric cryptosystems on such off-the-shelf GPUs. More precisely, we present improved and novel implementations employing GPUs as accelerator for RSA and DSA cryptosystems as well as for Elliptic Curve Cryptography (ECC). Using a recent Nvidia 8800GTS graphics card, we are able to compute 813 modular exponentiations per second for RSA or DSA-based systems with 1024 bit integers. Moreover, our design for ECC over the prime field P-224 even achieves the throughput of 1412 point multiplications per second.	and gate;analysis of algorithms;central processing unit;coprocessor;cryptosystem;elliptic curve cryptography;gate count;geforce 8 series;graphics processing unit;public-key cryptography;throughput;video card	Robert Szerwinski;Tim Güneysu	2008		10.1007/978-3-540-85053-3_6	parallel computing;computer hardware;computer science;information security;theoretical computer science;operating system;central processing unit;digital signature algorithm;elliptic curve cryptography;computer security;algorithm	Arch	8.21816598906967	44.40805994660792	90614
d7f548db45a22e635d9bbf7f4a7fb77ce1d487fd	toward on-chip acceleration of the backpropagation algorithm using nonvolatile memory		By performing computation at the location of data, non-Von Neumann (VN) computing should provide power and speed benefits over conventional (e.g., VN-based) approaches to data-centric workloads such as deep learning. For the on-chip training of largescale deep neural networks using nonvolatile memory (NVM) based synapses, success will require performance levels (e.g., deep neural network classification accuracies) that are competitive with conventional approaches despite the inherent imperfections of such NVM devices, and will also require massively parallel yet low-power read and write access. In this paper, we focus on the latter requirement, and outline the engineering tradeoffs in performing parallel reads and writes to large arrays of NVM devices to implement this acceleration through what is, at least locally, analog computing. We address how the circuit requirements for this new neuromorphic computing approach are somewhat reminiscent of, yet significantly different from, the well-known requirements found in conventional memory applications. We discuss tradeoffs that can influence both the effective acceleration factor (“speed”) and power requirements of such on-chip learning accelerators. P. Narayanan A. Fumarola L. L. Sanches K. Hosokawa S. C. Lewis R. M. Shelby G. W. Burr	analog computer;approximation algorithm;artificial neural network;backpropagation;block cipher mode of operation;computation;crossbar switch;deep learning;electronic circuit;file system permissions;furby;low-power broadcasting;method overriding;multiply–accumulate operation;neuromorphic engineering;non-volatile memory;nonvolatile bios memory;overhead (computing);peripheral;requirement;routing;sigmoid function;software propagation;sparse matrix;von neumann architecture;whole earth 'lectronic link	Pritish Narayanan;Alessandro Fumarola;Lucas L. Sanches;Kohji Hosokawa;S. C. Lewis;Robert M. Shelby;Geoffrey W. Burr	2017	IBM Journal of Research and Development		chip;computer engineering;massively parallel;parallel computing;conventional memory;electronic engineering;computer science;deep learning;artificial neural network;artificial intelligence;non-volatile memory;backpropagation;neuromorphic engineering	Arch	4.309895184010397	42.203038001014725	90616
2a2eb0e00483288a8b3d2b561dd98e013c5c0275	exploring the hidden dimension in graph processing		Task partitioning of a graph-parallel system is traditionally considered equivalent to the graph partition problem. Such equivalence exists because the properties associated with each vertex/edge are normally considered indivisible. However, this assumption is not true for many Machine Learning and Data Mining (MLDM) problems: instead of a single value, a vector of data elements is defined as the property for each vertex/edge. This feature opens a new dimension for task partitioning because a vertex could be divided and assigned to different nodes. To explore this new opportunity, this paper presents 3D partitioning, a novel category of task partition algorithms that significantly reduces network traffic for certain MLDM applications. Based on 3D partitioning, we build a distributed graph engine CUBE. Our evaluation results show that CUBE outperforms state-of-the-art graph-parallel system PowerLyra by up to 4.7× (up to 7.3× speedup against PowerGraph).	algorithm;data mining;graph (abstract data type);graph partition;indivisible;machine learning;network traffic control;partition problem;speedup;task allocation and partitioning of social insects;turing completeness	Mingxing Zhang;Yongwei Wu;Kang Chen;Xuehai Qian;Xue Li;Weimin Zheng	2016			equivalence (measure theory);theoretical computer science;vertex (geometry);speedup;partition (number theory);cube;computer science;vertex (graph theory);graph partition;graph	OS	-3.19334225043315	42.795343192973476	90699
6672eddbfa1b0bd51d7dc28a12aa9d54f2b6e669	parallelization of a recursive decoupling method for solving tridiagonal linear systems on distributed memory computer	distributed memory;distributed system;algoritmo paralelo;methode recursive;eigenvalue problem;systeme reparti;parallel algorithm;shared memory;numerical method;matrice reelle;methode newton;memoria compartida;metodo recursivo;recursive method;probleme valeur propre;paralelisacion;matriz simetrica;matriz toeplitz;linear system;real matrix;algorithme parallele;symmetric matrix;sistema repartido;metodo numerico;parallelisation;specification tests;matrice toeplitz;parallelization;matrice symetrique;metodo newton;newton method;toeplitz matrix;methode numerique;memoire partagee;matriz real;problema valor propio	This work presents a parallelization of a recursive decoupling method for solving tridiagonal linear system on distributed memory computer. We study the fill-in in the algorithm to optimize the execution of the scalar algorithm and to perform the communications. Finally, we evaluate the algorithm through specific test on the Fujitsu AP3000.	automatic parallelization;coupling (computer programming);distributed memory;linear system;parallel computing;recursion (computer science)	Margarita Amor;Francisco Argüello;Juan López;Emilio L. Zapata	2000		10.1007/3-540-44942-6_28	shared memory;parallel computing;distributed memory;numerical analysis;computer science;calculus;toeplitz matrix;mathematics;parallel algorithm;newton's method;linear system;algorithm;symmetric matrix	HPC	-2.8055746639257486	36.81295706302052	90749
e6c898cb6a70db866a5d009ddd79d84291fc70a0	fast block-iterative domain decomposition algorithm for ir drop analysis in large power grid	power grid analysis algorithm;divide and conquer strategy;convergence;concurrent computing;domain decomposition;iterative algorithms;iterative methods divide and conquer methods integrated circuit design integrated circuit reliability;optical computing;block iterative domain decomposition algorithm;direct solver block iterative domain decomposition algorithm ir drop analysis power grid analysis algorithm divide and conquer strategy parallel processing;runtime;divide and conquer methods;algorithm design and analysis power grids iterative algorithms grid computing optical computing runtime parallel processing concurrent computing iterative methods failure analysis;failure analysis;iterative methods;integrated circuit design;domain decomposition method;mathematical model;ir drop analysis;power grid;power grids;strips;integrated circuit reliability;iteration method;grid computing;divide and conquer;algorithm design and analysis;direct solver;parallel processing	Due to the extremely large sizes of power grids, IR drop analysis has become a computationally challenging problem both in terms of runtime and memory usage. In order to design scalable algorithms to handle ever increasing power-grid sizes, the most promising approach is to use a “divide-and-conquer” strategy such as domain decomposition. Such an approach not only decomposes a large problem into manageable sub-problems, it also naturally allow a parallel processing solution for further speedup in computation time. As a result, a power-grid analysis algorithm based upon the traditional domain decomposition method has been reported in [9]. Unfortunately, the method in [9] has strong limitation on the size of the interfaces between the sub-problems and therefore severely limits its capability in solving very large problems. In this paper, we present a block-iterative domain-decomposition algorithm which effectively combines the advantages of direct solvers and iterative methods. With a carefully chosen domain decomposition strategy, our approach does not suffer from the difficulties of [9]. While the algorithm in [9] fails to analyze a power grid of 4 millions nodes, our algorithm solves a power grid of 42 millions nodes accurately in 1.5 hours.	computation;direct method in the calculus of variations;domain decomposition methods;iterative method;parallel algorithm;parallel computing;scalability;speedup;time complexity	Yu Zhong;Martin D. F. Wong	2010	2010 11th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2010.5450430	parallel processing;mathematical optimization;parallel computing;concurrent computing;computer science;theoretical computer science;iterative method	HPC	-1.0661851683223285	39.85459013435414	90817
c0aab0d1add5edb81ac3bd960b53081c10911aa0	pipeline and parallel-pipeline fft processors for vlsi implementations	computers;electronic circuits;digital computers;traitement signal;microelectronic circuits;integral transformations;general and miscellaneous mathematics computing and information science;algorithm analysis;fourier transform;integrated circuit;transformations 990200 mathematics computers;cordic;very large scale integration;implementation;performance;program transformation;mathematical logic;signal processing cordic fast fourier transform integrated circuits parallel processors pipeline processors;chip;pipeline processors;fast fourier transform;computer architecture;integral transforms;fourier transformation;signal processing;computer calculations;pipelines;data flow processing;fast fourier transforms;bandwidth;algorithms;design;analyse algorithme;modular construction;processeur;signal processing algorithms;discrete fourier transforms;transformation fourier rapide;high performance;programming;integrated circuits;parallel processing;processor;circuit integre;parallel processors;fast fourier transformation;hardware	In some signal processing applications, it is desirable to build very high performance fast Fourier transform (FFT) processors. To meet the performance requirements, these processors are typically highly pipelined. Until the advent of VLSI, it was not possible to build a single chip which could be used to construct pipeline FFT processors of a reasonable size. However, VLSI implementations have constraints which differ from those of discrete implementations, requiring another look at some of the typical FFT'algorithms in the light of these constraints.	central processing unit;fast fourier transform;pipeline (computing);requirement;signal processing;very-large-scale integration	Erling Wold;Alvin M. Despain	1984	IEEE Transactions on Computers	10.1109/TC.1984.1676458	parallel processing;fast fourier transform;computer architecture;parallel computing;computer science;theoretical computer science;signal processing;algorithm	Arch	4.888792929887243	45.87228895560376	90875
50fa3c3e5575bf816428a0c47427ad1f78c41db9	experimental evaluation of efficient sparse matrix distributions	distributed memory machine;experimental evaluation;sparse matrix	Sparse matrix problems are difficult to parallelize efficiently on distributed memory machines since non-zero elements are unevenly scattered and are accessed via multiple levels of indirection. Irregular distributions that achieve good load balance and locality are hard to compute, have high memory overheads and also lead to further indirection in locating distributed data. This paper evaluates alternative semi-regular distribution strategies which trade off the quality of loadbalance and locality for lower decomposition overheads and efficient lookup. The proposed techniques are compared to an irregular sparse matrix partitioned and the relative merits of each distribution method are outlined.	acm/ieee supercomputing conference;amortized analysis;brs/search;benchmark (computing);binary space partitioning;chaos;distributed memory;high memory;indirection;iteration;iterative method;lecture notes in computer science;load balancing (computing);locality of reference;lookup table;machine-readable dictionary;nicol prism;overhead (computing);parallel computing;preprocessor;requirement;semiconductor industry;sparse matrix;tame	Manuel Ujaldon;Shamik D. Sharma;Emilio L. Zapata;Joel H. Saltz	1996		10.1145/237578.237588	parallel computing;sparse matrix;theoretical computer science;machine learning;sparse approximation;matrix-free methods	HPC	-1.8379026814795203	39.41463482715675	90910
3edbf4f719006185353712de395b3fd31177ba26	efficient arithmetic for lattice-based cryptography on gpu using the cuda platform	iterative ntt;graphical processing units;fft based method;gpu;schoolbook multiplication method;polynomials;parallel architectures;cryptography;signal processing;graphics processing units;fast fourier transforms;polynomial multiplication algorithms;cuda platform;graphics processing units instruction sets cryptography signal processing conferences polynomials educational institutions;lattice based cryptographic schemes;conferences;instruction sets	The demand to lattice-based cryptographic schemes has been inreasing. Due to processing unit having multiple processors, there is a need to implements such protocols on these platforms. Graphical processing units (GPU) have attracted so much attention. In this paper, polynomial multiplication algorithms, having a very important role in lattice-based cryptographic schemes, are implemented on a GPU (NVIDIA Quadro 600) using the CUDA platform. FFT-based and schoolbook multiplication methods are implemented in serial and parallel way and a timing comparison for these techniques is given. It's concluded that for the polynomials whose degrees are up to 2000 the fastest polynomial multiplication method is iterative NTT.	cuda;central processing unit;fast fourier transform;fastest;graphical user interface;graphics processing unit;iterative method;lattice-based cryptography;multiplication algorithm;nvidia quadro;polynomial ring;serial port	Sedat Akleylek;Zaliha Yüce Tok	2014	2014 22nd Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2014.6830364	computer vision;parallel computing;computer hardware;computer science;cryptography;theoretical computer science;signal processing;statistics	HPC	9.093158290714848	43.54677456613982	91418
3aa8924a93e17ca20af42be7a57059f39ace78ac	high-frequency financial statistics with parallel r and intel xeon phi coprocessor	portfolios resource management optimization risk management vectors libraries parallel processing;time series asset management coprocessors financial data processing investment optimisation parallel processing pricing risk management stock markets;high frequency financial statistics financial world high frequency trading pricing securities valuation business analytics economic analytics financial risk management optimization procedures portfolio allocation problem high frequency financial data hybrid parallelization solution asset allocations intra day high frequency data hpc techniques parallel r intel math kernel library automatic offloading intel xeon phi coprocessor high frequency price data stocks trading new york stock exchange large scale multiple hypothesis testing time series data software parallelism hardware parallelism;parallel r;high frequency financial analysis;intel xeon phi coprocessor;massive parallelism;intel xeon phi coprocessor high frequency financial analysis massive parallelism parallel r	Financial statistics covers a wide array of applications in the financial world, such as (high frequency) trading, risk management, pricing and valuation of securities and derivatives, and various business and economic analytics. Portfolio allocation is one of the most important problems in financial risk management. One most challenging part in portfolio allocation is the tremendous amount of data and the optimization procedures that require computing power beyond the currently available desktop systems. In this article, we focus on the portfolio allocation problem using high-frequency financial data, and propose a hybrid parallelization solution to carry out efficient asset allocations in a large portfolio via intra-day high-frequency data. We exploit a variety of HPC techniques, including parallel R, Intel Math Kernel Library, and automatic offloading to Intel Xeon Phi coprocessor in particular to speed up the simulation and optimization procedures in our statistical investigations. Our numerical studies are based on high-frequency price data on stocks traded in New York Stock Exchange in 2011. The analysis results show that portfolios constructed using high-frequency approach generally perform well by pooling together the strengths of regularization and estimation from a risk management perspective. We also investigate the computation aspects of large-scale multiple hypothesis testing for time series data. Using a combination of software and hardware parallelism, we demonstrate a high level of performance on high-frequency financial statistics.	computation;coprocessor;desktop computer;high-level programming language;math kernel library;mathematical optimization;modern portfolio theory;numerical analysis;parallel computing;risk management;simulation;time series;value (ethics);xeon phi	Jian Zou;Hui Zhang	2014	2014 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2014.7004414	parallel computing;real-time computing;computer science;operating system	HPC	-2.6318980511756016	41.103239697652405	91639
2e29edaf4482129a8e9b0b3f2ee2270ef361bd8b	area-efficient fpga implementations of the sha-3 finalists	field programmable gate array;random access memory;clocks;cryptographic hash function;compact implementation;fpga;computer architecture;fpga implementation;efficient implementation;hash functions;adders;cryptography;blake area efficient fpga implementations sha 3 finalists cryptographic hash functions challenge response authentication systems digital signature schemes national institute of standards and technology sha 1 family sha 2 family;field programmable gate arrays cryptography;hash function;sha 3;field programmable gate arrays;fpga cryptography hash functions sha 3 compact implementation;digital signature scheme;random access memory clocks pipeline processing field programmable gate arrays computer architecture adders throughput;pipeline processing;throughput;national institute of standards and technology	Secure cryptographic hash functions are core components in many applications like challenge-response authentication systems or digital signature schemes. Many of these applications are used in cost-sensitive markets and thus slow budget implementations of such components are very important. In the present paper, we focus on the new SHA-3 competition, started by the National Institute of Standards and Technology (NIST), which searches for a new hash function in response to security concerns regarding the previous hash functions SHA-1 and the SHA-2 family. This work adds new valuable data to the competition, by providing an evaluation of area-efficient implementations of all finalists. Our results show, that it is possible to implement all candidates reasonably small. We focus on area-efficiency and therefore we do not rank the candidates by absolute throughput, but rather by the area and the throughput-area ratio. The results hint that Grøstl and Keccak are the best overall performers for compact implementations, if the throughput-area ratio is most important. The following candidate is BLAKE, while the Skein and JH implementations trail behind. The area ranking changes the results and puts JH on the top, followed by BLAKE, Grøstl, Keccak and Skein.	challenge–response authentication;cryptographic hash function;cryptography;digital signature;field-programmable gate array;grøstl;jh (hash function);nist hash function competition;parallel computing;sha-1;sha-2;sha-3;skein (hash function);throughput	Bernhard Jungk;Jürgen Apfelbeck	2011	2011 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2011.16	embedded system;parallel computing;hash function;computer hardware;computer science;theoretical computer science;operating system;skein;field-programmable gate array	EDA	8.56832310631982	45.60495469872357	91730
0cbe1321572bf56a4c44eef91ca10b452bb2915d	multipredicate join algorithms for accelerating relational graph processing on gpus		Recent work has demonstrated that the use of programmable GPUs can be advantageous during relational query processing on analytical workloads. In this paper, we take a closer look at graph problems such as finding all triangles and all four-cliques of a graph. In particular, we present two different join algorithms for the GPU. The first is an implementation of Leapfrog-Triejoin (LFTJ), a recently presented worst-case optimal multi-predicate join algorithm. The second is a novel approach, inspired by the former but more suitable for GPU architectures. Our preliminary performance benchmarks show that for both approaches using GPUs is cost-effective. (the GPU implementation outperforms respective CPU variants). While the second algorithm is faster overall, it comes with increased implementation complexity and storage requirements for intermediary results. Furthermore, both our algorithms are competitive with the hand-written C++ implementation for finding triangles and four-cliques in the graph-processing system GraphLab executing on a multi-core CPU.	algorithm;best, worst and average case;c++;central processing unit;dhrystone;graph (abstract data type);graph (discrete mathematics);input/output;join (sql);leapfrog integration;memory footprint;multi-core processor;relational database;requirement	Haicheng Wu;Daniel Zinn;Molham Aref;Sudhakar Yalamanchili	2014			parallel computing;computer science;theoretical computer science;distributed computing	DB	-3.281915187635205	43.219401133165974	91822
04c0ec9d2ace164a2f405a5c0ff06bef34559024	efficient iterative processing in the scidb parallel array engine	contrast set mining;scientific data management;bitmap indexing	Many scientific data-intensive applications perform iterative computations on array data. There exist multiple engines specialized for array processing. These engines efficiently support various types of operations, but none includes native support for iterative processing. In this paper, we develop a model for iterative array computations and a series of optimizations. We evaluate the benefits of an optimized, native support for iterative array processing on the SciDB engine and real workloads from the astronomy domain.	array processing;computation;data-intensive computing;iteration;iterative method;parallel array;scidb	Emad Soroush;Magdalena Balazinska;K. Simon Krughoff;Andrew J. Connolly	2015		10.1145/2791347.2791362	computer science;theoretical computer science;data mining;database	HPC	-0.9108125119045003	39.55858637042174	91936
04442c51fc5183f62f914a6885bf4afba5eceb4c	advances in sensors-centric microprocessors and system-on-chip	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;n a;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	"""Sensors-based systems are nowadays an extended technology for many markets due to their great potential in the collection of data from the environment and the processing of such data for different purposes. A typical example is the wireless sensor devices, where the outer temperature, humidity, luminosity and many other parameters can be acquired, measured and processed in order to build useful and fascinating applications that contribute to human welfare. In this scenario, the processing architectures of the sensors-based systems play a very important role. The requirements that are necessary for many such applications (real-time processing, low-power consumption, reduced size, reliability, security and many others) means that research on advanced architectures of Microprocessors and System-on-Chips (SoC) is needed to design and implement a successful product. In this sense, there are many challenges and open questions in this area that need to be addressed. Along this line, the special issue """" Microprocessors and System-on-Chip """" of Sensors journal seeks to explore the latest advances in sensors-centric theoretical or practical aspects, such as: microprocessors, SoC, real-time processing, reconfigurable architectures and field programmable gate arrays (FPGAs), low-power devices, operating systems, interfaces and protocols, compilers and software tools, performance analysis, and applications and case studies, among others. For this purpose, of 25 manuscripts received, 11 original and high quality manuscripts were selected to be included in the special issue. The authors of the selected manuscripts belong to universities in China, Italy, Korea, Mexico, Portugal, Spain and Taiwan. Each manuscript was reviewed by between two and four reviewers—prestigious researchers in the same topics as the articles—and underwent up to three rounds of peer-review. The first article [1] in this special issue is entitled """" Network Coding on Heterogeneous Multi-Core"""	3d scanner;architecture as topic;compiler;display resolution;exposure to humidity;field-programmable gate array;linear network coding;low-power broadcasting;manuscripts;microprocessor;multi-core processor;operating system;power semiconductor device;profiling (computer programming);protocols documentation;real-time clock;requirement;system on a chip;universities;sensor (device)	Juan Antonio Gómez Pulido;Miguel A. Vega-Rodríguez	2012		10.3390/s120404820	medical research;telecommunications;computer science;bioinformatics;engineering;electrical engineering;nanotechnology;biological engineering;operations research	EDA	4.800580256431491	32.600328012649655	92130
a704d4f0e47ac8500435d254e9bc5351420beca3	a systolic array architecture for the smith-waterman algorithm with high performance cell design	local alignment;systolic array	To infer homology and subsequently gene function, the Smith-Waterman (S-W) algorithm is used to find the optimal local alignment between two sequences. When searching sequence databases that may contain hundreds of millions of sequences, this algorithm becomes computationally expensive. In this paper, we present a systolic array architecture for the S-W algorithm with a new high performance cell design. The results demonstrate that the implementation of this architecture achieves a speedup of up to 652x, as compared to a software-only implementation, which is almost double the best case reported in the literature. The results also demonstrate that when mapped on the same FPGA platform, our design performs 1.47 to 5.75 times faster in terms of Cell Updates Per Second (CUPS), in comparison with other published systolic array designs, while utilizing 3.69 to 6.36 times less resources.	analysis of algorithms;best, worst and average case;cups;cell signaling;field-programmable gate array;homology (biology);sequence database;smith–waterman algorithm;speedup;systolic array	Laiq Hasan;Yahya M. Khawaja;Abdul Bais	2008			field-programmable gate array;parallel computing;architecture;smith–waterman algorithm;systolic array;speedup;computer science	HPC	0.2737853267704974	43.18927842419361	92401
3182511c054dac8308a08b408b55ed9520650d27	optimizing graph algorithms for improved cache performance	linear algebra;graph theory;microprocessors;cache storage;performance evaluation;dense linear algebra;routing;distributed computing;traffic control;parallel programming;linear algebra costs distributed computing optimizing compilers tree graphs traffic control microprocessors routing computer networks circuit synthesis;computer networks;tree graphs;fundamental graph problem;dijkstra s algorithm;recursion;graph representation;minimum spanning tree;cache performance;single source shortest path;graph algorithm;graph representation cache performance recursion dense linear algebra fundamental graph problem transitive closure floyd warshall algorithm;transitive closure;performance evaluation cache storage graph theory parallel programming;floyd warshall algorithm;optimizing compilers;circuit synthesis	We develop algorithmic optimizations to improve the cache performance of four fundamental graph algorithms. We present a cache-oblivious implementation of the Floyd-Warshall algorithm for the fundamental graph problem of all-pairs shortest paths by relaxing some dependencies in the iterative version. We show that this implementation achieves the lower bound on processor-memory traffic of /spl Omega/(N/sup 3///spl radic/C), where N and C are the problem size and cache size, respectively. Experimental results show that this cache-oblivious implementation shows more than six times the improvement in real execution time over that of the iterative implementation with the usual row major data layout, on three state-of-the-art architectures. Second, we address Dijkstra's algorithm for the single-source shortest paths problem and Prim's algorithm for minimum spanning tree problem. For these algorithms, we demonstrate up to two times the improvement in real execution time by using a simple cache-friendly graph representation, namely adjacency arrays. Finally, we address the matching algorithm for bipartite graphs. We show performance improvements of two to three times in real execution time by using the technique of making the algorithm initially work on subproblems to generate a suboptimal solution and, then, solving the whole problem using the suboptimal solution as a starting point. Experimental results are shown for the Pentium III, UltraSPARC III, Alpha 21264, and MIPS R12000 machines.	analysis of algorithms;cache-oblivious algorithm;dijkstra's algorithm;file spanning;floyd–warshall algorithm;graph (abstract data type);graph theory;iterative method;list of algorithms;minimum spanning tree;optimizing compiler;prim's algorithm;r10000;run time (program lifecycle phase);shortest path problem;ultrasparc	Joon-Sang Park;Michael Penner;Viktor K. Prasanna	2002	IEEE Transactions on Parallel and Distributed Systems	10.1109/IPDPS.2002.1015509	recursion;routing;cache-oblivious algorithm;parallel computing;cache coloring;dijkstra's algorithm;floyd–warshall algorithm;computer science;graph theory;theoretical computer science;linear algebra;minimum spanning tree;distributed computing;graph;programming language;cache algorithms;transitive closure;tree;algorithm	HPC	2.7568878146316806	37.51334026441719	92450
cf90792fd59b096fc13a29e25a5c8ccef31148fc	gpu accelerated microarray data analysis using random matrix theory	biology computing;genomics;computer graphic equipment;inference mechanisms;matrix algebra;programmable logic arrays;graphics processing unit correlation eigenvalues and eigenfunctions symmetric matrices instruction sets computer architecture loading;coprocessors;random processes biology computing computer graphic equipment coprocessors data analysis genomics inference mechanisms matrix algebra parallel architectures parallel machines programmable logic arrays;data analysis;microarray data analysis;parallel architectures;random processes;pearson correlation matrix gpu accelerated microarray data analysis random matrix theory high throughput genomic technology gene expression data colossal data size inference mechanism transcription networks graphical processing units general purpose computing programmable parallel processor cpu automated microarray data analysis remote high performance computing facility lapack routine;parallel machines;random matrix theory	Recent advances in high-throughput genomic technology, such as micro arrays, usually produce vast amounts of gene expression data under many experimental conditions. Analyzing such data is often difficult due to the colossal data size and the intensive computing involved. In addition, many existing analysis tools often require the inference of experienced analysts and subjective judgments. In this paper, we developed a parallel approach based on Random Matrix Theory (RMT) to generate transcription networks using Graphical Processing Units (GPUs). Recently, GPUs have been redesigned into a more unified architecture, which has allowed them to be used more readily in general purpose computing. This architectural advancement has resulted in GPUs becoming easily programmable parallel processors with performance that is vastly superior to CPUs. Our GPU-based approach makes automated micro array data analysis faster, more accurate and noise resistant without engaging remote high performance computing facilities, such as a cluster or supercomputer. The implementation moves some computationally intensive tasks, such as the calculations of Pearson correlation coefficients, tridiagonal reduction, back transformation of eigenvectors, and orthogonal rotation, to the GPU. Experimental results on real micro array datasets show that our GPU implementation runs faster than a CPU version using highly optimized LAPACK routines. The runtime speedup gets higher as the number of genes and sample points in a micro array dataset increases.	cuda;central processing unit;cluster analysis;clustering coefficient;coefficient;computation;graphics processing unit;greedy algorithm;high-throughput computing;judgment (mathematical logic);lapack;microarray;speedup;supercomputer;throughput;time complexity;transcription (software);virtual economy	Joey Ingram;Mengxia Zhu	2011	2011 IEEE International Conference on High Performance Computing and Communications	10.1109/HPCC.2011.119	microarray analysis techniques;genomics;parallel computing;computer science;theoretical computer science;operating system;random matrix;distributed computing;data analysis;coprocessor	HPC	-2.006100732751631	41.326180706056036	92505
41bfd55ff7f311072cc9ea121bd0abbc3636b37c	efficient parallel strategy improvement for parity games	paper;cuda;package;nvidia;algorithms;logic in computer science;data structures and algorithms;computer science;nvidia geforce gtx 780	We study strategy improvement algorithms for solving parity games. While these algorithms are known to solve parity games using a very small number of iterations, experimental studies have found that a high step complexity causes them to perform poorly in practice. In this paper we seek to address this situation. Every iteration of the algorithm must compute a best response, and while the standard way of doing this uses the Bellman-Ford algorithm, we give experimental results that show that one-player strategy improvement significantly outperforms this technique in practice. We then study the best way to implement one-player strategy improvement, and we develop an efficient parallel algorithm for carrying out this task, by reducing the problem to computing prefix sums on a linked list. We report experimental results for these algorithms, and we find that a GPU implementation of this algorithm shows a significant speedup over single-core and multi-core CPU implementations.	bellman–ford algorithm;best practice;central processing unit;graphics processing unit;iteration;linked list;multi-core processor;parallel algorithm;parity bit;prefix sum;single-core;speedup	John Fearnley	2017		10.1007/978-3-319-63390-9_8	implementation;linked list;theoretical computer science;computer science;speedup;parallel algorithm;logic in computer science;best response;data structure;cuda	PL	1.028175640532869	40.189361247648954	92676
26fb8a8e73dd909c173dc4955eb655ef5a9e94ac	an intelligent bandwidth manager for cnn applications on embedded devices		Adapting complex Convolution Neural Network (CNN) applications on embedded processors is a challenge due to the massive memory bandwidth and computational requirements. In particular, the CNN memory bandwidth requirement poses a huge challenge for the processors with Scratch Pad Memory (SPM), usually of limited size. In this paper, we present an Intelligent Bandwidth Manager (IBWM) to efficiently handle the CNN bandwidth for SPM based processors. The proposed IBWM is a two fold approach which includes Intelligent SPM Manager (ISM) to optimize the number of accesses to SDRAM by analysing the data patterns, and Feature Map Compression (FMC) to further reduce the bandwidth by exploiting the feature map data sparsity. The IBWM is independent of any processor architecture and can be adopted in any processor with SPM. The proposed IBWM is experimented with ResNet-50 [1] and AlexNet [2] networks on a Samsung Reconfigurable Processor (SRP) [3] for various SPM sizes. The SDRAM bandwidth results show, 2x improvement compared to MIT Eyeriss [4] for AlexNet, and 4x-8x improvement compared to primitive bandwidth management techniques for AlexNet and ResNet-50. The proposed method achieves the bandwidth closer to the minimum possible bandwidth.		Sirish Kumar Pasupuleti;Aishwarya Rajaram;Narasinga Rao Miniskar;Raj Narayana Gadde;Deepanshu Yadvandu;Vasanthakumar Rajagopal;Ashok Vishnoi;Chandra Kumar Ramasamy	2018	2018 25th IEEE International Conference on Image Processing (ICIP)	10.1109/ICIP.2018.8451706	memory management;parallel computing;computer vision;convolutional neural network;artificial intelligence;memory bandwidth;computer science;frequency modulation;convolution;bandwidth management;microarchitecture;bandwidth (signal processing)	EDA	3.387930284549273	43.94666193531006	93139
11d6062b536a3e6dc4d6ee2b554a194db55fa401	end-to-end dnn training with block floating point arithmetic		The wide adoption of DNNs has given birth to unrelenting computing requirements, forcing datacenter operators to adopt domain-specific accelerators to train them. These accelerators typically employ densely packed full precision floating-point arithmetic to maximize performance per area. Ongoing research efforts seek to further increase that performance density by replacing floating-point with fixedpoint arithmetic. However, a significant roadblock for these attempts has been fixed point’s narrow dynamic range, which is insufficient for DNN training convergence. We identify block floating point (BFP) as a promising alternative representation since it exhibits wide dynamic range and enables the majority of DNN operations to be performed with fixed-point logic. Unfortunately, BFP alone introduces several limitations that preclude its direct applicability. In this work, we introduce HBFP, a hybrid BFP-FP approach, which performs all dot products in BFP and other operations in floating point. HBFP delivers the best of both worlds: the high accuracy of floating point at the superior hardware density of fixed point. For a wide variety of models we show that HBFP matches floating point’s accuracy while enabling hardware implementations that deliver up to 8.5× higher throughput.	computer memory;data center;dynamic range;fp (complexity);fixed point (mathematics);half-precision floating-point format;least fixed point;requirement;throughput	Mario Drumond;Tao Lin;Martin Jaggi;Babak Falsafi	2018	CoRR		tensor;block floating-point;operator (computer programming);computation;end-to-end principle;dot product;orders of magnitude (numbers);mathematics;server;arithmetic	ML	2.8229340559536236	43.29952979841781	93417
9d29945f15ce6706269bed9dfed1ad4c2acf26f5	high performance algorithms for structured matrix problems		The main aim of this volume is to summarize the state of the art in the area of high performance solutions of structured linear systems as well as the area of structured eigenvalue and singular-value problems. The volume highlights research directions perceived to be the most important for computing the structured problems. The topics covered range from parallel solvers for sparse or banded linear systems to parallel computation of eigenvalues and singular values of tridiagonal and bidiagonal matrices. In addition, the volume contains articles on specialized solution techniques for dense Toeplitz and Hankel matrices. The papers also discuss implementation issues on numerous parallel architectures such as vector computers, shared and distributed memory multiprocessors, and clusters of workstations.	algorithm	Laurence Tianruo Yang	1999	Scalable Computing: Practice and Experience		parallel computing;computer science;theoretical computer science;distributed computing	HPC	-2.4404125527240748	38.536535245913804	93436
c9d54fd7d6911a0ecec3dc1c9b37abd1873c3870	spintronic nano-devices for bio-inspired computing	bioinspired computing;magnetic tunnel junctions (mtjs);spintronics	1 — Bio-inspired hardware holds the promise of low-energy, intelligent and highly adaptable computing systems. Applications span from automatic classification for big data management, through unmanned vehicle control, to control for bio-medical prosthesis. However, one of the major challenges of fabricating bio-inspired hardware is building ultra-high density networks out of complex processing units interlinked by tunable connections. Nanometer-scale devices exploiting spin electronics (or spintronics) can be a key technology in this context. In particular, magnetic tunnel junctions are well suited for this purpose because of their multiple tunable functionalities. One such functionality, non-volatile memory, can provide massive embedded memory in unconventional circuits, thus escaping the von-Neumann bottleneck arising when memory and processors are located separately. Other features of spintronic devices that could be beneficial for bio-inspired computing include tunable fast non-linear dynamics, controlled stochasticity, and the ability of single devices to change functions in different operating conditions. Large networks of interacting spintronic nano-devices can have their interactions tuned to induce complex dynamics such as synchronization, chaos, soliton diffusion, phase transitions, criticality, and convergence to multiple metastable states. A number of groups have recently proposed bio-inspired architectures that include one or several types of spintronic nanodevices. In this article we show how spintronics can be used for bio-inspired computing. We review the different approaches that have been proposed, the recent advances in this direction, and the challenges towards fully integrated spintronics-CMOS (Complementary metal – oxide – semiconductor) bio-inspired hardware.	academy;algorithm;ambiguous name resolution;artificial neural network;big data;bio-inspired computing;british informatics olympiad;cmos;central processing unit;chaos theory;circuit design;cognitive computing;complex dynamics;dynamical system;embedded system;experiment;gnu nano;institut für unternehmenskybernetik;interaction;neuron;non-volatile memory;nonlinear system;nonvolatile bios memory;prospective search;self-organized criticality;semiconductor;soliton;spintronics;uncrewed vehicle;unmanned aerial vehicle;volatile memory;von neumann architecture	Julie Grollier;Damien Querlioz;Mark D. Stiles	2016	CoRR		electronic engineering;telecommunications;engineering;electrical engineering;nanotechnology	ML	5.2047263662759855	41.13840815805753	93600
ac9caa876f6f17dd7802447e061f4809f9f4731f	hardware accelerator for analytics of sparse data		Rapid growth of Internet led to web applications that produce large unstructured sparse datasets (e.g., texts, ratings). Machine learning (ML) algorithms are the basis for many important analytics workloads that extract knowledge from these datasets. This paper characterizes such workloads on a high-end server for real-world datasets and shows that a set of sparse matrix operations dominates runtime. Further, they run inefficiently due to low compute-per-byte and challenging thread scaling behavior. As such, we propose a hardware accelerator to perform these operations with extreme efficiency. Simulations and RTL synthesis to 14nm ASIC demonstrate significant performance and performance/Watt improvements over conventional processors, with only a small area overhead.	algorithm;application-specific integrated circuit;byte;central processing unit;computer simulation;hardware acceleration;machine learning;overhead (computing);performance per watt;server (computing);sparse matrix;web application	Eriko Nurvitadhi;Asit K. Mishra;Yu Wang;Ganesh Venkatesh;Debbie Marr	2016	2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)		embedded system;algorithm design;analytics;parallel computing;real-time computing;hardware acceleration;sparse matrix;computer science;theoretical computer science;operating system;programming language;server	EDA	-3.7131405089638934	45.55691528911969	93623
718b18913c49319ebdd61658b2d6dc060aed75c1	high performance and low power hardware implementation for cryptographic hash functions	thesis;m s in electrical engineering may 2013	Since hash functions are cryptography’s most widely used primitives, efficient hardware implementation of hash functions is of critical importance. The proposed high performance hardware implementation of the hash functions used sponge construction which generates desired length digest, considering two key design metrics: throughput and power consumption. Firstly, this paper introduces unfolding transformation which increases the throughput of hash function and pipelining and parallelism design techniques which reduce the delay. Secondly, we propose a frequency trade-off technique which can give us a scope of frequency value for making a trade-off between low dynamic power consumption and high throughput. Finally, we use load-enable based clock gating scheme to eliminate wasted toggle rate of signals in the idle mode of hash encryption system. We demonstrated the proposed design techniques by using 45 nm CMOS technology at 10MHz. The results show that we can achieve up to 47.97 times higher throughput, 6.31% delay reduction, and 13.65% dynamic power reduction.	cmos;clock gating;clock rate;critical path method;cryptographic hash function;cryptography;encryption;feature toggle;parallel computing;pipeline (computing);sponge function;throughput;unfolding (dsp implementation)	Yunlong Zhang;Joohee Kim;Ken Choi;Taeshik Shon	2014	IJDSN	10.1155/2014/736312	embedded system;security of cryptographic hash functions;parallel computing;real-time computing;hash function;computer science;theoretical computer science;swifft	EDA	9.7238181601347	45.61743959459164	93696
703789a9f8c2475c7f4892c7e9dc9deaddd7d982	fast graph partitioning and its application in sparse matrix ordering	graph partitioning;sparse matrix	Graph partitioning is a fundamental problem in several scientific and engineering applications, including task partitioning for parallel processing. In this paper, we describe heuristics that improve the state-of-the-art practical algorithms used in graph partitioning software in terms of both partitioning speed and quality. An important use of graph partitioning is in ordering sparse matrices for obtaining direct solutions to sparse systems of linear equations arising in engineering and optimization applications. The experiments reported in this paper show that the use of these heuristics results in a considerable improvement in the quality of sparse matrix orderings over conventional ordering methods. In addition, our graph-partitioning based ordering algorithm is more parallelizable than minimum-degree based orderings algorithms and it renders the ordered matrix more amenable to parallel factorization.	fast fourier transform;graph partition;sparse matrix	Fred G. Gustavson	1996		10.1007/3-540-62095-8_34	cuthill–mckee algorithm;graph energy;degree matrix;graph bandwidth;sparse matrix;computer science;graph partition;sparse approximation;adjacency matrix	HPC	-1.8740206596875912	38.30537912618491	93824
8987a55b751e9441f62fc2a393b2121ecd06fd38	arm+fpga platform to manage solid-state-smart transformer in smart grid application	smart solid state transformer;fpga;smart grid;arm processor;soc	This paper proposes a suitable digital platform based on the Xilinx Zynq®-7000 family to process the functions performed by a smart-solid-state-transformer (3ST) in smart grids (SG) application. These functions include: link to information and communication technologies (ICT), voltage transformation, integration of distributed renewable energy resources (DRER) and distributed energy storage devices (DESD). The Zynq platform embeds a double-core ARM® Cortex™-A9 processor and Field Programmable Gates Array (FPGA) technology, all within a programmable system on a chip (SoC). The main advantages of this technology are modularity, scalability, quick and easy maintenance and low-costs. Experimental results are included to show some capabilities of the proposed platform in a 3ST laboratory test-bed.	arm architecture;big data;distributed control system;embedded system;encryption;fault detection and isolation;field-programmable gate array;modularity (networks);portable c compiler;scalability;suicidegirls;system on a chip;testbed;transformer;voltage regulation	N. Nila-Olmedo;Fortino Mendoza-Mondragon;A. Espinosa-Calderon;Moreno	2016	2016 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2016.7857155	system on a chip;embedded system;computer hardware;computer science;smart grid;arm architecture;field-programmable gate array	EDA	2.1724425189855325	45.199000999255624	93829
103e213244d675b033c271281e6ce680ac0f52a8	feedforward-cutset-free pipelined multiply–accumulate unit for the machine learning accelerator		Multiply–accumulate (MAC) computations account for a large part of machine learning accelerator operations. The pipelined structure is usually adopted to improve the performance by reducing the length of critical paths. An increase in the number of flip-flops due to pipelining, however, generally results in significant area and power increase. A large number of flip-flops are often required to meet the feedforward-cutset rule. Based on the observation that this rule can be relaxed in machine learning applications, we propose a pipelining method that eliminates some of the flip-flops selectively. The simulation results show that the proposed MAC unit achieved a 20% energy saving and a 20% area reduction compared with the conventional pipelined MAC.		Sungju Ryu;Naebeom Park;Jae-Joon Kim	2019	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2018.2873716	fold (higher-order function);feed forward;% area reduction;computer science;computation;machine learning;pipeline (computing);artificial intelligence;adder	EDA	3.8869497759819556	43.08430005043942	94345
c487742baf1f706ae2f705bd2ff4da1f024a5380	hardware architecture and trade-offs for generic inversion of one-way functions	one way function;concurrent computing;hardware sorting concurrent computing signal processing computer architecture statistics table lookup pipeline processing parallel processing cryptography;sorting;parallel architectures cryptography;functions inversion;special purpose hardware;hardware architecture;one way functions;systematic architecture;computer architecture;parallel architectures;cryptography;signal processing;statistics;generalized inverse;time memory trade off;systematic architecture functions inversion time memory trade off one way functions special purpose hardware;table lookup;parallel processing;pipeline processing;hardware	Time-memory trade-off (TMTO) is a twenty five years old technique for inverting one-way functions. The most feasible implementation of TMTO is in special purpose hardware. Till date the work on hardware architecture for TMTO has been somewhat sketchy. In this paper, we describe a systematic architecture for implementing TMTO. We break down the offline and online phases into simpler tasks and identify opportunities for pipelining and parallelism. This results in a sufficiently detailed top-level architecture. To the best of our knowledge, such architecture does not appear in the literature	one-way function;online and offline;parallel computing;pipeline (computing)	Sourav Mukhopadhyay;Palash Sarkar	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693716	embedded system;parallel processing;computer architecture;parallel computing;concurrent computing;computer science;theoretical computer science;signal processing;hardware architecture;one-way function;algorithm;statistics	Arch	7.323371620426297	43.07269203510492	94437
bf0cc3fa14d214b370816ecd82e49b160af16e1c	accelerating large-scale hpc applications using fpgas	field programmable gate arrays convolution stacking optimization acceleration bandwidth kernel;optimisation;fpga;acceleration;acceleration fpga;large scale;data flow computing;digital arithmetic;field programmable gate arrays;optimisation data flow computing digital arithmetic field programmable gate arrays;ieee format large scale hpc application field programmable gate array glue logic cpu conventional machine fpga accelerator nonstandard arithmetic data flow programming model arithmetic operator	Field Programmable Gate Arrays (FPGAs) are conventionally considered as 'glue-logic'. However, modern FPGAs are extremely competitive compared to state-of-the-art CPUs for commercial HPC workloads, such as those found in Oil and Gas and Finance. For example, an FPGA accelerated system can be 31-37 times faster than an equivalently sized conventional machine, and consume 1/39 of the power. The key to achieving the best performance in FPGA accelerators, while maintaining correctness, is optimization of arithmetic units and data types to suit the range/precision at each point in the computation. The flexibility of the FPGA to implement non-standard arithmetic, combined with a data-flow programming model that instantiates a separate unit for each arithmetic operator in the code provides a wide design space. As such, FPGA computing offers significant opportunity for arithmetic research into 'large scale' HPC applications, where there is an opportunity to move away from standard IEEE formats, either to improve precision compared to the CPU version or to increase speed.	central processing unit;computation;correctness (computer science);dataflow;field-programmable gate array;mathematical optimization;programming model	Robert G. Dimond;Sébastien Racanière;Oliver Pell	2011	2011 IEEE 20th Symposium on Computer Arithmetic	10.1109/ARITH.2011.34	parallel computing;real-time computing;arbitrary-precision arithmetic;computer hardware;reconfigurable computing;programmable logic array;computer science;operating system;saturation arithmetic;field-programmable gate array	Arch	-0.7069010657631117	46.144072923355736	94612
c0424ea736fc53d73af8ea5a1c9b530ec9248735	parallel implementation of a machine learning algorithm on gpu		The capability for understanding data passes through the ability of producing an effective and fast classification of the information in a time frame that allows to keep and preserve the value of the information itself and its potential. Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. A powerful tool is provided by self-organizing maps (SOM). The goal of learning in the self-organizing map is to cause different parts of the network to respond similarly to certain input patterns. Because of its time complexity, often using this method is a critical challenge. In this paper we propose a parallel implementation for the SOM algorithm, using parallel processor architecture, as modern graphics processing units by CUDA. Experimental results show improvements in terms of execution time, with a promising speed up, compared to the CPU version and the widely used package SOM_PAK.	algorithm;cuda;central processing unit;computer graphics;graphics processing unit;linear algebra;machine learning;organizing (structure);parallel computing;run time (program lifecycle phase);self-organization;self-organizing map;speedup;time complexity	Salvatore Cuomo;Pasquale De Michele;Emanuel Di Nardo;Livia Marcellino	2017	International Journal of Parallel Programming	10.1007/s10766-017-0554-6	architecture;theoretical computer science;time complexity;parallel computing;speedup;graphics;cuda;computer science;central processing unit;parallel processing;machine learning;algorithm;artificial intelligence	ML	-2.3707775846730725	43.40380353750056	94628
62fe2a6a521d6ac59b4256494f32f830274c5bb4	gpu implementation of an audio fingerprints similarity search algorithm	spectrogram;shared memory systems audio signal processing graphics processing units;fast search audio fingerprint copy detection gpu cuda parallel;acceleration;computer architecture;fingerprint recognition;graphics processing units;intersection algorithm gpu implementation audio fingerprint similarity search algorithm audio fingerprinting system gpu memory spaces shared memory concurrent threads;graphics processing units fingerprint recognition instruction sets acceleration algorithm design and analysis spectrogram computer architecture;algorithm design and analysis;instruction sets	This paper describes a parallel implementation of a promising similarity search algorithm for an audio fingerprinting system. Efficient parallel implementation on a GPU accelerates the search on a dataset containing over 61 million audio fingerprints. The similarity between two fingerprints is defined as the intersection of their elements. We evaluate GPU implementations of two intersection algorithms for this dataset. We show that intelligent use of the GPU memory spaces (shared memory in particular) that maximizes the number of concurrent threads has a significant impact on the overall compute time when using fingerprints of varying dimensions. With simple modifications we obtain up to 4 times better GPU performance when using GPU memory to maximize concurrent threads. Compared to the CPU only implementations, the proposed GPU implementation reduces run times by up to 150 times for one intersection algorithm and by up to 379 times for the other intersection algorithm.	acoustic fingerprint;central processing unit;fingerprint (computing);graphics processing unit;intersection algorithm;search algorithm;shared memory;similarity search	Chahid Ouali;Pierre Dumouchel;Vishwa Gupta	2015	2015 13th International Workshop on Content-Based Multimedia Indexing (CBMI)	10.1109/CBMI.2015.7153625	acceleration;algorithm design;computer vision;parallel computing;real-time computing;computer science;theoretical computer science;spectrogram;instruction set;fingerprint recognition	HPC	-0.4609155289929304	41.55177592290951	94838
06ca344095945861931c25b0bc43a59e30b6b81d	a scalable pipeline for transcriptome profiling tasks with on-demand computing clouds	genomics;electronic mail;system recovery biology computing cloud computing data analysis parallel processing pipeline processing resource allocation rna;iaas transcriptome profiling tasks on demand computing clouds scalable data analytics amazon ec2 parallel computing optimal computing resource allocation time to completion reduction ttc reduction cost reduction failure avoidance radical pilot system distributed heterogeneous computing resources advanced dynamically adaptive execution workflows transcript assembly accuracy evaluation de novo assembler multiassembler multiparameter methods scalable pipeline large scale rna seq data analysis infrastructure as a service;rna seq;cloud;sequential analysis;computing;scalable;data analysis;big data;pipelines;big data scalable pipeline cloud rna seq data analysis infrastructure computing rnnotator;rnnotator;pipelines cloud computing sequential analysis bioinformatics data analysis genomics electronic mail;pipeline;infrastructure;cloud computing;bioinformatics	We introduce a pilot-based approach with which scalable data analytics essential for a large RNA-seq data set are efficiently carried out. Major development mechanisms, designed in order to achieve the required scalability, in particular, targeting cloud environments with on-demand computing, are presented. With an example of Amazon EC2, by harnessing distributed and parallel computing implementations, our pipeline is able to allocate optimally computing resources to tasks of a target workflow in an efficient manner. Consequently, decreasing time-to-completion (TTC) or cost, avoiding failures due to a limited resource of a single node, and enabling scalable data analysis with multiple options can be achieved. Our developed pipeline benefits from the underlying pilot system, Radical Pilot, being readily amenable to scalable solutions over distributed heterogeneous computing resources and suitable for advanced workflows of dynamically adaptive executions. In order to provide insights on such features, benchmark experiments, using two real data sets, were carried out. The benchmark experiments focus on the most computationally expensive transcript assembly step. Evaluation and comparison of transcript assembly accuracy using a single de novo assembler or the combination of multiple assemblers are also presented, underscoring its potential as a platform to support multi-assembler multi-parameter methods or ensemble methods which are statistically attractive and easily feasible with our scalable pipeline. The developed pipeline, as manifested by results presented in this work, is built upon effective strategies that address major challenging issues and viable solutions toward an integrative and scalable method for large-scale RNA-seq data analysis, particularly maximizing merits of Infrastructure as a Service (IaaS) clouds	amazon elastic compute cloud (ec2);analysis of algorithms;assembly language;benchmark (computing);cloud computing;de novo transcriptome assembly;ensemble learning;experiment;heterogeneous computing;parallel computing;pipeline pilot;scalability;utility computing	Shayan Shams;Nayong Kim;Xiandong Meng;Ming Tai Ha;Shantenu Jha;Zhong Wang;Joohyun Kim	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.129	genomics;parallel computing;real-time computing;big data;cloud computing;computer science;bioinformatics;operating system;data mining;distributed computing	HPC	-3.423841517453377	44.47560512530747	94992
328555a973d14d0328d8db660b77d8f0668c73bd	randomized motion planning on parallel and distributed architectures	parallel architectures path planning parallel programming virtual reality;processing element;electrical capacitance tomography;high dimensionality;identity based encryption;application software;search space;path planning;degree of freedom;virtual reality;parallel programming;robotics;kinematics;computer applications;parallel architectures;complex system;motion planning;parallel machines;search problems;parallel implementation;parallel architecture;parallel machine;search problems motion planning robotics automation virtual reality parallel architecture sgi onyx2 parallel machine;path planning motion planning electrical capacitance tomography application software automation identity based encryption kinematics aircraft virtual reality computer applications;aircraft;distributed architecture;sgi onyx2;automation	Motion planning is a fundamental problem in a number of application areas, including robotics, automation, and virtual reality. This paper describes a parallel implementation of a motion planning algorithm particularly suited for complex systems characterized by many degrees of freedom. The implementation is based on the concurrent exploration of the search space by a randomized planner replicated on each node of the parallel architecture. All processing elements compete to obtain a solution over the entire search space in an OR-parallel fashion. Reported results refer to a low-cost cluster of PCs and an SGI Onyx2 parallel machine. The experiments emphasize the effectiveness of the approach for complex, high-dimensionality planning problems. We believe that the approach may be useful in other complex search problems, especially when the parallel architecture exhibits relatively high communication latency.	automated planning and scheduling;central processing unit;complex systems;experiment;mobile robot;motion planning;non-uniform memory access;parallel computing;randomized algorithm;robotics;sgi onyx2;virtual reality	Stefano Caselli;Monica Reggiani	1999		10.1109/EMPDP.1999.746692	simulation;computer science;theoretical computer science;distributed computing	Robotics	-1.2649628660736028	35.040384115551234	95333
57ecd3f1567bb5348d7d00bda43f640b9418a87d	circa-gpus: increasing instruction reuse through inexact computing in gp-gpus	microarchitecture;energy efficiency instruction reuse spatiotemporal matching approximate computing parallelism gp gpus simd;parallel processing energy conservation graphics processing units;computer architecture;graphics processing units;spatiotemporal phenomena computer architecture approximation methods parallel processing graphics processing units microarchitecture context modeling;spatiotemporal phenomena;approximation methods;context modeling;parallel processing;temporal instruction reuse circa gpu instruction reuse inexact computing gp gpu architecture fine grained parallelism approximate computing energy efficiency spatial instruction reuse	The authors introduce a method that exploits fine-grained parallelism and approximate computing in GP-GPU architecture to increase the energy efficiency through spatial and temporal reuse of instructions.	amd radeon rx 200 series;approximate computing;approximation algorithm;exploit (computer security);graphics processing unit;linear programming relaxation;parallel computing;radeon hd 7000 series;scalability	Abbas Rahimi;Luca Benini;Rajesh K. Gupta	2016	IEEE Design & Test	10.1109/MDAT.2015.2497334	parallel processing;computer architecture;parallel computing;microarchitecture;computer science;theoretical computer science;context model	Arch	-2.104561247703843	44.883023986863996	95689
eb6a1cbae9bd4dfcb853cf7904f86d7d3dc62d2c	size oblivious programming with infinimem		Many recently proposed BigData processing frameworks make programming easier, but typically expect the datasets to fit in the memory of either a single multicore machine or a cluster of multicore machines. When this assumption does not hold, these frameworks fail. We introduce the InfiniMem framework that enables size oblivious processing of large collections of objects that do not fit in memory by making them disk-resident. InfiniMem is easy to program with: the user just indicates the large collections of objects that are to be made disk-resident, while InfiniMem transparently handles their I/O management. The InfiniMem library can manage a very large number of objects in a uniform manner, even though the objects have di↵erent characteristics and relationships which, when processed, give rise to a wide range of access patterns requiring di↵erent organizations of data on the disk. We demonstrate the ease of programming and versatility of InfiniMem with 3 di↵erent probabilistic analytics algorithms, 3 di↵erent graph processing size oblivious frameworks; they require minimal e↵ort, 6–9 additional lines of code. We show that InfiniMem can successfully generate a mesh with 7.5 million nodes and 300 million edges (4.5 GB on disk) in 40 minutes and it performs the PageRank computation on a 14GB graph with 134 million vertices and 805 million edges at 14 minutes per iteration on an 8-core machine with 8 GB RAM. Many graph generators and processing frameworks cannot handle such large graphs. We also exploit InfiniMem on a cluster to scale-up an object-based DSM.	algorithm;big data;computation;experiment;graph (abstract data type);information;input/output;iteration;multi-core processor;object-based language;open road tolling;pagerank;parallel computing;random-access memory;simple data format;source lines of code;vertex (geometry)	Sai Charan Koduru;Rajiv Gupta;Iulian Neamtiu	2015		10.1007/978-3-319-29778-1_1	parallel computing;computer science;theoretical computer science;distributed computing;programming language	DB	-4.030838623996728	43.13049893681329	95719
d287a2b52b38d323115e4e74234a21788d6147dc	a cryptosystem based on palmprint feature	databases;image recognition;false reject rate;error correction codes;image coding;gaussian processes;biometrics access control;false rejection rate palmprint feature biometric cryptography biometric features data encryption biometric cryptosystem gaussian derivative filters reed solomon error correcting technique logical xor operation data decryption;reed solomon codes;data encryption;palmprint feature;data decryption;cryptography biometrics fingerprint recognition data security information security iris filters image databases spatial databases computer science;gaussian derivative filters;biometric features;cryptography;error correction;feature extraction;pixel;biometric cryptography;reed solomon;false rejection rate;face;reed solomon error correcting technique;logical xor operation;security;biometric cryptosystem;filtering theory;reed solomon codes biometrics access control cryptography error correction codes feature extraction filtering theory gaussian processes image coding image recognition	Biometric cryptography is a technique using biometric features to encrypt data, which can improve the security of the encrypted data and overcome the shortcomings of the traditional cryptography. This paper proposes a novel biometric cryptosystem based on palmprint features. In this system, the palmprint features, called DoG code, are extracted using Gaussian derivative filters. Then the Reed-Solomon error correcting technique and the logical XOR operation are employed to encrypt and decrypt the data. Experimental results show that this system can obtain a high security with a low false rejection rate.	biometrics;code;cryptography;cryptosystem;exclusive or;fingerprint;reed–solomon error correction;rejection sampling	Xiangqian Wu;Kuanquan Wang;David Zhang	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761117	arithmetic;face;feature extraction;computer science;cryptography;information security;theoretical computer science;computer security;pixel;statistics	EDA	7.508630550970848	34.28242203858033	95927
7efb5969a8b2eb7c89e0ef9008bac3392452783c	brief announcement: hypergraph partitioning for parallel sparse matrix-matrix multiplication	sparse matrix matrix multiplication;mathematics and computing;communication costs;communication lower bounds	The performance of parallel algorithms for sparse matrix-matrix multiplication is typically determined by the amount of interprocessor communication performed, which in turn depends on the nonzero structure of the input matrices. In this paper, we characterize the communication cost of a sparse matrix-matrix multiplication algorithm in terms of the size of a cut of an associated hypergraph that encodes the computation for a given input nonzero structure. Obtaining an optimal algorithm corresponds to solving a hypergraph partitioning problem. Our hypergraph model generalizes several existing models for sparse matrix-vector multiplication, and we can leverage hypergraph partitioners developed for that computation to improve application-specific algorithms for multiplying sparse matrices.	computation;graph partition;inter-process communication;matrix multiplication algorithm;parallel algorithm;partition problem;sparse matrix	Grey Ballard;Alex Druinsky;Nicholas Knight;Oded Schwartz	2015		10.1145/2755573.2755613	parallel computing;theoretical computer science;sparse approximation	HPC	-1.193500264048856	38.38621197866156	96428
5f51e25a9cc2f5328c5a83a2a4ff4286427159da	plx: an instruction set architecture and testbed for multimedia information processing	processor architecture;software tool;parallel algorithm;multimedia;median filter;isa;instruction set architecture;multimedia systems;discrete cosine transform;low power;media processing;compiler optimization;information processing;very long instruction word;cost effectiveness;parallel architecture;high performance	PLX is a concise instruction set architecture (ISA) that combines the most useful features from previous generations of multimedia instruction sets with newer ISA features for high-performance, low-cost multimedia information processing. Unlike previous multimedia instruction sets, PLX is not added onto a base processor ISA, but designed from the beginning as a standalone processor architecture optimized for media processing. Its design goals are high performance multimedia processing, general-purpose programmability to support an ever-growing range of applications, simplicity for constrained environments where low power and low cost are paramount, and scalability for higher performance in less constrained multimedia systems. Another design goal of PLX is to facilitate exploration and evaluation of novel techniques in instruction set architecture, microarchitecture, arithmetic, VLSI implementations, compiler optimizations, and parallel algorithm design for new computing paradigms. Key characteristics of PLX are a fully subword-parallel architecture with novel features like wordsize scalability from 32-bit to 128-bit words, a new definition of predication, and an innovative set of subword permutation instructions. We demonstrate the use and high performance of PLX on some frequently-used code kernels selected from image, video, and graphics processing applications: discrete cosine transform, pixel padding, clip test, and median filter. Our results show that a 64-bit PLX processor achieves significant speedups over a basic 64-bit RISC processor and over IA-32 processors with MMX and SSE multimedia extensions. Using PLX’s wordsize scalability feature, PLX-128 often provides an additional 2× speedup over PLX-64 in a cost-effective way. Superscalar or VLIW (Very Long Instruction Word) PLX implementations can also add additional performance through interinstruction, rather than intra-instruction parallelism. We also describe the PLX testbed and its software tools for architecture and related research.	128-bit;32-bit;64-bit computing;algorithm design;arithmetic logic unit;assembly language;central processing unit;circuit design;computer graphics;cryptography;data security;design space exploration;discrete cosine transform;execution unit;general-purpose markup language;handheld game console;hands-on computing;ia-32;image scaling;information processing;logic gate;low-power broadcasting;mmx (instruction set);median filter;microarchitecture;optimizing compiler;parallel algorithm;parallel computing;pixel;processor register;program optimization;scalability;simulation;sorting;speedup;standard cell;streaming simd extensions;substring;superscalar processor;testbed;very long instruction word;windows 3.0	Ruby B. Lee;A. Murat Fiskiran	2005	VLSI Signal Processing	10.1007/s11265-005-4940-8	median filter;embedded system;computer architecture;parallel computing;real-time computing;cost-effectiveness analysis;information processing;industry standard architecture;microarchitecture;computer science;very long instruction word;operating system;discrete cosine transform;instruction set;optimizing compiler;parallel algorithm;programming language	Arch	6.773710397180168	45.199985225784985	96563
7de19523aa53234b85b5ea8bccabab9183b662d8	bigger buffer k-d trees on multi-many-core systems	paper;nvidia geforce gtx titan z;package;kd tree;nvidia;nearest neighbour;computer science;opencl	A buffer k-d tree is a k-d tree variant for massively-parallel nearest neighbor search. While providing valuable speed-ups on modern many-core devices in case both a large number of reference and query points are given, buffer k-d trees are limited by the amount of points that can fit on a single device. In this work, we show how to modify the original data structure and the associated workflow to make the overall approach capable of dealing with massive data sets. We further provide a simple yet efficient way of using multiple devices given in a single workstation. The applicability of the modified framework is demonstrated in the context of astronomy, a field that is faced with huge amounts of data.	airline control program (acp);commodity computing;data structure;experiment;forward error correction;ibm notes;job control (unix);machine learning;manycore processor;nearest neighbor search;overhead (computing);workstation;z-buffering	Fabian Gieseke;Cosmin E. Oancea;Ashish Mahabal;Christian Igel;Tom Heskes	2015	CoRR		ball tree;parallel computing;computer science;theoretical computer science;operating system;k-d tree;distributed computing;cover tree;programming language;package;computer graphics (images)	ML	-3.6227680050753395	43.12110804075445	96637
6256b2c041704387bc682f252a7ad1a43edd23fe	efficient implementation of estream ciphers on 8-bit avr microcontrollers	aes stream cipher software performance avr microcontroller embedded security estream dragon hc 128 lex salsa20 salsa20 12 sosemanuk;microprocessors;microcontrollers;random access memory;private key cryptography;embedded security;intellectual property;application software;secret keystream generation;secret keystream generation estream cipher implementation memory constrained 8 bit avr microcontroller intellectual property right aes block cipher c code implementation embedded system dragon lex salsa20 salsa20 12 sosemanuk encryption speed flash storage sram storage;block cipher;aes;portfolios;embedded system;software performance;embedded systems;sosemanuk;flash storage;stream cipher;efficient implementation;cryptography;sram storage;aes block cipher;intellectual property right;salsa20;dragon;encryption speed;microcontrollers cryptography security portfolios throughput application software microprocessors intellectual property embedded system random access memory;c code implementation;sram chips embedded systems flash memories microcontrollers private key cryptography;lex;avr microcontroller;memory constrained 8 bit avr microcontroller;high throughput;security;salsa20 12;estream cipher implementation;hc 128;flash memories;throughput;estream;sram chips	This work is motivated by the question of how efficient modern stream ciphers in the eSTREAM project (Profile I) can be implemented on small embedded microcontrollers that are also constrained in memory resources. In response to this question, we present the first implementation results for Dragon, HC-128, LEX, Salsa20, Salsa20/12, and Sosemanuk on 8-bit microcontrollers. These ciphers are definitively free for any use, i.e., their use is not covered by intellectual property rights. For the evaluation process, we follow a two-stage approach and compare with efficient implementations of the AES block cipher. First, the C code implementation provided by the cipherspsila designers was ported to an 8-bit AVR microcontroller and the suitability of these stream ciphers for the use in embedded systems was assessed. In the second stage we implemented Dragon, LEX, Salsa20, Salsa20/12, and Sosemanuk in assembler to tap the full potential of an embedded implementation. Our efficiency metrics are memory usage in flash and SRAM and performance of keystream generation, key setup, and IV setup. Regarding encryption speed, all stream ciphers except for Salsa20 turned out to outperform AES. In terms of memory needs, Salsa20, Salsa20/12, and LEX are almost as compact as AES. In view of the final eSTREAM portfolio (Profile I), Salsa20/12 is the only promising alternative for the AES cipher on memory constrained 8-bit embedded microcontrollers. For embedded applications with high throughput requirements, Sosemanuk is the most suitable cipher if its considerable higher memory needs can be tolerated.	8-bit;assembly language;atmel avr;block cipher;estream;embedded system;encryption;hc-256;lex (software);microcontroller;requirement;sosemanuk;static random-access memory;stream cipher;throughput	Gordon Meiser;Thomas Eisenbarth;Kerstin Lemke-Rust;Christof Paar	2008	2008 International Symposium on Industrial Embedded Systems	10.1109/SIES.2008.4577681	embedded system;parallel computing;real-time computing;computer hardware;computer science;information security;operating system;key schedule;intellectual property	Embedded	8.122683146649944	45.40920369335754	96725
7a23bccf3337434d78a91d1a9700828cafb3a48e	structured weight matrices-based hardware accelerators in deep neural networks: fpgas and asics		Both industry and academia have extensively investigated hardware accelerations. To address the demands in increasing computational capability and memory requirement, in this work, we propose the structured weight matrices (SWM)-based compression technique for both Field Programmable Gate Array (FPGA) and application-specific integrated circuit (ASIC) implementations. In the algorithm part, the SWM-based framework adopts block-circulant matrices to achieve a fine-grained tradeoff between accuracy and compression ratio. The SWM-based technique can reduce computational complexity from O( n 2 ) to O( n log n ) and storage complexity from O( n 2 ) to O( n ) for each layer and both training and inference phases. For FPGA implementations on deep convolutional neural networks (DCNNs), we achieve at least 152X and 72X improvement in performance and energy efficiency, respectively using the SWM-based framework, compared with the baseline of IBM TrueNorth processor under same accuracy constraints using the data set of MNIST, SVHN, and CIFAR-10. For FPGA implementations on long short term memory (LSTM) networks, the proposed SWM-based LSTM can achieve up to 21X enhancement in performance and 33.5X gains in energy efficiency compared with the ESE accelerator. For ASIC implementations, the proposed SWM-based ASIC design exhibits impressive advantages in terms of power, throughput, and energy efficiency. Experimental results indicate that this method is greatly suitable for applying DNNs onto both FPGAs and mobile/IoT devices.	algorithm;application-specific integrated circuit;artificial neural network;baseline (configuration management);circulant matrix;computation;computational complexity theory;convolutional neural network;deep learning;extensible storage engine;field-programmable gate array;long short-term memory;mnist database;throughput;truenorth	Caiwen Ding;Ao Ren;Geng Yuan;Xiaolong Ma;Jiayu Li;Ning Liu;Bo Yuan;Yanzhi Wang	2018		10.1145/3194554.3194625	throughput;electronic engineering;convolutional neural network;application-specific integrated circuit;field-programmable gate array;computer science;time complexity;computer hardware;deep learning;truenorth;artificial intelligence;mnist database	EDA	3.8598784014095124	42.87180390913304	96819
26cee0f387549bbfc5a81693e362f59e66ba7494	efficient hardware implementation of modular multiplication and exponentiation for public-key cryptography	public key cryptography;systolic array;public key cryptosystem;left to right;modular exponentiation;hardware implementation;modular multiplication	Modular multiplication and modular exponentiation are fundamental operations in most public-key cryptosystems such as RSA and DSS. In this paper, we propose a novel implementation of these operations using systolic arrays based architectures. For this purpose, we use the Montgomery algorithm to compute the modular product and the left-to-right binary exponentiation method to yield the modular power. Our implementation improves time requirement as well as the time × area factor when compared that of Blums and Paars.	public-key cryptography	Nadia Nedjah;Luiza de Macedo Mourelle	2002		10.1007/3-540-36569-9_30	modular arithmetic;parallel computing;kochanski multiplication;systolic array;computer science;theoretical computer science;public-key cryptography;algorithm;modular exponentiation	Crypto	9.262384859775317	44.0995757020437	97019
8fe09c5c88c3dd0700051c48fd917ac480436a4c	privacy-preserving ecg classification with branching programs and neural networks	secure multiparty computation techniques;cryptographic primitives;protocols;decision tree;neural networks;signal classification communication complexity cryptography data privacy decision trees electrocardiography linear programming medical signal processing neural nets;neural nets;secure multi party computation;semihonest model privacy preserving ecg classification electrocardiogram linear branching programs neural networks biomedical signal processing secure multiparty computation techniques privacy preserving automatic diagnosis system decision tree fixed point arithmetic cryptographic primitives computational complexity perspectives communication complexity perspectives;communication complexity;quadratic discriminant function;privacy preservation;secure biomedical systems;indexing terms;discriminant function;privacy protection;semihonest model;accuracy;artificial neural networks;electrocardiography;data privacy;biomedical system;cryptography;signal processing;privacy preserving automatic diagnosis system;signal classification;branching program;secure electrocardiogram ecg classification linear branching programs neural networks nns privacy protection quadratic discriminant function secure biomedical systems;biomedical signal processing;artificial neural networks electrocardiography protocols neurons accuracy cryptography signal processing algorithms;linear programming;fixed point arithmetic;floating point;neural networks nns;computational complexity perspectives;neurons;secure electrocardiogram ecg classification;signal processing algorithms;decision trees;linear branching programs;medical signal processing;communication complexity perspectives;privacy preserving ecg classification;artificial neural network;electrocardiogram;neural network	Privacy protection is a crucial problem in many biomedical signal processing applications. For this reason, particular attention has been given to the use of secure multiparty computation techniques for processing biomedical signals, whereby nontrusted parties are able to manipulate the signals although they are encrypted. This paper focuses on the development of a privacy preserving automatic diagnosis system whereby a remote server classifies a biomedical signal provided by the client without getting any information about the signal itself and the final result of the classification. Specifically, we present and compare two methods for the secure classification of electrocardiogram (ECG) signals: the former based on linear branching programs (a particular kind of decision tree) and the latter relying on neural networks. The paper deals with all the requirements and difficulties related to working with data that must stay encrypted during all the computation steps, including the necessity of working with fixed point arithmetic with no truncation while guaranteeing the same performance of a floating point implementation in the plain domain. A highly efficient version of the underlying cryptographic primitives is used, ensuring a good efficiency of the two proposed methods, from both a communication and computational complexity perspectives. The proposed systems prove that carrying out complex tasks like ECG classification in the encrypted domain efficiently is indeed possible in the semihonest model, paving the way to interesting future applications wherein privacy of signal owners is protected by applying high security standards.	artificial neural network;binary decision diagram;computational complexity theory;cryptographic primitive;cryptography;decision tree;encryption;fixed point (mathematics);fixed-point arithmetic;privacy;requirement;secure multi-party computation;server (computing);signal processing;truncation	Mauro Barni;Pierluigi Failla;Riccardo Lazzeretti;Ahmad-Reza Sadeghi;Thomas Schneider	2011	IEEE Transactions on Information Forensics and Security	10.1109/TIFS.2011.2108650	computer science;theoretical computer science;machine learning;decision tree;data mining;computer security;artificial neural network;algorithm	Security	7.066495392365004	34.98948387349876	97301
58ccc404a490d9a45a4e8a988345ea95398ba3d1	local interpolation-based polar format sar: algorithm, hardware implementation and design automation	chip generator;logic in memory;synthetic aperture radar	In this paper we present a local interpolation-based variant of the well-known polar format algorithm used for synthetic aperture radar (SAR) image formation. We develop the algorithm to match the capabilities of the applicationspecific logic-in-memory processing paradigm, which offloads lightweight computation directly into the SRAM and DRAM. Our proposed algorithm performs filtering, an image perspective transformation, and a local 2D interpolation and supports partial and low-resolution reconstruction. We implement our customized SAR grid interpolation logic-inmemory hardware in advanced 14nm silicon technology. Our high-level design tools allow to instantiate various optimized design choices to fit image processing and hardware needs of application designers. Our simulation results show that the logic-in-memory approach has the potential to enable substantial improvements in energy efficiency without sacrificing image quality.	algorithm;aperture (software);automation;computation;dynamic random-access memory;fast fourier transform;high- and low-level;image formation;image processing;image quality;in-memory database;in-memory processing;integrated circuit design;interpolation;iterative reconstruction;level design;programming paradigm;semiconductor research corporation;simulation;static random-access memory;synthetic data;toolchain;whole earth 'lectronic link	Qiuling Zhu;Christian R. Berger;Eric L. Turner;Lawrence T. Pileggi;Franz Franchetti	2013	Signal Processing Systems	10.1007/s11265-012-0720-4	embedded system;computer vision;parallel computing;synthetic aperture radar;computer science;theoretical computer science	EDA	-1.4134618189184462	44.69050736460759	97467
4ba42eca01485cfe2ae165c20e3f180a0d05d42b	survey on run-to-run control algorithms in high-mix semiconductor manufacturing processes	high mix production run to run control exponentially weighted moving average ewma threaded ewma non threaded ewma;state estimation;manufacturing processes;context informatics manufacturing processes process control instruction sets state estimation;process control;semiconductor technology manufacturing processes process control semiconductor industry;informatics;process control communities run to run control algorithms high mix semiconductor manufacturing process product quality r2r control algorithms real world operating conditions high mix production environments;context;instruction sets	Recently, to ensure product quality in high-mix semiconductor manufacturing processes, run-to-run (R2R) control algorithms have received increasing attention. Difficult challenges have been met with new and advanced approaches that address the real-world operating conditions exhibited in high-mix production environments (e.g., different products with different frequencies and high numbers of threads). In this paper, various proposed controllers for high-mix semiconductor manufacturing processes are surveyed from an application and theoretical point of view. Remaining challenges and directions for future work are also summarized with the intent of drawing attention to these problems in the systems and process control communities.	algorithm;semiconductor device fabrication	Fei Tan;Tianhong Pan;Zhengming Li;Shan Chen	2015	IEEE Transactions on Industrial Informatics	10.1109/TII.2015.2490039	embedded system;process development execution system;computer science;engineering;industrial engineering;operating system;instruction set;process control;control theory;computer-integrated manufacturing;informatics;manufacturing engineering	Robotics	5.316711429725041	35.80647198283871	97632
6dbbbb18993e7a1d123dc8f43964d3706d3e3811	towards a parallel disk-based algorithm for multilevel k-way hypergraph partitioning	graph theory;disc storage parallel algorithms graph theory storage management;disc storage;storage management;graph partitioning;state of the art parallel graph partitioning tool parallel disk based algorithm multilevel k way hypergraph partitioning algorithm parallel coarsening sequential partitioning parallel refinement memory minimisation;partitioning algorithms delay concurrent computing workstations very large scale integration sparse matrices heuristic algorithms runtime clustering algorithms educational institutions;hypergraph partitioning;parallel algorithms	Summary form only given. Here we present a disk-based parallel formulation of the multilevel k-way hypergraph partitioning algorithm. This algorithm provides the capability to partition very large hypergraphs that hitherto could not be partitioned since the memory required exceeds that available on a single workstation. The algorithm has three main phases: parallel coarsening, sequential partitioning of the coarsest hypergraph and parallel refinement. At each parallel coarsening and refinement step disk is used to minimise memory usage. We apply the algorithm to very large hypergraphs with /spl Theta/(10/sup 7 /) vertices from the domain of performance modelling and show that the partitioning quality is approximately 20% better in terms of the (k - 1) metric than approximate partitionings produced by a state-of-the-art parallel graph partitioning tool.	approximation algorithm;graph partition;refinement (computing);sequential consistency;workstation	Aleksandar Trifunovic;William J. Knottenbelt	2004	18th International Parallel and Distributed Processing Symposium, 2004. Proceedings.	10.1109/IPDPS.2004.1303286	parallel computing;computer science;graph partition;graph theory;theoretical computer science;distributed computing;parallel algorithm	HPC	1.559836343163688	37.491648106709654	97752
759a73e40460ffda58b8b6953d66c06f2f7dc0fd	encryption and data dependent permutations: implementation cost and performance evaluation	block ciphering;cryptage bloc;field programmable gate array;evaluation performance;performance evaluation;encryption;implementation;block cipher;evaluacion prestacion;circuit vlsi;cifrado;red puerta programable;reseau porte programmable;permutation;vlsi circuit;cryptage;data dependence;criptografia;cryptography;permutacion;data transformation;cifrado en bloque;cryptographie;procesador oleoducto;circuito vlsi;processeur pipeline;implementacion;high speed;hardware implementation;pipeline processor	Recently, Data Dependent Permutations (DDP) have attracted the interest of cryptographers and ciphers designers. SPECTR-H64 and CIKS-1 are latest published powerful encryption algorithms, based on DDP transformations. In this paper, the implementation cost in different hardware devices (FPGA and ASIC) for DDP is introduced. In addition, the performance of these data transformation components is presented. Detailed analysis is shown, in terms of covered area, frequency, and throughput for DDP VLSI integration. Furthermore, two different architectures for hardware implementation of CIKS-1 and SPECTR-H64 are proposed. The first, based on full rolling technique minimizes the area resources. The second uses a pipelined development design and has high-speed performance. Both architectures have been implemented in FPGA and ASIC devices.	encryption;performance evaluation	Nicolas Sklavos;Alexander A. Moldovyan;Odysseas G. Koufopavlou	2003		10.1007/978-3-540-45215-7_29	embedded system;block cipher;parallel computing;computer science;cryptography;theoretical computer science;operating system;permutation;implementation;data transformation;computer security;encryption;algorithm;statistics	HPC	8.160990065265912	44.17872087303804	97941
fde04b7b524045969614382529bb1d6cc88efcf0	swaphi-ls: smith-waterman algorithm on xeon phi coprocessors for long dna sequences	kernel;buffer storage;vectors computational modeling instruction sets computer architecture kernel parallel processing buffer storage;parallel algorithms bioinformatics c language coprocessors dna multiprocessing systems multi threading;computer architecture;smith waterman algorithm on xeon phi coprocessors for long dna sequences swaphi ls parallel sw algorithm instruction level parallelism single instruction multiple data simd instructions vectorization thread level parallelism many cores multithreading device level parallelism xeon phi clusters distributed computing mpi offload model genome sequences nucleotides c openmp source code boinformatics;computational modeling;vectors;parallel processing;instruction sets	As an optimal method for sequence alignment, the Smith-Waterman (SW) algorithm is widely used. Unfortunately, this algorithm is computationally demanding, especially for long sequences. This has motivated the investigation of its acceleration on a variety of high-performance computing platforms. However, most work in the literature is only suitable for short sequences. In this paper, we present SWAPHI-LS, the first parallel SW algorithm exploiting emerging Xeon Phi coprocessors to accelerate the alignment of long DNA sequences. In SWAPHI-LS, we have investigated three parallelization approaches (naïve, tiled, and distributed) in order to deeply explore the inherent parallelism within Xeon Phis. To achieve high speed, we have explored two levels of parallelism within a single Xeon Phi and one more level of parallelism between Xeon Phis. Within a single Xeon Phi we exploit instruction-level parallelism within 512-bit single instruction multiple data (SIMD) instructions (vectorization) as well as thread-level parallelism over the many cores (multi-threading). Between Xeon Phis we employ device-level parallelism in order to harness the compute power of Xeon Phi clusters (distributed computing based on the MPI offload model). The performance of our algorithm has been evaluated using a variety of genome sequences of lengths ranging from 4.4 million to 50 million nucleotides. Our performance evaluation reveals that our implementation achieves a stable performance of up to 30.1 billion cell updates per second (GCUPS) on a single Xeon Phi and up to 111.4 GCUPS on four Xeon Phis sharing the same host. SWAPHI-LS is written in C++ (with a set of SIMD intrinsics), OpenMP and MPI. The source code is publicly available at http://swaphi-ls.sourceforge.net.	align (company);automatic vectorization;backtracking;benchmark (computing);c++;cuda;central processing unit;clock rate;coprocessor;distributed computing;graphics processing unit;instruction-level parallelism;intrinsic function;kepler (microarchitecture);least squares;message passing interface;openmp;parallel computing;performance evaluation;simd;sequence alignment;shattered world;smith–waterman algorithm;sourceforge;speedup;supercomputer;task parallelism;thread (computing);xeon phi	Yongchao Liu;Tuan Tu Tran;Felix Lauenroth;Bertil Schmidt	2014	2014 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2014.6968772	parallel processing;computer architecture;parallel computing;kernel;computer hardware;computer science;operating system;instruction set;data parallelism;computational model;instruction-level parallelism;task parallelism	HPC	-1.3342440308217685	42.94629797606009	98066
7d10d4bbf82d6845fd7ff0d1b2c2e98d9b692e4b	support vector machine acceleration for intel xeon phi manycore processors		Support vector machines are widely used for classification and regression tasks. However, sequential implementations for support vector machines are usually unable to deal with the increasing size of current real-world learning problems. In this context, Intel®Xeon PhiTM processors allow easily incorporating high performance computing strategies to improve execution times. This article proposes a parallel implementation of the popular LIBSVM library, specially adapted to the Intel®Xeon PhiTM architecture. The proposed implementation is evaluated using publicly available datasets corresponding to classification and regression tasks. Results show that the proposed parallel version computes the same results than the original LIBSVM while reducing the time needed for training by up to a factor of 4.81.	manycore processor;support vector machine;xeon phi	Renzo Massobrio;Sergio Nesmachnow;Bernabé Dorronsoro	2017		10.1007/978-3-319-73353-1_20	parallel computing;computer science;acceleration;implementation;support vector machine;architecture;xeon phi;supercomputer	ML	-4.1814908779111155	44.420490209532	98660
5c4b61d017d4542dd9e1e4efaaa5f60eae2c25f9	cuhinesbatch: solving multiple hines systems on gpus human brain project*		The simulation of the behavior of the Human Brain is one of the most important challenges today in computing. The main problem consists of finding efficient ways to manipulate and compute the huge volume of data that this kind of simulations need, using the current technology. In this sense, this work is focused on one of the main steps of such simulation, which consists of computing the Voltage on neurons’ morphology. This is carried out using the Hines Algorithm. Although this algorithm is the optimum method in terms of number of operations, it is in need of non-trivial modifications to be efficiently parallelized on NVIDIA GPUs. We proposed several optimizations to accelerate this algorithm on GPU-based architectures, exploring the limitations of both, method and architecture, to be able to solve efficiently a high number of Hines systems (neurons). Each of the optimizations are deeply analyzed and described. To evaluate the impact of the optimizations on real inputs, we have used 6 different morphologies in terms of size and branches. Our studies have proven that the optimizations proposed in the present work can achieve a high performance on those computations with a high number of neurons, being our GPU implementations about 4× and 8× faster than the OpenMP multicore implementation (16 cores), using one and two K80 NVIDIA GPUs respectively. Also, it is important to highlight that these optimizations can continue scaling even when dealing with number of neurons.	algorithm;computation;graphics processing unit;human brain project;image scaling;mathematical morphology;multi-core processor;neuron;openmp;parallel programming model;simulation	Pedro Valero-Lara;I. Martinez-Perez;Antonio J. Peña;Xavier Martorell;Raúl Sirvent;Jesús Labarta	2017		10.1016/j.procs.2017.05.145		HPC	-2.584488562719171	43.280382481375526	98803
55b404f26405fcb0ad286532e032b9b7e919ec1d	on speeding-up parallel jacobi iterations for svds	convergence;symmetric matrices;matrix converters;signal processing algorithms;jacobian matrices;algorithm design and analysis;parallel algorithms	We live in an era of big data and the analysis of these data is becoming a bottleneck in many domains including biology and the internet. To make these analyses feasible in practice, we need efficient data reduction algorithms. The Singular Value Decomposition (SVD) is a data reduction technique that has been used in many different applications. For example, SVDs have been extensively used in text analysis. The best known sequential algorithms for the computation of SVDs take cubic time which may not be acceptable in practice. As a result, many parallel algorithms have been proposed in the literature. There are two kinds of algorithms for SVD, namely, QR decomposition and Jacobi iterations. Researchers have found out that even though QR is sequentially faster than Jacobi iterations, QR is difficult to parallelize. As a result, most of the parallel algorithms in the literature are based on Jacobi iterations. For example, the Jacobi Relaxation Scheme (JRS) of the classical Jacobi algorithm has been shown to be very effective in parallel. In this paper we propose a novel variant of the classical Jacobi algorithm that is more efficient than the JRS algorithm. Our experimental results confirm this assertion. The key idea behind our algorithm is to select the pivot elements for each sweep appropriately. We also show how to efficiently implement our algorithm on such parallel models as the PRAM and the mesh.	assertion (software development);big data;cpu cache;computation;experiment;gnu scientific library;iteration;jacobi method;java topology suite (jts);linear programming relaxation;norm (social);parallel algorithm;polynomial-time approximation scheme;qr decomposition;singular value decomposition;time complexity	Soumitra Pal;Sudipta Pathak;Sanguthevar Rajasekaran	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0013	algorithm design;mathematical optimization;parallel computing;convergence;computer science;theoretical computer science;jacobi eigenvalue algorithm;distributed computing;parallel algorithm;algorithm;computer network;symmetric matrix	DB	-1.021912199065223	39.2442184678187	99239
8d93985b5fe6468a9514fde7690870fb60c5b9d3	a reconfigurable multi-function computing cache architecture	level 2;public key cryptosystems;special purpose factoring hardware;cache memory;configurable computing technologies;sieving;chip;functional unit;number factoring algorithms;level 1	A considerable portion of a chip is dedicated to a cache memory in a modern microprocessor chip. However, some applications may not actively need all the cache storage, especially the computing bandwidth limited applications. Instead, such applications may be able to use some additional computing resources. If the unused portion of the cache could serve these computation needs, the on-chip resources would be utilized more efficiently. This presents an opportunity to explore the reconfiguration of a part of the cache memory for computing. In this paper, we present a cache architecture to convert a cache into a computing unit for either of the following two structured computations, FIR and DCT/IDCT. In order to convert a cache memory to a function unit, we include additional logic to embed multi-bit output LUTs into the cache structure. Therefore, the cache can perform computations when it is reconfigured as a function unit. The experimental results show that the reconfigurable module improves the execution time of applications with a large number of data elements by a large factor (as high as 50 and 60). In addition, the area overhead of the reconfigurable cache module for FIR and DCT/IDCT is less than the core area of those functions. Our simulations indicate that a reconfigurable cache does not take a significant delay penalty compared with a dedicated cache memory. The concept of reconfigurable cache modules can be applied at Level-2 caches instead of Level-1 caches to provide an active-Level-2 cache similar to active memories.	access time;cpu cache;computation;convolution;core (optical fiber);discrete cosine transform;finite impulse response;input/output;microprocessor;overhead (computing);run time (program lifecycle phase);simulation	Huesung Kim;Arun K. Somani;Akhilesh Tyagi	2000		10.1145/329166.329185	chip;bus sniffing;embedded system;least frequently used;pipeline burst cache;sieve;computer architecture;cache-oblivious algorithm;snoopy cache;parallel computing;cache coloring;page cache;cpu cache;computer hardware;cache;computer science;write-once;cache invalidation;operating system;smart cache;cache algorithms;cache pollution;mesif protocol;cache-only memory architecture;non-uniform memory access	Arch	6.442214992826747	46.06044077731862	99385
faa9866869cf6a1dd84b50eea964b7515b1f6de4	software modems: a new way to communicate	computer industry;communication industry;modems;modems communication industry computer industry hardware;hardware	T he computer industry, always looking for ways to do more with less, has turned its attention to a new technology—inex-pensive software modems. These modems utilize software, some unused instruction cycles from the host computer's CPU, a bit of circuitry, and a codec chip (which translates between a computer's digital signals and a telephone line's analog signals). Because they don't use the digital signal processors (DSPs) that form the heart of conventional hardware modems, they can cost at least $10 per unit less. This is important in a market where manufacturers are looking for ways to save pennies per unit. Software modems are also half as power-hungry as hardware modems because they eliminate the need to supply power to a DSP. In addition, they are smaller (which makes them good for use in portable computers) and more easily upgraded. To acquire new features and functionality, users simply add the necessary software. Meanwhile, software modems can achieve the same speeds as their hardware counterparts. In fact, PCtel is already marketing a 56-Kbps software modem, which is as fast as the speediest hardware modem. The trade-off is that software modems operate by using resources from the host computer's microprocessor. A typical software modem running at 33 Kbps uses about a third of a 166-MHz Pentium CPU's processing time. Thus, users must have powerful computers to avoid limited functionality while running modems and other applications simultaneously. Lisa Pelgrim, a senior analyst at the Dataquest market research firm, said software modems will appeal to new modem users who want to get online with a minimum of expense. However, she said, users whose computing needs grow will find it increasingly difficult to run their software modems as they add applications. A hardware modem needs a fairly fast DSP (a specialized microprocessor whose architecture has been optimized to perform signal processing operations quickly) to generate the complex analog waveforms it must send over a telephone line. A software modem, on the other hand, uses the PC's microprocessor to mimic a DSP chip's functions. The software in the modems is very efficient with code. This lets the modems implement complex data compression, decompression, and modulation algorithms while still processing each chunk of data in time to receive the next chunk. Achieving this level of efficiency is tricky when a modem must work with a host CPU that might be unavailable when occupied with other tasks, rather than …	algorithm;analog signal;central processing unit;codec;data compression;data rate units;digital signal processor;electronic circuit;host (network);microprocessor;modem;modulation;portable computer;signal processing;softmodem;still processing;telephone line	Lee Goldberg	1997	Computer	10.1109/MC.1997.587544	embedded system;computer science;software engineering	Arch	7.385824498686339	40.95285956198165	99548
14d83bc6b85cb28882ef139d81c373a6f527c54e	hotspot identification through call trace analysis	meters;long term evolution;radio frequency;geology;planning;optimization;buildings	Network-collected call trace data has the potential to provide relevant information that can be leveraged to enhance the accuracy of network planning and optimization algorithms.In this paper, we present a geolocation technique that relies on prediction-based RF Pattern Matching (RFPM) and its application in the generation of high-accuracy traffic hotspot maps. The method relies solely on call trace data and an uptodate configuration of the cellular network. We demonstrate the value of this algorithm over existing solutions and validate the approach with LTE measurements collected in a very dense urban environment (Tokyo center). Unlike existing techniques, the proposed solution is capable of identifying traffic hotspots at the level of individual buildings and the resulting map is consistent with building heights.	algorithm;compaq lte;geolocation;java hotspot virtual machine;map;mathematical optimization;pattern matching;radio frequency;real-time clock;web traffic	Regis Lerbour;Yann Le Helloco;Razvan-Florentin Trifan	2016	2016 IEEE 84th Vehicular Technology Conference (VTC-Fall)	10.1109/VTCFall.2016.7881035	planning;simulation;telecommunications;meter;radio frequency	Metrics	5.48823582186534	34.63495346939239	99670
4e20b6879dc24de86b91d9d31e27b9cded69c2d3	design of a reconfigurable processor for nist prime field ecc	nist;elliptic curve cryptographic operations;decoding;elliptic curve;reconfigurable architectures;nist prime field;inverters;ecc operations;elliptic curve cryptography;192 to 521 bit reconfigurable processor nist prime field elliptic curve cryptographic operations ecc operations;registers;adders;cryptography;signal processing;arithmetic;192 to 521 bit;read write memory;nist elliptic curve cryptography signal processing read write memory registers inverters adders decoding hardware arithmetic;reconfigurable architectures cryptography microprocessor chips;reconfigurable processor;microprocessor chips;hardware	This paper describes a reconfigurable processor that provides support for basic elliptic curve cryptographic (ECC) operations over GF(p), such as modular addition, subtraction, multiplication, and inversion. The proposed processor can be configured for any of the five NIST primes with sizes ranging from 192 to 521 bits	cryptography;reconfigurable computing	Kendall Ananyi;Daler N. Rakhmatov	2006	2006 14th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2006.36	parallel computing;nist;computer hardware;computer science;cryptography;theoretical computer science;signal processing;elliptic curve cryptography;elliptic curve;algorithm	Arch	9.638361699535109	44.32551352520949	99848
08b5659be3d6a5178766a58c6f4a76cd385f444d	bridging the semantic gap: emulating biological neuronal behaviors with simple digital neurons	hebbian learning;biocomputing;biological neuronal behavior emulation power efficient hardware design hebbian learning spike timing dependent plasticity neural learning rules synaptic plasticity area overheads power overheads llif spiking neurons linear leak integrate and fire spiking neurons digital neuromorphic architecture neuromorphic hardware design community cisc risc power efficient processing elements complex nonlinear neuronal behaviors cortical properties transcendental functions floating point minimal power consumption neuron like computational element computer architects nonvon neumann computational models digital neurons;reduced instruction set computing biocomputing hebbian learning neural net architecture power aware computing;reduced instruction set computing;neural net architecture;power aware computing;neuromorphics nerve fibers computer architecture biological neural networks computational modeling	The advent of non von Neumann computational models, specifically neuromorphic architectures, has engendered a new class of challenges for computer architects. On the one hand, each neuron-like computational element must consume minimal power and area to enable scaling up to biological scales of billions of neurons; this rules out direct support for complex and expensive features like floating point and transcendental functions. On the other hand, to fully benefit from cortical properties and operations, neuromorphic architectures must support complex non-linear neuronal behaviors. This semantic gap between the simple and power-efficient processing elements and complex neuronal behaviors has rekindled a RISC vs. CISC-like debate within the neuromorphic hardware design community. In this paper, we address the aforementioned semantic gap for a recently-described digital neuromorphic architecture that constitutes simple Linear-Leak Integrate-and-Fire (LLIF) spiking neurons as processing primitives. We show that despite the simplicity of LLIF primitives, a broad class of complex neuronal behaviors can be emulated by composing assemblies of such primitives with low area and power overheads. Furthermore, we demonstrate that for the LLIF primitives without built-in mechanisms for synaptic plasticity, two well-known neural learning rules-spike timing dependent plasticity and Hebbian learning-can be emulated via assemblies of LLIF primitives. By bridging the semantic gap for one such system we enable neuromorphic system developers, in general, to keep their hardware design simple and power-efficient and at the same time enjoy the benefits of complex neuronal behaviors essential for robust and accurate cortical simulation.	bridging (networking);canonical account;computation;computational model;emulator;hebbian theory;image scaling;neuromorphic engineering;neuron;nonlinear system;simulation;synaptic package manager	Andrew Nere;Atif Hashmi;Mikko H. Lipasti;Giulio Tononi	2013	2013 IEEE 19th International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2013.6522342	reduced instruction set computing;parallel computing;hebbian theory;computer science;artificial intelligence;theoretical computer science;operating system;machine learning	Arch	4.585209270784029	41.114574607983734	99858
a953bf7ff14c9c9bd9d0a6b4b495d8e7829b9a44	a unifying lattice-based approach for the partitioning of systolic arrays via lpgs and lsgp	value system;convex programming;systolic array;space time;necessary and sufficient condition;indexation;signal and image processing;greedy algorithm;data flow	Various methods for the synthesis of systolic arrays from signal and image processing algorithms have been developed in the past few years. In this paper, we propose a technique for the partitioning problem, the problem to synthesize systolic arrays whose size does not match the problem size. Our technique generalizes most of the known lattice-based approaches to the partitioning problem and combines the multiprojection method for the synthesis of systolic arrays with the locally sequential-globally parallel (LSGP) and locally parallel-globally sequential (LPGS) partitioning schemes. Starting from (1) a k-dimensional large-size systolic array obtained from a system of n-dimensional uniform recurrences by a space-time transformation and (2) an arbitrary lattice in k-space inducing a partitioning of the array into subarrays, a small-size systolic array with a scalar-valued system clock is constructed via the LSGP or LPGS paradigm. In particular, the allocation function for the small-size array can be written in closed form and the timing function is obtained from timing functions for the subdomains, the set of operations performed by the subarrays, by simple greedy algorithms. In this way, the problem of finding optimal timing functions can in various cases be reduced to finding optimal timing functions for the subdomains. For problems of large size, these greedy algorithms seem to be preferable when compared with existing integer or non-convex programming formulations for finding (sub-)optimal timing functions. We also provide some new results, a necessary and sufficient condition for the existence of counter data flow, a formal relationship between partitionings of processor space and index space of the uniform recurrences in terms of counter data flow, and the structural equivalence between the lattice-based LSGP and LPGS schemes applied to the partitioning of index and processor space.		Karl-Heinz Zimmermann	1997	VLSI Signal Processing	10.1023/A:1007944932429	data flow diagram;mathematical optimization;greedy algorithm;discrete mathematics;parallel computing;convex optimization;systolic array;image processing;computer science;space time;mathematics;value system;algorithm	Arch	4.45236201123497	37.98462386360483	99906
03b768753641140347cea77757234a952a0e7bd4	a performance model for idealized multiprocessors				Peter Schatte	1982	Elektronische Informationsverarbeitung und Kybernetik		combinatorics;discrete mathematics;mathematics	Theory	9.074691896366973	32.538318408589454	99925
51861655f70236fa8356e9b5665f4320369d3d7c	memory optimisation for hardware induction of axis-parallel decision tree	random access memory;decision tree induction memory optimisation hardware induction data mining machine learning supervised learning method ensemble learning technique memory communication parallel processing fpga device training process embedded memory blocks concurrent communication data compression scheme memory utilisation axis parallel decision tree classifier;training random access memory decision trees field programmable gate arrays training data data compression hardware;data compression;training;training data;field programmable gate arrays;decision trees;pattern classification concurrency control data compression data mining decision trees field programmable gate arrays learning artificial intelligence parallel processing;hardware	In data mining and machine learning applications, the Decision Tree classifier is widely used as a supervised learning method not only in the form of a stand alone model but also as a part of an ensemble learning technique (i.e. Random Forest). The induction of Decision Trees (i.e. training stage) involves intense memory communication and inherent parallel processing, making an FPGA device a promising platform for accelerating the training process due to high memory bandwidth enabled by the embedded memory blocks in the device. However, peak memory bandwidth is reached when all the channels of the block RAMs on the FPGA are free for concurrent communication, whereas to accommodate large data sets several block RAMs are often combined together making unavailable a number of memory channels. Therefore, efficient use of the embedded memory is critical not only for allowing larger training dataset to be processed on an FPGA but also for making available as many memory channels as possible to the rest of the system. In this work, a data compression scheme is proposed for the training data stored in the embedded memory for improving the memory utilisation of the device, targeting specifically the axis-parallel decision tree classifier. The proposed scheme takes advantage of the nature of the problem of the decision tree induction and improves the memory efficiency of the system without any compromise on the performance of the classifier. It is demonstrated that the scheme can reduce the memory usage by up to 66% for the training datasets under investigation without compromise in training accuracy, while a 28% reduction in training time is achieved due to extra processing power enabled by the additional memory bandwidth.	apache axis;data compression;data mining;decision tree learning;embedded system;ensemble learning;field-programmable gate array;high memory;machine learning;mathematical optimization;memory bandwidth;parallel computing;random forest;supervised learning	Chuan Cheng;Christos-Savvas Bouganis	2014	2014 International Conference on ReConFigurable Computing and FPGAs (ReConFig14)	10.1109/ReConFig.2014.7032538	data compression;memory address;cuda pinned memory;uniform memory access;shared memory;embedded system;training set;interleaved memory;parallel computing;distributed memory;computer science;physical address;theoretical computer science;operating system;machine learning;decision tree;overlay;extended memory;flat memory model;memory segmentation;registered memory;field-programmable gate array;computing with memory;memory map;memory management	ML	3.482922709182507	43.1249614229999	100076
4a51f90f9d0f8295f230b6547d182b5fbc79b1c5	permdnn: efficient compressed dnn architecture with permuted diagonal matrices		Deep neural network (DNN) has emerged as the most important and popular artificial intelligent (AI) technique. The growth of model size poses a key energy efficiency challenge for the underlying computing platform. Thus, model compression becomes a crucial problem. However, the current approaches are limited by various drawbacks. Specifically, network sparsification approach suffers from irregularity, heuristic nature and large indexing overhead. On the other hand, the recent structured matrix-based approach (i.e., CirCNN) is limited by the relatively complex arithmetic computation (i.e., FFT), less flexible compression ratio, and its inability to fully utilize input sparsity. To address these drawbacks, this paper proposes PermDNN, a novel approach to generate and execute hardware-friendly structured sparse DNN models using permuted diagonal matrices. Compared with unstructured sparsification approach, PermDNN eliminates the drawbacks of indexing overhead, non-heuristic compression effects and time-consuming retraining. Compared with circulant structure-imposing approach, PermDNN enjoys the benefits of higher reduction in computational complexity, flexible compression ratio, simple arithmetic computation and full utilization of input sparsity. We propose PermDNN architecture, a multi-processing element (PE) fully-connected (FC) layer-targeted computing engine. The entire architecture is highly scalable and flexible, and hence it can support the needs of different applications with different model configurations. We implement a 32-PE design using CMOS 28nm technology. Compared with EIE, PermDNN achieves 3.3x~4.8x higher throughout, 5.9x~8.5x better area efficiency and 2.8x~4.0x better energy efficiency on different workloads. Compared with CirCNN, PermDNN achieves 11.51x higher throughput and 3.89x better energy efficiency.		Chunhua Deng;Siyu Liao;Yi Xie;K. K. Parhi;Xuehai Qian;Bo Yuan	2018	2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)	10.1109/MICRO.2018.00024	parallel computing;fast fourier transform;throughput;deep learning;computer science;search engine indexing;architecture;scalability;artificial neural network;computational complexity theory;artificial intelligence	Arch	3.246985546242025	42.67173225288818	100100
8aff0b4157b572a99c3770b1607c358436b56fe9	efficient algorithm-based fault tolerance for sparse matrix operations	runtime;checkpointing;error analysis;fault tolerant systems;fault tolerance;sparse matrices	We propose a fault tolerance approach for sparse matrix operations that detects and implicitly locates errors in the results for efficient local correction. This approach reduces the runtime overhead for fault tolerance and provides high error coverage. Existing algorithm-based fault tolerance approaches for sparse matrix operations detect and correct errors, but they often rely on expensive error localization steps. General checkpointing schemes can induce large recovery cost for high error rates. For sparse matrix-vector multiplications, experimental results show an average reduction in runtime overhead of 43.8%, while the error coverage is on average improved by 52.2% compared to related work. The practical applicability is demonstrated in a case study using the iterative Preconditioned Conjugate Gradient solver. When scaling the error rate by four orders of magnitude, the average runtime overhead increases only by 31.3% compared to low error rates.	algorithm;application checkpointing;conjugate gradient method;convex conjugate;error detection and correction;fault tolerance;image scaling;iterative method;overhead (computing);sensor;solver;sparse matrix;subroutine	Alexander Schöll;Claus Braun;Michael Andreas Kochte;Hans-Joachim Wunderlich	2016	2016 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)	10.1109/DSN.2016.31	fault tolerance;parallel computing;real-time computing;sparse matrix;computer science;distributed computing	HPC	-0.007319145240719685	38.15967275754026	100208
e549f1255659bca0b8b6030ab7d5393ccd13cd67	rsa chips (past/present/future)	chip	We review the issues involved in building a special-purpose chip for performing RSA encryption/decryption, and review a few of the current implementation efforts.	encryption;rsa (cryptosystem)	Ronald L. Rivest	1984		10.1007/3-540-39757-4_16	chip;parallel computing;computer science;theoretical computer science;computer security	Crypto	8.366965812861784	44.34374201677923	100223
8e0de06951b55273667db85a65cc1a6aff158a56	exploring the granularity of sparsity in convolutional neural networks		Sparsity helps reducing the computation complexity of DNNs by skipping the multiplication with zeros. The granularity of sparsity affects the efficiency of hardware architecture and the prediction accuracy. In this paper we quantitatively measure the accuracy-sparsity relationship with different granularity. Coarse-grained sparsity brings more regular sparsity pattern, making it easier for hardware acceleration, and our experimental results show that coarsegrained sparsity have very small impact on the sparsity ratio given no loss of accuracy. Moreover, due to the index saving effect, coarse-grained sparsity is able to obtain similar or even better compression rates than fine-grained sparsity at the same accuracy threshold. Our analysis, which is based on the framework of a recent sparse convolutional neural network (SCNN) accelerator, further demonstrates that it saves 30% – 35% of memory references compared with fine-grained sparsity.	artificial neural network;computation;concurrency (computer science);convolutional neural network;deep learning;experiment;hardware acceleration;sparse matrix	Huizi Mao;Song Han;Jeff Pool;Wenshuo Li;Xingyu Liu;Yu Wang;William J. Dally	2017	2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)	10.1109/CVPRW.2017.241	pattern recognition;convolutional neural network;artificial intelligence;kernel (linear algebra);computer science;granularity;artificial neural network;machine learning;hardware architecture;hardware acceleration	Vision	3.4316811326476517	42.715646258962195	100318
049575eade7fcf0fcca7c5974041b2785424e9bc	a collaborative cpu-gpu approach for principal component analysis on mobile heterogeneous platforms		Abstract The advent of the modern GPU architecture has enabled computers to use General Purpose GPU capabilities (GPGPU) to tackle large scale problem at a low computational cost. This technological innovation is also available on mobile devices, addressing one of the primary problems with recent devices: the power envelope. Unfortunately, recent mobile GPUs suffer from a lack of accuracy that can prevent them from running any large scale data analysis tasks, such as principal component analysis (Shlens, 0000) (PCA). The goal of our work is to address this limitation by combining the high precision available on a CPU with the power efficiency of a mobile GPU. In this paper, we exploit the shared memory architecture of mobile devices in order to enhance the CPU–GPU collaboration and speed up PCA computation without sacrificing precision. Experimental results suggest that such an approach drastically reduces the power consumption of the mobile device while accelerating the overall workload. More generally, we claim that this approach can be extended to accelerate other vectorized computations on mobile devices while still maintaining numerical accuracy.	central processing unit;graphics processing unit;principal component analysis	Olivier Valery;Pangfeng Liu;Jan-Jan Wu	2018	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2018.05.006	workload;parallel computing;architecture;distributed computing;principal component analysis;speedup;computation;exploit;mobile device;general-purpose computing on graphics processing units;computer science	HPC	-1.8209029892913984	41.796362685996215	100626
c2b88d5f4bfd1949ccbd8d541d7de3b6a3cb50fc	simultaneous multithreaded matrix processor	performance evaluation;fpga vhdl implementation;data parallel applications;simultaneous multithreading;microblaze;vector matrix processing	This paper proposes a simultaneous multithreaded matrix processor (SMMP) to improve the performance of data-parallel applications by exploiting instruction-level parallelism (ILP) data-level parallelism (DLP) and thread-level parallelism (TLP). In SMMP, the well-known five-stage pipeline (baseline scalar processor) is extended to execute multi-scalar/vector/matrix instructions on unified parallel execution datapaths. SMMP can issue four scalar instructions from two threads each cycle or four vector/matrix operations from one thread, where the execution of vector/matrix instructions in threads is done in round-robin fashion. Moreover, this paper presents the implementation of our proposed SMMP using VHDL targeting FPGA Virtex-6. In addition, the performance of SMMP is evaluated on some kernels from the basic linear algebra subprograms (BLAS). Our results show that, the hardware complexity of SMMP is 5.68 times higher than the baseline scalar processor. However, speedups of 4.9, 6.09, 6.98, 8.2, 8.25, 8.72, 9.36, 11.84 and 21.57 are achieved on BLAS kernels of applying Givens rotation, scalar times vector plus another, vector addition, vector scaling, setting up Givens rotation, dot-product, matrix–vector multiplication, Euclidean length, and matrix–matrix multiplications, respectively. The average speedup over the baseline is 9.55 and the average speedup over complexity is 1.68. Comparing with Xilinx MicroBlaze, the complexity of SMMP is 6.36 times higher, however, its speedup ranges from 6.87 to 12.07 on vector/matrix kernels, which is 9.46 in average.	simultaneous multithreading	Mostafa I. Soliman;Elsayed A. Elsayed	2015	Journal of Circuits, Systems, and Computers	10.1142/S0218126615501145	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;simultaneous multithreading	Theory	-4.411780391382666	42.518023953087614	100662
30ebc845fd1a3f0f7d47dd58d171ccb88ebbeffe	keh-gait: using kinetic energy harvesting for gait-based user authentication systems		With the rapid development of sensor networks and embedded computing technologies, miniaturized wearable healthcare monitoring devices have become practically feasible. For many of these devices, accelerometer-based user authentication systems by gait analysis are becoming a hot research topic. However, a major bottleneck of such system is it requires continuous sampling of accelerometer, which reduces battery life of wearable sensors. In this paper, we present KEH-Gait, which advocates use of output voltage signal from kinetic energy harvester (KEH) as the source for gait recognition. KEH-Gait is motivated by the prospect of significant power saving by not having to sample the accelerometer at all. Indeed, our measurements show that, compared to conventional accelerometer-based gait detection, KEH-Gait can reduce energy consumption by 82.15 percent. The feasibility of KEH-Gait is based on the fact that human gait has distinctive movement patterns for different individuals, which is expected to leave distinctive patterns for KEH as well. We evaluate the performance of KEH-Gait using two different types of KEH hardware on a data set of 20 subjects. Our experiments demonstrate that, although KEH-Gait yields slightly lower accuracy than accelerometer-based gait detection when single step is used, the accuracy problem can be overcome by the proposed Probability-based Multi-Step Sparse Representation Classification (PMSSRC). Moreover, the security analysis shows that the EER of KEH-Gait against an active spoofing attacker is 11.2 and 14.1 percent using two different types of KEH hardware, respectively.		Weitao Xu;Guohao Lan;Qi Lin;Sara Khalifa;Mahbub Hassan;Neil W. Bergmann;Wen Hu	2019	IEEE Transactions on Mobile Computing	10.1109/TMC.2018.2828816	real-time computing;wearable computer;wireless sensor network;accelerometer;gait analysis;spoofing attack;gait;distributed computing;computer science;gait (human);bottleneck	Security	2.8318281996177856	34.275379453159466	100673
3727e20bee9f2c1063a20a1ccb5a2ed85d014dfa	a parallel-pipeline architecture of the fast polynomial transform for computing a two-dimensional cyclic convolution	computers;convolution integrals;pipeline cyclic convolution fast polynomial transform fft butterfly;fft butterfly;general and miscellaneous mathematics computing and information science;image processing;very large scale integration;functional programming;polynomials;pipelining computers;digital techniques;fast fourier transformations;signal processing;data flow processing;cyclic convolution;parallel processing computers;programming 990200 mathematics computers;data flow;architecture computers;architecture;functions;parallel processing;real time operation;pipeline;fast polynomial transform;synthetic aperture radar	In this paper, a parallel-pipeline, radix-2 architecture is proposed to implement the fast polynomial transform (FPT). It is shown that such a structure can be used to efficiently compute a two-dimensional convolution of d<inf>1</inf>× d<inf>2</inf>complex number points, where d1 = 2<sup>m-r+1</sup>and d<inf>2</inf>= 2<sup>m</sup>for 1 ≤ r ≤ m.	circular convolution;parameterized complexity;pipeline (computing);polynomial	Trieu-Kien Truong;Kuang Yung Liu;Irving S. Reed	1983	IEEE Transactions on Computers	10.1109/TC.1983.1676222	overlap–add method;parallel processing;discrete mathematics;parallel computing;synthetic aperture radar;convolution theorem;image processing;computer science;theoretical computer science;architecture;signal processing;functional programming;pipeline	Vision	8.324764785516793	39.34948745909838	100730
802661f647e2ea657ca4a905410645b84faf2f90	unbridle the bit-length of a crypto-coprocessor with montgomery multiplication	rsa;montgomery multiplication;public key;smartcard;security requirements;crypto coprocessor	We present a novel approach for computing 2n-bit Montgomery multiplications with n-bit hardware Montgomery multipliers. Smartcards are usually equipped with such hardware Montgomery multipliers; however, due to progresses in factoring algorithms, the recommended bit length of public-key schemes such as RSA is steadily increasing, making the hardware quickly obsolete. Thanks to our doublesize technique, one can re-use the existing hardware while keeping pace with the latest security requirements. Unlike the other double-size techniques which rely on classical n-bit modular multipliers, our idea is tailored to take advantage of n-bit Montgomery multipliers. Thus, our technique increases the perenniality of existing products without compromises in terms of security.	algorithm;bit-length;coprocessor;dadda multiplier;integer factorization;matrix multiplication;montgomery modular multiplication;public-key cryptography;requirement;smart card	Masayuki Yoshino;Katsuyuki Okeya;Camille Vuillaume	2006		10.1007/978-3-540-74462-7_14	parallel computing;computer science;theoretical computer science;computer security	Crypto	8.159063904504082	44.775301521604355	101230
3ead4d4711182a4e1a144376ed98200012d889e7	accelerating graph analytics by co-optimizing storage and access on an fpga-hmc platform		Graph analytics, which explores the relationships among interconnected entities, is becoming increasingly important due to its broad applicability, from machine learning to social sciences. However, due to the irregular data access patterns in graph computations, one major challenge for graph processing systems is performance. The algorithms, softwares, and hardwares that have been tailored for mainstream parallel applications are generally not effective for massive, sparse graphs from the real-world problems, due to their complex and irregular structures. To address the performance issues in large-scale graph analytics, we leverage the exceptional random access performance of the emerging Hybrid Memory Cube (HMC) combined with the flexibility and efficiency of modern FPGAs. In particular, we develop a collaborative software/hardware technique to perform a level-synchronized Breadth First Search (BFS) on a FPGA-HMC platform. From the software perspective, we develop an architecture-aware graph clustering algorithm that exploits the FPGA-HMC platform»s capability to improve data locality and memory access efficiency. From the hardware perspective, we further improve the FPGA-HMC graph processor architecture by designing a memory request merging unit to take advantage of the increased data locality resulting from graph clustering. We evaluate the performance of our BFS implementation using the AC-510 development kit from Micron and achieve $2.8 \times$ average performance improvement compared to the latest FPGA-HMC based graph processing system over a set of benchmarks from a wide range of applications.	algorithm;baseline (configuration management);benchmark (computing);best, worst and average case;breadth-first search;cluster analysis;collaborative software;computation;data access;entity;field-programmable gate array;graph (abstract data type);graphics processing unit;hybrid memory cube;locality of reference;machine learning;mathematical optimization;optimizing compiler;performance;random access;sparse matrix	Soroosh Khoram;Jialiang Zhang;Maxwell Strange;Jing Li	2018		10.1145/3174243.3174260	hybrid memory cube;parallel computing;computer science;software;distributed computing;breadth-first search;clustering coefficient;data access;random access;microarchitecture;analytics	HPC	-3.3155772849223957	43.30646887720804	101347
47b3eff881eaa6b9f163cc6299194daef7aa4b05	high-performance implementations of a clustering algorithm for finding network communities	social networking online data mining graphics processing units multiprocessing systems pattern clustering;social network services;cluster algorithm;pattern clustering;standards;performance evaluation;measurement;social networking services;communities measurement clustering algorithms graphics processing unit standards social network services optimization;data mining;social network;graphics processing units;data mining algorithm;social networking online;graphic processing unit;clustering algorithms;optimization;multiprocessing systems;communities;large scale distributed systems;graphics processing unit;high performance;large scale distributed system high performance implementation clustering algorithm network community social network interconnectedness resource intensive endeavor agglomerative algorithm egocentric community finding performance evaluation egocentric data mining algorithm gpu multicore system	The size and interconnectedness of social networks continues to increase. As a result, finding communities or subsets of like nodes within these large networks has become a resource-intensive endeavor. In this paper, we characterize community-finding organized on the basis of network/set properties, and describe an agglomerative algorithm called egocentric community finding. The primary contribution of this paper is a performance evaluation in which the egocentric data-mining algorithm is optimized for execution on various computing platforms, including GPU's, multicore systems, and large-scale distributed systems.	algorithm;apache hadoop;cuda;cluster analysis;computation;data access;data mining;distributed computing;graphics processing unit;interconnectedness;multi-core processor;multithreading (computer architecture);openmp;parallel computing;performance evaluation;programmer;real-time clock;real-time computing;social network;symmetric multiprocessing;thread (computing)	Alex Restrepo;Andres Solano;Jerry Scripps;Christian Trefftz;Jonathan Engelsma;Gregory Wolffe	2012	2012 IEEE International Conference on Electro/Information Technology	10.1109/EIT.2012.6220744	computer science;data science;theoretical computer science;operating system;machine learning;data mining;cluster analysis;measurement;social network	HPC	-2.218259915101706	42.82396926309299	101371
f76261226d53d52f6c91c74eae2145eef67024ba	mpeg video encryption algorithms	mpeg video;real time;mpeg codec;multimedia application;video conferencing;motion vector;multimedia data;video on demand;multimedia data security;mpeg video encryption;software implementation;large data	Multimedia data security is important for multimedia commerce. Previous cryptography studies have focused on text data. The encryption algorithms developed to secure text data may not be suitable to multimedia applications because of the large data size and real time constraint. For multimedia applications, light weight encryption algorithms are attractive. We present four fast MPEG video encryption algorithms. These algorithms use a secret key to randomly change the sign bits of Discrete Cosine Transform (DCT) coefficients and/or the sign bits of motion vectors. The encryption is accomplished by the inverse DCT (IDCT) during the MPEG video decompression processing. These algorithms add a small overhead to MPEG codec. Software implementations are fast enough to meet the real time requirement of MPEG video applications. The experimental results show that these algorithms achieve satisfactory results. They can be used to secure video-on-demand, video conferencing, and video email applications.	algorithm;codec;coefficient;computation;cryptography;data compression;data security;discrete cosine transform;emoticon;encryption;key (cryptography);moving picture experts group;overhead (computing);randomness;real-time transcription;text corpus;video email	Bharat K. Bhargava;Changgui Shi;Sheng-Yih Wang	2004	Multimedia Tools and Applications	10.1023/B:MTAP.0000033983.62130.00	telecommunications;computer science;multimedia;internet privacy;videoconferencing;computer network	Security	8.56636388568029	36.63514049728049	101430
2a4cee13ea4bcef3f8c6b26532569dea32a8e7ef	suresense: sustainable wireless rechargeable sensor networks for the smart grid	wireless sensor networks power system measurement power system reliability smart power grids;power system measurement;energy replenishment suresense sustainable wireless rechargeable sensor networks electrical power grid information and communication technologies electrical services efficiency electrical services safety electrical services reliability electrical services sustainability radio frequency based wireless energy transfer smart grid monitoring missions rf based wireless energy transfer electromagnetic waves data communication protocols mobile chargers;data communication;sensor network;wireless sensor network;wireless communication;energy transfer;radio frequency;information and communication technology;smart power grids;information exchange;wireless sensor networks wireless communication smart grids electricity supply industry robot sensing systems batteries energy exchange information exchange power system reliability;sensor nodes;power system reliability;electric power;electricity supply industry;wireless sensor networks;optimization model	The electrical power grid has recently been embracing the advances in Information and Communication Technologies (ICT) for the sake of improving efficiency, safety, reliability and sustainability of electrical services. For a reliable smart grid, accurate, robust monitoring and diagnosis tools are essential. Wireless Sensor Networks (WSNs) are promising candidates for monitoring the smart grid, given their capability to cover large geographic regions at low-cost. On the other hand, limited battery lifetime of the conventional WSNs may create a performance bottleneck for the long-lasting smart grid monitoring tasks, especially considering that the sensor nodes may be deployed in hard to reach, harsh environments. In this context, recent advances in Radio Frequency (RF)-based wireless energy transfer can increase sustainability of WSNs and make them operationally ready for smart grid monitoring missions. RF-based wireless energy transfer uses Electromagnetic (EM) waves and it operates in the same medium as the data communication protocols. In order to achieve timely and efficient charging of the sensor nodes, we propose the Sustainable wireless Rechargeable Sensor network (SuReSense). SuReSense employs mobile chargers that charge multiple sensors from several landmark locations. We propose an optimization model to select the minimum number of landmarks according to the locations and energy replenishment requirements of the sensors.	mathematical optimization;radio frequency;rechargeable battery;requirement;sensor	Melike Erol-Kantarci;H. T. Mouftah	2012	IEEE Wireless Communications	10.1109/MWC.2012.6231157	embedded system;wireless sensor network;telecommunications;computer science;smart grid;key distribution in wireless sensor networks;mobile wireless sensor network;computer network	Mobile	3.3245882453044824	32.4555819799302	101807
de99b4bfb311aa98ad6ca67d371b8abcf76d8289	optical techniques for information security	near infrared radiation;protection information;modulation phase;rayonnement ir proche;imageria termica;information security holographic optical components optical interferometry biomedical optical imaging holography optical recording authentication data security space technology cryptography;holograma;image numerique;information encoding;two dimensional data security;recording;digital holographic recording;id tag;articulo sintesis;modulacion fase;visible imaging;modelo 3 dimensiones;free space propagation;encryption;propagation espace libre;holografia optica;information security;telecommunication sans fil;article synthese;phase modulation;information retrieval;random codes cryptography encoding holographic interferometry identification technology information retrieval infrared imaging optical images optical information processing;modele 3 dimensions;degree of freedom;biometrie;optical waveform;spectral content;authentication;three dimensional data processing;biometrics;biometria;interferometrie;three dimensional model;code aleatoire;free space optical technology;optical holography;digital holography;cifrado;securite donnee;multiplaje;multiplexing;three dimensional;radiacion infrarroja cercana;authentification;random coding;interferometric method free space optical technology information security optical encryption optical waveform spectral content information encoding two dimensional data security three dimensional data security data retrieval optical holographic recording digital holographic recording three dimensional data processing optical authentication near infrared imaging optical id tag biometrics random codes visible imaging;holographic interferometry;codificacion;enregistrement;automatic recognition;multiplexage;telecomunicacion optica;telecommunication optique;autenticacion;optical encryption;interferometric method;thermal imaging;cryptage;proteccion informacion;infrared imaging;optical information processing;modelo 2 dimensiones	This paper presents an overview of the potential of free space optical technology in information security, encryption, and authentication. Optical waveform posses many degrees of freedom such as amplitude, phase, polarization, spectral content, and multiplexing which can be combined in different ways to make the information encoding more secure. This paper reviews optical techniques for encryption and security of two-dimensional and three-dimensional data. Interferometric methods are used to record and retrieve data by either optical or digital holography for security applications. Digital holograms are widely used in recording and processing three dimensional data, and are attractive for securing three dimensional data. Also, we review optical authentication techniques applied to ID tags with visible and near infrared imaging. A variety of images and signatures, including biometrics, random codes, and primary images can be combined in an optical ID tag for security and authentication.	antivirus software;authentication;biometrics;code;digital holography;encryption;information security;multiplexing;polarization (waves);waveform	Osamu Matoba;Takanori Nomura;Elisabet Pérez-Cabre;María S. Millán;Bahram Javidi	2009	Proceedings of the IEEE	10.1109/JPROC.2009.2018367	telecommunications;computer science;interferometry;information security;authentication;optics;holography;computer security	Security	8.268256047815608	35.12691062645044	102217
74d51a2b0d61e339b807ad6bee431aa2e1caf5f5	design of a versatile and cost-effective hybrid floating-point/lns arithmetic processor	logarithmic number system;word length;logarithmic number system lns arithmetic;cost effectiveness;floating point;floating point arithmetic;exponential computation;high performance;high speed;logarithmic computation	LNS (logarithmic number system) arithmetic has the advantages of high-precision and high performance in complex function computation. However, the large hardware problem in LNS addition/subtraction computation has made the large word-length LNS arithmetic implementation impractical. In this research, we proposed a hybrid floating-point (FLP)/LNS processor that can utilize the FLP multiplication-addition-fused (MAF) unit and the FLP division unit for implementing the computation of LNS addition/subtraction. With unified representation format in FLP and LNS numbers, this hybrid processor is versatile because it can execute the FLP-to-LNS and LNS-to-FLP conversions easily, without any extra hardware cost, in addition to the FLP multiplication-addition/subtraction, FLP division, and LNS addition/subtraction instructions. It is cost-effective because the FLP hardware is shared by the LNS unit. A 32-bit hybrid FLP/LNS processor is implemented on the Xilinx Virtex II multimedia FF896 development board. From the synthesis results, the hardware of the 32-bit hybrid processor is at most three times that of a 32-bit pure FLP processor. Our proposed hybrid FLP/LNS approach has made the design of very large word-length LNS arithmetic processors become practical.	32-bit;central processing unit;computation;floating-point unit;logarithmic number system;microprocessor development board;virtex (fpga);windows fundamentals for legacy pcs	Chichyang Chen;Paul Chow	2007		10.1145/1228784.1228912	arithmetic;parallel computing;computer hardware;computer science;floating point;operating system	Arch	6.653182952967167	45.633884461456375	102256
9f97a4ddac55082a55b31b4b1d504270ade35cef	ble parameter optimization for iot applications		Bluetooth Low Energy (BLE) has been designed as a power efficient protocol for small portable and autonomous devices, showing its efficiency for connecting these devices with smartphones to periodically and frequently exchange data, like heart rate or notifications. Additionally, BLE is present in almost every smartphone, turning it into perfect ubiquitous remote control for smart homes, buildings or cities. Nevertheless, there is still room to improve BLE performance for typical IoT use cases where battery lifetime should reach several years. In this paper we propose an extension to a model for evaluating BLE performance, latency and energy consumption, in order to provide realistic results based on various scenario conditions. In addition, we propose a parameter optimization of the BLE Neighbor Discovery process, in order to obtain the best performance possible depending on the constraints of specific use cases. Our results on two typical IoT test-cases show that advertiser battery lifetime can be increased up to ~89x (9.55 days to 2.32 years) for a first case, and ~281x (9.55 months to 7.36 years) for a second case.	autonomous robot;bluetooth;experiment;mathematical optimization;remote control;requirement;smartphone;test case	Andreina Liendo;Dominique Morche;Roberto Guizzetti;Franck Rousseau	2018	2018 IEEE International Conference on Communications (ICC)	10.1109/ICC.2018.8422714	computer network;still room;real-time computing;latency (engineering);battery (electricity);energy consumption;neighbor discovery protocol;computer science;testbed;internet of things;remote control	Embedded	1.8019789982211385	33.78100046967198	102294
89c94d80ba2903bcd12fee020299013d5dafb22f	compact clefia implementation on fpgas	random access storage cryptography field programmable gate arrays;field programmable gate arrays throughput hardware table lookup processor scheduling pipelines encryption;symmetrical encryption;fpga;virtex 4 fpga compact hardware structure clefia encryption algorithm bram virtex 5 fpga lut reduction;fpga implementation;cryptography;fpga clefia symmetrical encryption;random access storage;field programmable gate arrays;clefia;symmetric encryption	In this paper two compact hardware structures for the computation of the CLEFIA encryption algorithm are presented. One structure based on the existing state of the art and a novel structure with a more compact organization. This paper shows that, with the use of the existing embedded FPGA components and a careful scheduling, throughputs above 1Gbit/s can be achieved with a resource usage as low as 86 LUTs and 3 BRAMs on a VIRTEX 5 FPGA. Implementation results suggest that a LUT reduction up to 67% can be achieved at a performance cost of 17% on a VIRTEX 4 FPGA, resulting in Throughput/Slice efficiency gains up to 2.5 times, when compared with the related state of the art.	algorithm;clefia;computation;embedded system;field-programmable gate array;scheduling (computing);throughput;virtex (fpga)	Paulo Proenca;Ricardo Chaves	2011	2011 21st International Conference on Field Programmable Logic and Applications	10.1109/FPL.2011.101	embedded system;parallel computing;computer science;theoretical computer science;field-programmable gate array	EDA	9.275720156919654	45.02740828249	102396
a09fc2efd6a991ebf6ec090399b4eb9a35b53b54	a case study of grid computing and computer algebra: parallel gröbner bases and characteristic sets	distributed system;algoritmo paralelo;haute performance;systeme reparti;parallel algorithm;calcul formel;grobner basis;machine parallele;distributed computing;paralelisacion;supercomputer;base grobner;grobner bases;methode caracteristiques;calculo formal;algorithme parallele;grid;supercomputador;characteristic sets;sistema repartido;method of characteristics;rejilla;parallelisation;parallelization;alto rendimiento;grille;calculo repartido;parallel machines;parallel implementation;metodo caracteristicas;computer algebra;grid computing;high performance;calcul reparti;superordinateur	This paper describes a parallel implementation of the Gröbner Bases algorithm and the Characteristic Sets method using a grid environment. The two algorithms, their parallelization, and grid-enabled implementations are presented. The performance of the implementations has been evaluated and the experiments have demonstrated considerable speedups.	experiment;gigabyte;grid computing;gröbner basis;parallel algorithm;parallel computing;symbolic computation	Iyad A. Ajwa	2007	The Journal of Supercomputing	10.1007/s11227-007-0103-y	supercomputer;parallel computing;computer science;method of characteristics;gröbner basis;theoretical computer science;distributed computing;parallel algorithm;grid;algorithm;grid computing	HPC	-2.929901903786838	36.46378944305054	102464
30fe9d108e62082b664b7cac9156c5a46f2faa40	high-performance general-purpose arithmetic operations using the massive parallel dna-based computation		In this paper, we presented a graph-based computing model to perform the basic arithmetic operations using the DNA computing model with maximum parallelization capabilities. In other words, we proposed a mathematical transformation model to map the basic arithmetic operations to the Hamilton Path Problem (HPP) which can be solved with DNA computers easily and efficiently. Our analyses and simulations show the feasibility and effectivity of the proposed transformation. This technique can open new horizons for performing the generalpurpose arithmetic operations with massive parallelism of the DNA strands.	computation;computer;dna computing;general-purpose markup language;general-purpose modeling;hamiltonian path problem;parallel computing;simulation	Mercedeh Sanjabi;Ali Jahanian;Maryam Tahmasebi	2017	2017 Euromicro Conference on Digital System Design (DSD)	10.1109/DSD.2017.23	parallel computing;theoretical computer science;dna computing;massively parallel;computer science;computation;arithmetic;parallel processing;hamiltonian path;graph	EDA	0.8909429002176548	37.12271315112995	102476
45ac36224672c87f2821ba0f8ce2bcce580ec5d2	optimization of triangular and banded matrix operations using 2d-packed layouts		Over the past few years, multicore systems have become increasingly powerful and thereby very useful in high-performance computing. However, many applications, such as some linear algebra algorithms, still cannot take full advantage of these systems. This is mainly due to the shortage of optimization techniques dealing with irregular control structures. In particular, the well-known polyhedral model fails to optimize loop nests whose bounds and/or array references are not affine functions. This is more likely to occur when handling sparse matrices in their packed formats. In this article, we propose using 2d-packed layouts and simple affine transformations to enable optimization of triangular and banded matrix operations. The benefit of our proposal is shown through an experimental study over a set of linear algebra benchmarks.	algorithm;automatic vectorization;benchmark (computing);code;colour banding;computation;control flow;data structure;experiment;linear algebra;low-pass filter;mathematical optimization;multi-core processor;numerical linear algebra;out of the box (feature);parallel computing;polyhedron;polytope model;program optimization;run time (program lifecycle phase);source-to-source compiler;sparse matrix;standard streams;supercomputer;tiling window manager;triangular matrix;whole earth 'lectronic link	Toufik Baroudi;Rachid Seghir;Vincent Loechner	2017	TACO	10.1145/3162016	band matrix;parallel computing;computer science;economic shortage;discrete mathematics;linear algebra;sparse matrix;affine transformation;multi-core processor;polytope model	HPC	-2.830790787950676	40.69675097484466	103024
1a9391795ad7a5461bb30123af35f95fb0351ddc	memristor models for machine learning	systems;neural networks;technology and engineering	In the quest for alternatives to traditional complementary metal-oxide-semiconductor, it is being suggested that digital computing efficiency and power can be improved by matching the precision to the application. Many applications do not need the high precision that is being used today. In particular, large gains in area and power efficiency could be achieved by dedicated analog realizations of approximate computing engines. In this work we explore the use of memristor networks for analog approximate computation, based on a machine learning framework called reservoir computing. Most experimental investigations on the dynamics of memristors focus on their nonvolatile behavior. Hence, the volatility that is present in the developed technologies is usually unwanted and is not included in simulation models. In contrast, in reservoir computing, volatility is not only desirable but necessary. Therefore, in this work, we propose two different ways to incorporate it into memristor simulation models. The first is an extension of Strukov’s model, and the second is an equivalent Wiener model approximation. We analyze and compare the dynamical properties of these models and discuss their implications for the memory and the nonlinear processing capacity of memristor networks. Our results indicate that device variability, increasingly causing problems in traditional computer design, is an asset in the context of reservoir computing. We conclude that although both models could lead to useful memristor-based reservoir computing systems, their computational performance will differ. Therefore, experimental modeling research is required for the development of accurate volatile memristor models.	analog;approximate computing;approximation algorithm;cmos;computation (action);computer architecture;dynamical system;heart rate variability;matching;machine learning;memristor;modulation;modulator device component;nonlinear system;nonlinear system identification;performance per watt;rem sleep behavior disorder;reservoir device component;reservoir computing;semiconductor;simulation;spatial variability;volatility;replication compartment	Juan Pablo Carbajal;Joni Dambre;Michiel Hermans;Benjamin Schrauwen	2015	Neural Computation	10.1162/NECO_a_00694	computer science;artificial intelligence;theoretical computer science;machine learning;system;mathematics;artificial neural network	AI	4.939617117622878	41.399821357593936	103047
6ecc59e0ea25ae9643bd9a840ae23e804cde6808	accelerating non-negative matrix factorization for audio source separation on multi-core and many-core architectures		Non-negative matrix factorization (NMF) has been successfully used in audio source separation and parts-based analysis; however, iterative NMF algorithms are computationally intensive, and therefore, time to convergence is very slow on typical personal computers. In this paper, we describe high performance parallel implementations of NMF developed using OpenMP for shared-memory multicore systems and CUDA for many-core graphics processors. For 20 seconds of audio, we decrease running time from 18.5 seconds to 2.6 seconds using OpenMP and 0.6 seconds using CUDA. These performance increases allow source separation to be carried out on entire songs in a number of seconds, a process which was previously impractical with respect to time. We give insight into how such significant speed gains were made and encourage the development and use of parallel music information retrieval software.	algorithm;cuda;central processing unit;graphics processing unit;information retrieval;iterative method;manycore processor;multi-core processor;non-negative matrix factorization;openmp;personal computer;quartz (graphics layer);shared memory;source separation;time complexity	Eric Battenberg;David Wessel	2009			music information retrieval;parallel computing;software;multi-core processor;source separation;matrix decomposition;cuda;real-time computing;non-negative matrix factorization;convergence (routing);computer science	HPC	-1.4240502056934237	41.218714268565954	103295
49ce6b9b2b357080a659c5752c3cbe333863a160	two-level hierarchical parallelization of second-order møller-plesset perturbation calculations in divide-and-conquer method	second order moller plesset perturbation theory;electron correlation;large molecule;parallel algorithm;parallel performance;granularity;divide and conquer method;linear scaling method;parallel implementation;extended system	A two-level hierarchical parallelization scheme including the second-order Møller-Plesset perturbation (MP2) theory in the divide-and-conquer method is presented. The scheme is a combination of coarse-grain parallelization assigning each subsystem to a group of processors, with fine-grain parallelization, where the computational tasks for evaluating MP2 correlation energy of the assigned subsystem are distributed among processors in the group. Test calculations demonstrate that the present scheme shows high parallel efficiency and makes MP2 calculations practical for very large molecules.	central processing unit;møller–plesset perturbation theory;parallel computing;personnameuse - assigned;speedup	Michio Katouda;Masato Kobayashi;Hiromi Nakai;Shigeru Nagase	2011	Journal of computational chemistry	10.1002/jcc.21855	macromolecule;granularity;theoretical computer science;computational chemistry;mathematics;parallel algorithm;electronic correlation	HPC	-3.8696483933686245	37.88013359699266	103404
c371462a8139b101c8c9712b2deac53026b0c382	scheduling iterative task computation on message-passing architectures	numerical solution;nonlinear problems;message passing;algorithms;task scheduling;parallel processing;mathematics computers information science management law miscellaneous	Scheduling iterative task graphs on message-passing architectures with nonzero communication overhead so as to minimize execution time is a difficult problem. In this paper, we propose a scheduling algorithm which explores task parallelism within and across iterations by balancing processor loads, overlapping communication and computation, and eliminating unnecessary communication overhead. We present experimental results to demonstrate the effectiveness of this algorithm.	computation;message passing;scheduling (computing)	Tao Yang;Pedro C. Diniz;Apostolos Gerasoulis;Vivek Sarkar	1995			fair-share scheduling;fixed-priority pre-emptive scheduling;parallel computing;dynamic priority scheduling;computer science;rate-monotonic scheduling;theoretical computer science;two-level scheduling;distributed computing	NLP	-4.140951940862522	39.28972074351779	103632
cc5b9883667b946de1c220608df3d2c7fd2cd366	efficient sorting design on a novel embedded parallel computing architecture with unique memory access	esort algorithm;high-throughput dsp;novel epuma dsp;intra-core sort;in-core sort;optimized bitonic sort;epuma multi-core;unique memory access;parallel computing architecture;bitonic sort;algorithm esort;epuma platform	Article history: Received 23 October 2012 Received in revised form 21 June 2013 Accepted 21 June 2013 Available online xxxx Embedded Parallel computing architecture with Unique Memory Access (ePUMA) is a domain-specific embedded heterogeneous 9-core chip multiprocessor, which has a unique design with low power and high silicon efficiency for high-throughput DSP in emerging telecommunication and multimedia applications. Sorting is one of the most widely studied algorithms, more embedded applications also need efficient sorting. This paper proposes an efficient bitonic sorting algorithm eSORT for the novel ePUMA DSP. eSORT algorithm consists of two parts: an in-core sorting algorithm and an intra-core sorting algorithm. Both algorithms are adapted to the novel architecture and take advantage of the ePUMA platform. This paper implemented and evaluated the eSORT for variable datasets on ePUMA multi-core DSP and compared its performance with the Cell BE processors with the same SIMD parallelization structure. Results show that bitonic sort on ePUMA multicore DSP has much better performance and scalability. Compared with optimized bitonic sort on Cell BE, the in-core sort is 11 times faster and intra-core sort is 15 times faster in average. 2013 Elsevier Ltd. All rights reserved.	128-bit;algorithm design;bitonic sorter;byte;cell (microprocessor);central processing unit;computer architecture;computer memory;digital signal processor;embedded system;high-throughput computing;multi-core processor;multiprocessing;network on a chip;parallel computing;read-only memory;ring network;simd;scalability;sorting algorithm;star network;throughput	Wenbiao Zhou;Zhaoyun Cai;Ruiqiang Ding;Chen Gong;Dake Liu	2013	Computers & Electrical Engineering	10.1016/j.compeleceng.2013.06.007	computer architecture;parallel computing;computer science;bitonic sorter;sorting algorithm;distributed computing	EDA	-1.354648977835317	45.72192438236747	103633
093c48223ce529bcd1ed15881454ab827854f551	algorithmic performance studies on graphics processing units	optimisation sous contrainte;tratamiento paralelo;calcul matriciel;linear algebra;constrained optimization;scientific application;algoritmo paralelo;largeur bande;nvidia geforce 8800 gtx;haute performance;non linear programming;paper;parallel algorithm;algorithmique;minimally invasive;sparse direct solvers;traitement parallele;carte graphique;producto matriz;interior point;programacion no lineal;distributed computing;methode point interieur;programmation non lineaire;algorithme parallele;chip;optimizacion con restriccion;descomposicion matricial;matrice creuse;metodo punto interior;decomposition matricielle;algorithmics;mathematical programming;algoritmica;matrix decomposition;graphics processing units;anchura banda;alto rendimiento;nvidia;graphic processing unit;calculo repartido;bandwidth;floating point;matrix multiplication;unidad de proceso grafico;coma flotante;matrix calculus;computer science;sparse matrix;produit matrice;interior point method;nonlinear optimization;constrained optimization problem;programmation mathematique;high performance;calcul reparti;programacion matematica;sparse matrices;parallel processing;calculo de matrices;matrix product;matriz dispersa;virgule flottante	We report on our experience with integrating and using graphics processing units (GPUs) as fast parallel floa tingpoint co-processors to accelerate two fundamental computa tional scientific kernels on the GPU: sparse direct factorization a nd nonlinear interior-point optimization. Since a full re-imple mentation of these complex kernels is typically not feasible, we ident ify the matrix-matrix multiplication as a first natural entry-p oint for a minimally invasive integration of GPUs. We investigate the performance on the NVIDIA GeForce 8800 multicore chip initially architectured for intensive gaming applications. We exploit the architectural features of the GeForce 8800 GPU t o design an efficient GPU-parallel sparse matrix solver. A prototype approach to leverage the bandwidth and computing power of GPUs for these matrix kernel operation is demonstrated resu lting in an overall performance of over110 GFlops/s on the desktop for large matrices. We use our GPU algorithm for PDE-constrained optimization problems and demonstrate that the commodity GPU is a useful co-processor for scientific applications.	algorithm;central processing unit;computer graphics;constrained optimization;coprocessor;desktop computer;energy (psychological);geforce 8 series;graphics processing unit;kernel (linear algebra);mathematical optimization;matrix multiplication;multi-core processor;nonlinear system;prototype;solver;sparse matrix;the matrix	Olaf Schenk;Matthias Christen;Helmar Burkhart	2008	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2008.05.008	parallel computing;sparse matrix;matrix multiplication;nonlinear programming;computer science;theoretical computer science;interior point method;algorithm	HPC	-2.63561682031134	37.4063207902462	103719
f388ada0e8698300988cb83944e2e2fdc24abaf3	an fpga implementation of an elliptic curve processor gf(2m)	elliptic curve;cosic;systolic array;public key cryptosystem;fpga;fpga implementation;multiplication;elliptic curve cryptosystem;hardware implementation;modular multiplication;montgomery modular;elliptic curve cryptosystems	This paper describes a hardware implementation of an arithmetic processor which is efficient for elliptic curve (EC) cryptosystems, which are becoming increasingly popular as an alternative for public key cryptosystems based on factoring. The modular multiplication is implemented using a Montgomery modular multiplication in a systolic array architecture, which has the advantage that the clock frequency becomes independent of the bit length m.	bit-length;clock rate;cryptosystem;elliptic curve cryptography;field-programmable gate array;integer factorization;montgomery modular multiplication;public-key cryptography;systolic array	Nele Mentens;Siddika Berna Ors Yalcin;Bart Preneel	2004		10.1145/988952.989062	arithmetic;modular arithmetic;parallel computing;systolic array;elliptic curve digital signature algorithm;computer science;theoretical computer science;curve25519;mathematics;elliptic curve cryptography;elliptic curve;multiplication;field-programmable gate array	Crypto	9.296737662001068	44.210685336997955	104053
894fbca0c6be9e2388490f4394d6615461a5af95	rlgraph: flexible computation graphs for deep reinforcement learning		Reinforcement learning (RL) tasks are challenging to implement, execute and test due to algorithmic instability, hyper-parameter sensitivity, and heterogeneous distributed communication patterns. We argue for the separation of logical component composition, backend graph definition, and distributed execution. To this end, we introduce RLgraph, a library for designing and executing high performance RL computation graphs in both static graph and define-by-run paradigms. The resulting implementations yield high performance across different deep learning frameworks and distributed backends.	algorithm;computation;deep learning;instability;open-source software;reinforcement learning	Michael Schaarschmidt;Sven Mika;Kai Fricke;Eiko Yoneki	2018	CoRR		machine learning;implementation;theoretical computer science;deep learning;reinforcement learning;computation;mathematics;graph;artificial intelligence	HPC	-3.5924644882409624	43.44868985730633	104171
a1b0ca97ac8088149fc265ab9be49b7ba01bcda0	microprogrammed significance arithmetic: a perspective and feasibility study	real time;feasibility study	This study is an attempt to evaluate the feasibility of microprogrammed routines for monitoring significant digits in the numerical result of digital computers in real time. The first part is tutorial and, in the second part, microprograms for two methods of significance arithmetic are designed and evaluated.	binary logarithm;computation;computer;diagram;microarchitecture;microcode;newton's method;numerical analysis;operand;overhead (computing);parallel computing;requirement;run time (program lifecycle phase);significance arithmetic;significand;significant figures	C. V. Ramamoorthy;Masahiro Tsuchiya	1971		10.1145/1478873.1478962	real-time computing;computer science;algorithm;computer engineering	Arch	4.78757267529583	45.315829625680735	104232
91de22aeacdff84c2cfaea9a000fec284a9cd3c0	exploring area/delay tradeoffs in an aes fpga implementation	block ciphering;cryptage bloc;field programmable gate array;optimum pareto;diseno circuito;algorithmique;securite;building block;reconfigurable architectures;block cipher;circuit design;red puerta programable;reseau porte programmable;circuit a la demande;fpga implementation;custom circuit;circuito integrato personalizado;algorithmics;algoritmica;criptografia;cryptography;safety;retard;secure system;cost efficiency;cifrado en bloque;cryptographie;conception circuit;advanced encryption standard;retraso;pareto optimum;seguridad;optimo pareto;architecture reconfigurable;pareto optimality;software implementation	Field-Programmable Gate Arrays (FPGAs) have lately become a popular target for implementing cryptographic block ciphers, as a well-designed FPGA solution can combine some of the algorithmic flexibility and cost efficiency of an equivalent software implementation with throughputs that are comparable to custom ASIC designs. The recently selected Advanced Encryption Standard (AES) is slowly replacing older ciphers as the building block of choice for secure systems and is well suited to an FPGA implementation. In this paper we explore the design decisions that lead to area/delay tradeoffs in a single-core AES FPGA implementation. This work provides a more thorough description of the defining AES hardware characteristics than is currently available in the research literature, along with implementation results that are pareto optimal in terms of throughput, latency, and area efficiency.	algorithm;application-specific integrated circuit;block cipher;cost efficiency;cryptography;encryption;field-programmable gate array;pareto efficiency;scientific literature;single-core;throughput;top-down and bottom-up design	Joseph Zambreno;David Nguyen;Alok N. Choudhary	2004		10.1007/978-3-540-30117-2_59	advanced encryption standard;embedded system;block cipher;real-time computing;computer science;cryptography;operating system;circuit design;algorithmics;cost efficiency	EDA	7.573674115638693	44.921096661985395	104775
1ad852394d2469563323eeccf21eec690df3cf24	application of fpga technology to accelerate the finite-difference time-domain (fdtd) method	fdtd method;finite difference time domain;fpga;fiber optic;computer assisted design;electrical engineering;programmable logic;manual placement and pipelining;event horizon	The continuing advances in the field of electrical engineering, in areas like cellular communications, fiber optics, mobile and multi-gigahertz electronics have necessitated a computer-assisted design approach to the complex electromagnetic interactions and problems that arise. Finite-Difference Time-Domain (FDTD) Analysis is a very powerful tool for the modeling of electromagnetic phenomena. The algorithm is computationally intensive and simulations can run for a few hours to several days. Increasing the computation speed and decreasing the run times of this algorithm would bring greater productivity and new avenues of research to many facets of electrical engineering.The algorithm is transferred to custom FPGA-based hardware using a pipelined bit-serial arithmetic architecture. A one-dimensional resonator is used to verify the implementation and explore the hardware speed and costs. The computational speed is extremely fast and is not related to the number of computational cells in the simulation. Finally, a discussion of future research is presented.	algorithm;computation;computer-aided design;electrical engineering;field-programmable gate array;finite-difference time-domain method;interaction;optical fiber;pipeline (computing);serial communication;simulation	Ryan N. Schneider;Laurence E. Turner;Michal M. Okoniewski	2002		10.1145/503048.503063	finite-difference time-domain method;embedded system;parallel computing;real-time computing;computer science;optical fiber;theoretical computer science;operating system;computer aided design;programmable logic device;event horizon;field-programmable gate array	EDA	4.828860259572368	45.23707838058253	104879
0556fd54652792f437dc31a7b9c2aa35a93fcb5f	zcipher algorithm specification		A full period single cycle linear congruential generator f(x) 5 n x + C is using as an invertible mapping for both fixed and cryptovariable-dependent data substitution. Exponent n can be any natural number. If n = 1 then the generator can be implemented with LEA instruction in just a single CPU cycle on Intel Pentium and above. The recommended n for Zcipher is 13; it gives a maximal 5 n value less than 2 32 .	addressing mode;algorithm;cipher;instruction cycle;linear congruential generator;maximal set	Ilya O. Levin	2008	IACR Cryptology ePrint Archive		specification pattern;invertible matrix;parallel computing;pentium;instruction cycle;linear congruential generator;algorithm design;mathematics;exponent;specification language	Crypto	8.528479906472679	42.86309265790668	105146
1cd294f3bcd647c8a2b2bbce47e827a8ece8b973	implementing sparse matrix-vector multiplication on throughput-oriented processors	linear algebra;paper;flow;dense linear algebra;nvidia geforce gtx 285;streamlines;quad core intel clovertown system sparse matrix vector multiplication throughput oriented processor spmv sparse linear algebra dense linear algebra throughput oriented architecture gpu peak bandwidth structured grid unstructured mesh matrices geforce gtx 285 cell be;cuda;memory access;visualization;scaling;package;parallel;nvidia;unstructured mesh;vectors graphics processing units linear algebra matrix multiplication parallel architectures sparse matrices;computer science;sparse matrix	Sparse matrix-vector multiplication (SpMV) is of singular importance in sparse linear algebra. In contrast to the uniform regularity of dense linear algebra, sparse operations encounter a broad spectrum of matrices ranging from the regular to the highly irregular. Harnessing the tremendous potential of throughput-oriented processors for sparse operations requires that we expose substantial fine-grained parallelism and impose sufficient regularity on execution paths and memory access patterns. We explore SpMV methods that are well-suited to throughput-oriented architectures like the GPU and which exploit several common sparsity classes. The techniques we propose are efficient, successfully utilizing large percentages of peak bandwidth. Furthermore, they deliver excellent total throughput, averaging 16 GFLOP/s and 10 GFLOP/s in double precision for structured grid and unstructured mesh matrices, respectively, on a GeForce GTX 285. This is roughly 2.8 times the throughput previously achieved on Cell BE and more than 10 times that of a quad-core Intel Clovertown system.	cell (microprocessor);central processing unit;double-precision floating-point format;flops;geforce 200 series;graphics processing unit;intel core (microarchitecture);matrix multiplication;multi-core processor;parallel computing;regular grid;sparse matrix;throughput	Nathan Bell;Michael Garland	2009	Proceedings of the Conference on High Performance Computing Networking, Storage and Analysis	10.1145/1654059.1654078	parallel computing;flow;visualization;sparse matrix;streamlines, streaklines, and pathlines;scaling;computer science;theoretical computer science;linear algebra;sparse approximation;parallel;distributed computing;package;algebra	HPC	-3.3263155887214086	40.03603386288925	105383
6ff0273ba820882661b1187941d1f4c783e8494f	probabilistic certification of divide & conquer algorithms on global computing platforms: application to fault-tolerant exact matrix-vector product	divide conquer algorithms;global computing;fork join macro data flow graph;result checking	In [6], a new approach for certifying the correctness of program executions in hostile environments has been proposed. The authors presented probabilistic certification by massive attack detection through two algorithms MCT and EMCT. The execution to certify is represented by a macro-data flow graph which is used to randomly extract some tasks to be re-executed on safe resources in order to determine whether the execution is correct or faulty. Bounds associated with certification have been provided for general graphs and for tasks with out-tree dependencies.  In this paper, we extend those results with a cost analysis of parallel certification based on on-line scheduling by work-stealing. In particular, we focus on Divide & Conquer algorithms that are commonly used in symbolic computations and demonstrate the efficiency of EMCT for the certification of the resulting Fork-Join graph. Finally, we show how to combine EMCT with BCH codes to make a matrix-vector product both tolerant to falsifications and massive attacks.	algorithm;bch code;computation;constraint satisfaction dual problem;correctness (computer science);dataflow;fault tolerance;matrix multiplication;mobile data terminal;online and offline;randomness;scheduling (computing);work stealing	Jean-Louis Roch;Sébastien Varrette	2007		10.1145/1278177.1278191	parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language	PL	0.48952083604703056	35.50828339681104	105407
07913c069f192e7b581b5e6d474fd488430c70d9	modifying the singular value decomposition on the connection machine	parallel algorithm;numerical solution;efficiency;singular value decomposition;systolic array;downdating;matrices;algorithms;pipelining;cost benefit analysis;parallel processing;updating;mathematics computers information science management law miscellaneous	A fully parallel algorithm for updating and downdating the singular value decompositions (SVD’s) of an m-by-n(m≥n) matrix A is described. The algorithm uses similar chasing techniques for modifying the SVD’s described in [3], but requires fewer plane rotations, and can be implemented almost identically for both updating and downdating. Both cyclic and consecutive storage schemes are considered in parallel implementation. We show that the latter scheme outperforms the former on a distributed memory MIMD multiprocessor. We present the experimental results on the 32-node Connection Machine (CM-5).	connection machine;singular value decomposition	Jesse L. Barlow;Peter A. Yoon	1996	International Journal of High Speed Computing	10.1142/S0129053396000112	parallel processing;parallel computing;systolic array;computer science;cost–benefit analysis;theoretical computer science;operating system;distributed computing;parallel algorithm;efficiency;singular value decomposition;pipeline;algorithm;matrix	Arch	-2.1655444115929297	37.386102976770474	105470
42bd39c1896e31edf0b657b9ad29b783e8bcdacd	introduction to a system for implementing neural net connections on simd architectures	topology;simd computers;neural nets;simulation;massively parallel processors;error correcting devices;computer networks;neural net;parallel processing computers;algorithms;architecture	"""Neural networks have attracted much interest recently, and using parallel architectures to simulate neural networks is a natural and necessary application. The SIMD model of parallel computation is chosen, because systems of this type can be built with large numbers of processing elements. However, such systems are not naturally suited to generalized communication. A method is proposed that allows an implementation of neural network connections on massively parallel SIMD architectures. The key to this system is an algorithm that allows the formation of arbitrary connections between the """"neurons"""". A feature is the ability to add new connections quickly. It also has error recovery ability and is robust over a variety of network topologies. Simulations of the general connection system, and its implementation on the Connection Machine, indicate that the time and space requirements are proportional to the product of the average number of connections per neuron and the diameter of the interconnection network."""	apriori algorithm;central processing unit;computation;computer simulation;connection machine;embedded system;interconnection;network topology;neural networks;neuron;parallel computing;requirement;routing;simd;tree traversal	Sherryl Tomboulian	1987			parallel computing;computer science;theoretical computer science;architecture;machine learning;distributed computing;artificial neural network	ML	7.363499181121155	38.58535144375577	105587
915f52edccf6cd0e37ac6edfcbb6ecd62940925e	distributed fpga-based smart camera architecture for computer vision applications	distributed processing;fpga;softcore distributed fpga based smart camera architecture computer vision applications general purpose processor image processing image resolution massive parallelism capabilities full reconfigurable pipeline;smart camera;computer vision;general purpose computers;intelligent sensors cameras computer vision field programmable gate arrays general purpose computers;field programmable gate arrays;smart cameras computer architecture field programmable gate arrays image sensors conferences feature extraction;intelligent sensors;cameras	Smart camera networks (SCN) raise challenging issues in many fields of research, including vision processing, communication protocols, distributed algorithms or power management. Furthermore, application logic in SCN is not centralized but spread among network nodes meaning that each node must have to process images to extract significant features, and aggregate data to understand the surrounding environment. In this context, smart camera have first embedded general purpose processor (GPP) for image processing. Since image resolution increases, GPPs have reached their limit to maintain real-time processing constraint. More recently, FPGA-based platforms have been studied for their massive parallelism capabilities. This paper present our new FPGA-based smart camera platform supporting cooperation between nodes and run-time updatable image processing. The architecture is based on a full reconfigurable pipeline driven by a softcore.	aggregate data;business logic;centralized computing;computer vision;distributed algorithm;embedded system;field-programmable gate array;graph partition;image processing;image resolution;parallel computing;power management;real-time clock;reconfigurable computing;smart camera	Cédric Bourrasset;Luca Maggiani;Jocelyn Sérot;François Berry;Paolo Pagano	2013	2013 Seventh International Conference on Distributed Smart Cameras (ICDSC)	10.1109/ICDSC.2013.6778245	smart camera;embedded system;computer vision;machine vision;computer hardware;computer science;field-programmable gate array	Robotics	6.104635304052278	37.52537680314704	105649
1aad3b7abbebb7764177d37bb68380966cf41627	real-time dvb-mhp interactive data transcoding to blu-ray	real time	Digital TV systems are being deployed worldwide, and interactive applications are being offered as part of the services. The unparalleled success of DVD technology has motivated the development of interactive services for broadcast TV, with DVB-MHP being the most widely used open standard in the world. Despite the similarities between DVB-MHP-based interactive TV and the new Blu-ray format, there still exist substantial differences that make the two systems incompatible. In this paper, we analyze the differences in the DVB-MHP and Blu-ray interactive formats and propose a transcoding scheme to convert live broadcast interactive TV to a Blu-ray compatible format.	blu-ray;broadcast television systems;cc system;cpu cache;dvb-h;data and object carousel;digital video broadcasting;directory (computing);download;propagation constant;real-time transcription	Sergio Infante;Panos Nasiopoulos	2008	Int. J. Digital Multimedia Broadcasting	10.1155/2008/319063	computer science;multimedia;interactive television;world wide web;computer graphics (images)	HCI	6.09851569002297	33.678190707768856	105704
107fcdb714137ce860a4194ad4cc108ea802c604	cuthomasbatch and cuthomasvbatch, cuda routines to compute batch of tridiagonal systems on nvidia gpus: cuthomasbatch and cuthomasvbatch, cuda routines to compute batch of tridiagonal systems on nvidia gpus				Pedro Valero-Lara;I. Martinez-Perez;Raúl Sirvent;Xavier Martorell;Antonio J. Peña	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4909	computer science;parallel computing;scalability;tridiagonal matrix;cuda;tridiagonal matrix algorithm;parallel processing	HPC	-3.9221636791394507	38.61753082821272	105824
9366870846513c8949ee6ff04679b97d9a78e3e1	optimizing a mapreduce module of preprocessing high-throughput dna sequencing data	biology computing;bioinformatics genomics sequential analysis information management data handling data storage systems optimization;genome assembly;parallel programming;data analysis;error correction;optimization error correction genome assembly mapreduce next generation sequencing;parallel programming biology computing data analysis middleware molecular biophysics;molecular biophysics;index only strategy mapreduce module high throughput dna sequencing data data preprocessing mapreduce framework mapreduce programming model big data analysis middleware performance optimization cloudrs next generation sequencing data content exchange strategy content grouping strategy;middleware;optimization;mapreduce;next generation sequencing	The MapReduce framework has become the de facto choice for big data analysis in a variety of applications. In MapReduce programming model, computation is distributed to a cluster of computing nodes that runs in parallel. The performance of a MapReduce application is thus affected by system and middleware, characteristics of data, and design and implementation of the algorithms. In this study, we focus on performance optimization of a MapReduce application, i.e., CloudRS, which tackles on the problem of detecting and removing errors in the next-generation sequencing de novo genomic data. We present three strategies, i.e., content-exchange, content-grouping, and index-only strategies, of communication between the Map() and Reduce() functions. The three strategies differ in the way messages are exchanged between the two functions. We also present experimental results to compare performance of the three strategies.	algorithm;big data;computation;computer performance;de novo transcriptome assembly;high-throughput computing;mapreduce;mathematical optimization;middleware;optimizing compiler;preprocessor;programming model;sensor;throughput	Wei-Chun Chung;Yu-Jung Chang;Chien-Chih Chen;D. T. Lee;Jan-Ming Ho	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691694	computer science;bioinformatics;data-intensive computing;data mining;database	DB	-0.4615713210287277	42.57084105492116	105844
e42fbd0a960c0f0accbbe627fbb0328777e26d92	parallel-vector algorithms for particle simulations on shared-memory multiprocessors	digital computers;distributed memory;vectors computers;parallel algorithm;scalars;shared memory;particles;high performance computing;gpu computing;performance;simulation;mathematical logic;vectorization;memory access;computer architecture;mathematical methods and computing;mathematical models;computer calculations;array processors;computerized simulation;high performance computer;compatibility;parallel computer;parallelization;computer codes;graphic processing unit;algorithms;load balance;discrete element method;computational efficiency;particle modeling;shared memory system;memory bandwidth;shared memory multiprocessor;tensors	Over the last few decades, the computational demands of massive particle-based simulations for both scientific and industrial purposes have been continuously increasing. Hence, considerable efforts are being made to develop parallel computing techniques on various platforms. In such simulations, particles freely move within a given space, and so on a distributed-memory system, load balancing, i.e., assigning an equal number of particles to each processor, is not guaranteed. However, shared-memory systems achieve better load balancing for particle models, but suffer from the intrinsic drawback of memory access competition, particularly during (1) paring of contact candidates from among neighboring particles and (2) force summation for each particle. Here, novel algorithms are proposed to overcome these two problems. For the first problem, the key is a pre-conditioning process during which particle labels are sorted by a cell label in the domain to which the particles belong. Then, a list of contact candidates is constructed by pairing the sorted particle labels. For the latter problem, a table comprising the list indexes of the contact candidate pairs is created and used to sum the contact forces acting on each particle for all contacts according to Newton’s third law. With just these methods, memory access competition is avoided without additional redundant procedures. The parallel efficiency and compatibility of these two algorithms were evaluated in discrete element method (DEM) simulations on four types of shared-memory parallel computers: a multicore multiprocessor computer, scalar supercomputer, vector supercomputer, and graphics processing unit. The computational efficiency of a DEM code was found to be drastically improved with our algorithms on all but the scalar supercomputer. Thus, the developed parallel algorithms are useful on shared-memory parallel computers with sufficient memory bandwidth. 2010 Elsevier Inc. All rights reserved.	central processing unit;compiler;computation;computer architecture;computer graphics;discrete element method;distributed memory;expectation propagation;general-purpose computing on graphics processing units;graphics processing unit;load balancing (computing);mathematical optimization;memory bandwidth;multi-core processor;multiprocessing;multithreading (computer architecture);nehalem (microarchitecture);newton;openmp;parallel algorithm;parallel computing;particle filter;regular expression;sse3;scalability;shared memory;simulation;speedup;supercomputer;vector processor	Daisuke Nishiura;Hide Sakaguchi	2011	J. Comput. Physics	10.1016/j.jcp.2010.11.040	shared memory;particle;mathematical logic;supercomputer;parallel computing;tensor;distributed memory;performance;computer science;load balancing;theoretical computer science;mathematical model;vectorization;distributed computing;parallel algorithm;discrete element method;compatibility;memory bandwidth;general-purpose computing on graphics processing units;algorithm	HPC	-3.8086798889675055	37.997113843438704	105927
4f1461e403c87950b39cc91d19c84e4c70cd1847	design of a wireless sensor network platform for detecting rare, random, and ephemeral events	acoustic signal processing;pattern classification;sensor fusion;target tracking;user interfaces;wireless sensor networks;aa alkaline battery;exscal project;acoustic sensor;asynchronous processor wakeup circuitry;classification requirement;ephemeral event detection;extreme scale mote;grenade timer;human-in-the-loop operation;infrared sensor;low-power continuous operation;random event detection;rare event detection;recoverable retasking;user interface;wireless sensor network	We present the design of the eXtreme Scale Mote, a new sensor network platform for reliably detecting and classifying, and quickly reporting, rare, random, and ephemeral events in a large-scale, long-lived, and retaskable manner. This new mote was designed for the ExScal project which seeks to demonstrate a 10,000 node network capable of discriminating civilians, soldiers and vehicles, spread out over a 10km2 area, with node lifetimes approaching 1,000 hours of continuous operation on two AA alkaline batteries. This application posed unique functional, usability, scalability, and robustness requirements which could not be met with existing hardware, and therefore motivated the design of a new platform. The detection and classification requirements are met using infrared, magnetic, and acoustic sensors. The infrared and acoustic sensors are designed for low-power continuous operation and include asynchronous processor wakeup circuitry. The usability and scalability requirements are met by minimizing the frequency and cost of human-in-the-loop operations during node deployment, activation, and verification through improvements in the user interface, packaging, and configurability of the platform. Recoverable retasking is addressed by using a grenade timer that periodically forces a system reset. The key contributions of this work are a specific design point and general design methods for building sensor network platforms to detect exceptional events.	acoustic cryptanalysis;asynchronous circuit;continuous operation;electronic circuit;ibm websphere extreme scale;low-power broadcasting;requirement;scalability;schedule (computer science);sensor node;sensor web;software deployment;timer;usability;user interface	Prabal Dutta;Mike Grimmer;Anish Arora;Steven B. Bibyk;David E. Culler	2005	IPSN 2005. Fourth International Symposium on Information Processing in Sensor Networks, 2005.		embedded system;real-time computing;wireless sensor network;computer science;operating system	Mobile	3.6027166165487756	34.978896816975734	105954
46b33f7a14367706716362c1006dc4d58b92a2f8	combined hardware-software multi-parallel prefiltering on the convey hc-1 for fast homology detection	convey hc 1;hhblits;fpga;hybrid architecture;heterogeneity	We fully leveraged the hybrid system Convey-HC-1 to execute the tool HHblits.Our hardware-based design supports and accelerates prefiltering in HHblits.We developed a highly parallel hardware design for prefiltering a protein database.We achieved further speedup through a wisely workload-balanced approach. Protein databases used in research are huge and still grow at a fast pace. Many comparisons need to be done when searching similar (homologous) sequences for a given query sequence in these databases. Comparing a query sequence against all sequences of a huge database using the well-known Smith-Waterman algorithm is very time-consuming. Hidden Markov Models pose an opportunity for reducing the number of entries of a database and also enable to find distantly homologous sequences. Fewer entries are achieved by clustering similar sequences in a Hidden Markov Model. Such an approach is used by the bioinformatics tool HHblits. To further reduce the runtime, HHblits uses two-level prefiltering to reduce the number of time-consuming Viterbi comparisons. Still, prefiltering is very time-consuming. Highly parallel architectures and huge bandwidth are required for processing and transferring the massive amounts of data. In this article, we present an approach exploiting the reconfigurable, hybrid computer architecture Convey HC-1 for migrating the most time-consuming part. The Convey HC-1 with four FPGAs and high memory bandwidth of up to 76.8GB/s serves as the platform of choice. Other bioinformatics applications have already been successfully supported by the HC-1. Limited by FPGA size only, we present a design that calculates four first-level prefiltering scores per FPGA concurrently, i.e. 16 calculations in total. This score calculation for the query profile against database sequences is done by a modified Smith-Waterman scheme that is internally parallelized 128 times in contrast to the original Streaming 'Single Instruction Multiple Data (SIMD)' Extensions (SSE)-supported implementation where only 16-fold parallelism can be exploited and where memory bandwidth poses the limiting factor. Preloading the query profile, we are able to transform the memory-bound implementation to a compute- and resource-bound FPGA design. We tightly integrated the FPGA-based coprocessor into the hybrid computing system by employing task-parallelism for the two-level prefiltering. Despite much lower clock rates, the FPGAs outperform SSE-based execution for the calculation of the prefiltering scores by a factor of 7.9.	homology (biology)	Michael Bromberger;Fabian Nowak;Wolfgang Karl	2015	Parallel Computing	10.1016/j.parco.2014.09.006	parallel computing;real-time computing;computer science;theoretical computer science;heterogeneity;operating system;programming language;algorithm;field-programmable gate array	HPC	-0.01205644352766043	43.19722243074207	106114
24d474aa271b207191b83d2ae73ef0f352c2b0d8	heterogeneous cos pricing of rainbow options	cos method;option pricing;heterogeneous computation	This paper focuses on comparing different heterogeneous computational designs for the calculation of Rainbow options prices using the Fourier-cosine series expansion (COS) method. We also propose a simple enough way to automatically decide ratio of load balancing at runtime. A GPGPU implementation of the two-dimensional composite Simpson rule free of conditional statements with some degree of loop unrolling is also introduced. We will also show how to reduce the integration domain of coefficients appearing in the option pricing and by doing so, achieve a substantial speed-up and improve accuracy when compared versus a straightforward implementation.	cos;coefficient;computation;general-purpose computing on graphics processing units;load balancing (computing);loop unrolling;run time (program lifecycle phase);series expansion;simpson's rule	Aurelien Cassagnes;Yu Chen;Hirotada Ohashi	2013		10.1145/2535557.2535561	financial economics;real-time computing;economics;computer science;operations management;valuation of options;finance	HPC	-3.7234914326537756	40.89975229661406	106296
0b57c3836aacd8f8d49283ef3761c64604ccd262	sse based parallel solution for power systems network equations	linear algebra;algoritmo paralelo;microprocessor;systeme equation;computadora personal;ordinateur personnel;parallel algorithm;reseau electrique;personal computer;electrical network;red electrica;stockage donnee;calculateur simd;decomposition systeme;algorithme parallele;resolucion sistema ecuacion;resolution systeme equation;descomposicion matricial;data storage;sistema ecuacion;decomposition matricielle;simd computer;matrix decomposition;power system;algebre lineaire;equation system;system decomposition;almacenamiento datos;algebra lineal;streaming simd extensions;microprocesseur;equation system solving;systeme parallele;parallel system;descomposicion sistema;microprocesador;sistema paralelo	Streaming SIMD Extensions (SSE) is a unique feature embedded in the Pentium III class of microprocessors. By fully exploiting SSE, parallel algorithms can be implemented on a standard personal computer and a theoretical speedup of four can be achieved. In this paper, we demonstrate the implementation of a parallel LU matrix decomposition algorithm for solving power systems network equations with SSE and discuss advantages and disadvantages of this approach.	ibm power systems	Yu-Fai Fung;Muhammet Fikret Ercan;Tin-Kin Ho;Wai-leung Cheung	2001		10.1007/3-540-45545-0_98	electrical network;parallel computing;computer science;theoretical computer science;linear algebra;computer data storage;parallel algorithm;electric power system;matrix decomposition;algorithm	HPC	-2.687302634443723	36.89273449580058	106407
89647c19b6df02c19bd661ab8eb503a50fed043f	gaussian fuzzy commitment	gaussian processes authorisation biometrics access control data privacy fuzzy set theory;decoding authentication vectors error analysis error correction codes australia noise;random binary code word gaussian fuzzy commitment gaussian biometric template protection binary biometrics hard quantized gaussian biometrics juels wattenberg scheme	We discuss the protection of Gaussian biometric templates. We first introduce the Juels-Wattenberg for binary biometrics, where the binary biometrics are a result of hard-quantized Gaussian biometrics. The Juels-Wattenberg scheme adds a random binary code word to the biometric for privacy reasons and to allow errors in the biometric at authentication. We modify the Juels- Wattenberg scheme in such a way that we do not have to quantize the biometrics. We investigate and compare the performance of both approaches.	authentication;binary code;biometrics;code word;gaussian blur;missing data;performance;quantization (signal processing);scheme	A. J. Han Vinck;Aram Jivanyan;Jonas Winzen	2014	2014 International Symposium on Information Theory and its Applications		computer science;theoretical computer science;data mining;computer security	Arch	7.510159589167058	34.31593856967128	106588
91b769e5d16523efc72482d72c5ae051b1c953e5	eera-asr: an energy-efficient reconfigurable architecture for automatic speech recognition with hybrid dnn and approximate computing		This paper proposes a hybrid deep neural network (DNN) for automatic speech recognition and an energy-efficient reconfigurable architecture with approximate computing for accelerating the DNN. To accelerate the hybrid DNN and reduce the energy consumption, we propose a digital–analog mixed reconfigurable architecture with approximate computing units, including a binary weight network accelerator with analog multi-chain delay-addition units for bit-wise approximate computing and a recurrent neural network accelerator with approximate multiplication units for different calculation accuracy requirements. Implemented under TSMC 28nm HPC+ process technology, the proposed architecture can achieve the energy efficiency of 163.8TOPS/W for 20 keywords recognition and 3.3TOPS/W for common speech recognition.	approximate computing;approximation algorithm;artificial neural network;automated system recovery;deep learning;recurrent neural network;requirement;speech recognition	Bo Liu;Hai Qin;Yu Gong;Wei Ge;Mengwen Xia;Longxing Shi	2018	IEEE Access	10.1109/ACCESS.2018.2870273	architecture;field-programmable gate array;artificial neural network;energy consumption;computer science;multiplication;recurrent neural network;binary number;speech recognition;adder	Arch	4.044110910118754	42.714205218940045	106675
264e9e043bff55deb315b6cde1378926bb344f41	a space-time-ensemble parallel nudged elastic band algorithm for molecular kinetics simulation	parallel computing;nudged elastic band method;parallel algorithm;82 20 db;ensemble method;strain rate;molecular kinetics simulation;02 70 c;silica;space time;molecular dynamic simulation;nudged elastic band;simulation methods;message passing interface;massively parallel computer;transition state theory;parallel computer;kinetics;02 70 ns;thermal activation	A scalable parallel algorithm has been designed to study long-time dynamics of many-atom systems based on the nudged elastic band method, which performs mutually constrained molecular dynamics simulations for a sequence of atomic configurations (or states) to obtain a minimum energy path between initial and final local minimum-energy states. A directionally heated nudged elastic band method is introduced to search for thermally activated events without the knowledge of final states, which is then applied to an ensemble of bands in a path ensemble method for long-time simulation in the framework of the transition state theory. The resulting molecular kinetics (MK) simulation method is parallelized with a space–time-ensemble parallel nudged elastic band (STEP-NEB) algorithm, which employs spatial decomposition within each state, while temporal parallelism across the states within each band and band-ensemble parallelism are implemented using a hierarchy of communicator constructs in the Message Passing Interface library. The STEP-NEB algorithm exhibits good scalability with respect to spatial, temporal and ensemble decompositions on massively parallel computers. The MK simulation method is used to study low strain-rate deformation of amorphous silica.  2007 Elsevier B.V. All rights reserved. PACS: 02.70.-c; 02.70.Ns; 82.20.Db	computational thinking;computer cluster;computer simulation;data mining;energy citations database;energy minimization;graph theory;ibm notes;kinetics internet protocol;linux;maxima and minima;message passing interface;molecular dynamics;parallel algorithm;parallel computing;picture archiving and communication system;scalability	Aiichiro Nakano	2008	Computer Physics Communications	10.1016/j.cpc.2007.09.011	strain rate;parallel computing;computer science;message passing interface;theoretical computer science;space time;transition state theory;distributed computing;parallel algorithm;physics;kinetics	AI	-4.402007860185117	36.309534980761576	106795
1caa22bc7d78086cd5092d807f9204b57e13381b	signal processing for cryptography and security applications		Embedded devices need both an efficient and a secure implementation of cryptographic primitives. In this chapter we show how common signal processing techniques are used in order to achieve both objectives. Regarding efficiency, we first give an example of accelerating hash function primitives using the retiming transformation, a well known technique to improve signal processing applications. Second, we outline the use of some special features of DSP processors and techniques earlier developed for efficient implementations of public-key algorithms. Regarding the secure implementations we outline the concept of side channel attacks and show how a multitude of techniques for preprocessing the data are used in such scenarios. Finally, we talk about fuzzy secrets and point out the use of DSP techniques for an important role in cryptography — a key derivation.	algorithm;bcrypt;central processing unit;cryptographic primitive;digital signal processor;ecrypt;encryption;hash function;key derivation function;microtransaction;preprocessor;public-key cryptography;retiming;side-channel attack;signal processing	Miroslav Knezevic;Lejla Batina;Elke De Mulder;Junfeng Fan;Benedikt Gierlichs;Yong Ki Lee;Roel Maes;Ingrid Verbauwhede	2010		10.1007/978-1-4419-6345-1_7	telecommunications;computer science;theoretical computer science	Security	8.19855601920927	43.74633734940519	107053
28b7e66e106fb53b029ad2e511ba6d9408e45f23	a system-on-a-programmable-chip for real-time control of massively parallel arrays of biosensors and actuators	microcontrollers;intelligent actuators;real time systems control systems biosensors actuators microelectronics lab on a chip silicon signal design sensor systems signal processing;real time control;microcontrollers intelligent sensors intelligent actuators biosensors system on chip integrated circuit design field programmable gate arrays;system on a programmable chip;fpga implementation;integrated circuit design;control system;complex system;system on chip;lab on a chip;functional unit;field programmable gate arrays;intelligent sensors;biosensors;20 mhz system on a programmable chip real time control parallel array biosensor actuator lab on a chip device loac control unit sensor signal conditioning circuit signal processing circuit fpga implementation field programmable gate array control system operating frequency	The development of microelectronic lab-on-a-chip devices (LOACs) can now be pursued thanks to advances in silicon technology. As these kinds of devices may integrate different functional units, much care has to be put in the design of control units which have to provide real-time control capabilities in order to deal with complex systems composed of sensors, actuators, signal conditioning and processing circuits. Moreover, reconfigurability and expandability are key design features to get a flexible and reusable architecture. The FPGA implementation of a control system for an existing LOAC is presented as a case study with emphasis on the advantages of a programmable device approach. The presented system has been implemented on an Altera EPF10K200S device and the achieved operating frequency is 20MHz. The device was successfully tested and experimental results are hereby shown.	real-time transcription	Aldo Romani;Fabio Campi;S. Ronconi;Marco Tartagni;Gianni Medoro;Nicolò Manaresi	2003		10.1109/IWSOC.2003.1213041	embedded system;electronic engineering;computer hardware;engineering	Robotics	4.360890948497638	46.189968575993596	107234
3c9011f09170810f58d38b727ebc87f2a2fa6a8a	designing a novel hybrid algorithm for qr-code images encryption and steganography			encryption;hybrid algorithm;qr code;steganography	Mohammad Soltani;Amid Khatibi Bardsiri	2018	JCP		steganography;machine learning;artificial intelligence;encryption;hybrid algorithm;computer science	Crypto	8.518504312106122	35.6128034736451	107294
906b189e291fccecd9950c2d7acc0af6b7932751	the consistency analysis of addition chains for several fast algorithm of modular exponentiation	modular exponentiation	Modular exponentiation is the most frequently used and time-cost part in RSA, its fast algorithm is one of the focuses of RSA study, and to speed up the computation of modular exponentiation is most important to the performance and wide use of RSA. This paper studies Qin Jiu-shao algorithm, Blocking algorithm, Addition Chains algorithm, and Adaptive Binary Partition Table Searching Method. Another contribution of this paper is that the above algorithms are analyzed from the point of view of addition chains. In the point of view of addition chains, they are accordant, and Adaptive Binary Partition Table Searching Method can get higher efficiency than Qin Jiu-shao algorithm and Blocking algorithm, but may be further improved.	algorithm;computation;markov chain;modular exponentiation;partition table;point of view (computer hardware company)	Fu-guo Dong;Yu-rong Li;Jin-jiang Li	2010	JDCTA		modular exponentiation	Theory	9.032627953096572	42.53474760719092	107372
b8d5a727b6679e484e6c43a06e3c3d0a4cbb159d	trinity: enabling self-sustaining wsns indoors with energy-free sensing and networking		Whereas a lot of efforts have been put on energy conservation in wireless sensor networks (WSNs), the limited lifetime of these systems still hampers their practical deployments. This situation is further exacerbated indoors, as conventional energy harvesting (e.g., solar) may not always work. To enable long-lived indoor sensing, we report in this article a self-sustaining sensing system that draws energy from indoor environments, adapts its duty-cycle to the harvested energy, and pays back the environment by enhancing the awareness of the indoor microclimate through an “energy-free” sensing. First of all, given the pervasive operation of heating, ventilation, and air conditioning (HVAC) systems indoors, our system harvests energy from airflow introduced by the HVAC systems to power each sensor node. Secondly, as the harvested power is tiny, an extremely low but synchronous duty-cycle has to be applied whereas the system gets no energy surplus to support existing synchronization schemes. So, we design two complementary synchronization schemes that cost virtually no energy. Finally, we exploit the feature of our harvester to sense the airflow speed in an energy-free manner. To our knowledge, this is the first indoor wireless sensing system that encapsulates energy harvesting, network operating, and sensing all together.	bioinformatic harvester;data center;duty cycle;pervasive informatics;piezoelectricity;power management;printed circuit board;prototype;sensor node;software deployment;system integration;system monitoring;testbed;trinity;year 10,000 problem	Feng Li;Yanbing Yang;Zicheng Chi;Liya Zhao;Yaowen Yang;Jun Luo	2018	ACM Trans. Embedded Comput. Syst.	10.1145/3173039	real-time computing;wireless sensor network;energy harvesting;ventilation (architecture);airflow;synchronization;hvac;computer science;energy conservation;sensor node	Mobile	2.844685114247859	33.138202096337004	107497
7ebb9fad71ce8e08d5284b7644a5452cff6c75b3	enterprise: breadth-first graph traversal on gpus	virtualization;performance evaluation;arrays;scheduling;graphics processing units;optimization;switches;parallel processing;instruction sets;hardware	The Breadth-First Search (BFS) algorithm serves as the foundation for many graph-processing applications and analytics workloads. While Graphics Processing Unit (GPU) offers massive parallelism, achieving high-performance BFS on GPUs entails efficient scheduling of a large number of GPU threads and effective utilization of GPU memory hierarchy. In this paper, we present Enterprise, a new GPU-based BFS system that combines three techniques to remove potential performance bottlenecks: (1) streamlined GPU threads scheduling through constructing a frontier queue without contention from concurrent threads, yet containing no duplicated frontiers and optimized for both top-down and bottom-up BFS. (2) GPU workload balancing that classifies the frontiers based on different out-degrees to utilize the full spectrum of GPU parallel granularity, which significantly increases thread-level parallelism; and (3) GPU based BFS direction optimization quantifies the effect of hub vertices on direction-switching and selectively caches a small set of critical hub vertices in the limited GPU shared memory to reduce expensive random data accesses. We have evaluated Enterprise on a large variety of graphs with different GPU devices. Enterprise achieves up to 76 billion traversed edges per second (TEPS) on a single NVIDIA Kepler K40, and up to 122 billion TEPS on two GPUs that ranks No. 45 in the Graph 500 on November 2014. Enterprise is also very energy-efficient as No. 1 in the GreenGraph 500 (small data category), delivering 446 million TEPS per watt.	algorithm;breadth-first search;graph (abstract data type);graph traversal;graphics processing unit;kepler (microarchitecture);mathematical optimization;memory hierarchy;parallel computing;randomness;scheduling (computing);shared memory;task parallelism;top-down and bottom-up design;traversed edges per second;tree traversal;usb hub;vertex (geometry)	Hang Liu;H. Howie Huang	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807594	parallel processing;parallel computing;real-time computing;virtualization;network switch;computer science;theoretical computer science;operating system;instruction set;distributed computing;scheduling	HPC	-3.4540552115315593	43.049000380979855	107551
669508257d4621864011252d0423047f98d9329c	optimizing sparse tensor times matrix on multi-core and many-core architectures		This paper presents the optimized design and implementation of sparse tensor-times-dense matrix multiply (SpTTM) for CPU and GPU platforms. This primitive is a critical bottleneck in data analysis and mining applications based on tensor methods, such as the Tucker decomposition. We first design and implement sequential SpTTM to avoid explicit data transformations between a tensor and a matrix, which is the conventional approach. We further optimize SpTTM on multicore CPU and GPU systems by parallelizing, avoiding locks, and exploiting data locality. Our sequential SpTTM is up to 3.5× faster than the SpTTM from Tensor Toolbox and 1.5× over that from Cyclops Tensor Framework. Our parallel algorithms show 4.1× speedup on multicore Intel Core i7 and 18.8× speedup on NVIDIA K40c GPU over our sequential SpTTM respectively.	automatic parallelization;cpu cache;central processing unit;experiment;graphics processing unit;load balancing (computing);locality of reference;lock (computer science);mathematical optimization;matrix multiplication;multi-core processor;optimizing compiler;parallel algorithm;sparse matrix;speedup;tucker decomposition	Jiajia Li;Yuchen Ma;Chenggang Clarence Yan;Richard W. Vuduc	2016	2016 6th Workshop on Irregular Applications: Architecture and Algorithms (IA3)		computer architecture;parallel computing;computer science;theoretical computer science	HPC	-4.200187912468494	41.94762664053543	107706
e139bacc1058c7c970708745a8ef2b5a4b33125c	gardenia: a domain-specific benchmark suite for next-generation accelerators		This paper presents the Graph Analytics Repository for Designing Next-generation Accelerators (GARDENIA), a benchmark suite for studying irregular algorithms on massively parallel accelerators. Applications with limited control and data irregularity are the main focus of existing generic benchmarks for accelerators, while available graph analytics benchmarks do not apply state-of-the-art algorithms and/or optimization techniques. GARDENIA includes emerging irregular applications in big-data and machine learning domains which mimic massively multithreaded commercial programs running on modern large-scale datacenters. Our characterization shows that GARDENIA exhibits irregular microarchitectural behavior which is quite different from structured workloads and straightforward-implemented graph benchmarks.	algorithm;benchmark (computing);big data;machine learning;mathematical optimization;microarchitecture;thread (computing)	Zhen Xu;Xuhao Chen;Jie Shen;Yonghui Zhang;Cheng Chen;Canqun Yang	2017	CoRR		parallel computing;massively parallel;graph theory;computer science;cuda;suite;supercomputer;graph;analytics	Arch	-3.496949742412641	43.30587233792042	107808
71a99b50d0bcd45ecda975fa1d7fc8f87795df05	an analysis of computational models for accelerating the subtractive pixel adjacency model computation	spam;steganalysis;gpu;fpga;cuda	Two computational parallel models are developed using FPGA and GPU platforms.To the best of our knowledge, these are the first acceleration schemas for SPAM.An architectural and performance analysis of both computational models is presented.The FPGA architecture accelerates SPAM making an optimal use of hardware resources.The GPU model accelerates SPAMu0027s 1st stage facing a bottleneck on its 2nd stage. Detecting covert information in images by means of steganalysis techniques has become increasingly necessary due to the amount of data being transmitted mainly through the Internet. However, these techniques are computationally expensive and not much attention has been paid to reduce their cost by means of available parallel computational platforms. This article presents two computational models for the Subtractive Pixel Adjacency Model (SPAM) which has shown the best detection rates among several assessed steganalysis techniques. A hardware architecture tailored for reconfigurable fabrics is presented achieving high performance and fulfilling hard real-time constraints. On the other hand, a parallel computational model for the CUDA architecture is also proposed. This model presents high performance during the first stage but it faces a bottleneck during the second stage of the SPAM process. Both computational models are analyzed in detail in terms of their algorithmic structure and performance results. To the best of Authorsu0027 knowledge these are the first design proposals to accelerate the SPAM model calculation.	cuda;coefficient;computation;computational model;fastest;field-programmable gate array;graphics processing unit;parallel computing;pixel;real-time computing;real-time locating system;steganography;thrust	Marisol Rodriguez-Perez;Alicia Morales-Reyes;René Cumplido;Claudia Feregrino Uribe	2015	Computers & Electrical Engineering	10.1016/j.compeleceng.2015.01.004	spam;computer vision;simulation;steganalysis;computer science;artificial intelligence;theoretical computer science;operating system;machine learning;algorithm;field-programmable gate array	AI	2.8833183063854326	44.351181458777496	108119
33c902d80c8773a4e5e116e727cf831fd19954e5	adaptive manycore architectures for big data computing		This work presents a cross-layer design of an adaptive manycore architecture to address the computational needs of emerging big data applications within the technological constraints of power and reliability. From the circuits end, we present links with reconfigurable repeaters that allow single-cycle traversals across multiple hops, creating fast single-cycle paths on demand. At the microarchitecture end, we present a router with bi-directional links, unified virtual channel (VC) structure, and the ability to perform self-monitoring and self-configuration around faults. We present our vision for self-aware manycore architectures and argue that machine learning techniques are very appropriate to efficiently control various configurable on-chip resources in order to realize this vision. We provide concrete learning algorithms for core and NoC reconfiguration; and dynamic power management to improve the performance, energy-efficiency, and reliability over static designs to meet the demands of big data computing. We also discuss future challenges to push the state-of-the-art on fully adaptive manycore architectures.	algorithm;big data;machine learning;manycore processor;microarchitecture;multi-core processor;network on a chip;power management;router (computing);self-awareness;tree traversal;virtual channel	Janardhan Rao Doppa;Ryan Gary Kim;Mihailo Isakov;Michel A. Kinsy;Hyoukjun Kwon;Tushar Krishna	2017		10.1145/3130218.3130236	control reconfiguration;parallel computing;computer science;architecture;real-time computing;power management;dynamic demand;big data;router;microarchitecture	Arch	2.8866883572182243	44.087315181721394	108233
5d2528c3b356ce94be9c59f078d2d4cd074bde9a	energy disaggregation meets heating control	heating control;energy disaggregation	Heating control is of particular importance, since heating accounts for the biggest amount of total residential energy consumption. Smart heating strategies allow to reduce such energy consumption by automatically turning off the heating when the occupants are sleeping or away from home. The present context or occupancy state of a household can be deduced from the appliances that are currently in use. In this study we investigate energy disaggregation techniques to infer appliance states from an aggregated energy signal measured by a smart meter. Since most household devices have predictable energy consumption, we propose to use the changes in aggregated energy consumption as features for the appliance/occupancy state classification task. We evaluate our approach on real-life energy consumption data from several households, compare the classification accuracy of various machine learning techniques, and explain how to use the inferred appliance states to optimize heating schedules.	machine learning;real life;smart meter;vitalism	Stephan Spiegel;Sahin Albayrak	2014		10.1145/2554850.2555088	simulation;computer science;energy accounting	HCI	1.3286826901453996	33.85716227831648	108405
0260328bbf63236c90ec54aa6469fa6c25ff845d	an fpga-based accelerator for lambdarank in web search engines	search engine;cost function;accelerator;fpga;data format;web search engine;lambdarank algorithm;web search;coarse grained;learning to rank;low power consumption;back propagation;software implementation;neural network	In modern Web search engines, Neural Network (NN)-based learning to rank algorithms is intensively used to increase the quality of search results. LambdaRank is one such algorithm. However, it is hard to be efficiently accelerated by computer clusters or GPUs, because: (i) the cost function for the ranking problem is much more complex than that of traditional Back-Propagation(BP) NNs, and (ii) no coarse-grained parallelism exists in the algorithm. This article presents an FPGA-based accelerator solution to provide high computing performance with low power consumption. A compact deep pipeline is proposed to handle the complex computing in the batch updating. The area scales linearly with the number of hidden nodes in the algorithm. We also carefully design a data format to enable streaming consumption of the training data from the host computer. The accelerator shows up to 15.3X (with PCIe x4) and 23.9X (with PCIe x8) speedup compared with the pure software implementation on datasets from a commercial search engine.	algorithm;artificial neural network;batch processing;computer cluster;data buffer;experiment;field-programmable gate array;gradient;graphics processing unit;host (network);learning to rank;loss function;mathematical optimization;pci express;parallel computing;relevance;sorting;speedup;streaming media;web search engine	Jing Yan;Ning-Yi Xu;Xiongfei Cai;Rui Gao;Yu Wang;Rong Luo;Feng-hsiung Hsu	2011	TRETS	10.1145/2000832.2000837	embedded system;parallel computing;real-time computing;web search engine;computer science;backpropagation;theoretical computer science;operating system;world wide web;learning to rank;search engine;field-programmable gate array	HPC	2.1765167156315584	41.930481065577325	108438
952c170a0baac25c3c07687cbdc02759b0e3b160	a parallel resolution procedure based on connection graph	multiprocessor systems;connected graph	In this paper, we present a new approach towards a parallel resolution procedure which explores another dimension of parallelism in addition to the AND/OR formulation and special hardware constructs. The approach organizes the input clauses of a problem domain into a connection graph. The connection graph is then partitioned and each partition is worked on by a different processor of a multiprocessor system. These processors execute the resolution procedure independently on its partition, and exchange intermediate results via clause migrations. pair of literals which have the same predicate symbol and are complementary in sign. If the unification attempt between two literals succeeded, these two unifiable literals are marked by a link and the resulting MGU (the most general unifier) is used to label this link. Given the clause set of Figure l.a, the corresponding graph structure is shown in Figure 1.b. Preliminary test results and qualitative assessments of this procedure are also given.	admissible rule;central processing unit;multiprocessing;parallel computing;predicate variable;problem domain;unification (computer science)	P. Daniel Cheng;J. Y. Yuang	1987			call graph;parallel computing;directed graph;graph bandwidth;level structure;null graph;computer science;graph partition;clique-width;connectivity;theoretical computer science;voltage graph;distributed computing;graph;strength of a graph	AI	2.9803179530051858	36.93501549489942	108466
0f41f28df7608be78547c8043969320f440a2d5f	inperlys - independent personal location system	masterthesis;mems;acceleration pattern recognition;reconhecimento de padrao da aceleracao;gps;zigbee;pedestrian localization;dead reckoning;localizacao pedestre	This article describes a state of art for location systems, and presents the INPERLYS (Independent Personal Location System). INPERLYS is an outdoor and indoor location system capable of give the user ́s position in places where GPS signal fails or is not accurate enough. For that, a dead reckoning method, using sensors like Micro Electrical Mechanical System (MEMS) is used.	algorithm;communications protocol;dead reckoning;design review (u.s. government);gps signals;global positioning system;microelectromechanical systems;online and offline;sensor	Helder Ferreira;Lino Figueiredo;António Meireles	2009		10.3233/978-1-60750-481-8-93	embedded system;geography;telecommunications;cartography	Mobile	5.04910367841567	34.12742278139503	108594
794c66ab897492eb7ae7f0fc23ba62cacb4e08fa	variable interval positioning method for smartphone-based power-saving geofencing	gps variable interval positioning method smartphone based power saving geofencing technology user entrance detection predefined geographic area power consumption battery saving erroneous position measurement data position detection activation frequency positioning error terminal movement fluctuation access speed correction method false negative ratio;smart phones;position measurement batteries global positioning system power demand computational modeling accuracy energy efficiency;smart phones global positioning system;global positioning system;location based services geofencing variable interval positioning method power saving function	The objective of this research is to realize a method that detects a user's entrance to predefined geographic area so that appropriate services are provided without explicit user action. Though such geofencing technology is gathering attention due to its wide range of applications, it is a challenge to overcome trade-off between power consumption and detection accuracy. Conventional variable interval positioning methods fall short on battery saving due to difficulties such as sparse and erroneous position measurement data, and the unpredictable speed and trajectory of the terminal. In this paper, we propose a method for position detection whose activation frequency is determined by speed toward the target spot. The method is robust against positioning error and fluctuation of the terminal's movement by leveraging the access angle to the target spot. Simulation results show that the proposed access speed correction method reduces power consumption from 42% to 52% against conventional method, while holding the false negative ratio to detect the target spot to less than 5%.	geo-fence;global positioning system;quantum fluctuation;simulation;smartphone;sparse matrix;systems architecture	Tomohiro Nakagawa;Wataru Yamada;Chiaki Doi;Hiroshi Inamura;Ken Ohta;Makoto Suzuki;Hiroyuki Morikawa	2013	2013 IEEE 24th Annual International Symposium on Personal, Indoor, and Mobile Radio Communications (PIMRC)	10.1109/PIMRC.2013.6666751	embedded system;real-time computing;simulation;global positioning system;hybrid positioning system	Mobile	1.498233208731227	34.247184844871555	108751
2d6b58cd61b819becbfecd4bc8bd7a9d1508a1da	an ultra compact block cipher for serialized architecture implementations	cmos integrated circuits;serialized architecture;clocks;block cipher;hardware implementation cryptography block cipher asic;multiplexing;hardware components reuse;hardware components reuse ultra compact block cipher serialized architecture puffin2 cmos technologies;logic gates;registers;application specific integrated circuits;cryptography;ultra compact;cmos technologies;puffin2;schedules;cryptography hardware cmos technology application specific integrated circuits computer architecture design engineering application software smart cards rfid tags energy consumption;asic;cryptography application specific integrated circuits cmos integrated circuits;hardware implementation;ultra compact block cipher;hardware	In this paper, we present a new block cipher, referred as PUFFIN2, that is designed to be used with applications requiring very low circuit area. PUFFIN2 is designed to be implemented exclusively with CMOS technologies and in a serialized architecture, so that the maximum reuse of hardware components is achieved resulting in a very compact implementation. PUFFIN2 has a block size of 64 bits and a key size of 80 bits. Compared with a serialized implementation of cipher PRESENT, which has the same block size and key size and is claimed as the smallest practical block cipher implementation to date, our cipher has 16% fewer gates using the same CMOS technology. Further, PUFFIN2 inherently supports both encryption and decryption while the serialized PRESENT is an encryption-only implementation.	64-bit computing;block cipher;block size (cryptography);cmos;cryptography;encryption;key size	Cheng Wang;Howard M. Heys	2009	2009 Canadian Conference on Electrical and Computer Engineering	10.1109/CCECE.2009.5090296	substitution-permutation network;embedded system;block cipher;triple des;electronic engineering;real-time computing;residual block termination;ciphertext stealing;block cipher mode of operation;computer science;operating system;stream cipher;application-specific integrated circuit;cbc-mac;statistics	EDA	8.99862476205378	45.196954606215094	109074
6c5a37e8df1c19398ff94a5d55ad15869721db3d	isobar preconditioner for effective and high-throughput lossless data compression	scientific application;database system;arrays throughput standards data models equations probability distribution noise;standards;data compression;scientific data;data model;arrays;natural sciences computing data compression data handling entropy;file system;probability distribution;lossless data compression;entropy;data handling;high throughput;natural sciences computing;in situ orthogonal byte aggregate reduction compression methodology isobar preconditioner high throughput lossless data compression data handling exascale scientific application database system flops processor file system general purpose loss less compression framework bzlib2 scientific data set compression hard to compress dataset entropic content;high speed;noise;data models;throughput	Efficient handling of large volumes of data is a necessity for exascale scientific applications and database systems. To address the growing imbalance between the amount of available storage and the amount of data being produced by high speed (FLOPS) processors on the system, data must be compressed to reduce the total amount of data placed on the file systems. General-purpose loss less compression frameworks, such as zlib and bzlib2, are commonly used on datasets requiring loss less compression. Quite often, however, many scientific data sets compress poorly, referred to as hard-to-compress datasets, due to the negative impact of highly entropic content represented within the data. An important problem in better loss less data compression is to identify the hard-to-compress information and subsequently optimize the compression techniques at the byte-level. To address this challenge, we introduce the In-Situ Orthogonal Byte Aggregate Reduction Compression (ISOBAR-compress) methodology as a preconditioner of loss less compression to identify and optimize the compression efficiency and throughput of hard-to-compress datasets.	aggregate function;algorithm;byte;central processing unit;contour line;data compression;database;flops;general-purpose markup language;general-purpose modeling;lossless compression;preconditioner;throughput;zlib	Eric R. Schendel;Ye Jin;Neil Shah;Jackie Chen;C S Chang;Seung-Hoe Ku;Stéphane Ethier;Scott Klasky;Robert Latham;Robert B. Ross;Nagiza F. Samatova	2012	2012 IEEE 28th International Conference on Data Engineering	10.1109/ICDE.2012.114	data compression;lossy compression;probability distribution;high-throughput screening;data modeling;entropy;data compression ratio;throughput;parallel computing;data model;image compression;computer science;noise;theoretical computer science;group method of data handling;data mining;database;lossless compression;statistics;data	DB	1.2353539353354963	38.891030386071776	109092
0116ff186b0d2efa8c1f8613fcce5a2d2de9d8f0	heterogeneous implementation of ecg encryption and identification on the zynq soc	aes decipher ecg encryption ecg identification zynq soc human identification ecg signal decryption advanced encryption standard ecg biometrics xilinx zc702 zynq based prototyping board ip core high level synthesis;software;system on chip cryptography electrocardiography medical signal processing;encryption;biometrics access control;electrocardiography;electrocardiography hardware encryption ciphers table lookup software biometrics access control;ciphers;table lookup;article;hardware	This paper presents an innovative and safe connected health solution for human identification. The system consists of the encryption and decryption of ECG signals using the advanced encryption standard (AES) as well as the recognition of individuals based on ECG biometrics. Heterogeneous and efficient implementation of the proposed system has been performed on a Xilinx ZC702 Zynq based prototyping board. Various IP-cores have been created based on the high level synthesis (HLS) implementation of the AES cipher, AES decipher and ECG identification blocks. The proposed hardware implementation has shown promising results since it met the real-time requirements and outclassed current field programmable gate array (FPGA) based systems in multiple key metrics including power consumption, processing time and hardware resources usage. The implemented system needs 10.71 ms to process one ECG sample and consumes 107mW while using only 30% of all available on-chip resources.	biometrics;cipher;cryptography;encryption;field-programmable gate array;high-level programming language;high-level synthesis;real-time clock;requirement	Amine Ait Si Ali;Xiaojun Zhai;Abbes Amira;Faycal Bensaali;Naeem Ramzan	2016	2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)	10.1109/FCCM.2016.44	embedded system;encryption software;computer hardware;computer science;computer security;encryption	EDA	6.740344060206791	43.47417897039337	109104
5f90f0dfeccf85b58f2c11b7d3df06e14de751fa	foreword to the special section on reconfigurable computing		In the past two decades, reconfigurable computing has evolved from a niche design technology to a mainstream computing technology, as testified by its use in datacenters and its availability in cloud computing infrastructures. The main reason is that thanks to its flexibility, reconfigurable hardware can serve many different application domains, from arithmeticintensive digital signal-processing applications to narrow niche applications, such as enumerative combinatorics. This special section of the Springer’s Journal of Signal Processing Systems (JSPS) illustrates the diversity in the field of reconfigurable computing with its five papers describing application-specific FPGA accelerators that span a wide spectrum of applications. The five papers presented in this special section are briefly summarized below. The first paper, BA Flexible, High-Performance FPGA Implementation of a Feed-Forward Equalizer for Optical Interconnects up to 112 Gb/s^ by Maragos et al. (10.1007/s11265-016-1201-y) , presents a highperformance FPGA-based architecture aimed at supporting optical interconnects in datacenters. Specifically, the work focuses on the problem of implementing the digital equalization stage, which is addressed through the use of a highly parallel implementation of a feed-forward equalizer (FFE). The results show that links up to 112 Gbps can be supported on state-ofthe-art FPGA devices. The second paper, BAn Exploration Framework for Efficient High-Level Synthesis of Support Vector Machines: Case Study on ECG Arrhythmia Detection for Xilinx Zynq SoC^ by Tsoutsouras et al. (10.1007/s11265-017-1230-1), describes a design exploration framework for Support Vector Machine FPGA accelerators. Their approach is based on High Level Synthesis technology, and is validated on a an ECG analysis and Arrhythmia detection system. Their exploration is performed at two levels, the first one at the behavioral level to expose dataand instruction-level parallelism, and the second one at the architectural level, given the target platform memory system characteristics. The resulting accelerator has been implemented on a Zynq programmable SoC and authors show that the FPGA implementation can achieve a speedup up to 78 × . The next paper, BRImCom: Raster-order Image Compressor for Embedded Video Applications^ by Ugurdag et al. (10.1007/s11265-016-1211-9), presents a hardware solution that enables real-time compression of raster-order video streams. The approach delivers compression factors of 66% in lossless and lossy modes, and is the the first approach to reach 60 fps at Full HD with rate control. Thanks to the proposed approach, on-the-fly compression/decompression can be used for processing HD video streams, thus lowering the pressure on the memory system and leading to reductions in system cost, electromagnetic radiation and power consumption. * Steven Derrien steven.derrien@univ-rennes1.fr	business architecture;cloud computing;data compression;data rate units;equalization (communications);gigabyte;high-level synthesis;instruction-level parallelism;lossless compression;lossy compression;niche blogging;parallel computing;real-time clock;reconfigurable computing;signal processing;speaker wire;speedup;springer (tank);streaming media;support vector machine;system on a chip	Steven Derrien;Kubilay Atasu;João M. P. Cardoso;Jürgen Becker	2017	Signal Processing Systems	10.1007/s11265-017-1237-7	parallel computing;reconfigurable computing;computer science	EDA	3.255259873425577	43.67653737900932	109199
ca36cd16bde1592df54fbc38f916de532fe9d567	compact hardware architectures for blake and lake hash functions	place route performance;128 bit datapath architectures;clocks;shift register based compact hardware architectures;asic platforms;field programmable gate arrays application specific integrated circuits computer architecture cryptography;lakes;fpga platforms;standard hash algorithm;place route performance blake hash function lake hash function shift register based compact hardware architectures 128 bit datapath architectures 64 bit datapath architectures 32 bit datapath architectures fpga platforms asic platforms sha 256 standard hash algorithm;lake hash function;hardware lakes shift registers counting circuits nist application specific integrated circuits field programmable gate arrays energy consumption hydrogen logic;hardware architecture;computer architecture;application specific integrated circuits;adders;cryptography;32 bit datapath architectures;64 bit datapath architectures;shift registers;blake hash function;sha 256;hash function;power consumption;field programmable gate arrays;hardware implementation;hardware	BLAKE, one of SHA-3 candidates, and LAKE hash functions show the characteristic that the block length of the internal state is double its initial and final states, which means more registers are required for the implementation of the hash functions. In this paper, we explore shift register based compact hardware architectures for the two hash functions. This includes the 32−, 64−, and 128-bit datapath architectures for BLAKE. We provide post Place&Route performance results on both ASIC and FPGA platforms. The power consumption for each design is also given. Our results show that BLAKE has comparable performance when compared with the previous standard hash function of Whirlpool and less performance advantages over SHA-256. The results also indicate that BLAKE outperforms LAKE in the hardware implementation.	128-bit;32-bit;application-specific integrated circuit;blake (hash function);block code;computation;data rate units;datapath;field-programmable gate array;hash function;iterative method;sha-2;sha-3;shift register	Jianzhou Li;Ramesh Karri	2010	Proceedings of 2010 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2010.5537063	embedded system;double hashing;computer architecture;parallel computing;hash function;sha-2;computer science;cryptography;hardware architecture;hash array mapped trie;algorithm;swifft	Arch	9.896919869810965	44.902447738514994	109208
94f0723698f4ad4e21daf2f7fa7cc2f9bd851058	secure transport and adaptation of mc-ezbc video utilizing h.264-based transport protocols☆	biological patents;rtp srtp mane;biomedical journals;format compliance;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;video encryption;life sciences;clinical guidelines;in network adaptation;full text;scalable video coding mc ezbc;selective encryption;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Universal Multimedia Access (UMA) calls for solutions where content is created once and subsequently adapted to given requirements. With regard to UMA and scalability, which is required often due to a wide variety of end clients, the best suited codecs are wavelet based (like the MC-EZBC) due to their inherent high number of scaling options. However, most transport technologies for delivering videos to end clients are targeted toward the H.264/AVC standard or, if scalability is required, the H.264/SVC. In this paper we will introduce a mapping of the MC-EZBC bitstream to existing H.264/SVC based streaming and scaling protocols. This enables the use of highly scalable wavelet based codecs on the one hand and the utilization of already existing network technologies without accruing high implementation costs on the other hand. Furthermore, we will evaluate different scaling options in order to choose the best option for given requirements. Additionally, we will evaluate different encryption options based on transport and bitstream encryption for use cases where digital rights management is required.	6-thioinosine-5'-triphosphate;acclimatization;baseline (configuration management);bitstream;cpu (central processing unit of computer system);central processing unit;clients;codec;confidentiality;cryptography;data compression;digital item;digital rights management;digital video;encryption;h.264/mpeg-4 avc;header of a document;image scaling;mpeg-21;multicast;multimedia;overhead (computing);protocols documentation;requirement;scalability;secure real-time transport protocol;streaming media;test scaling;uniform memory access;user-managed access;video coding format;wavelet	Hermann Hellwagner;Heinz Hofbauer;Robert Kuschnig;Thomas Stütz;Andreas Uhl	2012		10.1016/j.image.2011.11.002	text mining;medical research;telecommunications;computer science;multimedia;world wide web	Networks	8.177650900035523	36.68109615564983	109464
9c29797f99785f63f7632fd40337cb4a0c16aadb	power optimization through peripheral circuit reusing integrated with loop tiling for rram crossbar-based cnn		Convolutional neural networks (CNNs) have been proposed to be widely adopted to make predictions on a large amount of data in modern embedded systems. Prior studies have shown that convolutional computations which consist of numbers of multiply and accumulate (MAC) operations, serve as the most computationally expensive portion in CNN. Compared to the manner of executing MAC operations in GPU and FPGA, CNN implementation in the RRAM crossbar-based computing system (RCS) demonstrates the outstanding advantages of high performance and low power. However, the current design is energy-unbalanced among the three parts of RRAM crossbar computation, peripheral circuits and memory accesses, the latter two factors can significantly limit the potential gains of RCS. Addressing the problem of high power overhead of peripheral circuits in RCS, the Peripheral Circuit Unit (PeriCU)-Reuse scheme has been proposed to meet given power budget. In this paper, it is further observed that memory accesses can be bypassed if two adjacent layers are assigned in different PeriCUs. In this way, memory accesses can be reduced and thus the performance and power can be improved. A loop tiling technique is proposed to save memory accesses. The experiments of two convolutional applications validate that the proposed loop tiling technique can reduce energy consumption by 61.7%.	analysis of algorithms;artificial neural network;computation;convolutional neural network;crossbar switch;embedded system;experiment;field-programmable gate array;graphics processing unit;mathematical optimization;overhead (computing);peripheral;power optimization (eda);resistive random-access memory;tiling window manager;unbalanced circuit	Yuanhui Ni;Weiwen Chen;Wenjuan Cui;Yuanchun Zhou;Keni Qiu	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342193	computer science;real-time computing;convolutional neural network;parallel computing;field-programmable gate array;power budget;crossbar switch;electronic circuit;power optimization;loop tiling;resistive random-access memory	EDA	3.664032112991282	43.34804971887994	109698
bbc7de3a5a8cb1ebbd71a840453800fd0541bc15	hybrid mpi/openmp strategy for biological multiple sequence alignment with dialign-tx in heterogeneous multicore clusters	dialign tx iterative heuristic method;dynamic programming;28 core heterogeneous cluster;multicore processing resource management bioinformatics biology program processors message passing;multiprocessing systems bioinformatics computational complexity dynamic programming iterative methods message passing;heterogeneous cluster;heuristic method;resource management;heterogeneous multicore clusters;biological multiple sequence alignment problem;dynamic program;biology;system performance;mpi openmp strategy;iterative methods;message passing interface;computational complexity;master slave parallel strategy;multicore processing;message passing;multiple sequence alignment;multiprocessing systems;dynamic programming mpi openmp strategy message passing interface biological multiple sequence alignment problem heterogeneous multicore clusters bioinformatics np complete problem master slave parallel strategy 28 core heterogeneous cluster dialign tx iterative heuristic method;program processors;np complete problem;bioinformatics	Multiple Sequence Alignment (MSA) is a fundamental problem in Bioinformatics that aims to align more than two biological sequences in order to emphasize similarity regions. This problem is known to be NP-Complete, so heuristic methods are used to solve it. DIALIGN-TX is an iterative heuristic method for MSA that is based on dynamic programming and generates alignments by concatenating ungapped regions with high similarity. This paper proposes an MPI/OpenMP master/slave parallel strategy to run DIALIGN-TX in heterogeneous multicore clusters, with multiple allocation policies. The results obtained in a 28-core heterogeneous cluster with real sequence sets show that the execution time can be drastically reduced. Also, we show that an appropriate choice of the allocation policy and the master node has great impact on the overall system performance.	align (company);bioinformatics;concatenation;dialign-tx;dynamic programming;heuristic;iterative method;master/slave (technology);message passing interface;multi-core processor;multiple sequence alignment;np-completeness;openmp;run time (program lifecycle phase)	Emerson de Araujo Macedo;Alba Cristina Magalhaes Alves de Melo;Gerson Henrique Pfitscher;Azzedine Boukerche	2011	2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum	10.1109/IPDPS.2011.169	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-0.7211956515951342	42.351380340072105	110048
a201b2962d20effbfaf833ca6960c372ead6a163	semi-two-dimensional partitioning for parallel sparse matrix-vector multiplication	electronic mail;standards;computational load balance semi 2d partitioning parallel sparse matrix vector multiplication sparse matrix partitioning scheme parallelization sparse matrix vector multiply operations distributed memory systems partitioning schemes computation organization communication organization;symmetric matrices;vectors distributed memory systems matrix multiplication resource allocation;matrix partitioning;matrix partitioning sparse matrix vector multiplication;organizations;program processors sparse matrices partitioning algorithms standards symmetric matrices electronic mail organizations;program processors;sparse matrices;sparse matrix vector multiplication;partitioning algorithms	We propose a novel sparse matrix partitioning scheme, called semi-two-dimensional (s2D), for efficient parallelization of sparse matrix-vector multiply (SpMV) operations on distributed memory systems. In s2D, matrix nonzeros are more flexibly distributed among processors than one dimensional (row wise or column wise) partitioning schemes. Yet, there is a constraint which renders s2D less flexible than two-dimensional (nonzero based) partitioning schemes. The constraint is enforced to confine all communication operations in a single phase, as in 1D partition, in a parallel SpMV operation. In a positive view, s2D thus can be seen as being close to 2D partitions in terms of flexibility, and being close 1D partitions in terms of computation/communication organization. We describe two methods that take partitions on the input and output vectors of SpMV and produce s2D partitions while reducing the total communication volume. The first method obtains optimal total communication volume, while the second one heuristically reduces this quantity and takes computational load balance into account. We demonstrate that the proposed partitioning method improves the performance of parallel SpMV operations both in theory and practice with respect to 1D and 2D partitionings.	ambiguous name resolution;binary space partitioning;central processing unit;column (database);computation;distributed memory;emoticon;experiment;heuristic;heuristic (computer science);input/output;load balancing (computing);parallel computing;rendering (computer graphics);semiconductor industry;sparse matrix	Enver Kayaaslan;Bora Uçar;Cevdet Aykanat	2015	2015 IEEE International Parallel and Distributed Processing Symposium Workshop	10.1109/IPDPSW.2015.20	parallel computing;sparse matrix;computer science;organization;theoretical computer science;sparse approximation;distributed computing;symmetric matrix	HPC	-1.6820914033098504	39.03169493899227	110096
adbe5a93ffd4724e52b2c3b2b31e5a97b953e893	event-based communication for iot networking	telecommunication power management data communication energy consumption internet of things;monitoring optimization mathematical model computational modeling energy consumption feature extraction data mining;control internet of things event triggering monitoring optimization;internet of things;monitoring;control;optimization;event triggering;overall energy consumption minimization internet of things iot networking event based communication operational efficiency challenge information exchange data driven event triggering technique	In the Internet of Things (IoT), smart devices embedded on a myriad of different objects will enable monitoring, control, and optimization with potentially transformative impact to the society. Evidently, operational efficiency has become an essential factor in the proliferation of these devices since higher efficiency will prolong device lifetime and achieve greater autonomy. In this work, the operational efficiency challenge is being looked at from the device point of view as well as from the networking side of things in which local and remote hosts need to exchange information for monitoring, control, and optimization functionalities. In the former case, a data-driven event triggering technique is developed to minimize the interaction between devices, while in the latter case an event-based communication strategy is investigated to minimize the overall energy consumption of the network. Both experimental results and results for a real-world application are presented to demonstrate the many-fold operational efficiency gains that can be realized using the proposed solution.	embedded system;internet of things;mathematical optimization;point of view (computer hardware company);smart device	Panayiotis Kolios;Christoforos Panayiotou;Georgios Ellinas;Marios M. Polycarpou	2015	2015 IEEE 2nd World Forum on Internet of Things (WF-IoT)	10.1109/WF-IoT.2015.7389076	real-time computing;simulation;human–computer interaction;computer science;computer security;internet of things;scientific control;computer network	Mobile	1.0999523287099953	32.98938175043938	110115
378b865f0bb0c5b5476e1ac7920c49916547cb9a	evaluating different distributed-cyber-infrastructure for data and compute intensive scientific application	software;genomics;parallel processing big data cloud computing;big data;distributed databases;benchmark testing;big data hardware software bioinformatics genomics distributed databases benchmark testing;distributed cyber infrastructure giraph workload parallel genome assembler ceresii microbrick based hyperscale system swatiii supermikeii hpc cluster data intensive application big data analytic software;hardware;bioinformatics	Scientists are increasingly using the current state of the art big data analytic software (e.g., Hadoop, Giraph, etc.) for their data-intensive applications over HPC environment. However, understanding and designing the hardware environment that these data- and compute-intensive applications require for good performance is challenging. With this motivation, we evaluated the performance of big data software over three different distributed-cyber-infrastructures, including a traditional HPC-cluster called SuperMikeII, a regular datacenter called SwatIII, and a novel MicroBrick-based hyperscale system called CeresII, using our own benchmark Parallel Genome Assembler (PGA). PGA is developed atop Hadoop and Giraph and serves as a good real-world example of a data- as well as compute-intensive workload. To evaluate the impact of both individual hardware components as well as overall organization, we changed the configuration of SwatIII in different ways. Comparing the individual impact of different hardware components (e.g., network, storage and memory) over different clusters, we observed 70% improvement in the Hadoop-workload and almost 35% improvement in the Giraph-workload in SwatIII over SuperMikeII by using SSD (thus, increasing the disk I/O rate) and scaling it up in terms of memory (which increases the caching). Then, we provide significant insight on efficient and cost-effective organization of these hardware components. Here, The MicroBrick-based CeresII prototype shows similar performance as SuperMikeII while giving more than 2-times improvement in performance/$ in the entire benchmark test.	apache hadoop;assembly language;benchmark (computing);big data;data center;data-intensive computing;distributed operating system;hyperscale;image scaling;input/output;low-power broadcasting;pci express;prototype;software deployment;solid-state drive	Arghya Kusum Das;Seung-Jong Park;Jae-Ki Hong;Wooseok Chang	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363750	computer science;data science;data mining;database	HPC	-4.461111505708761	45.13736075334595	110205
948e6f452a2c3a5c09a1f826332e893d2244e830	pulse-width modulation based dot-product engine for neuromorphic computing system using memristor crossbar array		The Dot-Product Engine (DPE) is a critical circuit for implementing neural networks in hardware. The recent-developed memristor crossbar array technology, which is able to efficiently carry out dot-product multiplication and update its weights in real time, has been considered as one of the viable technologies to build a high-efficient neural network computing system. In this paper, the Pulse-Width-Modulation (PWM) based DPE has been presented and analyzed. Here, the PWM based signal, instead of the traditional amplitude modulated (AM) signal, is used as the computation variable. Comparing to the existing AM based system, this PWM counterpart provides an alternative approach to reduce the power consumption and chip area of its peripheral circuits. Power and area saving becomes more prominent when the size and/or the number of arrays increase. This new approach also provides the critically needed scalability to accommodate the computation variable with higher precision. In this paper, a 4-bit (can be easily expanded to 8-bit) feed forward neural network with 3-bit weights (memristor's conductance) is constructed using the proposed PWM DPE to identify digits from the MNIST data set. The circuit system is implemented in 130 nm standard CMOS technology. The entire circuit system consumes about 53mW with more than 86% recognition accuracy in average.	4-bit;8-bit;artificial neural network;cmos;computation;conductance (graph);crossbar switch;mnist database;memristor;network computing system;neuromorphic engineering;peripheral;pulse-width modulation;scalability	Hao Jiang;Fanchen Luo;Dr. Susanne Buder;Thomas Kwok;Fu Luo;Qing Yang;Xiaorong Zhang;J. Joshua Yang;Qiangfei Xia;Yiran Chen;Hai Li;Qing Wu;Mark Barnell	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351276	memristor;electronic engineering;crossbar switch;electronic circuit;scalability;artificial neural network;computer science;mnist database;pulse-width modulation;neuromorphic engineering	EDA	4.809269562626925	42.05344280925909	110478
aa88e8e5814f30c148ecd4ffcbd0281014be5f71	design of i/o efficient, scalable array processors for multi-dimensional dft.	multi dimensional			Shietung Peng;Stanislav G. Sedukhin;Hiroshi Nagata	1999			computer science	Arch	8.058700315248961	39.39767444703105	110589
cedd4c8fc958613a3b80ba9f25cdb8f42c28359c	accelerating hmmer search using fpga	databases;hmmer search;dynamic programming;quadratic programming;software tool;feedback path;optimization technique;search space;hidden markov model;protein sequence;dynamic programming algorithm;memory banks hmmer search fpga hidden markov model field programmable gate arrays software tool viterbi algorithm quadratic dynamic programming algorithm protein sequence feedback path profile hmm quadratic search space;fpga;quadratic dynamic programming algorithm;memory banks;hidden markov models;proteins;viterbi algorithm;mathematical model;software tools;profile hmm;acceleration hidden markov models field programmable gate arrays databases viterbi algorithm feedback software tools dynamic programming heuristic algorithms protein sequence;field programmable gate arrays;quadratic search space;software tools dynamic programming field programmable gate arrays hidden markov models quadratic programming;parallel processing	This paper describes an implementation of HMMER with FPGA. HMMER is one of the most used software tools for sensitive profile HMM (Hidden Markov Model) searches of biological sequence databases. HMMER is a very cpuintensive program. In HMMER, the Viterbi algorithm, which is a quadratic dynamic programming algorithm, is used to align a profile HMM and a protein sequence. In the profile HMM, a feedback path from the end of the model to the beginning is allowed, and this loop makes it difficult to process the Viterbi algorithm in parallel. In our approach, the alignment is calculated speculatively in parallel, and when the feedback path is selected in the alignment, the alignment is recalculated from the beginning using the fedback score. According to our experiments, the ratio that the feedback path is selected is very low, and the performance loss by the recalculation is less than a few percent. Another problem for accelerating HMMER using FPGA is the large size of the score tables required for profile HMMs. By crossing the search direction in the quadratic search space and the moving direction of each processing unit in the search space, we can minimize the size of the memory banks for storing the score tables. This optimization technique makes it possible to process all profile HMMs in a database efficiently.	align (company);database;dynamic programming;experiment;field-programmable gate array;hmmer;hidden markov model;markov chain;mathematical optimization;memory bank;viterbi algorithm	Toyokazu Takagi;Tsutomu Maruyama	2009	2009 International Conference on Field Programmable Logic and Applications	10.1109/FPL.2009.5272276	embedded system;mathematical optimization;computer science;theoretical computer science;machine learning;dynamic programming;hidden markov model;field-programmable gate array	SE	0.890382885480487	42.99626402534755	111170
5bcf8b9218fcd9dc0493912447b8c1c6cb4a371e	a 4096-neuron 1m-synapse 3.8pj/sop spiking neural network with on-chip stdp learning and sparse weights in 10nm finfet cmos		A 4096-neuron, 1M-synapse SNN in 10nm FinFET CMOS achieves a peak throughput of 25.2GSOP/s at 0.9V, peak energy efficiency of 3.8pJ/SOP at 525mV, and $2.3\mu \text{W}$ /neuron operation at 450mV. The SNN skips zero-valued activations for up to $9.4\times$ lower energy. Fine-grained sparse weights reduce memory by up to $16\times$. On-chip STDP trains RBMs to de-noise MNIST digits and to reconstruct natural scene images with RMSE of 0.036. A 50% sparse weight MLP classifies MNIST digits with 97.9% accuracy at $1.7\mu \text{J}$ /classification.	cmos;mnist database;neuron;quad flat no-leads package;sparse matrix;spiking neural network;synapse;throughput	Gregory K. Chen;Raghavan Kumar;H. Ekin Sumbul;Zhiqiang Xiao;Ram Krishnamurthy	2018	2018 IEEE Symposium on VLSI Circuits	10.1109/VLSIC.2018.8502423	system on a chip;throughput;computer science;synapse;parallel computing;chip;spiking neural network;mnist database;cmos	Arch	3.9807411512886106	42.75511222355089	111294
1df5dc304aac6846cb0ad1e4ce2bdbebd826c9a0	large page address mapping in massive parallel processor systems	fpga;small world;binary search;massive parallel processing;address mapping	Large and sparse are the prominent characteristics of small-world graph. In many scientific domains, such as biomedical science and scientific computing, as small-word graph grows in scale, processing small-world graph poses severe challenges to address mapping in Massive Parallel Processing. Data driven computation, unstructured data organization, which are poor in spatial and temporal locality, which are high frequency of memory access, leads to large RAM footprint on address mapping management. This paper proposes a novel approach with special Implementation for massive parallel processors. In our technique, The block level address mapping table is stored in large pages in DDR3 memory. Considering the highly frequency in accessing memory, we maintain a big cache in RAM to store address mapping entries of data array recently searching. The search algorithm in searching the cache is binary search. The goal is to reduce address mapping overhead without excessively compromising system response time. This scheme is designed for our massive parallel coprocessor system. For reducing power consumption, we have an attempt to implement address mapping of each massive parallel coprocessor in Field-Programmable Gate Array(FPGA). The experiment have been conducted on a real System on chip(Soc). The result shows that when the number of processor node is 4096 and its frequency is 233MHz, The RAM cost is 2.4 MB in each processor, when there is missing, the largest response time is 160us, which is less than the mainstream software implementation in address translation. In the case of making full use of available storage resources, The hit ratio in graph problem could be achieve 100%.	associative entity;binary search algorithm;central processing unit;computation;computational science;coprocessor;graph theory;hit (internet);locality of reference;overhead (computing);page (computer memory);parallel computing;random-access memory;response time (technology);sparse matrix	Yichun Sun;Xiaodong Yi;Hengzhu Liu	2016	2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/ICPADS.2016.0118	parallel computing;computer science;physical address;theoretical computer science;operating system;massively parallel;logical address;database;distributed computing;address bus;field-programmable gate array;binary search algorithm	HPC	-1.278879126222774	43.14576371557988	111377
18c27d778e658724d3198e95bd3542e935d095a6	reconfigurable hardware implementation of a modified chaotic filter bank scheme	filter bank;encryption;embedded systems;chaotic filter banks;stream cipher;mcfb schemes;reconfigurable fpga;field programmable gate arrays;reconfigurable hardware;real time systems;chaos theory	Chaotic filter bank schemes have been proposed in the research literature to allow for the efficient encryption of data for real-time embedded systems. Some security flaws have been found in the underlying approaches which makes such a scheme unsafe for application in real life scenarios. In this paper, we first present an improved scheme to alleviate the weaknesses of the chaotic filter bank scheme, and add enhanced security features, to form a modified chaotic filter bank (MCFB) scheme. Next, we present a reconfigurable hardware implementation of the MCFB scheme. Implementation on reconfigurable hardware speeds up the performance of MCFB scheme by mapping some of the multipliers in design to reconfigurable look-up tables, while removing many unnecessary multipliers. An optimised implementation on Xilinx Virtex-5 XC5VLX330 FPGA gave a speedup of 30% over non-optimised direct implementation. A clock frequency of 88 MHz was obtained.	algorithm;clock rate;cryptanalysis;cryptography;data compression;embedded system;encryption;field-programmable gate array;filter bank;hard coding;image compression;jpeg 2000;lookup table;real life;real-time clock;reconfigurable computing;scientific literature;speedup	Amit Pande;Joseph Zambreno	2010	IJES	10.1504/IJES.2010.039028	embedded system;real-time computing;reconfigurable computing;computer science;filter bank;chaos theory;stream cipher;encryption;field-programmable gate array	EDA	8.973045594824614	45.31327376433084	111390
820797933e9303669013079a789f506700a0c4d6	deep learning challenges and solutions with xilinx fpgas		In this paper, we will describe the architectural, software, performance, and implementation challenges and solutions and current research on the use of programmable logic to enable deep learning applications. First a discussion of characteristics of building a deep learning system will described. Next architectural choices will be explained for how a FPGA fabric can efficiently solve deep learning tasks. Finally specific techniques for how DSPs, memories and are used in high performance applications will be described.	deep learning;field-programmable gate array	Elliott Delaye;Ashish Sirasao;Chaithanya Dudha;Sabya Das	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203877	computer science;field-programmable gate array;theoretical computer science;real-time computing;efficient energy use;programmable logic device;software;deep learning;approximate computing;artificial intelligence;data-flow analysis	EDA	3.223275203892177	43.871825572124706	111499
c715c671ae52d37d5a649a2500ee3dc3ee0ffbcd	stochastic-based convolutional networks with reconfigurable logic fabric (abstract only)	stochastic convolution;fpga;convolutional neural network	Large-scale convolutional neural network (CNN), well-known to be computationally intensive, is a fundamental algorithmic building block in many computer vision and artificial intelligence applications that follow the deep learning principle. This work presents a novel stochastic-based and scalable hardware architecture and circuit design that computes a convolutional neural network with FPGA. The key idea is to implement a multi-dimensional convolution accelerator that leverages the widely-used convolution theorem. Our approach has three advantages. First, it can achieve significantly lower algorithmic complexity for any given accuracy requirement. This computing complexity, when compared with that of conventional multiplierbased and FFT-based architectures, represents a significant performance improvement. Second, this proposed stochastic-based architecture is highly fault-tolerant because the information to be processed is encoded with a large ensemble of random samples. As such, the local perturbations of its computing accuracy will be dissipated globally, thus becoming inconsequential to the final overall results. Overall, being highly scalable and energy efficient, our stochastic-based convolutional neural network architecture is well-suited for a modular vision engine with the goal of performing real-time detection, recognition and segmentation of mega-pixel images, especially those perception-based computing tasks that are inherently fault-tolerant. We also present a performance comparison between FPGA implementations that use deterministic-based and Stochastic-based architectures.	analysis of algorithms;applications of artificial intelligence;artificial neural network;circuit design;computational complexity theory;computer vision;convolution;convolutional neural network;deep learning;fast fourier transform;fault tolerance;field-programmable gate array;havok vision game engine;network architecture;pixel;real-time clock;reconfigurable computing;scalability;stochastic process	Mohammed Alawad;Mingjie Lin	2016		10.1145/2847263.2847318	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;machine learning;convolutional neural network;field-programmable gate array	Vision	3.695504240389394	43.062537472053386	111624
0cbfeb00d0ef28741365611de4f8702c031c59a3	pselinv - a distributed memory parallel algorithm for selected inversion: the symmetric case	distributed memory parallel algorithm;high performance computation;electronic structure theory;sparse direct method;selected inversion	We describe an efficient parallel implementation of the selected inversion algorithm for distributed memory computer systems, which we call PSelInv. The PSelInv method computes selected elements of a general sparse matrix A that can be decomposed as A = LU, where L is lower triangular and U is upper triangular. The implementation described in this article focuses on the case of sparse symmetric matrices. It contains an interface that is compatible with the distributed memory parallel sparse direct factorization SuperLU_DIST. However, the underlying data structure and design of PSelInv allows it to be easily combined with other factorization routines, such as PARDISO. We discuss general parallelization strategies such as data and task distribution schemes. In particular, we describe how to exploit the concurrency exposed by the elimination tree associated with the LU factorization of A. We demonstrate the efficiency and accuracy of PSelInv by presenting several numerical experiments. In particular, we show that PSelInv can run efficiently on more than 4,000 cores for a modestly sized matrix. We also demonstrate how PSelInv can be used to accelerate large-scale electronic structure calculations.	central processing unit;concurrency (computer science);data structure;distributed memory;electronic structure;experiment;gene expression programming;graphics processing unit;intel core (microarchitecture);knights;lu decomposition;manycore processor;numerical analysis;parallel algorithm;parallel computing;scalability;sparse matrix;the matrix;tree (data structure);triangular matrix;xeon phi	Mathias Jacquelin;Lin Lin;Chao Yang	2016	ACM Trans. Math. Softw.	10.1145/2786977	mathematical optimization;parallel computing;computer science;theoretical computer science;distributed computing;algorithm	HPC	-3.084561437678827	38.56681684557376	111694
325c2e59e8d4fb2e85639753c8b3d4a28f9af6e7	hardware design for hash functions	one way function;cosic;cryptographic algorithm;digital signature;hardware design;hash function;message authentication;high speed;design methodology	Introduction to chapter. Due to its cryptographic and operational key features such as the one-way function property, high speed and a fixed output size independent of input size the hash algorithm is one of the most important cryptographic primitives. A critical drawback of most cryptographic algorithms is the large computational overheads. This is getting more critical since the data amount to process or communicate is dramatically increasing. In many of such cases, a proper use of the hash algorithm effectively reduces the computational overhead. Digital signature algorithm and the message authentication are the most common applications of the hash algorithms. The increasing data size also motivates hardware designers to have a throughput optimal architecture of a given hash algorithm. In this chapter, some popular hash algorithms and their cryptanalysis are briefly introduced, and a design methodology for throughput optimal architectures of MD4-based hash algorithms is described in detail.	algorithm;computation;cryptanalysis;cryptographic hash function;cryptographic primitive;cryptography;digital signature;information;iteration;linear function;md4;mathematical optimization;message authentication;nonlinear system;one-way function;overhead (computing);sha-1;sha-2;throughput	Yong Ki Lee;Miroslav Knezevic;Ingrid M. R. Verbauwhede	2010		10.1007/978-0-387-71829-3_5	message authentication code;security of cryptographic hash functions;double hashing;parallel computing;hash function;collision attack;merkle tree;sha-2;collision resistance;secure hash algorithm;theoretical computer science;secure hash standard;hash chain;hash-based message authentication code;hash buster;distributed computing;rolling hash;cryptographic hash function;fowler–noll–vo hash function;mdc-2;swifft;hash tree	Crypto	8.081136225423565	43.72992789442818	111794
d8f33d752ad567e5266ce916721b752162e051dc	architecture and design of ge1, a fccm for golomb ruler derivation	computer architecture field programmable gate arrays;control path;mathematics;fpga based custom compute engine;high end workstation;design engineering;datapath;fpga;cost performance ratio ge1 fccm golomb ruler derivation fpga based custom compute engine datapath control path functional correctness simulation results xilinx 5000 series fpga high end workstation;computer architecture;computer architecture field programmable gate arrays engines design engineering computational modeling costs mathematics radio frequency electromagnetic spectrum crystallography;computational modeling;ge1;radio frequency;engines;xilinx 5000 series fpga;functional correctness;cost performance ratio;golomb ruler derivation;fccm;field programmable gate arrays;custom;performance ratio;golomb ruler;crystallography;electromagnetic spectrum;architecture;simulation results	A new architecture for Golomb ruler derivation has been developed, and a FPGA-based custom compute engine of the new architecture has been fully designed. The new FCCM, called GE1, is presented in terms of its datapath, and control path. Portions of the GE1 have been implemented to verify functional correctness and accuracy of the simulation results. The new machine requires twenty Xilinx 5000 series FPGA’s for derivation of the 20 mark Golomb ruler, and its performance is roughly 30 times that of a high-end workstation, making its cost-performance ratio exceptionally good for derivation of new rulers.	correctness (computer science);datapath;field-programmable gate array;golomb ruler;google compute engine;microprocessor;rendering (computer graphics);simulation;vhdl;workstation	Apostolos Dollas;Euripides Sotiriades;Apostolos Emmanouelides	1998		10.1109/FPGA.1998.707880	embedded system;computer architecture;parallel computing;computer science;theoretical computer science;operating system;field-programmable gate array	Arch	7.90357052758177	43.1738190968077	111926
652de55b08eeda3292ab13057cd221c4036c80ed	main-memory requirements of big data applications on commodity server platform		The emergence of big data frameworks requires computational and memory resources that can naturally scale to manage massive amounts of diverse data. It is currently unclear whether big data frameworks such as Hadoop, Spark, and MPI will require high bandwidth and large capacity memory to cope with this change. The primary purpose of this study is to answer this question through empirical analysis of different memory configurations available for commodity server and to assess the impact of these configurations on the performance Hadoop and Spark frameworks, and MPI based applications. Our results show that neither DRAM capacity, frequency, nor the number of channels play a critical role on the performance of all studied Hadoop as well as most studied Spark applications. However, our results reveal that iterative tasks (e.g. machine learning) in Spark and MPI are benefiting from a high bandwidth and large capacity memory.	apache hadoop;big data;commodity computing;computer data storage;dynamic random-access memory;emergence;iterative method;machine learning;message passing interface;spark;server (computing)	Hosein Mohammadi Makrani;Setareh Rafatirad;Amir Houmansadr;Houman Homayoun	2018	2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)	10.1109/CCGRID.2018.00097	task analysis;memory management;big data;computer science;spark (mathematics);dram;commodity;communication channel;bandwidth (signal processing);computer architecture	HPC	-3.8332426787053726	44.92954159964997	112302
0c7b6e34f6626e3fb86f90f74da8678790fa42ab	mic-tandem: parallel x!tandem using mic on tandem mass spectrometry based proteomics data	databases;computers;microwave integrated circuits;x tandem;microwave integrated circuits proteins computer architecture mass spectroscopy computational modeling databases computers;computer architecture;proteins biology computing graphics processing units mass spectroscopy parallel processing;tandem mass spectrometry;computational modeling;proteins;protein identification mic x tandem tandem mass spectrometry;commodity hardware mic tandem parallel x tandem tandem mass spectrometry based proteomics data protein identification computational efficiency protein databases chip technology parallel computing technique multicore processor many core coprocessor multinode cluster parallel strategies intel many integrated core architecture graphics processing unit gpu;mass spectroscopy;mic;protein identification	The widespread use of mass spectrometry for protein identification has created an urgent demand for improving computational efficiency of matching mass spectrometry data to protein databases. With the rapid development of chip technology and parallel computing technique, such as multi-core processor, many-core coprocessor and cluster of multi-node, the speed and performance of the major mass spectral search engines are continuously improving. In recent ten years, X!Tandem as a popular and representative open-source program in searching mass spectral has extended several parallel versions and obtains considerable speedups. However, because these parallel strategies are mainly based on cluster of nodes, higher costs (e.g., charge of electricity and maintenance) is needed to get limited speedups. Fortunately, Intel Many Integrated Core (MIC) architecture and Graphics Processing Unit (GPU) are ideal for this problem. In this paper, we present and implement a parallel strategy to X!Tandem using MIC called MIC-Tandem, That shows excellent speedups on commodity hardware and produces the same results as the original program.	commodity computing;coprocessor;database;graphics processing unit;integrated circuit;manycore processor;multi-core processor;open-source software;parallel computing;proteomics;tandem computers;web search engine;xeon phi	Pinjie He;Keqin Li	2015	2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2015.31	tandem mass spectrometry;parallel computing;mass spectrometry;computer hardware;computer science;bioinformatics;computational model	HPC	-0.7367763866209023	42.729118012812975	112688
6cfe6e0bc745cb6aa86c116526a77f499b565800	a fast sat solver algorithm best suited to reconfigurable hardware	design automation;search space;satisfiability;cnf;sat;reconfigurable architecture;formal verification;davis putnam;sat solver;data structure;reconfigurable hardware;dpll;software evaluation	The majority of the existing reconfigurable hardware SAT solvers employ some variation of the Davis-Putnam algorithm; we propose a new algorithm for organizing the search in SAT Solvers best suited to reconfigurable architectures due to its vector-like operations. Its essence is to view each negated clause as a cube in the n-dimensional Boolean search space, and realizing that each of these clause-cubes denotes a sub-region of the search space where no satisfying assignments can be found. Starting from the universal cube, which represents the whole space, we systematically subtract all clause-cubes until we end up with a satisfying cube or an empty cube if the SAT formula is unsatisfiable. The algorithm for cube subtraction is the D-Sharp algorithm. We implemented this strategy in the well-known zChaff SAT solver. Improvements in execution time and number of aborted instances have been observed for the new algorithm. The test suite includes several instances from IBM-CNF BMC and Microprocessor's Formal Verification benchmarks. Given the breadth of the experimental software evaluation, we claim that D-Sharp subtraction search is an effective algorithm for improving the performance of SAT solvers and due to its data structure and bit-to-bit operations it is very well suited to reconfigurable hardware implementations. Design Automation (EDA).	automation;boolean algebra;boolean satisfiability problem;chaff algorithm;conjunctive normal form;data structure;davis–putnam algorithm;field-programmable gate array;formal verification;intelligent platform management interface;microprocessor;olap cube;organizing (structure);putnam model;reconfigurable computing;run time (program lifecycle phase);solver;test suite;whole earth 'lectronic link	Romanelli Lodron Zuim;José T. de Sousa;Claudionor José Nunes Coelho	2006		10.1145/1150343.1150380	parallel computing;data structure;formal verification;computer science;theoretical computer science;boolean satisfiability problem;programming language;algorithm	EDA	2.1490950147368846	40.63142859849463	113107
fb8d73edad3569eee85f98c0601317d980317893	flexible composite galois field gf((2^m)^2) multiplier designs		Composite Galois Field (GF((2^m)^n)) multiplications denote the multiplication with extension field over the ground field (GF(2^m)), that are used in cryptography and error correcting codes. In this paper, composite versatile and vector (GF((2^m)^2)) multipliers are proposed. The proposed versatile (GF((2^m)^2)) multiplier design is used to perform the (GF((2^x)^2)) multiplication, where (2le xle m). The proposed vector (GF((2^m)^2)) multiplier design is used to perform (2^k) numbers of (GF((2^{frac{m}{2^k}})^2)) multiplications in parallel, where throughput is comparatively higher than other designs and (kin {0, 1, ...(log_{2}m)-1) }). In both the works, the hardware cost is the trade-off while the flexibility is high. The proposed and existing multipliers are synthesised and compared using 45 nm CMOS technology. The throughputs of the proposed parallel and serial vector (GF((2^8)^2)) multipliers are (72.7%) and (53.62%) greater than Karatsuba based multiplier design [11] respectively.		M. Mohamed Asan Basiri;Sandeep K. Shukla	2017		10.1007/978-981-10-7470-7_1	electronic engineering;ground field;algebra;m.2;karatsuba algorithm;multiplication;gf(2);galois theory;computer science;composite number	HCI	9.802432021828633	44.41358723137368	113166
98c877d9094e6516adbfbe6a8c6b035d685abe1b	fpgas in the datacenter: combining the worlds of hardware and software development	catapult;configurable cloud;smartnic	The Catapult project has brought the power and performance of FPGA-based reconfigurable computing to Microsoft's hyperscale datacenters, accelerating major production cloud applications such as Bing web search and Microsoft Azure, and enabling a new generation of machine learning and artificial intelligence applications. Catapult is now deployed in nearly every new server across the more than a million machines that make up the Microsoft hyperscale cloud.  The presence of ubiquitous and programmable silicon in the datacenter ushers in a new era where the discipline and rigor of the VLSI community are combining with the speed and agility of the software community to form new opportunities in a blend development styles and techniques.  In this talk, I will describe the next generation of the Catapult configurable cloud architecture, and the tools and techniques that have made Catapult successful to date. I will also discuss areas where traditional hardware and software development flows fall short, and ways in which the VLSI community can branch into new opportunities in software and computing.	applications of artificial intelligence;cloud computing;data center;field-programmable gate array;hyperscale;machine learning;microsoft azure;next-generation network;reconfigurable computing;server (computing);software development;very-large-scale integration;web search engine	Andrew Putnam	2017		10.1145/3060403.3066860	embedded system;real-time computing;simulation;engineering;catapult	Arch	0.12684569295458453	45.13588471865294	113278
99c1e3cf9257cb9c51d306cbfca2c95c49e4b8a3	asynchronous distributed genetic algorithm for optimal channel routing	distributed system;genetic operator;systeme reparti;personal computer;routing;ethernet;communicating process;circuit vlsi;algoritmo encaminamiento;routage;algoritmo genetico;proceso comunicante;systeme linux;vlsi circuit;sistema repartido;operating system;sistema linux;algorithme routage;envoi message;algorithme reparti;processus communicant;routing algorithm;message passing;algorithme genetique;distributed genetic algorithm;genetic algorithm;algoritmo repartido;circuito vlsi;algoritmo optimo;linux system;algorithme optimal;optimal algorithm;distributed algorithm;enrutamiento	This paper presents a distributed genetic algorithm for the channel routing problem in MPI environments. This system is implemented on a network of personal computers running Linux operating system connected via 10Mbps Ethernet. Each slave processor generates its own sub-population using genetic operations and communicates with the master processor in an asynchronous manner to form the global population. The experimental results show that the proposed algorithm maintains the convergence properties of sequential genetic algorithm while it achieves linear speedup as the nets of the channel routing and the number of computing processors increase.	channel router;genetic algorithm;routing	Wonil Kim;Chul-Eui Hong;Yeong-Joon Kim	2004		10.1007/978-3-540-30497-5_31	distributed algorithm;routing;parallel computing;message passing;real-time computing;genetic algorithm;computer science;destination-sequenced distance vector routing;genetic operator;distributed computing;ethernet	EDA	-2.974315221569067	33.553998635936914	113589
77e930ca59a363dfd02e6629335636849d8088d0	embedded harmonic control for trajectory planning in large environments	optimisation;random access memory;convergence;trajectory planning;robot navigation;mobile robots;fpga;navigation trajectories;trajectory navigation embedded computing relaxation methods concurrent computing distributed computing field programmable gate arrays computer architecture robots digital circuits;embedded system;embedded harmonic control;computer architecture;embedded systems;robot navigation embedded harmonic control trajectory planning fpga based architecture navigation trajectories optimization;navigation;trajectory;position control;robots;fpga based architecture;optimization;position control embedded systems field programmable gate arrays mobile robots navigation optimisation;field programmable gate arrays;harmonic control;embedded systems harmonic control trajectory planning fpga;harmonic analysis	This paper presents an embedded FPGA-based architecture to compute navigation trajectories along a harmonic potential. The goals and obstacles may be changed during computation. Large environments are split into blocks. This approach, together with the use of an increasing precision, enables an optimization of the overall computation time that is theoretically and experimentally studied. Implementation results confirm outstanding speedup factors.	computation;embedded system;experiment;field-programmable gate array;mathematical optimization;quantum harmonic oscillator;speedup;time complexity	César Torres-Huitzil;Bernard Girau;Amine M. Boumaza;Bruno Scherrer	2008	2008 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2008.78	embedded system;real-time computing;computer science;harmonic analysis;field-programmable gate array	Robotics	5.332052626893066	44.739353533061255	113748
200c5eebcd65b0860fd1c81b81a4704131016e7e	vlsi implementation of deep neural networks using integral stochastic computing	erbium;irrigation;adders logic gates hardware very large scale integration neural networks computer architecture engines;vlsi deep neural network dnn hardware implementation integral stochastic computation machine learning pattern recognition;logic gates;adders	The hardware implementation of deep neural networks (DNNs) has recently received tremendous attention since many applications require high-speed operations. However, numerous processing elements and complex interconnections are usually required, leading to a large area occupation and a high power consumption. Stochastic computing has shown promising results for area-efficient hardware implementations, even though existing stochastic algorithms require long streams that exhibit long latency. In this paper, we propose an integer form of stochastic computation and introduce some elementary circuits. We then propose an efficient implementation of a DNN based on integral stochastic computing. The proposed architecture uses integer stochastic streams and a modified Finite State Machine-based tanh function to improve the performance and reduce the latency compared to existing stochastic architectures for DNN. The simulation results show the negligible performance loss of the proposed integer stochastic DNN for different network sizes compared to their floating point versions.	algorithm;artificial neural network;computation;deep learning;finite-state machine;simulation;stochastic computing;very-large-scale integration	Arash Ardakani;François Leduc-Primeau;Naoya Onizawa;Takahiro Hanyu;Warren J. Gross	2016	2016 9th International Symposium on Turbo Codes and Iterative Information Processing (ISTC)	10.1109/TVLSI.2017.2654298	embedded system;electronic engineering;erbium;logic gate;telecommunications;computer science;electrical engineering;theoretical computer science;operating system;machine learning;irrigation;algorithm;adder	Arch	4.510513490411055	42.850968800786845	114013
7189ce8612a8e9d93af68033231eb226c3625022	uchpc 2012: fifth workshop on unconventional high performance computing	innovative new programming model;best use;unconventional high performance computing;corresponding programming model;unconventional hardware;raw computing power;latest research;best performance;unconventional hpc;compiler technique;computational grid;good performance;computing power;conventional system;new programming approach;desktop system;low cost;computer and information science	"""As the word \""""UnConventional\"""" in the title suggests, the workshop focuses on hardware and platforms for HPC, which were not intended for HPC in the first place. Reasons could be raw computing power, good performance per watt, or low cost in general. To address this unconventional hardware, often, new programming approaches and paradigms are required to make best use of it. Thus, a second focus of the workshop - for the first time with UCHPC 2012 - is on innovative new programming models for unconventional hardware and how to best combine its computing power with more conventional systems."""	unconventional computing	Anders Hast;Josef Weidendorfer;Jan-Philipp Weiss	2012		10.1007/978-3-642-36949-0_58		HPC	-2.151626382924056	46.00573872836661	114040
63a7276b1ee11009ff420e8b55e28ff35443e579	an fpga implementation of the hestenes-jacobi algorithm for singular value decomposition	singular value decomposition;hestenes jacobi algorithm architecture fpga singular value decomposition;fpga;singular value decomposition computational complexity field programmable gate arrays graphics processing units jacobian matrices;hestenes jacobi algorithm;medium column dimensions fpga singular value decomposition svd dimensionality reduction computational complexity signal processing data analysis systems jacobi rotation hestenes jacobi algorithm arbitrary rectangular matrices gpu based implementations floating point hestenes jacobi architecture arbitrary sized matrices;jacobian matrices vectors computer architecture matrix decomposition field programmable gate arrays covariance matrices algorithm design and analysis;architecture	As a useful tool for dimensionality reduction, Singular Value Decomposition (SVD) plays an increasingly significant role in many scientific and engineering applications. The high computational complexity of SVD poses challenges for efficient signal processing and data analysis systems, especially for time sensitive applications with large data sets. While the emergence of FPGAs provides a flexible and low-cost opportunity to pursue high-performance SVD designs, the classical two-sided Jacobi rotation-based SVD architectures are restricted in terms of scalability and input matrix representation. The Hestenes-Jacobi algorithm offers a more parallelizable solution to analyze arbitrary rectangular matrices, however, to date both FPGA and GPU-based implementations have not lived up to the algorithm's potential. In this paper, we introduce a floating-point Hestenes-Jacobi architecture for SVD, which is capable of analyzing arbitrary sized matrices. Our implementation on an FPGA-based hybrid acceleration system demonstrates improved efficiency of our architecture compared to an optimized software-based SVD solution for matrices with small to medium column dimensions, even with comparably large row dimensions. The dimensional speedups can be achieved range from 3.8x to 43.6x for matrices with column dimensions from 128 to 256 and row sizes from 128 to 2048. Additionally, we also evaluate the accuracy of our SVD process through convergence analysis.	algorithm;computational complexity theory;dimensionality reduction;double-precision floating-point format;emergence;field-programmable gate array;graphics processing unit;ibm notes;jacobi method;jacobi rotation;latent semantic analysis;matrix representation;principal component analysis;scalability;signal processing;singular value decomposition	Xinying Wang;Joseph Zambreno	2014	2014 IEEE International Parallel & Distributed Processing Symposium Workshops	10.1109/IPDPSW.2014.29	mathematical optimization;parallel computing;computer science;theoretical computer science;architecture;singular value decomposition	HPC	-1.6234808207347329	40.31851020627962	114224
bdf116b5672cd3f126b9fe0d793415c968d82bd1	extending psblas to build parallel schwarz preconditioners	linear algebra;distributed memory;preconditionnement;distributed system;iterative method;algoritmo paralelo;evaluation performance;raisonnement base sur cas;operator algebra;razonamiento fundado sobre caso;systeme reparti;settore ing ind 13 meccanica applicata alle macchine;parallel algorithm;performance evaluation;memoria compartida;evaluacion prestacion;machine parallele;fluid flow;distributed computing;data exchange;preconditioning;motor explosion interna;linear system;algorithme parallele;metodo iterativo;programming model;matrice creuse;sistema repartido;moteur combustion interne;methode iterative;algebre lineaire;internal combustion engine;performance analysis;additive schwarz preconditioner;parallel computer;calculo repartido;algebre operateur;algebra lineal;parallel machines;precondicionamiento;fortran;sparse matrix;case based reasoning;memoire repartie;iteration method;calcul reparti;sparse linear system;algebra operador;matriz dispersa;numerical simulation	We describe some extensions to Parallel Sparse BLAS (PSBLAS), a library of routines providing basic Linear Algebra operations needed to build iterative sparse linear system solvers on distributed-memory parallel computers. We focus on the implementation of parallel Additive Schwarz preconditioners, widely used in the solution of linear systems arising from a variety of applications. We report a performance analysis of these PSBLAS-based preconditioners on test cases arising from automotive engine simulations. We also make a comparison with equivalent software from the well-known PETSc library.	additive schwarz method;additive model;automatic parallelization;blas;computer;distributed memory;iterative method;langrisser schwarz;linear algebra;linear system;petsc;parallel computing;performance;preconditioner;simulation;sparse matrix;test case;whole earth 'lectronic link	Alfredo Buttari;Pasqua D'Ambra;Daniela di Serafino;Salvatore Filippone	2004		10.1007/11558958_71	computer science;theoretical computer science;linear algebra;calculus;mathematics;iterative method;algorithm;algebra	HPC	-3.1419411268337227	36.42409317629463	114731
9a02dc244a367e7a8792c335b1f0f316d91eebdf	hls design of a hardware accelerator for homomorphic encryption	software;encryption;field programmable gate arrays;algorithm design and analysis;hardware	Modular polynomial multiplication is the most computationally intensive operation in many homomorphic encryption schemes. In order to accelerate homomorphic computations, we propose a software/hardware (SW/HW) co-designed accelerator integrating fast software algorithms with a configurable hardware polynomial multiplier. The hardware accelerator is implemented through a High-Level Synthesis (HLS) flow. We show that our approach is highly flexible, since the same generic high-level description can be configured and re-used to generate a new design with different parameters and very large sizes in negligible time. We show that flexibility does not preclude efficiency: the proposed solution is competitive in comparison with hand-made designs and can provide good performance at low cost.	algorithm;computation;cryptosystem;hardware acceleration;high- and low-level;high-level synthesis;homomorphic encryption;polynomial ring;sampling (signal processing)	Asma Mkhinini;Paolo Maistri;Régis Leveugle;Rached Tourki	2017	2017 IEEE 20th International Symposium on Design and Diagnostics of Electronic Circuits & Systems (DDECS)	10.1109/DDECS.2017.7934578	embedded system;algorithm design;parallel computing;computer science;theoretical computer science;operating system;disk encryption hardware;encryption;field-programmable gate array	EDA	8.692641807949572	44.41460180589909	114781
17439625dcfe137b1f1457d167747134b4a1d39e	parallel visualization on large clusters using mapreduce	isosurface extraction;pattern clustering;mesh simplification;fault tolerant;image resolution;query processing;volume rendering;rendering computer graphics cloud computing data visualisation fault tolerance image resolution mesh generation parallel processing pattern clustering query processing;data processing;runtime;ease of use;data visualisation;large scale;visualization;local commodity cluster parallel visualization large clusters large scale visualization systems graphics hardware exploratory visualization systems scalable data manipulation restructuring capability querying capability core visualization algorithms computing clouds large scale data exploration large scale visualization techniques parallel data processing framework cloud computing visualization tasks mesh rendering isosurface extraction mesh simplification mapreduce programs quantitative performance realistic datasets subsequent rendering image resolutions parallel scalability computing resources fault tolerance combined data manipulation data visualization system public cloud cluster;graphics hardware;visualization technique;large meshes;gigapixels;fault tolerance;data visualization;visualization runtime;data exploration;mapreduce;rendering computer graphics;hadoop;mesh generation;visual system;gigapixels mapreduce hadoop cloud computing large meshes volume rendering;parallel processing;cloud computing	Large-scale visualization systems are typically designed to efficiently “push” datasets through the graphics hardware. However, exploratory visualization systems are increasingly expected to support scalable data manipulation, restructuring, and querying capabilities in addition to core visualization algorithms. We posit that new emerging abstractions for parallel data processing, in particular computing clouds, can be leveraged to support large-scale data exploration through visualization. In this paper, we take a first step in evaluating the suitability of the MapReduce framework to implement large-scale visualization techniques. MapReduce is a lightweight, scalable, general-purpose parallel data processing framework increasingly popular in the context of cloud computing. Specifically, we implement and evaluate a representative suite of visualization tasks (mesh rendering, isosurface extraction, and mesh simplification) as MapReduce programs, and report quantitative performance results applying these algorithms to realistic datasets. For example, we perform isosurface extraction of up to l6 isovalues for volumes composed of 27 billion voxels, simplification of meshes with 30GBs of data and subsequent rendering with image resolutions up to 800002 pixels. Our results indicate that the parallel scalability, ease of use, ease of access to computing resources, and fault-tolerance of MapReduce offer a promising foundation for a combined data manipulation and data visualization system deployed in a public cloud or a local commodity cluster.	algorithm;baseline (configuration management);beowulf cluster;cloud computing;data visualization;fault tolerance;general-purpose modeling;graphics hardware;isosurface;level of detail;mapreduce;memory management;pixel;qualitative comparative analysis;scalability;scientific visualization;usability;visual analytics;voxel	Huy T. Vo;Jonathan Bronson;Brian Summa;João Luiz Dihl Comba;Juliana Freire;Bill Howe;Valerio Pascucci;Cláudio T. Silva	2011	2011 IEEE Symposium on Large Data Analysis and Visualization	10.1109/LDAV.2011.6092321	parallel computing;computer science;theoretical computer science;data-intensive computing;parallel rendering;database	Visualization	-4.373925207010738	33.293407115402054	114956
90b0ad76095f86167544766809b113ce18a227a4	an fpga implementation of a restricted boltzmann machine classifier using stochastic bit streams	neural networks;stochastic processes boltzmann machines field programmable gate arrays handwritten character recognition real time systems;testing;rbm handwritten digit recognition fpga implementation restricted boltzmann machine classifier stochastic bit streams artificial neural networks ann software based approach real time applications;computer architecture;stochastic processes;adders;field programmable gate arrays;field programmable gate arrays computer architecture neural networks hardware stochastic processes testing adders;hardware	Artificial neural networks (ANNs) usually require a very large number of computation nodes and can be implemented either in software or directly in hardware, such as FPGAs. Software-based approaches are offline and not suitable for real-time applications, but they support a large number of nodes. FPGA-based implementations, in contrast, can greatly speedup the computation time. However, resource limitations in an FPGA restrict the maximum number of computation nodes in hardware-based approaches. This work exploits stochastic bit streams to implement the Restricted Boltzmann Machine (RBM) handwritten digit recognition application completely on an FPGA. Exploiting this approach saves a large number of hardware resources making the FPGA-based implementation of large ANNs feasible.	artificial neural network;computation;field-programmable gate array;online and offline;real-time computing;real-time transcription;restricted boltzmann machine;speedup;time complexity	Bingzhe Li;M. Hassan Najafi;David. J. Lilja	2015	2015 IEEE 26th International Conference on Application-specific Systems, Architectures and Processors (ASAP)	10.1109/ASAP.2015.7245709	stochastic process;embedded system;computer architecture;parallel computing;reconfigurable computing;computer science;theoretical computer science;operating system;machine learning;software testing;artificial neural network;algorithm;adder;field-programmable gate array	HPC	4.628103627777955	43.17747028670874	115129
9297996a97b701db2cd62c1de0cb5d0265b3cd52	digital sound projector implementation and verification within dtv embedded system	software;manuals;dtv embedded system;decoding;digital tv;real time processing;sound beams;digital television;optical projectors digital television embedded systems;embedded system;optical projectors;embedded systems;registers;digital tv embedded system application software hardware microphones loudspeakers production facilities software testing registers automatic control;digital sound projector implementation;real time processing digital sound projector implementation dtv embedded system sound beams;hardware	The Digital Sound Projector technology provides surround sound with only one speaker, by producing beams of sound. In this paper, we propose an embedded system [1] for joint DTV and Digital Sound Projector, which consists of separate DTV real-time processing and Digital Sound Projector functionality [2]. The verification is done by PC application, by which we analyzed all register’s addresses and values.	embedded system;projector;real-time locating system;surround sound	Teodora Petrovic;Dragan Samardzija	2009	2009 First IEEE Eastern European Conference on the Engineering of Computer Based Systems	10.1109/ECBS-EERC.2009.22	embedded system;digital television;computer hardware;computer science;multimedia	EDA	6.704861590899226	38.47717817937199	115189
51a445ed1b4ea845a50e863c230b7e2173c7bcc1	a 2-clock-cycle naïve bayes classifier for dynamic branch prediction in pipelined risc microprocessors	history;clocks;radiation detectors;registers;adders;pipelines;table lookup	In this paper, we propose a Bayesian branch-prediction circuit consisting of an instruction-feature extractor and a naïve Bayes classifier (NBC). Its purpose is to replace conventional branch predictors in modern pipelined RISC microprocessors. The proposed circuit is based on the conventional neural branch predictor [1]; however, the linear classifier circuit is replaced by the proposed NBC circuit. Implementing approximate Bayesian computation and its highly-parallel architectures, the NBC circuit completes branch prediction within 2 clock cycles per instruction, and is this suitable for implementation on standard pipelined microprocessors.	approximation algorithm;branch predictor;clock signal;computation;cycles per instruction;kerrison predictor;linear classifier;microprocessor;naive bayes classifier;randomness extractor	Itaru Hida;Masayuki Ikebe;Tetsuya Asai;Masato Motomura	2016	2016 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)	10.1109/APCCAS.2016.7803958	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system;pipeline transport;processor register;particle detector;algorithm;adder	EDA	5.201788156462528	43.41960139438368	115473
d2fe03f02567268ebfb15a54ad3861984546e57b	performance study of matrix computations using multi-core programming tools	linear algebra;parallel programming;matrix computations;multi core	Basic matrix computations such as vector and matrix addition, dot product, outer product, matrix transpose, matrix - vector and matrix multiplication are very challenging computational kernels arising in scientific computing. In this paper, we parallelize those basic matrix computations using the multi-core and parallel programming tools. Specifically, these tools are Pthreads, OpenMP, Intel Cilk++, Intel TBB, Intel ArBB, SMPSs, SWARM and FastFlow. The purpose of this paper is to present an unified quantitative and qualitative study of these tools for parallel matrix computations on multicore. Finally, based on the performance results with compilation optimization we conclude that the Intel ArBB and SWARM parallel programming tools are the most appropriate because these give good performance and simplicity of programming. In particular, we conclude that the Intel ArBB is a good choice for implementing intensive computations such as matrix product because it gives significant speedup results over the serial implementation. On the other hand, the SWARM tool gives good performance results for implementing matrix operations of medium size such as vector addition, matrix addition, outer product and matrix - vector product.	cilk plus;compiler;computation;computational science;intel array building blocks;mathematical optimization;matrix multiplication;multi-core processor;openmp;outer product;posix threads;parallel computing;programming tool;speedup;swarm;threading building blocks	Panagiotis D. Michailidis;Konstantinos G. Margaritis	2012		10.1145/2371316.2371353	multi-core processor;computational science;parallel computing;computer science;theoretical computer science;linear algebra	HPC	-4.308468457999482	40.867273065134626	115638
b40a7b20c6d27de4ac174a78477c8b63df436826	two improved algorithms and hardware implementations for key distribution using extended programmable cellular automata	logic circuit features;public key cryptography;diffie hellman key distribution;binary algorithm;very large scale integration;efficient algorithm;logic circuits;hardware polynomials public key cryptography principal component analysis public key very large scale integration computer science logic circuits dh hemts arithmetic;dh hemts;polynomials;public key;key exchange;cryptography;principal component analysis;vlsi;integrated logic circuits public key cryptography cellular automata vlsi table lookup;arithmetic;vlsi implementation diffie hellman key distribution diffie hellman key exchange extended programmable cellular automata hardware implementation table look up algorithm binary algorithm cryptography logic circuit features;vlsi implementation;computer science;integrated logic circuits;table lookup;cellular automata;diffie hellman key exchange;extended programmable cellular automata;high speed;cellular automaton;hardware implementation;diffie hellman;hardware;key distribution;table look up algorithm	Presents two efficient algorithms and a simple hardware structure for the implementation of Diffie-Hellman (1976) key exchange (DHKE) in GF(2/sup n/). The two algorithms investigated are an improved table look-up algorithm and a binary algorithm. The hardware structure is an extended programmable cellular automaton (PCA), which is much more flexible and can be used for key distribution and conventional cryptography. Based on our improved methods and the PCA structure, we implement two hardware systems for Diffie-Hellman key distribution. With very high-speed and simple logic circuit features, these hardware systems are ideally suited for VLSI implementation.	algorithm;cellular automaton;key distribution	Chang Nian Zhang;Ming Y. Deng;Ralph Mason	1998		10.1109/CSAC.1998.738644	cellular automaton;computer science;theoretical computer science;diffie–hellman key exchange;distributed computing;very-large-scale integration;public-key cryptography;computer security;algorithm	Theory	9.592334423493558	43.71228179233757	115652
fb994808f0a5c037001016ab44c414561244b78e	parallel gf(2n) multipliers		Operations over polynomial Galois fields GF(2n) are employed in a variety of cryptographic systems. These operations include multiplication and reduction with respect to an irreducible polynomial modulus. Fast parallel multipliers can be designed but require substantial die area. Building on prior work, two fully parallel polynomial n× n multipliers are presented with O(log2 n) latency, which use lookup tables to store modular reduction terms.	cryptography;irreducible polynomial;lookup table;modulus of continuity;most significant bit;precomputation	Trenton J. Grale;Earl E. Swartzlander	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335505	mathematical optimization;irreducible polynomial;discrete mathematics;modulus;computer science;multiplication;gf(2);finite field;logic gate;polynomial;lookup table	HPC	9.415880650085652	44.07672989740309	116103
466d5cc1bd5e9ba81fb72d2a9991d4a9c24dce13	on efficient implementation of fpga-based hyperelliptic curve cryptosystems	field programmable gate array;public key cryptosystems;public key cryptosystem;embedded system;mobile phone;finite field;algorithms implemented in hardware;cryptographic algorithm;efficient implementation;hyperelliptic curve cryptosystem;hyperelliptic curve;high performance;hardware implementation;reconfigurable hardware	In this age, where new technological devices such as PDAs and mobile phones are becoming part of our daily lives, providing efficient implementations of suitable cryptographic algorithms for devices built on embedded systems is becoming increasingly important. This paper presents an efficient design of a high-performance hyperelliptic curve cryptosystem for a field programmable gate array which is well suited for embedded systems having limited resources such as memory, space and processing power. In this paper, we investigate two architectures, one using a projective coordinate representation for hyperelliptic systems and the second using a mixed coordinate representation that eliminates the need for field inversions in the point arithmetic, which has been proven to be expensive in both time and space. In addition, both architectures are based on an explicit formula which allows one to compute the point arithmetic directly in the finite field, thereby eliminating a level of arithmetic. The operation time for the HECC system is also improved by considering simplifications of the hyperelliptic curve which are accomplished through simple transformation of variables. As a result, these implementations offer significantly faster operation time and smaller area consumption then other HECC hardware implementations done to date.	cryptosystem;field-programmable gate array	Grace Elias;Ali Miri;Tet Hin Yeap	2007	Computers & Electrical Engineering	10.1016/j.compeleceng.2007.05.006	discrete mathematics;real-time computing;reconfigurable computing;computer science;theoretical computer science;mathematics;hyperelliptic curve;finite field;field-programmable gate array	Crypto	9.33365236829528	43.73139438828525	116213
da816c07fede23c39b654bb34df158167e1c56aa	on partitioning two dimensional finite difference meshes for distributed memory parallel computers	graph theory;resource allocation distributed processing finite difference methods graph theory mesh generation;resource allocation;distributed processing;partitioning algorithms program processors diamonds measurement standards approximation algorithms shape;hypergraph partitioning algorithms two dimensional finite difference meshes distributed memory parallel computers perfect load balance geometry based partitioning algorithms linear running time graph partitioning algorithms;mesh generation;finite difference methods	We investigate the problem of partitioning finite difference meshes in two dimensions among the processors of a parallel computer. The objective is to achieve a perfect load balance while minimizing the communication cost. There are well-known graph, hypergraph, and geometry-based partitioning algorithms for this problem. The known geometric algorithms have linear running time and obtain the best results for very special mesh sizes and processor numbers. We propose another geometric algorithm. The proposed algorithm is linear, is applicable to much more cases than some well-known alternatives, obtains better results than the graph partitioning algorithms, obtains better results than the hypergraph partitioning algorithms almost always. Our algorithm also obtains better results than a known asymptotically-optimal algorithm for some small number of processors. We also catalog related theoretical results.	asymptotically optimal algorithm;central processing unit;computation;computer;distributed memory;finite difference;graph partition;heuristic (computer science);load balancing (computing);multi-core processor;parallel computing;time complexity;whole earth 'lectronic link	Anaël Grandjean;Bora Uçar	2014	2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing	10.1109/PDP.2014.10	mesh generation;parallel computing;resource allocation;computer science;finite difference method;graph theory;theoretical computer science;distributed computing	HPC	-0.7562615157287597	37.463222317841456	116220
9bea87d20f109e27f8414b9ed47033bf1a10d2bf	load balanced clustering coefficients	social network analysis;graph algorithms;scalable programming;parallel algorithms	Clustering coefficients is a building block in network sciences that offers insights on how tightly bound vertices are in a network. Effective and scalable parallelization of clustering coefficients requires load balancing amongst the cores. This property is not easy to achieve since many real world networks are scale free, which leads to some vertices requiring more attention than others. In this work we show two scalable approaches that load balance clustering coefficients. The first method achieves optimal load balancing with an Ο(|E|) storage requirement. The second method has a lower storage requirement of Ο(|V|) at the cost of some imbalance. While both methods have a similar time complexity, they represent a tradeoff between maintaining a balanced workload and memory complexity. Using a 40-core system we show that our load balancing techniques outperform the widely used and simple parallel approach by a factor of 3X-7.5X for real graphs and 1.5X-4X for random graphs. Further, we achieve 25X-35X speedup over the sequential algorithm for most of the graphs.	algorithmic efficiency;balanced clustering;cluster analysis;coefficient;load balancing (computing);parallel computing;random graph;scalability;sequential algorithm;speedup;time complexity;vertex (geometry)	Oded Green;Lluís-Miquel Munguía;David A. Bader	2014		10.1145/2567634.2567635	mathematical optimization;computer science;theoretical computer science;distributed computing	HPC	-2.260890950581956	41.851465792485435	116234
a6567bdd1f9d685e886b4d634b40bf9368f62a8e	interactive kernel dimension alternative clustering on gpus		Machine learning has seen tremendous growth in recent years thanks to two key advances in technology: massive data generation and highly-parallel accelerator architectures. The rate that data is being generated is exploding across multiple domains, including medical research, environmental science, web-search, and e-commerce. Many of these advances have benefited from emergent web-based applications, and improvements in data storage and sensing technologies. Innovations in parallel accelerator hardware, such as GPUs, has made it possible to process massive amounts of data in a timely fashion. Given these advanced data acquisition technology and hardware, machine learning researchers are equipped to generate and sift through much larger and complex datasets quickly. In this work, we focus on accelerating Kernel Dimension Alternative Clustering algorithms using GPUs. We conduct a thorough performance analysis by using both synthetic and real-world datasets, while also modifying both the structure of the data, and the size of the datasets. Our GPU implementation reduces execution time from minutes to seconds, which enables us to develop a web-based application for users to, interactively, view alternative clustering solutions.	algorithm;big data;cluster analysis;computer data storage;data acquisition;e-commerce;emergence;graphics processing unit;ibm notes;implicit shape model;interactivity;iterative method;kernel (operating system);machine learning;mathematical optimization;overhead (computing);performance evaluation;profiling (computer programming);rate of convergence;run time (program lifecycle phase);spectral method;synthetic data;synthetic intelligence;web application	Xiangyu Li;Chieh Wu;Shi Dong;Jennifer G. Dy;David R. Kaeli	2018	2018 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining (ASONAM)	10.1109/ASONAM.2018.8508264	kernel (linear algebra);data mining;big data;test data generation;cluster analysis;data acquisition;computer data storage;computer science	HPC	-1.7454485860185776	41.8183955787374	116456
2803194da199fb18ded649b59a17ffa7fa4415a5	an implementation of matrix–matrix multiplication on the intel knl processor with avx-512		The second generation Intel Xeon Phi processor codenamed Knights Landing (KNL) have recently emerged with 2D tile mesh architecture and the Intel AVX-512 instructions. However, it is very difficult for general users to get the maximum performance from the new architecture since they are not familiar with optimal cache reuse, efficient vectorization, and assembly language. In this paper, we illustrate several developing strategies to achieve good performance with C programming language by carrying out general matrix–matrix multiplications and without the use of assembly language. Our implementation of matrix–matrix multiplication is based on blocked matrix multiplication as an optimization technique that improves data reuse. We use data prefetching, loop unrolling, and the Intel AVX-512 to optimize the blocked matrix multiplications. When we use a single core of the KNL, our implementation achieves up to 98% of SGEMM and 99% of DGEMM using the Intel MKL, which is the current state-of-the-art library. Our implementation of the parallel DGEMM using all 68 cores of the KNL achieves up to 90% of DGEMM using the Intel MKL.	avx-512;assembly language;automatic vectorization;blas;blocking (computing);cpu cache;code;knights;loop unrolling;master of science in information technology;math kernel library;mathematical optimization;matrix multiplication;parallel computing;programming language;register file;scalability;set packing;the matrix;xeon phi	Roktaek Lim;Yeongha Lee;Raehyun Kim;Jaeyoung Choi	2018	Cluster Computing	10.1007/s10586-018-2810-y	parallel computing;basic linear algebra subprograms;single-core;real-time computing;loop unrolling;architecture;xeon phi;vectorization (mathematics);cache;assembly language;computer science	HPC	-4.159249942411766	41.60379468198882	116485
1f048d44316d08c4664f04b297bf795922e0498e	a proposal for energy-efficient cellular neural network based on spintronic devices		Due to the massive parallel computing capability and outstanding image and signal processing performance, cellular neural network (CNN) is one promising type of non-Boolean computing system that can outperform the traditional digital logic computation and mitigate the physical scaling limit of the conventional CMOS technology. The CNN was originally implemented by VLSI analog technologies with operational amplifiers and operational transconductance amplifiers as neurons and synapses, respectively, which are power and area consuming. In this paper, we propose a hybrid structure to implement the CNN with magnetic components and CMOS peripherals with a complete driving and sensing circuitry. In addition, we propose a digitally programmable magnetic synapse that can achieve both positive and negative values of the templates. After rigorous performance analyses and comparisons, optimal energy is achieved based on various design parameters, including the driving voltage and the CMOS driving size. At a comparable footprint area and operation speed, a spintronic CNN is projected to achieve about one order of magnitude energy reduction per operation compared to its CMOS counterpart.	artificial neural network;boolean algebra;cmos;cellular neural network;computation;electronic circuit;image scaling;parallel computing;peripheral;scaling limit;signal processing;spintronics;synapse;very-large-scale integration	Chenyun Pan;Azad Naeemi	2016	IEEE Transactions on Nanotechnology		electronic engineering;engineering;electrical engineering	EDA	4.853366552044142	41.90431499629885	116636
e03777559ef8aebb2bc95fccae34fcae2d9222b2	introduction to topic 12: theory and algorithms for parallel computation	communication cost;computing paradigm;euro-par community;accepted paper;parallel computation;parallel computing;g group;new challenge;optimal performance;communication network;model of computation;distributed algorithm;competitive ratio;shared memory;metric space;algorithm design;parallel computer;shortest path tree	The study of theoretical aspects related to the design, analysis and experimentation of efficient algorithms, and to the identification of effective models of computation, represents a fundamental research area in parallel computing, which has been alive and productive for over two decades and well represented in the Euro-Par community. A distinctive characteristic of this Topic 12 is the variety of contributions addressing classical problems as well as the new challenges posed by recent technological advances and emerging computing paradigms. This year 13 papers were submitted to the topic, investigating a variety of algorithmic and modeling problems for parallel computation and communication. Among all submissions, 4 papers were accepted as full papers for the conference, resulting in a 31% acceptance rate. Accepted papers contain the following contributions: new centralized and distributed algorithms for bufferless routing in leveled networks, which attain optimal performance within logarithmic factors; results concerning the existence and the design of truthful mechanisms for the computation of shortest path trees in communication networks where edges are owned by selfish agents, under both utilitarian and non-utilitarian scenarios; embeddings of the hypercube in the partitioned optical passive starts network consisting of g groups of d processors each, which are optimal for all values of g and d; on-line algorithms to serve sequences of adversarial access requests to a shared memory page issued by n processors moving in a certain metric space, which attain good competitive ratios with respect to communication costs.		Andrea Pietracaprina;Kieran T. Herley;Christos D. Zaroliagis;Casiano Rodriguez-Leon	2005		10.1007/11549468_101		Theory	-1.993714883272109	46.01231180730457	116721
1e8ab1fa9debe3a811b5f73acf229be1be85a227	handling stuck-at-faults in memristor crossbar arrays using matrix transformations		Matrix-vector multiplication is the dominating computational workload in the inference phase of neural networks. Memristor crossbar arrays (MCAs) can inherently execute matrix-vector multiplication with low latency and small power consumption. A key challenge is that the classification accuracy may be severely degraded by stuck-at-fault defects. Earlier studies have shown that the accuracy loss can be recovered by retraining each neural network or by utilizing additional hardware. In this paper, we propose to handle stuck-at-faults using matrix transformations. A transformation T changes a weight matrix W into a weight matrix, @ = T(W), which is more robust to stuck-at-faults. In particular, we propose a row flipping transformation, a permutation transformation, and a value range transformation. The row flipping transformation results in that stuck-off (stuck-on) faults are translated into stuck-on (stuck-off) faults. The permutation transformation maps small (large) weights to memristors stuck-off (stuck-on). The value range transformation is based on reducing the magnitude of the smallest and largest elements in the matrix, which results in that each stuck-at-fault introduces an error of smaller magnitude. The experimental results demonstrate that the proposed framework is capable of recovering 99% of the accuracy loss introduced by stuck-at-faults without requiring the neural network to be retrained.		Baogang Zhang;Necati Uysal;Deliang Fan;Rickard Ewetz	2019		10.1145/3287624.3287707	permutation;electronic engineering;memristor;algorithm;crossbar switch;artificial neural network;latency (engineering);multiplication;computer science;matrix (mathematics);transformation matrix	HPC	4.279605791300064	42.67690059078037	116771
5fb65bbdf934dd1c5f9113b0252fca12d9435549	a 'jacobi' signal processing unit for time-adaptive svd	universal processing unit jacobi algorithms parallel processor arrays time adaptive svd jacobi signal processing unit space time trade offs singular value decomposition cordic arithmetic asynchronous communication;singular value decomposition;space time;parallel processing signal processing singular value decomposition digital arithmetic jacobian matrices;signal processing;asynchronous communication;jacobian matrices signal processing signal processing algorithms matrix decomposition adaptive arrays singular value decomposition adaptive signal processing process design space technology asynchronous communication;digital arithmetic;jacobian matrices;parallel processing	Implementing Jacobi algorithms in parallel processor arrays is a non-trivial task, in particular when the algorithms are parameterized with respect to size and the architectures are parameterized with respect to space-time trade-offs. The objective of this paper is to demonstrate that practical, time-adaptive singular value decomposition can be implemented on a parallel processor array using Cordic arithmetic and asynchronous communication, such that any degree of parallelism, from single-processor implementation up to full-size array implementation is supported by a 'universal' processing unit. This result is the product of judicious application of transformations in the combined algorithm and architecture space. >	jacobi method;signal processing;singular value decomposition	Ed F. Deprettere;Hylke W. van Dijk;Gerben J. Hekstra	1994		10.1109/ICASSP.1994.389611	multidimensional signal processing;parallel processing;computer vision;discrete mathematics;parallel computing;computer science;theoretical computer science;asynchronous communication;space time;signal processing;mathematics;singular value decomposition	ML	4.541953338721612	38.935779212081	116822
c8e2b994da25093e848441b9ef9832eb822923a0	accelerating the singular value decomposition of rectangular matrices with the csx600 and the integrable svd	singular value decomposition;singular vector;floating point;matrix multiplication;qr decomposition	We propose an approach to speed up the singular value decomposition (SVD) of very large rectangular matrices using the CSX600 floating point coprocessor. The CSX600-based acceleration board we use offers 50GFLOPS of sustained performance, which is many times greater than that provided by standard microprocessors. However, this performance can be achieved only when a vendor-supplied matrix-matrix multiplication routine is used and the matrix size is sufficiently large. In this paper, we optimize two of the major components of rectangular SVD, namely, QR decomposition of the input matrix and back-transformation of the left singular vectors by matrix Q, so that large-size matrix multiplications can be used efficiently. In addition, we use the Integrable SVD algorithm to compute the SVD of an intermediate bidiagonal matrix. This helps to further speed up the computation and reduce the memory requirements. As a result, we achieved up to 3.5 times speedup over the Intel Math Kernel Library running on an 3.2GHz Xeon processor when computing the SVD of a 100,000 × 4000 matrix.	singular value decomposition	Yusaku Yamamoto;Takeshi Fukaya;Takashi Uneyama;Masami Takata;Kinji Kimura;Masashi Iwasaki;Yoshimasa Nakamura	2007		10.1007/978-3-540-73940-1_35	mathematical optimization;parallel computing;lu decomposition;matrix multiplication;floating point;theoretical computer science;mathematics;matrix decomposition;singular value decomposition;qr decomposition;algebra	HPC	-2.216681347623995	39.35662149835138	117049
892bdffdee65d36600696edd4cfeb26986f3a786	an fpga-based coprocessor for hash unit acceleration		In recent times, applications like web-based search, antivirus scanners, cloud computing, social media applications, and network applications are extremely common. The hash table is a heavily used data structure in such applications. Modern microprocessors have several special function units (SFUs) such as a floating point unit, a memory management unit, and a cryptography unit. However, hashing is typically performed in software, which reduces the performance of such applications. In this paper, we propose an FPGA-based implementation of a hash unit (a hash function and a hash table) in an FPGA. The FPGA-based hash unit is implemented as a coprocessor for a CPU. The CPU and the FPGA communicate through a PCI Express (PCIe) interface. The hash table in our hash unit is implemented as a content-addressable memory (CAM), to enhance the speed of hash operations. The hash unit (HU) coprocessor is tested in the context of virus checking application, when the hashing operation only requires membership checks. Our HU can be used in other hashing applications as well; we use virus checking as a representative application. Hashing operations are performed in a batch on the FPGA, to provide better utilization of the PCIe bus. We demonstrate a significant performance of up to 7.3&#xd7; for our FPGAbased hash unit implementation compared to a software-based hashing implementation. This speedup is for the entire virus checking application (not just the hash lookup portion of the virus checking application).	algorithm;antivirus software;central processing unit;cloud computing;content-addressable memory;coprocessor;cryptography;data structure;field-programmable gate array;floating-point unit;hash function;hash table;lookup table;memory management unit;microprocessor;pci express;social media;speedup;web application	Abbas Fairouz;Sunil P. Khatri	2017	2017 IEEE International Conference on Computer Design (ICCD)	10.1109/ICCD.2017.53	hash table;parallel computing;real-time computing;memory management unit;content-addressable memory;hash function;floating-point unit;data structure;central processing unit;coprocessor;computer science	Vision	1.0277259343414793	43.71999791021335	117125
3866987c17b75edb8b8d90db19ca208ef148c7ef	vectorized sparse matrix multiply for compressed row storage format	matrice diagonale;supercomputer;supercomputador;matrice creuse;innovation;matriz diagonal;compressed sparse row;sparse matrix;innovacion;superordinateur;matriz dispersa;diagonal matrix	The innovation of this work is a simple vectorizable algorithm for performing sparse matrix vector multiply in compressed sparse row (CSR) storage format. Unlike the vectorizable jagged diagonal format (JAD), this algorithm requires no data rearrangement and can be easily adapted to a sophisticated library framework such as PETSc. Numerical experiments on the Cray X1 show an order of magnitude improvement over the non-vectorized algorithm.	algorithm;cray x1;experiment;incomplete lu factorization;jad;microsoft access;petsc;sparse matrix	Eduardo F. D'Azevedo;Mark R. Fahey;Richard Tran Mills	2005		10.1007/11428831_13	innovation;supercomputer;parallel computing;sparse matrix;computer science;theoretical computer science;band matrix;diagonal matrix;computer graphics (images)	HPC	-2.1197902916500566	39.212191253670625	117456
7d0a8568a1ba45a95bd732eede60d2a5a9e68781	hardware acceleration of photon mapping			algorithm;analysis of algorithms;hardware acceleration;iterative proportional fitting;methods of computing square roots;photon mapping;run time (program lifecycle phase);run-time infrastructure (simulation);speedup	Carl Andrew Hoggins	2011			photon mapping;electronic engineering;computer science;hardware acceleration	EDA	-2.1560574090389557	39.846038261451554	117831
c4d0f6b0382b9084773f8547f7d0a2df68bac2ad	the evolution of magnetic storage	disk device;tape technology;improved device;disk manufacturing;magnetic drum;general review;device geometries;movable-head disk drivein;magnetic storage development;device improvement	Since delivery of thefirst vacuum-column magnetic-tape transport in 1953 and thefirst movable-head disk drive in 1957, tape and disk devices in many configurations have been the principal means for storage of the large volumes of data required by data processing systems. Magnetic drums and other device geometries have also been important system components, but to a lesser extent. Over the past twenty-five years signijicant developments have been made that increase the capacity, reduce the cost, and improve the performance and reliability of these devices. With each improved device the range and nature of the applications undertaken have expanded and, in turn, led to a need for further device improvement. This paper gives a general review and historical perspective of magnetic storage development within IBM and is an introduction to the subsequent papers on disk, diskette, and tape technology and on disk manufacturing.	disk storage;drum memory;floppy disk;magnetic storage;mike lesser	Louis D. Stevens	1981	IBM Journal of Research and Development	10.1147/rd.255.0663	engineering;electrical engineering;forensic engineering;engineering drawing	OS	5.8317923515360395	32.3409056254964	118275
57f1e3397004ea0ce2877e07f0cbbfbb3045de41	serving dnns in real time at datacenter scale with project brainwave		To meet the computational demands required of deep learning, cloud operators are turning toward specialized hardware for improved efficiency and performance. Project Brainwave, Microsofts principal infrastructure for AI serving in real time, accelerates deep neural network (DNN) inferencing in major services such as Bings intelligent search features and Azure. Exploiting distributed model parallelism and pinning over low-latency hardware microservices, Project Brainwave serves state-of-the-art, pre-trained DNN models with high efficiencies at low batch sizes. A high-performance, precision-adaptable FPGA soft processor is at the heart of the system, achieving up to 39.5 teraflops (Tflops) of effective performance at Batch 1 on a state-of-the-art Intel Stratix 10 FPGA.	artificial neural network;data center;deep learning;flops;field-programmable gate array;http public key pinning;microservices;microsoft azure;neural oscillation;parallel computing;stratix	Eric S. Chung;Jeremy Fowers;Kalin Ovtcharov;Michael Papamichael;Adrian M. Caulfield;Todd Massengill;Daniel Lo;Shlomi Alkalay;Michael Haselman;Maleen Abeydeera;Logan Adams;Hari Angepat;Christian Boehn;Derek Chiou;Oren Firestein;Alessandro Forin;Kang Su Gatlin;Mahdi Ghandi;Stephen Heil;Kyle Holohan	2018	IEEE Micro	10.1109/MM.2018.022071131	microservices;real-time computing;system on a chip;stratix;field-programmable gate array;parallel computing;computer science;operator (computer programming);deep learning;artificial neural network;cloud computing;artificial intelligence	HPC	2.667991548939179	42.94418674430125	118407
179b22e025de791c4de47302259afa50bbb7f862	a hybrid implementation of two-level domain decomposition algorithm for solving elliptic equation on cpu/gpus	elliptic equations;partial differential equations computer architecture elliptic equations finite element analysis graphics processing units;scalable algorithm pdes hybrid architecture domain decomposition;computer architecture;partial differential equations;graphics processing units;finite element analysis;local multigrid methods hybrid implementation two level domain decomposition algorithm elliptic equation cpu gpu compute hardware development hybrid architecture partial differential equations pde finite difference finite element methods;graphics processing units sparse matrices scalability libraries additives clustering algorithms parallel processing	The rapid compute hardware development has shifted to the hybrid architecture consisting of both CPUs and GPUs. For the class of problems described by partial differential equations (PDEs) discretized by finite difference or finite element methods, obtaining reasonably good performance on a CPU/GPU platform is still a challenge. In this paper, we propose and test an algorithm with several building layers matching the hybrid architecture. The scalability of the approach is obtained by a two-level domain decomposition method, and the GPU performance is effectuated by using various local multigrid methods with suitable smoothers. Efficiency and scalability of our algorithm are demonstrated by convincing performance analysis on a hybrid platform of CPU/GPU.	algorithm;central processing unit;discretization;domain decomposition methods;finite difference;finite element method;gauss–seidel method;graphics processing unit;multigrid method;numerical analysis;numerical partial differential equations;parallel computing;scalability;solver	Li Luo;Yubo Zhao;Xiao-Chuan Cai	2012	2012 13th International Conference on Parallel and Distributed Computing, Applications and Technologies	10.1109/PDCAT.2012.18	mathematical optimization;parallel computing;computer science;theoretical computer science;finite element method;partial differential equation;numerical partial differential equations;multigrid method	HPC	-4.473792180975425	38.22437701738006	118527
b431e082b0af343a375618839c351b537b247116	efficient human-like memory management based on walsh-based associative memory for real-time pattern recognition	image recognition;human like memory management;memory management;binary alphabet images;storage management;real time;long term memory;computer storage reduction;real time pattern recognition;computer storage reduction binary alphabet images human like memory management real time pattern recognition walsh based distributed associative memory;walsh based distributed associative memory;pattern recognition;working memory;associative memory;content addressable storage;storage management content addressable storage image recognition;data transfer;memory management associative memory pattern recognition humans pattern matching magnesium compounds power engineering and energy crosstalk distributed computing image recognition	This paper proposes an efficient human-like memory and memory management which utilizes Walsh-based distributed associative memory in reducing the computer storage and processing for pattern recognition. As a verification example, a memory storing 26 binary alphabet images takes only the physical space needed to store 8 patterns and yet capable of perfect recognition. Further, the experimental results show that the proposed memory management strategy can nicely deal with data transfer from short-term (working) memory to long-term memory.	autoassociative memory;computer data storage;content-addressable memory;hadamard transform;memory management;pattern recognition;real-time transcription	Hyun-Chul Choi;Se-Young Oh	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.247379	distributed shared memory;shared memory;interleaved memory;long-term memory;distributed memory;computer hardware;computer science;physical address;artificial intelligence;theoretical computer science;working memory;overlay;extended memory;flat memory model;registered memory;cache-only memory architecture;memory map;memory management	Robotics	6.50756103889222	39.06279849286563	118584
982d2610638f5bbe572d5dc4de749c3762d83217	parallel transient stability simulation for national power grid of china	symmetric configuration;simulation ordinateur;systeme temps reel;algoritmo paralelo;phenomene transitoire;haute performance;parallel algorithm;systeme grande taille;multiprocessor;gollete estrangulamiento;configuration symetrique;reponse transitoire;real time;dynamical processes;distributed computing;real time simulation;large scale system;transient stability;convergence numerique;parallel computation;transient analysis;algorithme parallele;grid;numerical convergence;simulation software;large scale;goulot etranglement;configuracion simetrica;transient response;calculo paralelo;respuesta transitoria;rejilla;power system;temps reel;analyse transitoire;fenomeno transitorio;high performance computer;network algorithm;alto rendimiento;grille;power grid;calculo repartido;tiempo real;real time system;sistema tiempo real;simulacion computadora;transients;multiprocesador;on line control;algoritmo optimo;algorithme optimal;optimal algorithm;high performance;calcul parallele;bottleneck;computer simulation;convergencia numerica;calcul reparti;smp cluster;sistema gran escala;multiprocesseur	With the development of modern power system, real-time simulation and on-line control are becoming more and more critical. Transient stability analysis, where the most intensive computation locates, is the bottleneck of real-time simulation for large-scale power system. Thus, the key to achieve the real-time simulation of large scale power systems is to find the new transient stability algorithms and parallel software with high-performance computers. This paper presents a new spatial parallel transient stability algorithm including an improved parallel network algorithm and an optimal convergence checking method. The simulation software with the spatial parallel algorithm is designed and implemented on a SMP-cluster system. The test cases of national power grid of China show that the optimal computation time of the parallel software is only 38% of the actual dynamic process. It is suggested that the algorithms described in this paper can achieve the super real-time simulation of very large scale power system and make the complex on-line power applications, especially on-line supervision and control for large scale power system, feasible.	simulation	Wei Xue;Jiwu Shu;Weimin Zheng	2004		10.1007/978-3-540-30566-8_89	computer simulation;parallel computing;real-time computing;multiprocessing;simulation;simulation software;computer science;parallel algorithm;electric power system;grid;transient response;algorithm	HPC	-3.0173938502983195	34.50051667340314	118926
aca9a8e3ca0fd609053fdb9d594658c09c0fbbf3	efficient $m$ -ary exponentiation over $gf(2^{m})$  using subquadratic ka-based three-operand montgomery multiplier	digital arithmetic;matrix decomposition;polynomials;2-way ka decomposition;3-way ka decomposition;karatsuba algorithm;efficient m-ary exponentiation;exponentiation-based cryptosystems;pairing based cryptography;subquadratic ka-based three-operand montgomery multiplier;subquadratic montgomery multiplier;three-operand polynomial multiplications;exponentiation;karatsuba algorithm;montgomery multiplication;three-operand multiplication	Karatsuba algorithm (KA) is popularly used for high-precision multiplication of long binary polynomials. The only well-known subquadratic multipliers using KA scheme are, however, based on conventional two-operand polynomial multiplication. In this paper, we propose a novel approach based on 2-way and 3-way KA decompositions for computing three-operand polynomial multiplications. Using these novel KA decompositions, we present here a new subquadratic Montgomery multiplier. Our proposed multiplier involves less area and less delay compared to the schoolbook three-operand multiplier as well as the two-operand multipliers based on conventional KA decomposition. We have used the proposed three-operand Montgomery multiplication to derive a novel efficient scheme for m-ary exponentiation, and proposed a novel architecture for exponentiation. We have analyzed the complexities of proposed design, and shown that the proposed exponentiator can have a small lower bound on time complexity amounting to √m-1 multiplication delays, while traditional exponentiators require nearly m multiplication delays. From synthesis results, it is shown that the proposed exponentiator using subquadratic three-operand multiplier approach has significantly less time complexity, less area-delay product, and less power consumption than the existing exponentiators. Moreover, exponentiation-based cryptosystems, such as pairing based cryptography, could achieve high-speed operation using by our proposed multiplier and m-ary exponentiator.	computation;cryptography;cryptosystem;dspace;exponentiation by squaring;karatsuba algorithm;lagrange multiplier;matrix multiplication;montgomery modular multiplication;operand;polynomial ring;symmetric multiprocessing;time complexity;usb on-the-go;vii	Chiou-Yng Lee;Pramod Kumar Meher;Chien-Ping Chang	2014	IEEE Transactions on Circuits and Systems I: Regular Papers	10.1109/TCSI.2014.2334992	arithmetic;discrete mathematics;mathematics;algebra	EDA	9.689695590769151	43.41711643286495	118967
8786b0854f4a9c41327a3cb15cac859d25b2959c	coordinated multi-robot exploration: out of the box packages for ros	wireless lan distributed control merging multi robot systems operating systems computers software packages;robot operating system coordinated multirobot exploration ros packages robot communication global map construction distributed control application layer protocols box installation box execution communication package reliable ad hoc communication local map exchange spatially spreading robots exploration time reduction research groups turtlebot robots pioneer robots;robot kinematics ad hoc networks corporate acquisitions multi robot systems reliability space exploration	We present and evaluate new ROS packages for coordinated multi-robot exploration, namely communication, global map construction, and exploration. The packages allow completely distributed control and do not rely on (but allow) central controllers. Their integration including application layer protocols allows out of the box installation and execution. The communication package enables reliable ad hoc communication allowing to exchange local maps between robots which are merged to a global map. Exploration uses the global map to spatially spread robots and decrease exploration time. The intention of the implementation is to offer basic functionality for coordinated multi-robot systems and to enable other research groups to experimentally work on multi-robot systems. The packages are tested in real-world experiments using Turtlebot and Pioneer robots. Further, we analyze their performance using simulations and verify their correct working.	dr-dos;distributed control system;ecosystem;experiment;hoc (programming language);map;multicast;out of the box (feature);pioneer;robot operating system;simulation;thinking outside the box;turtle (robot);unicast;wireless access point	Torsten Andre;Daniel Neuhold;Christian Bettstetter	2014	2014 IEEE Globecom Workshops (GC Wkshps)	10.1109/GLOCOMW.2014.7063639	real-time computing;simulation;operating system;distributed computing;computer network	Robotics	8.983366803499173	34.71945457219005	119095
600573c7cb9c9e7cab951111a340ccfbaf170f82	answering the min-cost quality-aware query on multi-sources in sensor-cloud systems	data quality;quality-aware query;sensor-based systems;sensor-cloud systems;source quality	In sensor-based systems, the data of an object is often provided by multiple sources. Since the data quality of these sources might be different, when querying the observations, it is necessary to carefully select the sources to make sure that high quality data is accessed. A solution is to perform a quality evaluation in the cloud and select a set of high-quality, low-cost data sources (i.e., sensors or small sensor networks) that can answer queries. This paper studies the problem of min-cost quality-aware query which aims to find high quality results from multi-sources with the minimized cost. The measurement of the query results is provided, and two methods for answering min-cost quality-aware query are proposed. How to get a reasonable parameter setting is also discussed. Experiments on real-life data verify that the proposed techniques are efficient and effective.		Mohan Li;Yanbin Sun;Yu Jiang;Zhihong Tian	2018	Sensors	10.3390/s18124486	information retrieval;wireless sensor network;cloud computing;data quality;computer science	DB	5.87992787593375	34.72417661231006	119179
8ab58f133422a43e117282d46408f91b89824be6	improving data partitioning performance on opencl-based fpgas	field programmable gate arrays kernel relational databases acceleration throughput hardware design languages;database;partitioning;fpga;relational databases field programmable gate arrays;stratix v gx fpga data partitioning performance opencl based fpgas relational database random memory accesses cpu gpu lock overhead memory stalls multikernel approach opencl sdk task kernel;database fpga opencl partitioning;opencl	We investigate the performance of relational database applications on recent OpenCL-based FPGAs. As a start, we study the performance of data partitioning, a core operation widely used in relational databases. Due to the random memory accesses, data partitioning is time-consuming and can become a major bottleneck for database operators such as hash joins. We start with the state-of-the-art OpenCL implementation which was originally designed for the CPU/GPU, and find that such an implementation suffers from lock overhead and memory stalls. To resolve those overheads, we develop a simple yet efficient multi-kernel approach to leverage two emerging features in Alter a OpenCL SDK, namely task kernel and channel. We evaluate the proposed design on a recent Alter a Stratix V GX FPGA. Our results demonstrate that our proposed approach can achieve roughly 10.7X speedup over the state-of-the-art OpenCL implementation.	central processing unit;field-programmable gate array;graphics processing unit;kernel (operating system);opencl api;overhead (computing);partition (database);powerpc 600;relational database;software development kit;speedup;stratix	Ze-ke Wang;Beixin Julie He;Wei Zhang	2015	2015 IEEE 23rd Annual International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2015.34	embedded system;computer architecture;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	Arch	-4.247804694182928	45.11113702705426	119320
acc5a4cdfb2d310b1f4f77c77ad4a41fd8bea7c3	fast prime field elliptic-curve cryptography with 256-bit primes		This paper studies software optimization of elliptic-curve cryptography with $$256$$ 256 -bit prime fields. We propose a constant-time implementation of the NIST and SECG standardized curve P- $$256$$ 256 , that can be seamlessly integrated into OpenSSL. This accelerates Perfect Forward Secrecy TLS handshakes that use ECDSA and/or ECDHE, and can help in improving the efficiency of TLS servers. We report significant performance improvements for ECDSA and ECDH, on several architectures. For example, on the latest Intel Haswell microarchitecture, our ECDSA sign is $$2.33\times $$ 2.33 × faster than OpenSSL’s implementation.	elliptic curve cryptography;forward secrecy;haswell (microarchitecture);mathematical optimization;microarchitecture;openssl;program optimization;secg	Shay Gueron;Vlad Krasnov	2013	Journal of Cryptographic Engineering	10.1007/s13389-014-0090-x	parallel computing;real-time computing;computer science;theoretical computer science	Security	8.377864942072128	44.688688890080094	119435
439309afbf9e4056f37721abcf837c79a010a028	a task-scheduling approach for efficient sparse symmetric matrix-vector multiplication on a gpu	rcm;gpu;65y05;intel mkl;sparse symmetric matrix vector product;65y04;kepler;65y20;68w10	In this paper, a task-scheduling approach to efficiently calculating sparse symmetric matrix-vector products and designed to run on graphics processing units (GPUs) is presented. The main premise is that, for many sparse symmetric matrices occurring in common applications, it is possible to obtain significant reductions in memory usage and improvements in performance when the matrix is prepared in certain ways prior to computation. The preprocessing proposed in this paper employs task scheduling to overcome the difficulties that have suppressed the development of methods taking advantage of the symmetry of sparse matrices. The performance of the proposed task-scheduling method is verified using a Kepler (Tesla K40c) graphics accelerator, and is compared to the performance of cuSPARSE library functions on a GPU and to functions from the Intel MKL on central processing units (CPUs) executed in the parallel mode. The obtained results indicate that the proposed approach for sparse symmetric matrix-vector products results in up to a 40% reduction in memory usage, as compared to nonsymmetric matrix storage formats, while retaining good throughput. Compared to cuSPARSE and Intel MKL functions for sparse symmetric matrices, the proposed TSMV approach allowed us to achieve a significant speedup (of over one order of magnitude).	algorithm;authorization;cuda;central processing unit;command & conquer:yuri's revenge;computation;computer graphics;coprocessor;dual ec drbg;graphics processing unit;kepler (microarchitecture);library (computing);math kernel library;numerical aperture;overhead (computing);point of view (computer hardware company);preprocessor;reliability-centered maintenance;scheduling (computing);sparse matrix;speedup;the matrix;throughput;ut-vpn	P. Mironowicz;Adam Dziekonski;M. Mrozowski	2015	SIAM J. Scientific Computing	10.1137/14097135X	parallel computing;computer hardware;computer science;theoretical computer science;sparse approximation;reliability centered maintenance;kepler	HPC	-1.9753989818261612	39.168952990165124	119528
0e24abe3877528edc15240cb69124e06387a64ab	a many processing element framework for the discrete fourier transform	processing element;digital signal processing;electrical and electronic engineering not elsewhere classified;parallel processing discrete fourier transforms;discrete fourier transforms hardware program processors digital signal processing computer architecture field programmable gate arrays throughput;sequential processors;computer architecture;performance improvement;processing element framework;discrete fourier transform;recursive algorithm processing element framework discrete fourier transform sequential processors parallel processors;recursive algorithm;field programmable gate arrays;discrete fourier transforms;program processors;parallel processing;domain specificity;parallel processors;throughput;hardware	For the greater part of the last 30 years of computing history, computational processing has been primarily based on performance improvements of sequential processors. Research has shown that the future no longer lies in the direction of sequential processor performance improvements, but rather on the adoption of parallel processors and the development of suitable architectures. Computing in general has settled on generic suitable-for-all processing architectures, however recent research has since highlighted the importance of adopting many simpler processing elements, with a tailored focus on domain specific designs. In light of this, this work focuses on the development of a parallel framework for the execution of the Discrete Fourier Transform using a recursive algorithm using many simple processing elements. The system described in this work shows a 64 point Discrete Fourier Transform computation in parallel, which achieves a throughput of 2.19GSPS(39.49Gbit/s). The framework proposed promotes adjustable parallelism using the many simple processing elements, and allows simple adoption on existing systems.	algorithm;central processing unit;computation;discrete fourier transform;high-throughput computing;parallel computing;recursion (computer science);throughput	Andrew van der Byl;Michael R. Inggs;Richardt H. Wilkinson	2010	2010 International Conference on Field-Programmable Technology	10.1109/FPT.2010.5681451	multidimensional signal processing;parallel processing;throughput;parallel computing;real-time computing;computer science;theoretical computer science;digital signal processing;discrete fourier transform;field-programmable gate array;recursion	HPC	1.7745935761239706	44.321052815514264	119666
406c10bad5b588687b4f52a293ef9c9395aaf8a6	accelerating matlab image processing toolbox functions on gpus	data sharing;nvidia geforce gtx 280;paper;image processing;rv870;ati;cuda;gpgpu;efficient implementation;data dependence;nvidia;graphic processing unit;opencl;ati radeon hd 5870;matlab;open source	In this paper, we present our effort in developing an open-source GPU (graphics processing units) code library for the MATLAB Image Processing Toolbox (IPT). We ported a dozen of representative functions from IPT and based on their inherent characteristics, we grouped these functions into four categories: data independent, data sharing, algorithm dependent and data dependent. For each category, we present a detailed case study, which reveals interesting insights on how to efficiently optimize the code for GPUs and highlight performance-critical hardware features, some of which have not been well explored in existing literature. Our results show drastic speedups for the functions in the data-independent or data-sharing category by leveraging hardware support judiciously; and moderate speedups for those in the algorithm-dependent category by careful algorithm selection and parallelization. For the functions in the last category, fine-grain synchronization and data-dependency requirements are the main obstacles to an efficient implementation on GPUs.	algorithm selection;computer graphics;data dependency;graphics processing unit;ibm notes;image processing;information processes and technology;library (computing);matlab;mathematical optimization;open-source software;parallel computing;requirement;iptables	Jingfei Kong;Martin Dimitrov;Yi Yang;Janaka Liyanage;Lin Cao;Jacob Staples;Mike Mantor;Huiyang Zhou	2010		10.1145/1735688.1735703	parallel computing;image processing;computer science;theoretical computer science;operating system;general-purpose computing on graphics processing units;computer graphics (images)	EDA	-3.9980771771597152	43.93491540949653	119711
36da27e44d09611fc9f621df10c73cf68eb82315	semantic multimedia remote display for mobile thin clients	laser;standard;3rd millennium;ibcn;x window system;technology and engineering;semantic multimedia remote display;vnc hextile;mpeg 4;rdp;video;mobile thin client;mpeg 4 multimedia scene bifs	Current remote display technologies for mobile thin clients convert practically all types of graphical content into sequences of images rendered by the client. Consequently, important information concerning the content semantics is lost. The present paper goes beyond this bottleneck by developing a semantic multimedia remote display. The principle consists of representing the graphical content as a real-time interactive multimedia scene graph. The underlying architecture features novel components for scene-graph creation and management, as well as for user interactivity handling. The experimental setup considers the Linux X windows system and BiFS/LASeR multimedia scene technologies on the server and client sides, respectively. The implemented solution was benchmarked against currently deployed solutions (VNC and Microsoft-RDP), by considering text editing and WWW browsing applications. The quantitative assessments demonstrate: (1) visual quality expressed by seven objective metrics, e.g., PSNR values between 30 and 42 dB or SSIM values larger than 0.9999; (2) downlink bandwidth gain factors ranging from 2 to 60; (3) real-time user event management expressed by network round-trip time reduction by factors of 4–6 and by uplink bandwidth gain factors from 3 to 10; (4) feasible CPU activity, larger than in the RDP case but reduced by a factor of 1.5 with respect to the VNC-HEXTILE.	central processing unit;display device;graphical user interface;interactivity;linux;mpeg-4 part 11;mpeg-4 part 20;microsoft windows;peak signal-to-noise ratio;real-time clock;real-time locating system;remote desktop protocol;scene graph;server (computing);structural similarity;telecommunications link;text editor;thin client;www;x window system	Bojan Joveski;Mihai Mitrea;Pieter Simoens;Iain James Marshall;Françoise J. Prêteux;Bart Dhoedt	2013	Multimedia Systems	10.1007/s00530-013-0304-6	computer vision;simulation;video;laser;telecommunications;computer science;operating system;multimedia;world wide web;mpeg-4;computer network	HCI	6.150116857533077	33.64854302491945	120166
09f7a1f7c0cde7f8758cd65821c1af9248bc7138	leveraging mlc stt-ram for energy-efficient cnn training		Graphics Processing Units (GPUs) are extensively used in training of convolutional neural networks (CNNs) due to their promising compute capability. However, GPU memory capacity, bandwidth, and energy are becoming critical system bottlenecks with increasingly larger and deeper training models. This paper proposes an energy-efficient GPU memory management scheme by employing MLC STT-RAM as GPU memory to accommodate the image classification training workloads. We propose a data remapping scheme that exploits the asymmetry access latency and energy across soft and hard bits in MLC STT-RAM cells and the memory access characteristics in image classification training workloads. Furthermore, our design enables (i) energy-efficient memory access by leveraging bit-level similarity in training data and (ii) optimal feature map encoding to compress the contiguous 0s in feature maps. Our design reduces VGG-19 and AlexNet training time, GPU memory access energy and capacity utilization by 76% and 70%, 45% and 40%, 26.9% and 26%, respectively.		Hengyu Zhao;Jishen Zhao	2018		10.1145/3240302.3240422	latency (engineering);convolutional neural network;memory management;parallel computing;efficient energy use;encoding (memory);contextual image classification;graphics;bandwidth (signal processing);computer science	Arch	3.4951485795878736	43.21539169260805	120310
146e869dbfb5575715cdbbd392b8f84c99654970	hybrid algorithms for list ranking and graph connected components	graph theory;kernel;connected components;image processing;list ranking;gpgpu hybrid multicore algorithms list ranking connected components;multiprocessing systems computer architecture graph theory graphics processing units;computer architecture;gpgpu;graph connectivity;graphics processing units;multicore processing;numerical algorithm;scientific computing;graph algorithm;graphic processing unit;multiprocessing systems;graphics processing unit algorithm design and analysis multicore processing instruction sets kernel;hybrid multicore algorithms;graphics processing unit;connected component;accelerator list ranking graph connected component multicore architecture many core architecture seminumerical algorithm gpu general purpose computation hybrid multicore computing cpu;algorithm design;algorithm design and analysis;hybrid algorithm;instruction sets	The advent of multicore and many-core architectures saw them being deployed to speed-up computations across several disciplines and application areas. Prominent examples include semi-numerical algorithms such as sorting, graph algorithms, image processing, scientific computations, and the like. In particular, using GPUs for general purpose computations has attracted a lot of attention given that GPUs can deliver more than one TFLOP of computing power at very low prices. In this work, we use a new model of multicore computing called hybrid multicore computing where the computation is performed simultaneously a control device, such as a CPU, and an accelerator such as a GPU. To this end, we use two case studies to explore the algorithmic and analytical issues in hybrid multicore computing. Our case studies involve two different ways of designing hybrid multicore algorithms. The main contribution of this paper is to address the issues related to the design of hybrid solutions. We show our hybrid algorithm for list ranking is faster by 50% compared to the best known implementation [Z. Wei, J. JaJa; IPDPS 2010]. Similarly, our hybrid algorithm for graph connected components is faster by 25% compared to the best known GPU implementation [26].	central processing unit;computation;connected component (graph theory);critical path method;general-purpose computing on graphics processing units;graph theory;graphics processing unit;hybrid algorithm;hybrid algorithm (constraint satisfaction);image processing;international parallel and distributed processing symposium;list ranking;manycore processor;multi-core processor;numerical analysis;regular expression;semiconductor industry;sorting	Dip Sankar Banerjee;Kishore Kothapalli	2011	2011 18th International Conference on High Performance Computing	10.1109/HiPC.2011.6152655	algorithm design;computer architecture;parallel computing;connected component;image processing;computer science;graph theory;theoretical computer science;operating system;distributed computing;algorithm	HPC	-2.7902507176693354	42.84718669615735	120881
0c6a00eea79af493a0857c9515445a1e820fdbad	dotstar: breaking the scalability and performance barriers in parsing regular expressions	data intensive application;multi core processor;software tool;parallel algorithm;data filtering;multicore;experimental evaluation;network intrusion detection system;regular expression;intermediate representation;parallel algorithms	Regular expressions (shortened as regexp ) are widely used to parse data, detect recurrent patterns and information, and are a common choice for defining configurable rules for a variety of systems. In fact, many data-intensive applications rely on regexp matching as the first line of defense to perform on-line data filtering. Unfortunately, few solutions can keep up with the increasing data rate and complexity of sets containing hundreds of expressions. In this paper we present DotStar (.*), a complete algorithmic solution and a software tool-chain, that can compile large sets of regexp into an automaton that can take advantage of the vector/SIMD extensions available on many commodity multi-core processors. DotStar relies on several algorithmic innovations to transform the user-provided regexp set into a sequence of manageable intermediate representations. The resulting automaton is both space and time efficient, and can search in a single pass without backtracking. The experimental evaluation, performed on a family of state-of-the-art processors, shows that DotStar can efficiently handle both small sets of regexp, used in protocol parsing, and larger sets designed for Network Intrusion Detection Systems (NIDS), achieving a performance between 1 and 5 Gbit/sec per core.	aho–corasick algorithm;backtracking;central processing unit;compile time;compiler;cylinder-head-sector;data breach;data-intensive computing;experiment;field-programmable gate array;gigabit;graphics processing unit;intrusion detection system;multi-core processor;nondeterministic finite automaton;online and offline;parsing;programming tool;regular expression;run time (program lifecycle phase);simd;scalability;toolchain;uncompressed video	Davide Pasetto;Fabrizio Petrini;Virat Agarwal	2010	Computer Science - Research and Development	10.1007/s00450-010-0106-4	multi-core processor;parallel computing;computer science;theoretical computer science;operating system;database;distributed computing;parallel algorithm;programming language	DB	1.205915810567685	43.283398225722685	120890
bd7e94e111bbd5b62ec12cdc489c15ad6d33647c	a note on stream ciphers that continuously use the iv				Matthias Hamann;Matthias Krause;Willi Meier	2017	IACR Cryptology ePrint Archive		parallel computing;computer science;stream cipher	Crypto	5.552331103000421	36.849149824510974	121085
1b7fcedcbbc7d8e64a38fcf1d6bc8f67afb356a0	spiral: a generator for platform-adapted libraries of signal processing alogorithms	fourier transform;fft;seach;program generation;search;signal processing;automatic performance tuning;domain specific language;optimization;signal transform;dft	SPIRAL is a generator of libraries of fast software implementations of linear signal processing transforms. These libraries are adapted to the computing platform and can be re-optimized as the hardware is upgraded or replaced. This paper describes the main components of SPIRAL: the mathematical framework that concisely describes signal transforms and their fast algorithms; the formula generator that captures at the algorithmic level the degrees of freedom in expressing a particular signal processing transform; the formula translator that encapsulates the compilation degrees of freedom when translating a specific algorithm into an actual code implementation; and, finally, an intelligent search engine that finds within the large space of alternative formulas and implementations the “best” match to the given computing platform. We present empirical data that demonstrates the high performance of SPIRAL generated code.	algorithm;compiler;fortran;library (computing);signal processing;spiral model;time complexity;web search engine	Markus Püschel;José M. F. Moura;Bryan Singer;Jianxin Xiong;Jeremy R. Johnson;David A. Padua;Manuela M. Veloso;Robert W. Johnson	2004	IJHPCA	10.1177/1094342004041291	multidimensional signal processing;fourier transform;fast fourier transform;parallel computing;computer science;domain-specific language;theoretical computer science;operating system;discrete fourier transform;signal processing;programming language;algorithm	PL	6.246277623992394	45.944186738328696	121464
94b1c1567dc2c40812d55dca3389829a65571e67	low-power step counting paired with electromagnetic energy harvesting for wearables		Fitness related wearables have become ubiquitous in the recent past. Nevertheless, short battery life of these devices is still a pressing issue. Limited battery capacity in small form factor and power hungry continuous monitoring of accelerometer have been significant concerns in this regard. To address these issues we propose a novel low-power step counting solution based on an Electromagnetic energy harvesting mechanism. Extremely simple nature of the step counter removes the requirement of any step detection algorithm, thereby reducing the power consumption, while the energy harvester generates a portion of energy requirement prolonging the battery life.	algorithm;low-power broadcasting;small form factor;step detection;wearable computer	Yvonne Joho;Jo&#x00E0;o Alves;Juan Guizado Vásquez;Kimberly-Ann Milton McSween;Kanchana Thilakarathna;Dileeka Dias	2018		10.1145/3267242.3267291	small form factor;embedded system;electromagnetic radiation;battery (electricity);wearable computer;step detection;energy harvesting;accelerometer;continuous monitoring;electronic engineering;computer science	EDA	2.677288216918873	34.14430523555577	121572
0b4c307af01109b0e061154eac7782ec319a76e6	a zigbee-based sensor node for tracking people's locations	localized protocol;energy efficiency;linear topology;routing;data collection;time synchronization;satisfiability;sensor network;wireless sensor network;context aware service;sensor nodes;power consumption;networked systems	A sensor network system has been developed for tracking people's locations in workplaces as part of a ubiquitous network system for providing context-aware services in daily activities. Since the installation of such a sensor is desired any place within its target domain with few limitations, it must operate by battery for a relatively long time, e.g., one month. To satisfy this requirement, we designed a battery-operated sensor node based on ZigBee technology and extended its operation period by developing a flexible sleep control protocol and a high-accuracy time synchronization mechanism between sensor nodes to reduce power consumption. From simulations based on actual data collected, we confirmed that a sensor node located in a hospital's medical ward can work over 21 days using four AA Ni-H batteries.	context-aware network;sensor node;simulation;ubiquitous computing	Satoshi Takahashi;Jeffrey Wong;Masakazu Miyamae;Tsutomu Terada;Haruo Noma;Tomoji Toriyama;Kiyoshi Kogure;Shojiro Nishio	2008		10.1145/1367943.1367947	embedded system;neurfon;real-time computing;wireless sensor network;sensor node;engineering;brooks–iyengar algorithm;key distribution in wireless sensor networks;mobile wireless sensor network;computer network;intelligent sensor;visual sensor network	Mobile	1.4417577379139799	33.82562065430948	121621
33692809ff922f0419597955dd305d81ab3acbfe	a high throughput fpga-based floating point conjugate gradient implementation for dense matrices	field programmable gate array;building block;journal article;higher order;fpga implementation;conjugate gradient;fpga architecture;scientific computing;place and route;floating point;linear equations;high throughput;software implementation	Recent developments in the capacity of modern Field Programmable Gate Arrays (FPGAs) have significantly expanded their applications. One such field is the acceleration of scientific computation and one type of calculation that is commonplace in scientific computation is the solution of systems of linear equations. A method that has proven in software to be very efficient and robust for finding such solutions is the Conjugate Gradient (CG) algorithm. In this article we present a widely parallel and deeply pipelined hardware CG implementation, targeted at modern FPGA architectures. This implementation is particularly suited for accelerating multiple small-to-medium-sized dense systems of linear equations and can be used as a stand-alone solver or as building block to solve higher-order systems. In this article it is shown that through parallelization it is possible to convert the computation time per iteration for an order n matrix from Θ(n2) clock cycles on a microprocessor to Θ(n) on a FPGA. Through deep pipelining it is also possible to solve several problems in parallel and maximize both performance and efficiency. I/O requirements are shown to be scalable and convergent to a constant value with the increase of matrix order. Post place-and-route results on a readily available VirtexII-6000 demonstrate sustained performance of 5 GFlops, and results on a Virtex5-330 indicate sustained performance of 35 GFlops. A comparison with an optimized software implementation running on a high-end CPU demonstrate that this FPGA implementation represents a significant speedup of at least an order of magnitude.	algorithm;central processing unit;clock signal;computation;computational science;conjugate gradient method;convex conjugate;field-programmable gate array;gradient descent;instruction pipelining;instrumental convergence;iteration;linear equation;microprocessor;parallel computing;pipeline (computing);place and route;requirement;scalability;solver;sparse matrix;speedup;system of linear equations;throughput;time complexity	Antonio Roldao Lopes;George A. Constantinides	2010	TRETS	10.1145/1661438.1661439	high-throughput screening;embedded system;parallel computing;real-time computing;higher-order logic;computer science;floating point;theoretical computer science;operating system;place and route;conjugate gradient method;linear equation;field-programmable gate array	HPC	-2.3486655725091796	40.00198362928107	121650
2f9fac07be4866c2a8d72c8afa44ba956137cba1	an fpga/hmc-based accelerator for resolution proof checking		Modern Boolean satisfiability solvers can emit proofs of unsatisfiability. There is substantial interest in being able to verify such proofs and also in using them for further computations. In this paper, we present an FPGA accelerator for checking resolution proofs, a popular proof format. Our accelerator exploits parallelism at the low level by implementing the basic resolution step in hardware, and at the high level by instantiating a number of parallel modules for proof checking. Since proof checking involves highly irregular memory accesses, we employ Hybrid Memory Cube technology for accelerator memory. The results show that while the accelerator is scalable and achieves speedups for all benchmark proofs, performance improvements are currently limited by the overhead of transitioning the proof into the accelerator memory.	field-programmable gate array;hybrid memory cube;resolution (logic)	Tim Hansmeier;Marco Platzner;David Andrews	2018		10.1007/978-3-319-78890-6_13	computer science;hybrid memory cube;field-programmable gate array;parallel computing;scalability;computation;mathematical proof;boolean satisfiability problem;exploit	EDA	0.9701189463106087	44.81770761968972	121782
70192255d1bbe7751a4540be416442d3e522eb34	a hybrid format for better performance of sparse matrix-vector multiplication on a gpu	adaptive;evc hyb format;memory coalescing;graphics processing unit;sparse matrix vector multiplication	In this paper, we present a new sparse matrix data format that leads to improved memory coalescing and more efficient sparse matrix-vector multiplication (SpMV) for a wide range of problems on high throughput architectures such as a graphics processing unit (GPU). The sparse matrix structure is constructed by sorting the rows based on the row length (defined as the number of non-zero elements in a matrix row) followed by a partition into two ranges: short rows and long rows. Based on this partition, the matrix entries are then transformed into ELLPACK (ELL) or vectorized compressed sparse row (vCSR) format. In addition, the number of threads are adaptively selected based on the row length in order to balance the workload for each GPU thread. Several computational experiments are presented to support this approach and the results suggest a notable improvement over a wide range of matrix structures. Keywords— SpMV, GPU, EVC-HYB format, adaptive	acm/ieee supercomputing conference;bibliothèque de l'école des chartes;blue waters;computation;computer graphics;consortium;data access;experiment;graphics processing unit;mathematical optimization;matrix multiplication;memory bandwidth;national center for supercomputing applications;oracle call interface;petsc;petascale computing;random-access memory;sorting;sparse matrix;the matrix;throughput;vesa enhanced video connector	Dahai Guo;William Gropp;Luke N. Olson	2016	IJHPCA	10.1177/1094342015593156	arithmetic;parallel computing;sparse matrix;computer science;generator matrix;theoretical computer science;adaptive behavior;sparse approximation	HPC	-1.5515042624612831	39.8464373094344	122071
36b8ba0f672ab88417726650d242f7b7496b81c3	energy prediction of cuda application instances using dynamic regression models		GPGPUs no longer seem to be an inconsequential component of supercomputing architectures, and a section of HPC application developers no longer refrain from utilizing GPGPUs. CUDA, in general, has remained a successful computing platform for those architectures. Thousands of scientific applications from various domains, such, as bio-informatics, HEP, and so forth, have been accelerated using CUDA in the past few years. In fact, the energy consumption issue still remains a serious challenge for the HPC and GPGPU communities. This paper proposes energy prediction approaches using dynamic regression models, such as parallel dynamic random forest modeling (P-DynRFM), dynamic random forest modeling (DynRFM), dynamic support vector machines (DynSVM), and dynamic linear regression modeling (DynLRM). These models identify energy efficient CUDA application instances while considering the block size, grid size, and the other tunable parameters, such as problem size. The predictions of CUDA application instances have been attained by executing a few CUDA application instances and predicting the other CUDA application instances based on the performance metrics of applications, such as number of instructions, memory issues, and so forth. The proposed energy prediction mechanisms were evaluated with CUDA applications such as Nbody and Particle Simulations on two GPGPU machines. The proposed dynamic prediction mechanisms achieved a 50.26 to 61.23 percentage of energy/performance prediction improvements when compared to the classical prediction models; and, the parallel implementation of the dynamic RFM (P-DynRFM) recorded over 83 percentage points of prediction time improvements.	algorithm;analysis of algorithms;bioinformatics;block size (cryptography);british informatics olympiad;cuda;exa;framing (world wide web);general-purpose computing on graphics processing units;heterogeneous element processor;indo;learning relationship management;performance prediction;random forest;simulation;supercomputer;support vector machine	R. S. Rejitha;Shajulin Benedict;Suja A. Alex;Shany Infanto	2016	Computing	10.1007/s00607-016-0534-5	parallel computing;real-time computing;computer science;theoretical computer science	HPC	-3.9094998101304608	44.712970958043734	122084
2de6f13caa52e913c3cff8cbeedc9023e0d391dd	traffic sensing through accelerometers	smart phones accelerometers global positioning system mobile communication mobile computing;sensors;receivers;vehicular networks traffic sensing speed estimation;estimation;battery life traffic sensing accelerometers wireless communication mobile computing traffic information system mobile sensors traffic sensing devices gps receiver;global positioning system;mobile communication;vehicles;accelerometers;vehicles mobile communication sensors accelerometers global positioning system receivers estimation	With the advances in wireless communication and mobile computing, a future infrastructureless self-organizing traffic information system, where vehicles can form a network for exchanging traffic information among themselves, will soon be realized. In an infrastructureless traffic information system, vehicles will act as mobile sensors and collect the traffic data as they travel. Smartphones are a great choice for traffic sensing devices as they are now equipped with a variety of sensors such as global positioning system (GPS) receivers, accelerometers, gyroscopes, cameras, and microphones. These sensors can be exploited to collect traffic data. Although there are many types of sensors available for traffic sensing, past studies have mainly focused on a GPS receiver. However, a GPS receiver consumes a lot of power; hence, it can significantly shorten the battery life. In this paper, we explore a possibility of using other types of sensors on a smartphone for traffic sensing. In particular, we investigate whether it is possible and how accurate it is to estimate the average speed of a vehicle from the data sensed by an accelerometer. Two estimation methods will be introduced, and their accuracy will be evaluated.	experiment;global positioning system;independent computing architecture;information system;iterative method;microphone;mobile computing;newton's method;organizing (structure);polynomial;self-organization;sensor;smartphone;smoothing;stationary process;stationary state	Sooksan Panichpapiboon;Puttipong Leakkaw	2016	IEEE Transactions on Vehicular Technology	10.1109/TVT.2015.2448237	embedded system;estimation;mobile telephony;floating car data;global positioning system;telecommunications;computer science;engineering;sensor;mathematics;computer security;accelerometer;statistics	Mobile	-0.05814034326829913	33.70271357154592	122088
0147fa18a952c16df8d6f69c95a387d7319300f9	poly: a new polynomial data structure for maple 17	poly data structure;new data structure;maple library routine;parallel speedup;large subset;high scalability;associated kernel operation;maple kernel;high level;new polynomial data structure;low overhead	We demonstrate how a new data structure for sparse distributed polynomials in the Maple kernel significantly accelerates a large subset of Maple library routines. The POLY data structure and its associated kernel operations (degree, coeff, subs, has, diff, eval, ...) are programmed for high scalability, allowing polynomials to have hundreds of millions of terms, and very low overhead, increasing parallel speedup in existing routines and improving the performance of high level Maple library routines.	data structure;diff utility;eval;high-level programming language;kernel (operating system);maple;overhead (computing);polynomial;scalability;sparse matrix;speedup	Michael B. Monagan;Roman Pearce	2012	ACM Comm. Computer Algebra	10.1145/2429135.2429173	theoretical computer science;algorithm	HPC	-2.6115192725954484	39.82107225485794	122508
5f8883ddb569cbed443efff30af121f5fdf0bc88	exploring the problem of gpu programming for data-intensive applications: a case study of multiple expectation maximization for motif elicitation	gpu;gpgpu;parallelization;many core	Recently General-Purpose Computing on Graphics Processing Units (GPGPU) has been used to reduce the processing time of various applications, but the degree of acceleration by the Graphical Processing Unit (GPU) depends on the application. This study focuses on data analysis as an application example of GPGPU, specifically, the design and implementation of GPGPU computation libraries for data- intensive workloads. The effects of efficient memory allocation and high-speed read-only memories on the execution time are evaluated. In addition to employing a single GPU, the scalability using multiple GPUs is also evaluated. Compared to a Central Processing Unit (CPU) alone, the memory allocation method reduces the execution time for memory copies by approximately 60% when a GPU is used, while utilizing read-only memories results in an approximately 20% reduction in the overall program execution time. Moreover, expanding the number of GPUs from one to four reduces the execution time by approximately 10%.	central processing unit;computation;data-intensive computing;entropy maximization;expectation–maximization algorithm;general-purpose computing on graphics processing units;graphical user interface;graphics processing unit;library (computing);memory management;motif;read-only memory;run time (program lifecycle phase);scalability	Yuki Kitsukawa;Manato Hirabayashi;Shinpei Kato;Masato Edahiro	2014		10.1145/2676585.2676616	parallel computing;real-time computing;computer science;theoretical computer science	HPC	-3.7397550688436434	44.18215010200029	122651
5b25e191bf6748b0d852fee18d4e3f63c336ad95	a fsm based approach for efficient implementation of k-means algorithm		After Fifty years of it's existence the K-means clustering is still popular among researchers due to lower computational complexity. Real time embedded applications require hardwiring of unsupervised learning algorithms like K-means within System-on-Chip for prompt processing in applications like image segmentation, pattern classification, speech recognition etc. This requirement is a must while analyzing Big Datasets. In this manuscript a FSM based architecture is developed for the efficient implementation of K-means algorithm. The proposed architecture has lower computational requirement due to the introduction of concepts like simplified Convergence Checker as well as Fibonacci linear feedback shift register for centroid initialization. To reduce hardware further, Manhattan distance is used as the distance metric instead of the conventional Euclidean distance. Benchmark IRIS flower dataset is used for testing the clustering performance of the proposed architecture. Results obtained after synthesis in Xilinx FPGA Artix7, reveals that the hardware performance is better than previous works, with respect to power (82mW), number of gates, area etc. and has good system clock frequency of 162MHz (6.1592ns), without using any DSP Blocks.	algorithm;benchmark (computing);clock rate;cluster analysis;computation;computational complexity theory;embedded system;euclidean distance;field-programmable gate array;image segmentation;k-means clustering;linear-feedback shift register;machine learning;speech recognition;taxicab geometry;unsupervised learning	Rahul Ratnakumar;Satyasai Jagannath Nanda	2016	2016 20th International Symposium on VLSI Design and Test (VDAT)	10.1109/ISVDAT.2016.8064848	theoretical computer science;electronic engineering;architecture;real-time computing;field-programmable gate array;k-means clustering;initialization;computational complexity theory;cluster analysis;algorithm design;image segmentation;computer science	Arch	5.900171187852476	41.635891949238626	122664
1d5c16ccc468530c3b197da4c2c208841bbeefc4	performance prediction and analysis of parallel out-of-core matrix factorization	algoritmo paralelo;matrix factorization;parallel algorithm;algorithm performance;algorithm complexity;algorithm analysis;complejidad algoritmo;algorithme parallele;descomposicion matricial;complexite algorithme;decomposition matricielle;matrix decomposition;resultado algoritmo;performance algorithme;performance model;performance prediction;analyse algorithme;lu factorization;analisis algoritmo	Description: a step-by-step guide to parallelizing cem codes The future of computational electromagnetics is changing drastically as the new generation of computer chips evolves from single-core to multi-core. The burden now falls on software programmers to revamp existing codes and add new functionality to enable computational codes to run efficiently on this new generation of multi-core CPUs. In this book, you'll learn everything you need to know to deal with multi-core advances in chip design by employing highly efficient parallel electromagnetic code. Focusing only on the Method of Moments (MoM), the book covers: In-Core and Out-of-Core LU Factorization for Solving a Matrix Equation A Parallel MoM Code Using RWG Basis Functions and ScaLAPACK-Based In-Core and Out-of-Core Solvers A Parallel MoM Code Using Higher-Order Basis Functions and ScaLAPACK-Based In-Core and Outof-Core Solvers Turning the Performance of a Parallel Integral Equation Solver Refinement of the Solution Using the Conjugate Gradient Method A Parallel MoM Code Using Higher-Order Basis Functions and Plapack-Based In-Core and Out-ofCore Solvers Applications of the Parallel Frequency Domain Integral Equation Solver Appendices are provided with detailed information on the various computer platforms used for computation; a demo shows you how to compile ScaLAPACK and PLAPACK on the Windows® operating system; and a demo parallel source code is available to solve the 2D electromagnetic scattering problems. Parallel Solution of Integral Equation-Based EM Problems in the Frequency Domain is indispensable reading for computational code designers, computational electromagnetics researchers, graduate students, and anyone working with CEM software.	basis function;central processing unit;code;compiler;computation;computational electromagnetics;conjugate gradient method;incomplete lu factorization;integrated circuit;lapack;lu decomposition;microsoft windows;multi-core processor;need to know;operating system;out-of-core algorithm;parallel computing;performance prediction;programmer;scalapack;single-core;solver	Eddy Caron;Dominique Lazure;Gil Utard	2000		10.1007/3-540-44467-X_15	mathematical optimization;parallel computing;computer science;calculus;mathematics;matrix decomposition;algorithm;algebra	HPC	-3.036029716057869	36.9685565421277	122733
14fa7036a2164830dfe6fe52d05533601a636827	a novel and uniform image partitioning on spiral architecture	spiral architecture;journal article;hexagonal architecture;image partitioning;distributed image processing	Uniform image partitioning based on spiral architecture plays an important role in parallel image processing in many aspects such as uniform data partitioning, load balancing, zero data exchange between the processing nodes et al. However, when the number of partitions is not the power of seven like 7, 49, every sub-image except one is split into a few fragments which are mixed together. We could not tell which fragments belong to which subimage. It is an unacceptable flaw to parallel image processing. This paper proposes a method to resolve the problem mentioned above. From the experimental results, it is shown that the proposed method correctly identifies the fragments belonging to the same subimage and successfully collects them together to be a complete subimage. Then, these subimages can be distributed into the different processing nodes for further processing.	computational science;feature extraction;flaw hypothesis methodology;high-level programming language;image processing;load balancing (computing)	Qiang Wu;Xiangjian He;Tom Hintz;Yuhuang Ye	2006	IJCSE	10.1504/IJCSE.2006.009935	parallel computing;computer science;theoretical computer science;distributed computing	DB	0.18369503192065273	36.952792848155944	123399
18f831eb67a259a09aab98d5397cc03194cc5796	two-hit filter synthesis for genomic database search	genomics;indexes field programmable gate arrays bioinformatics genomics matched filters radiation detectors;radiation detectors;storage allocation bioinformatics field programmable gate arrays genomics information filtering pattern matching query processing;indexes;c reconfigurable computing heterogeneous computing fpga blast approximate string matching regular expression pattern matching high performance computing sequence alignment automata processor database search computational biology bioinformatics;matched filters;field programmable gate arrays;bioinformatics;query workload sizes two hit filter synthesis genomic sequencing genomic database search software tool database query functionally equivalent variation ncbi blast algorithm fpga implementation i o requirement reduction pattern matching throughput chip memory structure chip memory allocation hierarchical table arrangement	Advancements in genomic sequencing technology is causing genomic database growth to outpace Moore's Law. This continues to make genomic database search a difficult problem and a popular target for emerging processing technologies. The de facto software tool for genomic database search is NCBI BLAST, which operates by transforming each database query into a filter that is subsequently applied to the database. This requires a database scan for every query, fundamentally limiting its performance by I/O bandwidth. In this paper we present a functionally-equivalent variation on the NCBI BLAST algorithm that maps more suitably to an FPGA implementation. This variation of the algorithm attempts to reduce the I/O requirement by leveraging FPGA-specific capabilities, such as high pattern matching throughput and explicit on chip memory structure and allocation. Our algorithm transforms the database -- not the query -- into a filter that is stored as a hierarchical arrangement of three tables, the first two of which are stored on chip and the third off chip. Our results show that -- while performance is data dependent -- it is possible to achieve speedups of up to 8X based on the relative reduction in I/O of our approach versus that of NCBI BLAST. More importantly, the performance relative to NCBI BLAST improves with larger databases and query workload sizes.	algorithm;blast;biological database;field-programmable gate array;input/output;map;moore's law;network synthesis filters;overhead (computing);pattern matching;programming tool;throughput	Jordan A. Bradshaw;Rasha Karakchi;Jason D. Bakos	2016	2016 IEEE 24th Annual International Symposium on Field-Programmable Custom Computing Machines (FCCM)	10.1109/FCCM.2016.24	database index;genomics;parallel computing;database tuning;computer science;bioinformatics;theoretical computer science;operating system;database;matched filter;particle detector;field-programmable gate array	DB	0.04677402848958608	43.08629571473549	123432
c10778de6d9799470cdfb75a715cfa7e908804cf	reconfigurable parallel hardware for computing local linear neuro-fuzzy model	cache storage;field programmable gate array;field programmable gate array reconfigurable parallel hardware local linear neuro fuzzy model caching scheme parallel processing engine on chip learning fpga;feed forward;caching scheme;fuzzy neural nets;local linear neuro fuzzy model;reconfigurable architectures;on chip learning;reconfigurable parallel hardware;fpga;system on chip cache storage field programmable gate arrays fuzzy neural nets parallel architectures reconfigurable architectures;null;chip;parallel architectures;system identification;system on chip;neuro fuzzy;pattern recognition;hardware concurrent computing engines parallel processing employment pattern recognition system identification feeds costs field programmable gate arrays;field programmable gate arrays;parallel processing engine;hardware implementation;parallel processing	Because of the computational-intensive nature of local linear neuro-fuzzy model (LLNFM), effective employment of LLNFM requires dedicated hardware implementation that can be reconfigured for different applications such as pattern recognition, system identification with ensured reusability. This paper introduces a new versatile and reusable parallel hardware engine for computations of LOLIMOT algorithm in local linear neuro-fuzzy model. Our LOLIMOT hardware engine exploits the inherent parallelism and reusability of data and redundancies of computation with its effective caching scheme and parallel processing engines. The designed hardware element for feed forward step can also perform training step with acceptable overhead which is implemented. This overcomes the on-chip learning complexity cost. Synthesis and implementation results on FPGA beds are presented to show its power of computations on reconfigurable platforms	algorithm;computation;field-programmable gate array;neuro-fuzzy;overhead (computing);parallel computing;pattern recognition;system identification	A. Pedram;Mohammad Reza Jamali;Sied Mehdi Fakhraie;Caro Lucas	2006	International Symposium on Parallel Computing in Electrical Engineering (PARELEC'06)	10.1109/PARELEC.2006.70	embedded system;parallel processing;parallel computing;real-time computing;computer science;operating system;field-programmable gate array	Arch	4.383392956488187	44.4488730682931	123530
7c4fc33c061744b3139c5a3dfb331fe5031987e9	performance of sequence alignment bioinformatics applications on general purpose processors: a case study	floating point;sequence alignment	Aligning specific sequences against other known sequences in a database is a central aspect of bioinformatics. New experimental data being added continuously to these databases necessitates the use of efficient computational resources. Although processor clock frequencies speeds have been increasing steadily, it does not necessarily translate into better performance. In this paper, we analyze the performance of sequence alignment bioinformatics benchmarks from the BLAST and FASTA suites from a microarchitectural standpoint. Results indicate that these benchmarks are memory intensive with negligible floating point content. They exhibit parallelism at the instruction level and their performance can be improved significantly by using bigger caches.	blast;benchmark (computing);bioinformatics;clock rate;computation;computational resource;database;fasta;microarchitecture;parallel computing;sequence alignment	Pradeep R. Nair;Eugene John	2006			theoretical computer science;floating point;sequence alignment;computer science	HPC	-0.7768220964368504	43.44295062836058	123888
28df0ee32effb1ae47e6ad41ffe640792f930bb7	efficient exhaustive verification of the collatz conjecture using dsp48e blocks of xilinx virtex-5 fpgas	field programmable gate array;field programmable gate array efficient exhaustive verification collatz conjecture dsp48e blocks xilinx virtex 5 fpgas coprocessor;collatz conjecture;xilinx virtex 5 fpgas;dsp48e blocks;coprocessor;dsp blocks;fpga implementation;block rams;formal verification;efficient implementation;efficient exhaustive verification;formal verification field programmable gate arrays;field programmable gate arrays;block rams hardware algorithm collatz conjecture fpga implementation dsp blocks;hardware algorithm;exhaustive search	Consider the following operation on an arbitrary positive number: if the number is even, divide it by two, and if the number is odd, triple it and add one. The Collatz conjecture asserts that, starting from any positive number m, repeated iteration of the operations eventually produces the value 1. The main contribution of this paper is to present an efficient implementation of a coprocessor that performs the exhaustive search to verify the Collatz conjecture using a DSP48E Xilinx Virtex-5 blocks, each of which contains one multiplier and one adder. The experimental results show that, our coprocessor can verify 3.88 × 108 64-bit numbers per second.	64-bit computing;adder (electronics);brute-force search;coprocessor;field-programmable gate array;iteration	Yasuaki Ito;Koji Nakano	2010	2010 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum (IPDPSW)	10.1109/IPDPSW.2010.5470837	embedded system;computer architecture;parallel computing;computer science	Embedded	8.570408573105777	43.052825519704584	124001
ca7c5a66db87bed73daa734efd17c2d51a0dc625	time-of-flight distance measurements using smart phones	time of arrival toa;software;distance measurements;time of flight tof;human computer interaction;smart phones;android;monotonic wireless sensor networks wsn distance measurements time of flight tof time of arrival toa smart phones android host controller interface hci snoop btsnoop;monotonic;accuracy;distance measurement;wireless sensor networks wsn;host controller interface hci;snoop;human computer interaction smart phones wireless sensor networks software java accuracy distance measurement;btsnoop;wireless sensor networks distance measurement smart phones;wsn time of flight distance measurement smart phone tof wireless sensor network;wireless sensor networks;java	An application of using time of flight (TOF) to measure distance between two wireless sensor network (WSN) nodes is explained. Both nodes are identical smart phones. Results show a large accuracy error (106m) but with several avenues for improvement. A discussion of the results and future work follows.	smartphone	Jacob Phillips;Robert C. Green;Mansoor Alam	2014	2014 IEEE International Conference on Mobile Services	10.1109/MobServ.2014.31	embedded system;real-time computing;engineering;computer network	Robotics	5.219771063003391	35.164203708092344	124240
39f2bc5b55aa3079249d4b7bef0ed0108a16439b	massively parallel realization of logical operations in distributed parallel systems	hardware accelerator;time delay;computational logic;parallel systems;necessary and sufficient condition;high performance;massively parallel processing	A parallel hardware accelerator for the speed-up of massively parallel processing systems is discussed. Its architecture is based on multi-functional combinational networks distributed over many processors and interconnected by sets of open collector lines. This accelerator allows to compute logical vector functions, based on searching, comparison, and sorting operations, and parallel exchange of data. Each processor is able to determine the correct result of an operation by taking into account its local characteristics only and those specified for the whole system. To achieve this result we introduce a new method which minimizes the processing time. The method is based on proving necessary and sufficient conditions, which allow to form all possible sets of individual time delays for processors involved in the parallel logical operation. The results shown allow to speed up the hardware realization of massively logical operations in multiple bus parallel processors. They can also be used for designing specialized high performance systems, e.g. schemes for finding maximum or minimum values, the median, or for implementing different priority schemes.		Mikhail Makhaniok;Victor Cherniavsky;Reinhard Männer;Oliver Stucky	1990		10.1007/3-540-53065-7_155	computer architecture;parallel computing;embarrassingly parallel;computer science;massively parallel;distributed computing	HPC	0.27000853119652435	38.88294765121555	124322
30c3a862a34db8933225c78d5031bba299d5ddc5	legato: first steps towards energy-efficient toolset for heterogeneous computing		LEGaTO is a three-year EU H2020 project which started in December 2017. The LEGaTO project will leverage task-based programming models to provide a software ecosystem for Made-in-Europe heterogeneous hardware composed of CPUs, GPUs, FPGAs and dataflow engines. The aim is to attain one order of magnitude energy savings from the edge to the converged cloud/HPC.		Adrián Cristal;Osman S. Unsal;Xavier Martorell;Paul Carpenter;Raúl de la Cruz;Leonardo Bautista-Gomez;Daniel A. Jiménez;Carlos Álvarez;Behzad Salami;Sergi Madonar;Miquel Pericàs;Pedro Trancoso;Micha vor dem Berge;Gunnar Billung-Meyer;Stefan Krupop;Wolfgang Christmann;Frank Klawonn;Amani Mihklafi	2018		10.1145/3229631.3239370	symmetric multiprocessor system;parallel computing;cloud computing;programming paradigm;software ecosystem;computer architecture;hardware security module;efficient energy use;dataflow;computer science;legato	Arch	-2.1697034028883624	46.369014169706446	124494
900a504a8fe4ac26336acaff7b799b124c80115f	the gsrc: bridging academia and industry	libraries;concurrent computing computer industry multithreading hardware parallel processing space technology government laboratories libraries;concurrent computing;high performance computing;government;computer industry;amd;chip;parallelism;concurrent systems;operating system;us department of defense;engineering education;tool providers;high performance computer;system development;amd high performance computing parallelism application writers tool providers concurrent systems;space technology;application writers;parallel processing;chip scale packaging;hardware;multithreading	Finding parallelism in application code and exploiting it through automatic tools is the Holy Grail of high-performance computing. Success in this endeavor requires major industry participation, not only among processor chip and hardware system developers but also among operating system and tool providers. Most important, participation is needed among the application writers and developers who will in many ways drive toward acceptable solutions.	bridging (networking);operating system;parallel computing;supercomputer	Richard Oehler	2008	IEEE Design & Test of Computers	10.1109/MDT.2008.112	chip;embedded system;parallel processing;computer architecture;parallel computing;engineering education;multithreading;concurrent computing;computer science;engineering;operating system;software engineering;space technology;programming language;government;computer engineering	HPC	-2.7546257587800085	44.93947319451605	124524
79cec2888cdcdd3c4ec78a69ad0be5a52c700af6	optimizing matrix multiplication for a short-vector simd architecture - cell processor	processing element;instruction level parallel;dense linear algebra;linear system of equations;cell processor;eigenvalues;vectorization;single instruction multiple data;least squares problem;numerical algorithm;matrix multiplication;loop optimization;instruction level parallelism;data structure;synergistic processing element;loop optimizations	Matrix multiplication is one of the most common numerical operations, especially in the area of dense linear algebra, where it forms the core of many important algorithms, including solvers of linear systems of equations, least square problems, and singular and eigenvalue computations. The STI CELL processor exceeds the capabilities of any other processor available today in terms of peak single precision, floating point performance, aside from special purpose accelerators like Graphics Processing Units (GPUs). In order to fully exploit the potential of the CELL processor for a wide range of numerical algorithms, fast implementation of the matrix multiplication operation is essential. The crucial component is the matrix multiplication kernel crafted for the short vector Single Instruction Multiple Data architecture of the Synergistic Processing Element of the CELL processor. In this paper, single precision matrix multiplication kernels are presented implementing the C 1⁄4 C A B operation and the C 1⁄4 C A B operation for matrices of size 64 64 elements. For the latter case, the performance of 25.55 Gflop/s is reported, or 99.80% of the peak, using as little as 5.9 kB of storage for code and auxiliary data structures. 2009 Elsevier B.V. All rights reserved.	algorithm;cell (microprocessor);computation;data structure;flops;graphics processing unit;linear algebra;linear system;matrix multiplication;numerical analysis;optimizing compiler;simd;single-precision floating-point format;synergy;the matrix	Jakub Kurzak;Wesley Alvaro;Jack J. Dongarra	2009	Parallel Computing	10.1016/j.parco.2008.12.010	system of linear equations;computer architecture;parallel computing;data structure;simd;eigenvalues and eigenvectors;matrix multiplication;computer science;loop optimization;theoretical computer science;vectorization;instruction-level parallelism;algebra	HPC	-2.730028059451412	39.880383962282025	125000
a0316c2ac671dcf3721f0db1fdda5325400ad181	sparse matrix-vector multiplication based on network-on-chip: on data mapping	omnet based noc simulator sparse matrix vector multiplication network on chip data mapping smvm parallel hardware irregular data communication requirements communication volume data distribution;parallel architectures programming;network on chip;data communication;public domain software;sparse matrices;parallel processing;sparse matrices data communication network on chip parallel processing public domain software	Sparse Matrix-Vector Multiplication (SMVM) on parallel hardware is a very sophisticated problem because of the irregular data communication requirements. The communication volume in the parallel hardware is determined by how data is distributed among the processing elements. In this paper we introduce two methods of data mapping for SMVM based on Network-on-Chip (NoC) in order to spread the load among its components. Later, we introduce the effect of reordering of the sparse matrix on those mapping methods. Simulations are performed using an OMNeT++ based NoC simulator.	network on a chip;requirement;simulation;sparse matrix	Ahmad Mansour;Jürgen Götze	2012	2012 Fifth International Symposium on Parallel Architectures, Algorithms and Programming	10.1109/PAAP.2012.14	computer architecture;parallel computing;computer science;theoretical computer science	HPC	-3.6301313467225955	38.98420206980203	125032
1eda9a2c40d793232ebb153721704092a92cd005	graph-based divide and conquer method for parallelizing spatial operations on vector data	computing efficiency;journal;spatial data dependence;spatial operations;graph;divide and conquer method	In computer science, dependence analysis determines whether or not it is safe to parallelize statements in programs. In dealing with the data-intensive and computationally intensive spatial operations in processing massive volumes of geometric features, this dependence can be well utilized for exploiting the parallelism. In this paper, we propose a graph-based divide and conquer method for parallelizing spatial operations (GDCMPSO) on vector data. It can represent spatial data dependences in spatial operations through representing the vector features as graph vertices, and their computational dependences as graph edges. By this way, spatial operations can be parallelized in three steps: partitioning the graph into graph components with inter-component edges firstly, simultaneously processing multiple subtasks indicated by the graph components secondly and finally handling remainder tasks denoted by the inter-component edges. To demonstrate how it works, buffer operation and intersection operation under this paradigm are conducted. In a 12-core environment, the two spatial operations both gain obvious performance improvements, and the speedups are more than eight. The testing results suggest that GDCMPSO contributes to a method for parallelizing spatial operations and can greatly improve the computing efficiency on multi-core architectures. OPEN ACCESS Remote Sens. 2014, 6 10108	automatic parallelization;boosting (machine learning);computer science;data dependency;data-intensive computing;dependence analysis;multi-core processor;parallel computing;programming paradigm;tag (game)	Xiaochen Kang;Xiangguo Lin	2014	Remote Sensing	10.3390/rs61010107	parallel computing;computer science;theoretical computer science;spatial network;distributed computing;graph;graph operations	DB	-3.6186802758771974	42.869684945654065	125285
b0b2478ed3874fa0ff5718726e7f0e858664bb3b	mapreduce based parallel suffix tree construction for human genome	bioinformatics genomics indexing random access memory partitioning algorithms data structures buildings;suffix tree;indexing;genome;parallel;map reduce;indexing map reduce suffix tree genome parallel;trees mathematics bioinformatics data structures genomics indexing parallel processing;mapreduce based parallel suffix tree construction uniprocessor system multiprocessor system commodity cluster space time tradeoff data structure sequence alignment application read mapping application bioinformatics application genome indexing technique human genome	Genome indexing is the basis for many bioinformatics applications. Read mapping(sequence alignment) is one such application where the goal is to align millions of short reads against reference genome. Several tools are available for read mapping which rely on different indexing techniques to expedite the alignment process. However, many of these contemporary alignment programs are sequential, memory intensive and cannot be easily scaled for larger genomes. Suffix tree is one of the most widely used data structures for indexing strings (genomes). Building a scalable suffix-tree based tool is particularly challenging due to the difficulties involved in parallel construction of the suffix tree. Several suffix tree construction techniques have been proposed till date with focus on space-time tradeoff. Most of these existing works address the construction issue for uniprocessor and cannot be easily extended to utilize modern multi-processor systems. In this paper we investigate and propose a MapReduce based parallel construction of suffix tree. We demonstrate the performance of the algorithm over commodity cluster using up to 32 nodes each having 8GB of primary memory.	algorithm;align (company);beowulf cluster;bioinformatics;computer data storage;data structure;in-memory database;mapreduce;multiprocessing;scalability;space–time tradeoff;suffix tree;tree (data structure);uniprocessor system;whole earth 'lectronic link	Umesh Chandra Satish;Praveenkumar Kondikoppa;Seung-Jong Park;Manish Patil;Rahul Shah	2014	2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/PADSW.2014.7097867	search engine indexing;parallel computing;computer science;bioinformatics;theoretical computer science;parallel;database;compressed suffix array;genome	DB	-0.16711249377132348	42.59693789120561	125465
ce5e192b346b0f2313f277bd768885bb93c22c8d	optimising memory management for belief propagation in junction trees using gpgpus	memory layout optimization memory management belief propagation junction trees gpgpu general purpose graphics processing unit bayesian network bp bn parallel architecture;storage management belief maintenance belief networks graphics processing units;belief propagation on junction trees;gpgpus;instruction sets memory management particle separators junctions parallel processing indexes algorithm design and analysis;gpgpus belief propagation on junction trees	Belief Propagation (BP) in Junction Trees (JT) is one of the most popular approaches to compute posteriors in Bayesian Networks (BN). Such approach has significant computational requirements that can be addressed by using highly parallel architectures (i.e., General Purpose Graphic Processing Units) to parallelise the message update phases of BP. In this paper, we propose a novel approach to parallelise BP with GPGPUs, which focuses on optimising the memory layout of the BN tables so to achieve better performance in terms of increased speedup, reduced data transfers between the host and the GPGPU, and scalability. Our empirical comparison with the state of the art approach on standard datasets confirms significant improvements in speedups (up to +594%), and scalability (as our method can operate on networks whose potential tables exceed the global memory of the GPGPU).	bayesian network;belief propagation;computation;constraint satisfaction;general-purpose computing on graphics processing units;graphics processing unit;jt (visualization format);mathematical optimization;memory management;message passing;requirement;scalability;software propagation;speedup	Filippo Bistaffa;Alessandro Farinelli;Nicola Bombieri	2014	2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)	10.1109/PADSW.2014.7097850	parallel computing;real-time computing;computer science;theoretical computer science;operating system;database;distributed computing;programming language	HPC	0.40809513416903515	41.779178829592354	125497
9a6d2d27e865aa0808b9f93499ce413b210db382	inexpensive correctly rounded floating-point division and square root with input scaling	konferenssijulkaisu conference paper;antenna arrays;software radio;digital signal processing chips;floating point arithmetic;mimo communication	Recent embedded DSPs are incorporating IEEE-compliant floating point arithmetic to ease the development of, e.g., multiple antenna MIMO in software-defined radio. An obvious choice of FPU architecture in DSP is to include a fused multiply-add (FMA) operation, which accelerates most DSP applications. Another advantage of FMA is that it enables fast software algorithms for, e.g., division and square root without much additional hardware. However, these algorithms are nontrivial to perform at the target accuracy to get the correctly rounded result without danger of overflow. Previous FMA-based systems either rely on a power-hungry wide intermediate format or forego correct rounding. A wide format is unattractive in a power-sensitive embedded environment since it requires enlarged register files, wider data buses and possibly a larger multiplier. We present provably correct algorithms for efficient IEEE-compliant division and square root with only a 32-bit format using hardware prescaling and postscaling steps. The required hardware has approximately 8% of area and power footprint of a single FMA unit.	32-bit;algorithm;bus (computing);correctness (computer science);embedded system;fma instruction set;floating-point unit;image scaling;mimo;multiply–accumulate operation;operating system;register file;rounding	Timo Viitanen;Pekka Jääskeläinen;Jarmo Takala	2013	SiPS 2013 Proceedings	10.1109/SiPS.2013.6674498	embedded system;real-time computing;computer hardware;telecommunications;computer science;floating point;electrical engineering;operating system;software-defined radio	Arch	7.319112939754966	43.32566285272148	125525
6fb1d54bb753489a678774c4d3500fc2f82981f4	a domain-specific architecture for deep neural networks		Tensor processing units improve performance per watt of neural networks in Google datacenters by roughly 50x.	artificial neural network;deep learning;performance per watt;tensor processing unit	Norman P. Jouppi;Cliff Young;Nishant Patil;David A. Patterson	2018	Commun. ACM	10.1145/3154484	theoretical computer science;architecture;artificial neural network;computer science	Arch	3.1914767326451723	42.94593538150849	125651
2bf1ddf1bbf46cce54cae18225df09a3027dc66a	fpga-based hyperspectral covariance coprocessor for size, weight, and power constrained platforms	geophysical image processing;aerospace engineering;remote sensing coprocessors field programmable gate arrays floating point arithmetic geophysical image processing;field programmable gate arrays coprocessors hardware hyperspectral sensors sensors clocks;coprocessors;electrical and computer engineering;airborne;remote sensing;spaceborne;swap;floating point arithmetic;field programmable gate arrays;floating point operations per watt kg fpga based hyperspectral covariance coprocessor size weight and power constrained platforms swap constrained remote sensing platforms unmanned air vehicles uav microsatellites image processing hardware covariance calculation hyperspectral image processing;hyperspectral	Size, weight, and power (SWaP) are important factors in the design of any remote sensing platform. These remote sensing platforms, such as Unmanned Air Vehicles (UAVs) and microsatellites, are becoming increasingly small. This creates a need for remote sensing and image processing hardware that consumes less area, weight, and power, while delivering processing performance. It is also advantageous to utilize the same hardware for multiple platform tasks. The purpose of this research is to design and characterize an FPGA-based hardware coprocessor that parallelizes the calculation of covariance, a time-consuming step common in hyperspectral image processing. Our design is compared to a CPU-based implementation and shown to have an overall SWaP advantage. We evaluate our coprocessor using a metric that is useful in the consideration of future SWaP-constrained remote sensing platforms: floating point operations per Watt-kg (FLOPs/W-kg). Additional hardware capacity exists in our design to implement other remote sensing platform tasks.	central processing unit;coprocessor;field-programmable gate array;image processing;parallel computing;unmanned aerial vehicle	David A. Kusinsky;Miriam Leeser	2013	2013 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2013.6670331	embedded system;electronic engineering;computer hardware;computer science	EDA	3.0446671596751034	45.683060796630876	125928
f36988988f6b43a6c1eae1d6325b4c72ac9712b8	parallel efficient hierarchical algorithms for module placement of large chips on distributed memory architectures	distributed memory;parallel distributed memory computers parallel efficient hierarchical algorithms large chip module placement distributed memory architectures proud hierarchical decomposition technique sparse linear systems resistive network analogy simulated annealing mproud algorithm coefficient matrices numerical stability improved proud algorithm iproud computational costs symmlq minres parallel algorithm matrix vector multiplications bottleneck;numerical stability;distributed memory systems;parallel algorithm;matrix multiplication parallel algorithms distributed memory systems parallel architectures sparse matrices numerical stability circuit layout cad;distributed memory architecture;inner product;memory architecture linear systems algorithm design and analysis circuit simulation simulated annealing convergence of numerical methods computational efficiency parallel algorithms costs global communication;simulated annealing;parallel architectures;circuit layout cad;matrix multiplication;sparse matrices;sparse linear system;parallel algorithms	The PROUD module placement algorithm mainly uses a hierarchical decomposition technique and the solution of sparse linear systems based on a resistive network analogy. It has been shown that the PROUD algorithm can achieve a comparable design of the placement problems for very large circuits with the best placement algorithm based on simulated annealing, but with several order of magnitude faster. The modified PROUD, namely MPROUD algorithm by perturbing the coefficient matrices performs much faster that the original PROUD algorithm. Due to the instability and unguaranteed convergence of MPROUD algorithm, we have proposed a new convergent and numerically stable PROUD, namely Improved PROUD algorithm, denoted as IPROUD with attractive computational costs to solve the module placement problems by making use of the SYMMLQ and MINRES methods based on Lanczos process (Yang, 1997). We subsequently propose parallel versions of the improved PROUD algorithms. The parallel algorithm is derived such that all inner products and matrix-vector multiplications of a single iteration step are independent. Therefore, the cost of global communication which represents the bottleneck of the parallel performance on parallel distributed memory computers can be significantly reduced, therefore, to obtain another order of magnitude improvement in the runtime without loss of the quality of the layout.		Laurence Tianruo Yang	2002		10.1109/PCEE.2002.1115310	parallel computing;computer science;theoretical computer science;distributed computing;parallel algorithm	EDA	-2.854564127233332	38.338980879529934	126002
4758023c09997b4e57d97e83f1c3f0e11577b9e5	an evaluation of left-looking, right-looking and multifrontal approaches to sparse cholesky factorization on hierarchical-memory machines	sparse cholesky factorization;caches;data management;multiprocessors;computer communications;systems analysis;structured programming;algorithms;computer benchmarking;sparse matrix;distributed data processing;hierarchical memory systems	In this paper we present a comprehensive analysis of the performance of a variety of sparse Cholesky factorization methods on hierarchical-memory machines. We investigate methods that vary along two different axes. Along the first axis, we consider three different high-level approaches to sparse factorization: leftlooking, right-looking, and muhi.frontal. Along the second axis, we consider the implementation of each of these high-level approaches using different sets of primitives. The primitives vary based on the structures they manipulate. One important structure in sparse Cholesky factorization is a single column of the matrix. We first consider primitives that manipulate single columns. These are the most commonly used primitives for expressing the sparse Cholesky computation. Another important structure is the supemode, a set of columns with identical non-zero structures. We consider sets of primitives that exploit the supernodal structure of the matrix to varying degrees. We find that primitives that manipulate larger structures greatly increase the amount of exploitable data reuse, thus leading to dramatically higher performance on hierarchical-memory machines. We observe performance increases of two to three times when comparing methods based on primitives that make extensive use of the supernodal structure to methods based on primitives that manipulate columns. We also find that the overall approach (left-looking, right-looking, or mu.ltifrontal) is less important for performance than the particular set of primitives used to implement the approach.	apache axis;cholesky decomposition;column (database);computation;high- and low-level;sparse matrix;the matrix	Edward Rothberg;Anoop Gupta	1993	International Journal of High Speed Computing	10.1142/S0129053393000232	systems analysis;parallel computing;sparse matrix;data management;computer science;theoretical computer science;distributed computing;minimum degree algorithm;structured programming;algebra	HPC	-2.1263680387612665	38.313332928695615	126271
22f86ab03cd2a684c8838acd0aef8ea2c6fa1aa3	resource allocation for multiple concurrent in-network stream-processing applications	difference operator;resource allocation;operator mapping;multiple concurrent applications;polynomial heuristics;trees of operators;polynomial time;stream processing;integer linear program;in network stream processing;steady state	This paper investigates the operator mapping problem for in-network stream-processing applications. In-network stream-processing amounts to applying one or more trees of operators in steady-state, to multiple data objects that are continuously updated at different locations in the network. The goal is to compute some final data at some desired rate. Different operator trees may share common subtrees. Therefore, it may be possible to reuse some intermediate results in different application trees. The first contribution of this work is to provide complexity results for different instances of the basic problem, as well as integer linear program formulations of various problem instances. The second second contribution is the design of several polynomial-time heuristics. One of the primary objectives of these heuristics is to reuse intermediate results shared by multiple applications. Our quantitative comparisons of these heuristics in simulation demonstrates the importance of choosing appropriate processors for operator mapping. It also allow us to identify a heuristic that achieves good results in practice.	central processing unit;heuristic (computer science);linear programming;simulation;steady state;stream processing;time complexity;tree (data structure)	Anne Benoit;Henri Casanova;Veronika Rehn-Sonigo;Yves Robert	2011	Parallel Computing	10.1016/j.parco.2010.09.005	time complexity;mathematical optimization;discrete mathematics;parallel computing;stream processing;resource allocation;computer science;operator;theoretical computer science;mathematics;steady state;algorithm	DB	4.014856423448904	37.10743865653137	126351
15f8d84018844ea2facd367b580c1a3436ac384a	bringing gesture recognition to all devices	allsee prototype;gesture-recognition system;allsee consumes;always-on gesture recognition;bringing gesture recognition;significant power;computational resource;computational overhead;gesture information;gesture recognition;magnitude lower power	Existing gesture-recognition systems consume significant power and computational resources that limit how they may be used in low-end devices. We introduce AllSee, the first gesture-recognition system that can operate on a range of computing devices including those with no batteries. AllSee consumes three to four orders of magnitude lower power than state-of-the-art systems and can enable always-on gesture recognition for smartphones and tablets. It extracts gesture information from existing wireless signals (e.g., TV transmissions), but does not incur the power and computational overheads of prior wireless approaches. We build AllSee prototypes that can recognize gestures on RFID tags and power-harvesting sensors. We also integrate our hardware with an off-the-shelf Nexus S phone and demonstrate gesture recognition in through-the-pocket scenarios. Our results show that AllSee achieves classification accuracies as high as 97% over a set of eight gestures.	computational resource;gesture recognition;high availability;nexus s;radio-frequency identification;sensor;smartphone	Bryce Kellogg;Vamsi Talla;Shyamnath Gollakota	2014			embedded system;speech recognition;computer science;operating system;computer security	Networks	2.9667258981629923	34.61652421946654	126668
416aadb5932e98ec7e434c29f84ad78e5b1f1fac	a data parallel approach to genetic programming using programmable graphics hardware	processor architecture;data parallel;genetic program;perforation;data parallelism;genetic programming;genetic program ming;graphics cards;next generation;programmable graphics hardware;scientific communication	In recent years the computing power of graphics cards has increased significantly. Indeed, the growth in the computing power of these graphics cards is now several orders of magnitude greater than the growth in the power of computer processor units. Thus these graphics cards are now beginning to be used by the scientific community aslow cost, high performance computing platforms. Traditional genetic programming is a highly computer intensive algorithm but due to its parallel nature it can be distributed over multiple processors to increase the speed of the algorithm considerably. This is not applicable for single processor architectures but graphics cards provide a mechanism for developing a data parallel implementation of genetic programming. In this paper we will describe the technique of general purpose computing using graphics cards and how to extend this technique to genetic programming. We will demonstrate the improvement in the performance of genetic programming on single processor architectures which can be achieved by harnessing the computing power of these next generation graphics cards.	central processing unit;computer;data parallelism;geforce;genetic programming;graphics hardware;graphics processing unit;supercomputer;video card;while	Darren M. Chitty	2007		10.1145/1276958.1277274	genetic programming;computer architecture;graphics pipeline;parallel computing;microarchitecture;computer science;theoretical computer science;real-time computer graphics;data parallelism;graphics address remapping table;general-purpose computing on graphics processing units	HPC	-0.4856452648823514	44.552581876709816	126740
74fcef7da430cc9b3c06cdd19288d0ba19eabf20	communication-driven automatic virtual prototyping for networked embedded systems	software;dse;bridges;automatic virtual prototyping;productivity gain communication driven automatic virtual prototyping networked embedded systems esl model virtual prototype bridge components design space exploration dse latency bridges process communication driven decomposition optimized implementation solution synthesis tool system integration process interface realization distributed control application;virtual prototyping embedded systems;virtual prototyping;computational modeling;unified modeling language;bridges hardware software ports computers virtual prototyping computational modeling unified modeling language;ports computers;communication driven decomposition esl dse automatic virtual prototyping;esl;communication driven decomposition;hardware	Today, parts of an ESL model can be automatically synthesized to a low-level implementation, e. g., via high-level synthesis. However, to build a complete working virtual prototype directly from a given ESL model, one still has to perform several design steps manually. The work-at-hand tackles this problem by introducing bridge components already in the ESL model. These components influence Design Space Exploration (DSE) by adding their characteristics like cost and latency into evaluation. The complete system is divided into several subsystems connected through bridges, we call this process communication-driven decomposition. Once an optimized implementation solution is found by DSE and selected by the designer, every subsystem of this ESL model is handed over to the individual synthesis tool. Here, if two subsystems will be synthesized by different tools, the bridge connecting these two subsystems will be automatically duplicated into two instances and assigned to each subsystem. Then, synthesis tools generate code for each subsystem (including the bridge inside each subsystem). In the last step, the system integration process merges the corresponding bridge pairs together to build a complete virtual prototype. To automate the proposed design flow, we have developed a framework that automatically divides an ESL model into subsystems and synthesizes the interfaces for all bridges which strongly simplifies system integration. The designer is therefore free from the interface realization. Hence, the overall design development cycle is shortened. As a proof of concept, a distributed control application is presented to give evidence of the proposed technique's applicability and the achieved productivity gain.	automatic control;co-simulation;cyber-physical system;design flow (eda);design space exploration;distributed control system;embedded system;high- and low-level;high-level synthesis;institut für dokumentologie und editorik;network planning and design;prototype;semiconductor industry;simulation;simulink;system integration;systemc;systems modeling;vii	Liyuan Zhang;Joachim Falk;Tobias Schwarzer;Michael Glaß;Jürgen Teich	2014	2014 17th Euromicro Conference on Digital System Design	10.1109/DSD.2014.88	unified modeling language;embedded system;real-time computing;computer science;operating system;computational model	EDA	5.658716019715372	38.49042846864461	126751
8557931c1ed2a45656f07935726f3eccc044e98b	a simple architectural enhancement for fast and flexible elliptic curve cryptography over binary finite fields gf(2m)	public key cryptography;diseno circuito;arithmetic operation;cryptographie cle publique;wireless devices;informatique mobile;elliptic curve;reconfigurable architectures;operation arithmetique;circuit design;operacion aritmetica;securite donnee;satisfiability;corps fini;courbe elliptique;power supply;reduction argument;general purpose processor;finite field;computer architecture;elliptic curve cryptography;architecture ordinateur;curva eliptica;alimentation electrique;red celular;cell network;reseau cellulaire;campo finito;code size;conception circuit;arquitectura ordenador;alimentacion electrica;mobile computing;range reduction;architecture reconfigurable;security of data	Mobile and wireless devices like cell phones and networkenhanced PDAs have become increasingly popular in recent years. The security of data transmitted via these devices is a topic of growing importance and methods of public-key cryptography are able to satisfy this need. Elliptic curve cryptography (ECC) is especially attractive for devices which have restrictions in terms of computing power and energy supply. The efficiency of ECC implementations is highly dependent on the performance of arithmetic operations in the underlying finite field. This work presents a simple architectural enhancement to a generalpurpose processor core which facilitates arithmetic operations in binary finite fields GF(2). A custom instruction for a multiply step for binary polynomials has been integrated into a SPARC V8 core, which subsequently served to compare the merits of the enhancement for two different ECC implementations. One was tailored to the use of GF(2) with a fixed reduction polynomial. The tailored implementation was sped up by 90% and its code size was reduced. The second implementation worked for arbitrary binary fields with a range of reduction polynomials. The flexible implementation was accelerated by a factor of nearly 10.	central processing unit;elliptic curve cryptography;executable;general-purpose markup language;general-purpose modeling;mobile phone;multi-core processor;personal digital assistant;polynomial;public-key cryptography;random-access memory;requirement;sparc;server (computing)	Stefan Tillich;Johann Großschädl	2004		10.1007/978-3-540-30102-8_24	embedded system;computer science;theoretical computer science;operating system;circuit design;mathematics;distributed computing;elliptic curve cryptography;public-key cryptography;elliptic curve;mobile computing;computer security;finite field;algorithm;satisfiability	Arch	8.112298520126314	44.643068231398985	126822
d37609916d3f66fc3eb84d27635ec62df1a5a830	simultaneous data compression and encryption	encryption.;. data compression;data compression;huffman codes;arithmetic coding	This paper describes cryptographic methods for concealing information during data compression processes. These include novel approaches of adding pseudo random shuffles into the processes of dictionary coding (Lampel-Ziv compression), arithmetic coding, and Huffman	arithmetic coding;cryptography;data compression;dictionary coder;encryption;huffman coding;pseudorandomness	Chung-E Wang	2003			computer network;computer science;modified huffman coding;theoretical computer science;tunstall coding;data compression;lempel–ziv–stac;context-adaptive binary arithmetic coding;shannon–fano coding;lossless compression;data compression ratio	Theory	9.001554666706154	36.75520736485296	127019
11c5406cae551e5c241d77a8efb6185b6cf9bdfa	an overview of datatype quantization techniques for convolutional neural networks		Convolutional Neural Networks (CNNs) are becoming increasingly popular due to their superior performance in the domain of computer vision, in applications such as objection detection and recognition. However, they demand complex, power-consuming hardware which makes them unsuitable for implementation on low-power mobile and embedded devices. In this paper, a description and comparison of various techniques is presented which aim to mitigate this problem. This is primarily achieved by quantizing the floating-point weights and activations to reduce the hardware requirements, and adapting the training and inference algorithms to maintain the network’s performance.	algorithm;backpropagation;best, worst and average case;computation;computer vision;convolutional neural network;embedded system;field-programmable gate array;low-power broadcasting;microcontroller;mobile device;multiply–accumulate operation;neural networks;requirement;software propagation	Ali Athar	2018	CoRR		convolutional neural network;machine learning;quantization (signal processing);artificial intelligence;inference;computer science	Vision	3.7264002484912844	42.4824033138959	127027
736e5cbb23b1b1f771b1aa07dd0b5dda13d0e87b	synfire chain emulation by means of flexible snn modeling on a simd multicore architecture		The implementation of a synfire chain (SFC) application that performs synchronous alignment mapped on a hardware multi-processor architecture (SNAVA) is reported. This demonstrates a flexible SNN modeling capability of the architecture. The neural algorithm is executed by means of a digital Spiking Neural Network (SNN) emulator, using single instruction multiple data (SIMD) processing. The flexibility and capability of SNAVA to solve complex nonlinear algorithm was verified using time slot emulation on a customized neural topology. The SFC application has been implemented on an FPGA Kintex 7 using a network of 200 neurons with 7500 synaptic connections.	artificial neural network;emulator;multi-core processor;simd	Mireya Zapata;Jordi Madrenas	2016		10.1007/978-3-319-44778-0_43	computer architecture;parallel computing;real-time computing;computer science	EDA	3.219964921095089	44.744103152240776	127242
4271911aec39925df52fa9a87a189187ae1ea898	activity counter: new optimization for the dynamic scheduling of simd control flow	turning;application software;boundary conditions;counting circuits;control flow;data flow computing;parallel machines;differential equations;parallel processing;dynamic scheduling;counting circuits dynamic scheduling parallel processing turning hardware application software parallel machines data flow computing differential equations boundary conditions;hardware	SIMD or vector computers and collection-oriented languages, like C*, are designed to perform the same computation on each data item or on just a subset of the data. Subsets of processors or data items implemented via an activiry bit and a stack of activity bits when subsets of subsets are supported. This method is also used in VLIW processors through if-conversion to implement paralalle1 control flow as in SIMD computers. Wt present a new method of dynamic scheduling of several SIMD control flow constructions which can be nested.	central processing unit;computation;computer;control flow;data item;simd;scheduling (computing)	Ronan Keryell;Nicolas Paris	1993	1993 International Conference on Parallel Processing - ICPP'93	10.1109/ICPP.1993.36	parallel processing;application software;parallel computing;real-time computing;dynamic priority scheduling;boundary value problem;computer science;theoretical computer science;operating system;control flow;differential equation	Arch	3.5899064979215973	39.00326787030966	127273
aaaf870dbe949c3cdef816848d7de77bf4aea188	autonomous, independent management of dynamic graphs on gpus		In this paper, we present a new, dynamic graph data structure, built to deliver high update rates while keeping a low memory footprint using autonomous memory management directly on the GPU. By transferring the memory management to the GPU, efficient updating of the graph structure and fast initialization times are enabled as no additional memory allocation calls or reallocation procedures are necessary since they are handled directly on the device. In comparison to previous work, this optimized approach allows for significantly lower initialization times (up to 300× faster) and much higher update rates for significant changes to the graph structure and equal rates for small changes. The framework provides different update implementations tailored specifically to different graph properties, enabling over 100 million of updates per second and keeping tens of millions of vertices and hundreds of millions of edges in memory without transferring data back and forth between device and host.	autonomous robot;dos;data structure;graph (abstract data type);graph (discrete mathematics);graph property;graphics processing unit;memory footprint;memory management	Martin Winter;Rhaleb Zayer;Markus Steinberger	2017	2017 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2017.8091058	parallel computing;memory footprint;memory management;graph (abstract data type);graph property;memory map;real-time computing;initialization;cuda pinned memory;interleaved memory;computer science	HPC	2.179091562740295	36.491310934928144	127279
c8ab4e74d6d04d910d6808a080e3fa3462ff6083	shad: the scalable high-performance algorithms and data-structures library		The unprecedented amount of data that needs to be processed in emerging data analytics applications poses novel challenges to industry and academia. Scalability and high performance become more than a desirable feature because, due to the scale and the nature of the problems, they draw the line between what is achievable and what is unfeasible. In this paper, we propose SHAD 1 , the Scalable High-performance Algorithms and Data-structures library [1]. SHAD adopts a modular design that confines low level details and promotes reuse. SHADu0027s core is built on an Abstract Runtime Interface which enhances portability and identifies the minimal set of features of the underlying system required by the framework. The core library includes common data-structures such as: Array, Vector, Map and Set. These are designed to accommodate significant amount of data which can be accessed in massively parallel environments, and used as building blocks for SHAD extensions, i.e. higher level software libraries. We have validated and evaluated our design with a performance and scalability study of the core components of the library. We have validated the design flexibility by proposing a Graph Library as an example of SHAD extension, which implements two different graph data-structures; we evaluate their performance with a set of graph applications. Experimental results show that the approach is promising in terms of both performance and scalability. On a distributed system with 320 cores, SHAD Arrays are able to sustain a throughput of 65 billion operations per second, while SHAD Maps sustain 1 billion of operations per second. Algorithms implemented using the Graph Library exhibit performance and scalability comparable to a custom solution, but with smaller development effort.	algorithm;array data structure;big data;data recovery;distributed computing;flops;fault tolerance;global variable;graph (abstract data type);integrated development environment;library (computing);map;modular design;non-volatile memory;pagerank;programming language;scalability;shared memory;software portability;thread safety;throughput;user space;volatile memory	Vito Giovanni Castellana;Marco Minutoli	2018	2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)	10.1109/CCGRID.2018.00071	supercomputer;massively parallel;scalability;big data;distributed database;algorithm;software portability;data structure;modular design;computer science	HPC	-3.283738309596637	43.334071452959776	127318
1dbb680b8bbb5276db646f78edb476846ffbc58d	movement detection for power-efficient smartphone wlan localization	location based service;mobile device;bepress selected works;mobility;localization;power efficiency;movement detection;mobile phone;accelerometer;wlan localization;experimental evaluation;power consumption;smartphone;smartphone movement detection power efficient wlan localization;high power;power efficient	Mobile phone services based on the location of a user have increased in popularity and importance, particularly with the proliferation of feature-rich smartphones. One major obstacle to the widespread use of location-based services is the limited battery life of these mobile devices and the high power costs of many existing approaches.  We demonstrate the effectiveness of a localization strategy that performs full localization only when it detects a user has finished moving. We characterize the power use of a smartphone, then verify our strategy using models of long-term walk behavior, recorded data, and device implementation. For the same sample period, our movement-informed strategy reduces power consumption compared to existing approaches by more than 80% with an impact on accuracy of less than 5%. This difference can help achieve the goal of near-continuous localization on mobile devices.	internationalization and localization;location-based service;mobile device;mobile phone;smartphone;software feature	Ilari Shafer;Mark L. Chang	2010		10.1145/1868521.1868536	embedded system;internationalization and localization;electrical efficiency;telecommunications;computer science;operating system;location-based service;mobile device;mobile computing;computer security;accelerometer	HCI	2.056950161738196	34.72061416817533	127345
92899802c1b440f470afb0241144705796f39393	heuristic assignment of cpds for probabilistic inference in junction trees	junction tree based architecture;belief networks;bayesian network;inference mechanisms;trees mathematics;data mining;probabilistic inference;junctions;hugin architecture;potential;computer architecture;probabilistic query;trees mathematics belief networks inference mechanisms;junction tree;reasoning under uncertainty;inference algorithms computer architecture message passing bayesian methods clustering algorithms tree graphs artificial intelligence computer science computer networks algorithm design and analysis;message passing;cpd heuristic assignment;particle separators;inference algorithms;global propagation method;probabilistic logic;global propagation method cpd heuristic assignment probabilistic inference probabilistic query bayesian network junction tree based architecture hugin architecture;propagation;reasoning under uncertainty bayesian network junction tree probabilistic inference propagation potential	Extensive research has been done for efficient computation of probabilistic queries posed to Bayesian networks (BNs). One popular architecture for exact inference on BNs is the Junction Tree (JT) based architecture. Among all variations developed, HUGIN is the most efficient JT-based architecture. The Global Propagation (GP) method used in the HUGIN architecture is arguably one of the best methods for probabilistic inference in BNs. Before the propagation, initialization is done to obtain the potential for each cluster in the JT. Then with the GP method, each cluster potential is transformed into cluster marginal through passing messages with its neighboring clusters. Improvements have been proposed to make the message propagation more efficient. Still, the GP method can be very slow for dense networks. As BNs are applied to larger, more complex and realistic applications, the design of more efficient inference algorithm has become increasingly important. Towards this goal, in this paper, we present a heuristic for initialization that avoids unnecessary message passing among clusters of a JT, therefore improving the performance of the architecture by passing fewer messages.	algorithm;amortized analysis;bayesian network;collaborative product development;computation;coupled cluster;heuristic (computer science);hugin;jt (visualization format);marginal model;mathematical optimization;message passing;propagation time;software propagation;tree decomposition	Dan Wu;Nasreen Mirza Tania;Karen H. Jin	2009	2009 21st IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2009.114	message passing;potential;computer science;theoretical computer science;machine learning;bayesian network;data mining;probabilistic logic;programming language	Robotics	0.8390258091720428	41.60210778639743	127791
51a11a2485a2152698006a4b97e67ba1a575459f	steps towards gpu accelerated aggregation amg	sparse matrices graphics processing units grid computing message passing parallel processing;multigrid accelerated aggregation amg aggregation technique parallel matrix representation finite volume discretisation icrs sparse matrix format asynchronous exchange mechanism mpi gpu coprocessor krylov accelerated amg nvidia tesla c2070 intel x5650 cpu;acceleration;gpgpu;computational modeling;mathematical model graphics processing unit vectors sparse matrices computational modeling acceleration equations;vectors;graphics processing units;finite volumes;message passing;mathematical model;algebraic multigrid;graphics processing unit;finite volumes algebraic multigrid gpgpu;grid computing;sparse matrices;parallel processing	We present an implementation of AMG with simple aggregation techniques on multiple GPUs. It supports the parallel matrix representations typically used for finite volume discretisation. We employ the ICRS sparse matrix format and the asynchronous exchange mechanism of MPI on CPUs that has been modified to make it suitable for the GPU coprocessors. We show that the solution phase of the standard v-cycle AMG with simple aggregation is accelerated by a factor of up to 12. The solution phase of the more advanced Krylov-accelerated AMG runs faster by a factor of up to 7 on Nvidia TESLA C2070 compared to calculation on Intel X5650 CPUs.	algorithm;central processing unit;coprocessor;discretization;finite volume method;graphics processing unit;krylov subspace;nvidia tesla;sparse matrix	Maximilian Emans;Manfred Liebmann;Branislav Basara	2012	2012 11th International Symposium on Parallel and Distributed Computing	10.1109/ISPDC.2012.19	acceleration;computational science;parallel processing;parallel computing;message passing;sparse matrix;computer science;theoretical computer science;operating system;mathematical model;computational model;general-purpose computing on graphics processing units;multigrid method;grid computing	HPC	-4.523954811098168	38.163321268972176	127803
75b1d64b063b9ad2e27ea87e4df398de41b35af0	scheduling conditional data-flow graphs with resource sharing	novel scheduling algorithm;dynamic resource sharing;scheduling algorithm;pipeline scheduling algorithm;conditional data-flow graph;modified rotation scheduling technique;loop construct;conditional branch;data structure;resource sharing;dynamic scheduling;tellurium;data flow graph;data structures;high level synthesis;data engineering;resource allocation;parallel algorithms;resource management	This paper proposes pipeline scheduling algorithms for conditional branches and loop constructs, which are represented in the form of a conditional data-flow graph, where each node is associated with a condition vector. A novel data structure for dynamic resource sharing and a novel scheduling algorithm for resource sharing are proposed. Based on such a data structure and a modified rotation scheduling technique, a scheduling algorithm that performs resource sharing and loop pipelining simultaneously is designed.	dataflow;schedule (project management)	Jayesh Siddhiwala;Liang-Fang Chao	1995			fair-share scheduling;shared resource;fixed-priority pre-emptive scheduling;parallel computing;earliest deadline first scheduling;flow shop scheduling;data structure;dynamic priority scheduling;resource allocation;computer science;rate-monotonic scheduling;theoretical computer science;genetic algorithm scheduling;data-flow analysis;two-level scheduling;stride scheduling;tellurium;distributed computing;parallel algorithm;lottery scheduling;high-level synthesis;programming language;round-robin scheduling;scheduling;proportionally fair	Embedded	3.9495617949465607	38.51656491477891	128009
748c66cc0c9faebfa1aaa8f4f60aa2c0d44b8c17	wireless sensor microsystem design: a practical perspective		The development of small sensory systems for use in distributed networks as well as single autonomous devices has been actively pursued for more than 50 years. The stimulant to this activity, as with most modern technologies, was the invention of the transistor that made miniaturisation possible. The earliest wireless sensor devices were developed for use in a range of applications, including wireless animal tracking and medical instrumentation (Mackay RS, Bio-medical telemetry – sensing and transmitting biological information from animals and man. Wiley, New York, 1968). Although there was considerable activity in the field during the 1950s and 1960s (Mackay RS, Science 134:1196–1202, 1961), continuing research appeared to recede in subsequent years. However, in the 1970s and 1980s, the microelectronics industry rapidly developed after the invention of the first integrated circuit (IC) microprocessor (Lewin MH, IEEE Trans Circuit Syst 22(7):577–585, 1975) for use in personal computers and work stations. The rapid growth in consumer electronic products, exemplified by mobile communications and the Internet in the 1990s, made researchers and practitioners realise the potential for personalised wireless systems incorporating location sensitive information and sensor technologies. In essence, these devices owe much to the early work of pioneers, but modern designs will depend heavily on new emerging technologies such as System-on-Chip (SoC) and the implementation of mobile (wireless) communication protocols.		Lei Wang;David R. S. Cumming;Paul A. Hammond;Jonathan M. Cooper;Erik A. Johannessen;Kamen Ivanov	2014		10.1007/978-1-4471-6374-9_11	electronic engineering	HCI	4.258549558061859	32.62474249005522	128161
ce004b0f1937c202093e56c0d635db3cfcea1a59	efficient finite field processor for gf(2163) and its implementation	cryptographic processors;ecc;finite field processor;finite field multiplier;finite field;elliptic curve cryptography;scalar multiplication	A high performance finite field processor for elliptic curve cryptography is presented. One of the contributions in this work is the modified Bit-Parallel Word-Serial (BPWS) finite field multiplication algorithm and its corresponding pipeline-fashion multiplier architecture. The proposed multiplier achieves a throughput of one multiplication every N + 1 clock cycles, in contrast with at least N + 3 clock cycles required in the recent other designs, where N is the ratio of field size to word size. Another contribution of this work is to explore parallelism at the instruction level in the proposed processor. Separated hardware modules for finite field multiplication, squaring and addition makes it possible that up to three finite field arithmetic operations be executed in parallel. At a higher level, data dependencies are detected at compile-time by analysing the data interdependency when performing elliptic curve point operations. Implemented using a CMOS 0.18 μm chip, which runs at 125MHz and performs one scalar multiplication in 62 μs.		Bijan Ansari;Huapeng Wu	2007	IJHPSA	10.1504/IJHPSA.2007.015396	parallel computing;theoretical computer science;scalar multiplication;elliptic curve cryptography;finite field	EDA	9.547023050342816	44.49344730442602	128253
6894fb3140b31734fb3ce28ff565edd6c1f56fae	logarithmic number system for deep learning		In this paper the logarithmic Number System (LNS) is adopted to implement Long-Short Term Memory (LSTM), the basic component of a deep learning network type. Initially, piece wise approximations to activation functions σ and tanh are proposed and evaluated in LNS. Secondly, LNS multipliers and adders are implemented for wordlengths of 9,10 and 11 bits. The circuits are implemented in an 90-nm 1.0 V CMOS standard-cell library and quantitative results are reported. Results demonstrate that LNS is a good candidate for data representation and processing in deep learning networks, as area reduction of up to 36% is possible.	activation function;approximation;cmos;data (computing);deep learning;logarithmic number system;long short-term memory	Ioannis Kouretas;Vassilis Paliouras	2018	2018 7th International Conference on Modern Circuits and Systems Technologies (MOCAST)	10.1109/MOCAST.2018.8376572	piecewise;logarithmic number system;deep learning;electronic circuit;arithmetic;external data representation;logic gate;artificial intelligence;adder;cmos;computer science	EDA	4.80722313049061	42.42872515765999	128346
47f8fb00847c6c03ea6cc054c06247df89d9ad72	multilane hashing mode suitable for parallel processing	parallel processing;hash function		hash function;parallel processing (dsp implementation)	Hidenori Kuwakado;Shoichi Hirose	2013	IEICE Transactions		parallel processing;hash table;double hashing;parallel computing;hash function;linear hashing;perfect hash function;dynamic perfect hashing;computer science;theoretical computer science;universal hashing;distributed computing;2-choice hashing	DB	2.2301267404774436	39.69996075118931	128355
28b5494e9760e9cc7ec7db41f46a2317cee22ff5	structural design optimization for deep convolutional neural networks using stochastic computing		Deep Convolutional Neural Networks (DCNNs) have been demonstrated as effective models for understanding image content. The computation behind DCNNs highly relies on the capability of hardware resources due to the deep structure. DCNNs have been implemented on different large-scale computing platforms. However, there is a trend that DCNNs have been embedded into light-weight local systems, which requires low power/energy consumptions and small hardware footprints. Stochastic Computing (SC) radically simplifies the hardware implementation of arithmetic units and has the potential to satisfy the small low-power needs of DCNNs. Local connectivities and down-sampling operations have made DCNNs more complex to be implemented using SC. In this paper, eight feature extraction designs for DCNNs using SC in two groups are explored and optimized in detail from the perspective of calculation precision, where we permute two SC implementations for inner-product calculation, two down-sampling schemes, and two structures of DCNN neurons. We evaluate the network in aspects of network accuracy and hardware performance for each DCNN using one feature extraction design out of eight. Through exploration and optimization, the accuracies of SC-based DCNNs are guaranteed compared with software implementations on CPU/GPU/binary-based ASIC synthesis, while area, power, and energy are significantly reduced by up to 776x, 190x, and 32835x.	application-specific integrated circuit;artificial neural network;central processing unit;computation;convolutional neural network;embedded system;feature extraction;graphics processing unit;low-power broadcasting;mathematical optimization;neural network software;sampling (signal processing);stochastic computing	Zhe Li;Ao Ren;Ji Li;Qinru Qiu;Bo Yuan;Jeffrey T. Draper;Yanzhi Wang	2017	Design, Automation & Test in Europe Conference & Exhibition (DATE), 2017		stochastic process;embedded system;mathematical optimization;electronic engineering;feature extraction;computer science;artificial intelligence;theoretical computer science;machine learning;algorithm;statistics	EDA	4.246453863962481	42.38611706326907	128719
22e4db309b5e59da7848629fb682a38d76e18aa7	the improbable but highly appropriate marriage of 3d stacking and neuromorphic accelerators	neurons three dimensional displays biological neural networks stacking hardware neuromorphics accuracy;spiking neural network 3d stacking neuromorphic accelerator;neuromorphics;neuromorphic accelerators 3d stacking power density energy constraints machine learning algorithms neural networks nn accelerators 3d structures across layer bandwidth requirements 3d stacked nn accelerator;neuromorphic accelerator;accuracy;spiking neural network;stacking;three dimensional displays;3d stacking;neurons;biological neural networks;three dimensional integrated circuits neural nets particle accelerators;hardware	3D stacking is a promising technology (low latency/power/area, high bandwidth); its main shortcoming is increased power density. Simultaneously, motivated by energy constraints, architectures are evolving towards greater customization, with tasks delegated to accelerators. Due to the widespread use of machine-learning algorithms and the re-emergence of neural networks (NNs) as the preferred such algorithms, NN accelerators are receiving increased attention. They turn out to be well matched to 3D stacking: inherently 3D structures with a low power density and high across-layer bandwidth requirements. We present what is, to the best of our knowledge, the first 3D stacked NN accelerator	algorithm;artificial neural network;emergence;machine learning;neuromorphic engineering;requirement;stacking;three-dimensional integrated circuit	Bilel Belhadj;Alexandre Valentian;Pascal Vivet;Marc Duranton;Liqiang He;Olivier Temam	2014	2014 International Conference on Compilers, Architecture and Synthesis for Embedded Systems (CASES)	10.1145/2656106.2656130	computer science;theoretical computer science;stacking;machine learning;accuracy and precision;spiking neural network	EDA	3.8180011452218063	42.17570865149433	128868
b844c0a71ad876016c9a7510f6b86697352c2bbe	parallel implementation of the ccsds 1.2.3 standard for hyperspectral lossless compression		Hyperspectral imaging is a technology which, by sensing hundreds of wavelengths per pixel, enables fine studies of the captured objects. This produces great amounts of data that require equally big storage, and compression with algorithms such as the Consultative Committee for Space Data Systems (CCSDS) 1.2.3 standard is a must. However, the speed of this lossless compression algorithm is not enough in some real-time scenarios if we use a single-core processor. This is where architectures such as Field Programmable Gate Arrays (FPGAs) and Graphics Processing Units (GPUs) can shine best. In this paper, we present both FPGA and OpenCL implementations of the CCSDS 1.2.3 algorithm. The proposed paralellization method has been implemented on the Virtex-7 XC7VX690T, Virtex-5 XQR5VFX130 and Virtex-4 XC2VFX60 FPGAs, and on the GT440 and GT610 GPUs, and tested using hyperspectral data from NASA’s Airborne Visible Infra-Red Imaging Spectrometer (AVIRIS). Both approaches fulfill our real-time requirements. This paper attempts to shed some light on the comparison between both approaches, including other works from existing literature, explaining the trade-offs of each one.	central processing unit;data system;fits;field-programmable gate array;graphics processing unit;lossless compression;need to know;opencl api;parallel algorithm;pixel;radiation hardening;real-time clock;requirement;single-core;vhdl	Daniel Báscones;Carlos González;Daniel Mozos	2017	Remote Sensing	10.3390/rs9100973	implementation;artificial intelligence;field-programmable gate array;parallel computing;computer vision;pixel;data system;reconfigurable computing;imaging spectrometer;hyperspectral imaging;computer science;lossless compression	Graphics	2.373757912419753	46.07275298178131	129007
7bce19b4d44e376ce2915d21592843454783b1b2	preconditioning index set transformations for time-optimal affine scheduling	optimal solution;generic algorithm;hardware synthesis;systolic array;space time;timing optimization;affine transformation;indexation;integer linear program	Many regular algorithms, suitable for VLSI implementation, are naturally described by sets of integer index vectors together with a rule that assigns a computation to each vector. Regular VLSI structures for such algorithms can be found by mapping the index vectors to a discrete space-time with integer coordinates. If the scope is restricted to linear or affine mappings, then the minimization of the execution time for the VLSI implementation with respect to the space-time mapping is essentially an integer linear programming (ILP) problem. If the entries in the vector describing the time function must be integers, ILP techniques can be applied directly. There are, however, index sets that allow space-time mappings with rational, nonintegral entries. In such cases, ILP will not consider all possible affine time functions and an optimal solution may go unnoticed. In this paper we give sufficient conditions on the index set for when only integer time functions are allowed. We also give a general algorithm to find a “preconditioning” affine transformation of the index set, such that the transformed index set allows only integer time functions. ILP methods can then be used to find time-optimal architectures for the transformed algorithm. This considerably extends the class of algorithms for which time-optimal VLSI structures can be found.	algorithm;computation;integer programming;linear programming;preconditioner;run time (program lifecycle phase);scheduling (computing);very-large-scale integration	Björn Lisper	1996	Algorithmica	10.1007/BF01941688	mathematical optimization;combinatorics;discrete mathematics;genetic algorithm;systolic array;computer science;space time;affine transformation;mathematics;geometry;affine combination	Theory	4.453469601826342	37.66485039035387	129060
01fb7d3f9af56049da108a8fa90dc5ce5f0d69c9	extending lyapack for the solution of band lyapunov equations on hybrid cpu–gpu platforms	linear algebra;gpus;mathematics;paper;banded matrix lyapunov equations;info eu repo semantics article;cuda;lyapack;nvidia;tesla k20;tesla s2070	The solution of large-scale Lyapunov equations is an important tool for the solution of several engineering problems arising in optimal control and model order reduction. In this work, we investigate the case when the coefficient matrix of the equations presents a band structure. Exploiting the structure of this matrix, we can achive relevant reductions in the memory requirements and the number of floating-point operations. Additionally, the new solver efficiently leverages the parallelism of CPU–GPU platforms. Furthermore, it is integrated in the lyapack library to facilitate its use. The new codes are evaluated on the solution of several benchmarks, exposing significant runtime reductions with respect to the original CPU version in lyapack.	benchmark (computing);central processing unit;code;coefficient;electronic band structure;graphics processing unit;lyapunov fractal;model order reduction;optimal control;parallel computing;requirement;solver	Peter Benner;Alfredo Remón;Ernesto Dufrechu;Pablo Ezzatti;Enrique S. Quintana-Ortí	2014	The Journal of Supercomputing	10.1007/s11227-014-1322-7	parallel computing;simulation;computer science;theoretical computer science;linear algebra;operating system;distributed computing;programming language	HPC	-4.100313086785264	39.436704443201876	129133
c807253ab3389ee651421a892efbe0362c13d7f3	research on the algorithm of decrypting encrypted synchronous sequential machine	diffusion mechanism;sequential machines;pins;weighted state transition diagram encrypted synchronous sequential machine off line reverse analysis data collecting decrypted state encrypted state search strategy diffusion mechanism;data collection;query formulation;synchrorous sequential machine;search strategy;encrypted state;arrays;search;registers;synchronization;data structures;cryptography;data collecting;search synchrorous sequential machine state transition diagram diffusion;weighted state transition diagram;off line reverse analysis;data handling;decrypted state;sequential machines data handling data structures query formulation reverse engineering;state transition diagram;diffusion;algorithm design and analysis;encrypted synchronous sequential machine;reverse engineering;cryptography samarium pins registers programmable logic devices clocks computer science information science electronic mail data analysis	As the core link of off-line reverse analysis, data collecting can be regarded as decryption of encrypted synchronous sequential machine, the key part of which is driving itself from decrypted state to encrypted state by building state transition diagram and using corresponding search strategy. Based on diffusion mechanism, weighted state transition diagram is built, on which single-branch search which can get the shortest input sequence is implemented. For the parallelism between diffusion operation and operations executed on collecting platform, the time required for data collecting is reduced.	algorithm;cryptography;encryption;online and offline;parallel computing;state diagram;state transition table	Guangyu Zeng;Min Fan;Qingbao Li;Li Zhou	2008	2008 International Symposium on Computer Science and Computational Technology	10.1109/ISCSCT.2008.281	computer science;theoretical computer science;data mining;database	Arch	9.503266687705755	36.18171599564596	129350
ebfcbd7f4285a3758692b24857d821c0f8dff24d	fiber optic placing, splicing, testing, and cable design experience at bell canada	splicing;cable laying;fibra optica;pose câble;optical fiber testing;cable optico;optical fiber connecting;field trial;plastic packaging;fiber optic;optical cable;telecomunicacion optica;telecommunication optique;optical fibers;optical fiber cables;colocacion cable;manufacturing;câble optique;optical telecommunication;steel;optical fibers splicing optical fiber testing optical fiber cables optical design plastic packaging manufacturing telecommunications steel copper;optical fiber;copper;optical fiber testing optical fiber cables optical fiber connecting;optical design;telecommunications;fibre optique	Bell Canada has passed the stage of field trials and is now into the day-to-day operations, maintenance, and installation of FD-135 (135 Mbit/s) fiber optic transmission systems. The outside plant cables are manufactured by Northern Telecom and consist of a central stranded steel strength member with an extruded plastic slotted profile. Fibers, as well as two copper pairs which are used for servicing purposes and pressure monitoring, are loosely placed in the slots. Manually cleaved fibers are spliced with a V-groove alignment fusion set and are protected by a plastic splice package. Splice packages are inserted in an orderly fashion into an organizer tray which is then placed into a plastic closure allowing for pressurization and easy access of the cable. This paper provides an overview of the outside plant hardware and describes the various installation, splicing, and cable testing techniques that have been successfully used by Bell Canada.	optical fiber	Jean-François Couturier;J. A. Gérard Mainville;Michel D. Papa;Michel Plouffe;Tony Scandella	1986	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.1986.1146377	telecommunications;computer science;optical fiber	Mobile	6.434008260148507	34.96332959388291	129457
43a3d8de539816aa9c0f11c3c2d63aed0e85c848	design space exploration of a reconfigurable hmac-hash unit	online resources;information resources;scholarly research;information sources;academic research;online databases;education resources;publishing;research databases;australasian research information;south east asian information;information databases;full content;education databases;australian databases;commissioning;electronic publisher;online;e titles;library resources	In this paper, a design space exploration of a reconfigurable HMAC-hash unit is discussed. This unit implements one of six standard hash algorithms, namely, MD5, SHA-1, RIPEMD-160, HMAC-MD5, HMAC-SHA-1, and HMAC-RIPEMD-160. The design space exploration of this unit is done using the Handel-C language. We propose key reuse mechanism for successive messages in order to improve the HMAC throughput. In addition, we explore the design space by providing two implementations of the HMAC algorithm, one for a general key size and another for a fixed key size. In each implementation, we use standard key use and the proposed key reuse mechanisms, and that results in four different implementations. The performance of these four implementations is analyzed with respect to three design metrics: area, delay, and throughput. We found that the proposed key reuse mechanism improves the HMAC throughput significantly when a large key is reused, with negligible increase in area and delay. In addition, we found that the implementation of HMAC for fixed key size is better in area, delay, and throughput than the HMAC implementation for general key size.	algorithm;design space exploration;handel-c;hash function;key size;md5;sha-1;throughput	Esam Khan;M. Watheq El-Kharashi;Fayez Gebali;Mostafa Abd-El-Barr	2006	2006 ITI 4th International Conference on Information & Communications Technology		project commissioning;computer science;engineering;artificial intelligence;theoretical computer science;operating system;software engineering;data mining;database;publishing;advertising;management;law;world wide web;research	EDA	8.357740675468634	45.80711731365107	129881
4e892deef85c4cb62716776657d66dc417574bcc	an improved spectral graph partitioning algorithm for mapping parallel computations	computers;distributed memory;performance 990200;graph theory;algoritmo paralelo;data transmission;hypercube;metodo espectral;teoria grafo;general and miscellaneous mathematics computing and information science;parallel algorithm;methode element fini;metodo elemento finito;metodo diferencia finita;efficient algorithm;efficiency;metodo descomposicion;performance;methode decomposition;informing science;finite element method;maillage;mathematical logic;theorie graphe;65y05;eigenvector;05c50;finite element;parallel computation;finite difference method;algorithme parallele;vector propio;methode difference finie;computer architecture;decomposition method;graph partitioning;celdarada;graph spectrum;array processors;68r10;spectral method;load balancing;scientific computing;parallel computer;programming 990200 mathematics computers;algorithms;grid pattern;methode spectrale;spectral graph theory;mapping;hypercube computers;vecteur propre;parallel processing;lower bound;mathematics and computers;eigenvectors;mathematics computers information science management law miscellaneous;hipercubo	Efficient use of a distributed memory parallel computer requires that the computational load be balanced across processors in a way that minimizes interprocessor communication. A new domain mapping algorithm is presented that extends recent work in which ideas from spectral graph theory have been applied to this problem. The generalization of spectral graph bisection involves a novel use of multiple eigenvectors to allow for division of a computation into four or eight parts at each stage of a recursive decomposition. The resulting method is suitable for scientific computations like irregular finite elements or differences performed on hypercube or mesh architecture machines. Experimental results confirm that the new method provides better decompositions arrived at more economically and robustly than with previous spectral methods. This algorithm allows for arbitrary nonnegative weights on both vertices and edges to model inhomogeneous computation and communication. A new spectral lower bound for graph bisection is also presented. Key words, graph partitioning, parallel computation, load balancing, graph spectrum, eigenvector AMS subject classifications. 05C50, 68R10, 65Y05	algorithm;central processing unit;computation;connection machine;distributed memory;finite element method;graph partition;inter-process communication;load balancing (computing);parallel computing;recursion;spectral graph theory;spectral method;vhdl-ams	Bruce Hendrickson;Robert W. Leland	1995	SIAM J. Scientific Computing	10.1137/0916028	graph energy;parallel processing;folded cube graph;mathematical optimization;combinatorics;mathematical analysis;graph bandwidth;eigenvalues and eigenvectors;computer science;graph theory;theoretical computer science;finite element method;hypercube graph;mathematics;voltage graph;quartic graph;algorithm;strength of a graph;statistics;algebra	HPC	-2.33448812999215	37.753467606791006	130364
36c35ba27dd163003f415fcff3324dccebdbc7b9	parallel computation of modular multivariate polynominal resultants on a shared memory machine	shared memory;multivariate polynomial;parallel computer	This paper reports our experience in parallelizing a modular algorithm for computing multivariate polynomial resultants over p. The modular algorithm has the well-known scheme of divide-conquercombine where the conquer phase is straightforwardly parallelizable. But the combine phase is structurally sequential, and requires certain modifications for efficient parallelization. We describe and compare various different parallelization schemes (in particular for the combine phase). The variants of the algorithm have been implemented on top of the Paclib kernel which provides C-primitives for task creation and non-deterministic wait on a shared memory machine.	computation;parallel computing;resultant;shared memory	Hoon Hong;Hans-Wolfgang Loidl	1994		10.1007/3-540-58430-7_29	parallel computing;computer science;theoretical computer science;distributed computing	Theory	-2.8733352336398985	37.38628295751629	130385
d9c4e1ccd5a136dfd9570cd785165c50bad6f9c1	trends in knowledge base processing using optical techniques	optical information processing knowledge based systems optical correlation;holography holographic optical components knowledge based systems relational databases optical interconnections optical modulation expert systems parallel processing matrices optical design;inner product;optical correlation;spatial light modulator;neural network knowledge representation knowledge base processing optical techniques matrix like formalisms optical inner product processors matrix vector multipliers optical correlators spatial light modulators symbolic substitution;optical information processing;optical correlator;system architecture;knowledge based systems;neural network;knowledge base	A review is presented of recent research efforts, along with a discussion of the relative merits and limitations of using optics as an implementation technology. To a large extent, past efforts have focused on representing knowledge using matrix-like formalisms and designing system architectures based on optical inner-product processors (such as matrix-vector multipliers) and optical correlators. Actual implementations are impeded primarily by the limitations of current spatial light modulators. New directions include the use of symbolic substitution and neural network ideas. >	knowledge base	James A. Kottas;Cardinal Warde	1989		10.1109/ICSMC.1989.71503	computer vision;knowledge base;dot product;computer science;artificial intelligence;theoretical computer science;artificial neural network	DB	7.786494839581692	39.146244907212846	130575
c5a8794c1f101b6c17f618d5fc842803484d5c4f	examining the problems of computer-based anxiety: a systemic approach	simd;system approach;parallel sorting;multiprocessor;linearly connected parallel computer;information system;mesh connected parallel computer	This paper describes the results of a large experiment that developed specific metrics for evaluating anxiety caused by computer-based information systems. The results suggest that this problem, especially prevalent among managers, may be one of the most significant ones in today's work-place. A three-step solution sequence is suggested, designed to recognize the problem and take full advantage of existing means for dealing with it efficiently.	information system	Stephen R. Ruth;Ella P. Gardner;Barry Render	1987		10.1145/322917.322970	parallel computing;multiprocessing;simd;computer science;theoretical computer science;distributed computing;information system	AI	-0.25737894453422	35.46324618578894	131063
75c41ee189f32b6444901f679a8def35439880c6	fm-index on gpu: a cooperative scheme to reduce memory footprint	memory management;instruction sets graphics processing units indexes bioinformatics memory management registers algorithm design and analysis;gpgpu;indexes;memory level parallelism gpgpu bioinformatics fm index fine grain parallelism;registers;graphics processing units;memory level parallelism;ubiquitous computing bioinformatics data structures graphics processing units;gpu architectures fm index cooperative scheme reduce memory footprint data structure bioinformatics pseudo random memory access pattern naive gpu implementations;fm index;fine grain parallelism;algorithm design and analysis;instruction sets;bioinformatics	The FM-index is a data structure which is seeing more and more pervasive use, in particular in the field of high-throughput bioinformatics. Algorithms based on it show a pseudo-random memory access pattern. As a consequence, they are usually bound by memory bandwidth rather than CPU usage. Naive GPU implementations are no exception. Here we show that the combination of a compact design of the FM-index and a thread-cooperative approach can be used to restore a proper balance. The resulting solution is less memory-bandwidth intensive, and allows full exploitation of the computational resources of the GPU across several GPU architectures.	approximate string matching;bandwidth (signal processing);bioinformatics;central processing unit;computation;computational resource;data access;data compaction;data structure;exception handling;fm-index;graphics processing unit;high-throughput computing;locality of reference;mapper;memory access pattern;memory bandwidth;memory footprint;offset binary;pervasive informatics;pipeline (computing);pseudorandomness;random access;string searching algorithm;throughput	Alejandro Chacón;Santiago Marco-Sola;Antonio Espinosa;Paolo Ribeca;Juan C. Moure	2014	2014 IEEE International Symposium on Parallel and Distributed Processing with Applications	10.1109/ISPA.2014.10	cuda pinned memory;uniform memory access;database index;algorithm design;interleaved memory;computer architecture;parallel computing;out-of-core algorithm;computer science;theoretical computer science;operating system;instruction set;database;conventional memory;processor register;flat memory model;programming language;fm-index;general-purpose computing on graphics processing units;memory map;memory management	Arch	0.33898702938533737	42.75103214578524	131519
d096705919dd4b62c9d50c95f1d26a52d67510c4	design cooperative awareness nodes using sopc in smart multimedia sensor networks	multimedia sensor networks;sopc;cooperative awareness;on board processing;feature extraction	The design mind of SOPC (System on Programmable Chip), hardware & software co-design, will bring the system optimization with larger degree of freedom. It will establish the foundation on realizing the miniaturization and more efficient local preprocessing in smart cooperative sensor networks. In this contribution, a solution is proposed for smart multimedia sensor nodes based on the SOPC framework, aiming at improving the on-board processing capability and the degree of freedom in development. This paper mainly presents the structure and design method of the smart cooperative multimedia sensor node based on SOPC. The on-board feature extraction is realized and some correlative experimental data and results are presented as well. The experimental results evaluate the feasibility and efficiency of the proposed node design method. Keyword SOPC .Multimedia sensor networks . Cooperative awareness . On-board processing . Feature extraction	algorithm;embedded system;feature extraction;mathematical optimization;on-board data handling;preprocessor;program optimization;sensor node	Yi Zhou;Huiping Li;Chunlin Wan;Yandong Hou	2013	MONET	10.1007/s11036-013-0433-3	embedded system;real-time computing;simulation;feature extraction;computer science	EDA	0.18797467089589795	33.00104793392542	131701
bc7e8ded32d2c4f647276b46e8c6209e0259da2c	design of a one million neuron single fpga neuromorphic system for real-time multimodal scene analysis	fpga neuromorphic system real time multimodal scene analysis architecture large scale neuromorphic systems digital approach arithmetic logic units communication processors scaling single chip million neuron system;logic design;real time;digital approach;microprocessor chips digital arithmetic field programmable gate arrays logic design;arithmetic logic unit;single chip million neuron system;chip;large scale;scaling;commercial off the shelf;correspondence analysis;digital arithmetic;communication processors;large scale neuromorphic systems;neurons;arithmetic logic units;field programmable gate arrays;architecture;fpga neuromorphic system;microprocessor chips;scene analysis;real time multimodal scene analysis	In this paper, we present an architecture and corresponding analysis for large-scale neuromorphic systems using a digital approach where neurons are abstracted as arithmetic logic units and communication processors. After presenting the architecture, we establish a few basic architectural principles, particularly the scaling of the system to large arrays. We demonstrate the reality of a single chip million neuron system built from these principles in commercial off the shelf FPGA.	arithmetic logic unit;array data structure;central processing unit;field-programmable gate array;image scaling;multimodal interaction;neuromorphic engineering;neuron;real-time clock;real-time computing	Andrew S. Cassidy;Andreas G. Andreou;Julius Georgiou	2011	2011 45th Annual Conference on Information Sciences and Systems	10.1109/CISS.2011.5766099	chip;computer architecture;computer hardware;scaling;computer science;architecture;arithmetic logic unit;correspondence analysis	EDA	4.77518573079866	45.56494692440707	131862
f24baf796b66e73cda28c1859e20bd463dc98288	parallel matrix-free implementation of frequency-domain finite difference methods for cluster computing.		Full-wave 3D electromagnetic simulations of complex planar devices, multilayer interconnects, and chip packages are presented for wide-band frequency-domain analysis using the finite difference integration technique developed in the PETSc software package. Initial reordering of the index assignment to the unknowns makes the resulting system matrix diagonally dominant. The rearrangement also facilitates the decomposition of large domain into slices for passing the mesh information to different machines. Matrix-free methods are then exploited to minimize the number of element-wise multiplications and memory requirements in the construction of the system of linear equations. Besides, the recipes provide extreme ease of modifications in the kernel of the code. The applicability of different Krylov subspace solvers is investigated. The accuracy is checked through comparisons with CST MICROWAVE STUDIO transient solver results. The parallel execution of the compiled code on specific number of processors in multi-core distributed-memory architectures demonstrate high scalability of the computational algorithm. Keywords— finite difference frequency domain (FDFD), finite integration technique, matrix-free method, parallel programming, computational electromagnetics.	algorithm;biconjugate gradient stabilized method;central processing unit;compiler;computation;computational electromagnetics;computer cluster;convex conjugate;diagonally dominant matrix;distributed memory;domain analysis;electrical connection;finite difference method;iterative method;kerbal space program;krylov subspace;linear equation;matrix-free methods;microwave;multi-core processor;multiprocessing;petsc;parallel computing;preprocessor;requirement;scalability;shared memory;simulation;solver;supercomputer;system of linear equations;the matrix	Amir Geranmayeh	2017	CoRR		artificial intelligence;system of linear equations;parallel computing;mathematical optimization;machine learning;kernel (linear algebra);theoretical computer science;finite difference method;matrix (mathematics);krylov subspace;computer cluster;solver;computer science;diagonally dominant matrix	HPC	-3.5031360550420603	38.100335521940764	132049
4752930dc94e7f545c4aafb8a5eca0025adc80f0	full-size high-security ecc implementation on msp430 microcontrollers	part of book or chapter of book	In the era of the Internet of Things, smart electronic devices facilitate processes in our everyday lives. Texas Instrument's MSP430 microcontrollers target low-power applications, among which are wireless sensor, metering and medical applications. Those domains have in common that sensitive data is processed, which calls for strong security primitives to be implemented on those devices. Curve25519, which builds on a 255-bit prime eld, has been proposed as an e cient, highly-secure elliptic-curve. While its high performance on powerful processors has been shown, the question remains, whether it is suitable for use in embedded devices. In this paper we present an implementation of Curve25519 for MSP430 microcontrollers. To combat timing attacks, we completely avoid conditional jumps and loads, thus making our software constant time. We give a comprehensive evaluation of di erent implementations of the modular multiplication and show which ones are favorable for di erent conditions. We further present implementation results of Curve25519, where our best implementation requires 9.1 million or 6.5 million cycles on MSP430Xs having a 16× 16-bit or a 32× 32-bit hardware multiplier respectively.	16-bit;32-bit;binary multiplier;central processing unit;elliptic curve cryptography;embedded system;internet of things;karatsuba algorithm;launchpad;low-power broadcasting;max;medical imaging;microcontroller;ti msp430;time complexity	Gesine Hinterwälder;Amir Moradi;Michael Hutter;Peter Schwabe;Christof Paar	2014		10.1007/978-3-319-16295-9_2	telecommunications;computer science;electrical engineering	OS	7.679437708142262	44.904720770107645	132138
2d978fbe97c3e701dc320c83bccf15f3bb5d1ce3	some aspects of parallel performance of a seismic ray analysis algorithm	distributed system;algoritmo paralelo;medida velocidad;duracion trayecto;systeme reparti;parallel algorithm;travel time;algorithm analysis;time measurement;chronometrie;mesure vitesse;onde sismique;onda sismica;analyse sismique;algorithme parallele;analisis sismico;speed measurement;sistema repartido;surface wave;transit time;cronometria;seismic wave;parallel computer;temps parcours;analyse algorithme;seismic analysis;tiempo recorrido;analisis algoritmo;duree trajet	We consider the reconstruction of the velocity structure from measurements of travel times of seismic surface waves. The main problem is reduced to finding solution of a system of linear inequalities in infinite-dimensional space. The solution process is based on a combination of parallelized versions of the Backus-Gilbert and the constraint aggregation methods. The experimental results obtained on the IBM SP parallel computer are presented and analyzed.	algorithm	Marcin Paprzycki;Boris Digas;John Kopsky	2003		10.1007/978-3-540-24669-5_65	seismic wave;parallel computing;telecommunications;surface wave;seismic analysis;computer science;parallel algorithm;algorithm;time	HPC	-3.2012183564789463	35.62535914422435	132233
55582f89f83b7aee7f8889d41cbe7ca057e2b7e5	portable parallel programming on cloud and hpc: scientific applications of twister4azure	performance measure;scientific application;data intensive application;pattern clustering;distributed database;fault tolerant;multi dimensional scaling;computer model;distributed computing;software fault tolerance cloud computing data analysis iterative methods parallel programming pattern clustering scientific information systems;software fault tolerance;parallel programming;runtime;iterative mapreduce;data model;programming model;iterative methods;computer architecture;data analysis;large scale;computational modeling;hpc;distributed databases;runtime cloud computing programming data models computational modeling computer architecture distributed databases;scientific applications;data intensive computing;utility computing;parallel programs;programming;blast sequence searching portable parallel programming hpc twister4azure application data intensive computing science discovery data intensive iterative computation utility computing model cloud computing cloud infrastructure services large scale distributed computation distributed decentralized iterative mapreduce runtime windows azure cloud mapreduce programming model large scale iterative data analysis fault tolerance multidimensional scaling k means clustering;scientific information systems;cloud computing;data models;scientific applications iterative mapreduce cloud computing hpc	Recent advancements in data intensive computing for science discovery are fueling a dramatic growth in use of data-intensive iterative computations. The utility computing model introduced by cloud computing combined with the rich set of cloud infrastructure services offers a very attractive environment for scientists to perform such data intensive computations. The challenges to large scale distributed computations on clouds demand new computation frameworks that are specifically tailored for cloud characteristics in order to easily and effectively harness the power of clouds. Twister4Azure is a distributed decentralized iterative MapReduce runtime for Windows Azure Cloud. It extends the familiar, easy-to-use MapReduce programming model with iterative extensions, enabling a wide array of large-scale iterative data analysis for scientific applications on Azure cloud. This paper presents the applicability of Twister4Azure with highlighted features of fault-tolerance, efficiency and simplicity. We study three data-intensive applications - two iterative scientific applications, Multi-Dimensional Scaling and KMeans Clustering, one data - intensive pleasingly parallel scientific application, BLAST+ sequence searching. Performance measurements show comparable or a factor of 2 to 4 better results than the traditional MapReduce runtimes deployed on up to 256 instances and for jobs with tens of thousands of tasks.	apache hadoop;blast;cloud computing;computation;data-intensive computing;distributed computing;e-science;embarrassingly parallel;fault tolerance;hpc challenge benchmark;iteration;iterative method;java;k-means clustering;loop invariant;mapreduce;microsoft azure;multidimensional scaling;parallel computing;programming model;scalability;scheduling (computing);twister;utility computing	Thilina Gunarathne;Bingjing Zhang;Tak-Lon Wu;Judy Qiu	2011	2011 Fourth IEEE International Conference on Utility and Cloud Computing	10.1109/UCC.2011.23	data modeling;programming;fault tolerance;parallel computing;multidimensional scaling;cloud computing;data model;computer science;theoretical computer science;operating system;data-intensive computing;distributed computing;iterative method;utility computing;programming paradigm;data analysis;computational model;distributed database;software fault tolerance	HPC	-3.7399708079080396	44.37234670840619	132499
7324d633efef0e5016766e167f3b653814270f74	efficient parallel algorithms for finding the majority element	parallel algorithm		parallel algorithm	Chin-Laung Lei;Horng-Twu Liaw	1993	J. Inf. Sci. Eng.		distributed computing;theoretical computer science;computer science;parallel algorithm	DB	0.21371599176969253	37.12213516555655	132513
7cf937b200fe0c961b5386bdf41df8229eff5c8e	activity in an interleaved memory	index terms incomplete gamma function;indexing terms;index terms incomplete gamma function interleaved memory random mapping;incomplete gamma function;interleaved memory;random mapping	A complicated expression for the average number of active modules, in a published model for interleaved memory, is shown to be a function well studied in other contexts.	interleaved memory	Donald E. Knuth;Gururaj S. Rao	1975	IEEE Transactions on Computers	10.1109/T-C.1975.224344	interleaved memory;index term;computer science;theoretical computer science;operating system;machine learning;incomplete gamma function;mathematics;algorithm	Visualization	6.484170718253878	39.11958886691991	132634
78d5189b7c2d906b0319dae2335f1758f84e9c52	high-speed design of montgomery inverse algorithm over gf(2m)	tecnologia electronica telecomunicaciones;modular inverse arithmetic;finite field;cryptography;montgomery algorithm;tecnologias;grupo a;high speed	Montgomery algorithm has demonstrated its effectiveness in applications like cryptosystems. Most of the existing works on finding the Montgomery inverse of an element over the Galois field are based on the software implementation, which is then extended to derive the scalable hardware architecture. In this work, we consider a fundamental change at the algorithmic level and eliminate the potential problems in hardware implementation which makes the resulting modified Montgomery inverse algorithm over GF(2m) very suitable for hardware realization. Due to its structural simplicity, the modified algorithm can be easily mapped onto a high-speed and possibly low-complexity circuit. Experimental results show that our development can achieve both the area and speed advantages over the previous work when the inversion operation over GF(2m) is under consideration and the improvement becomes more significant when we increase the value of m as in the applications of cryptosystems. The salient property of our development sustains the high-speed operation as well as low hardware complexity over a wide range of m for commercial cryptographic applications and makes it suitable for both the scalable architecture and direct hardware implementation.	algorithm;montgomery modular multiplication	Ming-Der Shieh;Jun-Hong Chen;Chien-Ming Wu	2006	IEICE Transactions	10.1093/ietfec/e89-a.2.559	arithmetic;cryptography;theoretical computer science;mathematics;finite field;algorithm;statistics;algebra	EDA	9.813701614529132	43.94413078972365	132692
975452a2cf9999997ed1b6eca253d4ea66eabfbd	an fpga-based processor for training convolutional neural networks		Convolutional neural networks (CNNs) have gained great success in various computer vision applications. However, training a CNN model is computation-intensive and time-consuming. Hence training is mainly processed on large clusters of high-performance processors like server CPUs and GPUs. In this paper, we propose an FPGA-based processor design to accelerate the training process of CNNs. We first analyze the operations in all types of CNN layers in the training process. A uniform computation engine design is proposed to efficiently carry out all kinds of operations based on the analysis. Then a scalable accelerator framework is presented that exploits the parallelism further by unrolling the loops in two levels. The proposed accelerator design is demonstrated by implementing a processor on the Xilinx ZU19EG FPGA working at 200 MHz. The evaluation results on a group of CNN models show that our processor is 5.7 to 10.7-fold faster than the software implementations on the Intel Core i5-4440 CPU(@3.10GHz).	artificial neural network;central processing unit;computation;computer vision;convolutional neural network;field-programmable gate array;graphics processing unit;list of intel core i5 microprocessors;parallel computing;performance per watt;processor design;prototype;scalability;server (computing);throughput	Zhiqiang Liu;Yong Dou;Jingfei Jiang;Qiang Wang;Paul Chow	2017	2017 International Conference on Field Programmable Technology (ICFPT)	10.1109/FPT.2017.8280142	convolutional neural network;convolutional code;implementation;parallel computing;field-programmable gate array;scalability;computer science;computation;processor design;software	EDA	3.4677560842054933	43.53292583124186	133176
37ef50c873c58e6c3ae920c9c6ae337fccdf0854	vlsi-soc: from algorithms to circuits and system-on-chip design		The Advanced Encryption Standard (AES) running in the Galois/Counter Mode of Operation represents a de facto standard in the field of hardware-accelerated, block-cipher-based high-speed authenticated encryption (AE) systems. We propose hardware architectures supporting the Ethernet standard IEEE 802.3ba utilizing different cryptographic primitives suitable for AE applications. Our main design goal was to achieve high throughput on FPGA platforms. Compared to previous works aiming at data rates beyond 100Gbit/s, our design makes use of an alternative block cipher and an alternative mode of operation, namely Serpent and the offset codebook mode of operation, respectively. Using four cipher cores for the encryption part of the AE architecture, we achieve a throughput of 141Gbit/s on an Altera Stratix IV FPGA. The design requires 39 kALMs and runs at a maximum clock frequency of 275MHz. This represents, to the best of our knowledge, the fastest full implementation of an AE scheme on FPGAs to date. In order to make the design applicable in a real-world environment, we developed a custom-designed printed circuit board for the Stratix IV FPGA, suitable to process data with up to 100Gbit/s.	algorithm;authenticated encryption;authentication;block cipher mode of operation;clock rate;codebook;cryptography;fastest;field-programmable gate array;galois/counter mode;hardware acceleration;printed circuit board;printing;stratix;system on a chip;throughput;very-large-scale integration	A. Coskun;Matthew R. Guthaus;Srinivas Katkoori;R. Reis	2012		10.1007/978-3-642-45073-0	system on a chip;very-large-scale integration;electronic circuit;computer science;electronic engineering	EDA	9.003013184505804	45.30637805851514	133188
4c222ab7423ad8343573b9ccb420eb6c8903ab4b	introduction to session r2 (session overiew): advanced computer architectures	processing element;reduced instruction set computer;design process;high density;integrated circuit;time complexity;design and development;integrable system;systolic array;data processing;very large scale integrated;vlsi design;chip;fast fourier transform;computer architecture;word length;system design;pattern matching;adaptive arrays;semantic gap;text retrieval;matrix multiplication;electrical engineering;high throughput;data flow;processing speed;visible light;high performance	Mead and Conway wrote in their pioneering book [1]. “Many LSI chips such as microprocessors, now consist of multiple complex subsystems, and thus are really integrated systems rather than integrated circuits.”. Gone are the days when design of the integrated circuits (IC's) were under the sole purview of the electrical engineers or more so of the solid state device physicists. Then the computer system designers were primarily responsible for designing their computer system based on the standard chips available in the market. But now the computer architects are involved in the chip design process right from the beginning. The close interaction between the engineers and computer scientists has resulted in increased automation of the whole design and fabrication process. This in turn has led to substantial reduction in cost and the turn-around time for the IC chips. It is predicted that by late 1980's it will be possible to fabricate chips containing millions of transistors. The devices and interconnections in such very large scale integrated (VLSI) systems will have linear dimensions smaller than the wavelength of visible light [1]. These advances in technology have produced tremendous impact in the area of computer architecture. The long standing semantic gap between the computer software and hardware now seems to be narrowing down. Several innovative ideas have developed and they have been implemented in VLSI. Some examples of those are Reduced Instruction Set Computer (RISC) [2], systolic arrays [3] and CHIP computer [4], etc. Ultimately, the circuits for these systems will encompass an entire wafer. These super chips will then be called wafer scale integrated (WSI) systems. At present, a wafer ranging from 2 to 8 inches in diameter in size can hold the equivalent of 25 to 100 microprocessors, such as Intel 8086. The same wafer can also hold 4 to 20 megabits of dynamic memory if an 0.8-micrometer Complementary MOS (CMOS) is used. There exists a large difference between the time taken for communication inside the chip and communication across the chip. Much of the time in a general computer system is wasted in data movement between various modules. The performance of a computer system can be tremendously improved if as many components as possible can be placed inside a single chip. VLSI offers this possibility to the system designers. Simple and regular interconnections lead to cheap implementations, high densities and good performance. The algorithms that have simple and regular data flows are particularly suitable for VLSI implementation. Some examples of such algorithms are Matrix Manipulations, Fast Fourier Transform (FFT), etc. Moreover, the concepts of pipelining and parallelism can be effectively employed to improve the overall execution speed. Systolic arrays [3] are a vivid example of a special purpose high performance system that exploit these opportunities offered by VLSI. In this session, we have three carefully refereed papers that exemplify the impact of VLSI on computer architecture. The first one by Kondo et al. describes an SIMD cellular array processor called Adaptive Array Processor (AAP-1). The system was designed and developed by the authors at the Nippon Telegraph and Telephone public corporation of Japan. The AAP-1 consists of a 256×256 array of bit organized processing elements that is built using 1024 custom n-mos LSI's. Extensive parallelism offers ultra high throughput for various types of two dimensional data processing. The processing speed of AAP-1 is shown to exceed that of a 1 MIPS sequential computer by a factor of approximately 100 for certain applications. The second paper by Hurson and Shirazi is on the design and performance issues of a special purpose hardware recognizer capable of performing pattern matching and text retrieval operations. Because of the similarities between the scanning process during compilation and the pattern matching operations, the proposed module can be used as a hardware scanner. The VLSI design, and the space and time complexities of the proposed organization are discussed. The third paper by P.L. Mills describes a design of bit-parallel systolic system for matrix-vector and matrix-matrix multiplication. All the circuits, described here, can be extended to any specification of word length size. The required modifications of the circuits for two's complement operation are also outlined in this paper. I truly appreciate the efforts of the authors without whom this session would not have been possible. I also thank all other authors who had submitted papers in this area. I am indebted to the referees who spent a considerable amount of their time in selecting the papers for this session. Finally, my sincere thanks are due to Terry M. Walker and Wayne D. Dominick for giving me the opportunity to chair this session on Advanced Computer Architectures.	algorithm;amiga walker;array processing;cmos;compiler;computer architecture;computer scientist;conway's game of life;document retrieval;electrical engineering;exemplification;fast fourier transform;finite-state machine;general computer corporation;integrated circuit;matrix multiplication;megabit;memory management;microprocessor;parallel computing;pattern matching;pipeline (computing);simd;semiconductor device fabrication;solid-state electronics;systolic array;throughput;transistor;two's complement;very-large-scale integration;wafer-scale integration	Laxmi N. Bhuyan	1985		10.1145/320599.320636	chip;high-throughput screening;time complexity;data flow diagram;integrable system;fast fourier transform;real-time computing;design process;data processing;computer hardware;systolic array;matrix multiplication;computer science;artificial intelligence;integrated circuit;software engineering;pattern matching;database;visible spectrum;very-large-scale integration;programming language;algorithm;semantic gap;systems design	Arch	5.371831258457472	45.591593214953924	133328
77a1e8448016ec4407da9aec6271a0cc671a9595	recent advances in the design and implementation of large integer factorization algorithms	computers;integer factorization;limiting;arrays;vectors;design and implementation;cryptography;optimization;algorithm design and analysis;algorithm design and analysis vectors computers arrays optimization limiting cryptography	The latest and possibly fastest of the general factoring methods for large composite numbers is the quadratic sieve of Carl Pomerance. A variation of the algorithm is described and an implementation is suggested which combines the forces of a fast pipeline computer such as the Cray I, and a high speed highly parallel array processor such as the Goodyear MPP. A running time analysis, which is based on empirical data rather than asymptotic estimates, suggests that this method could be capable of factoring a 60 digit number in as little as 10 minutes and a 100 digit number is as little as 60 days of continuous computer time.	algorithm;array processing;cray-1;fastest;goodyear mpp;integer factorization;parallel array;quadratic sieve;time complexity;vector processor	Marvin C. Wunderlich	1983	1983 IEEE Symposium on Security and Privacy	10.1109/SP.1983.10014	parallel computing;quadratic sieve;computer science;cryptography;integer factorization;theoretical computer science;algorithm;limiting	Security	7.259936672400974	42.90873444622025	133348
f2295d29997e0d4bd1743701d8ebf870be02ea99	on-board processing using reconfigurable hardware on the solar orbiter phi instrument		Radiation-tolerant Field-Programmable Gate Arrays (FPGAs) enable efficient and high-reliable data processing for scientific space missions at the drawback of highly complex and customized FPGA designs. This paper describes the design of a simple but efficient and flexible FPGA architecture as accelerator for data processing. It combines the performance of an FPGA implementation with the flexibility of software realizations. The efficient coupling of FPGA or ASIC accelerators to general-purpose processors (GPPs) is currently a general problem, for which our proposed architecture provides a dedicated solution. This is exemplarily implemented for the image data pre-processing on the in-flight reconfigurable, power-efficient and radiation-tolerant data processing module developed for the Solar Orbiter PHI instrument.	application-specific integrated circuit;central processing unit;data pre-processing;field-programmable gate array;general-purpose markup language;preprocessor	Tobias Lange;Björn Fiethe;Holger Michel;Harald Michalik;Kinga Albert;Johann Hirzberger	2017	2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2017.8046377	application-specific integrated circuit;parallel computing;computer science;architecture;real-time computing;orbiter;field-programmable gate array;software;fpga prototype;reconfigurable computing;embedded system;control system	EDA	3.2549909296834754	46.072391114729996	133530
bbab27a1fe4c21b32a0139c4608de4d89d3fc357	elliptic curve point multiplication in gf(2n) using polynomial residue arithmetic	elliptic curve discrete logarithm problem;elliptic curves;elliptic curve;residue number systems;polynomials;computer architecture;vlsi cryptography polynomials residue number systems;vlsi architecture elliptic curve point multiplication polynomial residue arithmetic cryptosystems;elliptic curve cryptography;cryptography;vlsi;elliptic curves polynomials elliptic curve cryptography galois fields algorithm design and analysis hardware digital arithmetic very large scale integration public key cryptography energy consumption;elliptic curve cryptosystem;conferences;hardware;vlsi architecture	Elliptic Curve Point Multiplication is the main operation employed in all elliptic curve cryptosystems, as it forms the basis of the Elliptic Curve Discrete Logarithm Problem. Therefore, the efficient realization of an Elliptic Curve Point Multiplier is of fundamental importance, as its performance is decisive for the performance of the overall cryptosystem. This work presents the first practical implementation of an Elliptic Curve Point Multiplier in GF(2n) using Polynomial Residue Arithmetic. Unlike the typical representation of GF(2n) elements as polynomials in GF(2)[x] of degree at most n − 1, data are represented as their remainder modulo a set of L pairwise prime polynomials m1,m2, … ,mL of degree w and such that Lw ≥ 2n. The methodology for incorporating Polynomial Residue Arithmetic in the elliptic curve point addition and doubling algorithms, as well as the VLSI architecture of the proposed point multiplier are analyzed, thus forming an interesting alternative to Elliptic Curve Cryptography realization.	algorithm;cost per mille;cryptosystem;degree (graph theory);elliptic curve cryptography;grammatical framework;irreducible polynomial;modulo operation;period-doubling bifurcation;physical review a;residue number system;very-large-scale integration	Dimitrios M. Schinianakis;Athanasios Kakarountas;Thanos Stouraitis;Alexander Skavantzos	2009	2009 16th IEEE International Conference on Electronics, Circuits and Systems - (ICECS 2009)	10.1109/ICECS.2009.5410842	arithmetic;supersingular elliptic curve;discrete mathematics;jacobian curve;twists of curves;lenstra elliptic curve factorization;tripling-oriented doche–icart–kohel curve;schoof–elkies–atkin algorithm;hyperelliptic curve cryptography;counting points on elliptic curves;curve25519;mathematics;elliptic curve cryptography;hessian form of an elliptic curve;elliptic curve;modular elliptic curve;elliptic curve point multiplication;schoof's algorithm;stable curve;algebra	Crypto	9.958467217234297	43.63810032772144	133649
710828974f8c2f63538484a2ede1ccc2ace3eba4	sign backpropagation: an on-chip learning algorithm for analog rram neuromorphic computing systems	multilayer perceptron (mlp);neural network;neuromorphic computing;on-chip learning;resistive random-access memory (rram)	Currently, powerful deep learning models usually require significant resources in the form of processors and memory, which leads to very high energy consumption. The emerging resistive random access memory (RRAM) has shown great potential for constructing a scalable and energy-efficient neural network. However, it is hard to port a high-precision neural network from conventional digital CMOS hardware systems to analog RRAM systems owing to the variability of RRAM devices. A suitable on-chip learning algorithm should be developed to retrain or improve the performance of the neural network. In addition, determining how to integrate the periphery digital computations and analog RRAM crossbar is still a challenge. Here, we propose an on-chip learning algorithm, named sign backpropagation (SBP), for RRAM-based multilayer perceptron (MLP) with binary interfaces (0, 1) in forward process and 2-bit (±1, 0) in backward process. The simulation results show that the proposed method and architecture can achieve a comparable classification accuracy with MLP on MNIST dataset, meanwhile it can save area and energy cost by the calculation and storing of the intermediate results and take advantages of the RRAM crossbar potential in neuromorphic computing.	algorithm;analog;artificial neural network;backpropagation;biological neural networks;cmos;central processing unit;color depth;computation (action);crossbar switch;deep learning;mnist database;memory disorders;memory-level parallelism;multilayer perceptron;name;neuromorphic engineering;quad flat no-leads package;random access;resistive random-access memory;romberg sign finding;sbp;scalability;silo (dataset);spatial variability	Qingtian Zhang;Huaqiang Wu;Peng Yao;Wenqiang Zhang;Bin Gao;Ning Deng;He Qian	2018	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2018.08.012	mathematics;architecture;deep learning;artificial neural network;resistive random-access memory;backpropagation;algorithm;mnist database;multilayer perceptron;neuromorphic engineering;artificial intelligence	EDA	3.9658193483920563	42.677909374979336	133785
0c29c56cbf40a794ae247677d5ab2748868a133c	a framework for generating high throughput cnn implementations on fpgas		"""We propose a framework to generate highly efficient accelerators for inferencing on FPGAs. Our framework consists of multiple algorithmic optimizations for computation complexity and communication volume reduction, a mapping methodology for efficient resource utilization, and a tool for automatic \textttVerilog  generation. The algorithmic optimizations improve throughput of frequency domain convolution so as to satisfy a given set of hardware constraints. While the Overlap-and-Add (OaA) technique has been known, it performs """"wasted"""" computation at the edges. We propose a novel Concatenate-and-Pad (CaP) technique, which improves OaA significantly by reducing the """"wasted"""" computation on the padded pixels. The proposed CaP used in conjunction with OaA enables us to choose a fixed FFT size at design time, and achieve low computation complexity for layers with various image sizes and kernel window sizes. We also develop a novel frequency domain loop tiling technique to further boost throughput by improving data reuse. Our mapping methodology optimizes the architecture for the target device by fast design space exploration. We quantitatively categorize FPGAs by capturing their DSP resources, on-chip memory size and external memory bandwidth into a device coefficient. We identify the optimal architectural parameters based on the tradeoff between computation and communication cost. Our framework includes a tool to automatically generate fully synthesizable  \textttVerilog. We demonstrate the framework by generating high throughput accelerators for state-of-the-art CNN models on Intel HARP heterogeneous platform. Using our framework, we achieve throughput of $780.6$ $GOPS$, $669.1$ $GOPS$ and $552.1$ $GOPS$ for AlexNet, VGG16 and FCN-16s respectively. These correspond to $6.8\times$ (AlexNet) and $4.9\times$ (VGG16) improvement compared with the state-of-the-art implementations."""	algorithm;categorization;coefficient;computation;concatenation;convolution;design space exploration;fast fourier transform;field-programmable gate array;harp;hybrid algorithm;memory bandwidth;overlap–add method;pixel;sparse matrix;throughput;tiling window manager	Hanqing Zeng;Ren Chen;Chi Zhang;Viktor K. Prasanna	2018		10.1145/3174243.3174265	throughput;architecture;parallel computing;field-programmable gate array;real-time computing;fast fourier transform;design space exploration;computation;digital signal processing;computer science;loop tiling	EDA	2.620887768363325	45.30115663772859	134170
521f3a4ebf250739e8d5fe85b382dafef92cc245	a study of average-case speedup and scalability of parallel computations on static networks.	parallel computer	A parallel system consists of a parallel algorithm and a parallel machine that supports the implementation of the algorithm. The scalability of a parallel system is a measure of its capability to increase speedup in proportion to the number of processors, or its capability to keep a constant efficiency as the number of processors increases. The present paper is devoted to the investigation of the average-case scalability of parallel algorithms executing on multicomputers with symmetric static networks, including the completely connected network, ring, hypercube, and torus. In particular, we characterize the communication overhead such that the expected efficiency can be kept at certain constant level, and that the number of tasks grows at the rate @Q(PlogP).		Keqin Li;Yi Pan;Hong Shen;Si-Qing Zheng	1997			parallel computing;speedup;computer science;theoretical computer science;distributed computing	HPC	0.005897394126952676	37.515204401318094	134216
5ce360fafbb72aafa0148466687d8f162b0f003a	gpu accelerated neh algorithm	percentage relative difference gpu accelerated neh algorithm cuda accelerated neh algorithm permutative flowshop scheduling problem makespan criterion gpu cores parallel evaluation cuda based neh taillard sets paired t test;kernel;graphics processing units instruction sets kernel parallel processing synchronization algorithm design and analysis arrays;arrays;synchronization;graphics processing units;production engineering computing flow shop scheduling graphics processing units parallel architectures;algorithm design and analysis;parallel processing;instruction sets	This research aims to develop a CUDA accelerated NEH algorithm for the permutative flowshop scheduling problem with makespan criterion. NEH has been shown in the literature as the best constructive heuristic for this particular problem. The CUDA based NEH aims to speed up the processing time by utilising the GPU cores for parallel evaluation. In order to show the versatility and scalability of the CUDA based NEH, four new higher dimensional Taillard sets are generated. The experiments are conducted on the CPU and GPU and pairwise compared. Percentage relative difference and paired t-test both confirm that the GPU based NEH significantly improves on the execution time compared to the sequential CPU version for all the high dimensional problem instances.	algorithm;blocking (computing);cuda;central processing unit;constructive heuristic;embedded system;evolutionary algorithm;experiment;graphics processing unit;kepler (microarchitecture);makespan;product requirements document;relative change and difference;rough set;run time (program lifecycle phase);scalability;scheduling (computing);speedup	Magdalena Metlicka;Donald Davendra;Frank Hermann;Markus Meier;Matthias Amann	2014	2014 IEEE Symposium on Computational Intelligence in Production and Logistics Systems (CIPLS)	10.1109/CIPLS.2014.7007169	parallel computing;real-time computing;computer science;theoretical computer science	Embedded	0.19481843971326215	40.34830964405742	134429
d0cabccf0b9340c6a00e5a931ef3b68debe5840f	efficient deep convolutional neural networks accelerator without multiplication and retraining		Recently, low-precision weight method has been considered as a promising scheme to efficiently implement inference of deep convolutional neural networks (DCNN). But it suffers from expensive retraining cost and accuracy degradation. In this paper, a low-bit and retraining-free quantization method, which enables DCNNs to deal inference with only shift and add operations, is proposed. The efficiency is demonstrated in terms of power consumption and chip area. Huffman coding is adopted for further compression. Then by exploring two-level systolic, an efficient hardware accelerator is introduced with respect to the given quantization strategy. Experiment results show that our method achieves higher accuracy than other low-precision networks without retraining process on ImageNet. 5× to 8× compression is obtained on popular models compared to full-precision counterparts. Furthermore, hardware implementation indicates good reduction of slices whereas maintaining throughput.	artificial neural network;convolutional neural network;elegant degradation;hardware acceleration;huffman coding;imagenet;quantization (signal processing);throughput	Weihong Xu;Zaichen Zhang;Xiaohu You;Chuan Zhang	2018	2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2018.8461627	system on a chip;throughput;convolutional neural network;huffman coding;quantization (signal processing);machine learning;artificial intelligence;computer science;pattern recognition;inference;shift-and-add;hardware acceleration	EDA	3.8187020810694303	42.81003181369115	134530
ef14ecd10d00ce494965f024858a60546ade3702	hybrid neural network, an efficient low-power digital hardware implementation of event-based artificial neural network		Interest in event-based vision sensors has proliferated in recent years, with innovative technology becoming more accessible to new researchers and highlighting such sensors' potential to enable low-latency sensing at low computational cost. These sensors can outperform frame-based vision sensors regarding data compression, dynamic range, temporal resolution and power efficiency. However, available mature frame-based processing methods by using Artificial Neural Networks (ANNs) surpass Spiking Neural Networks (SNNs) in terms of accuracy of recognition. In this paper, we introduce a Hybrid Neural Network which is an intermediate solution to exploit advantages of both event-based and frame-based processing. We have implemented this network in FPGA and benchmarked its performance by using different event-based versions of MNIST dataset. HDL codes for this project are available for academic purpose upon request.	algorithmic efficiency;artificial neural network;code;data compression;digital electronics;dynamic range;dynamic voltage scaling;field-programmable gate array;hybrid neural network;image sensor;low-power broadcasting;mnist database;neuron;online and offline;performance per watt;spiking neural network	Amirreza Yousefzadeh;Garrick Orchard;Evangelos Stromatias;Teresa Serrano-Gotarredona;Bernab&#x00E9; Linares-Barranco	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8351562	hybrid neural network;artificial neural network;field-programmable gate array;electronic engineering;machine learning;data compression;spiking neural network;computer science;exploit;temporal resolution;mnist database;artificial intelligence	Arch	3.8661672855343263	42.57459803024142	134947
770f9c2c03c91f56a8c6c3511361e46ee47075d0	an extended-precision operand computer for integer factoring	integer factorization;word length;continued fraction	We describe an extended-precision operand computer (EPOC). The singleprecision word length is 128 bits. This makes possible calculations with large integers without resort to multiprecision techniques in software. Since this is a specialpurpose machine, the hardware and software have been developed from scratch to implement it. The application toward which the EPOC is directed is the factoring of large integers using the continued fraction algorithm. This application presents interesting mathematical and architectural problems to solve and has implications in cryptography.	algorithm;arbitrary-precision arithmetic;cryptography;epoc (operating system);extended precision;integer factorization;operand	Jeffrey W. Smith;Samuel S. Wagstaff	1984		10.1145/1499310.1499326	discrete mathematics;theoretical computer science;mathematics;algorithm	Arch	8.583751281358394	42.95668445902204	135124
7fc3a072fed2682bc7072ca3f2111065ac22cad3	multi-assignment single joins for parallel cross-match of astronomic catalogs on heterogeneous clusters	distributed triangle listing;graph analytics;mapreduce	Cross-match is a central operation in astronomic databases to integrate multiple catalogs of celestial objects. With the rapid development of new astronomy projects, large amounts of astronomic catalogs are generated and require fast cross-match with existing databases. In this paper, we propose to adopt a Multi-Assignment Single Join (MASJ) method for cross-match on heterogeneous clusters that consist of both CPUs and GPUs. We chose MASJ for cross-match, because (1) cross-matching records from astronomic catalogs is essentially a spatial distance join on two sets of points, and (2) each reference point is mapped to only a small number of search intervals. As a result, the MASJ cross-match, or MASJ-CM algorithm is feasible and highly efficient in a heterogeneous cluster environment. We have implemented MASJ-CM in two packages: one is an MPI-CUDA implementation, which fully utilizes the multi-core CPUs, GPUs, and InfiniBand communications; the other is on top of the popular distributed computing platform Spark, which greatly simplifies the programming. Our results on a six-node CPU-GPU cluster show that the MPI-CUDA implementation achieved a speedup of 2.69 times over a previous indexed nested-loop join algorithm. The Spark-based implementation was an order of magnitude slower than the MPI-CUDA; nevertheless, it is widely applicable and its source code much simpler.	algorithm;cuda;celestial coordinate system;central processing unit;database;distributed computing;gpu cluster;graphics processing unit;infiniband;join (sql);multi-core processor;nested loop join;speedup	Xiaoying Jia;Qiong Luo	2016		10.1145/2949689.2949705	parallel computing;computer science;theoretical computer science;data mining;database;distributed computing	DB	-1.5193030528398905	41.72801385210328	135206
6857ce558c208909dcd784b39b991200dd4e6a9e	implementing the projected spatial rich features on a gpu	paper;steganalysis;cuda;machine learning;graphics processing units;nvidia;tesla k20;computer hardware;computer science	The Projected Spatial Rich Model (PSRM) generates powerful steganalysis features, but requires the calculation of tens of thousands of convolutions with image noise residuals. This makes it very slow: the reference implementation takes an impractical 20–30 minutes per 1 megapixel (Mpix) image. We present a case study which first tweaks the definition of the PSRM features, to make them more efficient, and then optimizes an implementation on GPU hardware which exploits their parallelism (whilst avoiding the worst of their sequentiality). Some nonstandard CUDA techniques are used. Even with only a single GPU, the time for feature calculation is reduced by three orders of magnitude, and the detection power is reduced only marginally.	cuda;cache (computing);central processing unit;clock signal;compiler;computation;convolution;experiment;feature extraction;graphics processing unit;haswell (microarchitecture);image noise;kepler (microarchitecture);mathematical optimization;multi-core processor;parallel computing;pixel;real-time computing;real-time transcription;reference implementation;rendering (computer graphics);secure remove;spec#;steganalysis;video card;while	Andrew D. Ker	2014		10.1117/12.2042473	steganalysis;computer hardware;computer science;theoretical computer science;computer graphics (images)	Vision	-1.9799196887578019	44.70762743118182	135307
d4fcfd0df8efb664401ecbb613210a343df614b0	sequence alignment through the looking glass		"""Rapid advances in sequencing technologies are producing genomic data on an unprecedented scale. The first, and often one of the most time consuming, step of genomic data analysis is sequence alignment, where sequenced reads must be aligned to a reference genome. Several years of research on alignment algorithms has led to the development of several state-of-the-art sequence aligners that can map tens of thousands of reads per second. In this work, we answer the question """"How do sequence aligners utilize modern processors?"""" We examine four state-of-the-art aligners running on an Intel processor and identify that all aligners leave the processor substantially underutilized. We perform an in-depth microarchitectural analysis to explore the interaction between aligner software and processor hardware. We identify bottlenecks that lead to processor underutilization and discuss the implications of our analysis on next-generation sequence aligner design."""	approximate string matching;central processing unit;glass;microarchitecture;parallel computing;scalability;sequence alignment;string searching algorithm	Raja Appuswamy;Jacques Fellay;Nimisha Chaturvedi	2018	2018 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2018.00050	computer science;genomics;parallel computing;software;sequence alignment;reference genome;microarchitecture	Arch	-0.27200629414004923	43.167333823037595	135351
d5e43b4cacd4ce6cc1314b8bd81d4cb2a591df0a	dual-field arithmetic unit for gf(p) and gf(2m)	elliptic curve;inversion;corps fini;courbe elliptique;finite field;curva eliptica;criptografia;cryptography;finite field arithmetic;redundant number representation;campo finito;cryptographie;computer hardware;unidad aritmetica;unite arithmetique;multiplication;representation nombre redondant;materiel informatique;material informatica;hardware implementation;arithmetic unit;reduction modulaire;modular reduction	In this article we present a hardware solution for finite field arithmetic with application in asymmetric cryptography. It supports calculation in GF (p) as well as in GF (2). Addition and multiplication with interleaved modular reduction are the main functionality of the unit. Additional functions—like shift operations and integer incrementation—allow the calculation of the multiplicative inverse and covering all operations required to implement Elliptic Curve Cryptography. Redundant number representation and efficient modular reduction make it ready for future cryptographic bitlengths and allow operation at high clock frequency on moderate hardware resources.	clock rate;critical path method;datapath;digital signature;elliptic curve cryptography;embedded system;extended euclidean algorithm;full custom;grammatical framework;ieee journal of solid-state circuits;lecture notes in computer science;public-key cryptography;reduction (complexity);springer (tank)	Johannes Wolkerstorfer	2002		10.1007/3-540-36400-5_36	inversion;arithmetic;finite field arithmetic;discrete mathematics;cryptography;mathematics;elliptic curve;gf(2);finite field;multiplication;algorithm;algebra	Crypto	9.436190148074681	44.07928150012189	135433
3f517825f55d9d022614a57c33f36d2e4dd52215	design and asic acceleration of cortical algorithm for text recognition	random access memory;computer architecture;application specific integrated circuits;spatiotemporal phenomena;algorithm design and analysis;parallel processing;hardware	Cortical algorithms, inspired by the neocortex, promise to outperform conventional algorithms in unsupervised learning tasks, i.e. with unlabeled data. The aim of the work reported in this paper was to design and implement an application specific integrated circuit (ASIC) having a massive speedup of a cortical algorithm, as compared with a CPU baseline. This ASIC is designed to implement a scaled-down version of Sparsey, an algorithm based on structural and functional properties of the brain's cortex. The design was benchmarked on the Short Message Service (SMS) spam collection dataset from the UCI machine learning repository. It was found that the synthesis area and power consumption of a single column (i.e., mac or PE) are 0.122 mm2 and 5.15 mW using 45 nm technology and 0.171 mm2 and 7.94 mW using 65 nm technology. The processing time for a single frame was 3.075 µs (learning) and 0.675 µs (recognition). The performance speedup in learning and recognition modes of ASIC implementation was 203× and 843× times that of software implementation on a CPU based platform.	algorithm;application-specific integrated circuit;baseline (configuration management);central processing unit;general-purpose computing on graphics processing units;hardware acceleration;machine learning;optical character recognition;spamming;speedup;unsupervised learning	Sumon Dey;Paul D. Franzon	2016	2016 29th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2016.7905447	embedded system;parallel processing;algorithm design;electronic engineering;parallel computing;real-time computing;telecommunications;computer science;electrical engineering;theoretical computer science;operating system;application-specific integrated circuit	EDA	4.274495031786678	43.59940109468895	135637
394d002a4cde8622b1c782b2879f5184fefef6d6	gre: a graph runtime engine for large-scale distributed graph-parallel applications	parallel	Large-scale distributed graph-parallel computing is challenging. On one hand, due to the irregular computation pattern and lack of locality, it is hard to express parallelism efficiently. On the other hand, due to the scale-free nature, realworld graphs are hard to partition in balance with low cut. To address these challenges, several graph-parallel frameworks including Pregel and GraphLab (PowerGraph) have been developed recently. In this paper, we present an alternative framework, Graph Runtime Engine (GRE). While retaining the vertex-centric programming model, GRE proposes two new abstractions: 1) a Scatter-Combine computation model based on active message to exploit massive fined-grained edge-level parallelism, and 2) a Agent-Graph data model based on vertex factorization to partition and represent directed graphs. GRE is implemented on commercial off-the-shelf multi-core cluster. We experimentally evaluate GRE with three benchmark programs (PageRank, Single Source Shortest Path and Connected Components) on realworld and synthetic graphs of millions∼billion of vertices. Compared to PowerGraph, GRE shows 2.5∼17 times better performance on 8∼16 machines (192 cores). Specifically, the PageRank in GRE is the fastest when comparing to counterparts of other frameworks (PowerGraph, Spark,Twister) reported in public literatures. Besides, GRE significantly optimizes memory usage so that it can process a large graph of 1 billion vertices and 17 billion edges on our cluster with totally 768GB memory, while PowerGraph can only process less than half of this graph scale.	active message;benchmark (computing);connected component (graph theory);data model;directed graph;experiment;fastest;locality of reference;model of computation;multi-core processor;pagerank;parallel computing;programming model;runtime system;shortest path problem;synthetic intelligence;twister;vertex (geometry)	Jie Yan;Guangming Tan;Ninghui Sun	2013	CoRR		parallel computing;computer science;theoretical computer science;operating system;distributed computing;programming language;algorithm	OS	-3.26332431084284	42.76915147226018	135718
bf32337346ecaf26c45cbf5395ae274be90559ed	some multigrid algorithms for elliptic problems on data parallel machines	equation derivee partielle;data parallel;problema valor limite;partial differential equation;ecuacion derivada parcial;analisis numerico;software tool;algorithm performance;boundary value problem;performance;machine parallele;multigrille;informing science;elliptic problem;65y05;parallel computation;analyse numerique;algorithme;equation elliptique;elliptic equation;iterative methods;algorithm;computer architecture;65n55;calculo paralelo;numerical analysis;parallel architectures;architecture parallele;partial differential equations;resultado algoritmo;comparative evaluations;multigrid;performance algorithme;multigrilla;algorithms;parallel machines;iteration method;ecuacion eliptica;mesh generation;probleme valeur limite;calcul parallele;parallel processing;algoritmo;mathematics computers information science management law miscellaneous	Previously a semicoarsening multigrid algorithm suitable for use on data parallel architectures was investigated. Through the use of new software tools, the performance of this algorithm has been considerably improved. The method has also been extended to three space dimensions. The method performs well for strongly anisotropic problems and for problems with coefficients jumping by orders of magnitude across internal interfaces. The parallel efficiency of this method is analyzed, and its actual performance on the CM-5 is compared with its performance on the CRAY Y-MP and the Sparc-5. A standard coarsening multigrid algorithm is also considered, and we compare its performance on these three platforms as well.	algorithm;data parallelism;multigrid method	V. A. Bandy;Joel E. Dendy;W. H. Spangenberg	1998	SIAM J. Scientific Computing	10.1137/S1064827596303648	parallel processing;mathematical optimization;computer science;theoretical computer science;mathematics;iterative method;partial differential equation;algorithm	HPC	-3.8983128961996383	36.97882895283931	135864
34d564dcb4483aa0a8c9b7e6647f07de24223ba1	optimized fft computations on heterogeneous platforms with application to the poisson equation	parallel and vector implementations;cuda gpu;fast fourier transforms;poisson equations	We develop optimized multi-dimensional FFT implementations on CPU–GPU heterogeneous platforms for the case when the input is too large to fit on the GPU global memory, and use the resulting techniques to develop a fast Poisson solver. The solver involves memory bound computations for which the large 3D datamayhave to be transferred over the PCIe bus several times during the computation.Wedevelop anew strategy to decompose and allocate the computation between theGPUand the CPU such that the 3Ddata is transferred only once to the device memory, and the executions of the GPU kernels are almost completely overlapped with the PCI data transfer. We were able to achieve significantly better performance than what has been reported in previous related work, including over 145 GFLOPS for the three periodic boundary conditions (single precision version), and over 105 GFLOPS for the two periodic, one Neumann boundary conditions (single precision version). The effective bidirectional PCIe bus bandwidth achieved is 9–10 GB/s, which is close to the best possible on our platform. For all the cases tested, the single 3D data PCIe transfer time, which constitutes a lower bound onwhat is possible on our platform, takes almost 70% of the total execution time of the Poisson solver. © 2014 Elsevier Inc. All rights reserved.	central processing unit;computation;flops;fast fourier transform;fractional poisson process;gigabyte;graphics processing unit;memory bound function;pci express;periodic boundary conditions;run time (program lifecycle phase);single-precision floating-point format;solver	Jing Wu;Joseph JáJá	2014	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2014.03.009	fast fourier transform;mathematical optimization;parallel computing;real-time computing;computer science;theoretical computer science;operating system;algorithm	HPC	-4.247083056689926	39.81834414056968	136083
a18724b29a44032e5ee48daf04b3687a283d4fcd	load balancing versus occupancy maximization on graphics processing units: the generalized hough transform as a case study	paper;shared memory;image processing;generalized hough transform;compute unified device architecture;general techniques;cuda;memory access;programming techniques;occupancy;load balancing;nvidia;graphic processing unit;image processing techniques;sparse data;load balance;computer science;graphics processing unit;program development	Programs developed under the Compute Unified Device Architecture obtain the highest performance rate, when the exploitation of hardware resources on a Graphics Processing Unit (GPU) is maximized. In order to achieve this purpose, load balancing among threads and a high value of processor occupancy, i.e. the ratio of active threads, are indispensable. However, in certain applications, an optimally balanced implementation may limit the occupancy, due to a greater need for registers and shared memory. This is the case of the Fast Generalized Hough Transform (Fast GHT), an image-processing technique for localizing an object within an image. In this work, we present two parallelization alternatives for the Fast GHT, one that optimizes the load balancing and another that maximizes the occupancy. We have compared them using a large amount of real images to test their strong and weak points and we have drawn several conclusions about under which conditions it is better to use one or the other. We have also tackled several parallelization problems related to sparse data distribution, divergent execution paths, and irregular memory access patterns in updating operations by proposing a set of generic techniques, including compacting, sorting, and memory storage replication. Finally, we have compared our Fast GHT with the classic GHT, both on a current GPU, obtaining an important speed-up.		Juan Gómez-Luna;José María González-Linares;José Ignacio Benavides Benítez;Emilio L. Zapata;Nicolás Guil Mata	2011	IJHPCA	10.1177/1094342010383998	parallel computing;computer hardware;image processing;computer science;load balancing;theoretical computer science;operating system	Robotics	-4.504974029815647	43.52080220686449	136291
892e182f1a17abf8db0e71d1f0363328cff31314	towards a uniform template-based architecture for accelerating 2d and 3d cnns on fpga		Three-dimensional convolutional neural networks (3D CNNs) are used efficiently in many computer vision applications. Most previous work in this area has concentrated only on designing and optimizing accelerators for 2D CNN, with few attempts made to accelerate 3D CNN on FPGA. We find accelerating 3D CNNs on FPGA to be challenge due to their high computational complexity and storage demands. More importantly, although the computation patterns of 2D and 3D CNNs are analogous, the conventional approaches adopted for accelerating 2D CNNs may be unfit for 3D CNN acceleration. In this paper, in order to accelerate 2D and 3D CNNs using a uniform framework, we propose a uniform template-based architecture that uses templates based on the Winograd algorithm to ensure fast development of 2D and 3D CNN accelerators. Furthermore, we also develop a uniform analytical model to facilitate efficient design space explorations of 2D and 3D CNN accelerators based on our architecture. Finally, we demonstrate the effectiveness of the template-based architecture by implementing accelerators for real-life 2D and 3D CNNs (VGG16 and C3D) on multiple FPGA platforms. On S2C VUS440, we achieve up to 1.13 TOPS and 1.11 TOPS under low resource utilization for VGG16 and C3D, respectively. End-to-end comparisons with CPU and GPU solutions demonstrate that our implementation of C3D achieves gains of up to 13x and 60x in performance and energy relative to a CPU solution, and a 6.4x energy efficiency gain over a GPU solution.	artificial neural network;central processing unit;computation;computational complexity theory;computer vision;convolutional neural network;coppersmith–winograd algorithm;field-programmable gate array;graphics processing unit;performance prediction;powerpc 600;real life;tops	Junzhong Shen;You Huang;Zelong Wang;Yuran Qiao;Mei Wen;Chunyuan Zhang	2018		10.1145/3174243.3174257	field-programmable gate array;architecture;parallel computing;convolutional neural network;computational complexity theory;computation;computer science	Arch	3.4435720899342663	43.343352306013145	136638
f91d488d418d1470ba3c9fb0478bae668807e5a7	efficient dense matrix-vector multiplication on gpu			graphics processing unit;matrix multiplication;sparse matrix	Guixia He;Jiaquan Gao	2018	Concurrency and Computation: Practice and Experience	10.1002/cpe.4705	parallel computing;computer science;sparse matrix;multiplication;cuda	Arch	-2.604852303534348	39.39630960902712	136663
195061c0cac0ce5a69592ea39e6fa459cc2b84b4	a model design of a 2560-channel neural spike detection platform	input data transmission 2560 channel neural spike detection platform model design 2560 channel neural signal processing platform reconfigurable channels field programmable gate arrays fpga high speed serial transceiver;medical signal detection;logic design;reconfigurable architectures;high speed serial transceivers reconfigurable fpga neural spike detection;data communication;transceivers data communication field programmable gate arrays logic design medical signal detection neurophysiology reconfigurable architectures;transceivers;neurophysiology;field programmable gate arrays;field programmable gate arrays time division multiplexing signal processing neurons generators transceivers hardware	A model design of a 2560-channel neural signal processing platform is presented. The design carries out spike detection over a large reconfigurable number of channels on Field Programmable Gate Arrays (FPGAs), and integrates the application of high-speed serial transceivers to allow for the required massive input data transmissions.	field-programmable gate array;signal processing;transceiver	Nashwa Elaraby;Iyad Obeid	2012	2012 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2012.6416762	embedded system;logic synthesis;real-time computing;reconfigurable computing;programmable logic array;computer science;neurophysiology;field-programmable gate array;transceiver	EDA	5.421858291115561	44.197923107207444	136691
0b6ef07a32516a81c99774bb6e4ed2e175652cd0	dna assembly with de bruijn graphs using an fpga platform		This paper presents an FPGA implementation of a DNA assembly algorithm, called Ray, initially developed to run on parallel CPUs. The OpenCL language is used and the focus is placed on modifying and optimizing the original algorithm to better suit the new parallelization tool and the radically different hardware architecture. The results show that the execution time is roughly one fourth that of the CPU and factoring energy consumption yields a tenfold savings.	algorithm;cpu (central processing unit of computer system);central processing unit;compiler;compliance behavior;computers;de bruijn graph;expansion card;field-programmable gate array;graph - visual representation;graphics processing unit;hash table;hashtable;integer factorization;kernel;marina;opencl api;operating tables;parallel computing;preparation;run time (program lifecycle phase);salmonella enterica;sequence assembly;solar cell;supercomputer;suppressor of cytokine signaling proteins;system on a chip;vitis;workstation;x86	Carl Poirier;Benoit Gosselin;Paul Fortier	2018	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2017.2696522	de bruijn sequence;kernel (linear algebra);field-programmable gate array;computer science;hardware architecture;central processing unit;parallel processing;bioinformatics;graph	HPC	-0.19243983475829005	43.601799747556235	136885
a31eae87313c704611c343a3a76502debebc83f2	mapping algorithms and software environment for data parallel pde iterative solvers	tratamiento paralelo;parallelisme;optimal solution;iterative solver;data parallel;equation differentielle;partial differential equation;traitement parallele;differential equation;parallel computation;ecuacion diferencial;synchronisation;parallelism;calculo paralelo;paralelismo;synchronization;sincronizacion;communication;calcul parallele;comunicacion;parallel processing	We consider computations associated with data parallel iterative solvers used for the numerical solution of Partial Di erential Equations (PDEs). The mapping of such computations into load balanced tasks requiring minimum synchronization and communication is a di cult combinatorial optimization problem. Its optimal solution is essential for the e cient parallel processing of PDE computations. Determining data mappings that optimize a number of criteria, like workload balance, synchronization and local communication, often involves the solution of an NP-Complete problem. Although data mapping algorithms have been known for a few years there is lack of qualitative and quantitative comparisons based on the actual performance of the parallel computation. In this paper we present two new data mapping algorithms and evaluate them together with a large number of existing ones using the actual performance of data parallel iterative PDE solvers on the nCUBE II. Comparisons on the performance of data parallel iterative PDE solvers on medium and large scale problems demonstrate that some computationally inexpensive data block partitioning algorithms are as e ective as the computationally expensive deterministic optimization algorithms. Also, these comparisons demonstrate that the existing approach in solving the data partitioning problem is ine cient for large scale problems. Finally, a software environment for the solution of the partitioning problem of data parallel iterative solvers is presented.	algorithm;analysis of algorithms;combinatorial optimization;computation;data parallelism;iteration;iterative method;mathematical optimization;np-completeness;numerical partial differential equations;optimization problem;parallel computing;partition problem;solver	Nikos Chrisochoides;Elias N. Houstis;John R. Rice	1994	J. Parallel Distrib. Comput.	10.1006/jpdc.1994.1043	parallel processing;synchronization;mathematical optimization;combinatorics;parallel computing;telecommunications;computer science;theoretical computer science;distributed computing;algorithm	HPC	-2.8079916884694587	36.57810914293689	136916
a189c2240fe04c10e7297b2e7d6fd8ebc2b362aa	offline permutation on the cuda-enabled gpu		The Hierarchical Memory Machine (HMM) is a theoretical parallel computing model that captures the essence of computation on CUDA-enabled GPUs. The offline permutation is a task to copy numbers stored in an array a of size n to an array b of the same size along a permutation P given in advance. A conventional algorithm can complete the offline permutation by executing b[p[i]]← a[i] for all i in parallel, where an array p stores the permutation P. We first present that the conventional algorithm runs Dw(P) + 2 n w + 3L − 3 time units using n threads on the HMM with width w and latency L, where Dw(P) is the distribution of P. We next show that important regular permutations including transpose, shuffle, and bitreversal permutations run 2 n w + 2 n kw + 2L − 2 time units on the HMM with k DMMs. We have implemented permutation algorithms for these regular permutations on GeForce GTX 680 GPU. The experimental results show that these algorithms run much faster than the conventional algorithm. We also present an offline permutation algorithm for any permutation running in 16 n w + 16 n kw + 16L − 16 time units on the HMM with k DMMs. Quite surprisingly, our offline permutation algorithm on the GPU achieves better performance than the conventional algorithm in random permutation, although the running time has a large constant factor. We can say that the experimental results provide a good example of GPU computation showing that a complicated but ingenious implementation with a larger constant factor in computing time can outperform a much simpler conventional algorithm. key words: memory machine models, offline permutation, GPU, CUDA	algorithm;cuda;computation;geforce 200 series;geforce 600 series;geforce 900 series;graphics processing unit;hidden markov model;online and offline;parallel computing;random permutation;time complexity	Akihiko Kasagi;Koji Nakano;Yasuaki Ito	2014	IEICE Transactions		parallel computing;theoretical computer science;machine learning	Theory	1.3156762944651075	39.9769352222981	137075
987c28bcee7dcae64d93616546e4373bc1892c31	fpga based dpa-resistant unified architecture for signcryption	gf 2 p field;dpa resistant unified architecture;authorisation;authentication;gf 2 p field fpga dpa resistant unified architecture signcryption cryptography data confidentiality authentication modular exponentiation gf p field;fpga;gf p field;data privacy;cryptography;field programmable gate arrays computer architecture public key cryptography elliptic curve cryptography microprogramming hardware authentication logic embedded system embedded computing;field programmable gate arrays authorisation cryptography data privacy;modular exponentiation;field programmable gate arrays;data confidentiality;high performance;signcryption	Signcryption is a cryptographic primitive supporting both confidentiality and authentication. This paper proposes a DPA-resistant unified architecture for signing, encryption and signcryption with high performance and area-efficiency. Modular exponentiation is the main operation of RSA and ECC and also the key part of implementing signcryption. A unified signed adder is proposed to address the possible method to unify the modular exponentiation on GF(p) field and GF(2p ) field. Our simulation results show that the overall speed (maximum frequency of 1024 key length for RSA and 160 key length for ECC) can be increased approximately 28% of the existing design when our proposed design ported to FPGA with the utilization of 4355 CLBs	adder (electronics);authentication;confidentiality;cryptographic primitive;cryptography;digital signature;ecc memory;encryption;field-programmable gate array;key size;modular exponentiation;rsa (cryptosystem);signcryption;simulation	Yi Wang;Jussipekka Leiwo;Thambipillai Srikanthan;Yu Yu	2006	Third International Conference on Information Technology: New Generations (ITNG'06)	10.1109/ITNG.2006.66	parallel computing;information privacy;computer science;theoretical computer science;signcryption;computer security;field-programmable gate array	EDA	8.951815826100816	44.760089220799806	137152
659b1ec8a60d1bf75f99a0ed81bed52e04d811d7	interactive synthesis of self-organizing tree models on the gpu	tree synthesis;68u20;gpu;tree growth model;massive parallelization;self organization;92c80;68u05;opencl;68w10	Real-time synthesis of realistic tree models is a desirable functionality for computer games, simulators, and landscape design software. Self-organizing tree models that adapt to the environment are a welcome addition and central to various 3D design tools but present a challenging task for interactive use even on modern commodity hardware. The paper describes the implementation of a complete self-organizing tree synthesis method running on a contemporary graphics processing unit using OpenCL. We demonstrate that generation and display of tree-populated scenes with shadows at interactive rates can be achieved by utilizing the massively parallel GPU architecture to accelerate the computationally intensive steps of the method. A comparison with the performance of single-threaded and CPU-based OpenCL implementation of the same method is reported.	algorithm;baseline (configuration management);central processing unit;commodity computing;computer graphics;data dependency;dynamic programming;graphics processing unit;landscape design software;load balancing (computing);network switch;noise shaping;opencl api;organizing (structure);pc game;parallel computing;population;ray casting;real-time transcription;relocation (computing);self-organization;shader;shadow volume;simulation;software propagation;thread (computing);throughput;tree (data structure);video card;z-buffering	Stefan Kohek;Damjan Strnad	2014	Computing	10.1007/s00607-014-0424-7	parallel computing;self-organization;computer science;theoretical computer science;computer graphics (images)	Graphics	-0.17418778241005473	44.414046519757754	137347
43106b12f77405152185377481a4e5af539f2c84	processor allocation in extended hypercube multiprocessor	multiprocessor;processor allocation;extended hypercube	Processor allocation is an important issue for efficient utilization of a multiprocessor system. Several well-known methods exist for processor allocation in the hypercube multiprocessor. Some disadvantages of the hypercube structure have been overcome in the Extended Hypercube. The paper attempts at showing how the processor allocation schemes of hypercube can be extended to the Extended Hypercube structure. Results of a simulation study are also provided.	multiprocessing	Sumeet Ahuja;Anil K. Sarje	1995	International Journal of High Speed Computing	10.1142/S0129053395000269	mathematical optimization;discrete mathematics;parallel computing;multiprocessing;computer science;theoretical computer science;operating system	Arch	0.5645003380676066	36.5056224132599	137357
7df45d6399e7549922e7ef5018813088d8598c07	a fast parallel matrix inversion algorithm based on heterogeneous multicore architectures	graphics processor geforce gt620 fast parallel matrix inversion algorithm heterogeneous multicore architectures graphics processor unit high performance computing gpu based software defined radio sdr computational power squared givens rotations sgr algorithm gpu architecture compute unified device architecture cuda cpu based only algorithm;graphics processing units signal processing algorithms multicore processing matrix decomposition mathematical model;matrix decomposition;graphics processing units;multicore processing;mathematical model;cuda matrix inversion high performance computing software defined radio gpu;parallel architectures graphics processing units matrix inversion multiprocessing systems parallel algorithms;signal processing algorithms	Large matrix inversion is usually a basic step in a wide range of signal processing or numerical problems, such as digital filtering, equalization detection, and etc. It is essential to figure out an algorithm to invert large matrix quickly and accurately. On the other hand, the Graphics Processor Unit (GPU) is able to provide a low-cost and flexible multicore architecture for high performance computing, which has attracted many researchers' attention for the building of GPU-based software-defined radio (SDR). In this paper, we propose a fast parallel algorithm for matrix inversion on heterogeneous multicore architectures to utilize the computational power of GPU. Our implementation is based on a modified Squared Givens Rotations (SGR) algorithm, which could adapt to the GPU architecture effectively. The result implemented on Compute Unified Device Architecture (CUDA) obtains a speedup ratio more than 20x versus the CPU-based-only algorithm when the matrix become large, and runs at up to 12.14 gigaflops/s on a graphics processor Geforce GT620 in our implementation.	cuda;central processing unit;digital filter;etsi satellite digital radio;flops;geforce;graphics processing unit;matrix multiplication;mean squared error;multi-core processor;numerical analysis;parallel algorithm;samsung sgr-a1;signal processing;speedup;supercomputer;the matrix;throughput	Denggao Yu;Shiwen He;Yongming Huang;Guangshi Yu;Luxi Yang	2015	2015 IEEE Global Conference on Signal and Information Processing (GlobalSIP)	10.1109/GlobalSIP.2015.7418328	computer architecture;parallel computing;computer science;theoretical computer science;general-purpose computing on graphics processing units	HPC	-1.8148234339338096	40.499350761783845	137392
2debc693da29974b47bf6bf2d1dde35a45af42de	scalable parallel interval propagation for sparse constraint satisfaction problems	sparse constraint satisfaction problem;implementation scale;parallel implementation;computational intensive algorithm;multi-core processor;constraint satisfaction problem;interval constraint propagation;scalable parallel interval propagation;cpu core;common design feature;non-linear constraint solvers	sparse constraint satisfaction problem;implementation scale;parallel implementation;computational intensive algorithm;multi-core processor;constraint satisfaction problem;interval constraint propagation;scalable parallel interval propagation;cpu core;common design feature;non-linear constraint solvers	constraint satisfaction;interval propagation;software propagation;sparse	Evgueni Petrov	2011		10.1007/978-3-642-29709-0_26	constraint programming;parallel computing;ac-3 algorithm;constraint satisfaction;constraint learning;computer science;theoretical computer science;constraint satisfaction dual problem;distributed computing;complexity of constraint satisfaction;constraint satisfaction problem;difference-map algorithm;hybrid algorithm;local consistency;backtracking	AI	-2.8276778065728356	39.07075426558717	137555
4c3f0c59387ae7a298d5f462539f6199604dc22e	direct solution of the (11, 9, 8)-minrank problem by the block wiedemann algorithm in magma with a tesla gpu		We show how some very large multivariate polynomial systems over finite fields can be solved by Gröbner basis techniques coupled with the Block Wiedemann algorithm, thus extending the Wiedemann-based 'Sparse FGLM' approach of Faugère and Mou. The main components of our approach are a dense variant of the Faugère F4 Gröbner basis algorithm and the Block Wiedemann algorithm, which have been implemented within the Magma Computer Algebra System (released in version V2.20 in late 2014). A major feature of the algorithms is that they map much of the computation to dense matrix multiplication, and this allows dramatic speedups to be achieved for large examples when an Nvidia Tesla GPU is available. As a result, the Magma implementation can directly solve a 16-bit random instance of the Courtois (11,9,8)-MinRank Challenge C in about 15.1 hours with a single Intel Sandybridge CPU core coupled with an Nvidia Tesla K40 GPU.	16-bit;block wiedemann algorithm;central processing unit;computation;computer algebra system;faugère's f4 and f5 algorithms;graphics processing unit;gröbner basis;magma;matrix multiplication;nvidia tesla;polynomial;sandy bridge;sparse matrix;tesla (microarchitecture)	Allan K. Steel	2015		10.1145/2790282.2791392	mathematical optimization;parallel computing;theoretical computer science;operating system;mathematics;algorithm	HPC	-2.6618973822131484	39.44398642135117	137716
ad46bcfcb8cd25945b1ae9eecb542fddefe061d6	an mpeg standard for rich media services	rich media;mobile multimedia;mpeg standards streaming media portals graphics mobile tv layout optical design wireless application protocol video compression packet radio networks;data compression;mpeg 4 systems;vector graphics;multimedia systems;video coding;scene description;mobile service;and binary encoding rich media mobile multimedia mobile services vector graphics scene description mpeg 4 systems svg;and binary encoding;svg;rich media data delivery mpeg standard rich media services lightweight application scene representation laser moving picture expert group mobile resource constrained devices;software standards;mobile services;mobile computing;software standards video coding multimedia systems mobile computing data compression	Lightweight Application Scene Representation (Laser) is the Moving Picture Expert Group's solution for delivering rich media services to mobile, resource-constrained devices. Laser provides easy content creation, optimized rich media data delivery, and enhanced rendering on all devices.	interactive media;mpeg-4 part 20;moving picture experts group	Jean-Claude Dufourd;Olivier Avaro;Cyril Concolato	2005	IEEE MultiMedia	10.1109/MMUL.2005.67	data compression;vector graphics;computer science;operating system;scalable vector graphics;multimedia;internet privacy;mobile computing;world wide web	Visualization	6.270825809001539	33.560732754420165	137851
6bb4660782de308409b523172fb077318a210245	robust watermark model based on subliminal channel	state space methods;security model;computational intelligence;security stochastic processes telecommunication control home automation multimedia systems state space methods accidents computational intelligence continuing education laboratories;telecommunication control;multimedia systems;continuing education;stochastic processes;accidents;security;telecommunication networks;home automation	Current watermark models cannot reflect the conflicting relationship among cover fidelity, watermark robustness and watermark capacity. And there is no effective guidance for designing robust watermark algorithms in content security applications, such as the copyright protection. A robust watermark model based on subliminal channel for content security applications is proposed. In this model, the half- symmetry of watermark communication is pointed out. Based on the model, the approaches to solve the conflicting relationship are presented as to increase entropy of cover, to decrease entropy of watermark message and to increase mutual information between cover and watermark through cover transformation, watermark encoding, public and subliminal channel encoding. The conditions and methods of the cover transformation and watermark encoding are presented. This model and its approaches will offer theory guidance for researches on robust watermark algorithms in content security applications.	algorithm;content security policy;digital watermarking;mutual information;subliminal channel	Cheng Yang;Jianbo Liu;Yaqing Niu	2007	2007 International Conference on Computational Intelligence and Security (CIS 2007)	10.1109/CIS.2007.63	computer security model;home automation;simulation;computer science;artificial intelligence;information security;computational intelligence;computer security	EDA	7.219436365740678	35.879592256383695	137895
85900729205a595c872b79ee33d024fb525cd10c	solving dense linear systems by gauss-huard's method on a distributed memory system	distributed memory;linear systems;gauss huard;universiteitsbibliotheek;linear system;gaussian elimination;parallel algorithms	Introduction In this paper we present a modification of Gauss-Huard's method for solving dense linear systems that allows an efficient implementation on machines with a hierarchical memory structure. GaussHuard's method resembles Gauss-Jordan's method in the fact that it reduces the given system by elementary transformations to a diagonal system and it resembles regular Gaussian elimination in the fact that it only uses 2 3 3 + O(n2) floating-point operations to calculate the solution, where GaussJordan's method uses n 3 + O(n2) floating-point operations to do so. The method of Gauss-Huard was introduced in 1979 in a version that not included a stabilizing pivoting strategy [6]. After the stability of Gauss-Jordan's method had been properly established in 1989 [1], a stabilizing pivoting strategy for Gauss-Huard's method could be introduced. In 1992 it was proven that Gauss-Huard's method in combination with this pivoting strategy is numerically stable [3]. Our new formulation of Gauss-Huard's method allows an implementation with high data locality; that is to say, this variant is very efficient on machines with a hierarchical memory structure where transport of data to processors may take non-negligible time. The possibility of a better data utilization permits a better performance in comparison with Gaussian elimination on this type of machines. On distributed memory systems with a fast local memory, the performance of our algorithm approaches the performance of Gaussian elimination for large matrices.	algorithm;central processing unit;distributed memory;gaussian elimination;gauss–seidel method;linear system;locality of reference;numerical stability	Walter Hoffmann;Kitty Potma;Gera Pronk	1994	Future Generation Comp. Syst.	10.1016/0167-739X(94)90037-X	mathematical optimization;gaussian elimination;parallel computing;computer science;theoretical computer science;distributed computing;linear system	HPC	-2.458314560358691	37.83585752229195	137961
5ca51ece58fb6d0dde1234ae859098521158006a	highly scalable web service composition using binary tree-based parallelization	parallelization techniques;data intensive applications web service composition binary tree based parallelization multiple parallel threads composition problem parallelization techniques multicore architectures sequential algorithms;data intensive application;multi threading;service composition;multiple parallel threads;pediatrics;multicore architectures;binary tree based parallelization;web service;trees mathematics;binary trees;web service composition;data intensive applications;sequential algorithms;web services instruction sets pediatrics algorithm design and analysis partitioning algorithms binary trees synchronization;parallelization web services service composition;composition problem;synchronization;web services;life sciences;parallelization;web services multiprocessing systems multi threading parallel algorithms trees mathematics;multiprocessing systems;algorithm design and analysis;partitioning algorithms;instruction sets;parallel algorithms;binary tree	Data intensive applications, e.g. in life sciences, pose new efficiency challenges to the service composition problem. Since today computing power is mainly increased by multiplication of CPU cores, algorithms have to be redesigned to benefit from this evolution. In this paper we present a framework for parallelizing service composition algorithms investigating how to partition the composition problem into multiple parallel threads. But in contrast to intuition, the straightforward parallelization techniques do not lead to superior performance as our baseline evaluation reveals. To harness the full power of multicore architectures, we propose two novel approaches to evenly distribute the workload in a sophisticated fashion. In fact, our extensive experiments on practical life science data resulted in an impressive speedup of over 300% using only 4 cores. Moreover, we show that our techniques can also benefit from all advanced pruning heuristics used in sequential algorithms.	automatic parallelization;baseline (configuration management);binary tree;central processing unit;experiment;heuristic (computer science);multi-core processor;parallel computing;sequential algorithm;service composability principle;speedup;web service	Patrick Hennig;Wolf-Tilo Balke	2010	2010 IEEE International Conference on Web Services	10.1109/ICWS.2010.45	web service;parallel computing;binary tree;computer science;theoretical computer science;operating system;distributed computing;programming language;law	HPC	-2.295663013900218	42.83145116778579	137987
8d5daa73aca1055b793ec7e29829d74806032fe6	the cognitive power meter: looking beyond the smart meter	power meters;energy conservation power meter smart meter load disaggregation cognitive analysis demand response;home appliances monitoring accuracy power measurement training clustering algorithms hidden markov models;load disaggregation cognitive power meter smart meter energy displays energy usage digital power meter;smart meters power meters;smart meters	The smart meter is often heralded as the key component supporting energy displays that can notify home occupants of their energy usage. But, a smart meter is only a digital power meter with enhanced communications capabilities - it is not actually smart. We need to look beyond the smart meter and define what intelligence is needed to actually make a meter smart. One area with promise is load disaggregation. Load disaggregation can be used to determine what loads contributing to the consumption reading at the smart meter. A smart meter incorporating load disaggregation intelligence can be seen as going beyond the traditional smart meter - what we call a cognitive power meter (c-meter). However, using load disaggregation, in its current form, is not feasible. We critically review the requirements for a c-meter and provide insights as to how load disaggregation research needs to change to make the c-meters a reality.	cognition;optical power meter;requirement;smart meter	Stephen Makonin;Fred Popowich;Bob Gill	2013	2013 26th IEEE Canadian Conference on Electrical and Computer Engineering (CCECE)	10.1109/CCECE.2013.6567686	embedded system;simulation;engineering;electrical engineering;smart grid;automatic meter reading	AI	1.2482356425097576	33.019060886228274	138033
b1b03242b1f3ea9926ed17dba5b7c62e63e1676a	optimization of data-intensive next generation sequencing in high performance computing	software;thread scalability;genomics;data intensive workload and concurrent parallelization next generation sequencing bwa high performance computing human genome sequence thread scalability;data intensive workload and concurrent parallelization;high performance computing;resource management;bioinformatics genomics resource management scalability sequential analysis next generation networking software;sequential analysis;bwa;human genome sequence;application scalability based workflows data intensive next generation sequencing optimization high performance computing genomic data empirical parallelism genetics personalized medicine bwakit based software gatk based software sidra cpu utilization optimal selection ngs workflow automation;optimisation biology computing genetics genomics;scalability;next generation sequencing;next generation networking;bioinformatics	"""Advancement in Next Generation Sequencing (NGS) technology are associated with ever-increasing volume of genomic data every year. These genomic data are efficiently processed by empirical parallelism using High Performance Computing (HPC). The processed data can be used for genome-wide association studies, genetics, personalized medicine and many other areas. There are different kind of algorithms and implementations used in different phases of genome processing. In this paper, we used BWAKIT and GATK based software for processing larger volume of genomic data that are referred as """"NGS workflow at SIDRA"""". We used BWAKIT for genome alignment and GATK for variant discovery in the NGS workflow that required larger computation and huge memory requirement respectively. We observed, the CPU utilization is not more than 45% during variant discovery and hence, it is necessary to understand the optimal selection (in terms of number of threads or cores) of the resources during the NGS workflow automation. We analyzed the performance bottleneck and application optimization in terms of """"scalability"""" (use maximum available CPUs and memory) and """"multiple instances of NGS workflow with different genome data within a node"""" (process more volume of genome data concurrently with limited set of CPUs and memory). We observed that, 40%, 65%, 71% and 76% improvement in performance while processing 2, 4, 8 and 16 samples concurrently using our own scheduling heuristics. As a result, our proposed NGS workflow automation will improve the performance upto 76% compared to application scalability based workflows."""	algorithm;automated planning and scheduling;central processing unit;communications satellite;computation;computer performance;data-intensive computing;heuristic (computer science);mathematical optimization;next-generation network;parallel computing;personalization;scalability;scheduling (computing);semiconductor intellectual property core;supercomputer	Nagarajan Kathiresan;Rashid Al-Ali;Puthen V. Jithesh;Tariq AbuZaid;Ramzi Temanni;Andrey Ptitsyn	2015	2015 IEEE 15th International Conference on Bioinformatics and Bioengineering (BIBE)	10.1109/BIBE.2015.7367654	dna sequencing;genomics;parallel computing;real-time computing;scalability;computer science;bioinformatics;resource management;sequential analysis	HPC	-0.6326734062446302	43.15751238244182	138054
916b293184a7a896b572255b0107f13e80fddbb6	high performance pseudo-analytical simulation of multi-object adaptive optics over multi-gpu systems	conference paper	Multi-object adaptive optics (MOAO) is a novel adaptive optics (AO) technique dedicated to the special case of wide-field multi-object spectrographs (MOS). It applies dedicated wavefront corrections to numerous independent tiny patches spread over a large field of view (FOV). The control of each deformable mirror (DM) is done individually using a tomographic reconstruction of the phase based on measurements from a number of wavefront sensors (WFS) pointing at natural and artificial guide stars in the field. The output of this study helps the design of a new instrument called MOSAIC, a multi-object spectrograph proposed for the European Extremely Large Telescope (E-ELT). We have developed a novel hybrid pseudo-analytical simulation scheme that allows us to accurately simulate in detail the tomographic problem. The main challenge resides in the computation of the tomographic reconstructor, which involves pseudo-inversion of a large dense symmetric matrix. The pseudo-inverse is computed using an eigenvalue decomposition, based on the divide and conquer algorithm, on multicore systems with multi-GPUs. Thanks to a new symmetric matrix-vector product (SYMV) multi-GPU kernel, our overall implementation scores significant speedups over standard numerical libraries on multicore, like Intel MKL, and up to 60% speedups over the standard MAGMA implementation on 8 Kepler K20c GPUs. At 40,000 unknowns, this appears to be the largest-scale tomographic AO matrix solver submitted to computation, to date, to our knowledge and opens new research directions for extreme scale AO simulations.		Ahmad Abdelfattah;Eric Gendron;Damien Gratadour;David E. Keyes;Hatem Ltaief;Arnaud Sevin;Fabrice Vidal	2014		10.1007/978-3-319-09873-9_59	simulation;computer science;computer graphics (images)	HPC	-3.304904727364319	39.99065865172268	138059
e04eaaa9a1b73b75a90429f1dff8a25e38037a4b	incomplete cholesky parallel preconditioners with selective inversion		"""Consider the solution of a large linear system when the coe cient matrix is sparse, symmetric, and positive de nite. One approach is the method of \conjugate gradient"""" (CG) with an incomplete Cholesky (IC) preconditioner (ICCG). A key problem with the design of a parallel ICCG solver is the bottleneck posed by repeated parallel sparse triangular solves to apply the preconditioner. Our work concerns the use of \selective inversion"""" techniques to replace parallel substitution schemes (for triangular solution) by e cient, scalable matrix-vector multiplication. We report on the performance of our parallel solver by summarizing results of experiments on both SMPs and message-passing multiprocessors."""	cholesky decomposition;conjugate gradient method;experiment;incomplete cholesky factorization;linear system;matrix multiplication;message passing;preconditioner;scalability;solver;sparse matrix	Esmond G. Ng;Padma Raghavan	1999			mathematical optimization;matrix (mathematics);cholesky decomposition;minimum degree algorithm;incomplete cholesky factorization;linear system;preconditioner;conjugate gradient method;solver;mathematics	HPC	-2.4892581252347643	38.24192147569085	138081
1d4e0b73507345a6c3d9ec55940d7d720c9a72c8	on-chip high-speed solver of inverse problems based on quantum-computing principle	microprocessors;field programmable gate array;one way function;concurrent computing;64 bit;multiprocessing;inverse problems quantum computing concurrent computing energy consumption field programmable gate arrays large scale integration frequency computer architecture quantum mechanics circuits;chip multiprocessor;3 4 ghz;chip;general purpose processor;computer architecture;large scale integration;inverse problem;quantum computer;quantum mechanics;energy consumption;discrete logarithm problem;circuits;multiprocessing systems;field programmable gate arrays;quantum computing;frequency;quantum computing field programmable gate arrays inverse problems microprocessor chips multiprocessing systems;high speed;64 bit chip multiprocessor inverse problems quantum computing field programmable gate array general purpose processor 3 4 ghz;microprocessor chips;inverse problems	A high-speed solver of an inverse problem to a oneway function becomes important although the problems become complicated according to the change in social situation. However, a chip multiprocessor dedicated to the inverse problem to the one-way function is nonexistent although a single-chip processor consuming limited power is useful in realizing a high-speed solver. In this paper, we propose a new chip multiprocessor operating with a procedure similar to that of quantum computing. The chip multiprocessor is implemented on a field programmable gate array (FPGA), and the factorization of a 64-bit integer is demonstrated. As a result, the proposed processor reduces the calculation time by 35% compared with a general-purpose processor with 3.4GHz clock frequency. As a result, it is shown that the proposed processor solves inverse problems such as factorization and discrete logarithm problems at higher speed than a general-purpose processor consuming limited power	64-bit computing;clock rate;discrete logarithm;field-programmable gate array;general-purpose modeling;multi-core processor;multiprocessing;one-way function;quantum computing;solver	M. Fujishima;M. Shimura	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1693170	embedded system;electronic engineering;parallel computing;concurrent computing;computer science;inverse problem;theoretical computer science;quantum computer;quantum mechanics;field-programmable gate array	Arch	8.335805806070695	44.29054014848896	138166
7f0eced24afe3d3d6fbf252a0afb19498c73e9cd	optimizations and heuristics to improve compression in columnar database systems		In-memory columnar databases have become mainstream over the last decade and have vastly improved the fast processing of large volumes of data through multi-core parallelism and in-memory compression thereby eliminating the usual bottlenecks associated with disk-based databases. For scenarios, where the data volume grows into terabytes and petabytes, keeping all the data in memory is exorbitantly expensive. Hence, the data is compressed efficiently using different algorithms to exploit the multi-core parallelization technologies for query processing. Several compression methods are studied for compressing the column array, post Dictionary Encoding. In this paper, we will present two novel optimizations in compression techniques Block Size Optimized Cluster Encoding and Block Size Optimized Indirect Encoding which perform better than their predecessors. In the end, we also propose heuristics to choose the best encoding amongst common compression schemes.	algorithm;block size (cryptography);column-oriented dbms;data dictionary;heuristic (computer science);in-memory database;multi-core processor;parallel computing;petabyte;terabyte	Jayanth Jayanth	2016	CoRR		parallel computing;computer science;theoretical computer science;data mining;database;move-to-front transform	DB	0.9106887835552046	39.04939284413172	138242
b820beeee333183bc6d416911c3483946027085a	sparse matrix-vector multiplication on the single-chip cloud computer many-core processor	power efficiency;performance;optimization;sparse matrix;many core	The microprocessor industry has responded to memory, power and ILP walls by turning to many-core processors, increasing parallelism as the primary method to improve processor performance. These processors are expected to consist of tens or even hundreds of cores. One of these future processors is the 48-core experimental processor Single-Chip Cloud Computer (SCC). The SCCwas created by Intel Labs as a platform for many-core software research. In this work we study the behavior of an important irregular application such as the Sparse Matrix–Vectormultiplication (SpMV) on the SCC processor in terms of performance and power efficiency. In addition, some of the most successful optimization techniques for this kernel are evaluated. In particular, reordering, blocking and data compression techniques have been considered. Our experiments give some key insights that can serve as guidelines for the understanding and optimization of the SpMV kernel on this architecture. Furthermore, an architectural comparison of the SCC processor with several leading multicore processors and GPUs is performed, including the new Intel Xeon Phi coprocessor. The SCC only outperforms the Itanium2 multicore processor. Best performance results are observed for the high-end GPUs and the Phi, while reaching low values with respect to their peak performance. In terms of power efficiency, we must highlight the good behavior of the ATI GPUs. © 2013 Elsevier Inc. All rights reserved.	amd firepro;algorithm;blocking (computing);cpu cache;central processing unit;clock rate;cloud computing;computation;coprocessor;data compression;experiment;fits;graphics processing unit;itanium;list of amd graphics processing units;locality of reference;manycore processor;map;mathematical optimization;matrix multiplication;mesh networking;microprocessor;multi-core processor;parallel computing;performance per watt;single-chip cloud computer;sparse matrix;testbed;working set size;xeon phi	Juan Carlos Pichel;Francisco F. Rivera	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.07.017	computer architecture;parallel computing;electrical efficiency;single-chip cloud computer;sparse matrix;computer hardware;performance;computer science;operating system;distributed computing	Arch	-3.5336578245802532	42.045981198942414	138290
bb1951ce16f2780be13d32dd2a0b9353a95645a9	multiplexer-based double-exponentiation for normal basis of gf(2m)	multiplier;time complexity;cryptographic protocol;exponentiation;normal basis;finite galois field arithmetic;cryptography;galois field	In many cryptographic protocols, double-exponentiation is a key arithmetic operation. In this study, we will present a multiplexer-based algorithm for double-exponentiation in GF(2^m). The proposed algorithm utilizes the concept of the modified Booth's algorithm. Multiplexers are employed for implementation of the proposed algorithm. The proposed double-exponentiation algorithm only requires m multiplications and saves about 66% time complexity while comparing with the ordinary binary method.	multiplexer;normal basis	Che Wun Chiou;Chiou-Yng Lee	2005	Computers & Security	10.1016/j.cose.2004.09.012	time complexity;normal basis;computer science;cryptography;cryptographic protocol;multiplier;exponentiation;gf(2)	Crypto	9.74508122785872	43.45935940395958	138368
ddaddfd8975b64b8084a7e15c4c82eb0c7be341a	a case study of machine learning hardware: real-time source separation using markov random fields via sampling-based inference		We explore sound source separation to isolate human voice from background noise on mobile phones, e.g. talking on your cell phone in an airport. The challenges involved are real-time execution and power constraints. As a solution, we present a novel hardware-based sound source separation implementation capable of real-time streaming performance. The implementation uses a recently introduced Markov Random Field (MRF) inference formulation of foreground/background separation, and targets voice separation on mobile phones with two microphones. We demonstrate a real-time streaming FPGA implementation running at 150 MHz with total of 207 KB RAM. Our implementation achieves a speedup of 20× over a conventional software implementation, achieves an SDR of 6.655 dB with 1.601 ms latency, and exhibits excellent perceived audio quality. A virtual ASIC design shows that this architecture is quite small (less than 10M gates), consumes only 69.977 mW running at 20 MHz (52× less than an ARM Cortex-A9 software reference), and appears amenable to additional optimization for power.	arm cortex-a9;arm architecture;application-specific integrated circuit;covox speech thing;etsi satellite digital radio;field-programmable gate array;machine learning;markov chain;markov random field;mathematical optimization;microphone;mobile phone;random-access memory;real-time clock;real-time transcription;sampling (signal processing);source separation;speedup	Glenn G. Ko;Rob A. Rutenbar	2017	2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2017.7952602	computer science;slice sampling;theoretical computer science;machine learning;pattern recognition;markov model;variable-order markov model	Mobile	1.504354024571736	45.41578881391834	138668
ba58e1e009bfa83141b62948a5c67107d3e04186	minimizing area and energy of deep learning hardware design using collective low precision and structured compression		Deep learning algorithms have shown tremendous success in many recognition tasks; however, these algorithms typically include a deep neural network (DNN) structure and a large number of parameters, which makes it challenging to implement them on power/area-constrained embedded platforms. To reduce the network size, several studies investigated compression by introducing element-wise or row-/column-/block-wise sparsity via pruning and regularization. In addition, many recent works have focused on reducing precision of activations and weights with some reducing down to a single bit. However, combining various sparsity structures with binarized or very-low-precision (2–3 bit) neural networks have not been comprehensively explored. In this work, we present design techniques for minimum-area/-energy DNN hardware with minimal degradation in accuracy. During training, both binarization/low-precision and structured sparsity are applied as constraints to find the smallest memory footprint for a given deep learning algorithm. The DNN model for CIFAR-10 dataset with weight memory reduction of 50X exhibits accuracy comparable to that of the floating-point counterpart. Area, performance and energy results of DNN hardware in 40nm CMOS are reported for the MNIST dataset. The optimized DNN that combines 8X structured compression and 3-bit weight precision showed 98.4% accuracy at 20nJ per classification.	algorithm;artificial neural network;cmos;deep learning;elegant degradation;embedded system;mnist database;machine learning;matrix regularization;memory footprint;sparse matrix	Shihui Yin;Gaurav Srivastava;Shreyas K. Venkataramanaiah;Chaitali Chakrabarti;Visar Berisha;Jae-sun Seo	2017	2017 51st Asilomar Conference on Signals, Systems, and Computers	10.1109/ACSSC.2017.8335696	machine learning;memory footprint;computer hardware;artificial intelligence;deep learning;compression (physics);sparse matrix;artificial neural network;computer science;mnist database;cmos	ML	4.012108277270477	42.560734488629464	138734
bb28bf9353db09e5431e308cf05cb1d56ffb3430	parallel computer architectures for image processing	computers;processing;general and miscellaneous mathematics computing and information science;image processing;computer model;parallel computer architecture;parallel systems;data flow processing;programming 990200 mathematics computers;image analysis;data flow;architecture;parallel processing	Image processing problems frequently involve large structured arrays of data and a need for very rapid computation. Special parallel processing schemes have evolved over the last 20 years to deal with these problems. In this paper many parallel systems which have been developed for image processing are outlined and the features of their underlying architectures are discussed. Most of these special architectures may be loosely classified as either SIMD or pipeline structures although some MIMD structures have been designed for high level image analysis. In recent years several multiple SIMD (MSIMD) schemes have been proposed as suitable architectures for image processing. The fundamental problems of developing an effective MSIMD system are discussed and a simple SIMD/MIMD computational model for comparison with such systems is proposed. 62 references.	computer architecture;image processing	Anthony P. Reeves	1984	Computer Vision, Graphics, and Image Processing	10.1016/0734-189X(84)90049-5	parallel processing;computer vision;computer architecture;parallel computing;image analysis;image processing;computer science;processing;theoretical computer science;architecture	Vision	7.682686562949596	38.416566850004564	139164
0c6f81e60514edbc6a936a5f8593838f14658653	parallel automata processor		Finite State Machines (FSM) are widely used computation models for many application domains. These embarrassingly sequential applications with irregular memory access patterns perform poorly on conventional von-Neumann architectures. The Micron Automata Processor (AP) is an in-situ memory-based computational architecture that accelerates non-deterministic finite automata (NFA) processing in hardware. However, each FSM on the AP is processed sequentially, limiting potential speedups.  In this paper, we explore the FSM parallelization problem in the context of the AP. Extending classical parallelization techniques to NFAs executing on AP is non-trivial because of high state-transition tracking overheads and exponential computation complexity. We present the associated challenges and propose solutions that leverage both the unique properties of the NFAs (connected components, input symbol ranges, convergence, common parent states) and unique features in the AP (support for simultaneous transitions, low-overhead flow switching, state vector cache) to realize parallel NFA execution on the AP.  We evaluate our techniques against several important benchmarks including NFAs used for network intrusion detection, malware detection, text processing, protein motif searching, DNA sequencing, and data analytics. Our proposed parallelization scheme demonstrates significant speedup (25.5x on average) compared to sequential execution on AP. Prior work has already shown that sequential execution on AP is at least an order of magnitude better than GPUs, multi-core processors and Xeon Phi accelerator.	alphabet (formal languages);amortized analysis;automata theory;baseline (configuration management);benchmark (computing);central processing unit;computation;computational complexity theory;crossbar switch;dataflow;deterministic finite automaton;electrical connection;field-programmable gate array;finite-state machine;graphics processing unit;intrusion detection system;malware;memristor;motif;multi-core processor;nondeterministic finite automaton;overhead (computing);parallel computing;partition (database);speedup;time complexity;xeon phi	Arun Subramaniyan;Reetuparna Das	2017	2017 ACM/IEEE 44th Annual International Symposium on Computer Architecture (ISCA)	10.1145/3079856.3080207	architecture;real-time computing;cache;parallel computing;finite-state machine;xeon phi;computational complexity theory;speedup;computer science;intrusion detection system;convergence (routing)	Arch	1.0665035041386	43.18460200494962	139511
7b89e128e7ca0896a0ef78c5a03e858488ca8a81	evaluating optimized implementations of stream cipher zuc algorithm on fpga	hardware evaluation;zuc;fpga;optimization	Compared with block ciphers, stream ciphers are more efficient when implemented in hardware environment, like Field Programma-ble Gate Array (FPGA). In this paper, we propose three optimized schemes in the FPGA implementation of a novel and recently proposed stream cipher, ZUC, which is a new cryptographic algorithm proposed for inclusion in the '4G' mobile standard called LTE (Long Term Evolution). These three schemes are based on reusing area of S-box, calculation of CSA tree and pipelined architecture to implement ZUC on FPGA respectively. We also evaluate each optimized scheme in terms of performance and consumed area in Xilinx FPGA device to compare their actual hardware efficiency. According to the evaluation results, the third scheme, namely pipelined architecture implementation, optimizes hardware implementation of ZUC for the best performance and achieves a throughput of 7.1 Gbps using only 575 slices by speeding up the keystream generating on FPGA. To our knowledge, it is an extremely efficient hardware implementation of ZUC at present. Moreover, it also shows that ZUC is quite flexible to balance different throughput with consumed area.	algorithm;field-programmable gate array;stream cipher	Lei Wang;Jiwu Jing;Zongbin Liu;Lingchen Zhang;Wuqiong Pan	2011		10.1007/978-3-642-25243-3_17	parallel computing;real-time computing;computer science;theoretical computer science;field-programmable gate array	EDA	9.17988369287326	45.28769364634232	139547
2b9f2a64f537dc22902aad5c241ac0ca572624dd	overlapped parallel computations of scalar multiplication with resistance against side channel attacks	parallel computing;crytpography;information security;spa attacks;computer security;scalar multiplication pipelining;simple power analysis;dpa attacks;differential power analysis;parallel computer;side channel attacks;eccs;sca;side channel atomicity;scalar multiplication;elliptic curve cryptosystems;information and computer security	This paper presents an efficient scheme for computing elliptic curve scalar multiplication. The proposed scheme uses side-channel atomicity to resist against simple power analysis (SPA) attacks. The inherent parallelism within point operations is exploited to perform parallel computations of atomic blocks within the same point operation. The computations of atomic blocks of subsequent point operations are then overlapped to increase the performance. The proposed scheme reduces the required memory location requirement by 35% in comparison to the pipelined scheme of Mishra (2006). In addition, the time complexity of the proposed scheme is improved by 17% and 21% in comparison to the pipelined scheme using binary and NAF encoding, respectively. Differential power analysis (DPA) attacks are also considered in the proposed scheme. Two countermeasures, randomizing the scalar multiplier and randomizing the projective coordinates, are applied together to immunize the scheme against DPA attacks. The time complexity of the proposed scheme with resistance against DPA attacks is improved by 6% and 13% in comparison to the pipelined scheme using binary and NAF encoding, respectively. The proposed scheme is also applied to the Comb scalar multiplication algorithm. The results show that the proposed scheme is highly efficient in comparison to the pipelined scheme, which outperformed previous sequential and parallel schemes.	atomicity (database systems);binary file;bitwise operation;computation;memory address;multiplication algorithm;nato architecture framework;parallel computing;pipeline (computing);side-channel attack;time complexity	Turki F. Al-Somani	2008	IJICS	10.1504/IJICS.2008.020605	parallel computing;power analysis;computer science;information security;theoretical computer science;side channel attack;scalar multiplication;computer security	Arch	9.134323348393231	43.29181104232667	139617
6e2560e194b1426af9a0b38d0c7426e4f2cc5f1a	hcw 2013 keynote talk	matrix algebra;linear algebra algorithms high performance computing matrix algebra gpu multicore architectures magma hybridization methodology;educational institutions awards activities high performance computing lifting equipment laboratories graphics processing units multicore processing;graphics processing units;multiprocessing systems;parallel processing;parallel processing graphics processing units matrix algebra multiprocessing systems	The talk highlights the emerging technologies in high performance computing. We look at the development of accelerators and some of the accomplishments in the Matrix Algebra on GPU and Multicore Architectures (MAGMA) project. We use a hybridization methodology that is built on representing linear algebra algorithms as collections of tasks and data dependencies, as well as properly scheduling the tasks' execution over the available multicore and GPU hardware components.	algorithm;data dependency;graphics processing unit;linear algebra;magma;multi-core processor;scheduling (computing);supercomputer;the matrix	Jack J. Dongarra	2013		10.1109/IPDPSW.2013.281	computer architecture;parallel computing;computer science;theoretical computer science	HPC	-2.905283832974669	44.58306522169736	139781
d6c4c76076efecb15655274adc648af8a445ed3a	csr5: an efficient storage format for cross-platform sparse matrix-vector multiplication	spmv;benchmarking;csr5;paper;performance;storage formats;amd radeon r9 290x;nvidia geforce gtx 980;gpu;cuda;xeon phi;package;nvidia;algorithms;csr;mathematical software;computer science;sparse matrix;cpu;opencl;intel xeon phi;sparse matrices	Sparse matrix-vector multiplication (SpMV) is a fundamental building block for numerous applications. In this paper, we propose CSR5 (Compressed Sparse Row 5), a new storage format, which offers high-throughput SpMV on various platforms including CPUs, GPUs and Xeon Phi. First, the CSR5 format is insensitive to the sparsity structure of the input matrix. Thus the single format can support an SpMV algorithm that is efficient both for regular matrices and for irregular matrices. Furthermore, we show that the overhead of the format conversion from the CSR to the CSR5 can be as low as the cost of a few SpMV operations.  We compare the CSR5-based SpMV algorithm with 11 state-of-the-art formats and algorithms on four mainstream processors using 14 regular and 10 irregular matrices as a benchmark suite. For the 14 regular matrices in the suite, we achieve comparable or better performance over the previous work. For the 10 irregular matrices, the CSR5 obtains average performance improvement of 17.6%, 28.5%, 173.0% and 293.3% (up to 213.3%, 153.6%, 405.1% and 943.3%) over the best existing work on dual-socket Intel CPUs, an nVidia GPU, an AMD GPU and an Intel Xeon Phi, respectively. For real-world applications such as a solver with only tens of iterations, the CSR5 format can be more practical because of its low-overhead for format conversion.	algorithm;benchmark (computing);best, worst and average case;central processing unit;graphics processing unit;high-throughput computing;irregular matrix;iteration;matrix multiplication;overhead (computing);solver;sparse matrix;throughput;xeon phi	Weifeng Liu;Brian Vinter	2015		10.1145/2751205.2751209	parallel computing;sparse matrix;computer hardware;computer science;operating system;mathematics;xeon phi;algebra	HPC	-2.5485804429260916	40.20504643583288	139783
b674ef775944cb1564e500174044784286a0a278	parfes: a method for solving finite element linear equations on multi-core computers	linear algebra;finite element;load balancing;load balance;finite element analysis;linear equations;factoring;high performance;sparse matrices;block matrices;data transfer;multithreading	The method suggested here is intended for solving sets of linear algebraic equations with symmetric sparse matrices. It is oriented at the usage in finite element analysis software operated on multi-core desktop computers. Algorithms of parallelization have been implemented which speed up the method intensively as more processors are added, both in the core memory mode and when using the hard disk storage. PARFES features a higher performance and speedup ability than the multi-frontal method do, because it requires a minimum amount of data transfers - actions that speed up poorly by parallelization available on multi-core desktop computers.	computer;finite element method;linear equation;multi-core processor	S. Fialko	2010	Advances in Engineering Software	10.1016/j.advengsoft.2010.09.002	mathematical optimization;parallel computing;computer science;load balancing;theoretical computer science;linear algebra;finite element method;mathematics;distributed computing;numerical linear algebra;mixed finite element method;algebra	HPC	-3.4846561731272776	38.339894825472605	140005
c710ff47f94aebfb645949dde0c514c05f712f6c	low-power wireless accelerometer-based system for wear detection of bandsaw blades	iwsn;bandsaw blades;power consumption low power wireless accelerometer based system wear detection bandsaw blades low cost accelerometer based system data processing low power design on board processing wake up radio capabilities wireless communication;vibrations;wear;sensors;low power wireless solution;data processing;wsn;wireless sensor networks accelerometers blades flaw detection low power electronics wear;low power wireless solution wsn iwsn data processing wear detection;on board processing;wireless communication;wear detection;monitoring;flaw detection;low power electronics;wake up radio capabilities;low power wireless accelerometer based system;blades;low power design;power consumption;wireless sensor networks sensors blades wireless communication monitoring power demand vibrations;accelerometers;power demand;wireless sensor networks;low cost accelerometer based system	The paper provides a framework to save energy and reduce the operative cost of some of today's industrial machinery. Low cost and low power wireless sensor networks is a novel approach to monitoring the tools in order to save energy and keep the tools monitored. Cutting tool wear degrades the product quality in manufacturing processes and also could have implications in health and safety of use. Monitoring tool wear value online is therefore needed to prevent degradation in machine quality. Unfortunately there is no direct way of measuring the tool wear online which is also very low cost. In this work is presented a low power and low cost accelerometer-based system for wear detection of bandsaw blade. The algorithm uses a simple data processing directly on board that can extract features and perform a classification on the state of the blade. Low power design of the node, on board processing and wake up radio capabilities reduce the wireless communication and the power consumption of the node significantly. Experimental results show the high accuracy, up to 100%, of the algorithm and the low power of the proposed approach.	algorithm;elegant degradation;refinement (computing);sensor;software deployment	Michele Magno;Emanuel M. Popovici;Alberto Bravin;Antonio Libri;Marco Storace;Luca Benini	2013	2013 11th IEEE International Conference on Industrial Informatics (INDIN)	10.1109/INDIN.2013.6622957	embedded system;electronic engineering;real-time computing;engineering	Robotics	3.3332601418455394	33.60986249351744	140024
49f8e15ae6930a2368cf2edc44d62d41db8db829	two parallel implementations for one dimension fft on symmetric multiprocessors	imbalance load;time complexity;fft;fast fourier transform;isoefficeicy function;bit reversal;mpi;smp;parallel implementation;scalability;super linear speedup;dft	In this paper, an empirical comparison is made between two parallel implementations of a one-dimensional Fast Fourier transform (FFT) that is targeted for a symmetric multiprocessor (SMP). The paper compares the run time characteristics and overhead (time complexity) associated with the two algorithms with that of previous research. The scalability of the two algorithms is also accessed using the isoefficiency function and the effect of caches on performance is presented. The isoefficiency function is defined as the rate at which the data should be increased with the number of processors to maintain constant efficiency. The two implementations are based on a tree and transpose, respectively. In the tree algorithm, the speedup does not increase linearly with the number of processors, but rather super linear speedup can be achieved for the two processor case. The transpose algorithm obtained (approximately) linearly speedup with respect to the number of processors with only moderate increase in the data size. Additional performance can be obtained by overlapping computation with communication and by efficient use of caches.	algorithm;cpu cache;central processing unit;computation;fast fourier transform;list of algorithms;overhead (computing);run time (program lifecycle phase);scalability;speedup;symmetric multiprocessing;time complexity	Rami A. AL-Na'mneh;Weiqiong Pan;B. Earl Wells	2004		10.1145/986537.986602	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-1.6143551123160382	39.122498919401366	140029
76a729d54cd4cbf5012af4344057cd29826693ad	applying randomized edge coloring algorithms to distributed communication: an experimental study	edge coloring;multimedia information system;parallel i o;data transfer	We propose a parameterized, randomized edge coloring algorithm for use in coordinating data transfers in fully connected distributed architectures such as parallel 1/0 subsystems and multimedia information systems. Our approach is to preschedule 1/0 requests to eliminate contention for 1/0 ports while maintaining an efficient use of bandwidth. Request scheduling is equivalent to edge coloring a bipartite graph representing pending 1/0 requests. Although efficient optimal algorithms exist for centralized edge coloring where the global request graph is known, in distributed architectures heuristics must be used. We propose such heuristics and use experimental analysis to determine their ability to approach the centralized optimal. The performance of our algorithms is also compared with the work of other researchers experimentally. Our results show that our algorithms produce schedules within 5% of the optimal schedule, a substantial improvement over existing algorithms. The use of experimental analysis allows us to evaluate the appropriateness of each heuristic for a variety of different architectural models and applications.	centralized computing;division by zero;edge coloring;experiment;graph coloring;heuristic (computer science);information system;norm (social);randomized algorithm;schedule (computer science);scheduling (computing)	Dannie Durand;Ravi Jain;David Tseytlin	1995		10.1145/215399.215456	combinatorics;computer science;theoretical computer science;edge coloring;distributed computing	DB	8.020663521802314	32.568329730109866	140345
7ec192b3c2414dd5dd30c5c37e0580b7405a9e95	impostor detection using facial stereoscopic images	biometrics (access control);computer crime;cryptography;face recognition;mesh generation;stereo image processing;biometric systems;complex attack;cryptographic attacks;facial stereoscopic images;high security applications;impostor attacks;impostor detection;password stealing;prerecorded image;replay attacks;simple attacks;transmission channel;triangulation	Biometric systems are prone to impostor attacks, which in case of high security applications could be critical. These attacks range from simple attacks consisting of stealing someone's password or complex as cryptographic attacks on the transmission channel. In this study we have focused on combating replay attacks, in which an impostor uses a prerecorded image of the client. The system captures stereoscopic images of the face and then decides if the images belong to a real face or a poster from depth calculated using triangulation.	biometrics;channel (communications);cryptography;dictionary attack;password;replay attack;stereoscopy;triangulation (geometry)	Abdelaali Benaiss;Usman Saeed;Jean-Luc Dugelay;Mohamed Jedra	2009	2009 17th European Signal Processing Conference		computer vision;computer science;internet privacy;computer security	Security	7.497544480310477	33.89519425581205	140414
075930c9520cee0a41c286f01de150e2e5d534a4	a comparison of floating point and logarithmic number systems for fpgas	logarithmic number system;engineering;digital signal processing chips floating point arithmetic field programmable gate arrays;semiconductor devices;format conversion floating point library logarithmic number system fpga multiplication computation division computation exponentiation computation addition computation subtraction computation lns library;comparative evaluations;calculation methods;digital signal processing chips;floating point;field programmable gate arrays hardware digital signal processing dynamic range costs software libraries signal processing algorithms laboratories us department of energy national security;microelectronics;floating point arithmetic;field programmable gate arrays	There have been many papers proposing the use of logarithmic numbers (LNS) as an alternative to floating point because of simpler multiplication, division and exponentiation computations. However, this advantage comes at the cost of complicated, inexact addition and subtraction, as well as the need to convert between the formats. In this work, we created a parameterized LNS library of computational units and compared them to an existing floating point library. Specifically, we considered multiplication, division, addition, subtraction, and format conversion to determine when one format should be used over the other and when it is advantageous to change formats during a calculation.	computation;denormal number;field-programmable gate array;logarithmic number system	Michael Haselman;Michael J. Beauchamp;Aaron Wood;Scott Hauck;Keith D. Underwood;Karl S. Hemmert	2005	13th Annual IEEE Symposium on Field-Programmable Custom Computing Machines (FCCM'05)	10.1109/FCCM.2005.6	floating-point unit;double-precision floating-point format;parallel computing;computer hardware;computer science;floating point;logarithmic number system;theoretical computer science;operating system	Logic	7.803355468426512	43.2768565336996	140814
c3c5d416a5e96046484a49b4d5bf011e0e03c456	a high-performance connected components implementation for gpus		Computing connected components is an important graph algorithm that is used, for example, in medicine, image processing, and biochemistry. This paper presents a fast connected-components implementation for GPUs called ECL-CC. It builds upon the best features of prior algorithms and augments them with GPU-specific optimizations. For example, it incorporates a parallelism-friendly version of pointer jumping to speed up union-find operations and uses several compute kernels to exploit the multiple levels of hardware parallelism. The resulting CUDA code is asynchronous and lock free, employs load balancing, visits each edge exactly once, and only processes edges in one direction. It is 1.8 times faster on average than the fastest prior GPU implementation running on a Titan X and faster on most of the eighteen real-world and synthetic graphs we tested.	cuda;compute kernel;connected component (graph theory);disjoint-set data structure;emitter-coupled logic;fastest;graphics processing unit;image processing;list of algorithms;load balancing (computing);non-blocking algorithm;parallel computing;pointer (computer programming);pointer jumping;synthetic intelligence;titan rain	Jayadharini Jaiganesh;Martin Burtscher	2018		10.1145/3208040.3208041	non-blocking algorithm;image processing;distributed computing;speedup;asynchronous communication;computer science;cuda;connected component;pointer jumping;load balancing (computing)	HPC	-3.4112891325892107	42.04284748521785	140922
ee7919e42245d85c9a85b2c34a2adbfe232d65c1	new block encryption algorithm misty	provable security;design principle;block cipher;data stream;ingenieria logiciel;software engineering;algorithme;algorithm;linear cryptanalysis;criptografia;cryptography;genie logiciel;cryptographie;high speed;software implementation;algoritmo	We propose secret-key cryptosystems MISTY1 andMISTY2, which are block ciphers with a 128-bit key, a 64-bit block and a variable number of rounds. MISTY is a generic name for MISTY1 and MISTY2. They are designed on the basis of the theory of provable security against di erential and linear cryptanalysis, and moreover they realize high speed encryption on hardware platforms as well as on software environments. Our software implementation shows that MISTY1 with eight rounds can encrypt a data stream in CBCmode at a speed of 20Mbps and 40Mbps on Pentium/100MHz and PA-7200/120MHz, respectively. For its hardware performance, we have produced a prototype LSI by a process of 0.5 CMOS gate-array and con rmed a speed of 450Mbps. In this paper, we describe the detailed speci cations and design principles of MISTY1 and MISTY2. 1 Fundamental Design Policies of MISTY Our purpose of designing MISTY is to o er secret-key cryptosystems that are applicable to various practical systems as widely as possible; for example, software stored in IC cards and hardware used in fast ATM networks. To realize this, we began its design with the following three fundamental policies: 1. MISTY should have a numerical basis for its security, 2. MISTY should be reasonably fast in software on any processor, 3. MISTY should be su ciently fast in hardware implementation. For the rst policy, we have adopted the theory of provable security against di erential and linear cryptanalysis [1][2][4], which was originally introduced by Kaisa Nyberg and Lars Knudsen. As far as we know, MISTY is the rst block encryption algorithm designed for practical use with provable security against di erential and linear cryptanalysis. Although this advantage does not mean information theoretic provable security, we believe that it is a good starting point for discussing secure block ciphers. Secondly, we have noticed the fact that many recent block ciphers were designed so that they could be fastest and/or smallest on speci c targets; for example, 32-bit microprocessors. This often results in slow and/or big implementation on other types of processors. Since we regarded seeking applicability to various systems as more important than pursuing maximum performance on	128-bit;32-bit;64-bit computing;atm turbo;algorithm;block cipher;cmos;central processing unit;cryptosystem;fast software encryption;fastest;gate array;integrated circuit;journal of cryptology;kaisa nyberg;key (cryptography);lars bak (computer programmer);lecture notes in computer science;linear approximation;linear cryptanalysis;microprocessor;naruto shippuden: clash of ninja revolution 3;numerical analysis;prototype;provable security;public-key cryptography;springer (tank);theory	Mitsuru Matsui	1997		10.1007/BFb0052334	block cipher;parallel computing;encryption software;computer science;cryptography;theoretical computer science;operating system;provable security;computer security;algorithm;linear cryptanalysis	Crypto	7.9706327895282465	44.08446587303054	140927
05154c1ea92403c7db5c6cd3300003ec9f724c36	power-time flexible architecture for gf(2k) elliptic curve cryptosystem computation	processor architecture;math;systems;elliptic curve;computer;elliptic curve cryptography;projective coordinate arithmetic;electrical;crypto systems power time tradeoff;parallel architecture;power consumption;elliptic curve cryptosystem	New elliptic curve cryptographic processor architecture is presented that result in considerable reduction in power consumption as well as giving a range of trade-off between speed and power consumption. This is achieved by exploiting the inherent parallelism that exist in elliptic curve point addition and doubling. Further trade-off is achieved by using digit serial-parallel multipliers instead of the serial-serial multipliers used in conventional architectures. In effect, the new architecture exploits parallelism at the algorithm level as well as at the arithmetic element level. This parallelism can be exploited either to increase the speed of operation or to reduce power consumption by reducing the frequency of operation and hence the supply voltage.	algorithm;computation;cryptographic accelerator;cryptosystem;elliptic curve cryptography;parallel computing;period-doubling bifurcation;serial ata	Adnan Abdul-Aziz Gutub;Mohammad K. Ibrahim	2003		10.1145/764808.764870	arithmetic;parallel computing;jacobian curve;microarchitecture;elliptic curve digital signature algorithm;tripling-oriented doche–icart–kohel curve;computer science;theoretical computer science;curve25519;system;mathematics;elliptic curve cryptography;elliptic curve;elliptic curve point multiplication;algorithm	Arch	9.456788480683006	44.13499801573926	141155
6d8a8819dea04c65b0badd2bc9a753dc21f31798	securing speech noise reduction in outsourced environment		Cloud data centers (CDCs) are becoming a cost-effective method for processing and storage of multimedia data including images, video, and audio. Since CDCs are physically located in different jurisdictions, and are managed by external parties, data security is a growing concern. Data encryption at CDCs is commonly practiced to improve data security. However, to process the data at CDCs, data must often be decrypted, which raises issues in security. Thus, there is a growing demand for data processing techniques in encrypted domain in such an outsourced environment. In this article, we analyze encrypted domain speech content processing techniques for noise reduction. Noise contaminates speech during transmission or during the acquisition process by recording. As a result, the quality of the speech content is degraded. We apply Shamir’s secret sharing as the cryptosystem to encrypt speech data before uploading it to a CDC. We then propose finite impulse response digital filters to reduce white and wind noise in the speech in the encrypted domain. We prove that our proposed schemes meet the security requirements of efficiency, accuracy, and checkability for both semi-honest and malicious adversarial models. Experimental results show that our proposed filtering techniques for speech noise reduction in the encrypted domain produce similar results when compared to plaintext domain processing.	cryptosystem;data center;data security;digital filter;effective method;encryption;finite impulse response;noise reduction;plaintext;requirement;secret sharing;semiconductor industry;upload;white noise	M. Abukari Yakubu;Namunu Chinthaka Maddage;Pradeep K. Atrey	2017	TOMCCAP	10.1145/3105970	filter (signal processing);computer network;encryption;cloud computing;cryptosystem;noise reduction;data security;computer science;secret sharing;plaintext	Security	8.12294662560155	35.43773509761021	141353
1fc09d73dabeaadd2650b80e85a5571a05b85dc2	efficient characteristic 3 galois field operations for elliptic curve cryptographic applications	elliptic curves;polynomials;elliptic curve cryptography;software algorithms;finite element analysis;galois fields polynomials algorithm design and analysis elliptic curves elliptic curve cryptography software algorithms finite element analysis;performance optimization public key cryptography elliptic curves characteristic 3 galois field theory;algorithm design and analysis;galois fields	Galois fields of characteristic 3, where the number of field elements is a power of 3, have a distinctive application in building high-security elliptic curve cryptosystems. However, they are not typically used because of their relative inefficiency in computing polynomial operations when compared to conventional prime or binary Galois fields. The purpose of this research was to design and implement characteristic 3 Galois field arithmetic algorithms with greater overall efficiency than those presented in current literature, and to evaluate their applicability to elliptic curve cryptography. The algorithms designed were tested in a C++ program and using a mapping of field element logarithms, were able to simplify the operations of polynomial multiplication, division, cubing, and modular reduction to that of basic integer operations. They thus significantly outperformed the best characteristic 3 algorithms presented in literature and showed a distinct applicability to elliptic curve cryptosystems. In conclusion, this research presents a novel method of optimizing the performance of characteristic 3 Galois fields and has major implications for the field of elliptic curve cryptography.	algorithm;balanced ternary;c++;cryptosystem;ecc memory;elliptic curve cryptography;polynomial ring;ternary numeral system	Vinay S. Iyengar	2013	2013 International Conference on Security and Cryptography (SECRYPT)		algorithm design;supersingular elliptic curve;normal basis;jacobian curve;twists of curves;lenstra elliptic curve factorization;elliptic curve digital signature algorithm;tripling-oriented doche–icart–kohel curve;schoof–elkies–atkin algorithm;hyperelliptic curve cryptography;counting points on elliptic curves;finite element method;edwards curve;elliptic curve cryptography;hessian form of an elliptic curve;elliptic curve;modular elliptic curve;elliptic curve point multiplication;finite field;schoof's algorithm;polynomial	Crypto	9.82222245016572	43.64432316265815	141359
058ae52cd0f1c7a99b61446950e8073c3c51b971	band preconditioners: application to preconditioned conjugate gradient methods on parallel computers	preconditioned conjugate gradient method;parallel computer;low level shmem message;shallow domain;openmp application program interface;band preconditioners;pcg algorithm;large sparse linear system;navier-stokes equation;band linear system;openmp directive;shallow water;divide and conquer algorithm	A divide and conquer method well suited for the solution of band linear systems on massively parallel computers is presented. This method is applied in the PCG algorithm to solve large sparse linear systems. Implementations, including the low level SHMEM message passing environment and the OpenMP directives are analyzed on a T3D and an Origin 2000. Applications to the solution of Navier-Stokes equations in shallow domains are presented. Some features of the OpenMP Application Program Interface (API) useful to these applications are discussed.	conjugate gradient method;parallel computing;preconditioner	Olivier Besson	1999	Scalable Computing: Practice and Experience		computational science;parallel computing;computer science;theoretical computer science	HPC	-4.249522853050732	37.33157883106348	141731
7d91d2944ed5b846739256029f2c9c79090fd0ca	tabla: a unified template-based framework for accelerating statistical machine learning	template based design;paper;tesla k40;nvidia geforce gtx 650 ti;fpga accelerators;stochastic programming field programmable gate arrays graphics processing units learning artificial intelligence multiprocessing systems;data flow graphs;performance;fpga;stochastic gradient descent;machine learning;nvidia;arm;nvidia jetson tk1;technical report;computer science;asic;machine learning algorithms field programmable gate arrays linear programming stochastic processes algorithm design and analysis hardware data models;tabla generated accelerators tabla framework unified template based framework statistical machine learning ml algorithm compute intensive applications field programmable gate arrays fpga machine learning algorithms asic application specific integrated circuits general purpose processors accelerators generation stochastic optimization problem gradient descent solver objective function multicore cpu many core gpu	A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650 Ti, and Tesla K40) using real hardware measurements. TABLA-generated accelerators provide 19.4x and 2.9x average speedup over the ARM and Xeon processors, respectively. These accelerators provide 17.57x, 20.2x, and 33.4x higher Performance-per-Watt in comparison to Tegra, GTX 650 Ti and Tesla, respectively. These benefits are achieved while the programmers write less than 50 lines of code.	arm cortex-a15;arm cortex-m;arm architecture;algorithm;application-specific integrated circuit;central processing unit;enterprise system;field-programmable gate array;geforce 600 series;geforce 900 series;general-purpose markup language;general-purpose modeling;graphics processing unit;high- and low-level;high-level programming language;ibm notes;machine learning;manycore processor;mathematical optimization;microsoft customer care framework;multi-core processor;optimization problem;performance per watt;programmer;semiconductor research corporation;solver;source lines of code;speedup;stochastic gradient descent;stochastic optimization;tegra;tesla (microarchitecture)	Divya Mahajan;Jongse Park;Emmanuel Amaro;Hardik Sharma;Amir Yazdanbakhsh;Joon Kyung Kim;Hadi Esmaeilzadeh	2016	2016 IEEE International Symposium on High Performance Computer Architecture (HPCA)	10.1109/HPCA.2016.7446050	parallel computing;real-time computing;performance;computer science;technical report;theoretical computer science;operating system;online machine learning;stochastic gradient descent;application-specific integrated circuit;programming language;computational learning theory;arm architecture;active learning;field-programmable gate array	Arch	-0.9349856781518094	45.69860737213673	141957
88df35ffd14d19106c67e44ed103be2d627913c6	performance, design, and autotuning of batched gemm for gpus		The general matrix-matrix multiplication (GEMM) is the most important numerical kernel in dense linear algebra, and is the key component for obtaining high performance in most LAPACK routines. As batched computations on relatively small problems continue to gain interest in many scientific applications, a need arises for a high performance GEMM kernel for batches of small matrices. Such a kernel should be well designed and tuned to handle small sizes, and to maintain high performance for realistic test cases found in the higher level LAPACK routines, and scientific computing applications in general. This paper presents a high performance batched GEMM kernel on Graphics Processing Units (GPUs). We address batched problems with both fixed and variable sizes, and show that specialized GEMM designs and a comprehensive autotuning process are needed to handle problems of small sizes. For most performance tests reported in this paper, the proposed kernels outperform state-of-the-art approaches using a K40c GPU.	algorithm;auto-tune;big data;computation;computational science;data mining;dynamic programming;graphics processing unit;kernel (operating system);lapack;linear algebra;matrix multiplication;numerical analysis;overhead (computing);parallel computing;software framework;sparse matrix;test case	Ahmad Abdelfattah;Azzam Haidar;Stanimire Tomov;Jack J. Dongarra	2016		10.1007/978-3-319-41321-1_2	simulation;computer graphics (images)	HPC	-3.06869602557832	40.09838895266852	142166
0bd8290d4e34b603191351c2fd0b2050a0832704	edge-based interface elements for solution of three-dimensional geomechanical problems	interface element;inexact newton method;three dimensional;finite element;nonlinear problem;unstructured mesh;numerical experiment;vector processor;computational efficiency;data structure	An edge-based three-dimensional interface element for simulation of joints, faults and other discontinuities present in several geomechanical applications is proposed. Edge-based data structures are used to improve computational efficiency of Inexact Newton methods for solving finite element nonlinear problems on unstructured meshes. Numerical experiments in the solution of three-dimensional problems in cache based and vector processors have shown that memory and computer time are reduced.	cpu cache;central processing unit;data structure;experiment;finite element method;newton;nonlinear system;numerical linear algebra;simulation;solver;vector processor	Rubens M. Sydenstricker;Marcos A. D. Martins;Alvaro L. G. A. Coutinho;José L. D. Alves	2002		10.1007/3-540-36569-9_4	three-dimensional space;computational science;mathematical optimization;vector processor;data structure;computer science;theoretical computer science;finite element method;programming language	HPC	-3.8391840580263654	37.352323324475435	142272
61a1b4b8a622ee80142429e8219704b73bca3990	smartenergy.kom: an intelligent system for energy saving in smart home	optimization potential smartenergy kom intelligent system energy saving energy conservation easy to use it services information and communications technology ict energy efficient smart homes wireless sensor networks human activity detection user activities electrical appliances fine grained appliance level energy consumption monitoring energy wasting user current activity detection energyadvisor framework;temperature sensors;home appliances;wireless sensor networks domestic appliances energy conservation energy consumption home automation optimisation;home appliances temperature sensors energy consumption electrical products monitoring temperature measurement;monitoring;energy consumption;electrical products;temperature measurement	Over the last twenty years, energy conservation has always been of great importance to individuals, societies and decision makers around the globe. As a result, IT researchers have shown a great interest in providing efficient, reliable and easy-to-use IT services which help users saving energy at home by making use of the current advances in Information and Communications Technology (ICT). Driven by the aforementioned motivation, we developed SMARTENERGY.KOM, our framework for realizing energy efficient smart homes based on wireless sensor networks and human activity detection. Our work is based on the idea that most of the user activities at home are related to a set of electrical appliances which are necessary to perform these activities. Therefore, we show how it is possible to detect the user's current activity by monitoring his fine-grained appliance-level energy consumption. This relation between activities and electrical appliances makes it possible to detect appliances which could be wasting energy at home. Our framework is organized in two components. On one hand, the activity detection framework which is responsible for detecting the user's current activity based on his energy consumption. On the other hand, the EnergyAdvisor framework which utilizes the activity detection for the purpose of recognizing the appliances which are wasting energy at home and informing the user about optimization potential.	activity recognition;artificial intelligence;behavioral pattern;data point;home automation;interactivity;mathematical optimization;multi-user;sensor;virtual appliance	Alaa Alhamoud;Felix Ruettiger;Andreas Reinhardt;Frank Englert;Daniel Burgstahler;Doreen Böhnstedt;Christian Gottron;Ralf Steinmetz	2014	39th Annual IEEE Conference on Local Computer Networks Workshops	10.1109/LCNW.2014.6927721	embedded system;simulation;temperature measurement;computer security	Mobile	1.1856867886524183	32.92526857875407	142291
dc498fc4fcbb4e958dded8abc1c358da033b7af3	the logp and mlogp models for parallel image processing with multi-core microprocessor	multi core processor;theoretical model;parallel algorithm;parallel models;parallel image processing;parallel computer;multi core processors;parallel algorithms	Despite the advancement and availability of the multiple core microprocessors, it remains an issue on how to fully utilize this relatively new computing platform to achieve optimal performance for a parallel algorithm. There are limitations to the existing theoretical model in analyzing parallel algorithms for multi-core microprocessor systems. The proposed Multi-core LogP (MLogP) model is a more realistic model for parallel computing with multi-core microprocessor. The MLogP model is a variant of the popular LogP model for parallel computation. Experiment with parallel image processing algorithms were used to determine the abilities of LogP and MLogP models in predicting the performance of parallel image processing algorithms on a Intel Core2 Quad 2.44 GHz microprocessor.	computation;image processing;logp machine;microprocessor;multi-core processor;parallel algorithm;parallel computing;theory	Chee-Kong Chui	2010		10.1145/1852611.1852616	computer architecture;parallel computing;embarrassingly parallel;computer science;theoretical computer science;analysis of parallel algorithms;cost efficiency	HPC	-2.5007201066996725	44.46791573772489	142651
0e90dc50608d90daaeba279c527e61300e7e66b8	affine-by-statement transformations of imperfectly nested loops	conjunctive affine constraints affine by statement transformations imperfectly nested loops loop restructuring techniques unimodular approach loop interchange skewing reversal iteration vector iteration space lexicographic order multiple unimodular transformations code generation algorithm;imperfectly nested loops;program control structures;nested loops;code generation;transform coding;matrix algebra;loop transformation;matrix algebra parallelising compilers program control structures;parallelising compilers;lexicographic order;mathematics statistics graphics matrices australia council scheduling algorithm terminology	A majority of loop restructuring techniques developed so far assume that loops are perfectly nested. The unimodular approach unifies three individual transformations – loop interchange, skewing and reversal – but is still limited to perfect loop nests. This paper outlines a framework that enables the use of unimodular transformations to restructure imperfect loop nests. The concepts previously used for perfect loop nests, such as iteration vector, iteration space and lexicographic order, are generalised to characterise imperfect loop nests. Multiple unimodular transformations are allowed, one on each statement in the loop nests. A code generation algorithm is developed that produces the transformed code by scanning a disjunction of conjunctive affine constraints.	algorithm;code generation (compiler);iteration;lexicographical order;loop interchange;unimodular polynomial matrix	Jingling Xue	1996		10.1109/IPPS.1996.508036	loop tiling;loop fusion;parallel computing;transform coding;nested loop join;loop interchange;computer science;lexicographical order;programming language;polytope model;algorithm;code generation	PL	4.323881273669454	37.768636569802254	142662
271a9af2336afa3902d446b77aa4d9d8e93aaa4f	an associative accelerator for large databases	add on boards;performance associative accelerator rapid 1 relational access processor intelligent data tuples logical formulas record relations hashing buckets reduced instruction set hardwired control bit parallel mode microprocessors data structures architecture environments;performance evaluation;reduced instruction set computing;instruction set architecture;data structures;relational databases;relational databases application specific integrated circuits information retrieval network servers distributed databases microprocessors real time systems information systems parallel machines content based retrieval;it evaluation;content addressable storage;data structure;relational databases add on boards content addressable storage data structures performance evaluation reduced instruction set computing	The RAPID-1 (relational access processor for intelligent data), an associative accelerator that recognizes tuples and logical formulas, is presented. It evaluates logical formulas instantiated by the current tuple, or record, and operates on whole relations or on hashing buckets. RAPID- 1 uses a reduced instruction set and hardwired control and executes all comparisons in a bit-parallel mode. It speeds up the database by a significant factor and will adapt to future generations of microprocessors. The principal design issues, data structures, instruction set, architecture, environments and performance are discussed.<<ETX>>	control unit;data structure;database;microprocessor	Pascal Faudemay;Mongia Mhiri	1991	IEEE Micro	10.1109/40.108570	reduced instruction set computing;parallel computing;data structure;relational database;computer science;theoretical computer science;operating system;instruction set;database;programming language	DB	6.008319130881737	39.3377068736507	142724
ffea227e7db4b50b0d859795a36712367f854891	a fine-granularity scheduling algorithm for parallel xdraw viewshed analysis	terrain visibility;viewshed analysis;xdraw algorithm;parallel computing;dem	Viewshed analysis is widely used in many terrain applications such as siting problem, path planning problem, and etc. But viewshed computation is very time-consuming, in particular for applications with large-scale terrain data. Parallel computing as a mainstream technique with the tremendous potential has been introduced to enhance the computation performance of viewshed analysis. This paper presents a revised parallel viewshed computation approach based on the existing serial XDraw algorithm in a distributed parallel computing environment. A layered data-dependent model for processing data dependency in the XDraw algorithm is built to explore scheduling strategy so that a fine-granularity scheduling strategy on the process-level and thread-level parallel computing model can be accepted to improve the efficiency of the viewshed computation. And a parallel computing algorithm, XDraw-L, is designed and implemented taken into account this scheduling strategy. The experimental results demonstrate a distinct improvement of computation performance of the XDraw-L algorithm in this paper compared with the coarse-partition algorithm, like XDraw-E which is presented by Song et al. (Earth Sci Inf 10(5):511–523, 2016), and XDraw-B that is the basic algorithm of serial XDraw. Our fine-granularity scheduling algorithm can greatly improve the scheduling performance of the grid cells between the layers within a triangle region.	algorithm;scheduling (computing);viewshed analysis	Wanfeng Dou;Yanan Li;Yanli Wang	2018	Earth Science Informatics	10.1007/s12145-018-0339-5	data mining;computer science;data dependency;grid;computation;granularity;scheduling (computing);theoretical computer science;motion planning;viewshed analysis	Logic	-2.849744570853603	43.66468995979698	142793
d97f5ce23a1d09a78524e95ae4335a586e67afd1	solving sparse triangular systems on distributed memory multicomputers	distributed memory;distributed memory systems;processor scheduling;parallel solution sparse triangular system solving distributed memory multicomputers distributed memory multiprocessor architectures preprocessing overheads coefficient matrix algorithms substitution method data driven flow nonblocking communications inherent parallelism reordering technique level scheduling adjacency graph;variable block size;sparse matrices memory architecture linear systems iterative methods gradient methods vectors computer science computer architecture data mining parallel processing;multiprocessor architecture;distributed memory multicomputer;sparse matrices;parallel algorithms distributed memory systems processor scheduling sparse matrices;parallel algorithms	The authors describe and compare two different methods for solving sparse triangular systems in distributed memory multiprocessor architectures. The two methods involve some preprocessing overheads so they are primarily of interest in solving many systems with the same coefficient-matrix. Both algorithms start off from the idea of the classical substitution method. The first algorithm presented introduces a concept of data driven flow, and makes use of non-blocking communications in order to dynamically extract the inherent parallelism of sparse systems. The second algorithm uses a reordering technique for the unknowns, so the final system can be grouped in variable block sizes where the rows are independent and can be solved in parallel. This latter technique is called level scheduling because of the way it is represented in the adjacency graph.	distributed memory;sparse matrix	Patricia González;José Carlos Cabaleiro;Tomás F. Pena	1998		10.1109/EMPDP.1998.647235	uniform memory access;distributed shared memory;shared memory;parallel computing;distributed memory;computer science;theoretical computer science;distributed computing	HPC	-1.0977013537580618	38.45874587714108	142799
e8cd3da4399599cb0c0b3a0ef52782ada8bc12dc	challenges of data center thermal management	한국전산유체공학회;high density;environmental conditions;challenges of data center thermal management;fluid mechanics;data communication;performance metric;data center;low power;numerical model;korean society of computational fluids engineering;roger schmidt;power consumption;한국전산유체공학회 2006년도 춘계학술대회;thermal management;life span;thermal environment	The need for more performance from computer equipment in data centers has driven the power consumed to levels that are straining thermal management in the centers. When the computer industry switched from bipolar to CMOS transistors in the early 1990s, low-power CMOS technology was expected to resolve all problems associated with power and heat. However, equipment power consumption with CMOS has been rising at a rapid rate during the past 10 years and has surpassed power consumption from equipment installed with the bipolar technologies 10 to 15 years ago. Data centers are being designed with 15–20-year life spans, and customers must know how to plan for the power and cooling within these data centers. This paper provides an overview of some of the ongoing work to operate within the thermal environment of a data center. Some of the factors that affect the environmental conditions of data-communication (datacom) equipment within a data center are described. Since high-density racks clustered within a data center are of most concern, measurements are presented along with the conditions necessary to meet the datacom equipment environmental requirements. A number of numerical modeling experiments have been performed in order to describe the governing thermo-fluid mechanisms, and an attempt is made to quantify these processes through performance metrics.	adobe air;air cooling;cmos;computer cooling;data center;experiment;low-power broadcasting;mathematical optimization;numerical analysis;perimeter;requirement;server (computing);set packing;thermal management of high-power leds;total cost of ownership;transistor;water cooling	Roger R. Schmidt;Edgar Cruz;Madhusudan K. Iyengar	2005	IBM Journal of Research and Development	10.1147/rd.494.0709	life expectancy;data center;thermal management of electronic devices and systems;simulation;telecommunications;computer science;engineering;electrical engineering;physics;fluid mechanics;mechanical engineering	Networks	5.79230089744736	32.33180868590643	143018
4532d4ba61fc0b9e1c9daba76ab5f9cf9263415c	parallelization of an evolving artificial neural networks system to forecast time series using openmp and mpi	time series forecasting;openmp parallelization artificial neural networks system time series forecasting tsf decision making production resources planning eann estimation distribution algorithm eda parallel programming message passing interface mpi open multi processing;neural nets;computer model;parallel programming;time series;control system;message passing interface;message passing;time series decision making message passing neural nets parallel programming production planning;production planning;parallel programs;standards yttrium biological cells artificial neural networks computational modeling;distributed algorithm;artificial neural network	Time Series Forecasting (TSF) is a key tool to support decision making, for instance by producing better estimates to be used when planning production resources. Artificial Neural Networks (ANN) are innate candidates for TSF due to advantages such as nonlinear learning and noise tolerance. The search for the best ANN is a complex task that strongly affects the forecasting performance while often requiring a high computational time. However, obtaining fast predictions is a relevant issue in several real-world scenarios, such as real-time and control systems. In this work, we present an Evolutionary (EANN) approach for TSF based on Estimation Distribution Algorithm (EDA) that evolves fully connected Artificial Neural Network (EANN). To speed up such approach, we propose the use of two parallel programming standards: Message Passing Interface (MPI) and Open Multi-Processing (OpenMP). Several experiments were held, using five real-world time series with different characteristics and from distinct domains, in order to compare with sequential EANN approach with the MPI and OpenMP parallel variants, under a number of cores that ranged from 1 to 6. Overall, the EANN results are competitive when compared with the popular ForecastPro tool. Moreover, the setup that included the MPI parallelization method and the use of 5 cores lead to the lowest execution times, while making a reasonable use of the available computational resources.	algorithm;artificial neural network;automatic parallelization;best practice;computation;computational resource;control system;experiment;message passing interface;multiprocessing;nonlinear system;openmp;parallel computing;real-time clock;real-time computing;time complexity;time series	Borja Prior González;Juan Peralta;Paulo Cortez;Germán Gutiérrez;Araceli Sanchis	2012	2012 IEEE Conference on Evolving and Adaptive Intelligent Systems	10.1109/EAIS.2012.6232827	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-1.58582164830902	34.44137258090712	143499
c8b6e807fb11c85a46b79d906db1408dcc0a5426	parallel montgomery multipliers	digital arithmetic;parallel algorithms;public key cryptography;table lookup;application-specific cryptoprocessors;look-up tables;modular multiplication;parallel montgomery multipliers;public-key cryptosystems	Modular multiplication is an essential operation in virtually all public-key cryptosystems in use today. This work presents four designs for speeding up modular multiplication on application-specific crypto-processors. All the approaches utilize small look-up tables and fast, massively parallel multipliers. Two of the approaches trade off smaller look-up tables for a larger, slightly slower multiplier. The other two approaches use larger look-up tables but a smaller, faster multiplier.	central processing unit;cryptosystem;flops;fan-out;fixed point (mathematics);fixed-point arithmetic;flip-flop (electronics);lookup table;matrix multiplication;montgomery modular multiplication;operand;parallel computing;pipeline (computing);public-key cryptography;pumping (computer systems);systolic array;throughput;time complexity	Moboluwaji O. Sanu;Earl E. Swartzlander;Craig M. Chase	2004	Proceedings. 15th IEEE International Conference on Application-Specific Systems, Architectures and Processors, 2004.	10.1109/ASAP.2004.10008	modular arithmetic;parallel computing;kochanski multiplication;lookup table;computer science;theoretical computer science;parallel algorithm;public-key cryptography;algorithm	HPC	8.272235706743768	43.45725662762552	143855
f8cfb1e60ae57ce6fa65753ad2f2d21efdda1ac9	machine learning on fpgas to face the iot revolution		FPGAs have been rapidly adopted for acceleration of Deep Neural Networks (DNNs) with improved latency and energy efficiency compared to CPU and GPU-based implementations. High-level synthesis (HLS) is an effective design flow for DNNs due to improved productivity, debugging, and design space exploration ability. However, optimizing large neural networks under resource constraints for FPGAs is still a key challenge. In this paper, we present a series of effective design techniques for implementing DNNs on FPGAs with high performance and energy efficiency. These include the use of configurable DNN IPs, performance and resource modeling, resource allocation across DNN layers, and DNN reduction and re-training. We showcase several design solutions including Long-term Recurrent Convolution Network (LRCN) for video captioning, Inception module for FaceNet face recognition, as well as Long Short-Term Memory (LSTM) for sound recognition. These and other similar DNN solutions are ideal implementations to be deployed in vision or sound based IoT applications.	artificial neural network;central processing unit;convolution;debugging;design space exploration;facial recognition system;field-programmable gate array;graphics processing unit;high-level synthesis;ips panel;long short-term memory;machine learning	Xiaofan Zhang;Anand Ramachandran;Chuanhao Zhuge;Di He;Wei Zuo;Zuofu Cheng;Kyle Rupnow;Deming Chen	2017	2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)	10.1109/ICCAD.2017.8203862	latency (engineering);debugging;computer science;artificial neural network;field-programmable gate array;real-time computing;implementation;design flow;design space exploration;resource allocation	EDA	3.2824427015028568	43.67729172956468	144136
c85ddfa30f38553095f0b36892a9a74f8f4b8b93	fpga implementations of the iceberg block cipher	field programmable gate array;nist;electronic mail;encryption;reconfigurable architectures;block cipher;field programmable gate arrays cryptography hardware nist electronic mail performance loss software performance table lookup throughput feedback loop;fpga;software performance;fpga implementation;plaintext;reconfigurable architectures cryptography field programmable gate arrays;feedback loop;key changing;cryptography;iceberg block cipher;decryption;side channel attacks;side channel attacks fpga iceberg block cipher field programmable gate array reconfigurable hardware encryption decryption key changing plaintext;field programmable gate arrays;table lookup;performance loss;hardware implementation;reconfigurable hardware;throughput;hardware	This paper presents FPGA (field programmable gate array) implementations of ICEBERG, a block cipher designed for reconfigurable hardware implementations and presented at FSE 2004. All its components are involutional and allow very efficient combinations of encryption/decryption. The implementations proposed also allow changing the key and encrypt/decrypt (E/D) mode for every plaintext, without any performance loss. In comparison with other recent block ciphers, the implementation results of ICEBERG show a significant improvement of hardware efficiency. Moreover, the key and E/D agility allows considering new encryption modes to counteract certain side-channel attacks.	block cipher;field-programmable gate array	François-Xavier Standaert;Gilles Piret;Gaël Rouvroy;Jean-Jacques Quisquater	2005		10.1109/ITCC.2005.155	embedded system;parallel computing;real-time computing;computer science	Crypto	8.955540195283636	45.191292800835136	144298
92e04c25c88037692032fa1229ebb2154532205d	a general methodology of partitioning and mapping for given regular arrays	difference equations processor scheduling topology computer architecture concurrent computing computer science;algorithm transformation;school of no longer in use;topology;electronics and computer science;automatic compilation;concurrent computing;mesh partitioning;processor array general methodology partitioning mapping regular arrays arbitrary uniform recurrence equations computation graphs regular array canonical dependencies parallelization arbitrary computation graph;helium;processor scheduling;computation graphs;systolic arrays;partitioning;general methodology;indexing terms;regular arrays;computer architecture;qa 75 electronic computers computer science;processor array;parallel algorithms systolic arrays;canonical dependencies;difference equations;arbitrary uniform recurrence equations;regular array;parallelization;mapping;computer science;arbitrary computation graph;uniform recurrence equation;high efficiency;given shape and fixed mesh partitioning;parallel algorithms	A methodology for partitioning and mapping of arbitrary uniform recurrence equations (UREs) expressed as computation graphs onto a given regular array is proposed. Deriving and based on a set of canonical dependencies together with two models of space projection, we give a general method of parallelization. The method has significant advantages in mapping an arbitrary computation graph onto a given processor array while preserving high efficiency in both communication and computation.	computation;parallel computing;processor array;recurrence relation	Xian Chen;Graham M. Megson	1995	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.473518	parallel computing;recurrence relation;index term;concurrent computing;computer science;theoretical computer science;distributed computing;parallel algorithm;helium	HPC	4.251804823606861	38.37832557510758	144654
5fde7e570bd16bd7fb1d321523f6784c1bd3662e	a tridiagonal system solver for distributed memory parallel processors with vector nodes	sistema lineal;linear algebra;parallelisme;distributed memory;distributed system;analisis numerico;systeme reparti;implementation;sistema informatico;computer system;linear system;analyse numerique;tridiagonal matrix;resolucion sistema ecuacion;resolution systeme equation;ejecucion;parallelism;sistema repartido;numerical analysis;paralelismo;algebre lineaire;matriz tridiagonal;algebra lineal;systeme informatique;equation system solving;systeme lineaire;vector processor;matrice tridiagonale;processeur vectoriel	Abstract   A variant of the odd-even cyclic reduction algorithm for solving tridiagonal linear systems is presented. Of particular interest is the case where the number of equations is much larger than the number of processors. The target architecture for this scheme is a distributed-memory parallel computer with nodes which are vector processors. The partitioning of the matrix system is governed by a parameter which determines the amount of redundancy in computations and the amount of communication. One feature of the method is that computations are well balanced, as each processor executes an identical algebraic routine. A brief description of the standard cyclic reduction algorithm is given. Then a divide and conquer strategy is presented along with some estimates of speedup and efficiency. Finally, FORTRAN programs for this algorithm which run on the Intel iPSC/2-VX and Floating Point Systems FPS T-20 computer are discussed along with experimental results. Of particular interest is the performance evaluation of this algorithm according to Gustafson's concept of scaled speedup.	distributed memory;solver	Christopher L. Cox;James A. Knisely	1991	J. Parallel Distrib. Comput.	10.1016/0743-7315(91)90079-O	tridiagonal matrix;vector processor;parallel computing;distributed memory;numerical analysis;computer science;theoretical computer science;linear algebra;mathematics;linear system;implementation;algorithm	HPC	-2.543591552587674	37.065788426746316	144861
73f9f8cdaf824a64b2532b7eb5e402abdb60ce63	static mapping of functional programs: an example in signal processing	target multiprocessor;static mapping;program transformation;complex signal-processing problem;inthis article;acyclic interconnection;functional program;multiprocessor computer;conventional dsp technology;signal processing;program module;program description tree;functional programming	Complex signal processing problems are naturally described by compositions of program modules that process streams of data. In this paper we discuss how such compositions may be analyzed and mapped onto multiprocessor computers to eeectively exploit the massive parallelism of these applications. The methods are illustrated with an example of signal processing for an optical surveillance problem. The method illustrated involves program transformation and analysis to construct a program description tree that represents the given computation as an acyclic interconnection of stream-processing modules. Each module may be mapped to a set of threads run on a group of processing elements of a target multiproces-sor. We estimate the parameters of performance that could be realized for two forms of multiprocessor architecture , one based on conventional DSP technology and one based on a multithreaded processing element architecture. For our example, we conclude that the multithreaded architecture ooers advantages in latency of results and in memory requirements, as well as in throughput.	computation;computer;directed acyclic graph;interconnection;multiprocessing;parallel computing;program transformation;requirement;signal processing;stream processing;successive over-relaxation;thread (computing);throughput	Jack B. Dennis	1996	Scientific Programming		parallel computing;real-time computing;computer science;theoretical computer science;operating system;programming language	Arch	4.728081854775072	46.14315242667208	145091
5af52ee7691c6d3f8edea2f25a1e98184697a35b	combining multi-core and gpu computing for solving combinatorial optimization problems	flowshop scheduling problem;multi core computing;parallel branch and bound;gpu accelerators	In this paper, we revisit the design and implementation of Branch-and-Bound (B&B) algorithms for solving large combinatorial optimization problems on GPU-enhanced multi-core machines. B&B is a tree-based optimization method that uses four operators (selection, branching, bounding and pruning) to build and explore a highly irregular tree representing the solution space. In our previous works, we have proposed a GPU-accelerated approach in which only a single CPU core is used and only the bounding operator is performed on the GPU device. Here, we extend the approach (LL-GB&B) in order to minimize the CPU-GPU communication latency and thread divergence. Such an objective is achieved through a GPU-based fine-grained parallelization of the branching and pruning operators in addition to the bounding one. The second contribution consists in investigating the combination of a GPU with multi-core processing. Two scenarios have been explored leading to two approaches: a concurrent (RLL-GB&B) and a cooperative one (PLL-GB&B). In the first one, the exploration process is performed concurrently by the GPU and the CPU cores. In the cooperative approach, the CPU cores prepare and off-load to GPU pools of tree nodes using data streaming while the GPU performs the exploration. The different approaches have been extensively experimented on the Flowshop scheduling problem. Compared to a single CPU-based execution, LL-GB&B allows accelerations up to (x160) for large problem instances. Moreover, when combining multi-core and GPU, we figure out that using RLL-GB&B is not beneficial while PLL-GB&B enables an improvement up to 36% compared to LL-GB&B.	combinatorial optimization;general-purpose computing on graphics processing units;graphics processing unit;mathematical optimization;multi-core processor	Imen Chakroun;Nouredine Melab;Mohand-Said Mezmaz;Daniel Tuyttens	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.07.023	mathematical optimization;parallel computing;computer science;theoretical computer science;operating system;distributed computing	HPC	-2.6934488328719266	43.485217303988684	145300
3d193031a7d100834bc5631b4b489193a7349c4e	architecture comparisons between nvidia and ati gpus: computation parallelism and data communications	energy efficiency;kernel;energy efficiency gpu clustering performance;paper;nvidia geforce gtx 580;energy efficient;graphics processing unit benchmark testing high definition video vliw computer architecture kernel computational modeling;computer model;energy efficient computing;performance;ati;gpu;data communication;vliw;computer architecture;power aware computing;large scale;computational modeling;energy consumption;clustering;memory architecture;graphics processing units;high performance computer;high definition video;nvidia;graphic processing unit;difference set;computer science;energy consumption architecture comparisons nvidia ati gpu computation parallelism data communications modern graphics processing units high performance computing gpu manufacturers design concepts processor cores memory subsystem architectural differences fermi cypress energy efficiency power consumption;graphics processing unit;opencl;ati radeon hd 5870;high definition;parallel processing;benchmark testing;power aware computing data communication graphics processing units memory architecture parallel processing	In recent years, modern graphics processing units have been widely adopted in high performance computing areas to solve large scale computation problems. The leading GPU manufacturers Nvidia and ATI have introduced series of products to the market. While sharing many similar design concepts, GPUs from these two manufacturers differ in several aspects on processor cores and the memory subsystem. In this paper, we conduct a comprehensive study to characterize the architectural differences between Nvidia's Fermi and ATI's Cypress and demonstrate their impact on performance. Our results indicate that these two products have diverse advantages that are reflected in their performance for different sets of applications. In addition, we also compare the energy efficiencies of these two platforms since power/energy consumption is a major concern in the high performance computing.	computation;computer graphics;graphics processing unit;parallel computing;supercomputer	Ying Zhang;Lu Peng;Bin Li;Jih-Kwon Peir;Jianmin Chen	2011	2011 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2011.6114180	parallel processing;computer architecture;parallel computing;computer hardware;computer science;operating system;efficient energy use	Arch	-3.603341771338554	46.16642946434829	145598
487bf98f99d95a69d525b13236f1fbb61f4d3dd0	probabilistic counter updates for predictor hysteresis and stratification	counting circuits hysteresis hardware negative feedback frequency computer science application software linear feedback shift registers encoding;performance evaluation;building block;perforation;parallel architectures;stratification;pseudo random number generator;frequency stratifier probabilistic counter update predictor hysteresis predictor stratification high performance processor pseudo random number generator branch confidence criticality predictor 4 bit counter likelihood of criticality counter increment counter decrement;performance evaluation parallel architectures;high performance	Hardware counters are a fundamental building block of modern high-performance processors. This paper explores two applications of probabilistic counter updates, in which the output of a pseudo-random number generator decides whether to perform a counter increment or decrement. First, we discuss a probabilistic implementation of counter hysteresis, whereby previously proposed branch confidence and criticality predictors can be reduced in size by factors of 2 and 3, respectively, with negligible impact on performance. Second, we build a frequency stratifier by making increment and decrement probabilities functions of the current counter value. The stratifier enables a 4-bit counter to classify an instruction's Likelihood of Criticality with sufficient accuracy to closely approximate the performance of an unbounded precision classifier. Because probabilistic updates are both simple and effective, we believe these ideas hold great promise for immediate use by industry, perhaps enabling the use of structures such as branch confidence predictors which may have previously been viewed as too expensive given their functionality.	1-bit architecture;4-bit;approximation algorithm;branch predictor;central processing unit;color depth;criticality matrix;experiment;hysteresis;increment and decrement operators;kerrison predictor;microprocessor;pseudorandom number generator;pseudorandomness;random number generation;randomness;read-modify-write;self-organized criticality	Nicholas Riley;Craig B. Zilles	2006	The Twelfth International Symposium on High-Performance Computer Architecture, 2006.	10.1109/HPCA.2006.1598118	stratification;parallel computing;real-time computing;computer science;theoretical computer science;pseudorandom number generator	Arch	5.488010711233957	43.266879345736044	145956
7d1752d9692ea67b0a3d2b212aaf11bb96b52bc5	a reconfigurable processor for the cryptographic ηt pairing in characteristic 3	bilinear pairing reconfigurable processor cryptographic eta t pairing cryptographic protocol;elliptic curve;hardware elliptic curve cryptography identity based encryption field programmable gate arrays iterative algorithms arithmetic proposals cryptographic protocols throughput elliptic curves;cryptographic protocols;cryptographic protocol;microprocessor chips cryptographic protocols;bilinear pairing;high throughput;cryptographic eta t pairing;tate pairing;hardware implementation;reconfigurable processor;microprocessor chips	Recently, there have been many proposals for secure and novel cryptographic protocols that are built on bilinear pairings. The eta T pairing is one such pairing and is closely related to the Tate pairing. In this paper we consider the efficient hardware implementation of this pairing in characteristic 3. All characteristic 3 operations required to compute the pairing are outlined in detail. An efficient, flexible and reconfigurable processor for the etaT  pairing in characteristic 3 is presented and discussed. The processor can easily be tailored for a low area implementation, for a high throughput implementation, or for a balance between the two. Results are provided for various configurations of the processor when implemented over the field F397 on an FPGA	bilinear filtering;cryptographic protocol;cryptography;field-programmable gate array;reconfigurable computing;throughput	Robert Ronan;Colm O'hEigeartaigh;Colin C. Murphy;Tim Kerins;Paulo S. L. M. Barreto	2007	Fourth International Conference on Information Technology (ITNG'07)	10.1109/ITNG.2007.19	cryptographic primitive;computer science;theoretical computer science;cryptographic protocol;distributed computing	EDA	9.077106365775403	44.46218987103298	145996
75828a4eb4abd2669a5a903e20a746153e86e181	tile qr factorization with parallel panel processing for multicore architectures	directed graphs;linear algebra;communication avoiding;directed acyclic graph;parallel linear algebra;distributed memory systems;shared memory;dense linear algebra;processor scheduling;communication avoiding qr;dense linear algebra library;distributed memory machine;plasmas;shared memory systems directed graphs distributed memory systems linear algebra matrix decomposition parallel architectures processor scheduling;shared memory multicore architectures;qr factorization;plasma library tile qr factorization parallel panel processing dense linear algebra library tile algorithms scheduling directed acyclic graph panel factorization skinny matrices small square matrices asynchronous method shared memory multicore architectures communication avoiding qr distributed memory machines asynchronous computations parallel linear algebra scalable multicore architectures;tiles multicore processing linear algebra libraries computer architecture scheduling algorithm algorithm design and analysis context concurrent computing distributed computing;panel factorization;shared memory systems;parallel panel processing;multicore;asynchronous method;parallel architectures;matrix decomposition;tile qr factorization;scheduling;dynamic scheduling qr factorization tile algorithms multicore communication avoiding;multicore processing;merging;distributed memory machines;plasma library;tile algorithms;tiles;magnetic cores;high performance;skinny matrices;dynamic scheduling;asynchronous computations;scalable multicore architectures;small square matrices	To exploit the potential of multicore architectures, recent dense linear algebra libraries have used tile algorithms, which consist in scheduling a Directed Acyclic Graph (DAG) of tasks of fine granularity where nodes represent tasks, either panel factorization or update of a block-column, and edges represent dependencies among them. Although past approaches already achieve high performance on moderate and large square matrices, their way of processing a panel in sequence leads to limited performance when factorizing tall and skinny matrices or small square matrices. We present a new fully asynchronous method for computing a QR factorization on shared-memory multicore architectures that overcomes this bottleneck. Our contribution is to adapt an existing algorithm that performs a panel factorization in parallel (named Communication-A voiding QR and initially designed for distributed-memory machines), to the context of tile algorithms using asynchronous computations. An experimental study shows significant improvement (up to almost 10 times faster) compared to state-of-the-art approaches. We aim to eventually incorporate this work into the Parallel Linear Algebra for Scalable Multi-core Architectures (PLASMA) library.	algorithm;blocking (computing);communication-avoiding algorithms;comparison of linear algebra libraries;computation;directed acyclic graph;distributed memory;expect;experiment;lu decomposition;library (computing);multi-core processor;naive bayes classifier;numerical stability;qr decomposition;scheduling (computing);shared memory;z/vm	Bilel Hadri;Hatem Ltaief;Emmanuel Agullo;Jack J. Dongarra	2010	2010 IEEE International Symposium on Parallel & Distributed Processing (IPDPS)	10.1109/IPDPS.2010.5470443	multi-core processor;parallel computing;computer science;theoretical computer science;linear algebra;operating system;distributed computing	HPC	-3.9909351565154934	41.92637731199695	146088
cb8d3a67db1e13491b55c207662b3b8b7c69e2c7	comparison of an o(n) and an o(n log n) n-body solver		In this paper we compare the performance characteristics of two 3-dimensional hierarchical N-body solvers: an O(N) and an O(N logN) solver. We present the execution times for numerous N-body force evaluations using the two methods, with various values of N and , where is the prescribed error. We nd that the O(N logN) method is more suited to problems which demand a high precision and large N. We then consider how parallelisation aaects the algorithms' relative performance.	algorithm;n-body problem;parallel computing;solver	Gavin J. Pringle	1995			algebra;time complexity;solver;chemistry	ML	-1.1351874402055293	36.92108315931148	146255
eba34ff599fa6854d825f13163627d237a5357d2	pipelining for locality improvement in rk methods	equation derivee partielle;microprocessor;equation differentielle;partial differential equation;ecuacion derivada parcial;metodo vectorial;partial dierential equation;differential equation;matrice diagonale;methode runge kutta;metodo runge kutta;initial value problem;ecuacion diferencial;matriz diagonal;vector method;estructura datos;problema valor inicial;methode vectorielle;structure donnee;oleoducto;microprocesseur;arithmetique pipeline;probleme valeur initiale;memory hierarchy;data structure;microprocesador;pipeline arithmetic;pipeline;runge kutta;ordinary dierential equation;runge kutta method;diagonal matrix	We consider embedded Runge-Kutta (RK) methods for the solution of ordinary differential equations (ODEs) arising from space discretizations of partial differential equations and study their efficient implementation on modern microprocessors with memory hierarchies. For those systems of ODEs, we present a block oriented pipelining approach with diagonal sweeps over the stage and approximation vector computations of RK methods. Comparisons with other efficient implementations show that this pipelining technique improves the locality behavior considerably. Runtime experiments are performed with the DOPRI5 method.	approximation;central processing unit;column-oriented dbms;computation;embedded system;experiment;linux/rk;locality of reference;memory hierarchy;microprocessor;pipeline (computing);run time (program lifecycle phase);runge–kutta methods;runtime system	Matthias Korch;Thomas Rauber;Gudula Rünger	2002		10.1007/3-540-45706-2_100	mathematical optimization;mathematical analysis;runge–kutta methods;data structure;computer science;calculus;mathematics;programming language	EDA	-3.0681207745931025	36.664793424242546	146405
9fdb88a564466c96d768de3afc99f975fabe157d	the relevance of new data structure approaches for dense linear algebra in the new multi-core / many core environments	dense linear algebra;computer architecture;high performance;data structure	For over ten years now, Bo K̊agström’s Group in Umea, Sweden, Jerzy Waśniewski’s Team at Danish Technical University in Lyngby, Denmark, and I at IBM Research in Yorktown Heights have been applying recursion and new data structures to increase the performance of Dense Linear Algebra (DLA) factorization algorithms. Later, John Gunnels, and later still, Jim Sexton, both now at IBM Research began working in this area. For about three years now almost all computer manufacturers have dramatically changed their computer architectures which they call Multicore (MC). The traditional designs of DLA libraries such as LAPACK and ScaLAPACK perform poorly on MC. Recent results of Jack Dongarra’s group at the Innovative Computing Laboratory in Knoxville, Tennessee have shown how to obtain high performance for DLA factorization algorithms on the Cell architecture, an example of an MC processor, but only when they used new data structures. We will give some reasons why this is so. We also present new algorithms for Blocked In-Place Rectangular Transposition of an M by N matrix A. This work adds blocking to the work by Gustavson and Swirszcz presented at Para06 on scalar in-place transposition. We emphasize the importance of RB format and also provide efficient algorithms between RB format and standard column and row major formats of 2-D arrays in the Fortran and C languages. Performance results are given. From a practical point of view, this work is very important as it will allow existing codes using LAPACK and ScaLAPACK to remain usable by new versions of LAPACK and ScaLAPACK.	algorithm;call of duty: black ops;cell (microprocessor);computer architecture;data structure;drive letter assignment;ibm research;lapack;library (computing);linear algebra;multi-core processor;rainbows end;recursion;relevance;scalapack	Fred G. Gustavson	2007		10.1007/978-3-540-68111-3_64	parallel computing;simulation;data structure;computer science;theoretical computer science;operating system;mathematics;distributed computing;programming language	HPC	-3.1986011746862677	41.107451234331656	146413
3d6ec34ff753ae03dcc0d7c2acb58f95e4a2b8bc	intelligent magnetic sensing system for low power wsn localization immersed in liquid-filled industrial containers	local algorithm;ambient intelligence;underwater wireless sensor networks;localization;non linear mapping;wireless sensor node;wireless sensor network;radio frequency;low power;stainless steel;sensor nodes;triangulation;acoustic waves;triaxial amr sensor	Wireless sensor networks (WSN) have become an important research domain and have been deployed in many applications, e.g., military, ambient intelligence, medical, and industrial tasks. The location of wireless sensor nodes is a crucial aspect to understand the context of the measured values in industrial processes. Numerous existing technologies, e.g., based on radio frequency (RF), light, and acoustic waves, have been developed and adapted for requirements of localization in WSN. However, physical constraints of the application environment and each localization technology lead to different aptness. In particular, for liquid media in industrial containers, determining the location of every sensor nodes becomes very challenging. In this paper, a localization concept based on intelligent magnetic sensing system using triaxial anisotropic magnetoresistive (AMR) sensor with appropriate switched coils combined with a centralized localization algorithm based on iterative nonlinear mapping (NLM) is presented. Here, our system is extended by low power and fast localization based on triangulation for feasible local position computation. The experimental results, both in the air as well as in liquid filled stainless steel container, delivered in the average an absolute localization error in the order of 6 cm for both NLM and triangulation. In future work, we will scale our approach to industrial container size required for beer brewing industry and increase the accuracy and speed by timely electronics and calibration.		Kuncup Iswandy;Stefano Carrella;Andreas Koenig	2010		10.1007/978-3-642-15390-7_37	embedded system;electronic engineering;telecommunications;engineering;key distribution in wireless sensor networks;mobile wireless sensor network	Robotics	3.476176060522121	32.45529088671567	146466
33155dd6d9dbc41a4c5e76a97bef05c66cbf55d1	development of parallel density functional program using distributed matrix to calculate all-electron canonical wavefunction of large molecules	large molecule;parallelization;density functional method;protein;distributed matrix;density functional	We developed a new parallel density-functional canonical molecular-orbital program for large molecules based on the resolution of the identity method. In this study, all huge matrices were decomposed and saved to the distributed local memory. The routines of the analytical molecular integrals and numerical integrals of the exchange-correlation terms were parallelized using the single program multiple data method. A conventional linear algebra matrix library, ScaLAPACK, was used for matrix operations, such as diagonalization, multiplication, and inversion. Anderson's mixing method was adopted to accelerate the self-consistent field (SCF) convergence. Using this program, we calculated the canonical wavefunctions of a 306-residue protein, insulin hexamer (26,790 orbitals), and a 133-residue protein, interleukin (11,909 orbitals) by the direct-SCF method. In regard to insulin hexamer, the total parallelization efficiency of the first SCF iteration was estimated to be 82% using 64 Itanium 2 processors connected at 3.2 GB/s (SGI Altix3700), and the calculation successfully converged at the 17-th SCF iteration. By adopting the update method, the computational time of the first and the final SCF loops was 229 min and 156 min, respectively. The whole computational time including the calculation before the SCF loop was 2 days and 17 h. This study put the calculations of the canonical wavefunction of 30,000 orbitals to practical use.		Toru Inaba;Fumitoshi Sato	2007	Journal of computational chemistry	10.1002/jcc.20549	macromolecule;chemistry;theoretical computer science;computational chemistry;mathematics;algorithm	HPC	-4.388234049341711	37.43048027264829	146621
8131fb053dd4eb38b088b3d56b6372957a3c5617	embedding hierarchical hypercube networks into the hypercube	multiprocessor interconnection networks;embedding;hypercube;parallel algorithm;concurrent computing;matrix transformations hierarchical hypercube networks embedding interconnection network parallel algorithms;hierarchical networks;hypercubes multiprocessor interconnection networks algorithm design and analysis parallel algorithms parallel architectures computer architecture concurrent computing embedded computing data structures multidimensional systems;interconnection network;computer architecture;parallel architectures;data structures;hierarchical hypercube networks embedding;matrix transformations;interconnection networks;hypercubes;cost effectiveness;parallel algorithms hypercube networks;dilation;algorithm design and analysis;multidimensional systems;embedded computing;hypercube networks;parallel algorithms	The embedding of one interconnection network into another is a very important issue in the design and analysis of parallel algorithms. Through such embeddings, the algorithms originally developed for one architecture can be directly mapped to another architecture. This paper describes a new embedding method, based on matrix transformations, for optimally embedding hierarchical hypercube networks (HHNs) into the hypercube (binary n-cube). Thus, this embedding method has practical importance in enhancing the capabilities and extending the usefulness of the hypercube, since hierarchical hypercube networks have proven to be very cost-effective for a wide range of applications.		Mounir Hamdi;Siang Wun Song	1997	IEEE Trans. Parallel Distrib. Syst.	10.1109/71.615435	parallel computing;concurrent computing;computer science;theoretical computer science;distributed computing;parallel algorithm;hypercube	Arch	7.817381223394618	38.33739205075007	146904
99e1d70688d88cfdac8245c6ab51e1ccaee5d84d	data hiding in scrambled images: a new double layer security data hiding technique	pseudorandom data hiding;digital data;encrypted image;embedded data;layer security;data security;data embedding;scenario data hiding;secret data;scrambled image;new double layer security;encrypted data	pseudorandom data hiding;digital data;encrypted image;embedded data;layer security;data security;data embedding;scenario data hiding;secret data;scrambled image;new double layer security;encrypted data		Shabir A. Parah;Javaid A. Sheikh;Abdul M. Hafiz;Ghulam Mohiuddin Bhat	2014	Computers & Electrical Engineering	10.1016/j.compeleceng.2013.11.006	telecommunications;computer science;theoretical computer science;internet privacy;computer security	Security	8.30987052224772	35.480403485382865	147228
1ddccf36ad644d5716e340c4f6e14d0b0030a9b4	load balancing for computation of steady viscous flows using adaptive unstructured grids	domain decomposition;viscous flow;simulated annealing;decomposition method;unstructured grid;domain decomposition method;low mach number;load balance;parallel implementation	Parallel implementation of unstructured grid solvers for steady viscous flows via domain decomposition method is considered. Various methods for grid partitioning are analysed and a new efficient realisation of a simulated annealing method is developed. The method, being still rather slow, provides grid partition structure superior to ones produced by more fast decomposition methods. This is of primary importance for stationary problems considered in the present paper: grid partition procedure is invoked only a few times (initially and after each grid refinement) during computation in contrast to dynamic problems where the execution time of domain decomposition step is a critical factor. The code has been implemented on a cluster of Sun workstations using PVM communications library. The code has been used to compute 3D internal low-Mach number viscous nonisothermal flows.	computation;load balancing (computing)	E. I. Levin;Alexander I. Zhmakin	1996		10.1007/3540617795_34	mathematical optimization;parallel computing;computer science;distributed computing	HPC	-4.3956811576754635	36.57999677338631	147282
37d424468f01d87756aea67293d306e9ae330e44	hardware emulation building blocks for real-time simulation of large-scale power grids	real time systems electric machines electromechanical effects field programmable gate arrays hardware description languages power grids power system simulation power transmission lines;power system simulation;multidomain dynamics hardware emulation building block large scale power grid digital hardware building block concept power system networks emulation field programmable gate array fpga transmission line nonlinear element machine vhdl language portability three phase bus electromagnetic transient electromechanical dynamics hebb based modeling real time digital simulator multiscale dynamics;hardware description languages;field programmable gate arrays real time systems hardware computational modeling power systems random access memory mathematical model;power grids;field programmable gate arrays;smart grids field programmable gate arrays fpgas large scale systems parallel processing power system simulation real time systems;power transmission lines;electromechanical effects;real time systems;electric machines	This paper proposes digital hardware building block concept for emulating power system networks on field-programmable gate arrays (FPGAs) in real time. Basic hardware emulation building blocks (HEBBs) for machines, transmission lines, nonlinear elements, and loads are presented to demonstrate how real-time simulation can be achieved for realistic power systems. All of the hardware modules were developed using the VHDL language for portability and extensibility. Employing multiple FPGAs, a large-scale power system consisting of 1260 three-phase buses is modeled in detail in real time to show both electromagnetic transients and electromechanical dynamics in the system. The advantage of HEBB-based modeling is that the design and development of new technologies can be accelerated by utilizing massive real-time digital simulators capable of modeling multiscale and multidomain dynamics.	bandwidth (signal processing);broadcast delay;digital electronics;emulator;extensibility;field-programmability;field-programmable gate array;ibm power systems;nonlinear system;real-time clock;real-time operating system;real-time transcription;simulation;software portability;transient (computer programming);transmission line;vhdl	Yuan Chen;Venkata Dinavahi	2014	IEEE Transactions on Industrial Informatics	10.1109/TII.2013.2243742	embedded system;electronic engineering;real-time computing;computer science;electrical engineering;operating system;power system simulation;hardware description language;electric power transmission;field-programmable gate array;hardware emulation	Embedded	2.1616253358652924	45.23860726898109	147624
ad6b171469203ab78addefa2f40e57a2a772bc02	a comprehensive performance analysis of hardware implementations of caesar candidates		Authenticated encryption with Associated Data (AEAD) plays a significant role in cryptography because of its ability to provide integrity, confidentiality and authenticity at the same time. Due to the emergence of security at the edge of computing fabric, such as, sensors and smartphone devices, there is a growing need of lightweight AEAD ciphers. Currently, a worldwide contest, titled CAESAR, is being held to decide on a set of AEAD ciphers, which are distinguished by their security, run-time performance, energy-efficiency and low area budget. For accurate evaluation of CAESAR candidates, it is of utmost importance to have independent and thorough optimization for each of the ciphers both for their corresponding hardware and software implementations. In this paper, we have carried out an evaluation of the optimized hardware implementation of AEAD ciphers selected in CAESAR third round. We specifically focus on manual optimization of the micro-architecture, evaluations for ASIC technology libraries and the effect of CAESAR APIs on the performances. While these has been studied for FPGA platforms and standalone S. Kumar, J. Haj-Yahya, and A. Chattopadhyay School of Computer Science and Engineering Nanyang Technological University Singapore E-mail: sachinkumar@ntu.edu.sg,jawad@ntu.edu.sg, and anupam@ntu.edu.sg M. Khairallah School of Physical and Mathematical Sciences Nanyang Technological University Singapore E-mail: mustafam001@e.ntu.edu.sg M. A. Elmohr Department of Electrical and Computer Engineering University of Waterloo Ontario, Canada E-mail: mahmoud.a.elmohr@ieee.org cipher implementation to the best of our knowledge, this is the first detailed ASIC benchmarking of CAESAR candidates including manual optimization. In this regard, we benchmarked all prior reported designs, including the code generated by high-level synthesis flows. Detailed optimization studies are reported for NORX, CLOC and Deoxys-I. Our pre-layout results using commercial ASIC technology library and synthesis tools show that optimized NORX is 40.81% faster and 18.02% smaller, optimized CLOC is 38.30% more energy efficient and 20.65% faster and optimized Deoxys-I is 35.16% faster, with respect to the best known results. Similar or better performance results are also achieved for FPGA platforms.	application-specific integrated circuit;authenticated encryption;authentication;benchmark (computing);block cipher;caesar;computer engineering;computer science;confidentiality;control unit;cryptography;data rate units;emergence;fabric computing;field-programmable gate array;high- and low-level;high-level programming language;high-level synthesis;ibm i;library (computing);mathematical optimization;microarchitecture;performance;profiling (computer programming);requirement;sensor;smartphone;throughput	Sachin Kumar;Jawad Haj-Yihia;Mustafa Khairallah;Anupam Chattopadhyay	2017	IACR Cryptology ePrint Archive		implementation;computer architecture;computer science	Arch	7.922531469563131	45.85567512527998	147686
bbf08bbc5847825086e80ffe2783430381e1df19	an efficient lossless compression algorithm for electrocardiogram signals		This paper focuses on a novel lossless compression algorithm which can be efficiently used for compression of electrocardiogram (ECG) signals. The proposed algorithm has low memory requirements and relies on a simple and efficient encoding scheme which can be implemented with elementary counting operations. Thus it can be easily implemented even in resource constrained microcontrollers as those commonly used in several low-cost ECG monitoring systems. Despite its simplicity, simulation results carried out on real-world ECG signals show that the proposed algorithm achieves higher compression ratios as even compared to other more complex state-of-the-art solutions.		Giuseppe Campobello;Antonino Segreto;Sarah Zanafi;Salvatore Serrano	2018	2018 26th European Signal Processing Conference (EUSIPCO)	10.23919/EUSIPCO.2018.8553597	compression ratio;microcontroller;data compression;algorithm;compression (physics);matrix decomposition;lossless compression;computer science	ML	7.149551474901169	41.099476819690274	147709
1a3ef25fb9209fcb4b34fac721bfc687650d84aa	parallel reduction of matrices in gröbner bases computations	parallel reduction;high parallelization;matrix reduction;bner bases computation;reduction matrix;special structure;reduction step;pivot column;bner basis;matrix transformation;bigger part;open source	Unfortunately the computation is time-and memory intensive. Mathematical knowledge is used to optimize the algorithms. Computer science provides another possibility to increase the computations: parallelization 2 / 24 Motivation Gröbner bases are used, to solve polynomial equation systems, move robotics, verify programs,. .. Unfortunately the computation is time-and memory intensive. Mathematical knowledge is used to optimize the algorithms. Computer science provides another possibility to increase the computations: parallelization 2 / 24 Motivation Gröbner bases are used, to solve polynomial equation systems, move robotics, verify programs,. .. Unfortunately the computation is time-and memory intensive. Mathematical knowledge is used to optimize the algorithms. Computer science provides another possibility to increase the computations: parallelization 2 / 24	algebraic equation;algorithm;computation;computer science;gröbner basis;parallel computing;polynomial;robotics	Severin Neumann	2012		10.1007/978-3-642-32973-9_22	discrete mathematics;theoretical computer science;mathematics;algorithm	Theory	3.234927960092393	39.63069892579386	148051
a6474e3ef790f6364f58a76e4dd55f0b6167461c	spark job performance analysis and prediction tool		Spark is one of most widely deployed in-memory big data technology for parallel data processing across cluster of machines. The availability of these big data platforms on commodity machines has raised the challenge of assuring performance of applications with increase in data size. We have build a tool to assist application developer and tester to estimate an application execution time for larger data size before deployment. Conversely, the tool may also be used to estimate the competent cluster size for desired application performance in production environment. The tool can be used for detailed profiling of Spark job, post execution, to understand performance bottleneck. This tool incorporates different configurations of Spark cluster to estimate application performance. Therefore, it can also be used with optimization techniques to get tuned value of Spark parameters for an optimal performance. The tool's key innovations are support for different configurations of Spark platform for performance prediction and simulator to estimate Spark stage execution time which includes task execution variability due to HDFS, data skew and cluster nodes heterogeneity. The tool using model [3] has been shown to predict within 20% error bound for Wordcount, Terasort,Kmeans and few SQL workloads.	apache hadoop;big data;deployment environment;heart rate variability;in-memory database;k-means clustering;mathematical optimization;performance prediction;profiling (computer programming);run time (program lifecycle phase);spark;sql;simulation;software deployment	Rekha Singhal;Chetan Phalak;Praveen Singh	2018		10.1145/3185768.3185772	performance prediction;software deployment;real-time computing;sql;profiling (computer programming);control engineering;k-means clustering;big data;engineering;spark (mathematics);bottleneck	HPC	-4.232999376537987	45.24313319495492	148182
7a4dac8e15f335ddbf8b15f07cc19f7feb8ad007	automated generation of hdl implementations of dadda and wallace tree multipliers		Convolutional Neural Networks are being studied to provide features such as real time image recognition. One of the key operations to support HW implementations of this type of network is the multiplication. Despite the high number of operations required by Convolutional Neural Networks, they became feasible in the past years due the high availability of computing power, present on devices such as Graphic Processing Units. However, those implementations are unsuitable for energy constrained scenarios, such as embedded devices. FPGAs are programmable devices that are being considered as a low power alternative for GPUs. This work proposes and implement a generator of two fast combinatorial multipliers: Dadda and Wallace tree. Our generator is capable of generating structural descriptions of both designs for any operand width, an operation considered unfeasible by hand. We evaluated our generator using two low-cost FPGA platforms, easily found on the market.	64-bit computing;computer vision;convolutional neural network;embedded system;field-programmable gate array;graphical user interface;graphics processing unit;hardware description language;high availability;neural networks;open-source software;operand;parallel computing;systemc;systemverilog;vhdl;verilog;wallace tree	Lucas Gaia de Castro;Henrique Seiti Ogawa;Bruno de Carvalho Albertini	2017	2017 VII Brazilian Symposium on Computing Systems Engineering (SBESC)	10.1109/SBESC.2017.9	parallel computing;convolutional neural network;implementation;operand;field-programmable gate array;wallace tree;high availability;logic gate;multiplication;algorithm;computer science	Arch	4.449982961038662	45.49084797231878	148226
3936cf1205e52f2868c0d140f0bc580f6e79cb8b	parallelizing a global optimization method in a distributed-memory environment	distributed memory;distributed memory systems;seismology;mining;optimal method;geophysics computing genetic algorithms mining distributed memory systems workstation clusters seismology;master slave model global optimization method parallelisation distributed memory environment genetic algorithms coal mine tremor sources s p method tremor hypocenter localisation error function minimisation coal mining seismometers sequential zone parallel method parallel zone parallel method local area network sun ultra workstations island model;optimization methods genetic algorithms computer science geology local area networks upper bound acceleration tunneling simulated annealing;parallel models;geophysics computing;genetic algorithm;genetic algorithms;global optimization;workstation clusters;local area network;coal mining	We present research into parallelizing the zone-parallel method of global optimization. The method belongs to the class of genetic algorithms and is briefly described in the paper. Upon introduction to genetic algorithms, parallelization models for genetic algorithms are presented. The subsequent part of the paper is devoted to the global optimization problem of finding sources of tremors in coal mines. First, a short description of the S-P method for localizing hypocenters of tremors is given; the method requires minimizing the error function for hypocenter location. Next, a practical coal-mining example is given where data on a tremor are collected by seismometers and the location of the hypocenter is found by employing the zone parallel method. Experimental results are presented which were obtained from implementing both the sequential and parallel versions of the zone-parallel method in a local area network of Sun Ultra workstations. The results show suitability of the island model of parallelization for this optimization method, as well as disproving usefulness of the master-slave model.	automatic parallelization;computation;distributed memory;genetic algorithm;global optimization;master/slave (technology);mathematical optimization;optimization problem;parallel computing;sequential algorithm;supercomputer;workstation	Zdzislaw Szczerbinski;Stanislaw Kowalik	2000		10.1109/EMPDP.2000.823390	parallel computing;real-time computing;engineering;theoretical computer science	AI	-3.354791579319821	35.30723230448449	148253
8549b56243c5093a4060ea488d363f65fbdb0246	art-directable continuous dynamic range video	continuous dynamic range;dynamic range mapping;hdr;lumipath	We present a novel, end-to-end workflow for content creation and distribution to a multitude of displays that have different dynamic ranges. The emergence of new, consumer level HDR displays with various peak luminances expected in 2015 gives rise to two new research questions: (i) how can the raw source content be graded for a diverse set of displays both efficiently and without restricting artistic freedom, and (ii) how can an arbitrary number of graded video streams be represented and encoded in an efficient way. In this work we propose a new editing paradigm which we call dynamic range mapping to obtain a novel Continuous Dynamic Range (CDR) video representation, where the luminance of the video content, instead of being a scalar value, is defined as a continuous function of the display dynamic range. We present an interactive interface where CDR videos can be efficiently created while providing full artistic control. In addition, we discuss the efficient approximation of CDR video using a polynomial series approximation, and its encoding and distribution to an arbitrary set of target displays. We validate our workflow in a subjective study, which suggests that a visually lossless CDR video representation can be achieved with little bandwidth overhead. Our solution can be implemented easily in the current distribution infrastructure and consists of transmitting two gradings and an additional meta-data stream, which occupies less than 13% current standard video distribution bandwidth.	approximation;digital video;dynamic range;emergence;end-to-end principle;lossless compression;overhead (computing);polynomial;programming paradigm;streaming media;transmitter;user interface	Alexandre Chapiro;Tunç Ozan Aydin;Nikolce Stefanoski;Simone Croci;Aljoscha Smolic;Markus H. Gross	2015	Computers & Graphics	10.1016/j.cag.2015.08.006	computer vision;real-time computing;computer science;operating system;video tracking;multimedia;computer graphics (images)	HCI	6.261822303063528	33.621715508040296	148261
b911ae5d464884478c9e0a43b6d3d245615021f1	evaluation of dgemm implementation on intel xeon phi coprocessor	linear algebra;paper;high performance computing;performance;intel math kernel library;performance measurements;algorithms;matrix multiplication;computer science;matrix matrix multiplication;intel xeon phi coprocessor;intel xeon phi	"""In this paper we will present a detailed study of implementing double-precision matrix-matrix multiplication (DGEMM) utilizing the Intel Xeon Phi Coprocessor. We discuss a DGEMM algorithm implementation running """"natively"""" on the coprocessor, minimizing communication with the host CPU. We will run DGEMM across a range of matrix sizes natively as well using Intel Math Kernel Library. Our optimizations were designed to support maximal reuse of on-die cache, which significantly reduces transfer from GDDR. Finally we analyze the improvement of a classic matrix multiplication implementation based on Cauchy algorithm compared to the latest results achieved using the Intel Math Kernel Library DGEMM subroutine."""	algorithm;automatic vectorization;blas;brute-force attack;cpu cache;central processing unit;compiler;coprocessor;double-precision floating-point format;gddr sdram;math kernel library;mathematical optimization;matrix multiplication;maximal set;scalability;subroutine;the matrix;thread (computing);xeon phi	Pawel Gepner;Victor Gamayunov;David L. Fraser;Eric Houdard;Ludovic Sauge;Damien Déclat;Mathieu Dubois	2014	JCP	10.4304/jcp.9.7.1566-1571	computer architecture;parallel computing;performance;matrix multiplication;computer science;linear algebra;operating system;xeon phi;coprocessor	HPC	-3.5119100638944514	41.313056594186776	148271
11ec50cdeb207036daf074c70bea9f73cf82ae29	an optimized parallel failure-less aho-corasick algorithm for dna sequence matching		The Aho-Corasick algorithm is a multiple patterns searching algorithm running sequentially in various applications like network intrusion detection and bioinformatics for finding several input strings within a given large input string. The parallel version of the Aho-Corasick algorithm is called as Parallel Failure-less Aho-Corasick algorithm because it doesnt need failure links like in the original Aho-Corasick algorithm. In this research, we implemented an application specific parallel failureless Aho-Corasick algorithm on the general purpose graphic processing unit by applying several cache optimization techniques for matching DNA sequences. Our parallel Aho-Corasick algorithm shows better performance than the available parallel Aho-Corasick algorithm library due to its simplicity and optimized cache memory usage of graphic processing units for matching DNA sequences.		Vajira Thambawita;Roshan G. Ragel;Dhammika Elkaduwe	2016	2016 IEEE International Conference on Information and Automation for Sustainability (ICIAfS)	10.1109/ICIAFS.2016.7946533	computer science;parallel computing;dinic's algorithm;commentz-walter algorithm;algorithm design;parallel algorithm;cache-oblivious algorithm;cost efficiency;fsa-red algorithm;search algorithm	HPC	-0.024019723024552617	41.14806145879931	148282
2798b44439e23d0a83a1806996b524d09b977fd3	hotspot thermal floorplan solver using conjugate gradient to speed up		We proposed to use the conjugate gradient method to effectively solve the thermal resistance model in HotSpot thermal floorplan tool. /e iterative conjugate gradient solver is suitable for traditional sparse matrix linear systems. We also defined the relative sparse matrix in the iterative thermal floorplan of Simulated Annealing framework algorithm, and the iterative method of relative sparse matrix could be applied to other iterative framework algorithms. /e experimental results show that the running time of our incremental iterative conjugate gradient solver is speeded up approximately 11x compared with the LU decomposition method for case ami49, and the experiment ratio curve shows that our iterative conjugate gradient solver accelerated more with increasing number of modules.		Zhonghua Jiang;Ning Xu	2018	Mobile Information Systems	10.1155/2018/2921451	mathematical optimization;iterative method;computer science;sparse matrix;distributed computing;simulated annealing;floorplan;linear system;lu decomposition;conjugate gradient method;solver	AI	-2.6962771511416643	38.201530414004935	148316
9eae10f218f42c8883e8d82d727413fb7d22f329	fpga based eigenfiltering for real-time portfolio risk analysis	eigenvalues and eigenfunctions;graphics processing unit fpga based eigenfiltering real time portfolio risk analysis empirical correlation matrix asset returns investment portfolio market microstructure jacobi algorithm eigensolver method parallel ja noise filtering cpu implementations gpu implementations high performance dsp technologies real time risk management field programmable gate array;risk analysis;risk analysis eigenvalues and eigenfunctions field programmable gate arrays graphics processing units investment matrix algebra parallel algorithms;matrix algebra;investment;graphics processing units;jacobian matrices field programmable gate arrays graphics processing units portfolios vectors correlation;field programmable gate arrays;parallel algorithms	The empirical correlation matrix of asset returns in an investment portfolio has its built-in noise due to market microstructure. This noise is usually eigenfiltered for robust risk analysis and management. Jacobi algorithm (JA) has been a popular eigensolver method due to its stability and efficient implementations. We present a fast FPGA implementation of parallel JA for noise filtering of empirical correlation matrix. Proposed FPGA implementation is compared with CPU and GPU implementations. It is shown that FPGA implementation of eigenfiltering operator in real-time significantly outperforms the others. We expect to see such emerging high performance DSP technologies to be widely used by the financial sector for real-time risk management and other tasks in the coming years.	canonical account;central processing unit;eigenvalue algorithm;field-programmable gate array;graphics processing unit;jacobi method;real-time clock;real-time transcription;risk management	Mustafa U. Torun;Onur Yilmaz;Ali N. Akansu	2013	2013 IEEE International Conference on Acoustics, Speech and Signal Processing	10.1109/ICASSP.2013.6639370	parallel computing;real-time computing;risk analysis;investment;computer science;theoretical computer science	Embedded	-1.9939478266515718	40.93530159708524	148579
18f3cdf0cb2ae613556adf60003b77416d9375b6	memory size computation for multimedia processing applications	symmetry constrain;estimation method;layout automation;real time;algebraic techniques;multimedia systems real time systems energy consumption costs process design design optimization memory architecture multimedia computing data analysis computer applications;scalar signals;multimedia processing;device merging;multimedia computing;data storage;real time multimedia algorithms;algebra;signal path;memory architecture;analog placement;data flow analysis;multimedia computing algebra data flow analysis digital storage memory architecture;scalar signals multimedia processing memory size real time multimedia algorithms algebraic techniques data flow analysis;digital storage;power consumption;circuit partition;memories;memory size;data transfer	In real-time multimedia processing systems a very large part of the power consumption is due to the data storage and data transfer. Moreover, the area cost is often largely dominated by the memory modules. The computation of the memory size is an important step in the process of designing an optimized (for area and/or power) memory architecture for multimedia processing systems. This paper presents a novel non-scalar approach for computing exactly the memory size in real-time multimedia algorithms. This methodology uses both algebraic techniques specific to the data-flow analysis used in modern compilers, and also recent advances in the theory of integral polyhedra. In contrast with all the previous works which are only estimation methods, this approach performs exact memory computations even for applications with a large number of scalar signals.	algorithm;compiler;computation;computer data storage;dimm;data-flow analysis;dataflow;linear algebra;polyhedron;real-time clock;real-time transcription	Hongwei Zhu;Ilie I. Luican;Florin Balasa	2006	Asia and South Pacific Conference on Design Automation, 2006.	10.1145/1118299.1118483	cuda pinned memory;uniform memory access;embedded system;electronic engineering;parallel computing;real-time computing;distributed memory;computer science;theoretical computer science;operating system;data-flow analysis;computer data storage;overlay;flat memory model;memory;computing with memory	EDA	5.506813169067181	46.306388467519426	148597
17e925bf060ac5cdb11c5a7b8fb40ed83919539e	mapping affine loop nests: solving of the alignment and scheduling problems	linear algebra;parallelisme;distributed memory;alignement;transformation affine;distribution donnee;memoria compartida;ecuacion algebraica;distributed computing;ecuacion lineal;data distribution;parallel computation;parallelism;calculo paralelo;paralelismo;boucle imbriquee;scheduling;affine transformation;alineamiento;bucle imbricado;parallel computer;scheduling problem;calculo repartido;procesador oleoducto;equation algebrique;linear equation;memoire repartie;processeur pipeline;calcul parallele;algebraic equation;distribucion dato;calcul reparti;alignment;nested loop;ordonnancement;transformacion afin;equation lineaire;pipeline processor;reglamento	The paper is devoted to the problem of mapping affine loop nests onto distributed memory parallel computers. An algorithm to find an efficient scheduling and distribution of data and operations to virtual processors is presented. It reduces the sheduling and the alignment problems to the solving of linear algebraic equations. The algorithm finds the maximal degree of pipelined parallelism and tries to minimize the number of nonlocal communications.	scheduling (computing)	Evgeniya V. Adutskevich;Nickolai A. Likhoded	2003		10.1007/978-3-540-45145-7_1	algebraic equation;combinatorics;discrete mathematics;parallel computing;distributed memory;nested loop join;computer science;theoretical computer science;linear algebra;affine transformation;mathematics;distributed computing;linear equation;scheduling;algorithm;algebra	PL	-2.340125737787362	36.87032063373544	148734
c1e406f7d87f278477e8b9beb98894f536a5992e	towards a hpc-oriented parallel implementation of a learning algorithm for bioinformatics applications	software;animals;databases nucleic acid;computational biology bioinformatics;algorithms;humans;europe;combinatorial libraries;computational biology;computer appl in life sciences;computing methodologies;microarrays;bioinformatics	The huge quantity of data produced in Biomedical research needs sophisticated algorithmic methodologies for its storage, analysis, and processing. High Performance Computing (HPC) appears as a magic bullet in this challenge. However, several hard to solve parallelization and load balancing problems arise in this context. Here we discuss the HPC-oriented implementation of a general purpose learning algorithm, originally conceived for DNA analysis and recently extended to treat uncertainty on data (U-BRAIN). The U-BRAIN algorithm is a learning algorithm that finds a Boolean formula in disjunctive normal form (DNF), of approximately minimum complexity, that is consistent with a set of data (instances) which may have missing bits. The conjunctive terms of the formula are computed in an iterative way by identifying, from the given data, a family of sets of conditions that must be satisfied by all the positive instances and violated by all the negative ones; such conditions allow the computation of a set of coefficients (relevances) for each attribute (literal), that form a probability distribution, allowing the selection of the term literals. The great versatility that characterizes it, makes U-BRAIN applicable in many of the fields in which there are data to be analyzed. However the memory and the execution time required by the running are of O(n3) and of O(n5) order, respectively, and so, the algorithm is unaffordable for huge data sets. We find mathematical and programming solutions able to lead us towards the implementation of the algorithm U-BRAIN on parallel computers. First we give a Dynamic Programming model of the U-BRAIN algorithm, then we minimize the representation of the relevances. When the data are of great size we are forced to use the mass memory, and depending on where the data are actually stored, the access times can be quite different. According to the evaluation of algorithmic efficiency based on the Disk Model, in order to reduce the costs of the communications between different memories (RAM, Cache, Mass, Virtual) and to achieve efficient I/O performance, we design a mass storage structure able to access its data with a high degree of temporal and spatial locality. Then we develop a parallel implementation of the algorithm. We model it as a SPMD system together to a Message-Passing Programming Paradigm. Here, we adopt the high-level message-passing systems MPI (Message Passing Interface) in the version for the Java programming language, MPJ. The parallel processing is organized into four stages: partitioning, communication, agglomeration and mapping. The decomposition of the U-BRAIN algorithm determines the necessity of a communication protocol design among the processors involved. Efficient synchronization design is also discussed. In the context of a collaboration between public and private institutions, the parallel model of U-BRAIN has been implemented and tested on the INTEL XEON E7xxx and E5xxx family of the CRESCO structure of Italian National Agency for New Technologies, Energy and Sustainable Economic Development (ENEA), developed in the framework of the European Grid Infrastructure (EGI), a series of efforts to provide access to high-throughput computing resources across Europe using grid computing techniques. The implementation is able to minimize both the memory space and the execution time. The test data used in this study are IPDATA (Irvine Primate splice- junction DATA set), a subset of HS3D (Homo Sapiens Splice Sites Dataset) and a subset of COSMIC (the Catalogue of Somatic Mutations in Cancer). The execution time and the speed-up on IPDATA reach the best values within about 90 processors. Then the parallelization advantage is balanced by the greater cost of non-local communications between the processors. A similar behaviour is evident on HS3D, but at a greater number of processors, so evidencing the direct relationship between data size and parallelization gain. This behaviour is confirmed on COSMIC. Overall, the results obtained show that the parallel version is up to 30 times faster than the serial one.	algorithm;algorithmic efficiency;bioinformatics;boolean;brain neoplasms;cosmic;central processing unit;coefficient;communications protocol;computation (action);computer;computers;dspace;disjunctive normal form;dynamic programming;egi;economic development;equilibrium;grid computing;high- and low-level;high-throughput computing;input/output;iterative method;java;list of intel core 2 microprocessors;literal (mathematical logic);load balancing (computing);locality of reference;mass effect trilogy;mass storage;mathematics;message passing interface;parallel computing;programming language;programming model;programming paradigm;rafivirumab;random access memory device component;random-access memory;run time (program lifecycle phase);spmd;silo (dataset);solutions;splice (system call);subgroup;test data;throughput	Gianni D'Angelo;Salvatore Rampone	2014		10.1186/1471-2105-15-S5-S2	biology;dna microarray;computer science;bioinformatics;theoretical computer science;algorithm	HPC	-0.9867150656077374	43.73970029425937	148800
98152f5c17c3e0ae3f98ba4dce3a0823b258456a	block-parallel ida* for gpus (extended manuscript)		We investigate GPU-based parallelization of IterativeDeepening A* (IDA*). We show that straightforward thread-based parallelization techniques which were previously proposed for massively parallel SIMD processors perform poorly due to warp divergence and load imbalance. We propose Block-Parallel IDA* (BPIDA*), which assigns the search of a subtree to a block (a group of threads with access to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA* on a NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to a highly optimized sequential IDA* implementation on a Xeon E5-2670 core. 1	15 puzzle;cuda;central processing unit;graphics processing unit;iterative deepening a*;parallel computing;simd;shared memory;speedup;tree (data structure)	Satoru Horie;Alex S. Fukunaga	2017	CoRR		parallel computing;computer hardware;computer science;operating system	HPC	-1.3541510223738524	41.328058182444245	148897
90a8269dc3ee4d4e8ec221b0e0f4d0d14c1851f8	occupancy detection using gas sensors		Room occupancy is an important variable in high performance building management. Presence of people is usually detected by dedicated sensing systems. The most popular ones exploit physical phenomena. Such sensing solutions include passive infrared motion detectors, magnetic reed switches, ultrasonic, microwave and audible sensors, video cameras and radio-frequency identification. However, in most cases either human movement is needed to succeed in detection or privacy issues are involved. In this work, we studied occupancy detection using chemical sensors. In this case, the basis for detecting human presence indoors is their influence of chemical composition of air. Movement of people is not needed to succeed and privacy of occupants is secured. The approach was reported effective when using carbon dioxide, which is one of major human metabolites. We focused on volatile organic compounds (VOCs). Their consideration is justified because numerous human effluents belong to this group. The analysis showed that VOCs’ sensors, such as semiconductor gas sensors, offer comparable occupancy detection accuracy (97.16 %) as nondispersive infrared sensor (NDIR) (97.36 %), which is considered as the benchmark. In view of our results, semiconductor gas sensors are interesting candidates for nodes of sensor nets dedicated to detection of human presence indoors. They are smaller, cheaper and consume less energy.	benchmark (computing);heart rate variability;information;maxima and minima;microwave;motion detector;network switch;non-volatile memory;pid;population;privacy;radio frequency;radio-frequency identification;semiconductor;sensor;time series	Andrzej Szczurek;Monika Maciejewska;Tomasz Pietrucha	2017		10.5220/0006207100990107	occupancy;computer science;distributed computing	Mobile	3.4323159975569695	33.44787740993038	149092
20047b9802081cc6be5fb7992887f74e032d7e1c	automatic generation of high-performance multipliers for fpgas with asymmetric multiplier blocks	composable multipliers;automatic generation;asymmetric multipliers;high performance;multiplier design	The introduction of asymmetric embedded multiplier blocks in recent Xilinx FPGAs complicates the design of larger multiplier sizes. The two different input bitwidths of the embedded multipliers lead to two different shifting factors for the partial product outputs. This makes even the most straightforward multiplier design less intuitive. In this paper, we present a methodology and set of equations to automatically generate the Verilog for a multiplier using asymmetric embedded multiplier cores. The presented technique also uses intelligent rearrangement of the multiplier block outputs into partial product terms to reduce the overall delay of the circuit. Multipliers created with our generator are faster and use fewer DSP blocks than those created using Xilinx Core Generator or by simply using the '*' operator in Verilog. It also uses fewer LUTs than those created using the '*' operator. Finally, the presented generator can create multipliers larger than possible with Core Generator, and is limited only by the number of available embedded multipliers.	embedded system;field-programmable gate array;lagrange multiplier;verilog	Shreesha Srinath;Katherine Compton	2010		10.1145/1723112.1723123	parallel computing;computer hardware	EDA	6.97230743515065	46.29190811966625	149942
6990ae2f00153ae64b7bf3089b0a103d30b00722	karatsuba-like formulae and their associated techniques		Efficient polynomial multiplication formulae are required for cryptographic computation. From elliptic curve cryptography to homomorphic encryption, many cryptographic systems need efficient multiplication formulae. The most widely used multiplication formulae for cryptographic systems are the Karatsuba-like polynomial multiplication formulae. In this paper, these formulae and Montgomery’s work yielding more efficient such formulae are introduced. Moreover, recent efforts to improve these results are discussed by presenting associated techniques. The state of art for this area is also discussed.	computation;elliptic curve cryptography;homomorphic encryption;karatsuba algorithm;montgomery modular multiplication;polynomial ring	Murat Cenk	2017	Journal of Cryptographic Engineering	10.1007/s13389-017-0155-8	arithmetic;discrete mathematics;mathematics;algebra	Crypto	9.273392880009963	43.1024904783704	150226
ac24291dcba69f1ea76e1d03dd1cc0d2c46523f3	neuromorphic processing: a new frontier in scaling computer architecture	parallel computing;spiking neural network neural network;neuromorphic;spiking neural network;low power;zeroth;neural network	The desire to build a computer that operates in the same manner as our brains is as old as the computer itself. Although computer engineering has made great strides in hardware performance as a result of Dennard scaling, and even great advances in 'brain like' computation, the field still struggles to move beyond sequential, analytical computing architectures. Neuromorphic systems are being developed to transcend the barriers imposed by silicon power consumption, develop new algorithms that help machines achieve cognitive behaviors, and both exploit and enable further research in neuroscience. In this talk I will discuss a system im-plementing spiking neural networks. These systems hold the promise of an architecture that is event based, broad and shallow, and thus more power efficient than conventional computing solu-tions. This new approach to computation based on modeling the brain and its simple but highly connected units presents a host of new challenges. Hardware faces tradeoffs such as density or lower power at the cost of high interconnection overhead. Consequently, software systems must face choices about new language design. Highly distributed hardware systems require complex place and route algorithms to distribute the execution of the neural network across a large number of highly interconnected processing units. Finally, the overall design, simulation and testing process has to be entirely reimagined. We discuss these issues in the context of the Zeroth processor and how this approach compares to other neuromorphic systems that are becoming available.	algorithm;artificial neural network;computation;computer architecture;computer engineering;dennard scaling;image scaling;interconnection;neuromorphic engineering;overhead (computing);place and route;simulation;software system;spiking neural network	Jeff Gehlhaar	2014		10.1145/2541940.2564710	computer science;artificial intelligence;theoretical computer science;zeroth law of thermodynamics;distributed computing;neuromorphic engineering;spiking neural network	Arch	4.417074786240786	41.49976048380229	150406
b399a52c88b6cc08bd020adb4062e54970bf7cc6	efficient deep neural network acceleration through fpga-based batch processing		Deep neural networks are an extremely successful and widely used technique for various pattern recognition and machine learning tasks. Due to power and resource constraints, these computationally intensive networks are difficult to implement in embedded systems. Yet, the number of applications that can benefit from the mentioned possibilities is rapidly rising. In this paper, we propose a novel architecture for processing previously learned and arbitrary deep neural networks on FPGA-based SoCs that is able to overcome these limitations. A key contribution of our approach, which we refer to as batch processing, achieves a mitigation of required weight matrix transfers from external memory by reusing weights across multiple input samples. This technique combined with a sophisticated pipelining and the usage of high performance interfaces accelerates the data processing compared to existing approaches on the same FPGA device by one order of magnitude. Furthermore, we achieve a comparable data throughput as a fully featured x86-based system at only a fraction of its energy consumption.	activation function;artificial neural network;batch processing;central processing unit;deep learning;embedded system;fastest;field-programmable gate array;machine learning;memory controller;multiplexing;pattern recognition;pipeline (computing);system on a chip;throughput;x86	Thorbjörn Posewsky;Daniel Ziener	2016	2016 International Conference on ReConFigurable Computing and FPGAs (ReConFig)	10.1109/ReConFig.2016.7857167	embedded system;parallel computing;real-time computing;computer science;theoretical computer science;operating system	EDA	3.6414465840709584	43.025395605225015	150420
37436ecb5e4386e68134c50711c806a20706136e	is intel high performance analytics toolkit a good alternative to apache spark?		This paper compares the performance and stability of two Big Data processing tools: the Apache Spark and the High Performance Analytics Toolkit (HPAT). The comparison was performed using two applications: a unidimensional vector sum and the K-means clustering algorithm. The experiments were performed in distributed and shared memory environments with different numbers and configurations of virtual machines. By analyzing the results we are able to conclude that HPAT has performance improvements in relation to Apache Spark in our case studies. We independently validated the results and potential presented by the HPAT developers. We also provide an analysis of both frameworks in the presence of failures.	algorithm;apache spark;big data;cluster analysis;experiment;k-means clustering;shared memory;virtual machine	Rafael Aquino de Carvalho;Alfredo Goldman;Gerson Geraldo H. Cavalheiro	2017	2017 IEEE 16th International Symposium on Network Computing and Applications (NCA)	10.1109/NCA.2017.8171351	machine learning;computer science;artificial intelligence;spark (mathematics);big data;cluster analysis;virtual machine;shared memory;analytics	HPC	-4.327675595422968	45.0946549003484	150454
13d79f13e5dc04cc2a1064acb801c76f609e0b25	revisiting fpga acceleration of molecular dynamics simulation with dynamic data flow behavior in high-level synthesis		Molecular dynamics (MD) simulation is one of the past decade’s most important tools for enabling biology scientists and researchers to explore human health and diseases. However, due to the computation complexity of the MD algorithm, it takes weeks or even months to simulate a comparatively simple biology entity on conventional multicore processors. The critical path in molecular dynamics simulations is the force calculation between particles inside the simulated environment, which has abundant parallelism. Among various acceleration platforms, FPGA is an attractive alternative because of its low power and high energy efficiency. However, due to its high programming cost using RTL, none of the mainstream MD software packages has yet adopted FPGA for acceleration. In this paper we revisit the FPGA acceleration of MD in high-level synthesis (HLS) so as to provide affordable programming cost. Our experience with the MD acceleration demonstrates that HLS optimizations such as loop pipelining, module duplication and memory partitioning are essential to improve the performance, achieving a speedup of 9.5X compared to a 12-core CPU. More importantly, we observe that even the fully optimized HLS design can still be 2X slower than the reference RTL architecture due to the common dynamic (conditional) data flow behavior that is not yet supported by current HLS tools. To support such behavior, we further customize an array of processing elements together with a datadriven streaming network through a common RTL template, and fully automate the design flow. Our final experimental results demonstrate a 19.4X performance speedup and 39X energy efficiency for the widely used ApoA1 MD benchmark on the Convey HC1ex FPGA compared to a 12-core Intel Xeon server.	algorithm;benchmark (computing);central processing unit;computation;critical path method;dataflow architecture;dynamic data;electronic design automation;field-programmable gate array;high- and low-level;high-level synthesis;molecular dynamics;multi-core processor;parallel computing;pipeline (computing);server (computing);simulation;speedup;virtual reality	Jason Cong;Zhenman Fang;Hassan Kianinejad;Peng Wei	2016	CoRR		parallel computing;real-time computing;computer hardware;computer science	EDA	-0.08124702999498841	45.26157753799221	150576
49880a708ea3c2bf4b53f328c8e2de74affd96f9	4.6 a 65nm cmos 6.4-to-29.2pj/flop@0.8v shared logarithmic floating point unit for acceleration of nonlinear function kernels in a tightly coupled processor cluster	energy efficiency;cmos integrated circuits;kernel;multiprocessing systems cmos integrated circuits fixed point arithmetic floating point arithmetic logic design microprocessor chips;voltage 0 8 v cmos flop logarithmic floating point unit nonlinear function kernel tightly coupled processor cluster energy efficient computing ultra low power operation fixed point processor precision floating point logarithmic number system integer arithmetic units look up table lut interpolation floating point unit multicore system lnu design size 65 nm;energy efficiency kernel program processors table lookup benchmark testing cmos integrated circuits;table lookup;program processors;benchmark testing	Energy-efficient computing and ultra-low-power operation are requirements for many application areas, such as IoT and wearables. While for some applications, integer and fixed-point processor instructions suffice, others (e.g. simultaneous localization and mapping - SLAM, stereo vision, nonlinear regression and classification) require a larger dynamic range, typically obtained using single/double-precision floating point (FP) instructions. Logarithmic number systems (LNS) have been proposed [1,2] as an energy-efficient alternative to conventional FP, as several complex operations such as MUL, DIV, and EXP translate into simpler arithmetic operations in the logarithmic space and can be efficiently calculated using integer arithmetic units. However, ADD and SUB become nonlinear and have to be approximated by look-up tables (LUTs) and interpolation, which is typically implemented in a dedicated LNS unit (LNU) [1,2]. The area of LNUs grows exponentially with the desired precision, and an LNU with accuracy comparable to IEEE single-precision format is larger than a traditional floating-point unit (FPU). However, we show that in multi-core systems optimized for ultra-low-power operation such as the PULP system [3], one LNU can be efficiently shared in a cluster as indicated in Fig. 4.6.1. This arrangement not only reduces the per-core area overhead, but more importantly, allows several costly operations such as FP MUL/DIV to be processed without contention within the integer cores without additional overhead. We show that for typical nonlinear processing tasks, our LNU design can be up to 4.2× more energy efficient than a private-FP design.	approximation algorithm;arithmetic logic unit;cmos;core (optical fiber);double-precision floating-point format;dynamic range;exptime;floating-point unit;interpolation;l (complexity);logarithmic number system;lookup table;low-power broadcasting;multi-core processor;multiprocessing;nl (complexity);nonlinear system;overhead (computing);requirement;simultaneous localization and mapping;single-precision floating-point format;stereopsis;wearable computer	Michael Gautschi;Michael Schaffner;Frank K. Gürkaynak;Luca Benini	2016	2016 IEEE International Solid-State Circuits Conference (ISSCC)	10.1109/ISSCC.2016.7417917	floating-point unit;embedded system;benchmark;electronic engineering;parallel computing;kernel;computer hardware;computer science;operating system;efficient energy use;cmos	EDA	5.421203909485024	43.385657399015805	150797
47285d15fee59f0a3ac1baf3946ba92970f7f0ae	implementation of elliptic curve cryptosystems on a reconfigurable computer	field programmable gate array;elliptic curve cryptography microprocessors field programmable gate arrays prototypes workstations elliptic curves hardware computer architecture partitioning algorithms application software;elliptic curve;reconfigurable computing;reconfigurable architectures;hardware description languages;performance comparison;hardware architecture;optimized microprocessor elliptic curve cryptosystems reconfigurable computer microprocessors field programmable gate arrays fpga computationally intensive problems codebreaking elliptic curve scalar multiplication hardware architecture algorithm partitioning strategy data transfer reconfiguration overheads;programming model;efficient implementation;cryptography;field programmable gate arrays;hardware description languages reconfigurable architectures microprocessor chips field programmable gate arrays cryptography;galois field;elliptic curve cryptosystem;scalar multiplication;data transfer;microprocessor chips	During the last few years, a considerable effort has been devoted to the development of reconfigurable computers, machines that are based on the close interoperation of traditional microprocessors and Field Programmable Gate Arrays (FPGAs). Several prototype machines of this type have been designed, and demonstrated significant speedups compared to conventional workstations for computationally intensive problems, such as codebreaking. Nevertheless, the efficient use and programming of such machines is still an unresolved problem. In this paper, we demonstrate an efficient implementation of an Elliptic Curve scalar multiplication over GF(2), using one of the leading reconfigurable computers available on the market, SRC-6E. We show how the hardware architecture and programming model of this reconfigurable computer has influenced the choice of the algorithm partitioning strategy for this application. A detailed analysis of the control, data transfer, and reconfiguration overheads is given in the paper, together with the performance comparison of our implementation against an optimized microprocessor implementation.	algorithm;computer;cryptanalysis;cryptosystem;field-programmable gate array;interoperation;microprocessor;programming model;prototype;reconfigurable computing;workstation	Nghi Nguyen;Kris Gaj;David Caliga;Tarek A. El-Ghazawi	2003		10.1109/FPT.2003.1275732	embedded system;computer architecture;parallel computing;computer science;theoretical computer science;hardware architecture;field-programmable gate array	Arch	8.40137611711586	44.61370167286187	150811
3e6e05eed87c5796cd676a81310494b4378a58a4	implementation of the six channel redundancy to achieve fault tolerance in testing of satellites	fault tolerant;spectrum;sound pressure level;data acquisition	-This paper aims to implement the six channel redundancy to achieve fault tolerance in testing of satellites with acoustic spectrum. We mainly focus here on achieving fault tolerance. An immediate application is the microphone data acquisition and to do analysis at the Acoustic Test Facility (ATF) centre, National Aerospace Laboratories. It has an 1100 cubic meter reverberation chamber in which a maximum sound pressure level of 157 dB is generated. The six channel Redundancy software with fault tolerant operation is devised and developed. The data are applied to program written in C language. The program is run using the Code Composer Studio by accepting the inputs. This is tested with the TMS 320C 6727 DSP, Pro Audio Development Kit (PADK).	acoustic cryptanalysis;code composer studio;cubic function;data acquisition;decibel;digital signal processor;fault tolerance;microphone	H. S. Aravinda;H. D. Maheshappa;Ranjan Moodithaya	2010	CoRR		embedded system;spectrum;fault tolerance;electronic engineering;real-time computing;sound pressure;telecommunications;engineering;electrical engineering;operating system;data acquisition;physics;software fault tolerance;quantum mechanics	SE	6.156710153845405	36.60877011504987	151439
17f7633bb9a5acb600a6c721c83e31cbeb2f3be3	a genetic exploration of dynamic load balancing algorithms	genetic search;dynamic load balancing;evolutionary computation;parallel matrices multiplication;matrix multiplication resource allocation genetic algorithms parallel processing;search space;resource allocation;space exploration;genetics;computer architecture;adaptive algorithm;heuristic algorithms;load management;evolutionary algorithms;clustering algorithms;genetic algorithm;genetic algorithms;genetic exploration;parallel matrices multiplication genetic exploration dynamic load balancing algorithms evolutionary algorithms search space genetic algorithm distributed load balancing parallel processing genetic search;matrix multiplication;load balance;space technology;load management heuristic algorithms space technology space exploration genetic algorithms algorithm design and analysis clustering algorithms adaptive algorithm computer architecture evolutionary computation;evolutionary algorithm;distributed load balancing;algorithm design and analysis;parallel applications;parallel processing;dynamic load balancing algorithms	Evolutionary algorithms provide ways to explore wide search spaces. Thus, it is possible to get some conclusions about the characteristics of these spaces in order to aid in the determination of the best alternatives to solve the problem at hand. We have applied a genetic algorithm to assess the problem of distributed load balancing in parallel processing. To do that, we propose a classification of the space of design of distributed load balancing algorithms that takes into account the different alternatives for each dimension of the algorithm. This classification allows the codification of each load balancing strategy, thus making possible to apply a genetic search to determine the distributed load balancing procedure that provides the best performance for the type of parallel application at hand and the parallel platform where it is implemented. As an example, in this paper we provide the results corresponding to the parallel multiplication of matrices implemented in a cluster.	evolutionary algorithm;genetic algorithm;load balancing (computing);parallel computing	Mohammed Aldasht;Julio Ortega;Carlos García Puntonet;Antonio F. Díaz	2004	Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)	10.1109/CEC.2004.1330992	mathematical optimization;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;distributed computing	HPC	4.093336799749449	39.16550136066979	151706
aa0b385f03d71d7c0c7d586f4c9ef418a0da6187	large-scale time-harmonic electromagnetic field analysis using a multigrid solver on a distributed memory parallel computer	multigrid method;parallel ordering;finite element method;electromagnetic field analysis;multiplicative schwarz smoother;hybrid parallel programming model	This paper reports on an investigation into large-scale parallel time-harmonic electromagnetic field analysis based on the finite element method. The parallel geometric multigrid preconditioned iterative solver for the resulting linear system was developed on a cluster of shared memory parallel computers. We propose a hybrid parallel ordering method for the parallelization of a multiplicative Schwarz smoother, which is a key component of the multigrid solver for electromagnetic field analysis. The method, using domain decomposition ordering for multi-process parallelism and introducing block multi-color ordering for multi-thread parallel processing, attains a high convergence rate with a small number of message passing interface communications and thread synchronizations. The numerical test confirms that the proposed method attains a solver performance more than twice as good as the conventional method based on multi-color ordering. Furthermore, an approximately 800 million degrees of freedom problem is successfully solved on 256 quad-core processors.	distributed memory;multigrid method;parallel computing;solver	Takeshi Iwashita;Yu Hirotani;Takeshi Mifune;Toshio Murayama;Hideki Ohtani	2012	Parallel Computing	10.1016/j.parco.2012.05.004	mathematical optimization;parallel computing;computer science;theoretical computer science;finite element method;algorithm;multigrid method	HPC	-3.7372214362453895	38.38850773130425	151743
410e6387a37b02aa75ffbc93582c9204bccaa105	paraspice: a parallel circuit simulator for shared-memory multiprocessors	linear systems;text;concurrent computing;performance evaluation;very large scale integration;benchmark circuits paraspice parallel circuit simulator shared memory multiprocessors parallel tasks algorithmic level compute intensive module spice2 performance;performance;parallel circuit simulator;spice2;compute intensive module;performance evaluation circuit analysis computing parallel algorithms;circuit simulation computational modeling linear systems parallel processing timing problem solving research and development concurrent computing circuit analysis very large scale integration;circuit simulation;computational modeling;circuit analysis;research and development;shared memory multiprocessors;benchmark circuits;intensity modulation;algorithmic level;paraspice;computer science;circuit analysis computing;engineering electronics and electrical;parallel processing;parallel tasks;direct method;problem solving;shared memory multiprocessor;parallel algorithms;timing	This paper presents a general approach to parallelizing direct method circuit simulation. The approach extracts parallel tasks at the algorithmic level for each compute-intensive module and therefore is suitable for a wide range of shared-memory multiprocessors. The implementation of the approach in SPICE2 resulted in a portable parallel direct circuit simulator, PARASPICE. The superior performance of PARASPICE is demonstrated on an 8-CE Alliant FX/80 using a number of benchmark circuits.	alliant computer systems;benchmark (computing);direct method in the calculus of variations;electronic circuit simulation;parallel computing;spice;shared memory	Gung-Chung Yang	1990		10.1145/123186.123318	direct method;parallel processing;computer architecture;parallel computing;concurrent computing;network analysis;performance;computer science;intensity modulation;theoretical computer science;parallel algorithm;very-large-scale integration;linear system;computational model	EDA	-0.7243459427561902	40.141139604352105	152321
1308b213dade3133153e1452e8c3f97d6e2932ac	auto-optimization of linear algebra parallel routines: the cholesky factorization	linear algebra;cholesky factorization	In last years different techniques to obtain software able to tune automatically to the conditions of the execution platform have been studied [1, 2, 3, 4]. Such development will facilitate efficient utilization of the routines by non-expert users, e.g. those normally using linear algebra routines in the solution of large scientific or engineering problems. In this paper we discuss the approach in [3] for modelling the behavior of linear algebra algorithms, with the aim of obtaining self-optimizing routines, and the technique is applied to the parallel routine for the Cholesky factorization. The factorization is obtained with a block-cyclic partitioning in a logical two-dimensional mesh of p = r × c processes (in ScaLAPACK style). An analytical model of the execution time of the parallel algorithm is developed as a function of the problem size, the system parameters (parameters of a target platform) and the algorithmic parameters. The algorithm is studied both theoretically and experimentally in order to determine the effect of the value of the system parameters on the algorithmic parameters. The typical system parameters considered in the study are the cost of arithmetic operations using BLAS kernels of levels 1, 2 or 3 (k1, k2, k3) and the cost of the communication parameters (start-up, ts, and word sending time, tw) for the used MPI library. The algorithmic parameters are the block size b (a block based algorithm is considered) and the parameters r and c defining the logical topology of the process grid and data distribution. In this paper we will see that in order to obtain a good estimation it is necessary to use different costs for different routines of the same level and to use different costs for different types of MPI communication mechanisms Servicio de Apoyo a la Investigación Tecnológica, Universidad Politécnica de Cartagena, 30203 Cartagena, SPAIN, luis.garcia@sait.upct.es Departamento de Ingenieŕıa y Tecnoloǵıa de Computadores, Universidad de Murcia, 30071 Murcia, SPAIN, javiercm@ditec.um.es Departamento de Informática y Sistemas, Universidad de Murcia, 30071 Murcia, SPAIN, domingo@dif.um.es	analysis of algorithms;blas;block size (cryptography);cholesky decomposition;experiment;linear algebra;logical topology;message passing interface;parallel algorithm;run time (program lifecycle phase);scalapack;separation kernel	Luis-Pedro García;Javier Cuenca;Domingo Giménez	2005			parallel computing;computer science;theoretical computer science;linear algebra;mathematics;cholesky decomposition;algorithm;algebra	HPC	-4.180291852630188	40.09009295871479	152419
046f7d6aec2320eb37ca1d5369a98870ad51a83a	perform-ml: performance optimized machine learning by platform and content aware customization	user defined error bounds performance optimized machine learning content aware customization perform ml iterative analysis algorithms flop parametric data projection algorithm elastic dictionary exd parameters platform aware tuning api dense data;neural networks;storage management application program interfaces learning artificial intelligence optimisation;neuromorphic computing;truenorth	We propose Perform-ML, the first Machine Learning (ML) framework for analysis of massive and dense data which customizes the algorithm to the underlying platform for the purpose of achieving optimized resource efficiency. Perform-ML creates a performance model quantifying the computational cost of iterative analysis algorithms on a pertinent platform in terms of FLOPs, communication, and memory, which characterize runtime, storage, and energy. The core of Perform-ML is a novel parametric data projection algorithm, called Elastic Dictionary (ExD), that enables versatile and sparse representations of the data which can help in minimizing performance cost. We show that Perform-ML can achieve the optimal performance objective, according to our cost model, by platform aware tuning of the ExD parameters. An accompanying API ensures automated applicability of Perform-ML to various algorithms, datasets, and platforms. Proof-of-concept evaluations of massive and dense data on different platforms demonstrate more than an order of magnitude improvements in performance compared to the state of the art, within guaranteed user-defined error bounds.	algorithm;algorithmic efficiency;analysis of algorithms;application programming interface;computation;dictionary;flops;iterative method;machine learning;relevance;sparse matrix	Azalia Mirhoseini;Bita Darvish Rouhani;Ebrahim M. Songhori;Farinaz Koushanfar	2016	2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2897937.2898060	embedded system;mathematical optimization;electronic engineering;real-time computing;computer science;theoretical computer science;operating system;machine learning;neuromorphic engineering;artificial neural network	EDA	-1.000843970100173	45.34795510567969	152586
a6113c75767d2f08d8d00c55214826475f63a8dd	hardware architecture for adaptive filtering based on energy-cfar processor for radar target detection	cfar algorithm;adaptive filtering;hardware architecture;target detection;adaptive filter	A hardware architecture that implements an adaptive filter based on energy analysis of radar echoes to improve the detection of the Constant False Alarm Rate (CFAR) algorithm is presented. Signal processing based on energy analysis emphasizes the edge of the echoes improving the performance of the detection process. The energy filter coefficients and CFAR parameters are calculated adaptively by the architecture, reconfiguring the block of coefficient weights according to environment conditions. The architecture accelerates the data processing by a pipeline structure and sliding window for the coefficients convolution with data, resulting in high performance operation. Results of implementing the architecture in a FPGA device are presented and discussed.	adaptive filter;algorithm;coefficient;constant false alarm rate;convolution;field-programmable gate array;institute of electronics, information and communication engineers;radar;signal processing	Santos López-Estrada;René Cumplido	2010	IEICE Electronic Express	10.1587/elex.7.628	adaptive filter;computer vision;electronic engineering;real-time computing;space-time adaptive processing;computer science;hardware architecture	HPC	9.20660702919731	41.25223947635243	153101
498bebf16f2d8e58a6fd1d85c1ca83ce29280f90	adaptive parallel householder bidiagonalization	matrix factorization;high resolution;singular value decomposition;multimedia application;multimedia data;high performance computer;automatic parallelization	With the increasing use of large image and video archives and high-resolution multimedia data streams in many of today’s research and application areas, there is a growing need for multimedia-oriented high-performance computing. As a consequence, a need for algorithms, methodologies, and tools that can serve as support in the (automatic) parallelization of multimedia applications is rapidly emerging. This paper discusses the parallelization of Householder bidiagonalization, a matrix factorization method which is an integral part of full Singular Value Decomposition (SVD) — an important algorithm for many multimedia problems. Householder bidiagonalization is hard to parallelize efficiently because the total number of matrix elements taking part in the calculations reduces during runtime. To overcome the growing negative performance impact of load imbalances and overprovisioning of compute resources, we apply adaptive runtime techniques of periodic matrix remapping and process reduction for improved performance. Results show that our adaptive parallel execution approach provides a significant improvement in efficiency, even when applying a set of compute resources which is (initially) very large.	algorithm;archive;automatic parallelization;bidiagonalization;data parallelism;floor and ceiling functions;householder transformation;image resolution;parallel computing;singular value decomposition;supercomputer	Fangbin Liu;Frank J. Seinstra	2009		10.1007/978-3-642-03869-3_76	parallel computing;image resolution;computer science;theoretical computer science;distributed computing;matrix decomposition;singular value decomposition;automatic parallelization	HPC	-2.328680718641779	38.832965116573675	153116
e4d3e5f8e8490787aa87a5045e55484abb771b5f	content-aware line-based power modeling methodology for image signal processor		Early power modeling and analysis using electronic system-level methodology enables designers to explore energy saving opportunities more efficiently at a higher abstraction level. However, power modeling for third party IPs are challenging due to the limited observability and unknown architecture details. To model the data dependency for blackbox IPs, several works rely on adopting Hamming distance of input data to approximate the switching activity, which might be not enough for modeling complex IPs such as image signal processors (ISP). This work introduces a content-aware line-based power modeling method for ISP by training an associated energy table. To effectively estimate ISP energy consumption which involves many two-dimensional data processing, this work presents a direct energy-mapping strategy using pixel luminance and gradient. Moreover, an iterative box-constrained least-squares estimation and its associated constraint refinement scheme is proposed to increase the robustness of the trained energy table even with limited training data. Simulation results show that the proposed method can reduce at least 11.54% of average error and 55.52% of max error as compared to the existing content-blind power model.	abstraction layer;approximation algorithm;cache coherence;central processing unit;data dependency;electronic system-level design and verification;gradient;hamming distance;image processor;iterative method;least squares;overfitting;pixel;refinement (computing);signal processing;simulation;subdivision surface	Chun-Wei Chen;Ming-Der Shieh;Juin-Ming Lu;Hsun-Lun Huang;Yao-Hua Chen	2017	2017 30th IEEE International System-on-Chip Conference (SOCC)	10.1109/SOCC.2017.8226075	robustness (computer science);hamming distance;architecture;data dependency;digital signal processor;energy consumption;observability;abstraction layer;real-time computing;computer science	EDA	2.521300631747035	44.684201904944324	153206
47dbb6e67720bab6fa4fb5004a6982db0da7f0ce	regraph: a graph processing framework that alternately shrinks and repartitions the graph		"""""""Think Like a Sub-Graph (TLASG)"""" is a philosophy proposed for guiding the design of graph-oriented programming models. As TLASG-based models allow information to flow freely inside a partition, they usually require much fewer iterations to converge when compared with """"Think Like a Vertex (TLAV)""""-based models.  In this paper, we further explore the idea of TLASG by enabling users to 1) proactively repartition the graph; and 2) efficiently scale down the problem's size. With these methods, our novel TLASG-based distributed graph processing system ReGraph requires even fewer iterations (typically ≤ 6) to converge, and hence achieves better performance (up to 45.4X) and scalability than existing TLAV and TLASG-based frameworks. Moreover, we show that these optimizations can be enabled without a large change in the programming model. We also implement our novel algorithm on top of Spark directly and compare it with other Spark-based implementation, which shows that our speedup is not bounded to our own platform."""	algorithm;converge;graph (abstract data type);iteration;programming model;scalability;speedup	Xue Li;Mingxing Zhang;Kang Chen;Yongwei Wu	2018		10.1145/3205289.3205292	parallel computing;vertex (geometry);speedup;scalability;programming paradigm;spark (mathematics);partition (number theory);bounded function;computer science;graph	HPC	2.1299250084533505	36.633890352981076	153523
f3b956907b05a6a22332c20a9c241efc9106ea5e	a paralle implementation of genetic programming that achieves super-linear performance	genetic program;human performance;benchmark problem;genetic algorithm;parallel implementation	This paper describes the successful parallel implementation of genetic programming on a network of processing nodes using the transputer architecture. With this approach, researchers of genetic algorithms and genetic programming can acquire computing power that is intermediate between the power of currently available workstations and that of supercomputers at intermediate cost. This approach is illustrated by a comparison of the computational effort required to solve a benchmark problem. Because of the decoupled character of genetic programming, our approach achieved a nearly linear speed up from parallelization. In addition, for the best choice of parameters tested, the use of subpopulations delivered a super linear speed-up in terms of the ability of the algorithm to solve the problem. Several examples are also presented where the parallel genetic programming system evolved solutions that are competitive with human performance on the same problem.	admissible numbering;benchmark (computing);computation;genetic algorithm;genetic programming;human reliability;parallel computing;speedup;supercomputer;transputer;workstation	David Andre;John R. Koza	1996	Inf. Sci.	10.1016/S0020-0255(97)10011-1	genetic programming;human performance technology;mathematical optimization;parallel computing;genetic algorithm;computer science;theoretical computer science;genetic operator;machine learning;genetic representation;distributed computing;algorithm	HPC	-1.547551251561815	35.006406043978544	153579
78eb6fd7921612cd2c7ad16792c2b40c4033a877	scaling large-data computations on multi-gpu accelerators	out of card computations;gpu;software engineering;compilers;tuning;openmp;software tools;software notations;pipelining;large data	Modern supercomputers rely on accelerators to speed up highly parallel workloads. Intricate programming models, limited device memory sizes and overheads of data transfers between CPU and accelerator memories are among the open challenges that restrict the widespread use of accelerators. First, this paper proposes a mechanism and an implementation to automatically pipeline the CPU-GPU memory channel so as to overlap the GPU computation with the memory copies, alleviating the data transfer overhead. Second, in doing so, the paper presents a technique called Computation Splitting, COSP, that caters to arbitrary device memory sizes and automatically manages to run out-of-card OpenMP-like applications on GPUs. Third, a novel adaptive runtime tuning mechanism is proposed to automatically select the pipeline stage size so as to gain the best possible performance. The mechanism adapts to the underlying hardware in the starting phase of a program and chooses the pipeline stage size. The techniques are implemented in a system that is able to translate an input OpenMP program to multiple GPUs attached to the same host CPU. Experimentation on a set of nine benchmarks shows that, on average, the pipelining scheme improves the performance by 1.49x, while limiting the runtime tuning overhead to 3% of the execution time.	benchmark (computing);central processing unit;computation;graphics processing unit;openmp;overhead (computing);pipeline (computing);run time (program lifecycle phase);runtime system;supercomputer	Amit Sabne;Putt Sakdhnagool;Rudolf Eigenmann	2013		10.1145/2464996.2465023	computer architecture;compiler;parallel computing;real-time computing;computer science;operating system;programming language;pipeline	Arch	-4.25497050768864	46.1514743519079	153757
bc5f83462d567547a009b425fb17c5ed734d9e0c	acceleration of derivative calculations with application to radial basis function: finite-differences on the intel mic architecture	spmv;simd;radial basis function;sparse matrix;mic	In this paper, we develop an efficient scheme for the cal- culation of derivatives within the context of Radial Ba- sis Function Finite-Difference (RBF-FD). RBF methods express functions as a linear combination of spherically symmetric basis functions on an arbitrary set of nodes. The Finite-Difference component expresses this combi- nation over a local set of nodes neighboring the point where the derivative is sought. The derivative at all points takes the form of a sparse matrix/vector multiplication (SpMV). In this paper, we consider the case of local stencils with a fixed number of nodes at each point and encode the sparse matrix in ELLPACK format. We increase the number of operations relative to memory bandwidth by interleaving the calculation of four derivatives of four different functions, or 16 different derivatives. We demonstrate a novel implementation on the Intel MIC archi- tecture, taking into account its advanced swizzling and channel interchange features. We present benchmarks on a real data set that show an almost sevenfold in- crease in speed compared to efficient implementations of a single derivative, reaching a performance of almost 140 Gflop/s in single precision. We explain the results through consideration of operation count versus memory bandwidth.	apple–intel architecture;encode;finite difference;forward error correction;memory bandwidth;pointer swizzling;radial (radio);radial basis function;single-precision floating-point format;sparse matrix;swizzling (computer graphics);xeon phi	Gordon Erlebacher;Erik Saule;Natasha Flyer;Evan F. Bollig	2014		10.1145/2597652.2597656	mathematical optimization;radial basis function;parallel computing;simd;sparse matrix;computer science;theoretical computer science;operating system;mathematics;algebra	HPC	-4.435659875404968	39.68469213540898	153806
55e35982614db1452e9575622f805365eb8a89cf	similarity (range and knn) queries processing on an intel xeon phi coprocessor	range search;knn;xeon phi;metric spaces;similarity search	Nowadays, the evolution of information technologies requires fast similarity search tools for analyzing new data types as audio, video, or images. The usual search by keys or records is not possible and to search on these databases is a compute-intensive problem. Regarding this, in the latest years, compute-intensive coprocessors (mainly NVIDIA GPUs) have been studied as a tool for accelerating sequential processing algorithms. In this work, we implement kNN and range queries on the recently launched Intel Xeon Phi coprocessor. We developed exhaustive and also indexing algorithms using the LC index. This index has been widely studied in sequential computing to accelerate similarity search on multimedia databases. We implement and compare different exhaustive and indexing versions showing some key factors in Xeon Phi to deal with this type of search. For indexing algorithms, we used a strategy based on cluster distribution among cores LC MIC Dist-C obtaining up to 168 $$\times $$ × over the sequential exhaustive algorithm. Our algorithms using exhaustive strategies in Xeon Phi for range queries achieve up to 22 $$\times $$ × speed-up over the sequential counterpart compared to the 12 $$\times $$ × of a 20-core machine, and a similar advantage is achieved for kNN queries. Comparing with GPUs, we obtain higher performance on our indexing algorithms on Intel Xeon Phi. However, GPU works faster with memory-aligned access exhaustive algorithms. Our exhaustive approaches on Xeon Phi can be used on a wide class of databases, for example, non-metric spaces. Finally, we extend our algorithms to be used with large databases that do not fit in the coprocessor memory, showing a good scalability with the number of elements.	algorithm;coprocessor;database;graphics processing unit;range query (data structures);scalability;similarity search;xeon phi	Carlos M. Toledo;Ricardo J. Barrientos;Andrés I. Ávila	2015	Cluster Computing	10.1007/s10586-015-0515-z	parallel computing;metric space;computer science;theoretical computer science;operating system;xeon phi;k-nearest neighbors algorithm	DB	-0.4098377483996526	41.55381109624203	153971
a5190ba12c36ed2d8f013cfe92b73304c8e11153	massively parallelizing the rrt and the rrt	manipulators;kernel;high dimensionality;asymptotic optimality;robot manipulator;configuration space;trajectory;rapidly exploring random tree;robot motion planning;scientific computing;motion planning;graphic processing unit;manipulators kernel planning graphics processing unit instruction sets trajectory;planning;parallel implementation;graphics processing unit;central processing unit;instruction sets	In recent years, the growth of the computational power available in the Central Processing Units (CPUs) of consumer computers has tapered significantly. At the same time, growth in the computational power available in the Graphics Processing Units (GPUs) has remained strong. Algorithms that can be implemented on GPUs today are not only limited to graphics processing, but include scientific computation and beyond. This paper is concerned with massively parallel implementations of incremental sampling-based robot motion planning algorithms, namely the widely-used Rapidly-exploring Random Tree (RRT) algorithm and its asymptotically-optimal counterpart called RRT*. We demonstrate an example implementation of RRT and RRT* motion-planning algorithm for a high-dimensional robotic manipulator that takes advantage of an NVidia CUDA-enabled GPU. We focus on parallelizing the collision-checking procedure, which is generally recognized as the computationally expensive component of sampling-based motion planning algorithms. Our experimental results indicate significant speedup when compared to CPU implementations, leading to practical algorithms for optimal motion planning in high-dimensional configuration spaces.	analysis of algorithms;asymptotically optimal algorithm;automated planning and scheduling;automatic parallelization;cuda;central processing unit;collision detection;computation;computational science;computer graphics;experiment;graph traversal;graphics processing unit;motion planning;parallel computing;rapidly-exploring random tree;robot;sampling (signal processing);speedup	Joshua Bialkowski;Sertac Karaman;Emilio Frazzoli	2011	2011 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2011.6095053	planning;rapidly exploring random tree;configuration space;parallel computing;kernel;simulation;computer science;artificial intelligence;trajectory;theoretical computer science;central processing unit;instruction set;motion planning	Robotics	0.5319504961806671	39.70055134390235	154025
1db96a355a77e74e339654fa46b862d89d7b13be	fpga-specific arithmetic optimizations of short-latency adders	logic design adders field programmable gate arrays;carry chain;logic design;fpga;carry increment;carry increment adders fpga specific arithmetic optimizations short latency adders integer addition elliptic curve cryptography ripple carry addition carry select adders;carry increment fpga addition carry chain carry select;adders;addition;carry select;field programmable gate arrays;adders table lookup field programmable gate arrays multiplexing delay computer aided instruction	Integer addition is a pervasive operation in FPGA designs. The need for fast wide adders grows with the demand for large precisions as, for example, required for the implementation of IEEE-754 quadruple precision and elliptic-curve cryptography. The FPGA realization of fast and compact binary adders relies on hardware carry chains. These provide a natural implementation environment for the ripple-carry addition (RCA) scheme. As its latency grows linearly with operand width, wide additions call for acceleration, which is quite reasonably achieved by addition schemes built from parallel RCA blocks. This study presents FPGA-specific arithmetic optimizations for the mapping of carry-select and carry-increment adders targeting the hardware carry chains of modern FPGAs. Different trade-offs between latency and area are explored. The proposed architectures can be successfully used in the context of latency-critical systems or as attractive alternatives to deeply pipelined RCA schemes.	adder (electronics);automatic acoustic management;card security code;carry flag;comparator;computation;critical path method;elliptic curve cryptography;field-programmable gate array;multiplexer;open-source software;operand;pipeline (computing);quadruple-precision floating-point format;rca connector;ripple effect;speculative execution	Hong Diep Nguyen;Bogdan Pasca;Thomas B. Preußer	2011	2011 21st International Conference on Field Programmable Logic and Applications	10.1109/FPL.2011.49	embedded system;parallel computing;real-time computing;computer hardware;computer science;field-programmable gate array	EDA	10.00152627257562	44.77875765034313	154162
89b2ddc042215f4ac1121114f73a30505f44a838	unifying data, model and hybrid parallelism in deep learning via tensor tiling		Deep learning systems have become vital tools across many fields, but the increasing model sizes mean that training must be accelerated to maintain such systems’ utility. Current systems like TENSORFLOW and MXNET focus on one specific parallelization strategy, data parallelism, which requires large training batch sizes in order to scale. We cast the problem of finding the best parallelization strategy as the problem of finding the best tiling to partition tensors with the least overall communication. We propose an algorithm that can find the optimal tiling. Our resulting parallelization solution is a hybrid of data parallelism and model parallelism. We build the SOYBEAN system that performs automatic parallelization. SOYBEAN automatically transforms a serial dataflow graph captured by an existing deep learning system frontend into a parallel dataflow graph based on the optimal tiling it has found. Our evaluations show that SOYBEAN is 1.5×−4× faster than pure data parallelism for AlexNet and VGG. We present this automatic tiling in a new system, SOYBEAN, that can act as a backend for TENSORFLOW, MXNET, and others.	algorithm;automatic parallelization;data model;data parallelism;data-flow analysis;dataflow;deep learning;graphics processing unit;parallel computing;pure data;tensorflow;tiling window manager	Minjie Wang;Chien-chin Huang;Jinyang Li	2018	CoRR		data parallelism;parallel computing;tensor;data model;dataflow;deep learning;distributed computing;computer science;automatic parallelization;pure data;artificial intelligence;graph	HPC	-3.59217115392575	43.05722812182594	154470
f581380211a026589cb2664bb1794917f2ad6473	squaring algorithms with delayed carry method and efficient parallelization		Increasing amounts of information that needs to be protecting put in claims specific requirements for information security systems. The main goal of this paper is to find ways to increase performance of cryptographic transformation with public key by increasing performance of integers squaring. Authors use delayed carry mechanism and approaches of effective parallelization for Comba multiplication algorithm, which was previously proposing by authors. They use the idea of carries accumulation by addition products of multiplying the relevant machine words in columns. As a result, it became possible to perform addition of such products in the column independently of each other. However, independent accumulation of products and carries require correction of the intermediate results to account for the accumulated carries. Due to the independence of accumulation in the columns, it became possible to parallelize the process of products accumulation that allowed formulating several approaches. In this paper received theoretical estimates of the computational complexity for proposed squaring algorithms. Software implementations of algorithms in C++ allowed receiving practical results of the performance, which are not contrary to theoretical estimates. The authors first proposed applying the method of delayed carry and parallelization techniques for squaring algorithms, which was previously proposing for integer multiplication.	automatic parallelization;c++;column (database);computational complexity theory;information security;multiplication algorithm;numerical integration;parallel computing;public-key cryptography;requirement;tree accumulation	Vladyslav Kovtun;Andrew Okhrimenko	2014	IACR Cryptology ePrint Archive		parallel computing;theoretical computer science;automatic parallelization;computer science	Theory	8.09398944202522	37.9904661879188	154472
39897b3832f6bfd9929ff79e5958cc558830b3b8	the evolution of babbage's calculating engines	printing;computers;mathematics;history;application software;evolution biology;engines;difference engines;software algorithms;fasteners;point of view;microprogramming;difference engines algorithm design and analysis history microprogramming mathematics application software equations hardware software algorithms;register transfer level;simultaneous equations;algorithm design and analysis;wheels;hardware	This paper traces the evolution of Charles Babbage's design ideas for automatic computing machines from Difference Engine No. 1 (1822-1833), through the Analytical Engine (1834-1846), to Difference Engine No. 2 (1846-1847). The design evolution is discussed from four essentially hierarchically related points of view: (1) mechanism - the basic mechanical apparatus for storing, transferring, and adding numbers; (2) architecture - the arrangement and interconnection of the basic mechanisms in the complete design; (3) algorithms - the functional utilization of the basic mechanism in carrying out such operations as multiplication, division, and signed addition (the microprogram or register-transfer level of description); (4) programs - the user-level application of the machine in solving such problems as tabulating series or solving simultaneous equations. The paper examines how developments at each level interacted in the design of the Analytical Engine. It also discusses the ways Babbage's designs anticipated, or failed to anticipate, modern computer designs.	algorithm;analytical engine;babbage;computer;difference engine;evolution;interconnection;microcode;register-transfer level;tracing (software);user space	Allan G. Bromley	1987	Annals of the History of Computing	10.1109/MAHC.1987.10013	algorithm design;application software;simultaneous equations;computer science;engineering;electrical engineering;artificial intelligence;theoretical computer science;operating system;database;microcode;programming language;register-transfer level;algorithm	Arch	3.5980302313614825	39.40061433643989	154763
f59ab2d004f31a20de5c0c4062f2f6f51a2cbf9e	analog computing in a modern context: a linear algebra accelerator case study		Approaching the post-Moore's law era, researchers are looking for scalable ways to get useful computation from existing silicon technology. This article presents a programmable analog accelerator for solving systems of linear equations. The authors compensate for commonly perceived downsides of analog computing, such as low precision and accuracy, limited problem sizes, and difficulty applying it to different workloads. On the basis of a prototyped analog accelerator chip, they compare the analog solver's performance and energy consumption against an efficient digital algorithm running on a general-purpose processor. The analog accelerator approach is 10 times faster and provides 33 percent energy savings. Owing to the speed and efficiency of linear algebra algorithms running on digital computers, an analog accelerator that matches digital performance needs a large silicon footprint, which limits scalability. The authors conclude that problem classes outside of systems of linear equations could hold more promise for analog acceleration.	algorithm;analog computer;computation;general-purpose markup language;linear algebra;linear equation;moore's law;scalability;solver;system of linear equations	Yipeng Huang;Ning Guo;Mingoo Seok;Yannis P. Tsividis;Simha Sethumadhavan	2017	IEEE Micro	10.1109/MM.2017.55	parallel computing;real-time computing;system of linear equations;computer science;analog multiplier;analog computer;numerical linear algebra;computation;hybrid computer;field-programmable analog array;solver	Arch	6.014737013295436	43.48715169384975	154818
d649fbb31bd980b68ad110b89c0d7566f42d9324	a scalable dataflow accelerator for real time onboard hyperspectral image classification	08 information and computing sciences;conference paper;artificial intelligence image processing	Real-time hyperspectral image classification is a necessary primitive in many remotely sensed image analysis applications. Previous work has shown that Support Vector Machines (SVMs) can achieve high classification accuracy, but unfortunately it is very computationally expensive.This paper presents a scalable dataflow accelerator on FPGA for real-time SVM classification of hyperspectral images.To address data dependencies, we adapt multi-class classifier based on Hamming distance. The architecture is scalable to high problem dimensionality and available hardware resources. Implementation results show that the FPGA design achieves speedups of 26x, 1335x, 66x and 14x compared with implementations on ZYNQ, ARM, DSP and Xeon processors. Moreover, one to two orders of magnitude reduction in power consumption is achieved for the AVRIS hyperspectral image datasets.	arm architecture;analysis of algorithms;artificial neural network;central processing unit;computer vision;convolutional neural network;data dependency;dataflow;dataflow programming;digital signal processor;feature extraction;field-programmable gate array;hamming distance;horizontal situation indicator;image analysis;on-board data handling;real-time clock;scalability;support vector machine	Shaojun Wang;Xinyu Niu;Ning Ma;Wayne Luk;Philip Leong;Yu Peng	2016		10.1007/978-3-319-30481-6_9	embedded system;computer vision;parallel computing;computer science;theoretical computer science;operating system;machine learning	EDA	2.933556618709053	45.152643190686504	154905
646181575a871cb6bc2e98005ce517ebe5772d66	comparison of the hardware performance of the aes candidates using reconfigurable hardware.	field programmable gate array;hardware architecture;reconfigurable hardware	The results of implementations of all five AES finalists using Xilinx Field Programmable Gate Arrays are presented and analyzed. Performance of four alternative hardware architectures is discussed and compared. The AES candidates are divided into three classes depending on their hardware performance characteristics. Recommendation regarding the optimum choice of the algorithms for AES is provided.	aes instruction set;advanced encryption standard process;algorithm;field-programmable gate array	Kris Gaj;Pawel Chodowiec	2000			embedded system;computer architecture;hardware acceleration;reconfigurable computing;hardware register;fpgac;computer engineering	Arch	8.890329041660427	45.520820482229254	155004
69cce7b7be76e4623b1211bf278874a8a80483db	automated energy saving (aes) paradigm to support pedagogical activities over wireless sensor networks	saving energy;ubiquitous communication;bt sensor node;aes mathematical model	Fast expansion in ambient intelligence (AmI) has attracted different walks of people. AmI systems provide robust communication in open, dynamic and heterogeneous environments. This paper presents a AES paradigm that introduces wireless sensor networks to control remote servers or other devices at remote place through mobile phones. The main focus of paper is to consume minimum energy for obtaining the objectives. To realize the paradigm, mathematical model is formulated. The proposed paradigm consists of automatic energy saving model senses the environment to activate either the passive or active mode of sensor nodes for saving energy. Simulations are conducted to validate the proposed paradigm; we use two types of simulations: Test bed simulation is done to check practical validity of proposed approach and Ns2 simulation is performed to simulate the behavior of wireless sensors network with supporting mathematical model. The prototype can further be implemented to handle several objects simultaneously in university and other organizations.	ambient intelligence;computer simulation;mathematical model;mobile phone;programming paradigm;prototype;sensor;sleep mode	Abdul Razaque;Khaled M. Elleithy	2012		10.1007/978-3-642-35377-2_70	embedded system;real-time computing;simulation;operating system;key distribution in wireless sensor networks;computer security;computer network	Mobile	0.35210419212895117	32.38549386300984	155349
6d2312e8ae7362d00c3cbff0fe8f8fa045efee6e	a multi-fpga application-specific architecture for accelerating a floating point fourier integral operator	kernel;field programmable gate arrays throughput coordinate measuring machines clocks magnetic cores computer architecture acceleration;personal computer;software prototyping;clocks;floating point computing elements;hardware accelerator;acceleration;fixed point;fast fourier transform;computer architecture;field programmable gate arrays fast fourier transforms;complex system;fpgas;fast fourier transforms;floating point;fourier integral operator;multiple fpgas;floating point arithmetic;field programmable gate arrays;magnetic cores;coordinate measuring machines;microcomputers;multiple fpgas fast fourier transform fourier integral operator fpgas floating point computing elements;software implementation;throughput;hardware	Many complex systems require the use of floating point arithmetic that is exceedingly time consuming to perform on personal computers. However, floating point operators are also hardware resource intensive and require longer latencies than fixed point operators to complete. Due to the reduced logic density of FPGAs relative to ASICs, it is often only possible to accelerate a portion of a floating point application in hardware. This paper presents an application-specific architecture for the hardware acceleration of a complete Fourier Integral Operator (FIO) kernel used in seismic imaging on a multi-FPGA platform. The design utilizes several floating point computing elements (CEs) to calculate the FIO kernel in parallel stages on multiple FPGAs. A detailed study of floating point CEs, including a Fast Fourier Transform (FFT) CE, and a complete FIO prototype implementation on the BEE2 platform is described. The prototype implementation has a 12.4x increase in throughput over an optimized software implementation, and a predicted 15.8x increase in throughput on the BEE3 platform.	application-specific integrated circuit;complex systems;fast fourier transform;field-programmable gate array;fixed point (mathematics);hardware acceleration;kernel (operating system);personal computer;prototype;throughput	Jason Lee;Lesley Shannon;Matthew J. Yedlin;Gary F. Margrave	2008	2008 International Conference on Application-Specific Systems, Architectures and Processors	10.1109/ASAP.2008.4580178	embedded system;fast fourier transform;complex systems;computer architecture;parallel computing;computer hardware;computer science;floating point;operating system;field-programmable gate array	EDA	0.5370040983402565	45.687785887551	155404
2914c9d5a7ae0cc1adb7da52d0d5267de0553e47	vlsi mesh of trees for data base processing	time complexity;chip	The VLSI chips introduced in preceding sections can be combined to construct a data base machine.  In order to execute all the operations required in a data base transaction or set of transactions a control processor to coordinate the MT devices is needed.      The control processor has to generate instructions for the MT's and to supervise their execution. Besides, it has to control the flow of data between SMT's, PMT's and Mass Storage. A straightforward architecture of such a system is illustrated in figure 2. We note that several PMT's are required both to enhance the parallelism of processing and to provide interconnection between SMT's and PMT's.      As a final note, we point out that the state of the art of VLSI technology imposes severe limitations on the size of MT's and, consequently on the size of data bases that can be stored or processed, in these elements. Although it can be expected that these limitations will eventually be overcome by the progress of technology, a relevant problem is how to extend the results given in this paper in order to deal with the case where relations exceed the size of SMT's and PMT's.      This result could be achieved by assuming that large relations be partitioned in several parts; high level operations be performed on the parts and the resulting relation be later recomposed from parts.      An efficient strategy to obtain this result, still sublinear in terms of time complexity, is currently under investigation.    	very-large-scale integration	Maurizio A. Bonuccelli;Elena Lodi;Fabrizio Luccio;Piero Maestrini;Linda Pagli	1983		10.1007/3-540-12727-5_8	computer architecture;parallel computing;computer science;theoretical computer science	DB	-0.38888895286264413	35.98660455369928	155491
d051de4f4a53b8118e41912de58d1799729df552	accelerated parallel genetic programming tree evaluation with opencl	gp gpu;parallel genetic programming;opencl;accelerated tree evaluation	Inspired by the process of natural selection, genetic programming (GP) aims at automatically building arbitrarily complex computer programs. Being classified as an ‘‘embarrassingly’’ parallel technique, GP can theoretically scale up to tackle very diverse problems by increasingly adding computational power to its arsenal. With today’s availability of many powerful parallel architectures, a challenge is to take advantage of all those heterogeneous compute devices in a portable and uniformway. This work proposes both (i) a transcription of existing GP parallelization strategies into the OpenCL programming platform; and (ii) a freely available implementation to evaluate its suitability for GP, by assessing the performance of parallel strategies on the CPU and GPU processors from different vendors. Benchmarks on the symbolic regression and data classification domains were performed. On the GPUwe could achieve 13 billion node evaluations per second, delivering almost 10 times the throughput of a twelve-core CPU. © 2012 Elsevier Inc. All rights reserved.	benchmark (computing);central processing unit;computation;computer program;embarrassingly parallel;genetic programming;graphics processing unit;opencl api;parallel computing;symbolic regression;throughput;transcription (software)	Douglas Adriano Augusto;Helio J. C. Barbosa	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2012.01.012	parallel computing;real-time computing;computer science;theoretical computer science;operating system;distributed computing	HPC	-2.13316257110284	43.54488185577343	155569
3868dcc3f56b99534d997e63011e9344f8d51ecb	a massively parallel implementation of the full search vector quantization algorithm	full search;audio visual speech recognition;parallel computer;vector quantizer;parallel implementation;product development	We present a massively parallel version of a full search vector quantization and its application in the development of an audio-visual speech recognition system. The parallel implementation reduced the worst case runtime of (estimated) 80–100 hours on a 10 MFLOP SPARC to less then 2 hours on a 2.4 GFOLP MasPar MP2216. This demonstrates how the use of parallel computers reduces product development time and leads to a more mature design by allowing for more extensive experimentation with different data sets.	algorithm;vector quantization	Paul Lukowicz;Jutta Schiffers;Rudi Cober	1994		10.1007/BFb0020410	parallel computing;computer science;theoretical computer science;machine learning;new product development	ML	-1.6327517942776932	35.148333168467154	155912
7f3b920d649d888f9608fe0b317712105b6931e5	cascading signal-model complexity for energy-aware detection	environmental monitoring geophysics;cascading detector cascading signal model complexity energy aware detection context awareness situational awareness natural event detection energy constrained device cascade architecture signal model complexity device energy consumption optimal threshold allocation threshold tracking algorithm time varying environmental condition energy supply low power mcu controllable supply voltage scaling acoustic wildlife monitoring system;environmental monitoring geophysics acoustic signal detection;signal detection;scalable systems;energy aware;passive vigilance;signal detection cascaded detectors energy aware passive vigilance scalable systems;acoustic signal detection;detectors hardware computational modeling computer architecture monitoring energy consumption;cascaded detectors	Equipping everyday objects with sensing and computational capabilities creates the potential to achieve context and situational awareness through the detection of natural events. A major challenge in energy-constrained devices is that the detection of natural events generally demands sophisticated signal modeling and processing, along with continuous sensing. In this paper, we propose to use a cascade architecture to control signal-model complexity; this approach allows the device to sense continuously and trigger a more accurate signal model only when an event of interest is likely to be occurring. The sensitivity of the triggering affects detection performance and device energy consumption, so we formulate and solve the problem of optimal threshold allocation to control this sensitivity. We prove that for controlling signal-model complexity, triggering as often as possible maximizes detection performance; this seemingly intuitive result does not hold in other popular cascading techniques, most notably in low-power wakeup mechanisms. Our analysis leads to a simple threshold-tracking algorithm that can adjust to time-varying environmental conditions and energy supply. Combining a low-power MCU with controllable supply-voltage scaling, we present an acoustic wildlife monitoring system that exhibits 12× energy scalability from cascading detectors, and an additional 2× from synergistic hardware scaling on commercially available components. In practical scenarios, we demonstrate energy savings ranging from 3× to 20× with minimal loss in performance, compared to the conventional approach.	acoustic cryptanalysis;algorithm;dynamic voltage scaling;image scaling;low-power broadcasting;scalability;sensor;synergy	David M. Jun;Douglas L. Jones	2013	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2013.2243632	embedded system;electronic engineering;real-time computing;simulation;telecommunications;engineering;detection theory	Mobile	3.532206674163803	34.91153616479247	155944
60da989817b1cfe22c59dd8d98c91fe5e009ad3c	a bit-plane architecture for optical computing with two-dimensional symbolic substitution	computer architecture;optical information processing;arithmetic/logic operations;bit plane architecture;electronic array processors;fast fourier transforms;matrix algebra;optical computing;parallel computers;two-dimensional symbolic substitution	A novel architecture based on optical technology is presented for constructing parallel computers. The architecture exploits optics for its ultra-high speed, massive parallelism, and dense connectivity. The processing is based on a new technique called 2-D symbolic substitution which can be implemented with very fast optical components. Two-dimensional symbolic substitution algorithms are developed for arithmetic/logic operations as well as for complex scientific computations such as matrix algebra and FFT. The predicted performance of the system is compared with the performance of existing electronic array processors and is shown to be potentially superior. The bit-plane architecture is shown feasible and economical based on state-of-the-art optical and electro-optical technologies.	array processing;bit plane;central processing unit;computation;computer;fast fourier transform;optical computing;parallel algorithm;parallel computing;simd;vector processor	Ahmed Louri;Kai Hwang	1988			fast fourier transform;computer architecture;parallel computing;computer science;cellular architecture;theoretical computer science;optical computing	Arch	7.994443458347802	39.50874132228819	156149
f642835e1b3a7e50b5300accfa99553077fa57c0	green behavior generation: a digital approach to reduce consumption	home computing;sensor home network system;context data collection;ubiquitous computing distributed sensors environmental science computing home computing;sensor network;environmental science computing;distributed sensors;information and communication technology;green behavior generation;energy consumption;energy consumption global warming temperature sensor systems communications technology manufacturing industries research and development production systems human factors home appliances;environmental burden;networked home electric appliances;ubiquitous sensor network;environmental burden green behavior generation sensor home network system information and communication technology networked home electric appliances ubiquitous sensor network energy consumption context data collection;ubiquitous computing;mental stress;networked systems	In this paper, we proposes a new approach called green behavior generation (GBG), which is designed to reduce the environmental burden without mental stress in daily life. A GBG system consists of sensing, modeling, and service elements. For sensing various kinds of data, a sensor home is constructed. The network system of the sensor home is based on advanced information and communication technology (ICT), such as networked home electric appliances and a ubiquitous sensor network, which are applied to collect energy consumption and context data simultaneously. An experimental example and the usefulness of the sensing environment are shown.	context awareness;embedded system;sensor web;ubiquitous computing	Hideki Kobayashi;Koji Kimura;Toshimitsu Kumazawa;Ryohei Orihara;Tomoko Murakami;Y. Motomura;Yoichi Nishida	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414155	embedded system;information and communications technology;real-time computing;wireless sensor network;computer science;computer security;ubiquitous computing	Robotics	1.0719092295132826	32.53730466193545	156438
9f8dc8d09a4775e8491e5bb4c747d0056896ff3c	route diversity effect on 20 ghz band radio relay links	communication system;estimation method;circuit design;diversity communication;relays rain attenuation measurement telegraphy telephony physics telecommunications gas lasers laser theory remote sensing;radio communication diversity communication japan millimeter wave radio communications;radio communication;millimeter wave radio communications;correlation coefficient;japan	Hitoshi Mineno was born in Osaka, Japan, on March 10, 1955. He received the B.S. degree in electrical engineering from Doshisha University, Kyoto, Japan, in 1978. In 1978 he joined the Kashima Branch of Radio Research Laboratories (RRL), Ibaraki, Japan, where he has worked on research on satellite communications. Mr. Mineno is a member of the Institute of Electronics and Communication Engineers of Japan.	communications satellite;electrical engineering;relay;rich representation language	O. Sasaki;I. Nagamune	1983	IEEE Journal on Selected Areas in Communications	10.1109/JSAC.1983.1145970	telecommunications;computer science;circuit design;communications system	Mobile	6.884358694023052	35.8412096708082	156493
b9c77eb0c38703f731aafa31d13d5b27c7cd9137	using gpi-2 for distributed memory paralleliziation of the caffe toolbox to speed up deep neural network training		Deep Neural Network (DNN) are currently of great interest in research and application. The training of these networks is a compute intensive and time consuming task. To reduce training times to a bearable amount at reasonable cost we extend the popular Caffe toolbox for DNN with an efficient distributed memory communication pattern. To achieve good scalability we emphasize the overlap of computation and communication and prefer fine granular synchronization patterns over global barriers. To implement these communication patterns we rely on the the ”Global address space Programming Interface” version 2 (GPI-2) communication library. This interface provides a light-weight set of asynchronous one-sided communication primitives supplemented by non-blocking fine granular data synchronization mechanisms. Therefore, CaffeGPI is the name of our parallel version of Caffe. First benchmarks demonstrate better scaling behavior compared with other extensions, e.g., the IntelTMCaffe. Even within a single symmetric multiprocessing machine with four graphics processing units, the CaffeGPI scales better than the standard Caffe toolbox. These first results demonstrate that the use of standard High Performance Computing (HPC) hardware is a valid cost saving approach to train large DDNs. I/O is an other bottleneck to work with DDNs in a standard parallel HPC setting, which we will consider in more detail in a forthcoming paper.	benchmark (computing);blocking (computing);computation;computer graphics;computer science;data science;data synchronization;deep learning;distributed memory;graphics processing unit;image scaling;input/output;non-blocking algorithm;nvidia dgx-1;partitioned global address space;scalability;symmetric multiprocessing	Martin Kuehn;Janis Keuper;Franz-Josef Pfreundt	2017	CoRR		parallel computing;real-time computing;computer science;machine learning;distributed computing	HPC	-3.966837339016233	43.49464191370596	156656
1a4690d306e89451360a6f7492d9def9fbfd9871	dsps, brams and a pinch of logic: new recipes for aes on fpgas	xilinx virtex 5 fpga;software;digital signal processing;random access storage cryptography digital signal processing chips field programmable gate arrays flip flops;bit rate 1 76 gbit s;aes;flip flops;logic element;fpga;cipher implementations;dsp units;cryptography;blockram;logic elements;random access storage;lookup table;digital signal processing chips;source code;fpga aes dsp units;bit rate 6 21 gbit s;field programmable gate arrays;high throughput;table lookup;clock cycle;digital signal processing logic field programmable gate arrays cryptography throughput hardware embedded computing laboratories computer security clocks;flip flop;dsp;throughput;bit rate 6 21 gbit s xilinx virtex 5 fpga clock cycle logic elements flip flops cipher implementations blockram dsp bit rate 1 76 gbit s	"""We present an AES cipher implementation that is based on the BlockRAM and DSP units embedded within Xilinx's Virtex-5 FPGAs. An iterative """"basic"""" module outputs a 32 bit column of an AES round each clock cycle, with a throughput of 1.76 Gbit/s when processing two 128 bit inputs. This construct is replicated four times for a 128 bit datapath for a full AES round with 6.21 Gbit/s throughput when processing eight inputs. Finally, the """"round"""" module is replicated ten times for a fully unrolled design that yields over 55 Gbit/s of throughput. The combination and arrangement of the specialized embedded functions available in the FPGA allows us to implement our designs using very few traditional user logic elements such as flip-flops and lookup tables, yet still achieve these high throughputs. The complete source code for these designs is made publicly available for use in further research and for replicating our results. Our contribution ends with a discussion of comparing cipher implementations in the literature, and why these comparisons can be meaningless without a common reporting style, platform, or within the context of a specific constrained application."""	128-bit;32-bit;block cipher mode of operation;clock signal;datapath;digital signal processor;embedded system;flops;field-programmable gate array;flip-flop (electronics);gigabit;iteration;key schedule;lookup table;pipeline (computing);random-access memory;scheduling (computing);side-channel attack;throughput;type signature	Saar Drimer;Tim Güneysu;Christof Paar	2008	2008 16th International Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2008.42	embedded system;parallel computing;real-time computing;computer science;operating system;digital signal processing;field-programmable gate array	Arch	9.30724796476807	45.31026047246078	157118
9558a9d049209dd44bec7067ec90a41b6ce65970	binary trees and parallel scheduling algorithms	design tool;processing;general and miscellaneous mathematics computing and information science;mathematics;parallel algorithm;shared memory;shared memory model;efficient algorithm;queueing theory;multiprocessors;shared memory model complexity design methodologies parallel algorithms scheduling;data processing;computer communications;complexity;mathematical logic;boolean algebra;computer programming;computer architecture;scheduling algorithm;matrices mathematics;scheduling;parallel computer;input output processing;binary notation;communications networks;scheduling problem;programming 990200 mathematics computers;algorithms;design;design methodologies;task scheduling;memory devices;algorithm design;data links;logical process;parallel processing;parallel processors;fault tree analysis;parallel algorithms;binary tree	This paper examines the use of binary trees in the design of efficient parallel algorithms. Using binary trees, we develop efficient algorithms for several scheduling problems. The shared memory model for parallel computation is used. Our success in using binary trees for parallel computations, indicates that the binary tree is an important and useful design tool for parallel algorithms.	algorithm;recursion (computer science);scheduling (computing)	Eliezer Dekel;Sartaj Sahni	1983	IEEE Trans. Computers	10.1109/TC.1983.1676223	random binary tree;optimal binary search tree;parallel processing;red–black tree;parallel computing;binary search tree;binary expression tree;data processing;geometry of binary search trees;binary tree;computer science;theoretical computer science;operating system;self-balancing binary search tree;analysis of parallel algorithms;distributed computing;parallel algorithm;weight-balanced tree;ternary search tree;threaded binary tree;scheduling;algorithm	Theory	0.9072441461379629	36.45194236447822	157167
2e861465c86fbd6f9f6b31164be820e4ed11164c	parallel tree search in volunteer computing: a case study		While volunteer computing, as a restricted model of parallel computing, has proved itself to be a successful paradigm of scientific computing with excellent benefit on cost efficiency and public outreach, many problems it solves are intrinsically highly parallel. However, many efficient algorithms, including backtracking search, take the form of a tree search on an extremely uneven tree that cannot be easily parallelized efficiently in the volunteer computing paradigm. We explore in this article how to perform such searches efficiently on volunteer computing projects. We propose a parallel tree search scheme, and we describe two examples of its real-world implementation, Harmonious Tree and Odd Weird Search, both carried out at the volunteer computing project yoyo@home. To confirm the observed efficiency of our scheme, we perform a mathematical analysis, which proves that, under reasonable assumption that agrees with experimental observation, our scheme is only a constant multiplicative factor away from perfect parallelism. Details on improving the overall performance are also discussed.	algorithm;automatic parallelization;boinc client–server technology;backtracking;benoît and the mandelbrots;coefficient;computation;computational science;cost efficiency;discrete mathematics;entity–relationship model;hsinchun chen;human factors and ergonomics;list of distributed computing projects;parallel computing;parallel programming model;programming paradigm;requirement;scheduling (computing);search tree;throughput;tree traversal;volunteer computing	Wenjie Fang;Uwe Beckert	2017	Journal of Grid Computing	10.1007/s10723-017-9411-5	volunteer;backtracking;distributed computing;theoretical computer science;computer science;utility computing;cost efficiency	HPC	-0.7838701182901922	34.6825308636835	157187
6b503279734e16f934ac765f5a19842cf13e9bd7	activity recognition in smart homes based on electrical devices identification	nonintrusive appliance load monitoring nialm;smart home;load signature;activity recognition	Activity recognition constitutes the key challenge in the development of smart home assistive systems. In this paper, we propose a new algorithmic method for activity recognition in a smart home, based on load signatures of appliances. Most recognition approaches rely on distributed and heterogeneous sensors (ex. RFID), which are intrusive require complex installation, deployment and maintenance. On the other hand, most applications of appliance load monitoring (signal analysis) refer to the energy saving and the costs reducing of energy consumption. Consequently, our proposal constitutes an original application and new algorithmic method based on steady-state operations and signatures. The extraction process of load signatures of appliances is carried out in a three-dimensional space through a single power analyzer, which is non-intrusive (NIALM). We have rigorously tested this new approach by conducting an experiment in our smart home prototype by simulating daily scenarios taken from clinical trials previously done with Alzheimer patients. The promising results we obtained are presented and compared to other approaches, showing that, with an exceptionally minimal investment and the exploitation of relatively limited data, our method can efficiently recognize activities of daily living for providing assistive services.	activity recognition;algorithm;antivirus software;assistive technology;database schema;electronic signature;exploit (computer security);home automation;plug-in (computing);prototype;radio-frequency identification;real life;sensor;signal processing;simulation;software deployment;steady state;type signature	Corinne Belley;Sébastien Gaboury;Bruno Bouchard;Abdenour Bouzouane	2013		10.1145/2504335.2504342	embedded system;home automation;real-time computing;computer science;computer security;activity recognition	HCI	2.8610579178665465	34.19018461035152	157330
cb94246d95a8c47fa71d960824450a22e2d57ccd	recursive least squares problem implementation on a generalized interconnection of dsp processors	recursive least square;multiprocessor interconnection networks;digital signal processing;least squares approximations;performance evaluation;reconfigurable architectures;implementation;multiprocessor network;recursive functions adaptive systems digital signal processing chips least squares approximations multiprocessor interconnection networks parallel architectures performance evaluation reconfigurable architectures;resonance light scattering;recursive least squares;array signal processing;generalized network;system performance;least squares methods digital signal processing resonance light scattering sensor arrays signal processing algorithms array signal processing adaptive signal processing engines computer architecture algorithm design and analysis;computer architecture;dynamic environment;optimal system performance dsp processors recursive least squares multiprocessor network digital signal processor adaptive beamforming generalized network parallelization implementation minimum variance distortionless response;adaptive signal processing;engines;parallel architectures;efficient implementation;adaptive systems;optimal system performance;minimum variance distortionless response;parallelization;recursive functions;digital signal processor;digital signal processing chips;signal processing algorithms;adaptive beamforming;dsp processors;algorithm design and analysis;least squares methods;sensor arrays	The problem of implementing a recursive least squares (RLS) problem on a multiprocessor network comprising general-purpose digital signal processor is addressed. As an example of an application scenario, the adaptive beamforming problem has been chosen, where an RLS formulation has been adopted for obtaining the optimum weights. Although optimal array processors have been proposed for this problem, implementation on a generalized network affords a flexible as well as a modular and reconfigurable solution, suitable for a highly dynamic environment with changing applications and problem sizes. The parallelization issues are explored and a scheme for an efficient implementation is described. An implementation of the minimum-variance distortionless response (MVDR) problem on a network of DSP32C processors is evaluated. A 1024 size problem implementation is evaluated with the DSP32C as the computation engine, and it is shown that, when the number of processors in the system is constrained to be 10, near-optimal performance can be attained with four processors. Thus, this approach is a useful aid in determining optimal system performance parameters. >		Sati Banerjee;Paul M. Chau	1993		10.1109/ICASSP.1993.319563	adaptive filter;algorithm design;computer vision;digital signal processor;parallel computing;real-time computing;computer science;adaptive system;adaptive beamformer;digital signal processing;computer performance;implementation;recursive least squares filter	EDA	4.241181268751955	39.19394885090894	157517
6d0d414261511d375902455a3ab0aea12ebb6bc6	energy-efficient and fast collection method for smart sensor monitoring systems	smart phones;smart phones computerised instrumentation data handling intelligent sensors mobile computing;energy efficient collection method pervasive computing smart sensor monitoring systems wireless sensor networks smart world internet smart phone application data collection device enhanced fast sensor collection algorithm wake up signal smart sensor lifetime;computerised instrumentation;data handling;mobile computing;intelligent sensors;intelligent sensors data collection monitoring wireless sensor networks performance evaluation protocols	As the era of pervasive computing dawns, smart sensors and wireless sensor networks (WSN) are the main candidates to create a smart world where the things are able to interact to the Internet. This paper introduces a smart sensor monitoring system which is composed of a smartphone application, a data collection device, and smart sensors. Basically, the smart sensor monitoring system gets sensed data from smart sensors via the data collection device. To get sensed data, this paper proposes an enhanced fast sensor collection algorithm. Smart sensors in the proposed method learn their communication and collisions to adjust their schedule to gather sensor data from smart sensors, and find out suitable time to send sensor data. The proposed method also uses a wake-up signal to extend the lifetime of the smart sensor. In addition, this paper deals with the design and implementation of the smart sensor monitoring system, and the performance evaluation to verify the superiority of the proposed method. The proposed method is able to read sensor data 8% faster than the traditional approach.	algorithm;heuristic;mobile app;performance evaluation;self-awareness;sensor;simulation;smart tv;smart transducer;smartphone;ubiquitous computing	Hyuntae Cho;Chong-Min Kyung;Yunju Baek	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637391	sensor web;embedded system;real-time computing;computer science;engineering;sds protocol;group method of data handling;key distribution in wireless sensor networks;mobile computing;computer security;internet of things;intelligent sensor;visual sensor network	Robotics	0.015015716826495362	32.49934144347806	157660
7140dfc69ed2ca65dd8bbdcf5d5b3742f2d839c2	a fast x86 implementation of select		Rank and select are fundamental operations in succinct data structures, that is, data structures whose space consumption approaches the information-theoretic optimal. The performance of these primitives is central to the overall performance of succinct data structures. Traditionally, the select operation is the harder to implement efficiently, and most prior implementations of select on machine words use 50-80 machine instructions. (In contrast, rank on machine words can be implemented in only a handful of instructions on machines that support POPCOUNT.) However, recently Pandey et al. [15] gave a new implementation of machine-word select that uses only four x86 machine instructions; two of which were introduced in Intel’s Haswell CPUs. In this paper, we investigate the impact of this new implementation of machine-word select on the performance of general bit-vector-select. We first compare Pandey et al.’s machine-word select to the state-of-the-art implementations of Zhou et al. [20] (which is not specific to Haswell) and Gog et al. [8] (which uses some Haswell-specific instructions). We exhibit a speedup of 2× to 4×. We then study the impact of plugging Pandey et al.’s machine-word select into two state-of-the-art bit-vector-select implementations. Both Zhou et al.’s and Gog et al.’s select implementations perform a single machine-word select operation for each bit-vector select. We replaced the machine-word select with the new implementation and compared performance. Even though there is only a single machineword select operation, we still obtained speedups of 20% to 68%. We found that the new select not only reduced the number of instructions required for each bit-vector select, but also improved CPU instruction cache performance and memory-access parallelism. 1998 ACM Subject Classification “E.1 DATA STRUCTURES”, “E.2 DATA STORAGE REPRESENTATIONS”	bit array;central processing unit;data structure;hamming weight;haswell (microarchitecture);information theory;microsoft word for mac;parallel computing;speedup;x86	Prashant Pandey;Michael A. Bender;Rob Johnson	2017	CoRR		cache;parallel computing;select;implementation;succinct data structure;x86;speedup;computer architecture;data structure;computer science	OS	1.5035977547179311	41.54810708156605	157755
e56c6bb4730562c470fb16d149087be2424dbb99	scheduling independent tasks on heterogeneous parallel computing environments under the unidirectional one-port model	independent task;scheduling algorithm;collective communication;heterogeneous parallel computing environ- ment;steady state;parallel computer;col	For execution of computation-intensive applications, one of the most important paradigms is to divide the application into a large number of small independent tasks and execute them on heterogeneous parallel computing environments (abbreviated by HPCEs). In this paper, we aim to execute independent tasks efficiently on HPCEs. We consider the problem to find a schedule that maximizes the throughput of task execution for a huge number of independent tasks. First, we show that we can find, in polynomial time, a schedule that attains the optimal throughput. This algorithm, however, uses the ellipsoid method, hence it is very time-consuming. Therefore, secondly, we propose a fast -approximation algorithm for any constant . In addition, we also show that the framework of our approximation algorithm can be applied to other collective communications such as the gather operation. keywords: heterogeneous parallel computing environment, independent task, scheduling algorithm, steady state, collective communication	approximation algorithm;computation;ellipsoid method;fast fourier transform;parallel computing;polynomial;schedule (computer science);scheduling (computing);steady state;throughput;time complexity	Fukuhito Ooshita;Susumu Matsumae;Toshimitsu Masuzawa	2006			computer science;parallel computing;distributed computing;clamping;line (geometry);perpendicular;curvature	HPC	-4.45617291747396	40.44991369407272	157831
dd444a673efeb51662934c22bd12fe72dd284087	computational experience with branch, cut and price algorithms in grid environments	distributed system;haute performance;systeme reparti;branching;distributed computing;sistema repartido;computer experiment;mathematical programming;ramificacion;algorithme reparti;alto rendimiento;calculo repartido;ramification;algoritmo repartido;distributed algorithm;programmation mathematique;high performance;calcul reparti;programacion matematica;geographic distribution;large scale optimization	This paper presents our computational experience with parallel Branch, Cut and Price algorithms in a geographically-distributed grid environment. After a brief description of our framework for solving large-scale optimization problems, we describe the experimental grid environment and the tests performed, presenting the obtained performance	algorithm;computation;mpich;mathematical optimization;overhead (computing);performance evaluation;reduction (complexity);search tree;solver	Sonya Marcarelli;Emilio Pasquale Mancini;Umberto Villano	2006		10.1007/11942634_14	distributed algorithm;computer experiment;branching;computer science;artificial intelligence;theoretical computer science;distributed computing;ramification;algorithm	HPC	-3.1762308724389925	33.8299905300569	157924
acde2243a2e1e35fe612d1ef25b62c1d4b7d2c4e	register renaming for x86 superscalar design	instruction level parallel;microprocessors;register write;register renaming;register read;parallel architectures microprocessor chips;intel x86 superscalar design;simulation results register renaming intel x86 superscalar design storage conflicts instruction level parallelism data lengths register write register read hardware renaming schemes aggressive superscalar machine model;storage conflicts;out of order;computational modeling;parallel architectures;registers;data dependence;aggressive superscalar machine model;proceedings paper;electrostatic precipitators;councils;data lengths;computer science;performance ratio;registers hardware costs microprocessors computational modeling computer science parallel processing electrostatic precipitators councils out of order;instruction level parallelism;machine model;simulation results;parallel processing;microprocessor chips;hardware renaming schemes;hardware	CH CX CL ECX Register renaming eliminates storage conJicts for registers to allow more instruction level parallelism. This idea requires nontrivial implementation, however, especially when registers are accessible with different j e l d s and data lengths. As a result, not all bits in a register are to be updated upon a register write, and a register read may data-dependent on multiple register writes. We propose two hardware renaming schemes to solve these d@culties: One for its ultimate performance, and the other for its desirable cost/performance ratio. We evaluate these two schemes on an aggressive superscalar machine model for Intel 80x86 architecture. Simulation results show that the second scheme can effectively reduce the hardware cost while retaining about 99% of the performance ofthe j rs t .	data dependency;instruction-level parallelism;nikon cx format;parallel computing;register renaming;simulation;superscalar processor;x86	Chang-Chung Liu;R.-Ming Shiu;Chung-Ping Chung	1996		10.1109/ICPADS.1996.517580	parallel processing;computer architecture;parallel computing;register window;computer hardware;control register;computer science;out-of-order execution;operating system;register renaming;stack register;instruction register;processor register;flags register;computational model;hardware register;register allocation;instruction-level parallelism;register file;status register;memory address register	Arch	7.817954224330897	43.1993876220769	158193
e229f5fa943afb31d6219892cd47c73898a7285b	efficient model partitioning for distributed model transformations	atl;model transformation;data distribution;mapreduce;static analysis	As the models that need to be handled in model-driven engineering grow in scale, scalable algorithms for model transformation (MT) are becoming necessary. Programming models such as MapReduce or Pregel may simplify the development of distributed model transformations. However, because of the dense inter-connectivity of models and the complexity of transformation logics, scalability in distributed model processing is challenging.   In this paper, we adapt existing formalization of uniform graph partitioning to the case of distributed MTs by means of binary linear programming. Moreover, we propose a data distribution algorithm for declarative model transformation based on static analysis of relational transformation rules. We first extract footprints from transformation rules. Then we propose a fast data distribution algorithm, driven by the extracted footprints, and based on recent results on balanced partitioning of streaming graphs. To validate our approach, we apply it to an existing distributed MT engine for the ATL language, built on top of MapReduce. We implement our heuristic as a custom split algorithm for ATL on MapReduce and we evaluate its impact on remote access to the underlying backend.	declarative programming;edge dominating set;graph partition;greedy algorithm;heuristic;linear programming;mapreduce;model transformation;model-driven architecture;model-driven engineering;persistence (computer science);persistence framework;scalability;static program analysis	Amine Benelallam;Massimo Tisi;Jesús Sánchez Cuadrado;Juan de Lara;Jordi Cabot	2016		10.1145/2997364.2997385	real-time computing;computer science;theoretical computer science;database;programming language;static analysis	DB	-3.3790909599215686	42.80708577473945	158284
571f3eb70cd7f21a4dd1c077d71adfc69aa9384a	code obfuscation using very long identifiers for fft motion estimation models in embedded processors	hdl;psnr;motion estimation;fft;fpga;c compiler;nios ii;obfuscation;embedded processor;block matching algorithm	Motion estimation is extensively used in multimedia tasks, video coding standards and home consumer devices, appearing in many FFT-based motion algorithms. On other hand, the intellectual properties of embedded microprocessor systems are typically delivered on HDL and C source code levels. Obfuscating the code is most often the only way to protect and avoid reverse engineering. This paper presents an evaluation of operations widely used in motion estimation for an embedded microprocessor for protection purposes. A set of open source obfuscation tools has been developed that allows the use of very long and hard-to-read identifiers. The implementation of comment methods also allows for the addition of copyright and limited warranty information. The obfuscated code with identifiers of up to 2,048 characters in length is tested for Altera’s and Xilinx’s field programmable gate arrays for a typical HDL example. Compiler penalties as well as FFT runtime results are reported.	algorithm;american and british english spelling differences;central processing unit;compile time;compiler;data compression;embedded system;fast fourier transform;hardware description language;identifier;microprocessor;motion estimation;nios embedded processor;obfuscation (software);open-source software;reverse engineering;systems design;vhdl;verilog;video coding format;world wide web;xilinx ise	Uwe Meyer-Bäse;Anke Meyer-Bäse;Diego González;Guillermo Botella Juan;Carlos García;Manuel Prieto	2014	Journal of Real-Time Image Processing	10.1007/s11554-014-0421-2	embedded system;computer vision;fast fourier transform;parallel computing;real-time computing;obfuscation;peak signal-to-noise ratio;computer science;motion estimation;block-matching algorithm;field-programmable gate array	EDA	7.0124164029583005	45.22358493459094	158410
ae2b0c075180e80866f8ecf7e4b803cb34879f57	pervasive parallelism in data mining: dataflow solution to co-clustering large and sparse netflix data	memory management;predictive modeling;real time;program design;production system;data mining;co clustering;domain knowledge;large scale;dataflow;data mining application;prediction model;scalability	All Netflix Prize algorithms proposed so far are prohibitively costly for large-scale production systems. In this paper, we describe an efficient dataflow implementation of a collaborative filtering (CF) solution to the Netflix Prize problem [1] based on weighted coclustering [5]. The dataflow library we use facilitates the development of sophisticated parallel programs designed to fully utilize commodity multicore hardware, while hiding traditional difficulties such as queuing, threading, memory management, and deadlocks.  The dataflow CF implementation first compresses the large, sparse training dataset into co-clusters. Then it generates recommendations by combining the average ratings of the co-clusters with the biases of the users and movies. When configured to identify 20x20 co-clusters in the Netflix training dataset, the implementation predicted over 100 million ratings in 16.31 minutes and achieved an RMSE of 0.88846 without any fine-tuning or domain knowledge. This is an effective real-time prediction runtime of 9.7 us per rating which is far superior to previously reported results. Moreover, the implemented co-clustering framework supports a wide variety of other large-scale data mining applications and forms the basis for predictive modeling on large, dyadic datasets [4, 7].	algorithm;biclustering;cluster analysis;collaborative filtering;data mining;dataflow;deadlock;dyadic transformation;memory management;multi-core processor;netflix prize;parallel computing;predictive modelling;real-time operating system;real-time transcription;sparse matrix;thread (computing)	Srivatsava Daruru;Nena M. Marin;Matt Walker;Joydeep Ghosh	2009		10.1145/1557019.1557140	predictive analytics;scalability;computer science;artificial intelligence;theoretical computer science;machine learning;dataflow;data mining;database;program design language;predictive modelling;production system;biclustering;domain knowledge;memory management	HPC	-1.5771239955531167	42.11118216217704	158419
13c191d943f359a659a39bb486a7bd340879b44b	an adaptive scalable watermark scheme for high-quality audio archiving and streaming applications	watermarking;advanced audio zip;image coding;watermarking streaming media image coding transcoding cryptography humans decoding proposals protection consumer electronics;unauthorized access adaptive watermark embedding high quality audio archiving audio streaming application standardized scalable audio coder advanced audio zip aaz encryption mechanism;data compression;decoding;adaptive codes watermarking audio coding data compression data encapsulation authorisation cryptography;authorisation;adaptive watermark embedding;consumer electronics;adaptive codes;standardized scalable audio coder;data encapsulation;unauthorized access;protection;audio coding;streaming media;cryptography;aaz;humans;proposals;transcoding;encryption mechanism;high quality audio archiving;audio streaming application	In this paper, we present a scalable (i.e. lossy-to-lossless) watermark scheme based on a recently standardized scalable audio coder-AAZ (R.S. Yu, et al., 2004). The proposed framework enables the recovery of the original lossless audio after watermark embedding, and in the meanwhile, is able to make the watermark adaptive such that the watermark distortion to the lossy host audio is minimized. An encryption mechanism is further employed for restricting unauthorized access to lossless audio and watermark removal. Based on this framework, we elaborate its possible applications on high-quality audio archiving and streaming. Experimental results demonstrate the validity of our proposal.	archive;authorization;data compression;digital watermarking;distortion;encryption;lossless compression;lossy compression;scalability;streaming media	Zhi Li;Qibin Sun;Yong Lian	2005	2005 IEEE International Conference on Multimedia and Expo	10.1109/ICME.2005.1521405	data compression;transcoding;computer science;cryptography;theoretical computer science;multimedia;internet privacy;watermark;statistics	EDA	8.140024985761842	36.55488086278581	158432
ccd7a0c2903427a1dc6a9829642c563e81d55fbd	advanced computing and optimization infrastructure for extremely large-scale graphs on post peta-scale supercomputers		In this talk, we present our ongoing research project. The objective of this project is to develop advanced computing and optimization infrastructures for extremely large-scale graphs on post peta-scale supercomputers. We explain our challenge to Graph 500 and Green Graph 500 benchmarks that are designed to measure the performance of a computer system for applications that require irregular memory and network access patterns. The 1st Graph500 list was released in November 2010. The Graph500 benchmark measures the performance of any supercomputer performing a BFS (Breadth-First Search) in terms of traversed edges per second (TEPS). In 2014 and 2015, our project team was a winner of the 8th, 10th, and 11th Graph500 and the 3rd to 6th Green Graph500 benchmarks, respectively. We also present our parallel implementation for large-scale SDP (SemiDefinite Programming) problem. The semidefinite programming (SDP) problem is a predominant problem in mathematical optimization. The primal-dual interior-point method (PDIPM) is one of the most powerful algorithms for solving SDP problems, and many research groups have employed it for developing software packages. We solved the largest SDP problem (which has over 2.33 million constraints), thereby creating a new world record. Our implementation also achieved 1.774 PFlops in double precision for large-scale Cholesky factorization using 2,720 CPUs and 4,080 GPUs on the TSUBAME 2.5 supercomputer.	supercomputer	Katsuki Fujisawa;Toshio Endo;Yuichiro Yasui	2016		10.1007/978-3-319-42432-3_33	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-2.9191121120278276	42.04625121600008	158551
0a57cd63d079d4c46b2cfa9d9525afccce8e14f6	a blocking algorithm for fft on cache-based processors	algorithme rapide;evaluation performance;algorithm performance;performance evaluation;blocage;evaluacion prestacion;bloqueo;cache memory;blocking;transformacion fourier rapida;antememoria;fast fourier transform;antememoire;resultado algoritmo;fast algorithm;performance algorithme;ghz range;procesador;processeur;transformation fourier rapide;domaine frequence ghz;algoritmo rapido;processor;fast fourier transformation	In this paper, we propose a blocking algorithm for computing large one-dimensional fast Fourier transform (FFT) on cache-based processors. Our proposed FFT algorithm is based on the six-step FFT algorithm. We show that the block six-step FFT algorithm improves performance by effectively utilizing the cache memory. Performance results of one-dimensional FFTs on the Sun Ultra 10 and PentiumIII PC are reported. We succeeded in obtaining performance of about 108MFLOPS on the Sun Ultra 10 (UltraSPARC-IIi 333MHz) and about 247MFLOPS on the 1GHz PentiumIII PC for 220-point FFT.	algorithm;fast fourier transform	Daisuke Takahashi	2001		10.1007/3-540-48228-8_58	embedded system;fast fourier transform;cache-oblivious algorithm;parallel computing;cooley–tukey fft algorithm;split-radix fft algorithm;computer science;bruun's fft algorithm;prime-factor fft algorithm;algorithm;statistics	Theory	0.18656462248775044	38.54732656058976	158693
4258a10ac90538e1250ac6262cd20edb8a980fe5	video analysis for universal multimedia messaging (invited)	keyframe selection;image segmentation;mpeg 7 standard video analysis universal multimedia messaging next generation mobile computing mobile communication devices video structuring;mobile communication devices;prototypes;video analysis;video compression;media adaptation;mobile computer;universal multimedia messaging;code standards;camera work;multimedia systems;computer networks;multimedia computing;video coding;multimedia computing mobile communication mobile computing mpeg 7 standard computer networks prototypes multimedia systems image segmentation video compression discrete cosine transforms;discrete cosine transforms;multimedia communication;multimedia messaging;mobile communication;next generation;electronic messaging;next generation mobile computing;mobile computing;mpeg 7 standard;video coding multimedia communication electronic messaging mobile computing code standards;video structuring	Multimedia messaging is expected to become a major application for next generation mobile computing and communication devices. Heterogeneous capabilities of these devices, however require adaptation of multimedia messages before rendering on any specific device. To achieve this in case of video messages, video analysis has to be performed to extract the structure of the video and control the message adaptation. This paper describes the universal multimedia messaging scenario and presents a short overview of existing video structuring approaches. A computationally efficient method for analyzing a video message in this scenario is introduced and a prototype for universal multimedia messaging conforming to the emerging MPEG-7 standard is outlined.		André Kaup;Siripong Treetasanatavorn;Uwe Rauschenbach;Jörg Heuer	2002		10.1109/IAI.2002.999920	data compression;h.263;mobile telephony;computer science;video tracking;prototype;multimedia;image segmentation;internet privacy;mobile computing;world wide web	Logic	6.3518894525251515	33.48230027564869	158732
50007988ad615064aa12b3782930b47cc4b7ef0f	pilot-abstraction: a valid abstraction for data-intensive applications on hpc, hadoop and cloud infrastructures?		HPC environments have traditionally been designed to meet the compute demand of scientific applications and data has only been a second order concern. With science moving toward data-driven discoveries relying more and more on correlations in data to form scientific hypotheses, the limitations of existing HPC approaches become apparent: Architectural paradigms such as the separation of storage and compute are not optimal for I/O intensive workloads (e. g. for data preparation, transformation and SQL workloads). While there are many powerful computational and analytical kernels and libraries available on HPC (e. g. for scalable linear algebra), they generally lack the usability and variety of analytical libraries found in other environments (e. g. the Apache Hadoop ecosystem). Further, there is a lack of abstractions that unify access to increasingly heterogeneous infrastructure (HPC, Hadoop, clouds) and allow reasoning about performance trade-offs in these complex environments. At the same time, the Hadoop ecosystem is evolving rapidly with new frameworks for data processing and has established itself as de-facto standard for data-intensive workloads in industry and is increasingly used to tackle scientific problems. In this paper, we explore paths to interoperability between Hadoop and HPC, examine the differences and challenges, such as the different architectural paradigms and abstractions, and investigate ways to address them. We propose the extension of the Pilot-Abstraction to Hadoop to serve as interoperability layer for allocating and managing resources across different infrastructures providing a degree of unification in the concepts and implementation of resource management across HPC, Hadoop and other infrastructures. For this purpose, we integrate Hadoop compute and data resources (i. e. YARN and HDFS) with the Pilot-Abstraction. In-memory capabilities have been successfully deployed to enhance the performance of large-scale data analytics approaches (e. g. iterative machine learning algorithms) for which the ability to re-use data across iterations is critical. As memory naturally fits in with the Pilot concept of re. taining resources for a set of tasks, we propose the extension of the Pilot-Abstraction to in-memory resources. These enhancements to the Pilot-Abstraction have been implemented in BigJob. Further, we validate the abstractions using experiments on cloud and HPC infrastructures investigating the performance of the Pilot-Data and Pilot-Hadoop implementation, HDFS and Lustre for Hadoop MapReduce workloads, and Pilot-Data Memory for KMeans clustering. Using Pilot-Hadoop we evaluate the performance of Stampede, a compute-centric resource, and Gordon, a resource designed for data-intensive workloads providing additional memory and flash storage. Our benchmarks of Pilot-Data Memory show a significant improvement compared to the file-based Pilot-Data for KMeans with a measured speedup of 212.	algorithm;apache hadoop;cloud computing;cluster analysis;data-intensive computing;ecosystem;experiment;fits;flash memory;in-memory database;input/output;interoperability;iteration;k-means clustering;library (computing);linear algebra;lustre;machine learning;mapreduce;sql;scalability;speedup;unification (computer science);usability	André Luckow;Pradeep Kumar Mantha;Shantenu Jha	2015	CoRR		parallel computing;computer science;operating system;data mining;database;distributed computing	HPC	-4.192174148323158	44.5695186758917	159073
876aad8f5ceb6bb4d7cc2e021b3f8602de59deeb	memocode 2012 hardware/software codesign contest: dna sequence aligner	dna;biology computing;genomics;hardware software codesign;storage management;fpgas dna sequence alignment string matching hardware software codesign gpgpus multicore;integrated circuit design;gpgpus memocode 2012 hardware software codesign contest dna sequence aligner exact substring matching dna sequence alignment problem 100 base pair short read sequences 3 million base pair reference genome hash algorithm convey hc 1 fpga multicore hybrid aggressive memory system burrows wheeler hash hybrid 12 core intel system;pattern matching;genomics bioinformatics dna educational institutions humans runtime indexes;storage management biology computing dna field programmable gate arrays genomics graphics processing units hardware software codesign integrated circuit design multiprocessing systems pattern matching;graphics processing units;multiprocessing systems;field programmable gate arrays	The MEMOCODE design contest for 2012 is exact substring matching: a simplified form of the DNA sequence alignment problem. The challenge is to efficiently locate millions of 100-base-pair short read sequences in a 3-million-base-pair reference genome. Contestants had a month to create a fast system that ran on a given set of test data. Entries were judged both on absolute time and the product of time and system cost. The two winning groups, which were invited to contribute papers describing their solutions, judiciously chose algorithms that exploited powerful hardware. The two winning entries employed a hash algorithm running on a Convey HC-1 FPGA/multicore hybrid with an aggressive memory system and a Burrows-Wheeler/hash hybrid running on a 12-core Intel system was second.	algorithm;algorithm selection;burrows–wheeler transform;field-programmable gate array;hash function;multi-core processor;sequence alignment;substring;test data	Stephen A. Edwards	2012	Tenth ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMCODE2012)	10.1109/MEMCOD.2012.6292303	genomics;parallel computing;real-time computing;computer science;theoretical computer science;pattern matching;programming language;dna;field-programmable gate array;integrated circuit design	EDA	0.6581545394244236	42.91463608447527	159078
0d3168b903c013c73d5f0ee14366466a35b24bfe	high-throughput low-area design of aes using constant binary matrix-vector multiplication	advanced encryption standard aes;pipelining;matrix vector multiplication	In spite of many outstanding studies, the hardware implementation of Advanced Encryption Standard (AES) algorithm is still challenging because of recurrent computations in Galois Field GF(2 8 ). In this paper, in order to revolution up the hardware implementation, we propose a new design of  SubBytes  and  MixColumns  in AES using constant binary matrix-vector multiplications. By employing constant binary matrices reduced to AND and XOR operations, we could promote a synthesis compiler to optimize the design more efficiently. In addition, in order to achieve higher throughput, we propose a four-stage pipelined AES architecture. Evaluations show that the proposed method improves both in term of throughput and area complexity. Our proposed design of AES achieved 3.8  Gbps throughput with about 9.8k gates and 1k flip-flops which was the highest throughput and the lowest gate count at the same time, on 180 nm CMOS technology. By applying our proposed method to SubBytes, the area complexity decreased by 8.3% while the latency was reduced by 5.5%.	matrix multiplication;throughput	Hokyoon Lee;Yoonah Paik;Jaeyung Jun;Youngsun Han;Seon Wook Kim	2016	Microprocessors and Microsystems - Embedded Hardware Design	10.1016/j.micpro.2016.10.003	parallel computing;real-time computing;matrix multiplication;computer science;theoretical computer science;aes implementations;pipeline	EDA	9.8382430891827	45.02189245088635	159102
77bc25cc8ae6f71ebef4f049ac9f0a3f7542b3d4	high precision integer addition, subtraction and multiplication with a graphics processing unit	parallelisme;subtraction;algoritmo paralelo;methode diviser pour regner;unite centrale;parallel algorithm;algorithmique;carte graphique;central unit;sustraccion;metodo dividir para vencer;soustraction;arithmetique;transformacion fourier rapida;system on a chip;algorithme parallele;parallelism;aritmetica;paralelismo;sistema sobre pastilla;algorithmics;algoritmica;arithmetics;divide and conquer method;multiple precision arithmetic;graphic processing unit;unidad central;unidad de proceso grafico;systeme sur puce;graphics processing unit;transformation fourier rapide;fast fourier transformation	"""In this paper we evaluate the potential for using an NVIDIA graphics processing unit (GPU) to accelerate high precision integer multiplication, addition, and subtraction. The reported peak vector performance for a typical GPU appears to offer good potential for accelerating such a computation. Because of limitations in the on-chip memory, the high cost of kernel launches, and the nature of the architecture's support for parallelism, we used a hybrid algorithmic approach to obtain good performance on multiplication. On the GPU itself we adapt the Strassen FFT algorithm to multiply 32KB chunks, while on the CPU we adapt the Karatsuba divide-and-conquer approach to optimize application of the GPU's partial multiplies, which are viewed as """"digits"""" by our implementation of Karatsuba. Even with this approach, the result is at best a factor of three increase in performance, compared with using the GMP package on a 64-bit CPU at a comparable technology node. Our implementations of addition and subtraction achieve up to a factor of eight improvement. We identify the issues that limit performance and discuss the likely impact of planned advances in GPU architecture."""	graphics processing unit	Niall Emmart;Charles C. Weems	2010	Parallel Processing Letters	10.1142/S0129626410000259	system on a chip;fast fourier transform;parallel computing;subtraction;computer science;operating system;parallel algorithm;algorithmics;algorithm;computer graphics (images)	HPC	-0.43533158462477073	39.36654138271336	159343
96e39de802eebca204f7fab7f0303ad67da4f855	simple, fast and practicable algorithms for cholesky, lu and qr decomposition using fast rectangular matrix multiplication		This note presents fast Cholesky/LU/QR decomposition algorithms with O(n2.529) time complexity when using the fastest known matrix multiplication. The algorithms have potential application, since a quickly made implementation using Strassen multiplication has lesser execution time than the employed by the GNU Scientific Library for the same task in at least a few examples. The underlaying ideas are very simple. Despite this, I have been unable to find these methods in the literature.		Crist'obal Camarero	2018	CoRR			HPC	-0.6755025219745743	39.53940392606298	159491
cea3a2c56255d979cd72c35e2d9eca63bdd49444	signal processing in c: by christopher e. reid, valid logic systems and thomas b. passin, mitre corporation. publishers: john wiley & sons, inc., 1992, isbn 0-471-52713	signal processing	From the Publisher:Recent advances have made digital signal processing (DSP) one of the fastest growing areas in digital technology, expanding into many new applications, particularly on personal computers. This serves as a fundamental C programming reference, providing well structured, easy-to-use signal processing algorithms. Emphasizes the use of these algorithms (rather than their mathematical derivations) as well as C programming techniques. Prepared by authors who have written many algorithms in C for both simulations and real-time embedded systems, it presents algorithms for all topics, including a standard interface to these routines. In addition, execution time/computational complexity, memory requirements and algorithm tradeoffs are discussed.	international standard book number;john d. wiley;signal processing	Petri Jarske	1992	Signal Processing	10.1016/0165-1684(92)90026-S	computer vision;computer science;signal processing;mathematics	Arch	7.935496073177503	40.88679226208228	159746
81d829245ac89f5a0c8f9508e849d1ebb4e940d3	parallel data cubes on multi-core processors with multiple disks	multiple disk;new parallel data cube;construction method;previous method;contemporary data warehousing repository;new parallel;mcmd-cube method;parallel disk;data cube;multi-core processor	On-line Analytical Processing (OLAP) has become one of the most powerful and prominent technologies for knowledge discovery in VLDB (Very Large Database) environments. Central to the OLAP paradigm is the data cube, a multi-dimensional hierarchy of aggregate values that provides a rich analytical model for decision support. Various sequential algorithms for the efficient generation of the data cube have appeared in the literature. However, given the size of contemporary data warehousing repositories, multi-processor solutions are crucial for the massive computational demands of current and future OLAP systems. In this paper we discuss the development of MCMD-CUBE , a new parallel data cube construction method for multi-core processors with multiple disks. We present experimental results for a Sandy Bridge multi-core processor with four parallel disks. Our experiments indicate that MCMD-CUBE achieves very close to linear speedup. A critical part of our MCMDCUBE method is parallel sorting. We developed a new parallel sorting method termed MCMD-SORT for multi-core processors with multiple disks which significantly outperforms ∗Research partially supported by the IBM Centre for Advanced Studies Canada and the Natural Sciences and Engineering Research Council of Canada. Copyright c © 2008 F.Dehne & H.Zaboli. Permission to copy is hereby granted provided the original copyright notice is reproduced in copies made. the best previous method (PMSTXXL).	aggregate data;algorithm;central processing unit;computation;cube mapping;data cube;decision support system;experiment;ibm centers for advanced studies;klee–minty cube;merge sort;multi-core processor;multiprocessing;olap cube;online analytical processing;parallel computing;programming paradigm;sandy bridge;sorting;sorting algorithm;speedup;vldb	Frank Dehne;Hamidreza Zaboli	2011			parallel computing;computer science;theoretical computer science;operating system;database;distributed computing	DB	-0.9471595626638496	37.36930167050813	160117
264e342fd2ee1e1f62843def447f9f92ece5f2f7	file archival techniques using data compression	data compression;huffman coding;microcomputer systems;file compression;variable length code;huffman codes;storage capacity	The performance of most small computer systems is determined, to a large extent, by the characteristics of their mass storage devices. Data compression can expand the storage capacity of such devices and also slightly increase their speed. Here, a summary of one method of data compression using Huffman variable length coding is presented, along with statistics of effectiveness for various file types and practical techniques for integrating such methods into a small computer system.	computer;data compression;huffman coding;mass storage;variable-length code	Michael A. Pechura	1982	Commun. ACM	10.1145/358628.358635	data compression;lempel–ziv–stac;data compression ratio;canonical huffman code;computer hardware;computer science;theoretical computer science;database;lossless compression;tunstall coding;volume;deflate;statistics;huffman coding	Networks	9.639609234610939	37.8683777745324	160475
43f600de0558c915a126c1cfb4efbaaa7a5a63fe	an experimental study on deep learning based on different hardware configurations		Deep learning has exhibited high accuracy and applicability in machine learning field recently, by consuming tremendous computational resources processing massive data. To improve the performance of deep learning, GPUs have been introduced to accelerate the training phase. The complex data processing infrastructure demands high-efficient collaboration among underlying hardware components, such as CPU, GPU, memory, and storage devices. Unfortunately, few work has presented a systematic analysis about the impact of hardware configurations on the overall performance of deep learning. In this paper, we aim to make an experimental study on a standalone system to evaluate how various hardware configurations affect the overall performance of deep learning. We conducted a series of experiments using varied configurations on storage devices, main memory, CPU, and GPU to observe the overall performance quantitatively. Based on analyzing these results, we found that the performance greatly relies on the hardware configurations. Specifically, the computation is still the primary bottleneck as double GPUs and triple GPUs shorten the execution time by 44\% and 59\% respectively. Besides, both CPU frequency and storage subsystem can significantly affect running time while the memory size has no obvious effect on the running time for training neural network models. We believe our experimental results can help shed light on further optimizing the performance of deep learning in computer systems.	central processing unit;computation;computational resource;computer data storage;deep learning;experiment;graphics processing unit;machine learning;run time (program lifecycle phase);time complexity	Jingjun Li;Qiang Cao;Chuanyi Qi;Jianzhong Huang;Changsheng Xie	2017	2017 International Conference on Networking, Architecture, and Storage (NAS)	10.1109/NAS.2017.8026843	real-time computing;computation;deep learning;artificial neural network;computer science;computer hardware;complex data type;bottleneck;artificial intelligence	HPC	2.795832324689786	42.67744116159255	160614
0c178db764b1885febe93cde4dc34a32fe4d5789	on analyzing large graphs using gpus	graph theory;gpu optimization;cuda;graph problems;shared memory systems;triangle counting;data structures;simultaneous memory accesses large graphs gpu online social networks graphical processing units general purpose problems global memory memory hierarchy data structure breadth first search tree combinatorial counting problems triangle counting algorithm shared memory;tree searching;global memory;graphics processing units instruction sets data structures context social network services testing partitioning algorithms;gpu optimization triangle counting cuda graph problems global memory;tree searching data structures graph theory shared memory systems	Studying properties of graphs is essential to various applications, and recent growth of online social networks has spurred interests in analyzing their structures using Graphical Processing Units (GPUs). Utilizing the faster available shared memory on GPUs have provided tremendous speed-up for solving many general-purpose problems. However, when data required for processing is large and needs to be stored in the global memory instead of the shared memory, simultaneous memory accesses by threads in execution becomes the bottleneck for achieving higher throughput. In this paper, for storing large graphs, we propose and evaluate techniques to efficiently utilize the different levels of the memory hierarchy of GPUs, with the focus being on the larger global memory. Given a graph G = (V, E), we provide an algorithm to count the number of triangles in G, while storing the adjacency information on the global memory. Our computation techniques and data structure for retrieving the adjacency information is derived from processing the breadth-first-search tree of the input graph. Also, techniques to generate combinations of nodes for testing the properties of graphs induced by the same are discussed in detail. Our methods can be extended to solve other combinatorial counting problems on graphs, such as finding the number of connected sub graphs of size k, number of cliques (resp. independent sets) of size k, and related problems for large data sets. In the context of the triangle counting algorithm, we analyze and utilize primitives such as memory access coalescing and avoiding partition camping that offset the increase in access latency of using a slower but larger global memory. Our experimental results for the GPU implementation show at least 10 times speedup for triangle counting over the CPU counterpart. Another 6 - 8% increase in performance is obtained by utilizing the above mentioned primitives as compared to the naïve implementation of the program on the GPU.	access time;algorithm;breadth-first search;cas latency;central processing unit;computation;computer memory;data structure;general-purpose modeling;graphics processing unit;heuristic (computer science);independent set (graph theory);memory hierarchy;overhead (computing);qualitative comparative analysis;search tree;shared memory;social network;speedup;streaming media;throughput	Amlan Chatterjee;Sridhar Radhakrishnan;John K. Antonio	2013	2013 IEEE International Symposium on Parallel & Distributed Processing, Workshops and Phd Forum	10.1109/IPDPSW.2013.235	cuda pinned memory;uniform memory access;distributed shared memory;shared memory;parallel computing;out-of-core algorithm;distributed memory;computer science;physical address;theoretical computer science;distributed computing;overlay;flat memory model;computing with memory;memory map	HPC	-2.6262061195755444	42.64690277956965	160642
4b3971f5e666d1dbfe68e690064076a9c2c1c6e9	research on gpu-based computation method for line-of-sight queries	gpu-based computation;gpu-based computation method;aggregate multiple single los;military simulation;single-query level;line-of-sight queries;graphic process unit;los culling method;batch-query level;los computation;batch query;computational data;instruction sets;algorithm design and analysis;acceleration;time complexity;computational modeling	The line of sight (LOS) queries often consume a significant fraction of system resources in military simulation. The high time complexity of LOS computation not only limit the amount of entities in the simulation, but also hamper the CPU from doing more urgent and important tasks. To overcome this problem, we utilize graphic process units (GPU) to accelerate the LOS computation at two levels, single-query level and batch-query level. First, we decouple the dependency of data to parallelize the whole process of LOS computation, so that the potential of GPU can be exploited at single-query level. Second, a combine-and-partition algorithm is proposed to aggregate multiple single LOS queries into a GPU-based computation, so that the count of parallel threads can be maximized and the impact of communication latency can be minimized. It uses a combine function to assemble scattered single query into a batch query, and uses a partition function to get computational data or dispatch results. An early version of our prototype demonstrates at least 3x speedup at single-query level, and we expect to achieve a speedup eyond 200x at batch-query level based on the LOS culling methods in references 1 and 2.	computation;graphics processing unit	Bin Liu;Yiping Yao;Wenjie Tang;Yang Lu	2012			acceleration;algorithm design;parallel computing;simulation;computer science;theoretical computer science;operating system;instruction set;distributed computing;programming language;computational model	Logic	-0.88910548731809	41.38419177104836	160661
6cb33cb05de6a0a2c3283ae030e7943abedcf20c	acceleration experiment of genetic computations for sudoku solution on multi-core processors	parallel computing;multi core processor;evolutionary computation;genetics;parallel computer;genetic algorithm;genetic algorithms;parallel processing;evolutionary computing	We focus on parallel-processing effect for Sudoku-solving and we show that diversifying initial values can reduce the Sudoku solution time. In an experiment using the commercially available Intel Corei7 multi-core processor, we show that a correct solution rate of 100% can be achieved with an average execution time of several tens of seconds even for super-difficult problems.	central processing unit;computation;multi-core processor;run time (program lifecycle phase);sudoku	Mikiko Sato;Yuji Sato;Mitaro Namiki	2011		10.1145/2001858.2002107	parallel computing;genetic algorithm;computer science;theoretical computer science;algorithm;evolutionary computation	HPC	-0.3018308893712329	41.44480901738391	160742
2de953868b95608acb0f15feefd9df49bf06e958	bitslice vectors: a software approach to customizable data precision on processors with simd extensions		Customizing the precision of data can provide attractive trade-offs between accuracy and hardware resources. Custom hardware and FPGA designs allow bit-level control over precision, but software is typically limited by the range of types supported by the underlying processor. We propose a new form of vector computing aimed at arrays of custom-precision data on general-purpose processors with SIMD extensions. We represent these vectors in bitslice format and use bitwise instructions to build arithmetic operators that operate on the customized bit precision. We construct a domain-specific code generator that builds bit-level customizable floating-point and integer operators for our vector types. Using a hardware circuit optimization tool we optimize our logical expressions, and synthesize fast software arithmetic operators for bitslice vector types. We evaluate the resulting code and find that advanced logic optimization significantly improves performance. Experiments on a platform with Intel AVX2 SIMD extensions show that this approach is efficient for vectors of low-precision custom floating-point types, while providing arbitrary bit precision.	advanced vector extensions;bit slicing;bit-level parallelism;bitwise operation;central processing unit;code generation (compiler);domain-specific language;electronic circuit;field-programmable gate array;general-purpose modeling;logic optimization;mathematical optimization;simd;significant figures;vector processor	Shixiong Xu;David Gregg	2017	2017 46th International Conference on Parallel Processing (ICPP)	10.1109/ICPP.2017.53	field-programmable gate array;operator (computer programming);logic optimization;parallel computing;software;computer science;bitwise operation;code generation;adder;simd	SE	6.491624232928747	46.07585873753427	160777
08432280f5500d2f242f01ad86133bec89574324	design of multithreaded estimation of distribution algorithms	low priority;bayesian network;decision tree;cluster of workstations;optimization problem;probabilistic model;estimation of distribution algorithm;local structure;message passing interface;model building;high priority;bayesian optimization algorithm;high performance	Estimation of Distribution Algorithms (EDAs) use a probabilistic model of promising solutions found so far to obtain new candidate solutions of an optimization problem. This paper focuses on the design of parallel EDAs. More specifically, the paper describes a method for parallel construction of Bayesian networks with local structures in form of decision trees in the Mixed Bayesian Optimization Algorithm. The proposed Multithreaded Mixed Bayesian Optimization Algorithm (MMBOA) is intended for implementation on a cluster of workstations that communicate by Message Passing Interface (MPI). Communication latencies between workstations are eliminated by multithreaded processing, so in each workstation the high-priority model-building thread, which is communication demanding, can be overlapped by low-priority model sampling thread when necessary. High performance of MMBOA is verified via simulation in TRANSIM tool.	analysis of algorithms;artificial intelligence;bayesian network;bayesian optimization;central processing unit;computation;computational resource;computer cluster;data mining;decision tree;estimation of distribution algorithm;machine learning;mathematical optimization;message passing interface;multithreading (computer architecture);optimization problem;parallel computing;performance prediction;performance tuning;sampling (signal processing);scalability;simulation;software release life cycle;statistical model;thread (computing);workstation	Jiri Ocenasek;Josef Schwarz;Martin Pelikan	2003		10.1007/3-540-45110-2_1	optimization problem;statistical model;parallel computing;model building;estimation of distribution algorithm;computer science;message passing interface;theoretical computer science;machine learning;decision tree;bayesian network;distributed computing	HPC	-1.574714997229452	34.767342569936936	160821
3798efaf5ab15d230782b9f3b48f2fcf0f1b89de	invited — cross-layer approximations for neuromorphic computing: from devices to circuits and systems	energy efficiency;power aware computing learning artificial intelligence neural nets;approximation algorithms;neuromorphics;error tolerance;artificial neural networks;machine learning;approximate logic approximate computing neuromorphic computing neuromorphic algorithm deep learning energy efficient neuromorphic system artificial neural network;bitcoin;sha 256;neuromorphics energy efficiency neurons artificial neural networks machine learning approximation algorithms biological neural networks;neurons;approximate computing;biological neural networks	Neuromorphic algorithms are being increasingly deployed across the entire computing spectrum from data centers to mobile and wearable devices to solve problems involving recognition, analytics, search and inference. For example, large-scale artificial neural networks (popularly called deep learning) now represent the state-of-the art in a wide and ever-increasing range of video/image/audio/text recognition problems. However, the growth in data sets and network complexities have led to deep learning becoming one of the most challenging workloads across the computing spectrum. We posit that approximate computing can play a key role in the quest for energy-efficient neuromorphic systems. We show how the principles of approximate computing can be applied to the design of neuromorphic systems at various layers of the computing stack. At the algorithm level, we present techniques to significantly scale down the computational requirements of a neural network with minimal impact on its accuracy. At the circuit level, we show how approximate logic and memory can be used to implement neurons and synapses in an energy-efficient manner, while still meeting accuracy requirements. A fundamental limitation to the efficiency of neuromorphic computing in traditional implementations (software and custom hardware alike) is the mismatch between neuromorphic algorithms and the underlying computing models such as von Neumann architecture and Boolean logic. To overcome this limitation, we describe how emerging spintronic devices can offer highly efficient, approximate realization of the building blocks of neuromorphic computing systems.	approximate computing;approximation algorithm;artificial neural network;boolean algebra;data center;deep learning;neuromorphic engineering;optical character recognition;requirement;spintronics;von neumann architecture;wearable technology	Priyadarshini Panda;Abhronil Sengupta;Syed Shakib Sarwar;Gopalakrishnan Srinivasan;Swagath Venkataramani;Anand Raghunathan;Kaushik Roy	2016	2016 53nd ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2897937.2905009	embedded system;natural computing;electronic engineering;sha-2;computer science;artificial intelligence;theoretical computer science;machine learning;neuromorphic engineering;efficient energy use;approximation algorithm;artificial neural network;algorithm;unconventional computing	EDA	4.61583500506362	41.74448474721333	160837
d0603225d4dd883ac9de22fb95560d1acb4a64a7	stuck-at fault tolerance in rram computing systems		Emerging metal-oxide resistive switching random-access memory (RRAM) devices and RRAM crossbars have demonstrated their potential in boosting the speed and energy-efficiency of analog matrix-vector multiplication. However, due to the immature fabrication technology, commonly occurring Stuck-At-Faults (SAFs) seriously degrade the computational accuracy of an RRAM-based computing system (RCS). In this paper, we present a fault-tolerant framework for RCS. A mapping algorithm with inner fault tolerance is proposed to convert matrix parameters into RRAM conductances in RCS and tolerate SAFs by fully exploring the available mapping space. Two baseline redundancy schemes are proposed to ensure that RCS is effective when the percentage of faulty RRAM cells is high. To reduce the number of redundant RRAM cells when the SAFs follow a non-uniform distribution or an unknown distribution, a distribution-aware redundancy scheme and a re-configurable redundancy scheme are proposed to provide dynamic fault tolerance. Simulation results show that, the baseline redundancy schemes can improve the recognition accuracy of the MNIST data set to almost the same as the RRAM-fault-free case, with an energy overhead of approximately 30%. When SAFs follow a non-uniform and an unknown distribution, the distribution-aware and re-configurable schemes can reduce the number of redundant RRAM cells from more than 200% to less than 40% and 60%, respectively, without reducing the recognition accuracy.	algorithm;baseline (configuration management);computation;fault tolerance;internet relay chat;mnist database;matrix multiplication;overhead (computing);random access;resistive random-access memory;simulation;stuck-at fault;triple modular redundancy	Lixue Xia;Wenqin Huangfu;Tianqi Tang;Xiling Yin;Krishnendu Chakrabarty;Yuan Xie;Yu Wang;Huazhong Yang	2018	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2017.2776980	stuck-at fault;real-time computing;parallel computing;redundancy (engineering);artificial neural network;fault tolerance;computer science;mnist database;resistive random-access memory	HPC	4.263579275869492	42.718374687004996	160842
32b845a945bf2da5bd6effdcbe03a87abb91b33a	parallel direct solution of linear equations on fpga-based machines	forward backward;field programmable gate array;multiple instance;and forward;sopc;highly parallel block diagonal bordered algorithm parallel direct solution linear equations fpga based machines sparse matrices lu factorization backward substitutions forward substitutions parallel implementations computation intensive process field programmable gate array technologies system on a programmable chip computing platforms configurable computing parallel machine sopc board soft processor;fpga;system on a programmable chip;equations field programmable gate arrays concurrent computing parallel processing sparse matrices supercomputers real time systems costs parallel architectures power system transients;forward backward substitution;parallel architectures;design and implementation;system on chip;matrix decomposition;block diagonalization;parallel machines;parallel implementation;configurable computing;field programmable gate arrays;linear equations;lu factorization;sparse matrices;parallel processing;parallel architectures sparse matrices matrix decomposition parallel machines system on chip field programmable gate arrays	The efficient solution of large systems of linear equations represented by sparse matrices appears in many tasks. LU factorization followed by backward and forward substitutions is widely used for this purpose. Parallel implementations of this computation-intensive process are limited primarily to supercomputers. New generations of Field-Programmable Gate Array (FPGA) technologies enable the implementation of System-On-aProgrammable-Chip (SOPC) computing platforms that provide many opportunities for configurable computing. We present here the design and implementation of a parallel machine for LU factorization on an SOPC board, using multiple instances of a soft processor. A highly parallel Block -Diagonal-Bordered (BDB) algorithm for LU factorization is mapped to our multiprocessor. Our results prove the viability of our FPGA-based approach.	algorithm;bonaventura di bello;computation;field-programmable gate array;lu decomposition;linear equation;linear system;multiprocessing;parallel computing;processor design;qr decomposition;reconfigurable computing;shared memory;sparse matrix;supercomputer;system of linear equations;system on a chip;triangular matrix	Xiaofang Wang;Sotirios G. Ziavras	2003		10.1109/IPDPS.2003.1213224	parallel processing;parallel computing;computer science;theoretical computer science;operating system;distributed computing;field-programmable gate array	HPC	-2.7562967939306606	38.99816754839314	161048
43a380a9cf793e700931f915d2528cff7dc24e89	the signal passing interface and its application to embedded implementation of smart camera applications	face detection;dataflow;multiprocessor commu- nication;speech compression;smart camera	Embedded smart camera systems comprise computation- and resource-hungry applications implemented on small, complex but resource-hardy platforms. Efficient implementation of such applications can benefit significantly from parallelization. However, communication between different processing units is a nontrivial task. In addition, new and emerging distributed smart cameras require efficient methods of communication for optimized distributed implementations. In this paper, a novel communication interface, called the signal passing interface (SPI), is presented that attempts to overcome this challenge by integrating relevant properties of two different, yet important, paradigms in this context-dataflow and message passing interface (MPI). Dataflow is a widely used modeling paradigm for signal processing applications, while MPI is an established communication interface in the general-purpose processor community. SPI is targeted toward computation-intensive signal processing applications, and due to its careful specialization, more performance-efficient for embedded implementation in this domain. SPI is also much easier and more intuitive to use. In this paper, successful application of this communication interface to two smart camera applications has been presented in detail to validate a new methodology for efficient distributed implementation for this domain.	computation;dataflow;embedded system;general-purpose modeling;message passing interface;parallel computing;partial template specialization;programming paradigm;signal processing;smart camera	Sankalita Saha;Sebastian Puthenpurayil;Jason Schlessman;Shuvra S. Bhattacharyya;Marilyn Wolf	2008	Proceedings of the IEEE	10.1109/JPROC.2008.928744	message passing;application software;implementation;signal processing;real-time computing;dataflow;smart camera;message passing interface;speech processing;computer science	Embedded	2.13267784016295	45.93066859015475	161055
97d8b5a9b166263535a91de650b20005b24ac14b	memristor-based discrete fourier transform for improving performance and energy efficiency	memristor;baseband;memristor crossbar;memristors;memristor crossbar dft memristor;wireless communication;internet of things;signal processing circuit complexity discrete fourier transforms low power electronics matrix multiplication memristor circuits performance evaluation power aware computing;discrete fourier transforms;memristor based discrete fourier transform power efficiency reduction power efficiency improvement computation latency reduction computing speed hardware complexity dft hardware design low power high speed matrix multiplication energy efficiency performance improvement;dft;program processors;memristors discrete fourier transforms baseband program processors hardware wireless communication internet of things;hardware	Memristor has emerged as one of the most promising candidates for the fundamental device in the beyond-CMOS era. With their unique advantage on implementing low-power high-speed matrix multiplication, memristors have shown great and vast potentiality in many specific applications. This paper, for the first time, investigates the hardware design of DFT using memristors. Two implementations of DFT using memristors have been presented for effectively trading-off between hardware complexity and computing speed. Simulation results show that as compared to the conventional CMOS-based design, the proposed memristor-based design enables significant reduction in computation latency and improvement in power efficiency with very low inaccuracy. Simulation results show that the proposed memristor-based implementation could reach up to 10X improvement in speed and 109.8X reduction in power efficiency compared to CMOS-based design.	baseband;cmos;central processing unit;computation;discrete fourier transform;fast fourier transform;low-power broadcasting;matrix multiplication;memristor;performance per watt;simulation	Ruizhe Cai;Ao Ren;Yanzhi Wang;Bo Yuan	2016	2016 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)	10.1109/ISVLSI.2016.124	embedded system;electronic engineering;parallel computing;computer science;memistor	Arch	5.28029817016266	42.04661453087647	161298
fb841aa0eeb3829cd165d182a16a68fc65cd4be6	pd-l2ork raspberry pi toolkit as a comprehensive arduino alternative in k-12 and production scenarios		The following paper showcases new integrated Pd-L2Ork system and its K12 educational counterpart running on Raspberry Pi hardware. A collection of new externals and abstractions in conjunction with the Modern Device LOP shield transforms Raspberry Pi into a cost-efficient sensing hub providing Arduino-like connectivity with 10 digital I/O pins (including both software and hardware implementations of pulse width modulation) and 8 analog inputs, while offering a number of integrated features, including audio I/O, USB and Ethernet connectivity and video output.	arduino;cost efficiency;input/output;language-oriented programming;pulse-width modulation;raspberry pi 3 model b (latest version);usb hub	Ivica Ico Bukvic	2014			pi;arduino;computer science;usb;embedded system;software;blowing a raspberry;ethernet;pulse-width modulation	OS	2.125311626903987	45.05500489613952	161665
358c8cad285cc4569efb9953d77c672098f91233	an application specific instruction set processor (asip) for adaptive filters in neural prosthetics	matrix vector operation application specific instruction set processor adaptive filter neural prosthetics neuroprosthetic design neuron model coding chip low power general purpose processor neural population coding application specific integrated circuit neural decoding activity;field programmable gate array;encoding neurons adaptation models brain models hardware computer architecture;prosthetics adaptive filters application specific integrated circuits bioelectric potentials medical signal processing neurophysiology;neural coding;field programmable gate array neuroprosthetics neural coding adaptive filter application specific instruction set processor;brain models;computer architecture;neuroprosthetics;neurons;adaptation models;encoding;adaptive filter;application specific instruction set processor;hardware	Neural coding is an essential process for neuroprosthetic design, in which adaptive filters have been widely utilized. In a practical application, it is needed to switch between different filters, which could be based on continuous observations or point process, when the neuron models, conditions, or system requirements have changed. As candidates of coding chip for neural prostheses, low-power general purpose processors are not computationally efficient especially for large scale neural population coding. Application specific integrated circuits (ASICs) do not have flexibility to switch between different adaptive filters while the cost for design and fabrication is formidable. In this research work, we explore an application specific instruction set processor (ASIP) for adaptive filters in neural decoding activity. The proposed architecture focuses on efficient computation for the most time-consuming matrix/vector operations among commonly used adaptive filters, being able to provide both flexibility and throughput. Evaluation and implementation results are provided to demonstrate that the proposed ASIP design is area-efficient while being competitive to commercial CPUs in computational performance.	adaptive filter;algorithmic efficiency;application-specific instruction set processor;application-specific integrated circuit;central processing unit;computation;hl7publishingsubsection <operations>;low-power broadcasting;neural network simulation;neural coding;neural decoding;neural ensemble;neuron;neuroprosthetics;point process;prosthesis;requirement;system requirements;throughput	Yao Xin;Will X. Y. Li;Zhaorui Zhang;Ray C. C. Cheung;Dong Song;Theodore W. Berger	2015	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1109/TCBB.2015.2440248	adaptive filter;real-time computing;neuroprosthetics;application-specific instruction-set processor;computer hardware;computer science;neural coding;field-programmable gate array;encoding	ML	5.303505078142043	44.10476356912026	161735
873adb48ed740ce7fadad7ca497258fde028c39f	k-mer counting with fpgas and hmc in-memory operations		k-mer counting is an essential algorithm found in many genomic related processes. It may seem like a rather trivial task but is in fact computationally expensive due to the sheer amount of data. The ever growing rate at which data is generated in genomics requires the creationof novel solutions leveraging new technologies to keep up the pace. In this paper we explore the use of in-memory operations of Hybrid Memory Cubes (HMC) to accelerate k-mer counting. The resulting accelerator is compared to an existing accelerator also using HMC memory, as well asstate of the art k-mer counting software. The use of in-memory operations resulted in a 14.6% to 16.9% performance improvement over using the HMC without them. The accelerator showeda speed-up of 3-4x over software when running with a single FPGA and HMC and a speed-up of 16-17x when using 4 FPGAs and 4 HMCs.	algorithm;analysis of algorithms;cubes;field-programmable gate array;hybrid memory cube;in-memory database;k-mer;mer	Rick Wertenbroek;Yann Thoma	2018	2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2018.8541417	parallel computing;field-programmable gate array;software;data structure;k-mer;computer science	EDA	0.09670424160144699	43.247086367834925	161768
09dec7972a615e7f01e25907ebd21ecc5b82208f	efficient high-precision matrix algebra on parallel architectures for nonlinear combinatorial optimization	linear algebra;high performance computing;combinatorial optimization problem;nonlinear combinatorial optimization;matrix algebra;high precision linear algebra;high performance computer;matroid optimization;parallel implementation;parallel architecture;combinatorial optimization;computational efficiency	We provide a first demonstration of the idea that matrix-based algorithms for nonlinear combinatorial optimization problems can be efficiently implemented. Such algorithms were mainly conceived by theoretical computer scientists for proving efficiency. We are able to demonstrate the practicality of our approach by developing an implementation on a massively parallel architecture, and exploiting scalable and efficient parallel implementations of algorithms for ultra high-precision linear algebra. Additionally, we have delineated and implemented the necessary algorithmic and coding changes required in order to address problems several orders of magnitude larger, dealing with the limits of scalability from memory footprint, computational efficiency, reliability, and interconnect perspectives.	algorithm;combinatorial optimization;computer scientist;discrete optimization;mathematical optimization;matrix method;memory footprint;nonlinear system;numerical linear algebra;parallel computing;petascale computing;scalability;supercomputer	John A. Gunnels;Jon Lee;Susan Margulies	2010	Math. Program. Comput.	10.1007/s12532-010-0014-4	optimization problem;mathematical optimization;combinatorics;combinatorial optimization;computer science;theoretical computer science;linear algebra;mathematics;numerical linear algebra;quadratic assignment problem	HPC	-0.7716383930388543	37.754552982798735	161933
b97b07b311029682e85c1e18cc87433f460af8bd	design of a reconfigurable parallel nonlinear boolean function targeted at stream cipher		Nonlinear Boolean function plays a pivotal role in the stream cipher algorithms and trusted cloud computing platform, Based on the analysis of multiple algorithms, this paper proposes a hardware structure of reconfigurable nonlinear Boolean function. This structure can realize the number of variables and AND terms less than 80 arbitrary nonlinear Boolean function in stream cipher algorithms. The entire architecture is verified on the FPGA platform and synthesized under the 0.18 μm CMOS technology, the clock frequency reaches 248.7 MHz, the result proves that the design is propitious to carry out the most nonlinear Boolean functions in stream ciphers which have been published, compared with other designs, the structure can achieve relatively high flexibility, and it has an obvious advantage in the area of circuits and processing speed.	stream cipher	Su Yang	2016		10.1007/978-3-319-49109-7_13	parallel computing;computer science;theoretical computer science;distributed computing	EDA	9.168861421494302	44.546840717826036	162235
f49b6683f37122749ea3636dc69301fbbe6c959b	gpu accelerated lanczos algorithm with applications	spectral methods;eigenvalues and eigenfunctions;lanczos algorithm;linear algebra;graph theory;spectral methods gpgpu lanczos graph partitioning image segmentation;lanczos;image segmentation;numerical method;ubiquitous computing coprocessors eigenvalues and eigenfunctions graph theory image segmentation iterative methods;coprocessors;programming model;symmetric matrices;iterative methods;gpgpu;graph partitioning;spectral method;graphic processing unit;graphics processing unit eigenvalues and eigenfunctions image segmentation sparse matrices kernel instruction sets symmetric matrices;ubiquitous computing;openmp accelerated lanczos algorithm graphics processing units ubiquitous accelerator gpgpu general purpose computations linear algebra iterative method eigenvalues eigenvectors cuda programming graph bisection image segmentation multicore cpu intel math kernel library;iteration method;sparse matrices;eigenvectors	Graphics Processing Units provide a large computational power at a very low price which position them as an ubiquitous accelerator. GPGPU is accelerating general purpose computations using GPU's. GPU's have been used to accelerate many Linear Algebra routines and Numerical Methods. Lanczos is an iterative method well suited for finding the extreme eigenvalues and the corresponding eigenvectors of large sparse symmetric matrices. In this paper, we present an implementation of Lanczos Algorithm on GPU using the CUDA programming model and apply it to two important problems : graph bisection using spectral methods, and image segmentation. Our GPU implementation of spectral bisection performs better when compared to both an Intel Math Kernel Library implementation and a Matlab implementation. Our GPU implementation shows a speedup up to 97.3 times over Matlab Implementation and 2.89 times over the Intel Math Kernel Library implementation on a Intel Core i7 920 Processor, which is a quad-core CPU. Similarly, our image segmentation implementation achieves a speed up of 3.27 compared to a multicore CPU based implementation using Intel Math Kernel Library and OpenMP. Through this work, we therefore wish to establish that the GPU may still be a better platform for also highly irregular and computationally intensive applications.	cuda;central processing unit;computation;general-purpose computing on graphics processing units;graph partition;graphics processing unit;image segmentation;iterative method;lanczos algorithm;lanczos resampling;linear algebra;matlab;math kernel library;mathematical optimization;multi-core processor;numerical method;openmp;point of view (computer hardware company);programming model;regular expression;sparse matrix;spectral method;speedup	Kiran Kumar Matam;Kishore Kothapalli	2011	2011 IEEE Workshops of International Conference on Advanced Information Networking and Applications	10.1109/WAINA.2011.70	computational science;parallel computing;computer science;graph theory;theoretical computer science;linear algebra;iterative method;ubiquitous computing;spectral method	HPC	-2.2648208390694338	40.10817449909157	162487
55ba62233d96b80e72f237858b1afa91e3827fe9	new regular radix-8 scheme for elliptic curve scalar multiplication without pre-computation	public key cryptography mobile computing parallel processing;side channel attack;elliptic curves;elliptic curve;side channel attack new regular radix 8 scheme elliptic curve scalar multiplication mobile technologies parallel computing schemes abelian group point arithmetic field arithmetic parallel operations;elliptic curve cryptography elliptic curves algorithm design and analysis registers hardware parallel processing;elliptic curve cryptography;registers;parallel computing schemes elliptic curve scalar multiplication side channel attack;parallel computing schemes;algorithm design and analysis;scalar multiplication;parallel processing;hardware	The recent advances in mobile technologies have increased the demand for high performance parallel computing schemes. In this paper, we present a new algorithm for evaluating elliptic curve scalar multiplication that can be used on any abelian group. We show that the properties of the proposed algorithm enhance parallelism at both the point arithmetic and the field arithmetic levels. Then, we employ this algorithm in proposing a new hardware design for the implementation of an elliptic curve scalar multiplication on a prime extended twisted Edwards curve incorporating eight parallel operations. We further show that in comparison to the other simple side-channel attack protected schemes over prime fields, the proposed design of the extended twisted Edwards curve is the fastest scalar multiplication scheme reported in the literature.	algorithm;cell (microprocessor);computation;fastest;multiprocessing;parallel computing;scalar processor;side-channel attack;twisted	Ebrahim A. Hasan Abdulrahman;Arash Reyhani-Masoleh	2015	IEEE Transactions on Computers	10.1109/TC.2013.213	parallel processing;discrete mathematics;jacobian curve;lenstra elliptic curve factorization;elliptic curve digital signature algorithm;tripling-oriented doche–icart–kohel curve;schoof–elkies–atkin algorithm;computer science;hyperelliptic curve cryptography;theoretical computer science;operating system;counting points on elliptic curves;scalar multiplication;curve25519;mathematics;elliptic curve cryptography;elliptic curve;elliptic curve point multiplication;algorithm;schoof's algorithm;algebra	HPC	9.23030714580946	43.57193295776198	162648
34ea892c40520fc51336dedd58ca272b6391a3b1	augmented arithmetic operations proposed for ieee-754 2018		Algorithms for extending arithmetic precision through compensated summation or arithmetics like double-double rely on operations commonly called twoSum and twoProd-uct. The current draft of the IEEE 754 standard specifies these operations under the names augmentedAddition and augment-edMultiplication. These operations were included after three decades of experience because of a motivating new use: bitwise reproducible arithmetic. Standardizing the operations provides a hardware acceleration target that can provide at least a 33 % speed improvements in reproducible dot product, placing reproducible dot product almost within a factor of two of common dot product. This paper provides history and motivation for standardizing these operations. We also define the operations, explain the rationale for all the specific choices, and provide parameterized test cases for new boundary behaviors.	allocate-on-flush;arithmetic underflow;bitwise operation;design rationale;exception handling;hardware acceleration;kahan summation algorithm;matrix multiplication;significant figures;silicon compiler;test case	E. Jason Riedy;James Demmel	2018	2018 IEEE 25th Symposium on Computer Arithmetic (ARITH)	10.1109/ARITH.2018.8464813	kahan summation algorithm;theoretical computer science;dot product;computer science;parameterized complexity;test case;arithmetic;ieee floating point;bitwise operation;software;hardware acceleration	Arch	6.416251211632101	43.90036848190902	162764
470c3d95dc5e6f134acc23c6c5333540866e7939	special issue editorial: accelerators for high-performance computing	high-performance computing;special issue editorial	Welcome to this special issue of the Journal of Parallel and Distributed Computing on Accelerators for High-Performance Computing, which contains ten original manuscripts chosen among the thirty submissions that were received. The large number of papers received is a clear indicator of the current interest of the research community in the usage of parallel hardware accelerators such as Graphics Processing Units (GPUs) or the Cell Broadband Engine for General-Purpose computation. The reason behind it is that the use of these accelerators in computationally demanding disciplines such as high-performance scientific computing or realistic 3D computer graphics achieves speedups of orders of magnitude with respect to optimized sequential or even parallel CPU implementations. As a result their usage is exploding, to the point that they already appear in many supercomputers. Nevertheless, a weak point nowadays of these accelerators is their reduced programmability in comparison with traditional general purpose CPUs, which coupledwith the different approach needed to optimally exploit the specific hardware of these units, has led to extensive research on their programmability and the development of algorithms for them. This is in fact the reason why the focus of this special issue are experiences and new developments in the parallel and distributed programming of these systems, with an emphasis on novel algorithms, languages and development environments. The manuscripts were chosen in a rigorous refereeing process in which each paper received a minimum of three reviews of researchers with expertise in the specific area of the paper. The first contribution, by López-Portugués et al. [1], presents a parallel solver of a high frequency single level Fast Multipole Method (FMM) for the Helmholtz equation applied to acoustic scattering. Their application exploits both the CPUs and theGPUs of a workstation, resulting in a heterogeneous solution that achieves runtimes comparable to those of amulticore clusterwhile reducing the power consumption by more than an order of magnitude. De la Asunción et al. [2] tackle the acceleration of the simulation of a two-layer shallow water system in triangular meshes by exploiting the combined power of several Compute Unified Device Architecture (CUDA)-enabled GPUs in a GPU cluster. To that end, an improvement of a path conservative Roe-type finite volume scheme that is especially suitable for GPU implementation is presented, and a distributed implementation of this scheme that uses CUDA and MPI to exploit the potential of a GPU cluster is developed. Several numerical experiments performed on a cluster of modern CUDA-enabled GPUs show the efficiency of the distributed solver. Strzodka [3] develops a concise macro based solution for data layout optimization that requires only support for structures and unions andwhich can therefore be utilized in OpenCL. This enables the development of high performance code without an a priori commitment to a certain layout and brings the possibility of	3d computer graphics;acoustic cryptanalysis;algorithm;cuda;cell (microprocessor);central processing unit;computation;computational science;distributed computing;experiment;fast multipole method;finite volume method;gpu cluster;general-purpose markup language;graphics processing unit;hardware acceleration;linear algebra;mathematical optimization;message passing interface;numerical analysis;opencl api;simulation;solver;supercomputer;triangle mesh;workstation	Ramón Doallo;Basilio B. Fraguela	2012	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2012.05.003		HPC	-3.6698131314583446	39.853537806008134	162854
1ee0bb13cdf7b1b76f6e9873bcb0842ea508c953	a high performance implementation of spectral clustering on cpu-gpu platforms	libraries;pattern clustering eigenvalues and eigenfunctions graphics processing units matrix algebra microprocessor chips;reverse communication interface;cpu gpu platform;spectral clustering;k means clustering cpu gpu platform spectral clustering sparse similarity graph reverse communication interface;clustering algorithms graphics processing units sparse matrices libraries software algorithms instruction sets;graphics processing units;parallelized k means algorithm graph clustering algorithms matlab python big data applications spectral clustering algorithm cpu gpu heterogeneous platforms computational power multicore cpu multithreading simd capabilities standard sparse representation format k eigenvectors laplacian matrix reverse communication interfaces arpack software cusparse library;clustering algorithms;software algorithms;sparse similarity graph;sparse matrices;k means clustering;instruction sets	Spectral clustering is one of the most popular graph clustering algorithms, which achieves the best performance for many scientific and engineering applications. However, existing implementations in commonly used software platforms such as Matlab and Python do not scale well for many of the emerging Big Data applications. In this paper, we present a fast implementation of the spectral clustering algorithm on a CPU-GPU heterogeneous platform. Our implementation takes advantage of the computational power of the multi-core CPU and the massive multithreading and SIMD capabilities of GPUs. Given the input as data points in high dimensional space, we propose a parallel scheme to build a sparse similarity graph represented in a standard sparse representation format. Then we compute the smallest k eigenvectors of the Laplacian matrix by utilizing the reverse communication interfaces of ARPACK software and cuSPARSE library, where k is typically very large. Moreover, we implement a very fast parallelized k-means algorithm on GPUs. Our implementation is shown to be significantly faster compared to the best known Matlab and Python implementations for each step. In addition, our algorithm scales to problems with a very large number of clusters.	arpack;algorithm;blas;big data;cuda;central processing unit;cluster analysis;computation;data point;graphics processing unit;k-means clustering;laplacian matrix;linear algebra;matlab;multi-core processor;multithreading (computer architecture);parallel computing;python;simd;similarity measure;sparse approximation;sparse matrix;spectral clustering;speedup;subroutine;thread (computing)	Yu Jin;Joseph JáJá	2016	2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)	10.1109/IPDPSW.2016.79	correlation clustering;computational science;data stream clustering;parallel computing;sparse matrix;computer science;theoretical computer science;canopy clustering algorithm;operating system;instruction set;cluster analysis;biclustering;spectral clustering;k-means clustering;clustering high-dimensional data	HPC	-2.0044136229943685	40.72202843655706	163025
bad717b6930faef3aa344162ffc3309a04810ea0	simulating sve-optimised genomics workloads on gem5		SVE (Scalable Vector Extension) is Arm's new vector instruction extension targeting high performance workloads. SVE offers many opportunities to optimise compute intensive workloads but, with the availability of SVE-enabled hardware still years away, we have to rely on simulation techniques in order to evaluate our implementations. Working with simulators can be tricky and it comes with many limitations but, used properly, a simulator like Gem5 is a valuable tool that can provide an opportunity to explore the possibilities opened by this new extension. As a use case, we focus our attention on the field of genomics, where the recent advent of high-throughput sequencing machines producing big amounts of genomic data has boosted the interest in efficient approximate string matching and alignment techniques. Genomics algorithms are the key computational building blocks for the downstream data analysis on resequencing projects where hundreds of GBytes of sequenced data are analysed against a reference genome in order to filter sequencing errors and detect potential genomic variation events. The computational requirements and sheer size of the input data used by these applications make them a challenging problem. Fortunately, they also exhibit a high degree of data parallelism, making them good candidates for vectorisation techniques. In this work we explore the unique opportunities that SVE provides in order to exploit the parallelism present in genomics algorithms. We discuss preliminary results, our simulation strategy, some of the obstacles and limitations we faced, and how to work around them in order to obtain meaningful results.	approximate string matching;data parallelism;downstream (software development);gigabyte;high-throughput computing;parallel computing;requirement;simulation;string searching algorithm;throughput	Javier Setoain;Alejandro Chacón;Filippo Spiga	2018	2018 IEEE International Conference on Cluster Computing (CLUSTER)	10.1109/CLUSTER.2018.00081	genomics;real-time computing;computer science;data parallelism;implementation;scalability;approximate string matching;exploit;parallel processing	HPC	-0.5630608561788443	44.58873401483441	163430
770c055a3a96f7cfb0d9f447b47f7826b77a8127	context streams a theoretical basis for a generic form of mimd pipelining	graph theory;control systems;concurrent computing;context streams;canonical form;process contexts;queue length;data mining;acyclic graph;upper bound;interleaved codes;context flow graph;acyclic graph context streams mimd pipelining process contexts context flow graph dynamic multifunctional pipelines upper bound queue length;councils;pipeline processing hardware parallel processing interleaved codes computer science upper bound concurrent computing data mining councils control systems;computer science;mimd pipelining;dynamic multifunctional pipelines;parallel processing;pipeline processing;hardware;pipeline processing graph theory parallel algorithms parallel processing;parallel algorithms	Context flow (CF) is a canonical form of MIMD pipelining in which process contexts rather than either instructions or data are manipulated. The paper presents a formal definition of context flow. It defines the state of a CF computation as the stream of contexts which pass a particular point in a context flow graph during a given period of time. It shows that for a class of context flow graphs corresponding to dynamic multifunctional pipelines, the number of contexts which may exist at any point in a graph is bounded, and independent of the computation being performed. It presents methods for determining the numerical value of the upper bound or queue length for a variety of configurations of a acyclic graph. >	mimd;pipeline (computing)	T. E. A. Lees	1990		10.1109/SPDP.1990.143639	parallel processing;canonical form;parallel computing;concurrent computing;computer science;graph theory;theoretical computer science;operating system;distributed computing;parallel algorithm;upper and lower bounds;programming language;directed acyclic graph;algorithm	HPC	3.9578570375556352	38.51218729868111	163512
7e26cc340963800523d0b8669777bc4efa7cc26c	on improving the energy efficiency and robustness of position tracking for mobile devices		An important feature of a modern mobile device is that it can position itself and support remote position tracking. To be useful, such position tracking has to be energy-efficient to avoid having a major impact on the battery life of the mobile device. Furthermore, tracking has to robustly deliver position updates when faced with changing conditions such as delays and changing positioning conditions. Previous work has established dynamic tracking systems, such as our EnTracked system, as a solution to address these issues. In this paper we propose a responsibility division for position tracking into sensor management strategies and position update protocols and combine the sensor management strategy of EnTracked with position update protocols, which enables the system to further reduce the power consumption with up to 268 mW extending the battery life with up to 36%. As our evaluation identify that classical position update protocols have robustness weaknesses we propose a method to improve their robustness. Furthermore, we analyze the dependency of tracking systems on the pedestrian movement patterns and positioning environment, and how the power savings depend on the power characteristics of different mobile devices.	dead reckoning;fingerprint (computing);mobile device;tracking system	Mikkel Baun Kjærgaard	2010		10.1007/978-3-642-29154-8_14	robustness (computer science);efficient energy use;mobile device;control engineering;computer science	Mobile	2.0792632380116105	34.87938317587754	163581
9edbce6f21aecd64d5f60908d56f459683e55a30	huffman coding-based compression unit for embedded systems	huffman coding;radiation detectors;fpga;huffman codes;system on a chip;embedded system;embedded systems;design and implementation;system on chip;registers;system on chip embedded systems field programmable gate arrays huffman codes;compression unit;registers huffman coding radiation detectors approximation methods embedded systems system on a chip;soc;approximation methods;field programmable gate arrays;compression;huffman coding compression embedded systems fpga;memory resources;embedded processor;loss less compression;soc huffman coding compression unit embedded systems loss less compression memory resources fpga embedded processor system on a chip	The need for an efficient way to store data makes compression a common task. Huffman Coding is a technique used for loss less compression of data. In an embedded system whose main memory resources are limited, to perform the coding could be risky because it could construct very deep or wide trees. The purpose of this article is to show the design and implementation of a circuit that performs a coding of text files without using the main memory of the system. This circuit was added to a FPGA with embedded processor, therefore creating an embedded system or a system on a chip (SoC).	algorithm;code;combinational logic;computer data storage;embedded system;field-programmable gate array;huffman coding;system on a chip	Marco Antonio Soto Hernandez;Oscar Alvarado Nava;Francisco Javier Zaragoza Martínez	2010	2010 International Conference on Reconfigurable Computing and FPGAs	10.1109/ReConFig.2010.65	system on a chip;embedded system;parallel computing;computer hardware;computer science	EDA	9.77740552864964	39.96387531290697	163655
d43786cddcebbcb450b0958cd4417a10f1998bc9	correctness proofs outline for newton-raphson based floating-point divide and square root algorithms	libraries;newton raphson iterative method;ieee correctness;iterative algorithms;identity based encryption;availability;rigorous proof floating point divide square root newton raphson iterative method iterative floating point algorithms ieee correctness ieee 754 standard binary floating point operations proof;emulation;newton raphson method;iterative algorithm;binary floating point operations;iterative methods content addressable storage throughput emulation libraries cornea identity based encryption iterative algorithms software algorithms availability;iterative methods;formal verification;formal verification newton raphson method floating point arithmetic;software algorithms;divide;square root;floating point;newton raphson;floating point arithmetic;iteration method;content addressable storage;proof;rigorous proof;correctness proof;iterative floating point algorithms;cornea;ieee 754 standard;software implementation;throughput	This paper describes a study of a class of algorithms for the floating-point divide and square root operations, based on the Newton-Raphson iterative method. The two main goals were: (1) Proving the IEEE correctness of these iterative floating-point algorithms, i.e. compliance with the IEEE-754 standard for binary floating-point operations [1]. The focus was on software driven iterative algorithms, instead of the hardware based implementations that dominated until now. (2) Identifying the special cases of operands that require software assistance due to possible overflow, underflow, or loss of precision of intermediate results. This study was initiated in an attempt to prove the IEEE correctness for a class of divide and square root algorithms based on the Newton-Rapshson iterative methods. As more insight into the inner workings of these algorithms was gained, it became obvious that a formal study and proof were necessary in order to achieve the desired objectives. The result is a complete and rigorous proof of IEEE correctness for floating-point divide and square root algorithms based on the Newton-Raphson iterative method. Even more, the method used in proving the IEEE correctness of the square root algorithm is applicable in principle to any iterative algorithm, not only based on the Newton-Raphson method. Conditions requiring Software Assistance (SWA) were also determined, and were used to identify cases when alternate algorithms are needed to generate correct results. Overall, this is one important step toward flawless implementation of these floating-point operations based on software implementations. Introduction The floating-point algorithms for the floating-point divide and square root operations discussed in this paper differ fundamentally from those in the Intel or Hewlett-Packard processors, in that they can be implemented in software and are based on Newton-Raphson iterations. This approach allows for potentially higher throughput, as the divide, square root, and remainder operations are in this case pipelineable. It also allows for easier modification of the algorithms, should this be needed, and as a side benefit reduces the size of the chip that would adopt them. The decision to study such an approach was also influenced by the availability of relatively new algorithms [2] that were expected to generate the correct result and to set correctly the IEEE status flags in any IEEE rounding mode with only one computational sequence, and without the necessity of performing a correction step at the end. Performance and IEEE correctness remain the two main characteristics that have to be ensured. The expected levels of performance are possible in modern processors due to advanced features in their floating-point architecture (notably the fused multiply-add operation). Several variations of these algorithms can be developed, depending on whether the goal is to maximize throughput or to minimize latency, and also on the particular precision of the result. For example, different variants of floating-point divide and of floating-point square root were designed, verified, and implemented for single precision, double precision, and for double-extended precision floating-point computations. Algorithms for floating-point remainder are directly derived from those for the divide operation. All or part of these algorithms could be used by compilers that might inline the appropriate sequence for each situation, for non-native instruction emulation, for floating-point emulation libraries used by operating systems, in mathematical libraries, and in binary translators. The object of the work presented in this paper was to prove the IEEE correctness of the results generated by the divide, square root and remainder operations, of utmost importance for modern processors, often designed to excel also in floating-point. This study also identified special cases of operands that require software assistance (SWA) due to possible overflow, underflow, or loss of precision of intermediate	algorithm;arithmetic underflow;assembly language;central processing unit;compiler;computation;correctness (computer science);double-precision floating-point format;emulator;extended precision;iteration;iterative method;library (computing);methods of computing square roots;multiply–accumulate operation;newton;newton's method;operand;operating system;rounding;soap with attachments;single-precision floating-point format;throughput;verification and validation;wolfram mathematica	Marius A. Cornea-Hasegan;Roger A. Golliver;Peter W. Markstein	1999		10.1109/ARITH.1999.762834	mathematical optimization;computer science;floating point;theoretical computer science;operating system;fast inverse square root;mathematics;iterative method;algorithm;algebra	DB	7.71303174533094	42.9326342610722	163864
5e0d1d96762f53bfb5d23e5c932c4b6c72cb1d62	enabling system-level platform resilience through embedded data-driven inference capabilities in electronic devices	detectors;stochastic processes biosensors embedded systems learning artificial intelligence medical signal processing signal classification;circuit faults;support vector machines;digitally assisted analog;hardware resilience;biomedical devices hardware resilience machine learning stochastic computation digitally assisted analog;embedded systems;brain modeling;computational modeling;stochastic computation;machine learning;stochastic processes;signal classification;biomedical devices;learning artificial intelligence;analog circuit nonidealities system level platform resilience embedded data driven inference capabilities electronic devices electronic systems fundamental limitation intolerable levels hardware resilience data driven modeling framework application data hardware nonidealities biomedical sensors on line overhead digital circuit nonidealities;medical signal processing;brain modeling detectors hardware data models circuit faults computational modeling support vector machines;biosensors;data models;hardware	Advanced devices for embedded and ambient applications represent one of the most compelling classes of electronic systems, but they also impose more severe constraints on system resources than ever before. Although platform non-idealities have always posed a fundamental limitation, the overheads of conventional margining are now reaching intolerable levels. We describe an alternate approach to hardware resilience that applies to applications where advanced modeling and inference capabilities are required, a rapidly increasing emphasis in many applications. We show how a data-driven modeling framework for analyzing application data can also be used to effectively model and overcome a broad range of hardware non-idealities. Specific examples for biomedical sensors are shown that are able to retain performance with minimal on-line overhead despite the presence of severe digital- and analog-circuit non-idealities.	analogue electronics;embedded system;online and offline;overhead (computing);sensor	Naveen Verma;Kyong-Ho Lee;Kuk Jin Jang;Ali H. Shoeb	2012	2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2012.6289113	stochastic process;data modeling;support vector machine;detector;real-time computing;computer science;theoretical computer science;machine learning;computational model;biosensor	EDA	3.8398266423515235	35.410616656712556	163946
1ed8d4f3c0c438852b8fd13ef0d17f53d9566ecc	a gpu based implementation of center-surround distribution distance for feature extraction and matching	graphical processing unit;nvidia geforce gtx 280;graphical processing unit center surround distribution distance algorithm feature extraction general purpose gpu programming environments supercomputers computational power;kernel;paper;yarn;universal access;image processing;self reference;computational power;spin transfer torque;gpu programming;coprocessors;cuda;center surround distribution distance algorithm;synchronization;feature extraction;performance analysis;nvidia;algorithms;general purpose gpu programming environments;stt ram;computer science;graphics processing unit;feature extraction coprocessors;supercomputers;feature extraction filtering computer vision application software central processing unit programming environments distributed computing power engineering computing image processing computer architecture;iir filters	The release of general purpose GPU programming environments has garnered universal access to computing performance that was once only available to super-computers. The availability of such computational power has fostered the creation and re-deployment of algorithms, new and old, creating entirely new classes of applications. In this paper, a GPU implementation of the Center-Surround Distribution Distance (CSDD) algorithm for detecting features within images and video is presented. While an optimized CPU implementation requires anywhere from several seconds to tens of minutes to perform analysis of an image, the GPU based approach has the potential to improve upon this by up to 28X, with no loss in accuracy.	algorithm;central processing unit;computation;computer;feature extraction;general-purpose computing on graphics processing units;graphics processing unit;matching (graph theory);sensor;software deployment;supercomputer	Aditi Rathi;Michael DeBole;Weina Ge;Robert T. Collins;Narayanan Vijaykrishnan	2010	2010 Design, Automation & Test in Europe Conference & Exhibition (DATE 2010)	10.1109/DATE.2010.5457215	embedded system;spin-transfer torque;parallel computing;computer hardware;image processing;computer science;theoretical computer science;operating system	EDA	1.210411856190744	46.05170459341264	164118
3645bc542734c22699ba2ba77a5465f756101924	service robotics for data centers monitoring		This work a novel solutions that relies on autonomous robots with the aim of improving data centers power efficiency by providing an easy to use tool to perform environmental monitoring. Data center environmental monitoring has been highly explored in the last few years. In fact, due to the high power density managed by these buildings, precise monitoring is necessary to improve power efficiency without increasing the risk of hardware failure. This work proposes a robot system that is in charge of autonomously monitor temperature and humidity in the whole data center room environment. After that the robot have built a map of the data center room, it is able to localize in the map and performs localized measurements at different locations. Those measurements are stored and can be visualized by the user thanks to a web Graphical User Interface (GUI). The solution emulates an Environmental Sensor Network, a very popular solution for precise monitoring in these environments. The robot trajectories and the localized measurements can be easily reconfigured from the same GUI. For this reason, we name it a Virtual Sensor Network (VSN). In this work, we discuss the implementation choices and present some results collected during a use case experiment in a real environment.	autonomous robot;data center;emulator;graphical user interface;operating system;performance per watt;sensor	Ludovico Orlando Russo;Stefano Rosa;Stefano Primatesta;Marcello Maggiora;Basilio Bona	2016	IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2016.7793161	embedded system;real-time computing;simulation;engineering	Robotics	1.788005704565577	32.37819725865108	164330
aa6cda4be587cbdadddd8d276aed87c3b9e85512	vfloat: a variable precision fixed- and floating-point library for reconfigurable hardware	formation control;fixed point;clustering;satellite image;floating point;reconfigurable hardware;k means clustering	Optimal reconfigurable hardware implementations may require the use of arbitrary floating-point formats that do not necessarily conform to IEEE specified sizes. We present a variable precision floating-point library (VFloat) that supports general floating-point formats including IEEE standard formats. Most previously published floating-point formats for use with reconfigurable hardware are subsets of our format. Custom datapaths with optimal bitwidths for each operation can be built using the variable precision hardware modules in the VFloat library, enabling a higher level of parallelism. The VFloat library includes three types of hardware modules for format control, arithmetic operations, and conversions between fixed-point and floating-point formats. The format conversions allow for hybrid fixed- and floating-point operations in a single design. This gives the designer control over a large number of design possibilities including format as well as number range within the same application. In this article, we give an overview of the components in the VFloat library and demonstrate their use in an implementation of the K-means clustering algorithm applied to multispectral satellite images.	algorithm;cluster analysis;field-programmable gate array;fixed-point arithmetic;ieee 754-1985;k-means clustering;multispectral image;parallel computing;quadruple-precision floating-point format;reconfigurable computing	Xiaojun Wang;Miriam Leeser	2010	TRETS	10.1145/1839480.1839486	embedded system;double-precision floating-point format;parallel computing;real-time computing;reconfigurable computing;computer science;floating point;theoretical computer science;operating system;fixed point;extended precision;cluster analysis;half-precision floating-point format;k-means clustering	Arch	3.5277594798109857	45.95114087809477	164544
1c8a24fee514a5310e4bac153676ad906fc27f77	efficient parallel stochastic gradient descent for matrix factorization using gpu	graphics processing units	Matrix factorization is an advanced and efficient technique for recommender systems. Recently, Stochastic Gradient Descent (SGD) method is considered to be one of the most popular techniques for matrix factorization. SGD is a sequential algorithm, which is difficult to be parallelized for large-scale problems. Nowadays, researches focus on efficiently parallelizing SGD. In this research, we propose an efficient parallel SGD method, ESGD, for GPU. ESGD is more efficient than recent parallel methods because it utilizes GPU, reducing non-coalesced access of global memory and achieving load balance of threads. In addition, ESGD does not require any sorting and/or data shuffling as preprocessing phase. Although platform used for ESGD implementation is old, ESGD demonstrates 12.5× speedup over state-of-the-art GPU method, BSGD.	automatic parallelization;big data;cluster analysis;computational resource;experiment;fastest;graphics processing unit;iteration;load balancing (computing);parallel computing;preprocessor;recommender system;scheduling (computing);sequential algorithm;sorting;speedup;stochastic gradient descent	Mohamed A. Nassar;Layla A. A. El-Sayed;Yousry Taha	2016	2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)	10.1109/ICITST.2016.7856668	parallel computing;computer science;theoretical computer science;distributed computing	HPC	-1.737323831984849	41.680859825274894	164688
0511bb2b054363e901e270cf636d42fdb236c981	exploiting data sparsity in parallel matrix powers computations	computations;computation science;algorithms;sparse matrix;parallel processing	We derive a new parallel communication-avoiding matrix powers algorithm for matrices of the form A = D + USV H , where D is sparse and USV H has low rank and is possibly dense. We demonstrate that, with respect to the cost of computing k sparse matrix-vector multiplications, our algorithm asymptotically reduces the parallel latency by a factor of O(k) for small additional bandwidth and computation costs. Using problems from real-world applications, our performance model predicts up to 13× speedups on petascale machines.	algorithm;computation;petascale computing;sparse matrix	Nicholas Knight;Erin Carson;James Demmel	2013		10.1007/978-3-642-55224-3_2	parallel processing;mathematical optimization;parallel computing;sparse matrix;computer science;theoretical computer science;computation;algorithm	HPC	-1.8637378850951838	38.77143742415089	164753
8ddd4c95547770cf0835ff990dd228056f8a86a8	are field-programmable gate arrays ready for the mainstream?	field programmable gate array;gpu;fpga;field programmable gate arrays graphics processing unit parallel processing computer architecture adders;field programmable gate arrays embedded systems;multicore field programmable gate array fpga gpu;computer architecture;embedded systems;multicore;adders;graphic processing unit;embedded systems field programmable gate arrays state of the art microprocessors fpga accelerators mainstream application designers;field programmable gate arrays;graphics processing unit;parallel processing	For more than a decade, researchers have shown that field programmable gate arrays (FPGAs) can accelerate a wide variety of software, in some cases by several orders of magnitude compared to state-of-the-art microprocessors. Despite this performance advantage, FPGA accelerators have not yet been accepted by mainstream application designers and instead remain a niche technology used mainly in embedded systems.	embedded system;field-programmable gate array;microprocessor;niche blogging	Greg Stitt	2011	IEEE Micro	10.1109/MM.2011.99	parallel processing;computer architecture;parallel computing;macrocell array;computer hardware;reconfigurable computing;programmable logic array;computer science;gate equivalent;field-programmable gate array	Arch	0.005239136836293851	45.17289911416938	165311
2a17c90ed723d6a14415cc1f677a5c0aa512f501	graphreduce: processing large-scale graphs on accelerator-based systems	energy;architecture optimization;memory management;cache;data movement optimization;performance;locality;reuse;acceleration;gpgpu;computational modeling;big data;graph analytics;graphics processing units;programming;performance optimization;parallel processing;partitioning algorithms	Recent work on real-world graph analytics has sought to leverage the massive amount of parallelism offered by GPU devices, but challenges remain due to the inherent irregularity of graph algorithms and limitations in GPU-resident memory for storing large graphs. We present GraphReduce, a highly efficient and scalable GPU-based framework that operates on graphs that exceed the device's internal memory capacity. GraphReduce adopts a combination of edge- and vertex-centric implementations of the Gather-Apply-Scatter programming model and operates on multiple asynchronous GPU streams to fully exploit the high degrees of parallelism in GPUs with efficient graph data movement between the host and device. GraphReduce-based programming is performed via device functions that include gatherMap, gatherReduce, apply, and scatter, implemented by programmers for the graph algorithms they wish to realize. Extensive experimental evaluations for a wide variety of graph inputs and algorithms demonstrate that GraphReduce significantly outperforms other competing out-of-memory approaches.	algorithm;computer data storage;graph theory;graphics processing unit;list of algorithms;out of memory;parallel computing;programmer;programming model;scalability	Dipanjan Sengupta;Shuaiwen Song;Kapil Agarwal;Karsten Schwan	2015	SC15: International Conference for High Performance Computing, Networking, Storage and Analysis	10.1145/2807591.2807655	acceleration;parallel processing;programming;parallel computing;real-time computing;energy;big data;performance;cache;computer science;theoretical computer science;operating system;reuse;distributed computing;programming language;computational model;general-purpose computing on graphics processing units;memory management	HPC	-3.526605935311739	43.425643592803645	165501
0e2b319a51d8d84513fd1a6a8763945c289d8765	on the impact of target technology in sha-3 hardware benchmark rankings		Both FPGAs and ASICs are widely used as the technology for comparing SHA-3 hardware benchmarking process. However, the impact of target technology in SHA-3 hardware benchmark rankings has hardly been considered. A cross-platform comparison between the FPGA and ASIC results of the 14 second round SHA-3 designs demonstrates the gap between two sets of benchmarking results. In this paper we describe a systematic approach to analyze a SHA-3 hardware benchmark process for both FPGAs and ASICs, and we present our latest results for FPGA and ASIC evaluation of the 14 second round SHA-3 candidates. 1 About Paper Version 2.0 This version contains updated FPGA results with Xilinx Virtex-5 XC5VLX3302FF1760 FPGA. All the FPGA area, speed and power results are generated based on Xilinx XFLOW command-line tool (Version 12.2). All the Verilog/VHDL source codes and FPGA/ASIC scripts for 14 SHA-3 algorithms with the SHA256 reference design can be found at VT-SHA3 project website: (http://rijndael.ece.vt.edu/sha3/). 2 Introduction The SHA-3 competition organized by NIST aims to select, in three phases, a successor for the mainstream SHA-2 hash algorithms in use today. By the completion of Phase 1 in July 2009, 14 out of the 51 hash candidate submissions were identified for further consideration as SHA-3 candidates. These 14 candidates will be further analyzed with respect to security, cost and performance, covering both algorithm and implementation characteristics [1]. For the second phase of the competition, NIST is looking for additional cryptanalytic results, as well as for performance evaluation data on hardware platforms. Two major classes of hardware devices, Field Programmable Gate Arrays (FPGAs) and Application Specific Integrated Circuits (ASICs), were extensively studied during Round 2 SHA-3 hardware evaluation [2–12]. It is widely accepted 2 X. Guo, S. Huang, L. Nazhandali and P. Schaumont that FPGAs and ASICs implementing the same design show different characteristics [13]. A hardware benchmarking process, therefore, starts by fixing the target technology, either ASICs or FPGAs, and then report the results based on selected metrics that are appropriate for the target technology. Several SHA-3 hardware rankings have been obtained in this manner. In this paper we intend to address the question if the choice of target technology can affect the resulting ranking between FPGA and ASIC designs built based on the same HDL source code. We motivate our work by the need of the SHA-3 hardware benchmarking process. Different ASIC and FPGA rankings have been provided and implied the superiority of certain algorithms. In general, compared to ASICs, FPGAs offer many advantages including reduced nonrecurring engineering and shorter time to market. These advantages come at the cost of an increase in silicon area, a decrease in performance, and an increase in power consumption when designs are implemented on FPGAs. These inefficiencies in FPGA-based implementations are widely known and accepted, although there have been few attempts to quantify them. One exception is Kuon, who describes the gap between ASIC and FPGA in terms of area, performance, and power consumption [13]. Kuon compares a 90-nm CMOS FPGA and 90-nm CMOS standard-cell ASIC in terms of logic density, circuit speed, and power consumption for core logic. He finds that, for a representative set of benchmarks, the area gap between FPGA and ASIC is 35 times. He points out that the area gap may decrease when “hard” blocks in the FPGA fabric (multipliers, memories, and so on) would be used. The ratio of critical-path delay, from FPGA to ASIC, is roughly three to four times. The dynamic power consumption ratio is approximately 14 times and, with hard blocks, this gap generally becomes smaller. In this work we report on a methodology to provide a consistent comparison between SHA-3 FPGA and ASIC designs with three major steps. First, we select the technology node for both FPGAs and ASICS as the starting point for our cross-platform evaluation. Second, we propose several metrics to approach a comparison between FPGA and ASIC results. Third, present an analysis of such results for 14 candidates implemented in ASIC and FPGA. 3 Related Work The hardware evaluation of SHA-3 candidates has started shortly after the specifications and reference software implementations of 51 algorithms submitted to the contest became available. The majority of initial comparisons were limited to less than five candidates [2, 12]. More comprehensive efforts became feasible only after NIST’s announcement of 14 candidates qualified to the second round of the competition in July 2009. Since then, in both FPGA and ASIC categories, several comprehensive studies have been reported [3–11]. Matsuo et al. [8, 9] focused on the use of FPGA-based SASEBO-GII board from AIST, Japan. All the results are based on the prototyping results and real measurements on a Xilinx Virtex-5 FPGA on board. Gaj et al. [3, 4] conducted a much more comprehensive Technology Impact in SHA-3 Hardware Benchmark Rankings 3 FPGA evaluation based ATHENA, which can generate multiple sets of results for several representative FPGA families from two major vendors. Baldwin et al. compared hardware implementations of different message digest sizes, including hardware padding, on a Xilinx Virtex-5 FPGA. Guo et al. [10] used a consistent and systematic approach to move the SHA-3 hardware benchmark process from the FPGA prototyping by [8, 9] to ASIC implementations based 130nm CMOS standard cell technology. Tillich et al. [6] presented the first ASIC post-synthesis results using 180nm CMOS standard cell technology with high throughput as the optimization goal and further provided post-layout results [5]. Henzen et al. [7] implemented several architectures in a 90nm CMOS standard cell technology, targeting highand moderate-speed constraints separately, and presented a complete benchmark of post-layout results. Table 1 compares these benchmarking efforts, and demonstrates that a comparison between FPGA and ASIC is hard because of several reasons. First, most groups do not share the same source codes. Second, the ASIC benchmarks do not use a common hardware interface. Third, the reported metrics do not allow a cross-platform (ASIC-FPGA) comparison. Although the joint work done by Matsuo et al. [8, 9] and Guo et al. [10] satisfy the first two conditions, still we believe that the chosen metrics are not well-suited for a cross-platform comparison between FPGA and ASIC benchmarks. All of the above issues motivate our work, namely an investigation of the (dis)similarity between FPGA and ASIC benchmarks for SHA-3 hardware candidates with 256 bits digest. 4 Methodology In this section, we describe our efforts in comparing the FPGA and ASIC performance evaluations. We describe the overall design flow that combines FPGA prototyping with ASIC design, and next elaborate the efforts to automate and standardize the ASIC implementation process. 4.1 Standard Interface So far, several research groups have proposed standard hardware interfaces with well supported design flows, including the interfaces defined by [3, 7, 14, 11]. A more detailed discussion on hash interface issues can be found at [9]. The key issue for a fair comparison is to use a common interface for all candidates. Therefore, we selected the interface proposal of Chen et al. [14] (with a data I/O width of 16-bits), but observe that other proposals may be equally valid choices. 4.2 Technology Node Selection for FPGAs and ASICs It’s not the intention of this article to pitch ASIC against FPGA. Instead, we want to evaluate how the performance numbers found on these two different technologies would be different assuming that someoone starts from the same RTL source code. This consideration affects how the target technologies for comparison are selected. 4 X. Guo, S. Huang, L. Nazhandali and P. Schaumont T a b le 1 . C o m p a re th e re la te d S H A -3 h a rd w a re b en ch m a rk in g w o rk in b o th F P G A s a n d A S IC s	algorithm;application-specific integrated circuit;benchmark (computing);cmos;code;command-line interface;cryptanalysis;cryptographic hash function;electrical connector;entity–relationship model;fpga prototyping;field-programmable gate array;hardware description language;hardware interface design;input/output;internet;linear algebra;mathematical optimization;performance evaluation;reference design;sha-2;sha-3;semiconductor device fabrication;standard cell;throughput;vhdl;verilog	Xu Guo;Sinan Huang;Leyla Nazhandali;Patrick Schaumont	2010	IACR Cryptology ePrint Archive		embedded system;real-time computing;simulation;engineering	EDA	8.474544822829193	46.09454214406647	165529
d30e8aaf4066c0ffbac08bbb45bc629a2efdb254	neuromemristive systems: boosting efficiency through brain-inspired computing	energy efficiency;random access memory;neural networks;brain inspired computing;high performance computing;energy efficient computing;energy efficient computing neuromemristive systems neuromorphic systems memristors power management energy efficient systems low power design brain inspired computing neural networks neural network architecture high performance computing;memristors;neuromorphic systems;neural network architecture;power system management;multicore processing;memristors neurons multicore processing switches random access memory energy efficiency neuromemristive systems neural networks power system management low power electronics energy efficiency;low power electronics;power management;energy efficient systems;low power design;neurons;switches;neuromemristive systems	Neuromemristive systems (NMSs) are gaining traction as an alternative to conventional CMOS-based von Neumann systems because of their greater energy and area efficiency. A proposed NMS accelerator for machine-learning tasks reduced power dissipation by five orders of magnitude, relative to a multicore reduced-instruction set computing processor.	cmos;machine learning;multi-core processor;no man's sky;traction teampage;von neumann architecture	Cory E. Merkel;Raqibul Hasan;Nicholas Soures;Dhireesha Kudithipudi;Tarek M. Taha;Sapan Agarwal;Matthew J. Marinella	2016	Computer	10.1109/MC.2016.312	multi-core processor;parallel computing;memristor;network switch;computer science;theoretical computer science;operating system;efficient energy use;artificial neural network;low-power electronics	Arch	3.9398616916870495	42.065120209309136	165678
f083a15c8e1856b94d02240a8387d3c830a83581	low complexity cubing and cube root computation over $\f_{3^m}$ in polynomial basis	field arithmetic operations;software;complexity theory;elliptic curves;hamming weight;pairing based cryptography;cube root computation;cube root;computational geometry;supersingular elliptic curves low complexity cubing cube root computation polynomial basis irreducible trinomials irreducible tetranomials irreducible pentanomials field cubing field cube root operation field arithmetic operations pairing based cryptography;field cube root operation;data mining;polynomials;cubing;computer arithmetic;low complexity cubing;elliptic curve cryptography;polynomials complexity theory data mining hamming weight elliptic curves elliptic curve cryptography software;computational complexity;cryptography;supersingular elliptic curves;finite field arithmetic;polynomials computational complexity computational geometry cryptography;characteristic three;field cubing;irreducible trinomials;irreducible tetranomials;number theoretic computations;computations in finite fields;polynomial basis;irreducible pentanomials;cryptography finite field arithmetic cubing cube root characteristic three	We present low complexity formulae for the computation of cubing and cube root over IF3m constructed using special classes of irreducible trinomials, tetranomials and pentanomials. We show that for all those special classes of polynomials, field cubing and field cube root operation have the same computational complexity when implemented in hardware or software platforms. As one of the main applications of these two field arithmetic operations lies in pairing-based cryptography, we also give in this paper a selection of irreducible polynomials that lead to low cost field cubing and field cube root computations for supersingular elliptic curves defined over IF3m, where m is a prime number in the pairing-based cryptographic range of interest, namely, m ∈ [47, 541].	computation;computational complexity theory;cube;irreducibility;irreducible complexity;pairing-based cryptography;polynomial basis;trinomial	Omran Ahmadi;Francisco Rodríguez-Henríquez	2009	IEEE Transactions on Computers	10.1109/TC.2009.183	arithmetic;hamming weight;finite field arithmetic;discrete mathematics;cube root;computational geometry;cryptography;mathematics;elliptic curve cryptography;algorithm;algebra	Theory	9.958157566577443	43.23903835986786	165712
dc1278354cfcbce0b1055d34c04e1473fd948632	a tsqr based krylov basis computation method on hybrid gpu cluster	numerical stability;arnoldi;pattern clustering graphics processing units matrix decomposition numerical stability;standards;graphics processing units sparse matrices scalability computational modeling numerical stability instruction sets standards;krylov subspace basis;hybrid gpu cluster;hybrid gpu cluster krylov subspace basis arnoldi tsqr;computational modeling;graphics processing units;tsqr;scalability;sparse matrices;instruction sets;tall skinny qr factorization tsqr based krylov basis computation method hybrid gpu cluster sparse linear problems orthonormal subspace basis execution time arnoldi iteration matrix vector multiplications vector inner products parallel implementations numerical stability cpu clusters autotuning scheme	Krylov Subspace Methods are commonly used for solving large sparse linear problems. The computation of an orthonormal subspace basis usually consumes most of the execution time in methods like Arnoldi iteration, which suffers from substantial communication overhead due to matrix-vector multiplications and vector inner products in parallel implementations. In this paper, we propose a method that combines a hypergraph based power iteration and a Tall Skinny QR factorization to form a Krylov subspace basis. Experimentation shows that our method has a lower communication cost and better numerical stability than Arnoldi iteration on CPU-GPU clusters, and an auto-tuning scheme shall be incorporated to address problems with different conditions.	arnoldi iteration;central processing unit;computation;gpu cluster;graphics processing unit;iterative method;krylov subspace;numerical stability;overhead (computing);power iteration;qr decomposition;run time (program lifecycle phase);self-tuning;sparse matrix	Langshi Chen;Serge G. Petiton	2015	2015 IEEE International Conference on Cluster Computing	10.1109/CLUSTER.2015.25	mathematical optimization;parallel computing;scalability;sparse matrix;computer science;krylov subspace;theoretical computer science;generalized minimal residual method;instruction set;computational model;numerical stability;arnoldi iteration	HPC	-2.4887230991250413	39.039305197677216	165828
0b6ca81408cfdaf4bab8a69ec8a9168be8f11460	analysis-driven engineering of comparison-based sorting algorithms on gpus		We study the relationship between memory accesses, bank conflicts, thread multiplicity (also known as over-subscription) and instruction-level parallelism in comparison-based sorting algorithms for Graphics Processing Units (GPUs). We experimentally validate a proposed formula that relates these parameters with asymptotic analysis of the number of memory accesses by an algorithm. Using this formula we analyze and compare several GPU sorting algorithms, identifying key performance bottlenecks in each one of them. Based on this analysis we propose a GPU-efficient multiway merge-sort algorithm, GPU-MMS, which minimizes or eliminates these bottlenecks and balances various limiting factors for specific hardware.  We realize an implementation of GPU-MMS and compare it to sorting algorithm implementations in state-of-the-art GPU libraries on three GPU architectures. Despite these library implementations being highly optimized, we find that GPU-MMS outperforms them by an average of 21% for random integer inputs and 14% for random key-value pairs.	attribute–value pair;experiment;geforce;graphics processing unit;instruction-level parallelism;library (computing);matrix multiplication;merge sort;parallel computing;runtime system;shared memory;sorting algorithm	Ben Karsin;Volker Weichert;Henri Casanova;John Iacono;Nodari Sitchinava	2018		10.1145/3205289.3205298	asymptotic analysis;implementation;parallel computing;limiting;computer science;thread (computing);cuda;merge sort;sorting algorithm;sorting	HPC	-3.3665793469766854	40.935901769579715	166085
de0a517bc586c359b7bf6a352aa4b3351d5874f6	merged dictionary code compression for fpga implementation of custom microcoded pes	processing element;compression algorithm;dictionary based compression;fpga;chip;fpga implementation;memory optimization;code size;microcoded architectures;high performance;code compression;no instruction set computer	Horizontal Microcoded Architecture (HMA) is a paradigm for designing programmable high-performance processing elements (PEs). However, it suffers from large code size, which can be addressed by compression. In this article, we study the code size of one of the new HMA-based technologies called No-Instruction-Set Computer (NISC). We show that NISC code size can be several times larger than a typical RISC processor, and we propose several low-overhead dictionary-based code compression techniques to reduce its code size. Our compression algorithm leverages the knowledge of “don't care” values in the control words and can reduce the code size by 3.3 times, on average. Despite such good results, as shown in this article, these compression techniques lead to poor FPGA implementations because they require many on-chip RAMs. To address this issue, we introduce an FPGA-aware dictionary-based technique that uses the dual-port feature of on-chip RAMs to reduce the number of utilized block RAMs by half. Additionally, we propose cascading two-levels of dictionaries for code size and block RAM reduction of large programs. For an MP3 application, a merged, cascaded, three-dictionary implementation reduces the number of utilized block RAMs by 4.3 times (76%) compared to a NISC without compression. This corresponds to 20% additional savings over the best single level dictionary-based compression.	algorithm;data compression;dictionary;don't-care term;field-programmable gate array;mp3;microprocessor;no instruction set computing;overhead (computing);programming paradigm;random-access memory	Bita Gorjiara;Mehrdad Reshadi;Daniel Gajski	2008	TRETS	10.1145/1371579.1371583	data compression;chip;embedded system;computer architecture;parallel computing;computer science;theoretical computer science;operating system;field-programmable gate array	Arch	6.789445864581786	44.93270701503704	166099
cd6a72a65f4fc709af7a10ce1ff738e1652cee2e	acceleration of block-matching algorithms using a custom instruction-based paradigm on a nios ii microprocessor	informatica;signal image and speech processing;quantum information technology spintronics	This contribution focuses on the optimization of matching-based motion estimation algorithms widely used for video coding standards using an Altera custom instruction-based paradigm and a combination of synchronous dynamic random access memory (SDRAM) with on-chip memory in Nios II processors. A complete profile of the algorithms is achieved before the optimization, which locates code leaks, and afterward, creates a custom instruction set, which is then added to the specific design, enhancing the original system. As well, every possible memory combination between on-chip memory and SDRAM has been tested to achieve the best performance. The final throughput of the complete designs are shown. This manuscript outlines a low-cost system, mapped using very large scale integration technology, which accelerates software algorithms by converting them into custom hardware logic blocks and showing the best combination between on-chip memory and SDRAM for the Nios II processor. Keyword: Computer vision, Optical flow, MPEG compression, Block-matching algorithm, Nios II, FPGA, Custom instructions, Embedded systems	block-matching algorithm;central processing unit;computer vision;data compression;dynamic random-access memory;electronic hardware;embedded system;field-programmable gate array;integrated circuit;mathematical optimization;microprocessor;motion estimation;moving picture experts group;nios embedded processor;optical flow;programming paradigm;random access;throughput;very-large-scale integration;video coding format	Diego González;Guillermo Botella Juan;Carlos García;Manuel Prieto;Francisco Tirado	2013	EURASIP J. Adv. Sig. Proc.	10.1186/1687-6180-2013-118	parallel computing;real-time computing;computer hardware;telecommunications;computer science;electrical engineering;memory bandwidth;algorithm	EDA	6.8598196700329765	45.13581464706234	166276
21ec08d910bb9ca5aef06d554ebe2d08c3a6718f	on the performance of bwa on numa architectures		Rapid progress in genome sequencing techniques is creating the necessity of advanced algorithms to process such information in reasonable time. Alignment applications such as BWA (Burrows Wheeler Aligner) are essential for solving genomic variant calling studies. Although BWA takes advantage of multithreading execution, it exhibits significant scalability limitations on systems with a non-uniform memory architecture (NUMA). Data sharing between independent threads and irregular memory access patterns constitute performance limiting factors that affect BWA's scalability. We have analyzed performance problems of BWA on two NUMA systems: one based on Intel Xeon and the other one based on AMD Opteron. We present some simple techniques that can be applied at system level and do not require any application modification. Significant improvements in speedup were achieved when these techniques were applied to the execution of BWA on both systems.	algorithm;burrows–wheeler transform;computer cluster;data structure;experiment;interleaved memory;list of sequence alignment software;memory access pattern;memory bank;multi-core processor;multithreading (computer architecture);non-uniform memory access;parallel computing;regular expression;run time (program lifecycle phase);scalability;speedup;thread (computing);whole genome sequencing	Josefina Lenis;Miquel A. Senar	2015	2015 IEEE Trustcom/BigDataSE/ISPA	10.1109/Trustcom.2015.638	computer architecture;parallel computing;computer science;operating system	HPC	-1.0880062923332066	43.175845431133475	166508
ee6dcb433fb0eb65413d4457f2f675e121e4b41b	arithmetic of $$\tau $$ τ -adic expansions for lightweight koblitz curve cryptography		Koblitz curves allow very efficient elliptic curve cryptography. The reason is that one can trade expensive point doublings to cheap Frobenius endomorphisms by representing the scalar as a $$\tau $$ τ -adic expansion. Typically elliptic curve cryptosystems, such as ECDSA, also require the scalar as an integer. This results in a need for conversions between integers and the $$\tau $$ τ -adic domain, which are costly and hinder the use of Koblitz curves on very constrained devices, such as RFID tags, wireless sensors, or certain applications of the Internet of things. We provide solutions to this problem by showing how complete cryptographic processes, such as ECDSA signing, can be completed in the $$\tau $$ τ -adic domain with very few resources. This allows outsourcing conversions to a more powerful party. We provide several algorithms for performing arithmetic operations in the $$\tau $$ τ -adic domain. In particular, we introduce a new representation allowing more efficient and secure computations compared to the algorithms available in the preliminary version of this work from CARDIS 2014. We also provide datapath extensions with different speed and side-channel resistance properties that require areas from less than one hundred to a few hundred gate equivalents on 0.13-$$\upmu $$ μ m CMOS. These extensions are applicable for all Koblitz curves.	16-bit;adder (electronics);algorithm;antivirus software;binary multiplier;cmos;code;computation;cryptosystem;datapath;digital signature;elliptic curve cryptography;gate equivalent;hercules graphics card;internet of things;matrix multiplication;national fund for scientific research;outsourcing;pegasus;radio-frequency identification;sensor;sensor node;server (computing);side-channel attack;subtractor	Kimmo Järvinen;Sujoy Sinha Roy;Ingrid Verbauwhede	2018	Journal of Cryptographic Engineering	10.1007/s13389-018-0182-0		Security	9.135418015906302	43.433640712308794	167198
3f6852ac389625ad4befb367819cb770b61e7f31	cache conscious walsh-hadamard transform	cache storage;degradation;mips r10000 cache conscious walsh hadamard transform signal processing cache performance cache friendly technique data reorganization cache pollution search algorithm optimal factorization tree problem size stride access alpha 21264;cache conscious walsh hadamard transform;search algorithm;tree data structures;packaging;software architecture hadamard transforms signal processing cache storage software packages;alpha 21264;software architecture;performance improvement;walsh hadamard transform;cache friendly technique;cache pollution;codes;hadamard transforms;signal processing;cache performance;fast fourier transforms;software algorithms;stride access;signal processing algorithms;high performance;optimal factorization tree;data reorganization;problem size;software packages;signal processing algorithms degradation packaging pollution codes fast fourier transforms software packages software architecture software algorithms tree data structures;mips r10000;pollution	The Walsh-Hadamard Transform (WHT) is an important tool in signal processing because of its simplicity. One major problem with the existing packages is their poor scalability to larger problems. In computing large size WHT, nonunit stride accesses result in poor cache performance leading to severe degradation in performance. In this paper, we develop an efficient cache friendly technique that drastically enhances the performance of large size WHT. In our approach, data reorganization is performed between computation stages to reduce cache pollution. We develop an efficient search algorithm to determine the optimal factorization tree depending on the problem size and stride access in the decomposition. We have developed a package and demonstrated improved performance. For example, on Alpha 21264 and MIPS R10000, our technique results in upto 180% performance improvement over the state-ofthe-art package. The proposed optimization is applicable to other signal transforms and is portable across various platforms.	analysis of algorithms;cpu cache;cache pollution;computation;elegant degradation;hadamard transform;mathematical optimization;r10000;scalability;search algorithm;signal processing;stride of an array	Neungsoo Park;Viktor K. Prasanna	2001		10.1109/ICASSP.2001.941140	software architecture;fast fourier transform;packaging and labeling;cache-oblivious algorithm;parallel computing;real-time computing;cache coloring;degradation;pollution;computer science;theoretical computer science;signal processing;tree;cache algorithms;cache pollution;code;search algorithm	HPC	-0.21300982376696453	41.518202112058425	167366
4764cd23104fd3022dfdfabbabd705219b655109	a parallel function evaluation approach for solution to large-scale equation-oriented models	parallel computing;equation oriented model;multi core processor;gpu;function evaluation	The equation-oriented (EO) approach is widely used for process simulation and optimization. Nevertheless, large-scale EO models consist of a huge number of nonlinear equations and make the solution procedure a challenging and time-consuming task. For most gradient-based numerical algorithms, function evaluations are the dominant step during the solution procedure. Here, a parallel computation method is developed for function evaluations within EO optimization strategies. After dividing the equations into several groups, function evaluations are calculated by using multiple threads on a parallel hardware platform simultaneously. Theoretical analysis for the speedup ratio is conducted. The implementation of the proposed method on a multi-core processor platform as well as a graphics processing unit (GPU) platform is then presented with several case studies. Numerical results are compared and discussed to show that the multi-core processor implementation has good computational performance, whereas the GPU implementation only achieves computational acceleration under relatively specific conditions.		Yannan Ma;Zhijiang Shao;Xi Chen;Lorenz T. Biegler	2016	Computers & Chemical Engineering	10.1016/j.compchemeng.2016.07.015	multi-core processor;mathematical optimization;parallel computing;computer science;theoretical computer science	AI	-4.385264242160445	39.05161532474692	167412
c09b41135935f7da078d06d5773db25a5299f001	speech data compression for embedded systems	read only storage;digital signal processing chips vector quantisation speech coding runlength codes huffman codes read only storage microcomputer applications;data compression embedded system read write memory speech coding humans animals random access memory production systems costs read only memory;data compression;lossy compression;lossless compression;speech coding;8 bit speech data compression embedded systems voice related toys voice related devices interactive sound responses pc human speech symphony animal songs masked rom intel 8051 embedded chip vector quantization lossy data compression lossless compression runlength huffman encoding unary code processing speed length limited huffman code splaying 1 to 8 mbyte;huffman codes;embedded system;chip;runlength codes;digital signal processing chips;vector quantizer;production cost;microcomputer applications;vector quantisation;processing speed	The main concern of this paper is speech data compression for low-cost embedded systems such as voice-related toys or devices with interactive sound-responses. We use a PC to generate and compress 8-bit-speech-data that has various features such as human speech, symphony and animal songs; the compressed data are then transferred to a masked-ROM. An Intel 8051 embedded chip is employed to expand the compressed speech data stored in a masked-ROM with size ranging from 1 8 Mega Bytes. No RAM ( read/write memory ) is available in the system ( to reduce production cost 1. The ROM also stores the program that is responsible for expanding the compressed data. Since the compression is done off-line, we use simple vectorquantization techniques to perform a first stage lossy compression of the data; the lossy-compressed data are then treated by a second stage lossless compression. Because of the the lack of RAM in the expansion stage, an LZ compression technique cannot be applied. Instead, we use a runlengthHuffman encoding scheme coupled with a unary-code to perform the lossless compression. Also, because of the slow processing speed of the processor in the expansion stage, a length-limited Huffman code must be used. Instead of using the common package-merge[ll algorithm to obtain the lengthlimited code, we develop a technique that uses splaying to achieve the goal.	8-bit;algorithm;byte;data compression;embedded system;huffman coding;intel mcs-51;lz77 and lz78;line code;lossless compression;lossy compression;mask rom;online and offline;random-access memory;read-only memory;read-write memory;speech coding;splay tree;symphony;time-compressed speech;toys;unary coding;unary operation	Tong Lai Yu	1996		10.1109/DCC.1996.488392	data compression;chip;lossy compression;speech recognition;computer hardware;image compression;computer science;theoretical computer science;speech coding;lossless compression;algorithm;statistics;huffman coding	Embedded	9.75354529281188	39.749283887457544	167479
5dd605a5478b8ce19f3c33dc16bce51c02619ea9	group-scheme: simd-based compression algorithms for web text data	simd;data compression;inverted index;index compression;encoding decoding layout compression algorithms arrays indexes program processors;integer encoding simd inverted index index compression;text analysis;simd instruction sets simd based compression algorithms web text data storage layout format instruction level parallelizability simd group scheme public trec data sets compression ratio encoding speed;indexing;integer encoding;text analysis data compression indexing parallel processing;parallel processing	Compression algorithms have been quite important for data oriented tasks, especially in the era of Big Data. The rapid development of modern processors facilitates us with powerful SIMD instruction sets, which provides an opportunity for better performance. Although SIMD based optimization on compression have been explored in some studies [2, 7], these studies usually focus on modifying the existing algorithms to fit into the SIMD instruction. In this paper, we propose a compression framework with a novel storage layout format, which aims to improve instruction-level parallelizability of compression algorithms. By instantiating the framework, we design a novel compression algorithm family, called Group-Scheme, and present a parallelized version of Group-Scheme, called SIMD-Group-Scheme. We evaluate the proposed algorithms on two public TREC data sets. With very competitive performance on compression ratio and encoding speed, SIMD-Group-Scheme significantly outperforms the implementation without SIMD instructions and state-of-the-art algorithm (i.e. SIMD-G8IU [7]), w.r.t decoding speed.	algorithm;big data;central processing unit;data compression;experiment;mathematical optimization;parallel computing;simd;text retrieval conference;text corpus	Xudong Zhang;Wayne Xin Zhao;Dongdong Shan;Hongfei Yan	2013	2013 IEEE International Conference on Big Data	10.1109/BigData.2013.6691617	data compression;computer architecture;parallel computing;computer science;theoretical computer science;move-to-front transform;lossless compression	DB	1.1629850479706818	42.00427136438316	167796
a37b0cf27f9e85528fb88d9af190579f3a622a2a	limits of the distributed finite element time domain algorithm in multi-computer environment	basic metrics;finite element time domain;real value;paper deal;concurrent version;algorithm work;approximated asymptotic value;multi-computer environment;different multi-computer;spmd mode;message passing interface;distributed algorithms;hardware;distributed computing;finite element methods;mesh generation;distributed environment;symmetric matrices;finite difference methods;high performance computing;computational electromagnetics;message passing;sparse matrices	This paper deals with a concurrent version of the finite element time domain method. The algorithm works in SPMD mode and is executed in the MPI distributed environment. Its properties are validated with some different multi-computer, loosely coupled platforms. The real value and approximated asymptotic values of the basic metrics of speedup are presented and discussed.	approximation algorithm;finite element method;loose coupling;spmd;speedup	Boguslaw Butrylo;Christian Vollaire;Laurent Nicolas	2004	Parallel Computing in Electrical Engineering, 2004. International Conference on	10.1109/PCEE.2004.41	mesh generation;distributed algorithm;parallel computing;message passing;sparse matrix;computer science;finite difference method;message passing interface;theoretical computer science;finite element method;distributed computing;computational electromagnetics;distributed computing environment;symmetric matrix	HPC	-4.394513414507084	38.15558611762768	168042
8a49a92d56d80957317a614bbc1c0aebb626c59f	high speed reconfigurable fft design by vedic mathematics	digital signal processing;power saving;multiplication operator;software defined radio;design optimization;fast fourier transform;wireless communication;cs oh;high speed	technology that is omnipresent in almost every engineering discipline. Faster additions and multiplications are of extreme importance in DSP for convolution, discrete Fourier transform, digital filters, etc. The core computing process is always a multiplication routine; therefore, DSP engineers are constantly looking for new algorithms and hardware to implement them. Vedic mathematics is the name given to the ancient system of mathematics, which was rediscovered, from the Vedas between 1911 and 1918 by Sri Bharati Krishna Tirthaji. The whole of Vedic mathematics is based on 16 sutras (word formulae) and manifests a unified structure of mathematics. As such, the methods are complementary, direct and easy. Due to a growing demand for such complex DSP application, high speed, low cost system-on-a-chip (SOC) implementation of DSP algorithm are receiving increased the attention among the researchers and design engineer. Fast Fourier Transform (FFT) is the one of the fundamental operations that is typically performed in any DSP system. Basic formula of computation of FFT is	algorithm;computation;convolution;digital filter;digital signal processing;digital signal processor;discrete fourier transform;fast fourier transform;system on a chip	Ashish Raman;Anvesh Kumar;Rakesh Kumar Sarin	2010	CoRR		multiplication operator;fast fourier transform;electronic engineering;parallel computing;multidisciplinary design optimization;computer hardware;computer science;electrical engineering;digital signal processing;software-defined radio;wireless	EDA	7.93195473901566	40.88736122242734	168205
86d3df739f008d20effeddf11604b1386e62ea8e	gmu hardware api for authenticated ciphers		In this paper, we propose a universal hardware Application Programming Interface (API) for authenticated ciphers. In particular, our API is intended to meet the requirements of all algorithms submitted to the CAESAR competition. Two major parts of the API, the interface and the communication protocol, were developed with the goal of reducing any potential biases in benchmarking of authenticated ciphers in hardware. Our high-speed implementation of the proposed hardware API includes universal, open-source pre-processing and post-processing units, common for all CAESAR candidates and the current standards, such as AES-GCM and AES-CCM. Apart from the full documentation, examples, and the source code of the pre-processing and post-processing units, we have made available in public domain a) a universal testbench to verify the functionality of any CAESAR candidate implemented using our hardware API, b) a Python script used to automatically generate test vectors for this testbench, c) VHDL wrappers used to determine the maximum clock frequency and the resource utilization of all implementations, and d) RTL VHDL source codes of high-speed implementations of AES and the Keccak Permutation F, which may be used as building blocks in implementations of related ciphers. We hope that the existence of these resources will substantially reduce the time necessary to develop hardware implementations of all CAESAR candidates for the purpose of evaluation, comparison, and future deployment in real products. 1 Motivation The CAESAR competition [1], launched in 2014, aims at identifying a portfolio of future authenticated ciphers with security, performance, and flexibility exceeding that of the current standards, such as AES-GCM [2] and AES-CCM [3]. Although security is commonly accepted to be the most important criterion in all cryptographic contests, it is rarely by itself sufficient to determine a winner. This is because multiple candidates generally offer adequate security, and a tradeoff between security and performance must be investigated. The focus of this paper is to facilitate the comparison of modern authenticated ciphers in terms of their performance and cost in hardware, and in particular in FPGAs, All Programmable Systems on Chip, and ASICs. As a starting point for such a comparison we propose defining hardware API, composed of the specification of an interface of the authenticated cipher core, and the communication protocol describing the exact format of all inputs and outputs, as well as the timing dependencies among all data and control signals passing through the specified interface. Similarly to the case of previous contests, software implementations of the CAESAR candidates are being compared using a uniform API, clearly defined in the call for submissions [1]. So far, no similar hardware API has been proposed, not to mention accepted by the cryptographic community. As a result any attempt at the comparison of existing hardware implementations is highly dependent on specific assumptions about the hardware API, made independently by various hardware designers. These assumptions can have potentially a very high influence on all major performance measures of the developed implementations. Additionally, a hardware API is typically much more difficult to modify than a software API, making any last minute standardization efforts and code adjustments highly inefficient and questionable. Therefore, there is a clear need for a proposal regarding a uniform hardware API, which could be further modified and improved using feedback from the cryptographic community, and eventually endorsed by the CAESAR Committee, and adopted by majority of future hardware developers. Our goal is to address this issue by providing the exact specification of the proposed interface, as well as multiple supporting materials, such as open-source codes of pre-processing and post-processing units, a universal testbench, and uniform ways of generating optimized results. 2 Proposed Features The proposed features of our hardware API are as follows: – inputs of arbitrary size in bytes (but a multiple of a byte only) – size of the entire message/ciphertext does not need to be known before the encryption/decryption starts (unless required by the algorithm itself) – wide range of data port widths, 8 ≤ w ≤ 256 – independent data and key inputs – simple high-level communication protocol – support for the burst mode – possible overlap among processing the current input block, reading the next input block, and storing the previous output block – storing decrypted messages internally, until the result of authentication is known – support for encryption and decryption within the same core – ability to communicate with very simple, passive devices, such as FIFOs – ease of extension to support existing communication interfaces and protocols, such as AMBA-AXI4 – a de-facto standard for the System-on-Chip (SoC) buses [4], and PCI Express – high-bandwidth serial communication between PCs and hardware accelerator boards [5].	aes instruction set;advanced encryption standard process;algorithm;application programming interface;application-specific integrated circuit;authentication;burst mode (computing);byte;caesar;cipher;ciphertext;clock rate;code;communications protocol;documentation;emoticon;field-programmable gate array;google cloud messaging;hardware acceleration;high- and low-level;mpsoc;open-source software;pci express;preprocessor;pseudorandom permutation;python;requirement;sha-3;serial communication;software deployment;system on a chip;test bench;vhdl;video post-processing	Ekawat Homsirikamol;William Diehl;Ahmed Ferozpuri;Farnoud Farahmand;Umar Sharif;Kris Gaj	2015	IACR Cryptology ePrint Archive		cryptography;ciphertext;cipher;encryption;hardware application;application programming interface;vhdl;computer hardware;hardware acceleration;computer science	Arch	8.35764960119966	45.7951226400548	168267
6c956689db4e8595b21bd627beef41a35c401d7f	a survey on reconfigurable accelerators for cloud computing	reconfigurable architectures cloud computing computer centres field programmable gate arrays;data centers reconfigurable computing hardware accelerator cloud computing fpgas;measurement;fpgas reconfigurable hardware accelerators cloud computing data centers network traffic web applications bandwidth switches general purpose processor;data analysis;sparks;field programmable gate arrays;cloud computing field programmable gate arrays hardware data analysis benchmark testing sparks measurement;benchmark testing;cloud computing;hardware	Data centers are experiencing an exponential increase in the amount of network traffic that they have to sustain due to cloud computing and several emerging web applications. To face this network load, large data centers are required with thousands of servers interconnected with high bandwidth switches. Current data center, based on general purpose processor, consume excessive power while their utilization is quite low. Hardware accelerators can provide high energy efficiency for many cloud applications but they lack the programming efficiency of processors. In the last few years, there several efforts for the efficient deployment of hardware accelerators in the data centers. This paper presents a thorough survey of the frameworks for the efficient utilization of the FPGAs in the data centers. Furthermore it presents the hardware accelerators that have been implemented for the most widely used cloud computing applications. Furthermore, the paper provides a qualitative categorization and comparison of the proposed schemes based on their main features such as speedup and energy efficiency.	categorization;central processing unit;cloud computing;data center;field-programmable gate array;hardware acceleration;high-level programming language;high-level synthesis;in-memory database;memcached;network switch;network traffic control;opencl api;server (computing);software deployment;speedup;time complexity;web application	Christoforos Kachris;Dimitrios Soudris	2016	2016 26th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2016.7577381	embedded system;benchmark;parallel computing;real-time computing;cloud computing;reconfigurable computing;computer science;operating system;cloud testing;data analysis;fpgac;field-programmable gate array;measurement	HPC	-3.5872964075720017	45.98959405448058	168269
e01d5262980a2139b8dd2d7c0c12de785f52c539	large vocabulary speech recognition on parallel architectures	a decoder large vocabulary speech recognition parallel architectures integration capacity classical viterbi beam search multicore processor architectures graphics processing unit gpu a search;vocabulary;parallel architectures;speech recognition gpu multicore parallel;speech recognition;large scale systems pattern recognition optimization parallel processing vocabularies;multiprocessing systems;vocabulary multiprocessing systems parallel architectures speech recognition	The speed of modern processors has remained constant over the last few years but the integration capacity continues to follow Moore's law and thus, to be scalable, applications must be parallelized. The parallelization of the classical Viterbi beam search has been shown to be very difficult on multi-core processor architectures or massively threaded architectures such as Graphics Processing Unit (GPU). The problem with this approach is that active states are scattered in memory and thus, they cannot be efficiently transferred to the processor memory. This problem can be circumvented by using the A* search which uses a heuristic to significantly reduce the number of explored hypotheses. The main advantage of this algorithm is that the processing time is moved from the search in the recognition network to the computation of heuristic costs, which can be designed to take advantage of parallel architectures. Our parallel implementation of the A* decoder on a 4-core processor with a GPU led to a speed-up factor of 6.13 compared to the Viterbi beam search at its maximum capacity and an improvement of 4% absolute in accuracy at real-time.	a* search algorithm;beam search;central processing unit;computation;graphics processing unit;heuristic;memory management;moore's law;multi-core processor;parallel algorithm;parallel computing;real-time clock;scalability;speech recognition;vocabulary	Patrick Cardinal;Pierre Dumouchel;Gilles Boulianne	2013	IEEE Transactions on Audio, Speech, and Language Processing	10.1109/TASL.2013.2271591	computer architecture;parallel computing;speech recognition;computer science;theoretical computer science	HPC	2.120148981363504	39.40642841906251	168624
9190f28791c99d83959c2135a75b3a41a1ca2bac	real-time robot dynamic simulation on a vector/parallel supercomputer	complex dynamics;motion control;concurrent computing;real time;dynamics equations;manipulator dynamics;cray y mp8 864;real time simulation;parallel robots supercomputers computational modeling manipulator dynamics concurrent computing parallel processing real time systems equations orbital robotics motion control;orbital robotics;temporal parallelism;robot dynamic simulation;robots cray computers digital simulation parallel processing predictor corrector methods real time systems;computational modeling;parallel robots;robots;computational rate;parallel computer;motion integration;predictor corrector methods;vector parallel supercomputer;coarse grain parallel block predictor corrector method;coarse grained;robot dynamics;multiple cpus;computational rate robot dynamic simulation vector parallel supercomputer real time simulation dynamics equations coarse grain parallel block predictor corrector method multiple cpus temporal parallelism cray y mp8 864;supercomputers;parallel processing;cray computers;digital simulation;real time systems	The authors present the results of research performed in computational robot dynamics to achieve real-time simulation of a manipulator on a general-purpose vector/parallel computer. Once the complex dynamics equations required in the simulation have been effectively vectorized, a coarse-grain parallel block predictor-corrector method for performing the motion integration is realized on multiple CPUs to exploit a form of temporal parallelism. Results on a CRAY Y-MP8/864 show that effective use of vectorization and parallelization yields an order-of-magnitude speedup, resulting in a computational rate 50 times faster than real-time for reasonable end-effector position errors on the order of a micron. This translates to real-time performance on a less powerful parallel computing system. >	real-time transcription;robot;simulation;supercomputer	Scott McMillan;David E. Orin;P. Sadayappan	1991		10.1109/ROBOT.1991.131891	robot;motion control;computational science;parallel processing;parallel manipulator;parallel computing;complex dynamics;concurrent computing;computer science;artificial intelligence;theoretical computer science;computational model	Robotics	2.482763025963976	38.30720551928661	168657
0865e3ae28f9f55fe73b60bf492c0c4c159dad6f	architecture-based design and optimization of genetic algorithms on multi- and many-core systems	gpu;accuracy;genetic algorithm;speedup;architecture;multi core	A Genetic Algorithm (GA) is a heuristic to find exact or approximate solutions to optimization and search problems within an acceptable time. We discuss GAs from an architectural perspective, offering a general analysis of performance of GAs on multi-core CPUs and on many-core GPUs. Based on the widely used Parallel GA (PGA) schemes, we propose the best one for each architecture. More specifically, the Asynchronous Island scheme, Island/Master–Slave Hierarchy PGA and Island/Cellular Hierarchy PGA are the best for multi-core, multi-socket multi-core and many-core architectures, respectively. Optimization approaches and rules based on a deep understanding of multiand many-core architectures are also analyzed and proposed. Finally, the comparison of GA performance on multi-core and many-core architectures are discussed. Three real GA problems are used as benchmarks to evaluate our analysis and findings. There are three extra contributions compared to previous work. Firstly, our findings based on deeply analyzing architectures can be applied to all GA problems, even for other parallel computing, not for a particular GA problem. Secondly, the performance of GAs in our work not only concerns execution speed, also the solution quality has not been considered seriously enough. Thirdly, we propose the theoretical performance and optimization models of PGA onmulti-core andmany-core architectures, finding a more practical result of the performance comparison of the GA on these architectures, so that the speedup presented in this work is more reasonable and is a better guide to practical decisions. © 2013 Published by Elsevier B.V.	approximation algorithm;benchmark (computing);central processing unit;genetic algorithm;graphics processing unit;heuristic;intel core (microarchitecture);manycore processor;mathematical optimization;multi-core processor;parallel computing;performance evaluation;program optimization;software release life cycle;speedup	Long Tai Zheng;Yanchao Lu;Minyi Guo;Song Guo;Cheng-Zhong Xu	2014	Future Generation Comp. Syst.	10.1016/j.future.2013.09.029	parallel computing;real-time computing;simulation;genetic algorithm;speedup;computer science;theoretical computer science;architecture;operating system;distributed computing	HPC	-1.7151443658878587	44.54995739539545	168870
cd6f5c5fa6d30cd8f6cfe01224550d7871d8844e	fpga-based implementation of efficient sample rate conversion for software defined radios	field programmable gate array;standards;software defined radio;high level design methodology fpga based implementation efficient sample rate conversion software defined radios wireless communications direct hdl code generation power consumption reduction xilinx vertex ii pro fpga board;farrow interpolator;xilinx vertex ii pro fpga board;code generation;field programmable gate array fpga;strontium;fpga based implementation;software radio;wireless communication;computer architecture;efficient implementation;wireless communications;software defined radios;efficient sample rate conversion;sample rate conversion;digital filters;direct hdl code generation;computer architecture software radio field programmable gate arrays strontium digital filters matlab standards;software radio field programmable gate arrays power consumption;power consumption;field programmable gate arrays;cic filter;matlab;power consumption reduction;field programmable gate array fpga software radio sample rate conversion cic filter farrow interpolator;high level design methodology;design methodology	Software Radio is a burgeoning application in the domain of wireless communications. A lot of effort has recently been directed into developing practical algorithms for IF based, multi-standard Software Radio receiver architectures. Significant issues exist in developing an efficient implementation suitable for the processing of multiple standards through a single architecture with the help of sample rate conversion filters. In this context, this paper presents a practical implementation of multi-stage sample rate conversion in multi-standard software radios. This work includes complete design and subsequent implementation of sample rate conversion filters on a Xilinx Vertex II Pro FPGA Board. A high-level design methodology has been adopted that involves direct HDL code generation from algorithm description for testing and implementation on the FPGA. This approach is helpful in the rapid development and prototyping of multi-standard Software Radio and is facilitative in the initial stages of the design. The synthesized circuit utilizes 2% area of the Vertex II Pro FPGA. Power consumption reduction of around 2% is also visible in comparison to previously proposed reconfigurable decimation filter architecture	algorithm;code generation (compiler);decimation (signal processing);field-programmable gate array;hardware description language;high- and low-level;level design;paintshop pro;sample rate conversion	Muhammad Ali Siddiqi;Nabeel Samad;Shahid Masud;Faheem Sheikh	2010	2010 10th IEEE International Conference on Computer and Information Technology	10.1109/CIT.2010.410	embedded system;real-time computing;computer science;operating system;software-defined radio;fpga prototype	EDA	8.313657488223566	40.898563600893816	168938
e59a8f7ff0c61b3345a74937a32229b5015a573e	towards cosimulating network and electrical systems for performance evaluation in smart grid	network system simulation;power system simulation;cosimulation;smart grid;oocosim	In this paper, we construct a novel simulation framework OOCoSim which can simulate the distribution domain of Smart Grid. OOCoSim consists of OPNET, OpenDSS and CoSim module and it can perform dynamic and organic cosimulation. In order to present the effectiveness of OOCoSim, we firstly define a simulation model and then conduct a simulation study with the OOCoSim. The simulation results shows that OOCoSim can successfully simulate the integrated scenario of the power and network systems.	performance evaluation;simulation	Hwantae Kim;Wonkyun Park	2013		10.1145/2480362.2480493	embedded system;dynamic simulation;real-time computing;simulation;computer science;power system simulation;smart grid	HPC	6.112141359324219	35.73215471274073	169022
17609e9147c7ccb39a4e3535626ac81089ccbf5f	the kit cosmos processor: an application of multi-threading for dynamic optimization	dynamic optimization		program optimization	Toshiyuki Yamamoto;Kou Morita;Toshinori Sato;Itsujiro Arita	2002			simulation;computer science;operations research;computer graphics (images)	Arch	-2.3148547220405153	45.441500063027654	169124
063dc8b679fce4c9b0e4f35091bdc56c0b3a5980	multimedia instructions in ia-64	microprocessors;digital signal processing;video compression;instruction set architecture;arithmetic instruction sets acceleration video compression transform coding digital signal processing parallel processing pipelines microprocessors graphics;transform coding;acceleration;media processor;pipelines;arithmetic;cost effectiveness;floating point;parallel processing;graphics;instruction sets	We discuss the integer and floating-point multimedia instructions in the IA-64 instruction-set architecture (ISA). These multimedia instructions implement subword parallelism, also called packed parallelism or microSIMD parallelism. They are both a subset and a superset of the multimedia instructions from the predecessor architectures: MMX, SSE and SSE-2 from the IA-32 architecture, and MAX and MAX-2 from the PARISC architecture. We discuss the novel subword permutation instructions that are new in the IA-64, and their effectiveness, in combination with the subword arithmetic instructions, for speeding up multimedia programs. These packed arithmetic and permutation instructions can also be used in media processors and DSPs for very fast and cost-effective multimedia processing.	central processing unit;ia-32;ia-64;mmx (instruction set);max;media processor;multimedia acceleration extensions;pa-risc;parallel computing;streaming simd extensions;substring	Ruby B. Lee;A. Murat Fiskiran;Abdulla Bubsha	2001	IEEE International Conference on Multimedia and Expo, 2001. ICME 2001.	10.1109/ICME.2001.1237694	parallel processing;computer architecture;parallel computing;computer hardware;computer science;operating system;instruction set;instructions per cycle	HPC	6.622778649749482	44.84089299410469	169133
1d38aa6da4c2367231bdb1b29446f1232d54bbd6	graphs and scheduling (ecco xii)				Silvano Martello;Jean François Maurras	2002	European Journal of Operational Research	10.1016/S0377-2217(01)00205-3	mathematical optimization;theoretical computer science;scheduling (computing);mathematics;graph	Robotics	7.075824854478325	32.45363373296909	169599
8e4dad48cbc091ac08b81d273dfd7b22b84a266f	compressing the incompressible with isabela: in-situ reduction of spatio-temporal data	data intensive application;lossy compression;high performance computing;b spline;in situ processing	Modern large-scale scientific simulations running on HPC systems generate data in the order of terabytes during a single run. To lessen the I/O load during a simulation run, scientists are forced to capture data infrequently, thereby making data collection an inherently lossy process. Yet, lossless compression techniques are hardly suitable for scientific data due to its inherently random nature; for the applications used here, they offer less than 10% compression rate. They also impose significant overhead during decompression, making them unsuitable for data analysis and visualization that require repeated data access. To address this problem, we propose an effective method for In-situ SortAnd-B-spline Error-bounded Lossy Abatement (ISABELA) of scientific data that is widely regarded as effectively incompressible. With ISABELA, we apply a preconditioner to seemingly random and noisy data along spatial resolution to achieve an accurate fitting model that guarantees a ≥ 0.99 correlation with the original data. We further take advantage of temporal patterns in scientific data to compress data by ≈ 85%, while introducing only a negligible overhead on simulations in terms of runtime. ISABELA significantly outperforms existing lossy compression methods, such as Wavelet compression. Moreover, besides being a communication-free and scalable compression technique, ISABELA is an inherently local decompression method, namely it does not decode the entire data, making it attractive for random access.	approximation;b-spline;cubic function;data access;data compression;effective method;energy citations database;global telecommunications system;input/output;lossless compression;lossy compression;overhead (computing);preconditioner;random access;requirement;scalability;signal-to-noise ratio;simulation;sorting;terabyte;wavelet transform	Sriram Lakshminarasimhan;Neil Shah;Stéphane Ethier;Scott Klasky;Robert Latham;Robert B. Ross;Nagiza F. Samatova	2011		10.1007/978-3-642-23400-2_34	lossy compression;b-spline;supercomputer;parallel computing;real-time computing;computer science;theoretical computer science;operating system;data mining;lossless compression;algorithm;statistics	HPC	1.3667505659066197	38.814368042746814	169614
0d8a0ad31320376016264698e0580d94b0453615	design strategies and modified descriptions to optimize cipher fpga implementations: fast and compact results for des and triple-des	performance estimation;fft;fpga;resource use;fpga implementation;matrix multiplicaiton;energy efficient design techniques;design methodology	We propose a new mathematical DES description that allows optimized implementations. It also provides the best DES and triple-DES FPGA implementations known in term of ratio throughput/area, where area means the number of FPGA slices used. First, we get a less resource consuming unrolled DES implementation that works at data rates of 21.3 Gbps (333 MHz), using VIRTEX II technology. In this design, the plaintext, the key and the mode (encryption/decrytion) can be changed on a cycle-by-cycle basis with no dead cycles. In addition, we also propose sequential DES and triple-DES designs that are currently the most efficient ones in term of resources used as well as in term of throughput. Based on our DES and triple-DES results, we also set up conclusions for optimized FPGA design choices and possible improvement of cipher implementations with a modified structure description.	cipher;cycle basis;data rate units;encryption;field-programmable gate array;plaintext;throughput;triple des;virtex (fpga)	Gaël Rouvroy;François-Xavier Standaert;Jean-Jacques Quisquater;Jean-Didier Legat	2003		10.1145/611817.611879	embedded system;fast fourier transform;computer architecture;parallel computing;design methods;matrix multiplication;computer science;theoretical computer science;field-programmable gate array	Crypto	9.509420357241554	45.50950173633354	169649
f33a40d2ec7ccc61eb0a5ec619c693e63ab6cb7d	ana - black box (optical cpu)		"""—This paper demonstrates a model for Black Box CPU which is a part of Advanced Network Architecture (ANA) PC. The Black Box does not require any electronic or electrical devices; it uses optical devices for processing and storing of data. ANA is used for advanced networking operations. The Black Box is the part of the ANA PC, which serves as a generic CPU with fast processing and huge storage of data. The basic differences between the generic CPU and Black Box CPU are the storage procedures, I/O devices; devices used processing data and the mobility of the box. The basic principle behind the Black Box is to minimize (mostly eliminate) electronic devices used by the CPU for processing & accessing of data with effective storage mechanism, """" Data will be in the form of light throughout the network and through out the system. Replace all electrical and electronic devices with optical devices. A Black Box containing all optical devices, used for storing huge amount of data and fast retrieval of data from the Box in the form of light, when connected to any other devices or systems. """""""	black box;input/output;network architecture;thinking outside the box	Kotaru Kiran	2006			artificial intelligence;machine learning;black box (phreaking);theoretical computer science;computer science	OS	7.209592950132317	39.98944010738712	169652
27fc1e30c9592591c2707d21c9923fe71d0852bd	energy efficient neuromorphic processing using spintronic memristive device with dedicated synaptic and neuron terminology		Research towards brain-inspired computing based on beyond CMOS devices has gained momentum in recent years. The motivation beyond this vigorous research prevails in exploitation of the resemblance between the computing principles and the device characteristics. To this end, the devices are used to perform otherwise time-consuming and power hungry tasks required for brain-inspired computing. Due to their miniaturized dimensions, zero leakage and nonvolatility, spintronic devices are among the most promising class of beyond CMOS devices. In this paper, we propose a novel spintronic structure based on antiferrromagnetically coupled domain walls. The device structure enables dedicated terminology for synaptic and neuron connections. This characteristic enables more efficient design of neuromorphic systems by allowing larger design space for designers. Furthermore, thanks to the coupling between the domain walls, the device can potentially operate at higher speeds while maintaining the energy consumption of the device; this higher speed contributes to improved performance of the neuromorphic system. In order to evaluate our proposed device structure, we developed a cross-layer simulation framework. Our simulation framework analyzes the neuromorphic system at the device, circuit and algorithm levels. Our simulation results show an order of magnitude improvement in the energy consumption compared to CMOS and analog neurons and up to 2X performance improvement as well as 8% improvement in the energy over state-of-the-art neuromorphic platforms using spintronic devices.	algorithm;beyond cmos;exploit (computer security);neuromorphic engineering;neuron;simulation;spectral leakage;spintronics;synaptic package manager	Zoha Pajouhi	2018	2018 19th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2018.8357266	electronic engineering;memristor;beyond cmos;efficient energy use;artificial neural network;energy consumption;computer science;leakage (electronics);cmos;neuromorphic engineering	EDA	4.9058223952210875	41.74083353020031	169689
ea2664e0d55e8ca81eebcc693aca84d7acdf50b4	application of embedded systems in low earth orbit for measurement of ionospheric anomalies	ionosphere;satellite;picosatellite;low earth orbit;embedded systems;faraday rotation;space;cubesat;signal distortion;embedded system	Space is a hazardous environment for both man and machine and to explore such a terrain a rugged, yet easily implementable, platform is needed. Low-cost, low-power embedded systems are the ideal solution since they allow for parallel development of components, which results in quick turnaround from inception to design. We present a design for detection of ionospheric phenomenon in Low Earth Orbit using embedded systems on small scale satellites. We describe the operation and architecture of this design.	embedded system;low-power broadcasting;rugged computer	George J. Starr;J. M. Wersinger;Richard Chapman;Lloyd Riggs;Victor P. Nelson;John Klingelhoeffer;Charles E. Stroud	2009			control reconfiguration;earth's orbit;discrete mathematics;geodesy;computer science;geostationary orbit;computer network;interference (wave propagation);file system;queue;medium earth orbit;ionosphere	EDA	3.4132288404930455	45.52615138459332	169784
280bbaa66095fd6f89999003b802700935fdf77c	efficient and scalable computations with sparse tensors	tensile stress sparse matrices optimization matrix decomposition vectors standards memory management;tensors data analysis;data analysis;large scale sparse real data sets scalable computations sparse tensors high dimensional multiaspect data multiway arrays multilinear algebraic computations tensor decompositions data summarization data analysis sparse tensor storage formats tensor decomposition algorithms data reuse optimization synthetic small data sets;tensors	For applications that deal with large amounts of high dimensional multi-aspect data, it becomes natural to represent such data as tensors or multi-way arrays. Multi-linear algebraic computations such as tensor decompositions are performed for summarization and analysis of such data. Their use in real-world applications can span across domains such as signal processing, data mining, computer vision, and graph analysis. The major challenges with applying tensor decompositions in real-world applications are (1) dealing with large-scale high dimensional data and (2) dealing with sparse data. In this paper, we address these challenges in applying tensor decompositions in real data analytic applications. We describe new sparse tensor storage formats that provide storage benefits and are flexible and efficient for performing tensor computations. Further, we propose an optimization that improves data reuse and reduces redundant or unnecessary computations in tensor decomposition algorithms. Furthermore, we couple our data reuse optimization and the benefits of our sparse tensor storage formats to provide a memory-efficient scalable solution for handling large-scale sparse tensor computations. We demonstrate improved performance and address memory scalability using our techniques on both synthetic small data sets and large-scale sparse real data sets.	algorithm;code generation (compiler);compiler;computation;computer memory;computer vision;control system;data mining;high- and low-level;locality of reference;mathematical optimization;parallel computing;scalability;scheduling (computing);signal processing;sparse matrix;synthetic intelligence	Muthu Manikandan Baskaran;Benoît Meister;Nicolas Vasilache;Richard Lethin	2012	2012 IEEE Conference on High Performance Extreme Computing	10.1109/HPEC.2012.6408676	computer science;theoretical computer science;machine learning;sparse approximation;data mining;multilinear subspace learning	HPC	-2.3274799531724737	40.94255656237014	169874
71c9fc1492fe015c380bb5625e13ed23b9669bda	an mpi-cuda library for image processing on hpc architectures	parallel computing;computacion informatica;image processing;gpu;ciencias basicas y experimentales;matematicas;grupo a	Scientific image processing is a topic of interest for a broad scientific community since it is a mean of gaining understanding and insight into the data for a growing number of applications. Furthermore, the technological evolution permits large data acquisition, with sophisticated instruments, and their elaboration through complex multidisciplinary applications, resulting in datasets that are growing at an extremely rapid pace. This results in the need of huge computational power for the processing. It is necessary to move towards High Performance Computing (HPC) and to develop proper parallel implementations of image processing algorithms/operations. Modern HPC resources are typically highly heterogeneous systems, composed of multiple CPUs and accelerators such as Graphics Processing Units (GPUs) and Field-Programmable Gate Arrays (FPGAs). The actual barrier posed by heterogeneous HPC resources is the development and/or the performance efficient porting of software on such complex architectures. In this context, the aim of this work is to enable image processing on cluster of GPUs, through the use of PIMA(GE) 2 Lib, the Parallel IMAGE processing GEnoa Library. The library is able to exploit traditional clusters through MPI, GPU device through CUDA and a first experimentation is aimed to explore the use of GPU-clusters. Library operations are provided to the users through a sequential interface defined to hide the parallelism of the computation. The parallel computation, at each level, is managed employing specific policies designed to suitably coordinate the parallel processes/threads involved in the elaboration and their use is tightly coupled with the PIMA(GE) 2 Lib interface. In this paper, we present the incremental approach adopted in the development of the library and the performance gains in each implementations: quite linear speedup is achieved on cluster architecture, about a 30% improvement in the execution time on a single GPU and the first results on cluster of GPUs are promising.	accessibility;algorithm;cuda;central processing unit;computation;computer cluster;critical point (network science);data acquisition;edge detection;field-programmable gate array;graphics processing unit;image processing;message passing interface;parallel computing;run time (program lifecycle phase);smoothing;speedup;tower mounted amplifier	Antonella Galizia;Daniele D'Agostino;Andrea Clematis	2015	J. Computational Applied Mathematics	10.1016/j.cam.2014.05.004	image processing;theoretical computer science;algorithm	HPC	-4.214630600576447	39.93101898147356	169999
039fd4afd1597d4ab1e499fb1ae64a400fe6ada2	parallel computing with low-cost fpgas: a framework for copacobana.	parallel computer	In many disciplines such as applied sciences or computer scien ce, computationally challenging problems demand for extraordinary computing power, mostly p rovided by super computers or clusters of conventional desktop CPUs. During the last de ca s, several flavors of super computers have evolved, most of which are suitable for a specifi c type of problem. In general, dedicated clusters and super computers suffer from their ext remely high cost per computation and are, due to the lack of cheap alternatives, currently the only possible solution to computational hard problems. More recently, emerging low-cost FPGAs tend to be a very cost-effective alternative to conventional CPUs for solving at least some of the computational hard problems such as those appearing in cryptanalysis and bio-informatic s.	british informatics olympiad;central processing unit;computation;cost efficiency;cryptanalysis;cryptography;custom hardware attack;database;desktop computer;ext js javascript framework;field-programmable gate array;gigabit;overhead (computing);parallel computing;programming tool;requirement;smith–waterman algorithm;supercomputer;throughput;μclinux	Tim Güneysu;Christof Paar;Jan Pelzl;Gerd Pfeiffer;Manfred Schimmler;Christian Schleiffer	2007			computer architecture;parallel computing;reconfigurable computing;computer science	Arch	0.36312438789118384	44.5820425256874	170339
a8d36dda95e10af1ef0bbbaaeb217308dc438e18	parallel solution of partial symmetric eigenvalue problems from electronic structure calculations	eigenvalue and eigenvector computation;electronic structure calculations;divide and conquer tridiagonal eigensolver;parallelization;blocked householder transformations	The computation of selected eigenvalues and eigenvectors of a symmetric (Hermitian) matrix is an important subtask in many contexts, for example in electronic structure calculations. If a significant portion of the eigensystem is required then typically direct eigensolvers are used. The central three steps are: Reduce the matrix to tridiagonal form, compute the eigenpairs of the tridiagonal matrix, and transform the eigenvectors back. To better utilize memory hierarchies, the reduction may be effected in two stages: full to banded, and banded to tridiagonal. Then the back transformation of the eigenvectors also involves two stages. For large problems, the eigensystem calculations often are the computational bottleneck, in particular with large numbers of processors. In this paper we discuss variants of the tridiagonal-to-banded back transformation, improving the parallel efficiency for large numbers of processors as well as the per-processor utilization. We also modify the divide-and-conquer algorithm for symmetric tridiagonal matrices such that it can compute a subset of the eigenpairs at reduced cost. The effectiveness of our modifications is demonstrated with numerical experiments.	algorithm;blas;bandwidth (signal processing);blocking (computing);blue gene;central processing unit;colour banding;computation;computer cluster;eigenvalue algorithm;electronic structure;experience;experiment;image scaling;infiniband;loop nest optimization;math kernel library;memory hierarchy;numerical analysis;overhead (computing);reduced cost;reduction (complexity);scalapack;speedup;the matrix	Thomas Auckenthaler;Volker Blum;Hans-Joachim Bungartz;Thomas Huckle;Rainer Johanni;Lukas Krämer;Bruno Lang;H. Lederer;Paul R. Willems	2011	Parallel Computing	10.1016/j.parco.2011.05.002	tridiagonal matrix;mathematical optimization;combinatorics;parallel computing;eispack;computer science;mathematics;tridiagonal matrix algorithm;algebra	HPC	-3.1518244767977714	38.207156902196736	170621
f9e95e4a2a43c86276e8a2a7196777491a2d263a	a new parallel data cube construction scheme	molap;data cube;parallel database;shared memory multiprocessors;extendible array	The pre-computation of data cubes is critical for improving the response time of OLAP On-Line Analytical Processing systems. To meet the need for improved performance created by growing data sizes, parallel solutions for data cube construction are becoming increasingly important. This paper presents a new parallel data cube construction scheme based on an extendible multidimensional array, which is dynamically extendible along any dimension without relocating any existing data. The authors have implemented and evaluated their parallel data cube construction methods on shared-memory multiprocessors. Given the performance limit, the methods achieve close to linear speedup with load balance. The authors' experiments also indicate that their parallel methods can be more scalable on higher dimensional data cube construction.	data cube	Dong Jin;Tatsuo Tsuji	2012	IJGHPC	10.4018/jghpc.2012040103	parallel computing;computer science;database;distributed computing;data cube	DB	-1.238927159158854	37.598889388337305	170648
54750ae50ada3efae2a9a3e38c09e54f87df389f	f3: beyond the horizon of conventional computing: from deep learning to neuromorphic systems	neural networks;neuromorphics;computer architecture;machine learning;field programmable gate arrays;hardware	This forum brings together experts in software applications, system architectures, and chip designs to explore cognitive computing approaches over the near-, mid-, and long-term.	cognitive computing;deep learning;neuromorphic engineering	Meng-Fan Chang;Jun Deguchi;Vivek De;Masato Motomura;Shinichiro Shiratake;Marian Verhelst	2017	2017 IEEE International Solid-State Circuits Conference (ISSCC)	10.1109/ISSCC.2017.7870481	embedded system;computer science;theoretical computer science;machine learning;artificial neural network;field-programmable gate array;computer engineering	EDA	3.752909498859039	41.88604264483291	170726
d201a8e85c6d7673d9d2f5833302db8bb3319b6d	sparkga: a spark framework for cost effective, fast and accurate dna analysis at scale		In recent years, the cost of NGS (Next Generation Sequencing) technology has dramatically reduced, making it a viable method for diagnosing genetic diseases. The large amount of data generated by NGS technology, usually in the order of hundreds of gigabytes per experiment, have to be analyzed quickly to generate meaningful variant results. The GATK best practices pipeline from the Broad Institute is one of the most popular computational pipelines for DNA analysis. Many components of the GATK pipeline are not very parallelizable though. In this paper, we present a parallel implementation of a DNA analysis pipeline based on the big data Apache Spark framework. This implementation is highly scalable and capable of parallelizing computation by utilizing data-level parallelism as well as load balancing techniques. In order to reduce the analysis cost, the framework can run on nodes with as little memory as 16GB. For whole genome sequencing experiments, we show that the runtime can be reduced to about 1.5 hours on a 20-node cluster with an accuracy of up to 99.9981%. Our solution is about 71% faster than other state-of-the-art solutions while also being more accurate. The source code of the software described in this paper is publicly available at https://github.com/HamidMushtaq/SparkGA1.git.	apache spark;best practice;big data;communications satellite;computation;data parallelism;experiment;gigabyte;image scaling;load balancing (computing);next-generation network;parallel computing;pipeline (computing);pipeline (software);scalability;technical standard;throughput;whole genome sequencing	Hamid Mushtaq;Frank Liu;Carlos Costa;Gang Liu;H. Peter Hofstee;Zaid Al-Ars	2017		10.1145/3107411.3107438	gigabyte;scalability;big data;bioinformatics;software;source code;spark (mathematics);computer science;load balancing (computing);pipeline transport	HPC	-1.4256508747321508	42.67646447145018	170851
5b3d7454bfcd0cedf9c6790a48a2023eaff5d862	clus_gpu-blastp: accelerated protein sequence alignment using gpu-enabled cluster	bioinformatics;blast;compute unified device architecture (cuda);graphical processing unit (gpu);high-performance computing;sequence alignment	Basic Local Alignment Search Tool (BLAST) is one of the most frequently used algorithms for bioinformatics applications. In this paper, an accelerated implementation of protein BLAST, i.e., CLUS_GPU-BLASTP for multiple query sequence processing in parallel, on graphical processing unit (GPU)-enabled high-performance cluster is proposed. The experimental setup consisted of a high-performance GPU-enabled cluster. Each compute node of the cluster consisted of two hex-core Intel, Xeon 2.93 GHz processors with 50 GB RAM and 12 MB cache. Each compute node was also equipped with a NVIDIA M2050 GPU. In comparison with the famous GPU-BLAST, our BLAST implementation is 2.1 times faster on single compute node. On a cluster of 12 compute nodes, our implementation gave a speedup of 13.2X. In comparison with standard single-threaded NCBI-BLAST, our implementation achieves a speedup ranging from 7.4X to 8.2X.	blast;bioinformatics;bioinformatics;cuda;central processing unit;compiler;computer multitasking;graphics processing unit;hit-testing;parallel computing;random-access memory;scalability;scheduling (computing);sequence alignment;smith–waterman algorithm;speedup;texture memory;thread (computing)	Sita Rani;O. P. Gupta	2017	The Journal of Supercomputing	10.1007/s11227-017-2036-4	parallel computing;xeon;computer science;protein sequencing;speedup;cache;ranging;sequence alignment;basic local alignment search tool;supercomputer	HPC	-1.2732346460563368	42.32082815400679	170886
c7166bbe612055ff74ac69a410b0fcc21f1f9fd2	a shuffle-exchange network with simplified control	computers;dynamic memories;shuffle exchange network;permutation networks;probability density function;data mining;interconnection network;arrays;program processors data mining computers probability density function arrays digital systems computer science;boolean operation;array computers;digital systems;shuffle exchange network array computers dynamic memories parallel processing permutation networks;computer science;program processors;parallel processing	In this paper, a control mechanism for a shuffle-exchange interconnection network of N cells is proposed. With this network it is possible to realize some important permutations in log2 N shuffle-exchange steps. In the control mechanism presented, the control variables at step k are determined by a Boolean operation of the control variables at step k ¿1. The Boolean operation is very simple so that little additional hardware is required for this computation. This control scheme requires only one bit per cell instead of a destination tag of log2 N bits required by a control mechanism presented previously. The network can be used for the interconnection of memory modules and processors in an array computer, and for the accessing of blocks of consecutive data in large dynamic memories. It is also shown that the shuffle-exchange interconnection network permits the efficient partitioning of an array computer into subarrays to allow for the simultaneous computation of several identical problems.	binary logarithm;central processing unit;computation;control variable (programming);dimm;interconnection	Tomás Lang;Harold S. Stone	1976	IEEE Transactions on Computers	10.1109/TC.1976.5009205	embedded system;parallel processing;probability density function;parallel computing;computer science;theoretical computer science;operating system;distributed computing;statistics	Arch	9.610926041832034	36.14415164669527	170913
18f673ff63da054fcdff7411ba3ab5fd1b747147	strategies of parallelizing nested loops on the multicore architectures on the example of the wz factorization for the dense matrices	macierze;linear systems;openmp linear system wz factorization matrix factorization matrix computations multicore architecture parallel nested loops;parallel architectures matrix decomposition multiprocessing systems;dense matrix factorization parallel nested loop strategy multicore architectures wz factorization;algorytmy;matrix decomposition;heuristic algorithms;parallel processing matrix decomposition heuristic algorithms instruction sets multicore processing linear systems;multicore processing;algorithms;matrix;parallel processing;instruction sets	In the WZ factorization the outermost parallel loop decreases the number of iterations executed at each step and this changes the amount of parallelism in each step. The aim of the paper is to present four strategies of parallelizing nested loops on multicore architectures on the example of the WZ factorization.	data parallelism;iteration;multi-core processor;parallel computing;sparse matrix;winzip	Beata Bylina;Jaroslaw Bylina	2015	2015 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2015F354	multi-core processor;parallel processing;parallel computing;computer science;theoretical computer science;operating system;instruction set;distributed computing;linear system;matrix decomposition;matrix	HPC	-3.275979976043019	41.1843466154971	171120
f1a91f2bfdc43e8eaa4de61ca00e832693f42f2a	a compiler for cyber-physical digital microfluidic biochips		Programmable microfluidic laboratories-on-a-chip (LoCs) offer the benefits of automation and miniaturization to the life sciences. This paper presents an updated version of the BioCoder language and a fully static (offline) compiler that can target an emerging class of LoCs called Digital Microfluidic Biochips (DMFBs), which manipulate discrete droplets of liquid on a 2D electrode grid. The BioCoder language and runtime execution engine leverage advances in sensor integration to enable specification, compilation, and execution of assays (bio-chemical procedures) that feature online decision-making based on sensory data acquired during assay execution. The compiler features a novel hybrid intermediate representation (IR) that interleaves fluidic operations with computations performed on sensor data. The IR extends the traditional notions of liveness and interference to fluidic variables and operations, as needed to target the DMFB, which itself can be viewed as a spatially reconfigurable array. The code generator converts the IR into the following: (1) a set of electrode activation sequences for each basic block in the control flow graph (CFG); (2) a set of computations performed on sensor data, which dynamically determine the result of each control flow operation; and (3) a set of electrode activation sequences for each control flow transfer operation (CFG edge). The compiler is validated using a software simulator which produces animated videos of realistic bioassay execution on a DMFB.	basic block;british informatics olympiad;code generation (compiler);compiler;computation;control flow graph;interference (communication);intermediate representation;liveness;online and offline;simulation	Christopher Curtis;Daniel T. Grissom;Philip Brisk	2018		10.1145/3168826	automation;code generation;compiler;parallel computing;control flow;software;computer hardware;control flow graph;biochip;basic block;computer science	PL	6.82217616725785	37.718696860585275	171683
f9480dc02913b140f61439df2e183b04a9ba24f3	hardware accelerator for generating primitive polynomials over gf(3)	trinomial;stream cipher irreducible polynomial primitive polynomial trinomial linear feedback shift register;encryption;linear feedback shift register;hardware accelerator;primitive polynomial;polynomials;stream cipher;cryptography;polynomials cryptography;irreducible polynomial;polynomials hardware telecommunications encryption information theory;cryptographic devices hardware accelerator generating primitive polynomial gf 3 cryptographic power;information theory;telecommunications;hardware	The paper presents a hardware accelerator that could be used to generate irreducible primitive polynomials with coefficients over GF(3). The process of generating a primitive polynomial is done by replicating the other primitive polynomial which is fixed in the device. The implemented algorithm allows the unit to generate all possible primitive polynomials of the same degree as the stored polynomial. This approach allows us to extend the cryptographic power and capabilities of the existing cryptographic devices.	algorithm;coefficient;cryptography;hardware acceleration;irreducibility;primitive polynomial (field theory)	Grzegorz Borowik;Andrzej Paszkiewicz	2011	2011 21st International Conference on Systems Engineering	10.1109/ICSEng.2011.99	irreducible polynomial;arithmetic;discrete mathematics;information theory;computer science;cryptography;theoretical computer science;primitive polynomial;mathematics;stream cipher;factorization of polynomials;linear feedback shift register;trinomial;encryption;primitive element;statistics	EDA	9.947657492926423	43.26826519260843	172009
23f3717c5f84a597c49c96e3403681c0a84a6bec	selective encryption of the mc ezbc bitstream for drm scenarios	drm;universal multimedia access;selective encrpytion;in network adaption;scalability;security;selective encryption;wavelet	Universal Multimedia Access (UMA) calls for solutions where content is created once and subsequently adapted to given requirements. With regard to UMA and scalability, which is required often due to a wide variety of end clients, the best suited codecs are wavelet based (like the MC-EZBC) due to their inherent high number of scaling options. However, we do not only want to adapt the content to given requirements but we want to do so in a secure way. Through DRM we can ensure that the actual content is safe and copyright is observed. However, traditional encryption removes the option of scalability in the encrypted domain which is opposed to what we want to achieve for UMA. The solution is selective encryption where only a part of the content is encrypted, enough to ensure safety but at the same time little enough to keep scalability intact. Towards this goal we discuss various methods of applying encryption to the bitstream produced by the MC-EZBC in order to keep scalability intact in the encrypted domain while also keeping security intact with regard to various DRM scenarios.	bitstream;codec;digital rights management;encryption;image scaling;requirement;scalability;uniform memory access;user-managed access;wavelet	Heinz Hofbauer;Andreas Uhl	2009		10.1145/1597817.1597846	computer science;on-the-fly encryption;internet privacy;world wide web;computer security	Security	8.186007746380652	36.65229161487543	172041
13735c8813b54aa4e2d1a4bb5f542831f8baa1b6	exploiting user behaviour for context-aware power management	belief networks;bayesian network;context aware;context information;energy management energy consumption pervasive computing power system management computer network management costs pressing bayesian methods hard disks battery management systems;pervasive computing;context aware power management user;electricity consumption;energy consumption;user behaviour;power management;ubiquitous computing;location awareness;acoustic data user behaviour context aware power management pervasive computing location aware power management policies bayesian networks multimodal sensor data;poster;belief networks ubiquitous computing telecommunication network management;telecommunication network management	With more and more computing devices being deployed in buildings there has been a steady rise in buildings' electricity consumption. At the same time there is a pressing need to reduce overall building energy consumption. Pervasive computing could further exacerbate this problem but it could also provide a solution. Context information (e.g., user location) likely to be available in pervasive computing environments could enable highly effective device power management. The objective of such context-aware power management (CAPM) is to minimise the overall electricity consumption of a building while maintaining acceptable user-perceived device performance. To investigate the potential of CAPM we conducted experimental trials for two simple location-aware power management policies. Our results highlight the presence of two distinct user behaviour patterns but also show that location alone is not enough for effective power management. We therefore propose a CAPM framework that employs Bayesian networks to support prediction of user behaviour patterns from multi-modal sensor data for effective power management. We further propose the use of acoustic data as an interesting context for predicting finer-grained user behaviour. The paper presents an initial evaluation of the resulting framework.	acoustic cryptanalysis;bayesian network;capital asset pricing model;central processing unit;location awareness;modal logic;power management;ubiquitous computing	Colin Harris;Vinny Cahill	2005	WiMob'2005), IEEE International Conference on Wireless And Mobile Computing, Networking And Communications, 2005.	10.1109/WIMOB.2005.1512959	real-time computing;simulation;poster;computer science;bayesian network;computer security;ubiquitous computing;computer network	Mobile	1.294975328316492	34.294666127066826	172101
5f0866f3df99fd62fa70cfdb481b760198d0f51a	rapid techniques for performance estimation of processors	online resources;information resources;scholarly research;information sources;academic research;online databases;education resources;publishing;research databases;australasian research information;south east asian information;information databases;full content;education databases;australian databases;commissioning;electronic publisher;online;e titles;library resources	Current techniques for processor performance evaluation, which rely on using instruction set simulators to estimate the performance for each of the processors, may not be feasible due to the large amount of time taken to run the simulations. Moreover the simulators have to be modified to incorporate the new processor features that get implemented. In this paper we present efficient techniques to estimate processor performance by examining the intermediate level code and approximating the machine level code from it. These include a way to estimate library functions and a technique to estimate the IPC (instructions per cycle) for an application running on a processor. Unlike simulator based approaches our technique does not require the building of a compiler tool chain for the processors. Results in terms of the IPC and the number of instructions executed are also provided in order to fully validate the effectiveness of the proposed techniques. ACM Classification: B.8.2	acm computing classification system;benchmark (computing);central processing unit;compiler;computational mathematics;embedded system;high-level programming language;instruction set simulator;instructions per cycle;inter-process communication;llvm;machine code;network interface device;parallel computing;performance evaluation;real-time transcription;reconfigurable computing;simulation;system on a chip;toolchain	Abhijit Ray;Thambipillai Srikanthan;Wu Jigang	2010	Journal of Research and Practice in Information Technology		project commissioning;computer science;engineering;artificial intelligence;data science;theoretical computer science;operating system;software engineering;data mining;database;publishing;programming language;law;research	HPC	-2.563157445702035	45.64258386267469	172108
d1f85183db4ae7e19616272b41cb5b251f07be92	a performance model for integrated layer processing	ilp;integrated layer processing;performance modeling;protocol implementation techniques;performance model;synthetic data;communication protocol;memory bandwidth	Integrated Layer Processing is an implementation technique for data manipulation functions in communication protocols. The purpose of this technique is to increase communication performance. It reduces the number of memory accesses and thus relieves the memory bandwidth bottleneck. Integrated Layer Processing can however, in some situations, substantially increase the number of memory accesses, and therefore instead reduce performance. The main reason is contention for processor registers. We present a performance model that captures the memory behavior of data manipulation functions for both integrated and sequential implementations. By comparing the model to measurements of real and synthetic data manipulation functions, we show that the model accurately predicts the performance. The model can be used to assess whether an integrated implementation will perform better or worse than a sequential implementation. The situations where integration would reduce performance can then be avoided without spending a lot of effort on a more complex integrated implementation.	central processing unit;cksum;computer architecture;cryptographic hash function;encryption;memory bandwidth;potential energy surface;processor register;synthetic data	Bengt Ahlgren	1997			parallel computing;real-time computing;computer science;distributed computing	DB	-4.005251194392344	45.47014747474213	172331
53f02bda4959812530670cbe64d016d4e2004790	scaling a convolutional neural network for classification of adjective noun pairs with tensorflow on gpu clusters		Deep neural networks have gained popularity inrecent years, obtaining outstanding results in a wide range ofapplications such as computer vision in both academia andmultiple industry areas. The progress made in recent years cannotbe understood without taking into account the technologicaladvancements seen in key domains such as High PerformanceComputing, more specifically in the Graphic Processing Unit(GPU) domain. These kind of deep neural networks need massiveamounts of data to effectively train the millions of parametersthey contain, and this training can take up to days or weeksdepending on the computer hardware we are using. In thiswork, we present how the training of a deep neural networkcan be parallelized on a distributed GPU cluster. The effect ofdistributing the training process is addressed from two differentpoints of view. First, the scalability of the task and its performancein the distributed setting are analyzed. Second, the impact ofdistributed training methods on the training times and finalaccuracy of the models is studied. We used TensorFlow on top ofthe GPU cluster of servers with 2 K80 GPU cards, at BarcelonaSupercomputing Center (BSC). The results show an improvementfor both focused areas. On one hand, the experiments showpromising results in order to train a neural network faster. The training time is decreased from 106 hours to 16 hoursin our experiments. On the other hand we can observe howincreasing the numbers of GPUs in one node rises the throughput, images per second, in a near-linear way. Morever an additionaldistributed speedup of 10.3 is achieved with 16 nodes taking asbaseline the speedup of one node.	acm/ieee supercomputing conference;artificial neural network;binary symmetric channel;cns;computer cluster;computer hardware;computer vision;convolutional neural network;deep learning;elegant degradation;experiment;gpu cluster;graphics processing unit;image scaling;mac os x 10.3 panther;overhead (computing);parallel computing;ps (unix);samsung sgr-a1;scalability;severo ornstein;speedup;technical support;tensorflow;throughput	Victor L Campos;Francesc Sastre;Maurici Yagües;Jordi Torres;Xavier Giró	2017	2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)		scalability;convolutional neural network;gpu cluster;artificial neural network;speedup;deep learning;server;computer science;distributed computing;artificial intelligence;supercomputer	Arch	2.512629397488075	42.8380283427235	172334
712b41bd1dad8202707533b8d5b03056b80e9f8e	parallel algorithms for operations on multi-valued decision diagrams		Multi-valued Decision Diagrams (MDDs) have been extensively studied in the last ten years. Recently, efficient algorithms implementing operators such as reduction, union, intersection, difference, etc., have been designed. They directly deal with the graph structure of the MDD and a time reduction of several orders of magnitude in comparison to other existing algorithms have been observed. These operators have permitted a new look at MDDs, because extremely large MDDs can finally be manipulated as shown by the models used to solve complex application in music generation. However, MDDs become so large (50GB) that minutes are sometimes required to perform some operations. In order to accelerate the manipulation of MDDs, parallel algorithms are required. In this paper, we introduce such algorithms. We carefully design them in order to overcome inherent difficulties of the parallelization of sequential algorithms such as data dependencies, software lock-out, false sharing, or load balancing. As a result, we observe a speed-up , i.e. ratio between parallel and sequential runtimes, growing linearly with the number of cores.	data dependency;diagram;false sharing;linear function;model-driven engineering;parallel algorithm;parallel computing;reduction (complexity);runtime system;speedup	Guillaume Perez;Jean-Charles Régin	2018			machine learning;artificial intelligence;computer science;parallel algorithm	DB	-3.038840089711669	43.124530744468075	172448
604a5b49757b73212799bbea754cea8d0cb8d8cf	a robot swarm as a cellular multicore processor	robot sensing systems;topology;rf signals;multicore processing;batteries;universal serial bus;multicore processing topology robot sensing systems batteries rf signals universal serial bus	We present a Cellular Neural Network (CNN) model in which cells are constituted by autonomous robots implementing some standard templates. The system can be interpreted as a multi-core processor acting on the robot environment, being each robot one of the cores. This is a particular case of robot swarm which benefits from the simplicity of the CNN template implementation.	autonomous robot;cellular neural network;multi-core processor;swarm robotics	Jordi Albo-Canals;Joan Navarro;D. Serra-Puig;Xavier Vilasís-Cardona	2012	2012 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2012.6271596	multi-core processor;embedded system;electronic engineering;real-time computing;computer science;engineering;radio frequency	Embedded	5.368827099528391	42.25375205693058	172755
0dac6eac01c3d7284d3091620a557af8ba890cf3	hamming filters: a dynamic signature file organization for parallel stores	signature file	Partitioning, in general, has become the basic strategy for organizing data files to avoid an exhaustive search when executing queries. However, hardware limitations that constrain the performance of query execution mainly become a problem for partial-match queries, where the size of the result can equal the size of the data file. In such situations, a proper application of parallelism can bring the required breakthrough in performance. Hamming Filter is a parallel, part,itionetl organization of signature files that are stored in fixed size buckets with a guaranteed load and is based on the idea of linear code decomposition. It can efficiently manage dynamic data files by means of a partitioned structure that always grows and shrinks linearly and is appropriate to multidimensional partitioning and searching. This paper proves that the organization yields no expected execution skew for partial-match queries, provided the data is not skewed and the degree of parallelism is a power of two.	brute-force search;decomposition (computer science);degree of parallelism;digital signature;dynamic data;linear code;organizing (structure);parallel computing;power of two	Pavel Zezula;Paolo Ciaccia;Paolo Tiberio	1993			parallel computing;torrent file;computer file;computer science;class implementation file;operating system;database	DB	1.0801509173664885	38.53338514678261	172794
62fc8d4970945e128f0ba36957588dcda454eee5	parallel minimum norm solution of sparse block diagonal column overlapped underdetermined systems	minimum norm solution;underdetermined least square problems;balance method;parallel algorithms	Underdetermined systems of equations in which the minimum norm solution needs to be computed arise in many applications, such as geophysics, signal processing, and biomedical engineering. In this article, we introduce a new parallel algorithm for obtaining the minimum 2-norm solution of an underdetermined system of equations. The proposed algorithm is based on the Balance scheme, which was originally developed for the parallel solution of banded linear systems. The proposed scheme assumes a generalized banded form where the coefficient matrix has column overlapped block structure in which the blocks could be dense or sparse. In this article, we implement the more general sparse case. The blocks can be handled independently by any existing sequential or parallel QR factorization library. A smaller reduced system is formed and solved before obtaining the minimum norm solution of the original system in parallel. We experimentally compare and confirm the error bound of the proposed method against the QR factorization based techniques by using true single-precision arithmetic. We implement the proposed algorithm by using the message passing paradigm. We demonstrate numerical effectiveness as well as parallel scalability of the proposed algorithm on both shared and distributed memory architectures for solving various types of problems.	best, worst and average case;coefficient;computer cluster;distributed memory;experiment;linear least squares (mathematics);linear system;mathematical software;message passing;multi-core processor;numerical analysis;numerical stability;parallel algorithm;programming paradigm;qr decomposition;scalability;shared memory;signal processing;single-precision floating-point format;sparse matrix;speedup;synthetic data;synthetic intelligence;upwind scheme	F. Sukru Torun;Murat Manguoglu;Cevdet Aykanat	2017	ACM Trans. Math. Softw.	10.1145/3004280	mathematical optimization;combinatorics;theoretical computer science;mathematics;parallel algorithm;compressed sensing;algorithm	HPC	-2.174929439491052	38.72457322835635	172862
3095f3c353b1873e05b03800603fcb2e5b336d15	models of computational systems-cyclic to acyclic graph transformations	computer program;concurrent computing;computer aided instruction;sequential circuits;distributed computing;graph transformation;circuit simulation;computational modeling;utility programs;statistics;performance prediction;predictive models;model of computation;computational modeling predictive models concurrent computing circuit simulation computer aided instruction distributed computing statistics utility programs sequential circuits	This paper discusses cyclic to acyclic transformations performed on graphs representing computational sequences. Such transformations are critical to the development of models of computations and computer systems for performance prediction. The nature of cycles in computer programs for parallel processors is discussed. Transformations are then developed which replace cyclic graph structures by mean-value equivalent acyclic structures. The acyclic equivalents retain the noncyclic part of the structure in the original graph by evaluating a multiplicative factor associated with the mean time required for each vertex execution in the original graph. Bias introduced in the acyclic approximation is explored.	computation;graph rewriting	David F. Martin;Gerald Estrin	1967	IEEE Trans. Electronic Computers	10.1109/PGEC.1967.264607	model of computation;concurrent computing;computer science;theoretical computer science;machine learning;sequential logic;predictive modelling;graph;programming language;computational model;directed acyclic word graph;algorithm	Arch	1.1041217397283438	36.46427609897365	172940
4fdd1f8283cca1a53f640ec56e82125c3ae4d0b3	interactive graph cuts using fpga			cut (graph theory);field-programmable gate array	Daichi Kobori;Tsutomu Maruyama	2013		10.3233/978-1-61499-381-0-532	parallel computing;theoretical computer science;field-programmable gate array;computer science;cut	Vision	6.7236184135659425	39.84329315157843	173144
19f7eb5996291f09042b1506ca5a07a9e11074f5	efficient hardware design for computing pairings using few fpga in-built dsps		This paper is devoted to the design of a 258-bit mu ltiplier for computing pairings over Barreto-Naehri g (BN) curves at 128-bit security level. The proposed desi gn is optimized for Xilinx field programmable gate a rray (FPGA). Each 258-bit integer is represented as a po lynomial with five, 65 bit signed integer, coefficie nts. Exploiting this splitting we designed a pipelined 6 5-bit multiplier based on new KaratsubaOfman vari ant using non-standard splitting to fit to the Xilinx embedded digital signal processor (DSP) blocks. We prototyp e the coprocessor in two architectures pipelined and seri al on a Xilinx Virtex-6 FPGA using around 17000 slices and 11 DSPs in the pipelined design and 7 DSPs in the seri al. The pipelined 128-bit pairing is computed in 1. 8 ms running at 225MHz and the serial is performed in 2.2 ms run ning at 185MHz. To the best of our knowledge, this implementation outperforms all reported hardware de signs in term of DSP use. Keywords-Cryptography, Field Programmable Gate Array (FPGA), Modular Multiplication, Non-Standard Splitting, Pairing-Friendly Curves	128-bit;computation;coprocessor;cryptography;digital signal processor;embedded system;field-programmable gate array;integer factorization;null-terminated string;polynomial;scheduling (computing);serialization;signal processing;signed number representations	Riadh Brinci;Walid Khmiri;Mefteh Mbarek;Abdellatif Ben Rabaa;Ammar Bouallègue	2015	IACR Cryptology ePrint Archive			EDA	9.152997461403396	44.54978975712455	173235
150558c61b81816af2b9938c5fefdeb75d191d7d	the extended partitioning problem: hardware/software mapping and implementation-bin selection	minimisation;directed graphs;architectural constraints;iterative process;implementation bin motion;global criticality;logic partitioning directed graphs minimisation computational complexity logic cad software engineering resource allocation iterative methods;resource allocation;local optimality measures;global time criticality measure;adaptive optimization;complexity;bin sensitivity measure;design flexibility;software engineering;iterative methods;mibs heuristic algorithm;computational complexity;local phase driven algorithm;gclp algorithm;solution quality;precedence graph node mapping;logic partitioning;performance constraints;logic cad;solution quality extended partitioning problem hardware software mapping implementation bin selection precedence graph node mapping node implementation selection hardware area reduction architectural constraints performance constraints np complete problem mibs heuristic algorithm iterative process gclp algorithm global criticality local phase driven algorithm adaptive optimization objective global time criticality measure local optimality measures bin sensitivity measure implementation bin motion design flexibility complexity;extended partitioning problem;adaptive optimization objective;integer linear program;implementation bin selection;hardware area reduction;hardware partitioning algorithms iterative algorithms time measurement software algorithms np complete problem area measurement motion measurement software quality design optimization;np complete problem;node implementation selection;hardware software mapping	The extended partitioning problem is the joint problem of mapping nodes in aprecedence graph to hardware or software, and within each mapping, selecting an appropriate implementation for each node. The end-goal is to minimize the hardware area, subject to architectural and performance constraints. This is an NP-complete problem; we present an efficient heuristic called MIBS to solve it. The MIBS algorithm solves the extended partitioning problem by decomposing it into an iterative process consisting of two steps: mapping and implementation-bin selection. The GCLP algorithm computes a mapping by using an adaptive optimization objective at each iteration. This objective is selected on the basis of a global time criticality measure and local optimality measures. The IBS algorithm solves the implementation-bin selection problem. It uses a bin sensitivity measure, which correlates the implementationbin motion with the overall hardware area reduction, to determine the implementation bin o fa node for a given mapping. Experimental results indicate that the added dimension of design flexibility (offered by implementation bins) can be used effectively in partitioning to reduce the overall area. The MIBS algorithm has 0(lNl3) complexity, with a solution quality comparable to that of ILP.	adaptive optimization;heuristic;iteration;local optimum;mathematical optimization;np-completeness;partition problem;selection algorithm;self-organized criticality	Asawaree Kalavade;Edward A. Lee	1995		10.1109/IWRSP.1995.518565	adaptive optimization;minimisation;mathematical optimization;complexity;np-complete;directed graph;resource allocation;computer science;theoretical computer science;iterative and incremental development;distributed computing;iterative method;computational complexity theory	EDA	6.741823501528988	37.007896494776176	173260
5815cab7888a3ce3c0b86d49c553730de09e4936	hardware accelerated novel optical de novo assembly for large-scale genomes	genomics;kernel;graphics processing units assembly acceleration genomics instruction sets field programmable gate arrays kernel;acceleration;design space exploration hardware accelerated novel optical de novo assembly algorithm bioinformatics large scale structures human genomes optical label based technology large scale genome analysis computationally intensive alignment algorithm sequential cpu gpu fpga sequential software baseline multicore cpu design;assembly;graphics processing units;multiprocessing systems bioinformatics field programmable gate arrays genomics graphics processing units;field programmable gate arrays;instruction sets	De novo assembly is a widely used methodology in bioinformatics. However, the conventional short-read based de novo assembly is incapable of reliably reconstructing the large-scale structures of human genomes. Recently, a novel optical label based technology has enabled reliable large-scale de novo assembly. Despite its advantage in large-scale genome analysis, this new technology requires a more computationally intensive alignment algorithm than its conventional counterpart. For example, the run-time of reconstructing a human genome is on the order of 10; 000 hours on a sequential CPU. Therefore, in order to practically apply this new technology in genome research, accelerated approaches are desirable. In this paper, we present three different accelerated approaches, multi-core CPU, GPU and FPGA. Against the sequential software baseline, our multi-core CPU design achieved a 8.4× speedup while the GPU and FPGA designs achieved 13.6× and 115× speedups respectively. We also reveal the insights of the design space exploration of this new assembly algorithm on these three different devices by comparing the results.	algorithm;baseline (configuration management);bioinformatics;central processing unit;de novo transcriptome assembly;design space exploration;field-programmable gate array;graphics processing unit;multi-core processor;pipeline (software);processor design;scalability;speedup	Pingfan Meng;Matthew Jacobsen;Motoki Kimura;Vladimir Dergachev;Thomas Anantharaman;Michael Requa;Ryan Kastner	2014	2014 24th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2014.6927499	acceleration;embedded system;genomics;parallel computing;kernel;computer science;bioinformatics;theoretical computer science;operating system;instruction set;assembly;field-programmable gate array	EDA	-0.3589840857778039	43.24907286698411	173319
859f26144d3089f9ba33c143b01d2512e51e5da2	distributed prolog on a hypercube multicomputer.		The communication cost plays a key role in the performance of many parallel algorithms. In the particular case of the one-sided Jacobi method for symmetric eigenvalue and eigenvector computation the communication cost of previously proposed algorithms is mainly determined by the particular ordering being used. In this paper we proposed two novel Jacobi orderings: the permuted-BR ordering and the degree-4 ordering, aimed at efficiently exploiting the multi-port capability of a hypercube. It is shown that the former is nearly optimal for some scenarios and the latter outperforms previously known orderings by a factor of two.	computation;jacobi method;parallel algorithm;parallel computing;prolog	Sabri Büyüksoy;Mehmet Baray;Isik Aybay	1992			theoretical computer science	HPC	-1.5452061893230296	38.07647391539571	173541
3a6258a6d2890d6dc4aa6a1f3c3134579abb5126	a correctly rounded mixed-radix fused-multiply-add		The IEEE 754–2008 Standard governs Floating-Point Arithmetic in all types of Computer Systems. The Standard provides for two radices, 2 and 10. It specifies conversion operations between these radices, but does not allow floating-point formats of different radices to be mixed in computational operations. In contrast, the Standard does provide for mixing formats of one radix in one operation. In order to enhance the Standard and make it closed under all basic computational operations, we propose an algorithm for a correctly rounded mixed-radix Fused-Multiply-and-Add (FMA). Our algorithm takes any combination of IEEE754 binary64 and decimal64 numbers in argument and provides a result in IEEE754 binary64 and decimal64, rounded according to any for the five IEEE754 rounding modes. Our implementation does not require any dynamic memory allocation; its runtime can be bounded statically. We compare our implementation to a basic mixed-radix FMA implementation based on the GMP Multiple Precision library.	accumulator (computing);algorithm;apple pencil;approximation;best, worst and average case;bitwise operation;computation;correctness (computer science);decimal128 floating-point format;decimal64 floating-point format;double-precision floating-point format;flops;fma instruction set;gnu multiple precision arithmetic library;hardware description language;ieee 754-1985;input/output;instruction cycle;intellect;memory management;overhead (computing);precomputation;reference implementation;rounding;significand;single-precision floating-point format	Clothilde Jeangoudoux;Christoph Lauter	2018	2018 IEEE 25th Symposium on Computer Arithmetic (ARITH)	10.1109/ARITH.2018.8464818	radix;parallel computing;computer science;theoretical computer science;floating point;c dynamic memory allocation;mixed radix;bounded function;multiply–accumulate operation;ieee floating point;rounding	Embedded	6.723651183942862	44.23777318869496	173701
5fbebccd6fb647a6b63f2d3fa345b9a5921b6067	hybrid dynamic iterations for the solution of initial value problems	ordinary differential equation;initial value problem;hybrid method;state space;parallel computer;time domain;ode solver;time parallelization;hybrid dynamic iterations;iteration method;empirical evaluation	Many scientific problems are posed as Ordinary Differential Equations (ODEs). A large subset of these are initial value problems, which are typically solved numerically. The solution starts by using a known state-space of the ODE system to determine the state at a subsequent point in time. This process is repeated several times. When the computational demand is high due to large state space, parallel computers can be used efficiently to reduce the time to solution. Conventional parallelization strategies distribute the state space of the problem amongst cores and distribute the task of computing for a single time step amongst the cores. They are not effective when the computational problems have fine granularity, for example, when the state space is relatively small and the computational effort arises largely from the long time span of the initial value problem. We propose a hybrid dynamic iterations method which combines conventional sequential ODE solvers with dynamic iterations to parallelize the time domain. Empirical results demonstrate a factor of two to four improvement in performance of the hybrid dynamic iterations method over a conventional ODE solver on an 8 core processor. Compared to Picard iterations (also parallelized in the time domain), the proposed method shows better convergence and speedup results when high accuracy is required.	computation;computational problem;computer;iteration;microsoft windows;musicbrainz picard;numerical analysis;open dynamics engine;overhead (computing);parallel computing;solver;speedup;state space;time complexity	Yanan Yu;Ashok Srinivasan	2011	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2011.04.010	ordinary differential equation;mathematical optimization;discrete mathematics;parallel computing;time domain;computer science;state space;theoretical computer science;mathematics;iterative method;initial value problem	HPC	-4.435632182549731	38.92336322871703	173974
08698398d251d1d15a608f4207a7e630549bb7c5	energy-efficient motion related activity recognition on mobile devices for pervasive healthcare	healthcare;energy efficient;mobile devices;tri axial accelerometer;activity recognition	Activity recognition plays an important role for pervasive healthcare such as health monitoring, assisted living and pro-active services. Despite of the continuous and transparent sensing with various built-in sensors in mobile devices, activity recognition on mobile devices for pervasive healthcare is still a challenge due to the constraint of resources, such as battery limitation, computation workload, etc. Keeping in view the demand of energy-efficient activity recognition, we propose a hierarchical method to recognize user activities based on a single tri-axial accelerometer in smart phones for health monitoring. Specifically, the contribution of this paper is two-fold. First, it is demonstrated that the activity recognition based on the low sampling frequency is feasible for the long-term activity monitoring. Second, this paper presents a hierarchical recognition scheme. The proposed algorithm reduces the opportunity of usage of time-consuming frequency-domain features and adjusts the size of sliding window to improve recognition accuracy. Experimental results demonstrate the effectiveness of the proposed algorithm, with more than 85 % recognition accuracy rate for 11 activities and 3.2 h extended battery life for mobile phones. Our energy efficient recognition algorithm extends the battery time for activity recognition on mobile devices and contributes to the health monitoring for pervasive healthcare.	activity recognition	Yunji Liang;Xingshe Zhou;Zhiwen Yu;Bin Guo	2014	MONET	10.1007/s11036-013-0448-9	embedded system;simulation;computer science;operating system;mobile device;efficient energy use;computer security;activity recognition	HCI	2.303037052116165	34.52840521373182	173985
b776aa89fafa7b58afc00510f3030ce46c4eb8b5	recursive least-squares using a hybrid householder algorithm on massively parallel simd systems	recursive least square;processing element;least squares;householder transformations;least square;performance model;parallel computer;simd parallelism;timing models;qr decomposition	Within the context of recursive least-squares, the implementation of a Householder algorithm for block updating the QR decomposition, on massively parallel SIMD systems, is considered. Initially, two implementations based on dierent mapping strategies for distributing the data matrices over the processing elements of the parallel computer are investigated. Timing models show that neither of these implementations is superior in all cases. In order to increase computational speed, a hybrid implementation uses performance models to partition the problem into two subproblems which are then solved using the ®rst and second implementation, respectively. Ó 1999 Elsevier Science B.V. All rights reserved.	algorithm;householder transformation;parallel computing;qr decomposition;recursion (computer science);recursive least squares filter;simd	Erricos John Kontoghiorghes;Maurice Clint;Hans-Heinrich Naegeli	1999	Parallel Computing	10.1016/S0167-8191(99)00043-5	computer architecture;parallel computing;computer science;theoretical computer science;least squares;qr decomposition	HPC	-1.706188434425446	36.75040263825398	174026
2f7184619298cd5f5dbf4f2ed877551a5c4b0846	communication optimization of iterative sparse matrix-vector multiply on gpus and fpgas	constrained optimization;structured;modeling of computer architecture;kernel;graphics processing units field programmable gate arrays instruction sets kernel sparse matrices vectors registers;performance of systems;sparse;and very large systems;journal article;matrix multiplication field programmable gate arrays graphics processing units iterative methods mathematics computing;parallel architectures;vectors;spare matrix vector multiply;registers;graphics processing units;numerical algorithms;processing core communication communication optimization iterative sparse matrix vector multiplication graphics processing unit fpga field programmable gate array communication bound sparse iterative solvers communication cost reduction algorithmic parameter communication computation tradeoff hardware accelerators nvidia c2050 gpu architecture aware algorithm off chip communication;iterative numerical methods;field programmable gate arrays;graphics processing units gpus iterative numerical methods spare matrix vector multiply matrix powers kernel field programmable gate arrays fpgas;graphics processing units gpus;iterative solution techniques;matrix powers kernel;processor architectures;sparse matrices;reconfigurable hardware;field programmable gate arrays fpgas;instruction sets;computer systems organization;parallel algorithms	Trading communication with redundant computation can increase the silicon efficiency of FPGAs and GPUs in accelerating communication-bound sparse iterative solvers. While k iterations of the iterative solver can be unrolled to provide O(k) reduction in communication cost, the extent of this unrolling depends on the underlying architecture, its memory model, and the growth in redundant computation. This paper presents a systematic procedure to select this algorithmic parameter k, which provides communication-computation tradeoff on hardware accelerators like FPGA and GPU. We provide predictive models to understand this tradeoff and show how careful selection of k can lead to performance improvement that otherwise demands significant increase in memory bandwidth. On an Nvidia C2050 GPU, we demonstrate a 1.9×-42.6× speedup over standard iterative solvers for a range of benchmarks and that this speedup is limited by the growth in redundant computation. In contrast, for FPGAs, we present an architecture-aware algorithm that limits off-chip communication but allows communication between the processing cores. This reduces redundant computation and allows large k and hence higher speedups. Our approach for FPGA provides a 0.3×-4.4× speedup over same-generation GPU devices where k is picked carefully for both architectures for a range of benchmarks.	algorithm;benchmark (computing);computation;field-programmable gate array;graphics processing unit;hardware acceleration;iteration;iterative method;memory bandwidth;optimization problem;predictive modelling;procedural generation;redundancy (engineering);shadow volume;solver;sparse matrix;speedup;the matrix	Abid Rafique;George A. Constantinides;Nachiket Kapre	2015	IEEE Transactions on Parallel and Distributed Systems	10.1109/TPDS.2014.6	constrained optimization;computer architecture;parallel computing;kernel;sparse matrix;reconfigurable computing;computer science;theoretical computer science;operating system;instruction set;parallel algorithm;processor register;field-programmable gate array	HPC	-1.684444939285937	40.28963666783016	174196
2f7e194b5fa8b812b1a49a6cb2c28833591e896b	automatic tuning on many-core platform for energy efficiency via support vector machine enhanced differential evolution		The modern era of computing involves increasing the core count of the processor, which in turn increases the energy usage of the processor. How to identify the most energy-efficient way of running a multiple-program workload on a many-core processor while still maintaining a satisfactory performance level is always a challenge. Automatic tuning on the voltage and frequency level of a many-core processor is an effective method to aid solving this dilemma. The metrics we focus on optimizing are energy usage and energy-delay product (EDP). To this end, we propose SVM-JADE, a machine learning enhanced version of an adaptive differential evolution algorithm (JADE). We monitor the energy and EDP values of different voltage and frequency combinations of the cores, or power islands, as the algorithm evolves through generations. By adding a well-tuned support vector machine (SVM) to JADE, creating SVM-JADE, we are able to achieve energy-aware computing on many-core platform when running multiple-program workloads. Our experimental results show that our algorithm can further improve the energy by 8.3% and further improve EDP by 7.7% than JADE. Besides, in both EDP-based and energy-based fitness SVM-JADE converges faster than JADE. Parallel tree skeletons are basic computational patterns that can be used to develop parallel programs for manipulating trees. In this paper, we propose an efficient implementation of parallel tree skeletons on distributed-memory parallel computers. In our implementation, we divide a binary tree to segments based on the idea of m-bridges with high locality, and represent local segments as serialized arrays for high sequential performance. We furthermore develop a cost model for our implementation of parallel tree skeletons. We confirm the efficacy of our implementation with several experiments.	algorithm;automatic control;brute-force search;differential evolution;dynamic voltage scaling;effective method;electronic data processing;feedback;international conference on services computing;jade;machine learning;manycore processor;overhead (computing);support vector machine;test set	Zhiliu Yang;Zachary I. Rauen;Chen Liu	2017	Scalable Computing: Practice and Experience		binary tree;support vector machine;workload;differential evolution;parallel computing;efficient energy use;real-time computing;effective method;computer science	HPC	-1.7068916753569752	43.56630101617238	174257
1a573844db49b28604b7989210e3e9a5b69b039f	heterogeneous memory subsystem for natural graph analytics		As graph applications become more popular and diverse, it is important to design efficient hardware architectures that maintain the flexibility of high-level graph programming frameworks. Prior works have identified the memory subsystem of traditional chip multiprocessors (CMPs) as a key source of inefficiency; however, these solutions do not exploit locality that exists in the structure of many real-world graphs. In this work, we target graphs that follow a power-law distribution, for which there is a unique opportunity to significantly boost the overall performance of the memory subsystem. We note that many natural graphs, derived from web, social networks, even biological networks, follow the power law, that is, 20% of the vertices are linked to 80% of the edges. Based on this observation, we propose a novel memory subsystem architecture that leverages this structural graph locality. Our architecture is based on a heterogeneous cache/scratchpad memory subsystem and a lightweight compute engine, to which the cores can offload atomic graph operations. Our architecture provides 2x speedup, on average, over a same-sized baseline CMP running the Ligra framework, while requiring no modification to the Ligra programming interface.		Abraham Addisie;Hiwot Kassa;Opeoluwa Matthews;Valeria Bertacco	2018	2018 IEEE International Symposium on Workload Characterization (IISWC)	10.1109/IISWC.2018.8573480	parallel computing;memory architecture;architecture;speedup;scratchpad memory;cache;graph operations;computer science;data structure;analytics	Arch	-3.533023750004341	43.401440124906564	174270
a3a419c57dd73c5ff8aece4d46210e5b42adf0cc	on the design and implementation of a dependable space instrument	distributed system;digital wave processor;fault tolerant;implementation;parallel processing fault tolerant computing aerospace computing special purpose computers;dwp;special purpose computers;esa nasa cluster satellites;dependable systems;fault tolerant computing;instruments space vehicles testing telecommunication control data handling nasa satellites fault tolerance hardware earth;aerospace computing;design and implementation;dependable space instrument;fault tolerant behaviour;design;dependable distributed system;fault tolerant behaviour design implementation dependable space instrument digital wave processor dwp dependable distributed system esa nasa cluster satellites;parallel processing	The Digital Wave Processor, DWP [l-101, is an example of a dependable distributed system which has been built and will be flown on the ESA/NASA Cluster satellites. Pre-flight testing of the DWP instruments is nearly complete and the results of this programme of testing are described. Much of the fault-tolerant behaviour of DWP has been tested and found to function as expected. Of the problems which have been found the majority are attributed to the initial definition of hardware and software interfaces. Various small hardware limitations and problems have been experienced. These have generally been overcome without there being any impact on the fault-tolerant design, but they have consumed a considerable w u n t of engineering effort.	distributed computing;esa;fault tolerance	Les J. C. Woolliscroft;K. H. Yearby;H. St. C. Alleyne;Jon A. Thompson;C. M. Dunford	1995		10.1109/EMPDP.1995.389186	embedded system;real-time computing;engineering;computer engineering	OS	3.3491423163767564	45.548661007797214	174272
ce284b1ba75514352e9efe9c411c1af1118bc692	a 21.54 gbits/s fully pipelined aes processor on fpga	pipeline processing cryptography field programmable gate arrays random access storage;21 54 gbit s fully pipelined processor advanced encryption standard processor loop unrolling inner round pipelining techniques outer round pipelining techniques substitution phase virtexii pro fpga random access storage;cosic;logic;field programmable gate arrays pipeline processing throughput delay cryptography application specific integrated circuits table lookup hardware arithmetic logic;virtexii pro fpga;chip;fully pipelined processor;fpga implementation;loop unrolling;application specific integrated circuits;advanced encryption standard processor;cryptography;substitution phase;arithmetic;random access storage;inner round pipelining techniques;21 54 gbit s;field programmable gate arrays;table lookup;outer round pipelining techniques;pipeline processing;throughput;hardware	This paper presents the architecture of a fully pipelined AES encryption processor on a single chip FPGA. By using loop unrolling and inner-round and outer-round pipelining techniques, a maximum throughput of 21.54 Gbits/s is achieved. A fast and an area efficient composite field implementation of the byte substitution phase is designed using an optimum number of pipeline stages for FPGA implementation. A 21.54 Gbits/s throughput is achieved using 84 block RAMs and 5177 slices of a VirtexII-Pro FPGA with a latency of 31 cycles and throughput per area rate of 4.2 Mbps/Slice.	byte;data rate units;encryption;field-programmable gate array;gigabit;loop unrolling;maximum throughput scheduling;pipeline (computing)	Alireza Hodjat;Ingrid Verbauwhede	2004	12th Annual IEEE Symposium on Field-Programmable Custom Computing Machines	10.1109/FCCM.2004.1	chip;embedded system;throughput;computer architecture;parallel computing;real-time computing;computer science;cryptography;loop unrolling;logic	Arch	9.77706073432043	45.166190491314794	174347
0f97be873e2b3166e4fbda0a2bc05272925009e7	a practical low-power memristor-based analog neural branch predictor	branch prediction;design;performance analysis and design aids;experimentation;measurement;electronics;neural branch predictor;memristor;performance;low power electronics;memristors	Recently, the discovery of memristor brought the promise of high density, low energy, and combined memory/arithmetic capability into computing. This paper demonstrates a practical neural branch predictor based on memristor. By using analog computation techniques, as well as exploiting the accuracy tolerance of branch prediction, our design is able to efficiently realize a neural prediction algorithm. Compared to the digital counterpart, our method achieves significant energy reduction while maintaining a better prediction accuracy and a higher IPC. Our approach also reduces the resource and energy required by an alternative design.	algorithm;analog computer;branch predictor;computation;kerrison predictor;low-power broadcasting;memristor	Jianxing Wang;Yenni Tim;Weng-Fai Wong;Hai Li	2013	International Symposium on Low Power Electronics and Design (ISLPED)		embedded system;electronic engineering;parallel computing;memristor;computer science;memistor;operating system	Arch	4.947884831457302	41.76113617087479	174478
7393edc7c90ae4a77d650b50624bd52c2f4dcec0	scalable structured data access by combining autonomous memory blocks	parallel architectures field programmable gate arrays logic design reconfigurable architectures;logic design;reconfigurable architectures;performance estimation;2d filtering scalable structured data access autonomous memory blocks signal processing image processing fpga libraries field programmable gate arrays reconfigurable hardware devices logic implementations scalable architectures address generation logic parallel data access;parallel architectures;field programmable gate arrays digital signal processing libraries educational institutions hardware logic devices buffer storage data engineering image processing parallel processing;data access;signal and image processing;hardware design;field programmable gate arrays;reconfigurable hardware;structured data	Many hardware designs, especially those for signal and image processing, involve structured data access such as queues, stacks and stripes. This work presents parametric descriptions as abstractions for such structured data access, and explains how these abstractions can be supported either as FPGA libraries targeting existing reconfigurable hardware devices, or as dedicated logic implementations forming autonomous memory blocks (AMBs). Scalable architectures combining the address generation logic in AMBs together to provide larger storage with parallel data access, are also examined. The effectiveness of this approach is illustrated with size and performance estimates for our FPGA libraries and dedicated logic implementations of AMBs. It is shown that for two-dimensional filtering, the dedicated AMBs can be 7 times smaller and 5 times faster than the FPGA libraries performing the same function.	autonomous robot;data access;data model;field-programmable gate array;image processing;library (computing);queue (abstract data type);stack (abstract data type);stripes	Wim J. C. Melis;Peter Y. K. Cheung;Wayne Luk	2004	Proceedings. 2004 IEEE International Conference on Field- Programmable Technology (IEEE Cat. No.04EX921)	10.1109/FPT.2004.1393324	data access;embedded system;computer architecture;parallel computing;logic synthesis;image processing;reconfigurable computing;data model;programmable logic array;computer science;theoretical computer science;programmable logic device;field-programmable gate array	HPC	4.763742165423957	46.22716504718792	174618
e389a491f5aab163971cba613acdae152a0cb0ad	high performance implementation of the inverse tft	fast fourier transform;parallel algorithms	The inverse truncated Fourier transform (ITFT) is a key component in the fast polynomial and large integer algorithms introduced by van der Hoeven. This paper reports a high performance implementation of the ITFT which poses additional challenges compared to that of the forward transform. A general-radix variant of the ITFT algorithm is developed to allow the implementation to automatically adapt to the memory hierarchy. Then a parallel ITFT algorithm is developed that trades off small arithmetic cost for full vectorization and improved multi-threaded parallelism. The algorithms are automatically generated and tuned to produce an arbitrary-size ITFT library. The new algorithms and the implementation smooths out the staircase performance associated with power-of-two modular FFT implementations, and provide significant performance improvement over zero-padding approaches even when high-performance FFT libraries are used.	algorithm;automatic vectorization;fast fourier transform;library (computing);memory hierarchy;parallel computing;polynomial;power of two;thin-film-transistor liquid-crystal display;thread (computing)	Lingchuan Meng;Jeremy R. Johnson	2015		10.1145/2790282.2790292	fast fourier transform;mathematical optimization;parallel computing;computer science;theoretical computer science;parallel algorithm	HPC	-1.9590561090474232	39.57690419012714	174994
10e4d741b53d92296c8c27f6931e17a1a2245913	several aes variants under vhdl language in fpga		This paper provides four different architectures for encrypting and decrypting 128 bit information via the AES. The encryption algorithm includes the Key Expansion module which generates Key for all iterations on the fly, Double AEStwo-key triple AES, AESX and AES-EXE. These architectures are implemented and studied in Altera Cyclone III and STRATIX Family devices.	128-bit;aes instruction set;algorithm;altera hardware description language;cryptography;cyclone;encryption;field-programmable gate array;iteration;on the fly;stratix;vhdl	Sliman Arrag;Abdellatif Hamdoun;Abderrahim Tragha;Salah eddine Khamlich	2012	CoRR		embedded system;parallel computing;computer science;theoretical computer science	Crypto	8.984308574565455	44.86882200134939	175018
4455d406b65170d1c4ceeb9b883ac3b343172393	rank-based genetic algorithm with limited iteration for grid scheduling	grid scheduling;concurrent computing;high performance computing;processor scheduling;genetic algorithms grid computing processor scheduling distributed computing large scale systems computer networks concurrent computing high performance computing biological cells np complete problem;simulation;distributed computing;np complete problems;genetics;makespan genetic algorithms rank grid scheduling;scheduling computational complexity genetic algorithms grid computing;computer networks;optimization problem;biological cells;makespan;standard genetic algorithm;computational complexity;scheduling;scheduling problem;grid scheduling grid computing np complete problems standard genetic algorithm rank based roulette wheel selection genetic algorithm;genetic algorithm;genetic algorithms;rank based roulette wheel selection genetic algorithm;grid computing;rank;np complete problem;large scale systems;gallium	In Grid Computing the number of resources and tasks is usually very large, which makes the scheduling task very complex optimization problem. Genetic algorithms (GAs) have been broadly used to solve these NP-complete problems efficiently. On the other hand, the Standard Genetic algorithm (SGA) is too slow when used in a realistic scheduling due to its time-consuming iteration. This paper proposes a new Rank-based Roulette Wheel Selection Genetic Algorithm (RRWSGA) for scheduling independent tasks in the grid environment, which increases the performance and the quality of schedule with a limited number of iterations, RRWSGA improves the reliability in the selection process while matching an acceptable output. A fast reduction of makespan making the RRWSGA of practical concern for grid environment. The results are encouraging, and can be used for real-world scheduling problems.	fitness proportionate selection;genetic algorithm;grid computing;iteration;makespan;mathematical optimization;np-completeness;optimization problem;scheduling (computing);synthetic genetic array	Wael Abdulal;Omar Al Jadaan;Ahmad Jabas;Ramachandram Sirandas;Mustafa Kaiiali;C. Raghavendra Rao	2009	2009 First International Conference on Computational Intelligence, Communication Systems and Networks	10.1109/CICSYN.2009.23	fair-share scheduling;fixed-priority pre-emptive scheduling;job shop scheduling;mathematical optimization;genetic algorithm;np-complete;flow shop scheduling;dynamic priority scheduling;computer science;rate-monotonic scheduling;theoretical computer science;genetic algorithm scheduling;two-level scheduling;distributed computing;lottery scheduling;round-robin scheduling;algorithm	HPC	-1.4260968122038928	33.61777626548739	175057
c643e7cf69ce99a7cc17b816469e72f71d9c041c	multidimensional dynamic programming for homology search	dynamic programming;reconfigurable architectures dynamic programming field programmable gate arrays;homology search;reconfigurable architectures;dynamic program;three dimensional;high parallelism compact system off the shelf fpga board host computer 3d alignment configuring optimal circuit dimensional alignment reconfiguration multidimensional search 2d dynamic programming;field programmable gate arrays;computational biology;high performance;off the shelf;multidimensional systems dynamic programming sequences computational biology databases biology computing field programmable gate arrays circuits computer architecture parallel processing	Alignment problems in computational biology have been focused recently because of the rapid growth of sequence databases. By computing alignment, we can understand similarity among the sequences. Many systems for alignment have been proposed to date, but most of them are designed for two-dimensional alignment (alignment between two sequences). In this paper, we describe a compact system with an off-the-shelf FPGA board and a host computer for more than three-dimensional alignment based on dynamic programming. In our approach, high performance is achieved (1) by configuring optimal circuit for each dimensional alignment, and (2) by two phase search in each dimension by reconfiguration. In order to realize multidimensional search with a common architecture, two-dimensional dynamic programming is repeated along other dimensions. With this approach, we can minimize the size of units for alignment and achieve high parallelism. Our system with one XC2V6000 enables about 300-fold speedup as compared with single Intel Pentium 4 2GHz processor for four-dimensional alignment, and 100-fold speedup for five-dimensional alignment.	computational biology;data structure alignment;dynamic programming;homology (biology);host (network);parallel computing;pentium 4;sequence database;speedup	Shingo Masuno;Tsutomu Maruyama;Yoshiki Yamaguchi;Akihiko Konagaya	2005	International Conference on Field Programmable Logic and Applications, 2005.	10.1109/FPL.2005.1515718	embedded system;three-dimensional space;computer architecture;real-time computing;computer science;theoretical computer science;dynamic programming;field-programmable gate array	HPC	0.4649963330596859	42.7202402683239	175654
6489bb5d7716bb9ef6c61a64609385fdd231dced	a dwarf-based scalable big data benchmarking methodology		Different from the traditional benchmarking methodology that creates a new benchmark or proxy for every possible workload, this paper presents a scalable big data benchmarking methodology. Among a wide variety of big data analytics workloads, we identify eight big data dwarfs, each of which captures the common requirements of each class of unit of computation while being reasonably divorced from individual implementations. We implement the eight dwarfs on different software stacks, e.g., OpenMP, MPI, Hadoop as the dwarf components. For the purpose of architecture simulation, we construct and tune big data proxy benchmarks using the directed acyclic graph (DAG)-like combinations of the dwarf components with different weights to mimic the benchmarks in BigDataBench. Our proxy benchmarks preserve the micro-architecture, memory, and I/O characteristics, and they shorten the simulation time by 100s times while maintain the average micro-architectural data accuracy above 90 percentage on both X86 64 and ARMv8 processors. We will opensource the big data dwarf components and proxy benchmarks soon.		Wanling Gao;Lei Wang;Jianfeng Zhan;Chunjie Luo;Daoyi Zheng;Zhen Jia;Biwei Xie;Chen Zheng;Qiang Yang;Haibin Wang	2017	CoRR			Arch	-4.07941180258037	45.24124445793796	175954
8c3e170c7fdf77b120de9320ba2e8354f2513dc5	analyzing the hardware costs of different security-layer variants for a low-cost rfid tag		Radio-frequency identification (RFID) technology is the enabler for the future Internet of Things (IoT) where security will play an important role. In this work, we evaluate the costs of adding different security-layer variants that are based on symmetric cryptography to a low-cost RFID tag. In contrast to related work, we do not only consider the costs of the cryptographic-algorithm implementation, but also the costs that relate to protocol handling of the security layer. Further we show that using a tag architecture based on a low-resource 8-bit microcontroller is highly advantageous. Such an approach is not only flexibility but also allows combining the implementation of protocol and cryptographic algorithm on the microcontroller. Expensive resources like memory can be easily reused, lowering the overall hardware costs. We have synthesized the security-enabled tag for a 130 nm CMOS technology, using the cryptographic algorithms AES and NOEKEON to demonstrate the effectiveness of our approach. Average power consumption of the microcontroller is 2 μ W at a clock frequency of 106 kHz. Hardware costs of the security-layer variants range from about 1100 GEs using NOEKEON to 4500 GEs using AES.	radio-frequency identification	Thomas Plos;Martin Feldhofer	2011		10.1007/978-3-642-31909-9_24	microcontroller;computer hardware;clock rate;cryptography;noekeon;architecture;symmetric-key algorithm;internet of things;cmos;computer science	HCI	8.203828124673283	45.33126287429418	175965
9d81619749599fd9be8928a8c48ffcae760ffcdd	survey of parallel matlab techniques and applications to signal and image processing	distributed algorithms;computer languages;concurrent computing;image processing;high performance computing;explicit code;image processing distributed algorithms distributed computing array signal processing;matlab signal processing image processing computer languages productivity licenses high performance computing distributed computing signal processing algorithms concurrent computing;communication complexity;distributed computing;array signal processing;programming interfaces;image processing communication complexity;computational complexity;signal processing;licenses;programming interfaces parallel matlab techniques signal processing image processing sip applications explicit code inter processor communication communication complexity computational complexity;sip applications;signal and image processing;productivity;parallel matlab techniques;signal processing algorithms;distributed algorithm;inter processor communication;matlab	We present a survey of modern parallel MATLAB techniques. We concentrate on the most promising and well supported techniques with an emphasis in SIP applications. Some of these methods require writing explicit code to perform inter-processor communication while others hide the complexities of communication and computation by using higher level programming interfaces. We cover each approach with special emphasis given to performance and productivity issues.	computation;image processing;matlab	Ashok K. Krishnamurthy;John Nehrbass;Juan Carlos Chaves;Siddharth Samsi	2007	2007 IEEE International Conference on Acoustics, Speech and Signal Processing - ICASSP '07	10.1109/ICASSP.2007.367286	distributed algorithm;productivity;parallel computing;image processing;computer science;theoretical computer science;communication complexity;distributed computing;computational complexity theory	Robotics	2.155000721621484	45.705591299779115	176122
b946ab1f9dd64ca03741d11ea0c3c11336840460	parallel performance of block ilu preconditioners for a block-tridiagonal matrix	distributed memory;krylov subspace method;incomplete lu factorization;tridiagonal matrix	The parallelizable block ILU (incomplete LU) factorization preconditioners for a block-tridiagonal matrix have been recently proposed by the author. In this paper, we describe a parallelization of Krylov subspace methods with the block ILU factorization preconditioners on distributed-memory computers such as the Cray T3E, and then parallel performance results of a preconditioned Krylov subspace method are provided to evaluate the effectiveness and efficiency of the block ILU preconditioners on the Cray T3E.	biconjugate gradient stabilized method;computer;cray t3e;distributed memory;image scaling;incomplete lu factorization;iterative method;krylov subspace;parallel computing;preconditioner;rate of convergence	Jae Heon Yun	2003	The Journal of Supercomputing	10.1023/A:1020941526869	tridiagonal matrix;mathematical optimization;parallel computing;distributed memory;incomplete lu factorization;computer science;operating system	HPC	-2.9879868778376237	38.3058013557449	176198
10724b85eca8ef389d01b22f77dd57c68764f8bd	a cross-platform spmv framework on many-core architectures	spmv;intel mic;gpu;segmented scan;cuda;bccoo;opencl;parallel algorithms	Sparse Matrix-Vector multiplication (SpMV) is a key operation in engineering and scientific computing. Although the previous work has shown impressive progress in optimizing SpMV on many-core architectures, load imbalance and high memory bandwidth remain the critical performance bottlenecks. We present our novel solutions to these problems, for both GPUs and Intel MIC many-core architectures. First, we devise a new SpMV format, called Blocked Compressed Common Coordinate (BCCOO). BCCOO extends the blocked Common Coordinate (COO) by using bit flags to store the row indices to alleviate the bandwidth problem. We further improve this format by partitioning the matrix into vertical slices for better data locality. Then, to address the load imbalance problem, we propose a highly efficient matrix-based segmented sum/scan algorithm for SpMV, which eliminates global synchronization. At last, we introduce an autotuning framework to choose optimization parameters. Experimental results show that our proposed framework has a significant advantage over the existing SpMV libraries. In single precision, our proposed scheme outperforms clSpMV COCKTAIL format by 255% on average on AMD FirePro W8000, and outperforms CUSPARSE V7.0 by 73.7% on average and outperforms CSR5 by 53.6% on average on GeForce Titan X; in double precision, our proposed scheme outperforms CUSPARSE V7.0 by 34.0% on average and outperforms CSR5 by 16.2% on average on Tesla K20, and has equivalent performance compared with CSR5 on Intel MIC.	amd firepro;algorithm;apple–intel architecture;auto-tune;blocking (computing);computational science;double-precision floating-point format;geforce 900 series;graph bandwidth;graphics processing unit;high memory;library (computing);load balancing (computing);locality of reference;manycore processor;mathematical optimization;memory bandwidth;overhead (computing);single-precision floating-point format;sparse matrix;tcp global synchronization;the matrix;titan;xeon phi;yet another	Yunquan Zhang;Shigang Li;Shengen Yan;Huiyang Zhou	2016	TACO	10.1145/2994148	parallel computing;real-time computing;computer hardware;computer science;operating system;parallel algorithm;xeon phi	HPC	-3.3853591308145408	41.69936773107695	176211
adba1756c75f8727df2c9b5da443968cc57f9f63	fast parallel algorithms for graph similarity and matching	protein protein interaction network;wikipedia;parallel algorithm;parallel matching;graph similarity;supercomputer class cluster;protein protein interaction networks;graph alignment;vertex similarity;parallel processing;webgraphs;auction matching;auction algorithm	With widespread availability of graph-structured data from sources ranging from social networks to biochemical processes, there is increasing need for efficient and effective graph analyses techniques. Graphs with millions of vertices and beyond are commonplace, necessitating both efficient serial algorithms, as well as scalable parallel formulations. This paper addresses the problem of global graph alignment on supercomputer-class clusters. Given two graphs (or two instances of the same graph), we define graph alignment as a mapping of each vertex in the first graph to a unique vertex in the second graph so as to optimize a given similarity-based cost function. Graph alignment is typically implemented in two steps – in the first step, a similarity matrix is computed. Entries in the matrix quantify similarity of node pairs, one chosen from each graph. In the second step, similar vertices are extracted through a bipartite matching algorithm applied to the similarity matrix. Using a state of the art serial algorithm for similarity matrix computation called Network Similarity Decomposition (NSD), we derive corresponding parallel formulations. Coupling this parallel similarity algorithm with a parallel auction-based bipartite matching technique, we derive a complete graph matching pipeline that is highly efficient and scalable. We validate the performance of our integrated approach on a large, supercomputer-class cluster and diverse graph instances (including Protein Interaction (PPI) networks, Web graphs, and Wikipedia link structures). Experimental results demonstrate that our algorithms scale to large machine configurations and problem instances. Email addresses: gkollias@purdue.edu (Giorgos Kollias), madan.sathe@unibas.ch (Madan Sathe), olaf.schenk@unibas.ch (Olaf Schenk), ayg@cs.purdue.edu (Ananth Grama) 1In the sequel, we’ll be using the word alignment as a synonym for global graph alignment; this is in contrast to local graph alignment that permits a vertex to have different pairings in feasible local alignments, making it an inherently ambiguous process	approximation;bitext word alignment;computation;email;graph (abstract data type);graph theory;loss function;matching (graph theory);numerical linear algebra;parallel algorithm;pixel density;scalability;sequential algorithm;similarity measure;social network;supercomputer;the matrix;wikipedia	Giorgios Kollias;Madan Sathe;Olaf Schenk;Ananth Grama	2014	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.12.010	auction algorithm;parallel processing;folded cube graph;factor-critical graph;combinatorics;parallel computing;graph bandwidth;feedback vertex set;bipartite graph;null graph;computer science;3-dimensional matching;theoretical computer science;simplex graph;machine learning;brand;voltage graph;distributed computing;blossom algorithm;parallel algorithm;windmill graph;butterfly graph;complement graph;line graph;matching	ML	-2.513160214652078	42.04292678633533	176720
a6e8eb584b941db1b69c614fdd21f58e0d78b5a8	overview of the state of the art in embedded machine learning		Nowadays, the main challenges in embedded machine learning are related to artificial neural networks. Inspired by the biological neural networks, artificial neural networks are able to solve complex problems, by performing a tremendous amount of relatively simple parallel computations. Embedding such networks in autonomous devices raises the issues of energy efficiency, resource usage and accuracy. The aim of this paper is to provide a comprehensive analysis of the efforts made in recent years to implement artificial neural network architectures suitable for embedded applications.	artificial neural network;autonomous robot;computation;digital electronics;embedded system;integrated circuit design;machine learning	Liliana Andrade;Adrien Prost-Boucle;Frédéric Pétrot	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342164	system on a chip;computer science;artificial neural network;efficient energy use;computation;machine learning;non-volatile memory;artificial intelligence	EDA	4.380713868467813	41.86433769386705	177427
3be4fa05baed42b483bd55c733f29ee76281558b	special issue: euro-par 2016		This special issue of Concurrency and Computation: Practice and Experience contains revised and extended versions of selected papers presented at the conference Euro-Par 2016. Euro-Par—the European Conference on Parallel Computing—is an annual series of international conferences dedicated to the promotion and advancement of all aspects of parallel and distributed computing. Euro-Par covers a wide spectrum of topics from algorithms and theory to software technology and hardware-related issues, with application areas ranging from scientific to mobile and cloud computing. The major part of the Euro-Par audience consists of researchers in academic institutions, government laboratories, and industrial organisations. Euro-Par 2016, the 22nd conference in the Euro-Par series, was held in Grenoble, France. It was organised by Inria, Université Grenoble-Alpes, and IUT 2 Grenoble. Twelve broad topics were defined and advertised, covering a large variety of aspects of parallel and distributed computing. The call for papers attracted a total of 176 submissions. The submitted papers were reviewed at least 3 and, in most cases, 4 or even more times (4 reviews on average). A total of 47 papers were finally accepted for publication. This makes a global acceptance rate of 26.7 %. The authors of accepted papers came from 20 countries, with the 4 main contributing countries—France, the United States, Germany, and Spain—accounting for a bit more than half of them. Based on the results of the reviews and a majority opinion of the respective topic programme committees, a number of papers were recommended for this special issue. The authors who gave a convincing talk were contacted at the conference and invited to submit revised and extended versions of their papers. These new versions were given to 3 reviewers; 2 had previously reviewed the conference version, the third had not. Eventually, five papers were accepted for publication. This year, four Euro-Par topics are represented. Topic 3 on Scheduling and Load Balancing is represented by the paper Controlling the correlation of cost matrices to assess scheduling algorithm performance on heterogeneous platforms.1 The authors Louis-Claude Canon, Pierre-Cyrille Héam, and Laurent Philippe consider the problem of allocating independent tasks to a collection of different machines in an optimizing manner. Costs are assessed via the generation of matrices that advise on the cost of a specific task on a specific machine. The authors propose a new assessment metric comparing the uniform cost matrix and a cost matrix using task and machine correlations. Two generation methods are proposed, one of them new, the other a modification of an existing one, and their effect on performance evaluation heuristics from the literature is studied. One reviewer pointed out that, despite the numerous required mathematical proofs, the authors do an admirable job of making each one accessible. Topic 4 on High-Performance Architectures and Compilers is represented by 2 papers. In their paper Piecewise holistic autotuning of parallel programs with CERE,2 the authors Mihail Popov, Chadi Akel, Yohan Chatelain, William Jalby, and Pablo de Oliveira Castro describe how to autotune codelets that have been produced by the Codelet Extractor and Replayer CERE. Autotuning a set of codelets incurs lower costs than autotuning the entire program, and CERE helps in setting optimization options appropriately. The good design of the tool and the informative role of the experiments were appreciated in reviewing. The authors Juan Manuel Martinez Caamaño, Manuel Selva, Philippe Clauss, Artyom Baloian, and Willy Wolff follow, with their tool APOLLO, a similar approach in the polyhedron model for loop optimization. In their paper Full runtime polyhedral optimizing loop transformations with the generation, instantiation and scheduling of code-bones,3 they propose a compile-time generation of so-called code-bones that are then, at run time, instantiated and assembled to target code. In the optimization of the target code, the popular polyhedral tools PluTo and CLooG are being employed. Since the code-bones have an increased flexibility, the results show improvements over APOLLO's predecessor VMAD. Topic 5 on Parallel and Distributed Data Management and Analytics is represented by the paper A flexible I/O arbitration framework for netCDF-based Big Data processing workflows on high-end supercomputers.4 The authors Jianwei Liao, Balazs Gerofi, Guo-Yuan Lien, Takemasa Miyoshi, Seiya Nishizawa, Hirofumi Tomita, Wei-keng Liao, Alok Choudary, and Yutaka Ishikawa study a direct communication framework designed for complex workflows that eliminates unnecessary file I/O between job components. Their idea is to inject an I/O arbitration layer that provides direct parallel data transfer. Users are also given the opportunity to specify the desired data transfer pattern in a file. One reviewer noted that the engineering presented is of great interest to the community. Finally, Topic 9 on Multicore and Manycore Parallelism is represented by the paper A framework for dense triangular matrix kernels on various manycore architectures,5 authored by Ali Charara, David Keyes, and Hatem Ltaief. They present a new high-performance framework for architecture-oblivious dense triangular BLAS kernels—triangular matrix-matrix multiplication (TRMM) and triangular solve (TRSM)—on various manycore architectures. Compared to the conference version, this framework is enhanced further with the availability of customized CUDA kernels and a multiple-GPU implementation with almost linear scalability. The reviewers appreciated the high quality and scientific soundness of the treatise. Concluding this preface, we would like to thank Prof Geoffrey Fox, editor-in-chief of Concurrency and Computation: Practice and Experience, for his support of this special issue. We would also like to thank our peers who assisted us in reviewing the papers and helped strengthen the final	algorithm;alok r. chaturvedi;apollo computer;auto-tune;blas;big data;cuda;cloud computing;compile time;compiler;computation;concurrency control;concurrent computing;display resolution;distributed computing;euro-vo;experience;experiment;for loop;graphics processing unit;heuristic (computer science);holism;information;input/output;instance (computer science);ishikawa diagram;lazy evaluation;load balancing (computing);loop optimization;manycore processor;mathematical optimization;matrix multiplication;multi-core processor;netcdf;norm (social);parallel computing;performance evaluation;polyhedron model;randomness extractor;run time (program lifecycle phase);scalability;scheduling (computing);triangular matrix	Christian Lengauer;Luc Bougé;Denis Trystram	2017	Concurrency and Computation: Practice and Experience	10.1002/cpe.4204	computer science;distributed computing	HPC	-2.482275796123018	45.60865906818546	177758
b2ba65d5c28a8d62246cc2fbf4df90225003fb51	synthesis of custom networks of heterogeneous processing elements for complex physical system emulation	odes;paper;heterogeneous systems;fpga;cuda;ordinary differential equations;nvidia;differential equations;computer science;nvidia geforce gtx 460	Physical system models that consist of thousands of ordinary differential equations can be synthesized to field-programmable gate arrays (FPGAs) for highly-parallelized, real-time physical system emulation. Previous work introduced synthesis of custom networks of homogeneous processing elements, consisting of processing elements that are either all general differential equation solvers or are all custom solvers tailored to solve specific equations. However, a complex physical system model may contain different types of equations such that using only general solvers or only custom solvers does not provide all of the possible speedup. We introduce methods to synthesize a custom network of heterogeneous processing elements for emulating physical systems, where each element is either a general or custom differential equation solver. We show average speedups of 45x over a 3 GHz single-core desktop processor, and of 11x and 20x over a 3 GHz four-core desktop and a 763 MHz NVIDIA graphical processing unit, respectively. Compared to a commercial high-level synthesis tool including regularity extraction, the networks of heterogeneous processing elements were on average 10.8x faster. Compared to homogeneous networks of general and single-type custom processing elements, heterogeneous networks were on average 7x and 6x faster, respectively.	datapath;desktop computer;emulator;field-programmability;field-programmable gate array;graphical user interface;graphics processing unit;heuristic;high- and low-level;high-level synthesis;list of intel core i7 microprocessors;name binding;parallel computing;real-time clock;single-core;solver;speedup;thread (computing);vhdl	Chen Huang;Bailey Miller;Frank Vahid;Tony Givargis	2012		10.1145/2380445.2380483	computational science;parallel computing;computer science;theoretical computer science	EDA	0.7013726836275969	45.24291764013098	177856
06590d91ac7254bed7496583e9091caf52b3e912	all-pairs shortest-paths for large graphs on the gpu	internet node traffic;graph size;dram memory;efficient gpu implementation;large datasets;all-pairs shortest-paths;cuda api;nvidia g80 gpu architecture;shared memory cache;large graph;all-pairs shortest-path problem;shared memory;transitive closure;algorithm design;social network;all pairs shortest path;directed graph	The all-pairs shortest-path problem is an intricate part in numerous practical applications. We describe a shared memory cache efficient GPU implementation to solve transitive closure and the all-pairs shortest-path problem on directed graphs for large datasets. The proposed algorithmic design utilizes the resources available on the NVIDIA G80 GPU architecture using the CUDA API. Our solution generalizes to handle graph sizes that are inherently larger then the DRAM memory available on the GPU. Experiments demonstrate that our method is able to significantly increase processing large graphs making our method applicable for bioinformatics, internet node traffic, social networking, and routing problems.	bioinformatics;cuda;computer hardware;directed graph;distributed computing;dynamic random-access memory;floyd–warshall algorithm;geforce 8 series;geolocation;graph (discrete mathematics);graph theory;graphics hardware;graphics processing unit;introduction to algorithms;on-board data handling;resultant;routing;shared memory;shortest path problem;social network;transitive closure	Gary J. Katz;Joseph T. Kider	2008			shared memory;algorithm design;dependency graph;parallel computing;real-time computing;directed graph;computer science;graph theory;theoretical computer science;operating system;machine learning;distributed computing;graph;shortest path problem;programming language;transitive closure;general-purpose computing on graphics processing units;social network;computer graphics (images)	HPC	-2.7346066637036555	42.44343513600171	177916
a99efeba777463acbe00f564bbfec1cb7a8fbdc6	design and testing of a general-purpose neurocomputer	parallelisme;calculateur simd;parallelism;calculateur mimd;paralelismo;simd computer;automate cellulaire;reseau neuronal;cellular automaton;red neuronal;mimd computer;neural network;automata celular	Here we describe the logical design and testing of a generalpurpose neurocomputer, AMNIAC. It may be thought of as a programmable neural network that can simulate arbitrary SIMD and MIMD machines of practical interest (modeled as cellular automata, neural networks, or arbitrary automata networks). AMNIAC is purely bitwise (amnesic), i.e., requires no local memory or registers, other than short memory just long enough for a clock cycle. We discuss software serial and massively parallel simtilations of AMNIAC as (a) tests of the logical design; and (b) benchmarks for evaluation of the trade-off between its universality and memory advantage versus overhead cost of mapping and speed. Theoretical applications of the design are given. A 3D SIMD version of AMNIAC which stabilizes if and only if its input network stabilizes (on the same input data) establishes the unsolvability of the stability problem for networks of finite bandwidth and also its weak solvability by an extension to a larger activation set. We also discuss the feasibility and trade-offs of a physical implementation.	algorithm;artificial neural network;automata theory;backpropagation;bandwidth (signal processing);benchmark (computing);bitwise operation;blueprint;cellular automaton;clock signal;computer programming;embedded system;emoticon;general-purpose macro processor;general-purpose modeling;hypervisor;mimd;maspar;overhead (computing);simd;scalability;simulation;software propagation;universal turing machine;very-large-scale integration;vii	Max H. Garzon;Stan Franklin;William Baggett;William S. Boyd;Dinah Dickerson	1992	J. Parallel Distrib. Comput.	10.1016/0743-7315(92)90064-T	cellular automaton;computer architecture;parallel computing;mimd;computer science;theoretical computer science;algorithm	Arch	5.601099718536937	42.99272192624816	178418
fa8ccc234a3cb77081bcab5a07be665b1ec7eb99	parallelizing probabilistic inference: some early explorations	hypercube style machine;negligible opportunity;appropriate factoring;experimental investigation;available parallelism;clustering tree;belief net;style inference algorithm;belief-net inference;probabilistic inference;early exploration;individual conformal product operation	We report on an experimental investigation into opportunities for parallelism in belief­ net inference. Specifically, we report on a study performed of the available parallelism, on hypercube style machines, of a set of ran­ domly generated belief nets, using factoring (SPI) style inference algorithms. Our results indicate that substantial speedup is available, but that it is available only through paral­ lelization of individual conformal product op­ erations, and depends critically on finding an appropriate factoring. We find negligible op­ portunity for parallelism at the topological, or clustering tree, level.	algorithm;automatic parallelization;block cellular automaton;central processing unit;cluster analysis;computation;integer factorization;parallel computing;speedup	Bruce D'Ambrosio;Tony Fountain;Zhaoyu Li	1992			computer science;artificial intelligence;theoretical computer science;machine learning;data parallelism;algorithm;task parallelism	HPC	-3.193171228536508	43.80923102724339	178575
54154660e51b6d3206735ce709800d5a98947290	applications of conformal computing techniques to problems in computational physics: the fast fourier transform	fast fourier transform fft;design principle;conformal computing;integrated circuit;computational techniques;fast fourier transform;hardware design;optimization;ψ calculus;data structure;scalable coding;design methodology	Abstract   The techniques of Conformal Computing are introduced with an application to the Fast Fourier Transform. Conformal Computing is a design methodology, based on a rigorous mathematical foundation, which provides a systematic approach to the most efficient organization of all levels of the software and hardware design hierarchy from high-level software constructs all the way down to the design of the integrated circuits. We show that using these general design principles, without any specialized optimization, leads to portable, scalable, code that is competitive with other well-tuned machine specific routines. Further improvements are straightforward within our formalism by taking into account specific hardware details (e.g., cache loops) in a portable parametric way. We also argue that the present theory constitutes a uniform way of reasoning about physics and the data structures that define physics on computers.	computational physics;fast fourier transform	James E. Raynolds;Lenore R. Mullin	2005	Computer Physics Communications	10.1016/j.cpc.2005.02.004	fast fourier transform;data structure;design methods;computer science;theoretical computer science;integrated circuit;mathematics;algorithm	Theory	1.7882315201095413	46.01604339031331	178857
20d25d1e2539d7d0a3b0b59fc512e9737fa2c54e	a survey of pram simulation techniques	memoire;interconnection;complexity theory;shared memory;memoria acceso directo;memory management;algorithm complexity;algorithm analysis;gestion;memoria compartida;complejidad algoritmo;simulation;models of parallel computation;simulacion;parallel random access machine;parallel computation;algorithme;algorithm;calculo paralelo;complexite algorithme;general methods;memoria;simulation technique;memoire acces direct;theory;network model;interconnexion;teoria;random access memory ram;parallel computer;arquitectura;parallel complexity theory;bounded degree networks;analyse algorithme;procesador;parallel architecture;processeur;management;calcul parallele;architecture;analisis algoritmo;processor;interconeccion;memory;memoire partagee;theorie;algoritmo	The Parallel Random Access Machine (PRAM) is an abstract model of parallel computation which allows researchers to focus on the essential characteristics of a parallel architecture and ignore other details. The PRAM has long been acknowledged to be a useful tool for the study of parallel computing, but unfortunately it is not physically implementable in hardware. In order to take advantage of the broad base of algorithms and results regarding this high-level abstraction one needs general methods for allowing the execution of PRAM algorithms on more realistic machines. In the following we survey these methods, which we refer to as PRAM simulation techniques. The general issues of memory management and routing are discussed, and both randomized and deterministic solutions are considered. We show that good theoretical solutions to many of the subproblems in PRAM simulation have been developed, though questions still exist as to their practical utility. This article should allow those performing research in this field to become well acquainted with the current state of the art, while allowing the novice to get an intuitive feeling for the fundamental questions being considered. The introduction should provide a concise tutorial for those unfamiliar with the problem of PRAM simulation.	computation;high- and low-level;memory management;parallel computing;randomized algorithm;routing;simulation	Tim J. Harris	1994	ACM Comput. Surv.	10.1145/176979.176984	shared memory;parallel computing;computer science;theoretical computer science;architecture;operating system;interconnection;network model;parallel random-access machine;memory;theory;algorithm;memory management	Theory	9.577005609178073	34.90265299505741	179174
de513870273f405ad032d199c9c572c8d70faec7	using local memory to boost the performance of fft algorithms on the cray-2 supercomputer	parallel calculus;parallelisme;storage access;fourier transform;implementation;supercomputer;transformacion fourier rapida;fast fourier transform;algorithme;supercomputador;algorithm;ejecucion;parallelism;calculo paralelo;cray 2;paralelismo;acces memoire;acceso memoria;transformation fourier rapide;calcul parallele;superordinateur;fast fourier transformation;algoritmo	One of the many interesting architectural features of the CRAY-2 supercomputer is that each processor has access to 16K 64-bit words of local memory. This is in addition to the extremely large, 268-million-word common memory that is accessible by all four processors. By using local memory judiciously, it is possible to achieve increased performance on the CRAY-2. This is partly because accesses to local memory can be done simultaneously with accesses to common memory and other operations. Also, it is slightly faster to start up a vector access to local memory, and a processor does not have to compete with other processors when accessing its local memory. In this paper, we present an algorithm for computing the fast Fourier transform that takes advantage of the CRAY-2's local memory. It operates by solving subproblems, which are themselves Fourier transforms, entirely within local memory. By doing so it achieves a performance increase of between 25 and 40 percent over an equivalent algorithm that uses only common memory, and for some input sizes is able to outperform the CRAY-2 library FFT.	64-bit computing;algorithm;central processing unit;cray-2;fast fourier transform;supercomputer	David A. Carlson	1991	The Journal of Supercomputing	10.1007/BF00129835	uniform memory access;distributed shared memory;shared memory;fast fourier transform;interleaved memory;supercomputer;parallel computing;distributed memory;computer science;theoretical computer science;operating system;overlay;conventional memory;extended memory;flat memory model;registered memory;algorithm;computing with memory;cache-only memory architecture;memory map;non-uniform memory access;memory management	Arch	-0.4324091914959917	39.344934208338195	179200
1ad5963d64722d667a70f084ca3e30953c60619f	computing the singular value decomposition on a distributed system of vector processors	parallel calculus;linear algebra;distributed system;systeme reparti;algorithmique;multiprocessor;matrix diagonalization;singular value decomposition;factorization;calculo paralelo;sistema repartido;factorizacion;diagonalizacion matriz;algorithmics;diagonalisation matrice;algoritmica;algebre lineaire;methode jacobi;metodo jacobi;factorisation;algebra lineal;technical report;computer science;vector processor;multiprocesador;calcul parallele;jacobi method;processeur vectoriel;multiprocesseur	Jacobi methods for computing the singular value decomposition (SVD) of a matrix are ideally suited for multiprocessor environments due to their inherent parallelism. In this paper we show how a block version of the two-sided Jacobi method can be used to compute the SVD efficiently on a distributed architecture. We compare two variants of this method that differ mainly in the degree to which they diagonalize a given subproblem. The first method is a true block generalization of the scalar scheme in that each off-diagonal block is completely annihilated. The second method is a scalar Jacobi algorithm reorganized in such a manner that it conforms to the block decomposition of the problem. We have performed experiments on the Loosely Coupled Array Processor (LCAP) system at IBM Kingston which for the purposes of this article can be viewed as a ring of up to ten FPS-164/MAX array processors. These experiments show that the block Jacobi algorithm performs well on a distributed system, especially when the processors have vector processing hardware. As an example, we were able to achieve a sustained performance of 159 MFlops on a 960-by-720 SVD problem using eight processors. A surprising outcome of these experiments is that the determining factor for the performance of the algorithm on a high-performance architecture is the subproblem solver, not the communication overhead of the algorithm.	central processing unit;distributed computing;singular value decomposition;vector processor	Christian H. Bischof	1989	Parallel Computing	10.1016/0167-8191(89)90027-6	combinatorics;discrete mathematics;parallel computing;computer science;linear algebra;mathematics;algorithmics;factorization;algorithm;algebra	HPC	-2.3073270668333468	37.00973945663419	179259
89fc99f9db2103bf830aebece0fe3a32e9117d7b	fine granularity sparse qr factorization for multicore based systems	qr;sparse;least squares;multicore;multifrontal	The advent of multicore processors represents a disruptive event in the history of computer science as conventional parallel programming paradigms are proving incapable of fully exploiting their potential for concurrent computations. The need for different or new programming models clearly arises from recent studies which identify fine-granularity and dynamic execution as the keys to achieve high efficiency on multicore systems. This work presents an implementation of the sparse, multifrontal QR factorization capable of achieving high efficiency on multicore systems through using a fine-grained, dataflow parallel programming model.	central processing unit;computation;dataflow;history of computer science;history of computing hardware;multi-core processor;out-of-order execution;parallel computing;parallel programming model;programming paradigm;qr decomposition;sparse matrix	Alfredo Buttari	2010		10.1007/978-3-642-28145-7_23	multi-core processor;parallel computing;computer science;theoretical computer science;distributed computing;least squares	HPC	-2.8644964011987177	41.2510234841147	179264
d42b845602a9b41e3f57344618bca51ef295a9ea	competitive learning and vector quantization in digital vlsi systems	learning algorithm;digital vlsi circuits;competitive learning;artificial neural networks;low power;vector quantization;image compression;feature extraction;computer application;vector quantizer;digital circuits;artificial neural network	This paper reviews implementations of competitive learning algorithms in digital VLSI circuits and systems, including recent results from our laboratory on neural circuits for efficient learning computations and for vector quantization. Digital circuits for competitive learning, especially those operating with low-power requirements, are currently important for their applications in computationally efficient speech and image compression by vector quantization, as required, for example, in portable multimedia terminals. A summary of competitive learning models is presented to indicate the type of VLSI computations required, and the effects of weight quantization are discussed. Circuit representations of computational primitives for learning and evaluation of distortion metrics by digital circuits are also reviewed. The present state of VLSI implementations of hard and soft competitive learning algorithms are discussed, as well as those for topological feature maps. Recent results are also presented from simulations of frequency-sensitive competitive learning concerning sensitivity of these algorithms to limited precision in VLSI learning computations. Applications of these learning algorithms to unsupervised feature extraction and to vector quantization are also described.	competitive learning;vector quantization;very-large-scale integration	Howard C. Card;Srigouri Kamarsu;Dean K. McNeill	1998	Neurocomputing	10.1016/S0925-2312(97)00079-9	learning vector quantization;feature extraction;image compression;computer science;theoretical computer science;machine learning;competitive learning;computational learning theory;digital electronics;active learning;vector quantization;artificial neural network	ML	4.857278685480151	41.61661834974977	179722
50b4d37aa50dfc57d70707de6621399da682f936	era of customization and specialization	computer processors;energy efficiency;energy conservation;multiprocessing systems energy conservation;specialization;customization;computing cores;domain specific computing;network on chips energy efficiency computer processors computing cores domain specific computing customization specialization memory hierarchy;network on chips;multiprocessing systems;memory hierarchy	In order to drastically improve the energy efficiency, we believe that future computer processors need to go beyond parallelization, and provide architecture support of customization and specialization so that the processor architecture can be adapted and optimized for different application domains. Customization can be made to computing cores, memory hierarchy, and network-on-chips for efficient adaptation for different workload. Also, we believe that future processor architectures will make extensive use of accelerators to further increase energy efficiency. Such architectures present many new challenges and opportunities, such as accelerator scheduling, sharing, memory hierarchy optimization, and efficient compilation and runtime support. In this talk, I shall present our ongoing research in these areas in the Center for Domain-Specific Computing. SPEAKER’S BIOGRAPHY Jason Cong received his B. S. degree in computer science from Peking University in 1985, his M. S. and Ph. D. degrees in computer science from the University of Illinois at Urbana-Champaign in 1987 and 1990, respectively. Currently, he is a Chancellor’s Professor at the Computer Science Department of University of California, Los Angeles, the director of Center for Domain-Specific Computing (CDSC), and co-director of the VLSI CAD Laboratory. He served as the department chair from 2005 to 2008. Dr. Cong’s research interests include synthesis of VLSI circuits and systems, programmable systems, novel computer architectures, nano-systems, and highly scalable algorithms. He has over 300 publications in these areas, including six best paper awards. He was elected to an IEEE Fellow in 2000 and ACM Fellow in 2008. Dr. Cong is the recipient of the 2010 IEEE Circuits and System (CAS) Society Technical Achievement Award “For seminal contributions to electronic design automation, especially in FPGA synthesis, VLSI interconnect optimization, and physical design automation.” Dr. Cong has graduated 26 PhD students. A number of them are now faculty members in major research universities, including Georgia Tech., Purdue, SUNY Binghamton, UCLA, UIUC, and UT Austin. Others are taking key R&D or management positions in major EDA/computer/semiconductor companies, or being founding members of high-tech startups. He was a co-founder of Aplus Desgin Technologies (acquired by Magma in 2003) and AutoESL Design Technologies (acquired by Xilinx in 2011).	algorithm;application domain;central processing unit;compiler;computer architecture;computer science;computer-aided design;domain-specific language;electronic design automation;field-programmable gate array;gnu nano;graphics processing unit;jason cong;magma;mathematical optimization;memory hierarchy;parallel computing;partial template specialization;physical design (electronics);scalability;scheduling (computing);semiconductor;ut-vpn;very-large-scale integration	Jason Cong	2011		10.1109/ASAP.2011.6043228	computer architecture;parallel computing;real-time computing;energy conservation;computer science;operating system;efficient energy use	EDA	-1.660460328129967	46.04405411405228	179797
04871ebc2d1a3ce4ad344ea79f467c6294bb6ace	out-of-core solution of eigenproblems for macromolecular simulations		We consider the solution of large-scale eigenvalue problems that appear in the motion simulation of complex macromolecules on desktop platforms. To tackle the dimension of the matrices that are involved in these problems, we formulate out-of-core (OOC) variants of the two selected eigensolvers, that basically decouple the performance of the solver from the storage capacity. Furthermore, we contend with the high computational complexity of the solvers by off-loading the arithmeticallyintensive parts of the algorithms to a hardware graphics accelerator.	computational complexity theory;computer simulation;desktop computer;graphics processing unit;motion simulator;out-of-core algorithm;solver	José Ignacio Aliaga;Davor Davidovic;Enrique S. Quintana-Ortí	2013		10.1007/978-3-642-55224-3_46	parallel computing;computational complexity theory;matrix (mathematics);eigenvalues and eigenvectors;out-of-core algorithm;computer science;graphics;multi-core processor;solver	HPC	-4.035830079306859	37.37657139728855	179854
3eb0cd89ce609a23d92a459bf8cbbd92de8f85aa	an improved molecular computing model of modular-multiplication over finite field gf(2n)		With the rapid development of DNA computing, there are some questions worth study that how to implement the arithmetic operations used in cryptosystem based on DNA computing models. This paper proposes an improved DNA computing model to calculate modular-multiplication over finite field GF(2n). Comparing to related works, both assembly time complexity and space complexity are more optimal. The computation tiles performing 4 different functions assemble into the seed configuration with inputs to figure out the result. It is given that how the computation tiles be bitwise coded and how assembly rules work. The assembly time complexity is Θ(n) and the space complexity is Θ(n2). This model requires 148 types of computation tiles and 8 types of boundary tiles.	assembly language;bitwise operation;computation;cryptosystem;dna computing;dspace;time complexity	Yongnan Li;Limin Xiao	2016	2016 17th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)	10.1109/PDCAT.2016.063	parallel computing;theoretical computer science;time complexity;cryptosystem;dna computing;computation;distributed computing;finite field;bitwise operation;computer science;modular arithmetic	HPC	9.336356958212594	33.909766803596334	179988
63a3428a0c5831e15a7964dc34faa4965cbb6f64	theory and algorithms for parallel computation	parallel computation	Bridging models such as BSP and LogP are playing an increasingly important role in scalable parallel computing. They provide a convenient architectureindependent framework for the design, analysis, implementation and comparison of parallel algorithms and data structures. The Theory and Algorithms sessions at EuroPar '98 will cover a number of important issues concerning models of parallel computation. The topics covered will include: The BSP model and its relationship to LogP, BSP algorithms for PC clusters, distributed shared memory models, computational complexity of parallel algorithms, BSP algorithms for discrete event simulation, cost modelling of shared data types, systolic algorithms, cache optimisation, optimal scheduling of task graphs.	binary space partitioning;bridging (networking);cpu cache;computation;computational complexity theory;data structure;distributed shared memory;logp machine;mathematical optimization;parallel algorithm;parallel computing;scalability;scheduling (computing);simulation	William F. McColl;David Walker	1998		10.1007/BFb0057941	computational science;computer science;theoretical computer science	HPC	0.7087454757871982	36.506217239637216	181272
bd344efd4f9d4ff6083f6d00f121f2f662b3263a	fast implementation for sm4 cipher algorithm based on bit-slice technology		The SM4 block cipher algorithm used in IEEE 802.11i standard is released by the China National Cryptographic Authority and is one of the most important symmetric cryptographic algorithms in China. However, whether in the round encryption or key expansion phase of the SM4 algorithm, a large number of bit operations on the registers (e.g., circular shifting) are required. These operations are not effective to encryption in scenarios with large-scale data. In traditional implementations of SM4, different operands are assigned to different words and are processed serially, which can bring redundant operations in the process of encryption and decryption. Bit-slice technology places the same bit of multiple operands into one word, which facilitates bit-level operations in parallel. Bit-slice is actually a single instruction parallel processing technology for data, hence it can be accelerated by the CPU’s multimedia instructions. In this paper, we propose a fast implementation of the SM4 algorithm using bit-slice techniques. The experiment proves that the Bit-slice based SM4 is more efficient than the original version. It increases the encryption and decryption speed of the message by an average of 80%–120%, compared with the original approach.		Jingbin Zhang;Meng Ma;Ping Wang	2018		10.1007/978-3-030-05755-8_11	operand;cipher;cryptography;encryption;block cipher;central processing unit;bit slicing;algorithm;computer science;sm4 algorithm	EDA	8.990263141778739	44.60329985030507	181368
90aee472e43ba73b631cd327bcc45d610433f3d4	cluster-based evolutionary design of digital circuits using all improved multi-expression programming	genetic program;model combination;evolutionary design;cluster of workstations;genetic programming;message passing interface;multi expression programming;computational effort;improved multi expression programming;islands model;digital circuits;evolutionary computing;combinational circuits	Evolutionary Electronics (EE) is a research area which involves application of Evolutionary Computation in the domain of electronics. EE algorithms are generally able to find good solutions to rather small problems in a reasonable amount of time, but the need for solving more and more complex problems increases the time required to find adequate solutions. This is due to the large number of individuals to be evaluated and to the large number of generations required until the convergence process leads to the solution. As a consequence, there have been multiple efforts to make EE faster, and one of the most promising choices is to use distributed implementations. In this paper, we propose a cluster-based evolutionary design of digital circuits using a distributed improved multi expression programming method (DIMEP). DIMEP keeps, in parallel, several sub-populations that are processed by Impoved Multi-Expression Programming algorithms, with each one being independent from the others. A migration mechanism produces a chromosome exchange between the subpopulations using MPI (Message Passing Interface) on a dedicated cluster of workstations (Lido Cluster, Dortmund University). This paper presents the main ideas and shows preliminary experimental results.	algorithm;computer cluster;continuous design;digital electronics;evolutionary computation;message passing interface;multi expression programming;population;workstation	Fatima Zohra Hadjam;Claudio Moraga;Mohamed Benmohamed	2007		10.1145/1274000.1274013	evolutionary programming;genetic programming;mathematical optimization;computer science;artificial intelligence;message passing interface;theoretical computer science;machine learning;distributed computing;combinational logic;digital electronics;algorithm;evolutionary computation	EDA	-1.5509452515577582	34.81887017044404	181389
6158ad0fb4f022b2467915fdf587650a04627c2b	design of a parallel aes for graphics hardware using the cuda framework	parallel aes;cpu based openssl;graphics hardware cryptography parallel processing costs throughput parallel programming web server concurrent computing multicore processing;hardware resource;paper;yarn;concurrent computing;coprocessor parallel aes graphics hardware cuda framework web server encrypted data transfer multicore processor aes ctr symmetric cryptographic primitive cpu based openssl programming knowledge hardware resource advanced encryption standard compute unified device architecture;aes;computer graphic equipment;programming knowledge;parallel programming;compute unified device architecture;coprocessor;coprocessors;cuda;computer architecture;graphics hardware;cryptography;multicore processing;nvidia;nvidia geforce 8400 gs;block ciphers;multicore processors;nvidia geforce 8800 gt;multicore processor;multiprocessing systems;web server;encrypted data transfer;computer science;advanced encryption standard;security;program processors;parallel processing;graphics;aes ctr symmetric cryptographic primitive;throughput;hardware;parallel processing computer graphic equipment coprocessors cryptography multiprocessing systems;cuda framework	Web servers often need to manage encrypted transfers of data. The encryption activity is computationally intensive, and exposes a significant degree of parallelism. At the same time, cheap multicore processors are readily available on graphics hardware, and toolchains for development of general purpose programs are being released by the vendors. In this paper, we propose an effective implementation of the AES-CTR symmetric cryptographic primitive using the CUDA framework. We provide quantitative data for different implementation choices and compare them with the common CPU-based OpenSSL implementation on a performance-cost basis. With respect to previous works, we focus on optimizing the implementation for practical application scenarios, and we provide a throughput improvement of over 14 times. We also provide insights on the programming knowledge required to efficiently exploit the hardware resources by exposing the different kinds of parallelism built in the AES-CTR cryptographic primitive.	algorithm;baseline (configuration management);block cipher;cuda;central processing unit;compiler;coprocessor;cryptographic primitive;cryptography;data rate units;degree of parallelism;encryption;experiment;geforce 8 series;graphics hardware;graphics processing unit;image scaling;information;local optimum;multi-core processor;openssl;parallel computing;parameter (computer programming);plaintext;programming model;server (computing);software deployment;throughput;toolchain;web server	Andrea Di Biagio;Alessandro Barenghi;Giovanni Agosta;Gerardo Pelosi	2009	2009 IEEE International Symposium on Parallel & Distributed Processing	10.1109/IPDPS.2009.5161242	multi-core processor;advanced encryption standard;parallel processing;computer architecture;parallel computing;concurrent computing;computer science;operating system;distributed computing;programming language;coprocessor	Arch	-2.4595694514073463	44.36034414904533	181394
ede044b48decd6fd78132d1b648b9d8567acf376	towards an integer approximation of undirected graphical models		Data analytics for streaming sensor data brings challenges for the resource efficiency of algorithms in terms of execution time and the energy consumption simultaneously. Fortunately, optimizations which reduce the number of CPU cycles also reduce energy consumption. When reviewing the specifications of processing units, one finds that integer arithmetic is usually cheaper in terms of instruction latency, i.e. it needs a small number of clock cycles until the result of an arithmetic instruction is ready. This motivates the reduction of CPU cycles in which code is executed when designing a new, resource-aware learning algorithm. Beside clock cycle reduction, limited memory usage is also an important factor for small devices. Outsourcing parts of data analysis from data centers to ubiquitous devices that actually measure data would reduce the communication costs and thus energy consumption. If, for instance, a mobile medical device or smartphone can build a probabilistic model of the usage behavior of its user, energy models can be made more accurate and power management can be more efficient. The biggest hurdle in doing this, are the heavily restricted computational capabilities of very small devices—some do not even have a floating point processor. Consequently, computationally simple machine learning approaches have to be considered. Low complexity of machine learning models is usually achieved by independence assumptions among features or labels. In contrast, the joint prediction of multiple dependent variables based on multiple observed inputs is an ubiquitous subtask in real world problems from various domains. Probabilistic graphical models are well suited for such tasks, but they suffer from the high complexity of probabilistic inference. In the extended abstract at hand, we show how to write the joint probability mass function of undirected graphical models as rational number, if the parameters are integers. More details on the integer parametrization of undirected graphical models can be found in [1]. Inference algorithms and a new optimization scheme are proposed, that allow the learning of integer parameters without the need for any floating point computation. This opens up the opportunity of running machine learning tasks on very small, resource-constrained devices. To be more precise, based only on integers, it is possible to compute approximations to marginal probabilities, to maximum-a-posteriori (MAP) assignments and maximum likelihood estimate either via an approximate closed form solution or an integer variant of the stochastic gradient descent (SGD) algorithm. It turns	approximation algorithm;cpu cache;central processing unit;clock signal;computation;data center;floating-point unit;genetic algorithm;graph (discrete mathematics);graphical model;instruction cycle;machine learning;marginal model;mathematical optimization;outsourcing;power management;run time (program lifecycle phase);smartphone;statistical model;stochastic gradient descent	Nico Piatkowski;Katharina Morik	2014			real-time computing;instruction cycle;cycles per instruction;floating-point unit;floating point;probabilistic logic;stochastic gradient descent;mathematical optimization;small number;computer science;graphical model	ML	5.4519162372362	43.29868039398856	181611
e3c146b12184d6a00bff6e7be57c1df57b4686a4	load balancing problems for multiclass jobs in distributed/parallel computer systems	multiprocessor interconnection networks;file attente;evaluation performance;structural model;optimisation;communication networks;performance evaluation;methode plus grande pente;generic model;ordinateur parallele;steepest descent algorithm load balancing problems multiclass jobs i parallel computer systems distributed computer systems general network configurations delay function nonlinear optimization problem mean response time communication networks;queuing theory;resource allocation;equilibrio de carga;queueing theory;estudio comparativo;evaluacion prestacion;performance;reseau ordinateur;steepest descent method;equilibrage charge;non linear model;modele non lineaire;indexing terms;interconnection network;computer networks;distributed computer systems;algorithme;optimization problem;etude comparative;temps calcul;modelo no lineal;distributed parallel computer systems;community networks;parallel systems;network configuration;metodo mas grande inclinacion;comparative study;ordenador paralelo;multiclass jobs;load balancing;parallel computer;interconnection networks;multiprocessor interconnection networks parallel processing resource allocation performance evaluation;distributed parallel computing;algorithms;optimization;systeme informatique reparti;load balance;numerical experiment;tiempo computacion;computation time;load management concurrent computing distributed computing computer networks delay multiprocessor interconnection networks communication networks intelligent networks convergence of numerical methods computer network management;nonlinear optimization;management;parallel processing;steepest descent;reseau interconnexion	Load balancing problems for multiclass jobs in distributed/parallel computer systems with general network configurations are considered. We construct a general model of such a distributed/parallel computer system. The system consists of heterogeneous host computers/processors (nodes) which are interconnected by a generally configured communication/interconnection network wherein there are several classes of jobs, each of which has its distinct delay function at each host and each communication link. This model is used to formulate the multiclass job load balancing problem as a nonlinear optimization problem in which the goal is to minimize the mean response time of a job. A number of simple and intuitive theoretical results on the solution of the optimization problem are derived. On the basis of these results, we propose an effective load balancing algorithm for balancing the load over an entire distributed/parallel system. The proposed algorithm has two attractive features. One is that the algorithm can be implemented in a decentralized fashion. Another feature is simple and straightforward structure. Models of nodes, communication networks, and a numerical example are illustrated. The proposed algorithm is compared with a well-known standard steepest-descent algorithm, the FD algorithm. By using numerical experiments, we show that the proposed algorithm has much faster convergence in terms of computational time than the FD algorithm. Index Terms —Algorithms, communication networks, distributed/parallel computer systems, interconnection networks, load balancing, management, multiclass jobs, nonlinear optimization, performance, queuing theory. —————————— ✦ ——————————	algorithm;central processing unit;computer;experiment;gradient descent;interconnection;job stream;load balancing (computing);mathematical optimization;nonlinear programming;nonlinear system;numerical analysis;optimization problem;parallel computing;queueing theory;response time (technology);telecommunications network;time complexity	Jie Li;Hisao Kameda	1998	IEEE Trans. Computers	10.1109/12.660168	parallel processing;parallel computing;computer science;load balancing;theoretical computer science;operating system;distributed computing;parallel algorithm;queueing theory;algorithm	Metrics	-2.7022640482886717	34.05645655389861	181690
088b63603cefe5d75aa4f11978f15fac6d003873	flexible biometric online speaker-verification system implemented on fpga using vector floating-point units	microprocessors;hardware software codesign;support vector machines;vectors support vector machines feature extraction field programmable gate arrays algorithm design and analysis microprocessors computer architecture;biometrics;system on chip biometrics field programmable gate arrays fpgas hardware software codesign speaker recognition;speaker recognition;computer architecture;vectors;system on chip;speaker recognition field programmable gate arrays floating point arithmetic;feature extraction;clock cycles flexible biometric online speaker verification system fpga vector floating point units field programmable gate array embedded system microblaze microprocessor vfpu arm cortex a8 microprocessor;field programmable gate arrays;article;algorithm design and analysis;field programmable gate arrays fpgas	This paper presents the implementation of a speaker-verification system on field programmable gate array. The algorithm is executed by software over an embedded system that includes a MicroBlaze microprocessor connected to a vector floating-point unit (VFPU). The VFPU is designed to speed up the resolution of any vector floating-point operation involved in the verification algorithm, whereas the microprocessor manages the control of the process and executes the rest of operations. With a clock frequency of 40 MHz, the system is capable of executing the complete algorithm in real time, processing a voice frame in 9.1 ms. The same verification process was carried out for two different systems: 1) an ARM Cortex A8 microprocessor; and 2) configuring MicroBlaze with the scalar floating-point unit provided by Xilinx. The experimental results show that when comparing our proposed system against both systems, the number of clock cycles is reduced by a factor of 11.2× and 15.4×, respectively. The main advantage of the VFPU is its flexibility, which allows quick adaptation of the software to the potential changes produced in both the system and the user requirements. The algorithm was tested over a public database that contains the utterances of different users acquired under different environmental conditions, providing good recognition rates.	arm cortex-a8;arm architecture;algorithm;biometrics;clock rate;clock signal;computation;control unit;database;embedded system;flops;feature extraction;field-programmable gate array;floating-point unit;microprocessor;requirement;speaker recognition;throughput;user requirements document;vector processor	Enrique Cantó;Mariano López-García;Rafael Ramos-Lara;Raul Sánchez-Reillo	2015	IEEE Transactions on Very Large Scale Integration (VLSI) Systems	10.1109/TVLSI.2014.2377578	system on a chip;speaker recognition;embedded system;support vector machine;algorithm design;electronic engineering;computer hardware;feature extraction;computer science;operating system;field-programmable gate array;biometrics	EDA	6.4886084557191985	43.002768838220035	181828
34e887fc47167d78c16399570cb3ed3accbd91c9	efficient total-exchange in wormhole-routed toroidal cubes	collective communication;routing;interconnection network;programming model;algorithme;algorithm;wormhole routing;parallel computer;randomized algorithm;wormhole switching;adaptive routing;encaminamiento;systeme parallele;parallel system;high performance;communication pattern;red interconexion;sistema paralelo;acheminement;algoritmo;reseau interconnexion	The total-exchange is one of the most dense communication patterns and is at the heart of numerous applications and programming models in parallel computing. In this paper we present a simple randomized algorithm to efficiently schedule the total-exchange on a toroidal mesh with wormhole switching. This algorithm is based on an important property of the wormhole networks that reach high performance under uniform traffic using adaptive routing. The experimental results, conducted on a 256 nodes bi-dimensional torus, show that this algorithm reaches a very high level of performance, around 90% of the optimal bound, and is more efficient than other algorithms presented in the literature.	cubes;high-level programming language;parallel computing;randomized algorithm;routing;toroidal graph;wormhole switching	Fabrizio Petrini;Marco Vanneschi	1997	Computers and Artificial Intelligence	10.1007/BFb0002731	routing;parallel computing;adaptive routing;computer science;theoretical computer science;distributed computing;programming paradigm;randomized algorithm;algorithm	HPC	1.9801975274774826	37.333295571176556	182056
089c508e1a3c61239db92367b2681cfecd7883dc	a low cost home automation system design for conventional domestic wiring houses	home automation;system design	A method for passing data of an external memory device through a buffer to a data processor is provided. The method divides the storage region of the buffer into two subregions, and passes a first data set read from the external memory device through one of the two subregions to the data processor in response to a first instruction of the data processor. When a second data set required by said data processor is the same as the first data set, the data stored in the one subregion is passed directly to the data processor instead of retrieving the same from the external memory device. When the second data is different from the first data, it is read from the external memory device and passed through the other allocated subregion to the data processor.	home automation;wiring	Yashodhar Narvaneni	2009			auxiliary memory;discrete mathematics;embedded system;computer hardware;home automation;computer science;systems design;data processing system	EDA	7.248560977897745	37.41586170665442	182077
2964e7fb2e3b88d452184b464bebc96387241cdf	a mapping strategy for mimd computers	critical edge;mapping problem;parallel processing;heuristic algorithm	In this paper, a heuristic mapping approach which maps parallel programs, described by precedence graphs, to MIMD architectures, described by system graphs, is presented. The complete execution time of a parallel program is used as a measure, and the concept of critical edges is utilized as the heuristic to guide the search for a better initial assignment and subsequent refinement. An important feature is the use of a termination condition of the refinement process. This is based on deriving a lower bound on the total execution time of the mapped program. When this has been reached, no further refinement steps are necessary. The algorithms have been implemented and applied to the mapping of random problem graphs to various system topologies, including hypercubes, meshes, and random graphs. The results show reductions in execution times of the mapped programs of up to 77 percent over random mapping.	mimd	Jiyuan Yang;Lubomir F. Bic;Alexandru Nicolau	1991	International Journal of High Speed Computing	10.1142/S0129053393000062	heuristic;parallel processing;parallel computing;computer science;theoretical computer science;distributed computing;algorithm	EDA	2.913780301141965	37.006474202764124	182159
1a1532cc6c2ffd3750da7c3203ea9bf148c036dc	finding optimum parallel coprocessor design for genus 2 hyperelliptic curve cryptosystems	public key cryptography;smart card;genus 2;multiplying circuits;hardware accelerator;coprocessor;coprocessors;hardware architecture;embedded systems coprocessors public key cryptography multiplying circuits;embedded systems;parallelism;public key;replicated multiplier optimum parallel coprocessor genus 2 hyperelliptic curve cryptosystem hardware accelerator arithmetic intensive public key primitive high end smart card bit level parallelization arithmetic operation level parallelization;hyperelliptic curve cryptosystem;hyperelliptic curve;embedded processor;coprocessors hardware computer architecture elliptic curve cryptography clocks public key arithmetic security smart cards public key cryptography;hardware implementation;co processor	Hardware accelerators are often used in cryptographic applications for speeding up the highly arithmetic-intensive public-key primitives, e.g. in high-end smart cards. One of these emerging and very promising public-key schemes is based on hyperelliptic curve cryptosystems (HECC). In the open literature only a few considerations deal with hardware implementation issues of HECC. Our contribution appears to be the first one to propose architectures for the latest findings in efficient group arithmetic on HEC. The group operation of HECC allows parallelization at different levels: bit-level parallelization (via different digit-sizes in multipliers) and arithmetic operation-level parallelization (via replicated multipliers). We investigate the trade-offs between both parallelization options and identify speed and time-area optimized configurations. We found that a coprocessor using a single multiplier (D=8) instead of two or more is best suited. This coprocessor is able to compute group addition and doubling in 479 and 334 clock cycles, respectively. Providing more resources it is possible to achieve 288 and 248 clock cycles, respectively.	adder (electronics);bit-level parallelism;crc-based framing;clock signal;coprocessor;critical path method;cryptosystem;degree of parallelism;emoticon;field-programmable gate array;genus (mathematics);hyperelliptic curve cryptography;parallel computing;period-doubling bifurcation;power inverter;processor register;public-key cryptography;register file;smart card;throughput	Guido Bertoni;Luca Breveglieri;Thomas J. Wollinger;Christof Paar	2004	International Conference on Information Technology: Coding and Computing, 2004. Proceedings. ITCC 2004.	10.1109/ITCC.2004.1286710	parallel computing;computer hardware;computer science;theoretical computer science	EDA	9.380260764712533	44.50050350958432	182289
0d55b08fe48b0aa4ba29a33b1d95003afe745592	large-scale parallel genome assembler over cloud computing environment	big data genome assembly;giraph;hadoop;cloud computing;solid state drive (ssd);traditional hpc cluster	The size of high throughput DNA sequencing data has already reached the terabyte scale. To manage this huge volume of data, many downstream sequencing applications started using locality-based computing over different cloud infrastructures to take advantage of elastic (pay as you go) resources at a lower cost. However, the locality-based programming model (e.g. MapReduce) is relatively new. Consequently, developing scalable data-intensive bioinformatics applications using this model and understanding the hardware environment that these applications require for good performance, both require further research. In this paper, we present a de Bruijn graph oriented Parallel Giraph-based Genome Assembler (GiGA), as well as the hardware platform required for its optimal performance. GiGA uses the power of Hadoop (MapReduce) and Giraph (large-scale graph analysis) to achieve high scalability over hundreds of compute nodes by collocating the computation and data. GiGA achieves significantly higher scalability with competitive assembly quality compared to contemporary parallel assemblers (e.g. ABySS and Contrail) over traditional HPC cluster. Moreover, we show that the performance of GiGA is significantly improved by using an SSD-based private cloud infrastructure over traditional HPC cluster. We observe that the performance of GiGA on 256 cores of this SSD-based cloud infrastructure closely matches that of 512 cores of traditional HPC cluster.		Arghya Kusum Das;Praveen Kumar Koppa;Sayan Goswami;Richard Platania;Seung-Jong Park	2017	Journal of bioinformatics and computational biology	10.1142/S0219720017400030	parallel computing;bioinformatics;distributed computing	HPC	-4.139307714111205	44.71724656037316	182946
5a3319c03a3d5819e6b5a13ad107ae6660b8c124	sapphire middleware and software development kit for medical video analysis	real time quality measurement;load balancing sapphire middleware software development kit medical video analysis task parallelism data stream analysis real time quality measurement colonoscopy;frames per second;video signal processing;resource allocation;real time;data stream analysis;video analysis;medical computing;sapphire middleware;medical video analysis;streaming media;middleware streaming media instruction sets graphics processing unit synchronization parallel processing real time systems;synchronization;software development kit;load balancing;colonoscopy;graphic processing unit;video signal processing medical computing middleware resource allocation software tools;middleware;software tools;load balance;task parallelism;quality measures;graphics processing unit;parallel processing;instruction sets;real time systems	This paper presents SAPPHIRE — a novel middleware and software development kit, developed to reduce implementation efforts in utilizing task parallelism to speed up execution time of analysis of a stream of data, such as medical video. As a case study, we implemented a real-time quality measurement of colonoscopy using SAPPHIRE. We increased the number of threads in our case study from 4 (prior to our middleware) to 28 with ease. As a result, through better load-balancing and taking better advantage of available processor cores, we increased the case study's maximum processing rate from 40 frames per second to 90 frames per second.	central processing unit;computer cluster;directshow;graphics processing unit;load balancing (computing);middleware;multithreading (computer architecture);parallel computing;real-time clock;run time (program lifecycle phase);scheduling (computing);software development kit;task parallelism;thread (computing);throughput;video content analysis	Sean Stanek;Wallapak Tavanapong;Johnny S. Wong;Jung Hwan Oh;Ruwan Dharshana Nawarathna;Jayantha Muthukudage;Piet C. de Groen	2011	2011 24th International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2011.5999145	embedded system;parallel processing;parallel computing;real-time computing;computer science;load balancing;operating system;middleware;database	Embedded	-3.3630694066321474	45.322986613239976	183250
56c8d4419900fbbf10b9c1dacc5bfa3c55c24bae	iboard: a highly-capable, high-performance, reconfigurable fpga-based building block for flight instrument digital electronics	software;instruments;field programmable gate arrays aerospace instrumentation;process capability;building block;geoscience;high performance on board processing iboard reconfigurable fpga based building block flight instrument digital electronics space borne instrument;field programmable gate arrays;magnetic cores;high performance;aerospace instrumentation;instruments hardware geoscience software field programmable gate arrays radar magnetic cores;radar;hardware;design methodology	iBoard is a highly capable, highly reusable, and modular FPGA-based common building block for instrument digital electronics. It is targeted to those space-borne instruments that require high-performance on-board processing capabilities. The paper explains the design methodology, describes requirements of flight instrument digital electronics, presents implementation of the first prototype, iBoard 2.	common building block;digital electronics;field-programmable gate array;flight instruments;on-board data handling;prototype;requirement	Yutao He;Mohammad Ashtijou	2010	2010 NASA/ESA Conference on Adaptive Hardware and Systems	10.1109/AHS.2010.5546279	embedded system;process capability;design methods;computer science;electrical engineering;radar;field-programmable gate array	EDA	3.4810414776009146	46.03205423673192	183350
3432fd51a8a28c49091d6b7b27187d559a0a6b63	one word/cycle hc-128 accelerator via state-splitting optimization		As today’s high performance embedded systems are heterogeneous platforms, a crisp boundary between the software and the hardware ciphers is fast getting murky. This work takes up the design of a dedicated hardware accelerator for HC-128, one of the stream ciphers in the software portfolio of eSTREAM finalists. We discuss a novel idea of splitting states kept in SRAMs into multiple smaller SRAMs and exploit the increased parallel accesses to achieve higher throughput. We optimize the accelerator design with state splitting by different factors. A detailed throughput-area-power analysis of these design points follow along with a benchmarking with the state-of-the-art for HC-128. Our implementation marks an HC-128 ASIC with the highest throughput per area performance reported in the literature till date.	hc-256;program optimization	Ayesha Khalid;Prasanna Ravi;Anupam Chattopadhyay;Goutam Paul	2014		10.1007/978-3-319-13039-2_17	speech recognition;programming language	EDA	-0.49917378754755676	46.24151950169473	183623
2c667e8b7da1eff78ad11ed3b6c5f826f5b04c82	improving performance of iterative methods by lossy checkponting		Iterative methods are commonly used approaches to solve large, sparse linear systems, which are fundamental operations for many modern scientific simulations. When the large-scale iterative methods are running with a large number of ranks in parallel, they have to checkpoint the dynamic variables periodically in case of unavoidable fail-stop errors, requiring fast I/O systems and large storage space. To this end, significantly reducing the checkpointing overhead is critical to improving the overall performance of iterative methods. Our contribution is fourfold. (1) We propose a novel lossy checkpointing scheme that can significantly improve the checkpointing performance of iterative methods by leveraging lossy compressors. (2) We formulate a lossy checkpointing performance model and derive theoretically an upper bound for the extra number of iterations caused by the distortion of data in lossy checkpoints, in order to guarantee the performance improvement under the lossy checkpointing scheme. (3) We analyze the impact of lossy checkpointing (i.e., extra number of iterations caused by lossy checkpointing files) for multiple types of iterative methods. (4) We evaluate the lossy checkpointing scheme with optimal checkpointing intervals on a high-performance computing environment with 2,048 cores, using a well-known scientific computation package PETSc and a state-of-the-art checkpoint/restart toolkit. Experiments show that our optimized lossy checkpointing scheme can significantly reduce the fault tolerance overhead for iterative methods by 23%∼70% compared with traditional checkpointing and 20%∼58% compared with lossless-compressed checkpointing, in the presence of system failures.	adaptive multi-rate audio codec;application checkpointing;cg (programming language);computation;computational science;contact scraping;distortion;ecosystem;fail-stop;fast fourier transform;fault tolerance;generalized minimal residual method;imperative programming;input/output;iteration;iterative method;linear system;lossless compression;lossy compression;overhead (computing);petsc;simulation;sparse matrix;stationary process;supercomputer;transaction processing system	Dingwen Tao;Sheng Di;Xin Liang;Zizhong Chen;Franck Cappello	2018		10.1145/3208040.3208050	lossy compression;fault tolerance;iterative method;numerical linear algebra;distributed computing;distortion;computer science;linear system	HPC	-1.8902034376745265	38.78470716751297	183724
dbb77f2249bd86e666f282a1f353d2d9514b5896	gaussian techniques on shared memory multiprocessor computers	general and miscellaneous mathematics computing and information science;parallel algorithm;numerical solution;linear system;matrices;profitability;load balance;parallel implementation;functional unit;programming 990210 supercomputers 1987 1989;parallel processing;scalable shared memory multiprocessors;shared memory multiprocessor	We present performance results for parallel Gauss and Gauss-Jordan elimination algorithms on a shared memory multiprocessor. The Cerberus multiprocessor simulator, a simulator for a scalable shared memory multiprocessor with fully pipelined functional units, is used to evaluate algorithm performance. Our parallel implementations of these linear system solvers make extensive use of barrier synchronization. We show the need for barrier synchronization supported directly in hardware for tightly coupled algorithms. For a fixed problem size, the performance of Gauss-Jordan elimination crosses that of Gauss elimination as we increase the number of processors, even though the latter algorithm has a lower operation count. Sometimes, one can profit by trading operations for a better load balance and lower relative synchronization cost in a parallel algorithm.	multiprocessing;shared memory	Gregory A. Darmohray;Eugene D. Brooks	1987			distributed shared memory;shared memory;parallel computing;distributed memory;computer science;theoretical computer science;distributed computing;parallel algorithm	HPC	-3.41805449695845	38.29778869815556	184238
a6b73a05ac7097a5af345edb116c8bfd9f6c786c	forward and back substitution algorithms on gpu: a case study on modified incomplete cholesky preconditioner for three-dimensional finite difference method	forward and back substitution;finite difference method;gpgpu;preconditioned conjugate gradient;modified incomplete cholesky preconditioner	Forward and back substitution algorithms are widely used for solving linear systems of equations after performing LU decomposition on the coefficient matrix. They are also essential in the implementation of high performance preconditioners which improve the convergence properties of the various iterative methods. In this paper, we describe an efficient approach to implementing forward and back substitution algorithms on a GPU and provide the implementation details of these algorithms on a Modified Incomplete Cholesky Preconditioner for the Conjugate Gradient (CG) algorithm. The resulting forward and back substitution algorithms are then used on a Modified Incomplete Cholesky Preconditioned Conjugate Gradient method to solve the sparse, symmetric, positive definite and linear systems of equations arising from the discretization of three dimensional finite difference ground-water flow models. By utilizing multiple threads, the proposed method yields speedups up to 60 times on GeForce GTX 280 compared to CPU implementation and up to 4.8 times speedup compared to cuSPARSE library function optimized for GPU by NVIDIA.	algorithm;cpu cache;cuda;central processing unit;cholesky decomposition;coefficient;conjugate gradient method;discretization;double-precision floating-point format;fermi (microarchitecture);finite difference method;geforce 200 series;graphics processing unit;incomplete cholesky factorization;iterative method;jiffy (time);lu decomposition;linear system;list of intel core i5 microprocessors;mathematical optimization;preconditioner;scalability;shared memory;sparse matrix;speedup;thread block;triangular matrix	Yigitcan Aksari;Harun Artuner	2011	The Journal of Supercomputing	10.1007/s11227-011-0736-8	mathematical optimization;incomplete cholesky factorization;computer science;finite difference method;theoretical computer science;conjugate gradient method;minimum degree algorithm;cholesky decomposition;general-purpose computing on graphics processing units	HPC	-3.634958622478109	38.471881839944245	184488
04002f9c4f20b1444434d876037a9294792a5f48	efficient hardware implementation of itubee for lightweight application		Recently, a new lightweight block cryptography algorithm, ITUbee, has been proposed by Ferhat Karakoc in Lightsec 2013. An efficient hardware implementation of ITUbee is presented in this paper. Firstly, we reuse certain module, which takes a big share of hardware resource, to achieve better resource utilization. Secondly, we apply composite field to implement 8-bit S-box instead of the traditional looking up tables(LUTs) to save area requirements. In the end, we conclude that the hardware implementation of ITUbee requires about 6448 GE on 0.18 um technology. The area consumption of ITUbee is roughly 31.2% less than the round-based implementation. And it costs 365.6 GE to implement 8-bit S-box by using composite field, 32.7% less than by using LUTs.	8-bit;algorithm;cryptography;requirement;s-box;unified model	Juhua Liu;Wei Li;Guoqiang Bai	2017	2017 12th International Conference for Internet Technology and Secured Transactions (ICITST)	10.23919/ICITST.2017.8356424	computer hardware;the internet;reuse;encryption;composite field;cryptography;computer science	EDA	8.243020406380849	45.24531602902573	184681
d2dab14fa57a58033347fe22c8be7e457e19a1d1	iplar: towards interactive programming with parallel linear algebra in r		R is a widely-used statistical programming language in the data science community. However, in the big data era, R faces the challenges from large scale data analysis tasks. It lacks the ability of distributed linear algebra computation in its local interactive shell. In this paper, we propose iPLAR, a system that runs in the interactive R environment, wraps the high performance parallel linear algebra library, and provides a group of easy-to-use interfaces. iPLAR adopts the client-server model to uncouple the interactive shell from the ScaLAPACK/MPI distributed computing backend. In addition, it provides R users with a group of parallel-detail-transparent interfaces that are similar to the native R linear algebra interfaces. We evaluate the efficiency of iPLAR with representative basic matrix operations and two widely-used machine learning algorithms. Experimental results show that iPLAR achieves the near-linear data scalability and enhances the interactive processing capability of R to large problem scales.	interactive programming;linear algebra	Zhaokang Wang;Shiqing Fan;Rong Gu;Chunfeng Yuan;Yihua Huang	2015		10.1007/978-3-319-27140-8_8	theoretical computer science;programming language	AI	-3.967698468831173	43.43486622089079	184878
6e6abb26c729e9ebc8ec087cc77f32cfcee43959	area efficient hardware implementation of elliptic curve cryptography by iteratively applying karatsuba's method	hardware elliptic curve cryptography polynomials clocks communication channels communication system security energy resources costs galois fields energy consumption;karatsuba sformula;mobile device;logic design;hardware accelerator;secure communication;polynomials;iterative methods;karatsuba s formula extended galois fields polynomial multiplication elliptic curve cryptography;integrated circuit design;elliptic curve cryptography;energy consumption;cryptography;mobile radio;telecommunication security;extended galois fields;recursive karatsuba approach elliptic curve cryptography iterative method iterative karatsuba method secure communication channels mobile devices hardware accelerator polynomial multiplication extended galois fields energy consumption reduction area reduction;digital arithmetic;power consumption;logic design polynomials cryptography iterative methods telecommunication security mobile radio digital arithmetic galois fields power consumption integrated circuit design;galois field;hardware implementation;polynomial multiplication;galois fields	Securing communication channels is especially needed in wireless environments. But applying cipher mechanisms in software is limited by the calculation and energy resources of the mobile devices. If hardware is applied to realize cryptographic operations cost becomes an issue. In this paper we describe an approach which tackles all these three points. We implemented a hardware accelerator for polynomial multiplication in extended Galois fields (GF) applying Karatsuba's method iteratively. With this approach the area consumption is reduced to 2.1 mm^2 in comparison to. 6.2 mm^2 for the standard application of Karatsuba's method i.e. for recursive application. Our approach also reduces the energy consumption to 60 per cent of the original approach. The price we have to pay for these achievement is the increased execution time. In our implementation a polynomial multiplication takes 3 clock cycles whereas the recurisve Karatsuba approach needs only one clock cycle. But considering area, energy and calculation speed we are convinced that the benefits of our approach outweigh its drawback.	cipher;clock signal;elliptic curve cryptography;grammatical framework;hardware acceleration;mobile device;polynomial ring;recursion;run time (program lifecycle phase)	Zoya Dyka;Peter Langendörfer	2005	Design, Automation and Test in Europe	10.1109/DATE.2005.67	arithmetic;secure communication;discrete mathematics;karatsuba algorithm;computer science;cryptography;theoretical computer science;mobile device;mathematics;elliptic curve cryptography;integrated circuit design	EDA	9.84125209715675	43.52074214780976	185204
0238e924d6e764e220d51fe7a0ca2fba076442da	recryptor: a reconfigurable cryptographic cortex-m0 processor with in-memory and near-memory computing for iot security		"""Providing security for the Internet of Things (IoT) is increasingly important, but supporting many different cryptographic algorithms and standards within the physical constraints of IoT devices is highly challenging. Software implementations are inefficient due to the high bitwidth cryptographic operations; domain-specific accelerators are often inflexible; and reconfigurable crypto processors generally have large area and power overhead. This paper proposes Recryptor, a reconfigurable cryptographic processor that augments the existing memory of a commercial general-purpose processor with compute capabilities. It supports in-memory bitline computing using a 10-transistor bitcell to support different bitwise operations up to 512-bits wide. Custom-designed shifter, rotator, and S-box modules sit near the memory, providing high-throughput near-memory computing capabilities. We demonstrate Recryptor’s programmability by implementing the cryptographic primitives of various public/ secret key cryptographies and hash functions. Recryptor runs at 28.8 MHz in 0.7 V, achieving <inline-formula> <tex-math notation=""""LaTeX"""">$6.8\times $ </tex-math></inline-formula> average speedup and <inline-formula> <tex-math notation=""""LaTeX"""">$12.8\times $ </tex-math></inline-formula> average energy improvements over the state-of-the-art software- and hardware-accelerated implementations with only 0.128 mm<sup>2</sup> area overhead in 40-nm CMOS."""	arm cortex-m;algorithm;application-specific integrated circuit;baseline (configuration management);bitwise operation;cmos;central processing unit;computational resource;coprocessor;cryptographic accelerator;cryptography;general-purpose markup language;hardware acceleration;hash function;high-throughput computing;in-memory database;internet of things;key (cryptography);overhead (computing);s-box;speedup;throughput	Yiqun Zhang;Li Xu;Qing Dong;Jingcheng Wang;David Blaauw;Dennis Sylvester	2018	IEEE Journal of Solid-State Circuits	10.1109/JSSC.2017.2776302	parallel computing;cryptographic primitive;encryption;hash function;reconfigurable computing;electronic engineering;cryptography;speedup;computer science;bitwise operation;in-memory processing	Arch	7.757067059620643	44.87585117469208	185739
15214bc39bac963331ee4491f8a75ea934239453	a scalable parallel algorithm for incomplete factor preconditioning	preconditionnement;graph theory;algoritmo paralelo;matrix factorization;teoria grafo;convergence;incomplete factorization;parallel algorithm;incomplet;algorithm analysis;relacion orden;simultaneidad informatica;ordering;preconditioning;calculo automatico;incomplete;theorie graphe;computing;experimental result;factorisation matricielle;algorithme parallele;relajacion;calcul automatique;incompleto;factorizacion lu;methode matricielle;relation ordre;convergencia;en parallele;partition graphe;concurrency;iteraccion;graph partitioning;en paralelo;parallel preconditioning;68r10;analyse performance;matrix method;performance analysis;parallel;resultado experimental;ilu;metodo matriz;iteration;relaxation;precondicionamiento;procesador;65f50;processeur;resultat experimental;simultaneite informatique;factorizacion matricial;factorisation lu;lu factorization;65f10;processor;68w10;analisis eficacia	We describe a parallel algorithm for computing incomplete factor (ILU) preconditioners. The algorithm attains a high degree of parallelism through graph partitioning and a two-level ordering strategy. Both the subdomains and the nodes within each subdomain are ordered to preserve concurrency. We show through an algorithmic analysis and through computational results that this algorithm is scalable. Experimental results include timings on three parallel platforms for problems with up to 20 million unknowns running on up to 216 processors. The resulting preconditioned Krylov solvers have the desirable property that the number of iterations required for convergence is insensitive to the number of processors.	central processing unit;computation;concurrency (computer science);degree of parallelism;graph partition;iteration;krylov subspace;parallel algorithm;parallel computing;preconditioner;scalability	David Hysom;Alex Pothen	2001	SIAM J. Scientific Computing	10.1137/S1064827500376193	matrix method;combinatorics;computing;iteration;convergence;concurrency;graph theory;calculus;relaxation;parallel;mathematics;parallel algorithm;algorithm;algebra	HPC	-2.201143579613138	36.86854158573783	185759
9cb5b8a2e5cc7e2aade248f3bd600f622207a7a9	an area efficient 2d fourier transform architecture for fpga implementation		Two-dimensional Fourier transform plays a significant role in a variety of image processing problems, such as medical image processing, digital holography, correlation pattern recognition, hybrid digital optical processing, optical computing etc. 2D spatial Fourier transformation involves large number of image samples and hence it requires huge hardware resources of field programmable gate arrays (FPGA). In this paper, we present an area efficient architecture of 2D FFT processor that reuses the butterfly units multiple times. This is achieved by using a control unit that sends back the previous computed data of N/2 butterfly units to itself for (log − 1) times. A RAM controller is used to synchronize the flow of data samples between the functional blocks.The 2D FFT processor is simulated by VHDL and the results are verified on a Virtex-6 FPGA. The proposed method outperforms the conventional × point 2D FFT in terms of area which is reduced by a factor of log 2 with negligible increase in computation time. Introduction Fourier Transform is one of the most widely used operations in Digital Signal Processing and plays a significant role in many signal processing applications such as image processing, diffraction, propagation, holography, fiber optics, lasers etc. Basically spatial Fourier transform is used to get the information about phase and magnitude of any spatial signal such as image [1]. A spatial signal contains lots of information about itself. Therefore, to calculate the sine and cosine component of a spatial signal requires lots of hardware resources. Simultaneously, the area and power consumption are directly proportional to number of hardware resources. Though large number of computational blocks may increase the speed of operation but also increase the area and power dissipation of the processor [2,3]. In today’s world, large area and high power dissipation are two major drawbacks of a system. Fully spatial parallel FFT architecture also known as array architecture [4], based on the complete Cooley-Tukey algorithm layout, is one of the potential high throughput designs. However, the implementation of the array architecture is hardware intensive. It achieves high performance by using spatial parallelism, while requiring more routing resources. However, as the problem size grows, unfolding the architecture spatially is not feasible due to serious power and area issue brought by complex interconnections. The column architecture [5,6,7] uses an approach that requires less area on the chip than the array architecture. It is done by collapsing all the columns in array architecture into one column; hence a new frame cannot be processed until the processing of the current frame is finished. Hence, this architecture is not suitable for pipelining. The area requirement is obviously smaller, only / radix-r elements, than for the array architecture. The Pipelined architectures [8,9,10] are useful for FFTs that require high data throughput. The basic principle with pipelined architectures is to collapse the rows, instead of the stages like in column architectures. The architecture is built up from radix butterfly elements with commutators in between. Radix-2 Multi-path Delay Commutator [11,12] was probably the most popular approach for pipeline implementation of radix-2 FFT algorithm. In this paper, we propose an area efficient architecture of 2D Fourier transform by reusing /2 numbers of butterfly units instead of log butterfly units for each 1D FFT architecture. This is achieved by a control unit (CU) which sends back the previous computed data of /2 butterfly units to itself for (log − 1) times [13]. A RAM Control unit is used to synchronize the dataflow for both 1D FFT block and controls the internal data samples to prevent data loss for fast processing. The area requirement is obviously smaller, only /2 radix-2 elements, than the array architecture and pipelined architectures for each 1D FFT architecture, N being the number of sample points.	algorithm;analysis of algorithms;cpu power dissipation;column (database);computation;control unit;dataflow;digital holography;digital signal processing;fast fourier transform;field-programmable gate array;hd radio;image processing;medical imaging;optical computing;optical fiber;parallel computing;pattern recognition;pipeline (computing);radix point;random-access memory;routing;software propagation;toslink;throughput;time complexity;tip (unix utility);unfolding (dsp implementation);vhdl	Atin Mukherjee;Debesh Choudhury	2018	CoRR		fast fourier transform;image processing;field-programmable gate array;fourier transform;architecture;parallel computing;control unit;digital holography;computer science;vhdl	Arch	7.957109539394994	39.92074784415099	185842
a18ef0448173d480afe7e5ae4bf2d60cab746115	parallel tree algorithms for amr and non-standard data access		We introduce several parallel algorithms operating on a distributed forest of adaptive quadtrees/octrees. They are targeted at large-scale applications relying on data layouts that are more complex than required for standard finite elements. Such applications appear in various contexts, examples being the hp-adaptive discontinuous Galerkin method, element-based particle tracking, and in-situ post-processing and visualization. Specifically, we design algorithms to derive an adapted worker forest based on sparse data, to identify owner processes in a top-down search of remote objects, and to allow for variable process counts and per-element data sizes in partitioning and parallel file I/O. We demonstrate the algorithms' usability and performance in the context of a particle tracking example that we scale to 21e9 particles and 64Ki MPI processes on the Juqueen supercomputer.	adaptive multi-rate audio codec;data access;discontinuous galerkin method;finite element method;parallel algorithm;sparse matrix;supercomputer;top-down and bottom-up design;usability;video post-processing	Carsten Burstedde	2018	CoRR		discontinuous galerkin method;tree traversal;sparse matrix;finite element method;distributed computing;parallel algorithm;usability;data access;computer science;supercomputer	HPC	-4.37083894007786	33.313110312805016	186032
137f027dff643b2a5f7f64234ec1359924fb6ebb	cumf_sgd: parallelized stochastic gradient descent for matrix factorization on gpus		Stochastic gradient descent (SGD) is widely used by many machine learning algorithms. It is efficient for big data ap- plications due to its low algorithmic complexity. SGD is inherently serial and its parallelization is not trivial. How to parallelize SGD on many-core architectures (e.g. GPUs) for high efficiency is a big challenge. In this paper, we present cuMF_SGD, a parallelized SGD solution for matrix factorization on GPUs. We first design high-performance GPU computation kernels that accelerate individual SGD updates by exploiting model parallelism. We then design efficient schemes that parallelize SGD updates by exploiting data parallelism. Finally, we scale cuMF SGD to large data sets that cannot fit into one GPU's memory. Evaluations on three public data sets show that cuMF_SGD outperforms existing solutions, including a 64- node CPU system, by a large margin using only one GPU card.	algorithm;analysis of algorithms;big data;central processing unit;computation;computational complexity theory;data parallelism;graphics processing unit;information privacy;machine learning;manycore processor;parallel computing;stochastic gradient descent	Xiaolong Xie;Wei Tan;Liana L. Fong;Yun Liang	2017		10.1145/3078597.3078602	data parallelism;distributed computing;parallel computing;computation;theoretical computer science;big data;computer science;stochastic gradient descent;data set;matrix decomposition;general-purpose computing on graphics processing units	HPC	-1.6413316837668077	41.52288790718065	186105
f444e71235b383bf109a5cb398122031b30ce3f9	fpga-based architecture evaluation of cryptographic coprocessors for smartcards	microprocessors;design decisions;fpga based architecture evaluation;field programmable gate arrays cryptography coprocessors arithmetic clocks manufacturing microprocessors power system security read write memory writing;variable bus width;clocks;area efficient modular exponentiation;design parameters;indexing terms;montgomery multiplication;coprocessors;smartcards;arbitrary technologies fpga based architecture evaluation cryptographic coprocessors smartcards ic cards die sizes coprocessor architectures area efficient modular exponentiation montgomery multiplication evaluation board 8051 microprocessor xilinx fpga variable bus width design parameters design decisions;smartcard;smart cards;cryptography;8051 microprocessor;die sizes;architecture evaluation;manufacturing;writing;arithmetic;digital arithmetic;modular exponentiation;ic cards;read write memory;field programmable gate arrays;arbitrary technologies;coprocessor architectures;cryptographic coprocessors;xilinx fpga;evaluation board;digital arithmetic smart cards cryptography coprocessors field programmable gate arrays;power system security	In 1996, about 600 million IC cards were manufactured worldwide. Due to very small die sizes (max. 25 mm/sup 2/) smartcards encounter more severe restrictions than conventional coprocessors. We study coprocessor architectures for very fast but area efficient modular exponentiation (FME) based on Montgomery multiplication. For assessment purposes we developed an evaluation board containing a 8051 microprocessor, a XILINX FPGA and RAM with variable bus width (8b to 32b). We evaluated these architectures in terms of the main design parameters to ease design decisions for smartcards in arbitrary technologies.	coprocessor;cryptography;field-programmable gate array;smart card	Hagen Ploog;Dirk Timmermann	1998		10.1109/FPGA.1998.707922	embedded system;smart card;parallel computing;computer hardware;computer science;operating system	Crypto	8.63192105241758	45.345472850131145	186188
44f3039b78a1d7e8c94ce00923889ccc704c227f	a comparison of different message-passing paradigms for the parallelization of two irregular applications	active messages;routing algorithm;message passing;breadth first search	We present experimental results for parallelizing two breadth-first search-based applications on Thinking Machines CM-5 by using two different message-passing paradigms, one based onsend/receive and the other based onactive messages. The parallelization of these applications requires fine-grained communication. Our results show that the active messages-based implementation gives significant improvement over the send/receive-based implementation. The improvements can largely be attributed to the lower latency of the active messages implementation.	active message;automatic parallelization;breadth-first search;connection machine;message passing;parallel computing;search-based application	Seungjo Bae;Sanjay Ranka	1996	The Journal of Supercomputing	10.1007/BF00128099	parallel computing;message passing;breadth-first search;computer science;theoretical computer science;distributed computing;programming language	HPC	-3.803227820063192	42.492381854543794	186511
7364b8683b75677361befdeb1aea5c57c8858e13	rcra 2007: experimental evaluation of algorithms for solving problems with combinatorial explosion	experimental evaluation		algorithm	Marco Gavanelli;Toni Mancini	2008	J. Algorithms	10.1016/j.jalgor.2008.02.002	computational science;applied mathematics;computer science;mathematics;algorithm	Theory	-3.11981343691864	35.49244172620008	186665
e277762804aa4615b2258fbd367d91326c00b90e	x1000 real-time phoneme recognition vlsi using feed-forward deep neural networks	vlsi feedforward neural nets gaussian processes mixture models optimisation power consumption speech recognition;vlsi deep neural network fixed point optimization phoneme recognition;digital vlsi real time phoneme recognition vlsi feed forward deep neural networks speech recognition applications gmm gaussian mixture model high recognition accuracy chip size power consumption fixed point optimization method real time processing speed;clocks very large scale integration registers neural networks throughput computer architecture real time systems	Deep neural networks show very good performance in phoneme and speech recognition applications when compared to previously used GMM (Gaussian Mixture Model)-based ones. However, efficient implementation of deep neural networks is difficult because the network size needs to be very large when high recognition accuracy is demanded. In this work, we develop a digital VLSI for phoneme recognition using deep neural networks and assess the design in terms of throughput, chip size, and power consumption. The developed VLSI employs a fixed-point optimization method that only uses +Δ, 0, and -Δ for representing each of the weight. The design employs 1,024 simple processing units in each layer, which however can be scaled easily according to the needed throughput, and the throughput of the architecture varies from 62.5 to 1,000 times of the real-time processing speed.	artificial neural network;convolutional neural network;deep learning;mathematical optimization;real-time clock;real-time transcription;speech recognition;throughput;very-large-scale integration	Jonghong Kim;Kyuyeon Hwang;Wonyong Sung	2014	2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)	10.1109/ICASSP.2014.6855060	speech recognition;computer science;machine learning;time delay neural network	Robotics	4.440329249495858	42.95440165286233	186740
a75d36fa5c30c57893acfa6e891d9156c666d616	a modified rijndael algorithm and it's implementation using fpga	data transmission;indium phosphide;hardware description languages cryptography field programmable gate arrays;frequency 44 mhz modified rijndael algorithm fpga cryptography algorithms secure data transmission substitution and permutation network encryption process vhdl schematic generator based design core generator based design virtex xcv800 6bg432;encryption;aes;hardware description languages;rijndael;input output;cryptography;indium phosphide encryption;cryptography encryption security rijndael aes;place and route;field programmable gate arrays;security	Cryptography algorithms are becoming more necessary to ensure secure data transmission, which can be used in several applications. A modified Rijndael algorithm capable of encrypting a 128 bit input/output/key is presented. The presented algorithm depends on substitution and permutation network (SP-Network) rather than feistel network. A new stage is proposed in the encryption process. The introduced architecture was implemented by VHDL, schematic and core generator - Based Design which are synthesized, placed and routed in Virtex XCV800-6bg432 which resulted in an optimized area (7148) slices and (44) MHz clock speed. Post simulations for major functions and the final algorithm are presented and discussed.	128-bit;algorithm;clock rate;cryptography;encryption;feistel cipher;field-programmable gate array;input/output;routing;schematic;simulation;substitution-permutation network;vhdl;virtex (fpga)	Ahmed A. Mohamed;Ahmed H. Madian	2010	2010 17th IEEE International Conference on Electronics, Circuits and Systems	10.1109/ICECS.2010.5724521	cdmf;advanced encryption standard;embedded system;parallel computing;computer science;theoretical computer science;aes implementations	EDA	9.122602422286995	45.0324438935558	187202
73fbf2753432b4019757061c47ae5bd0816e1473	high performance computing of fast independent component analysis for hyperspectral image dimensionality reduction on mic-based clusters	microwave integrated circuits;load balancing of lower triangular matrix;fast independent component analysis;high performance computing;hyperspectral image dimensionality reduction;covariance matrices hyperspectral imaging microwave integrated circuits optimization symmetric matrices parallel processing;intel xeon phis high performance computing fast independent component analysis hyperspectral image dimensionality reduction mic based clusters fast ica parallel schemes covariance matrix multicores many integrated cores optimization tianhe 2 supercomputer;symmetric matrices;many integrated cores;covariance matrices;parallel processing covariance matrices hyperspectral imaging independent component analysis multiprocessing programs optimisation;optimization;load balancing of lower triangular matrix many integrated cores fast independent component analysis hyperspectral image dimensionality reduction high performance computing;hyperspectral imaging;parallel processing	Fast independent component analysis (Fast ICA) for hyper spectral image dimensionality reduction is computationally complex and time-consuming due to the high dimensionality of hyper spectral images. By analyzing the Fast ICA algorithm, we design parallel schemes for covariance matrix calculating, white processing and ICA iteration at three parallel levels: multicores, many integrated cores (MIC), and clusters. Then we present a series of optimization methods for different hotspots, and measure their performance effects. All the work has been implemented in a framework called Ms-Fast ICA. Our experiments on the Tianhe-2 Supercomputer show that the Ms-Fast ICA algorithm has a good scalability, and it can reach a maximum speed-up of 410 times on 64 nodes with 192 Intel Xeon Phis.	algorithm;dimensionality reduction;experiment;fast fourier transform;fastica;graphics processing unit;hotspot (wi-fi);independent computing architecture;independent component analysis;iteration;manycore processor;mathematical optimization;microsoft windows;scalability;supercomputer;xeon phi	Minquan Fang;Yi Yu;Weimin Zhang;Heng Wu;Mingzhu Deng;Jianbin Fang	2015	2015 44th International Conference on Parallel Processing Workshops	10.1109/ICPPW.2015.23	parallel processing;parallel computing;computer science;theoretical computer science;hyperspectral imaging;machine learning;symmetric matrix	HPC	-2.044526925886002	40.376858048082106	187471
130a95c78108b619669c60b43f53a304e25278dc	a block-asynchronous relaxation method for graphics processing units	mathematics and computing;physical sciences and mathematics;asynchronous relaxation;chaotic iteration;graphics processing units gpus;jacobi method	In this paper, we analyze the potential of asynchronous relaxation methods on Graphics Processing Units (GPUs). We develop asynchronous iteration algorithms in CUDA and compare them with parallel implementations of synchronous relaxation methods on CPU- or GPU-based systems. For a set of test matrices from UFMC we investigate convergence behavior, performance and tolerance to hardware failure. We observe that even for our most basic asynchronous relaxation scheme, the method can efficiently leverage the GPUs computing power and is, despite its lower convergence rate compared to the Gauss-Seidel relaxation, still able to provide solution approximations of certain accuracy in considerably shorter time than Gauss-Seidel running on CPUs- or GPU-based Jacobi. Hence, it overcompensates for the slower convergence by exploiting the scalability and the good fit of the asynchronous schemes for the highly parallel GPU architectures. Further, enhancing the most basic asynchronous approach with hybrid schemes-using multiple iterations within the ''subdomain'' handled by a GPU thread block-we manage to not only recover the loss of global convergence but often accelerate convergence of up to two times, while keeping the execution time of a global iteration practically the same. The combination with the advantageous properties of asynchronous iteration methods with respect to hardware failure identifies the high potential of the asynchronous methods for Exascale computing.	computer graphics;graphics processing unit;linear programming relaxation;relaxation (approximation);relaxation (iterative method)	Hartwig Anzt;Stanimire Tomov;Jack J. Dongarra;Vincent Heuveline	2013	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2013.05.008	mathematical optimization;parallel computing;jacobi method;computer science;theoretical computer science;operating system;distributed computing;algorithm	HPC	-3.9042726109049073	38.76088409536172	187502
2559f95f0af79383ebb4226ca58c7a39c5026f95	kokkos/qthreads task-parallel approach to linear algebra based graph analytics	linear algebra;kernel;arrays;data analysis;algorithm design and analysis;parallel processing	The Graph BLAS effort to standardize a set of graph algorithms building blocks in terms of linear algebra primitives promises to deliver high performing graph algorithms and greatly impact the analysis of big data. However, there are challenges with this approach, which our data analytics miniapp miniTri exposes. In this paper, we improve upon a previously proposed task-parallel approach to linear algebra-based miniTri formulation, addressing these challenges and describing a Kokkos/Qthreads task-parallel implementation that performs as well or slightly better than the highly optimized, baseline OpenMP data-parallel implementation.	algorithm;blas;baseline (configuration management);big data;graph theory;linear algebra;list of algorithms;openmp	Michael M. Wolf;H. Carter Edwards;Stephen Olivier	2016	2016 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2016.7761649	discrete mathematics;computer science;theoretical computer science;data mining;numerical linear algebra	HPC	-2.9917933781962924	40.37137229570487	187503
1235d2d128ed60cf79378678bac27a96bedd9173	follow-the-leader formation marching through a scalable o(log2n) parallel architecture.	distributed system;scalable o log 2 n parallel architecture;scalable multiprocessor parallel architecture scalable o log 2 n parallel architecture multirobot system motion coordination synchronization computational complexity large scale system;motion control;newton euler formulations;cooperative robotics;robotica e informatica industrial;multirobot system;scalable multiprocessor parallel architecture;multi robot system;large scale system;joints;force;convoying;synchronisation;navigation;computational modeling;formation keeping;synchronisation computational complexity large scale systems motion control multiprocessing systems multi robot systems parallel architectures;parallel architectures;dynamics;computational complexity;force computational modeling robot kinematics dynamics joints navigation;synchronization;multi robot systems;convoying cooperative robotics distributed systems multi robot systems newton euler formulations strictly parallel computation formation keeping;parallel computer;strictly parallel computation;motion coordination;multiprocessing systems;parallel architecture;distributed systems;large scale systems;robot kinematics	An important topic in the field of Multi Robot Systems focuses on motion coordination and synchronization for formation keeping. Although several works have addressed such problem, little attention has been devoted to study the computational complexity within the framework of large-scale systems. This paper presents our current work on how to achieve high computational performance for systems composed by a large number of robots that must fulfill with a marching and formation task. A scalable Multi-Processor Parallel Architecture is introduced with the purpose of achieving scalability, i.e., computation time of O(log2n) for a n-robots system. Our architecture has been tested onto a multi-processor system and validated against several simulations testing.	computation;computational complexity theory;multiprocessing;parallel computing;robot;scalability;simulation;time complexity	Julián Colorado;Antonio Barrientos;Claudio Rossi;Jaime del Cerro	2010	2010 IEEE/RSJ International Conference on Intelligent Robots and Systems	10.1109/IROS.2010.5649227	synchronization;simulation;computer science;theoretical computer science;distributed computing	Robotics	-1.375356928642067	36.02932782005131	187524
fd4b31a3d4373a704a16b611eddcac90b97bcdce	efficient implementation of sorting algorithms on asynchronous distributed-memory machines	processing element;distributed memory;parallel algorithm;parallel sorting;sorting;efficient algorithm;sorting merging costs delay distributed computing laboratories australia load management topology;distributed memory machine;parallel sorting algorithms;working technical paper;asynchronous distributed memory machines;efficient implementation;communication cost;algorithms;load balance;sorting algorithm;fujitsu ap1000 sorting algorithms asynchronous distributed memory machines merging processing elements merge operation load balancing	The problem of merging two sequences of elements which are stored separately in two processing elements (PEs) occurs in the implementation of many existing sorting algorithms. We describe efficient algorithms for the merging problem on asynchronous distributed-memory machines. The algorithms reduce the cost of the merge operation and of communication, as well as partly solving the problem of load balancing. Experimental results on a Fujitsu AP1000 are reported. Comments Only the Abstract is given here. The full report appeared as [2]. For related work, see [1]. References [1] A. Tridgell and R. P. Brent, An Implementation of a General-Purpose Parallel Sorting Algorithm, Technical Report TR-CS-93-01, Computer Sciences Laboratory, ANU, February 1993, 24 pp. rpb140. [2] B. B. Zhou, R. P. Brent and A. Tridgell “Efficient Implementation of Sorting Algorithms on Asynchronous Distributed-Memory Machines”, Proc. 1994 International Conference on Parallel and Distributed Systems (Hsinchu, Taiwan, December 1994), IEEE Computer Society Press, Los Alamitos, California, 1994, 102-106. Also Technical Report TR-CS-93-06, Computer Sciences Laboratory, ANU, March 1993 (revised May 1993), 7 pp. rpb142, rpb142tr. Computer Sciences Laboratory, Australian National University, Canberra, ACT 0200 E-mail address: {bing,rpb,tridge}@cslab.anu.edu.au 1991 Mathematics Subject Classification. Primary 68P10; Secondary 68Q22.	computer science;distributed memory;general-purpose markup language;icpads;load balancing (computing);mathematics subject classification;microprocessor;sorting algorithm	Bing Bing Zhou;Richard P. Brent;Andrew Tridgell	1994		10.1109/ICPADS.1994.590058	parallel computing;distributed memory;computer science;sorting;load balancing;theoretical computer science;sorting algorithm;distributed computing;parallel algorithm;merge algorithm;algorithm	Theory	-0.7552731512627506	36.38486188899865	187917
5de8162d8eb0cd52923a8da360461b2c066fb492	hardware/software co-design of elliptic-curve cryptography for resource-constrained applications	public key cryptography hardware software codesign;hardware software codesign variants elliptic curve cryptography resource constrained applications ecc asymmetric encryption cryptographic strength key sizes pure hardware solutions flexible design approach 163 bit gf2m elliptic curve 8 bit processor state of the art software algorithms;embedded devices;elliptic curve cryptography;rfid;embedded devices elliptic curve cryptography rfid;hardware random access memory coprocessors radiofrequency identification error correction codes software	ECC is an asymmetric encryption providing a comparably high cryptographic strength in relation to the key sizes employed. This makes ECC attractive for resource-constrained systems. While pure hardware solutions usually offer a good performance and a low power consumption, they are inflexible and typically lead to a high area.  Here, we show a flexible design approach using a 163-bit GF(2m) elliptic curve and an 8-bit processor. We propose improvements to state-of-the-art software algorithms and present innovative hardware/software codesign variants. The proposed implementation offers highly competitive performance in terms of performance and area.	8-bit;algorithm;elliptic curve cryptography;encryption;graphic art software;key size;public-key cryptography;strong cryptography	Andrea Höller;Norbert Druml;Christian Kreiner;Christian Steger;Tomaz Felicijan	2014	2014 51st ACM/EDAC/IEEE Design Automation Conference (DAC)	10.1145/2593069.2593148	radio-frequency identification;embedded system;parallel computing;elliptic curve digital signature algorithm;computer science;theoretical computer science;operating system;hardware architecture;elliptic curve cryptography	EDA	8.378266344029335	45.28412487916052	188065
baa5b65694ca5e0330b0abdc80996d265bc1ec1a	performance advantages of partitioned global address space languages	application framework;adaptive mesh refinement;partitioned global address space;programming model;immersed boundary method;message passing interface;parallel computer;data layout;high performance;data structure;titanium;reading and writing	For nearly a decade, the Message Passing Interface (MPI) has been the dominant programming model for high performance parallel computing, in large part because it is universally available and scales to thousands of processors. In this talk I will describe some of the alternatives to MPI based on a Partitioned Global Address Space model of programming, such as UPC and Titanium. I will show that these models offer significant advantages in performance as well as programmer productivity, because they allow the programmer to build global data structures and perform one-sided communication in the form of remote reads and writes, while still giving programmers control over data layout. In particular, I will show that these languages make more effective use of cluster networks with RDMA support, allowing them to outperform two-sided communication on both microbenchmarks and bandwidth-limited computational problems, such as global FFTs. The key optimization is overlap of communication with computation and pipelining communication. Surprisingly, sending smaller messages more frequently can be faster than a few large messages if overlap with computation is possible. This creates an interesting open problem for global scheduling of communication, since the simple strategy of maximum aggregation is not always best. I will also show some of the productivity advantages of these languages through application case studies, including complete Titanium implementations of two different application frameworks: an immersed boundary method package and an elliptic solver using adaptive mesh refinement.	algorithm;bisection bandwidth;cholesky decomposition;compiler;data structure;key;linpack benchmarks;multithreading (computer architecture);overhead (computing);parry;partitioned global address space;remote direct memory access;set packing;shared memory;sparse;thread (computing);universal product code	Katherine A. Yelick	2006		10.1007/11846802_6	parallel computing;computer science;theoretical computer science;distributed computing	PL	-4.291356097107052	40.88886536823024	188077
e4dcba9bbf1af8686856758993f4bd71a46c3185	perfect scaling of the electronic structure problem on a simd architecture	densite etat;processing element;simd;eigenvalue problem;computational grid;benchmark;implementation;structure electronique;probleme valeur propre;titanium compound;transition metal alloy;calculateur simd;densidad estado;metal transicion aleacion;paralelismo masivo;estructura electronica;ejecucion;mp 2;electron density;simd computer;mp 1;tio2;maspar;titanium oxide;titanio oxido;density of states;electronic structure;parallelisme massif;titanio compuesto;massive parallelism;o ti;titane oxyde;titane compose;metal transition alliage;problema valor propio	We report on benchmark tests of computations of the total electronic density of states of a micro-crystallite of rutile TiO, on MasPar MP-1 and MasPar MP-2 autonomous SIMD computers. The 3D spatial arrangement of atoms corresponds to the two dimensional computational grid of processing elements (PE) plus memory (2D + 1D) while the interactions between the constituent atoms correspond to the communication between the PEs. The largest sample we study consists of 491,520 atoms and its size is 41.5 X 41.5 X 1Snm. Mathematically, the problem is equzuknt to solving an n X n eigenvalue problem, where n N 2,5OO,UOO. The program is scalable in the number of atoms, so that the time required to run it is nearly independent of the size of the system in x and y directions (2D PE mesh) and is step-wise linear in z direction (memory axis). The total CPU time for the largest sample on a MasPar MP-2 computer with 16,384 processing elements is m 2.1 hour. Keyworak SIMD, Electronic structure; Benchmark; MasPar	benchmark (computing);central processing unit;computation;computer;digraphs and trigraphs;electronic density;electronic structure;grid computing;interaction;maspar;microprocessor;optic axis of a crystal;simd;scalability	Marek T. Michalewicz;Mark Priebatsch	1995	Parallel Computing	10.1016/0167-8191(94)00097-T	parallel computing;simulation;benchmark;simd;titanium oxide;computer science;theoretical computer science;operating system;density of states;implementation;electronic structure;electron density	ML	-4.16046829764974	37.09244425228541	188116
2fc4023239a79e4efd1631db112129a2170fb833	exploring large macromolecular functional motions on clusters of multicore processors	eigenvalue problems;macromolecular machines;krylov subspace method;clusters of multicore processors;info eu repo semantics article;normal modes;computational biology	Normal modes in internal coordinates (IC) furnish an excellent way to model functional collective motions of macromolecular machines, but exhibit a high computational cost when applied to large-sized macromolecules. In this paper, we significantly extend the applicability of this approach towards much larger systems by effectively solving the computational bottleneck of these methods, the diagonalization step and associated large-scale eigenproblem, on a small cluster of nodes equipped with multicore technology. Our experiments show the superior performance of iterative Krylov-subspace methods for the solution of the dense generalized eigenproblems arising in these biological applications over more traditional direct solvers implemented on top of state-of-the-art libraries. The presented approach expedites the study of the collective conformational changes of large macromolecules opening a new window for exploring the functional motions of such relevant systems. 2013 Elsevier Inc. All rights reserved.	algorithmic efficiency;central processing unit;experiment;iterative method;krylov subspace;library (computing);multi-core processor;normal mode;z-matrix (chemistry)	José Ramón López-Blanco;Ruymán Reyes;José Ignacio Aliaga;Rosa M. Badia;Pablo Chacón;Enrique S. Quintana-Ortí	2013	J. Comput. Physics	10.1016/j.jcp.2013.03.032	computational science;mathematical optimization;normal mode;computer science;theoretical computer science;physics;algorithm;quantum mechanics	HPC	-4.107903890476896	37.9147756327254	188956
a5729391fcb3af5e9b986ea99f4756d554d2e780	genetic algorithms and grid computing for artificial embryogeny	computational grid;artificial embryogeny;population size;genetic algorithm;genetic algorithms;grid computing;artificial life	Genetic algorithms are very demanding in terms of computing time and, when the population size is large, they need days to complete or even fail due to memory restrictions. It is particularly the case for artificial life where each evaluation can take more than one minute to develop an artificial creature, plant or organism. Indeed, creatures are developed in physical and chemical simulators that require important computation resources. In order to create more and more realistic creatures, we propose a grid parallelized version of genetic algorithms. Two possibilities exist to increase them: supercomputers or computational grids. Because of their scalability, we choose computational grid in their works.	artificial development;artificial life;computation;genetic algorithm;grid computing;parallel computing;scalability;simulation;supercomputer	Sylvain Cussat-Blanc;Fabien Viale;Hervé Luga;Yves Duthen;Denis Caromel	2008		10.1145/1389095.1389139	computational science;genetic algorithm;computer science;artificial intelligence;theoretical computer science	HPC	-1.812074943899915	34.64163492259593	189733
7dc53c35abd870915d162d834b508f8729fbfe9e	big data analytics on hpc architectures: performance and cost	data science;computer architecture;matrix decomposition;sparks;benchmark testing;cloud computing;hardware	Data driven science, accompanied by the explosion of petabytes of data, has called into need dedicated analytics computing resources. Dedicated analytics clusters require large capital outlays due to their expensive hardware requirements. Additionally, if such resources are located far from the data they analyze, they also incur substantial data transfer, which has both cost and latency implications. In this paper, we benchmark a variety of high-performance computing (HPC) architectures for classic data science algorithms, as well as conduct a cost analysis of these architectures. Additionally, we compare algorithms across analytic frameworks, as well as explore hidden costs in the form of queuing mechanisms. We observe that node architectures with large memory and high memory bandwidth are better suited for big data analytics on HPC hardware. We also conclude that cloud computing is more cost effective for small or experimental data workloads, but HPC is more cost effective at scale. Additionally, we quantify the hidden costs of queuing and how it relates to data science workloads. Finally, we observe that software developed for the cloud, such as Spark, performs significantly worse than pbdR when run in HPC environments.	algorithm;apache spark;benchmark (computing);big data;cloud computing;data science;graphics processing unit;high memory;job scheduler;memory bandwidth;petabyte;requirement;scheduling (computing);science 2.0;speedup;supercomputer;pbdr	Peter Xenopoulos;Jamison Daniel;Michael A. Matheson;Sreenivas Sukumar	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840861	parallel computing;real-time computing;computer science;operating system	HPC	-3.87288558059346	44.93148041198274	189822
2bcb263612016af3eaf5ae7c16840de2465c0ba6	a custom fpga processor for physical model ordinary differential equation solving	processing element;digital signal processing;field programmable gate array;random access memory;ordinary differential equation;real time;differential equation;custom processor;psychology;field programmable gate array fpga;mathematical model field programmable gate arrays random access memory table lookup digital signal processing;high level synthesis;ordinary differential equation ode solving;table lookup differential equations digital signal processing chips field programmable gate arrays high level synthesis psychology;xilinx microblaze soft core processor custom processor ordinary differential equation physical model simulation field programmable gate arrays differential equation processing element depe ode physiological models xilinx virtex6 130t fpga lookup tables high level synthesis hls digital signal processor;mathematical model;digital signal processor;lookup table;digital signal processing chips;differential equations;physical model;field programmable gate arrays;table lookup;chemical reaction;physical model simulation custom processor field programmable gate array fpga ordinary differential equation ode solving;physiological models;physical model simulation	Models of physical systems, such as of human physiology or of chemical reactions, are typically comprised of numerous ordinary differential equations (ODEs). Today's designers commonly consider simulating physical models utilizing field-programmable gate arrays (FPGAs). This letter introduces a resource efficient custom processor-the differential equation processing element, or DEPE-specifically designed for efficient solution of ODEs on FPGAs, and also introduces its accompanying compilation tools. We show that a single DEPE on a Xilinx Virtex6 130T FPGA executes several physiological models faster than real-time while requiring only a few hundred FPGA lookup tables (LUTs). Experiments with a commercial high-level synthesis(HLS) tool show that while a single DEPE is 5-50× slower than HLS circuits, DEPE is 10-200 × smaller. We show that a single DEPE is only 10× slower than a relatively massive and costly 3 GHz Pentium 4 desktop processor for ODE solving, and its speed is also competitive with a 700 Mhz TI digital signal processor and an 450 Mhz ARM9 processor. DEPE is 4×-17× faster than a Xilinx MicroBlaze soft-core processor and 3 ×-6 × smaller. DEPE thus represents an excellent processing element for use by itself for small physical models, and in future parallel networks for larger models.	application-specific instruction set processor;central processing unit;clock rate;compiler;cyber-physical system;desktop computer;digital signal processor;equation solving;field-programmability;field-programmable gate array;high- and low-level;high-level synthesis;lookup table;no instruction set computing;pentium 4;real-time transcription;signal processing;simulation;system testing	Chen Huang;Frank Vahid;Tony Givargis	2011	IEEE Embedded Systems Letters	10.1109/LES.2011.2170152	embedded system;parallel computing;computer hardware;computer science;operating system;differential equation;field-programmable gate array	EDA	1.1932724667809969	45.353867007085405	189824
332956f1acbf0d8760e4b4558e4ab3df7ab80d80	performance comparison of sequential and parallel compression applications for dna raw data	dna raw data compression;performance evaluation;parallel scalability;memory consumption;bioinformatics	We present an experimental performance comparison of lossless compression programs for DNA raw data in FASTQ format files. General-purpose (PBZIP2, P7ZIP and PIGZ) and domain-specific compressors (SCALCE, QUIP, FASTQZ and DSRC) were analyzed in terms of compression ratio, execution speed, parallel scalability and memory consumption. Results showed that domain-specific tools increased the compression ratios up to 70 %, while reducing the runtime of general-purpose tools up to $$7\times $$ 7 × during compression and up to $$3\times $$ 3 × during decompression. Parallelism scaled performance up to $$13\times $$ 13 × when using 20 threads. Our analysis indicates that QUIP, DSRC and PBZIP2 are the best tools in their respective categories, with acceptable memory requirements. Nevertheless, the end user must consider the features of available hardware and define the priorities among its optimization objectives (compression ratio, runtime during compression or decompression, scalability, etc.) to properly select the best application for each particular scenario.	algorithm;data compression ratio;dictionary;disk array;domain-specific language;fastq format;general-purpose markup language;general-purpose modeling;huffman coding;image scaling;library (computing);lossless compression;markov chain;markov model;mathematical optimization;parallel computing;random access;requirement;scalability;supercomputer;thread (computing);bzip2	Aníbal Guerra;Jaime Lotero;Sebastián Isaza	2016	The Journal of Supercomputing	10.1007/s11227-016-1753-4	embedded system;parallel computing;real-time computing;computer science;operating system;distributed computing	HPC	-0.34548514756943755	44.09524320125223	190250
ea298117399abd048a8a1a8b4d27d6b0d17894de	a dedicated very low power analog vlsi architecture for smart adaptive systems	feed forward;learning algorithm;cmos technology;analog vlsi neural networks;physical design;current mode;translinear circuits;chip;analog circuits;low power;perturbation based gradient descent algorithms;gradient descent;differential and balanced current mode signal coding;weak inversion;on chip learning implementation;adaptive system;intelligent system;analog vlsi;ubiquitous computing;power consumption;low power consumption;high power;artificial neural network;neural network;smart adaptive systems	This paper deals with analog VLSI architectures addressed to the implementation of smart adaptive systems on silicon. In particular, we addressed the implementation of artificial neural networks with on-chip learning algorithms with the goal of efficiency in terms of scalability, modularity, computational density, real time operation and power consumption. We present the analog circuit architecture of a feed-forward network with on-chip weight perturbation learning in CMOS technology. Novelty of the approach lies in the circuit implementation of the feed-forward neural primitives and on the overall analog circuit architecture. The proposed circuits feature very low power consumption and robustness with respect to noise effects. We extensively tested the analog architecture with simulations at transistor level by using the netlist extracted from the physical design. The results compare favourably with those reported in the open literature. In particular, the architecture exhibits very high power efficiency and computational density and remarkable modularity and scalability features. The proposed approach is aimed to the implementation of embedded intelligent systems for ubiquitous computing.	adaptive system;very-large-scale integration	Maurizio Valle;Francesco Diotalevi	2004	Appl. Soft Comput.	10.1016/j.asoc.2004.03.002	chip;gradient descent;physical design;embedded system;real-time computing;analogue electronics;computer science;machine learning;cmos;feed forward;artificial neural network	EDA	4.81208157986787	41.755765426942645	190333
646f43219c3d1e7bbbb3b628d6b040f69a95dcb3	on a tiling scheme from m. c. escher			escher;tiling window manager	Dan Davis	1997	Electr. J. Comb.			Theory	5.164359404413681	36.6815942020851	190592
7d7b2a76d35518adf9b328d2844ed69b82748f24	a threaded spike algorithm for solving general banded systems	spike algorithm;diagonally dominant;multicore;non diagonally dominant;banded system solver	A parallel SPIKE algorithm for solving banded diagonally-dominant and non-diagonally dominant linear systems, has been implemented for shared-memory architectures. The numerical experiments demonstrate that the SPIKE-OpenMP solvers offer a highly competitive threaded alternative strategy to the LAPACK BLAS-threaded LU model, for solving banded linear systems on current multicore architectures. 2011 Elsevier B.V. All rights reserved.	blas;diagonally dominant matrix;experiment;high- and low-level;iterative method;lapack;linear system;load balancing (computing);multi-core processor;numerical analysis;openmp;parallel computing;preprocessor;programming paradigm;recursion;spike algorithm;scalability;shared memory;spike-triggered average	Karan Mendiratta;Eric Polizzi	2011	Parallel Computing	10.1016/j.parco.2011.09.003	multi-core processor;mathematical optimization;parallel computing;computer science;theoretical computer science;operating system;diagonally dominant matrix;algorithm;algebra	HPC	-2.947363919256752	38.15940566082711	190727
b92f9e9dad33c863639a106ecfcac3f270e298d8	extreme scale breadth-first search on supercomputers	memory management;arrays;program processors;sparse matrices;algorithm design and analysis;supercomputers	Breadth-First Search(BFS) is one of the most fundamental graph algorithms used as a component of many graph algorithms. Our new method for distributed parallel BFS can compute BFS for one trillion vertices graph within half a second, using large supercomputers such as the K-Computer. By the use of our proposed algorithm, the K-Computer was ranked 1st in Graph500 using all the 82,944 nodes available on June and November 2015 and June 2016 38,621.4 GTEPS. Based on the hybrid-BFS algorithm by Beamer[3], we devise sets of optimizations for scaling to extreme number of nodes, including a new efficient graph data structure and optimization techniques such as vertex reordering and load balancing. Performance evaluation on the K shows our new BFS is 3.19 times faster on 30,720 nodes than the base version using the previously-known best techniques.	algorithm;bitmap;breadth-first search;data structure;distributed memory;graph (abstract data type);graph theory;graph500;ibm websphere extreme scale;image scaling;k computer;list of algorithms;load balancing (computing);mathematical optimization;matrix representation;performance evaluation;requirement;sparse matrix;speedup;supercomputer	Koji Ueno;Toyotaro Suzumura;Naoya Maruyama;Katsuki Fujisawa;Satoshi Matsuoka	2016	2016 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2016.7840705	parallel computing;computer science;theoretical computer science;distributed computing	DB	-2.860207326203795	41.975900149924904	190764
69b2f8482713be20b6025c01a475f6da6fddd15b	machine learning based auto-tuning for enhanced opencl performance portability	benchmarking;kernel;auto tuning;paper;tesla k40;performance evaluation;neural networks;heterogeneous systems;heterogeneous computing;ati radeon hd 7970;ati;opencl auto tuning machine learning artificial neural networks heterogeneous computing;artificial neural networks;software portability learning artificial intelligence neural nets power aware computing;tuning;machine learning;deep learning;graphics processing units;nvidia;amd radeon hd 7970 gpu machine learning based auto tuning enhanced opencl performance portability heterogeneous computing reduced energy consumption functional portability tuning parameter configuration space artificial neural network based model intel i7 3770 cpu nvidia k40 gpu;predictive models;neurons;computer science;performance evaluation tuning benchmark testing graphics processing units neurons kernel predictive models;opencl;benchmark testing	Heterogeneous computing, which combines devices with different architectures, is rising in popularity, and promises increased performance combined with reduced energy consumption. OpenCL has been proposed as a standard for programing such systems, and offers functional portability. It does, however, suffer from poor performance portability, code tuned for one device must be re-tuned to achieve good performance on another device. In this paper, we use machine learning-based auto-tuning to address this problem. Benchmarks are run on a random subset of the entire tuning parameter configuration space, and the results are used to build an artificial neural network based model. The model can then be used to find interesting parts of the parameter space for further search. We evaluate our method with different benchmarks, on several devices, including an Intel i7 3770 CPU, an Nvidia K40 GPU and an AMD Radeon HD 7970 GPU. Our model achieves a mean relative error as low as 6.1%, and is able to find configurations as little as 1.3% worse than the global minimum.	amd radeon rx 200 series;approximation error;artificial neural network;auto-tune;benchmark (computing);cuda;central processing unit;graphics processing unit;heterogeneous computing;machine learning;maxima and minima;network model;opencl api;radeon hd 7000 series;self-tuning;software portability;surround sound	Thomas L. Falch;Anne C. Elster	2015	2015 IEEE International Parallel and Distributed Processing Symposium Workshop	10.1109/IPDPSW.2015.85	embedded system;benchmark;parallel computing;kernel;computer hardware;computer science;operating system;predictive modelling;artificial neural network;symmetric multiprocessor system;benchmarking	Embedded	3.3972525623407055	42.255647562191214	190964
c62d666ff74a8de9ddd85de9cd4967bbbc6be47b	procedures for computing the maximum with dna strands	steps using and kinds of dna strands;respectively. keywords: dna computing;the maximum;primitive operations;dna computing	In recent works for high performance computing, computation with DNA molecules, that is, DNA computing, has considerable attention as one of nonsilicon based computings. In this paper, we consider three procedures for computing the maximum of binary numbers of bits, which are represented with DNA strands. The first procedure computes the maximum of the binary numbers in steps using kinds of DNA strands. The second and third procedures also compute the maximum in and steps using and kinds of DNA strands, respectively.	binary number;computation;dna computing;supercomputer;theory	Satoshi Kamio;Akiko Takehara;Akihiro Fujiwara	2003			dna;parallel computing;computer science;dna computing;computation;binary number;supercomputer	HPC	9.295363710132783	33.7786613610891	191015
9ba39873436714b3e77d5a84aedd56c67d2b974b	rbt-mf: a distributed rubber band technique for maximum flow problem in azure	rubber;high performance computing;jamming;image edge detection;mathematical model;conferences;cloud computing	"""In this paper, we present a novel approach based on a rubber band technique to solve the """"maximum flow"""" problem for a single-source, single-sink directed graph implemented on the Microsoft Azure cloud platform. The problem of finding the maximum possible flow for a given graph is a classic network optimization problem that arises in several real life circumstances. Having proposed several solutions to this problem in traditional distributed platforms, a solution that explicitly takes advantage of the cloud paradigm has not yet been thoroughly investigated. The rubber band technique, which is inspired by the behavior of an elastic rubber band on a plate with several poles, has been exploited in this paper to design a solution to the problem. Our experimental results show that the proposed technique can effectively find an answer for the maximum flow problem in a graph. As the size of the graph in terms of number of nodes, number of edges and flow value increase, the proposed scheme outperforms in comparison to the selected benchmarks."""	array data structure;benchmark (computing);boolean algebra;categorization;cloud computing;computation;dspace;directed graph;ecosystem;flow network;ford–fulkerson algorithm;mathematical optimization;maximum flow problem;memory management;microsoft azure;multi-function printer;optimization problem;platform as a service;programming paradigm;provisioning;real life;row (database);row hammer;sparse matrix;time complexity	Hamid R. Dehghani Samani;Javid Taheri;Albert Y. Zomaya	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0076	natural rubber;supercomputer;parallel computing;flow network;simulation;minimum-cost flow problem;cloud computing;multi-commodity flow problem;computer science;theoretical computer science;operating system;mathematical model;distributed computing;algorithm	EDA	4.101123589941821	36.14537836822924	191517
afbd652a78b8838ae533bba6a85b7542d5bad02c	fpga schemes for minimizing the power-throughput trade-off in executing the advanced encryption standard algorithm	performance;fpga;advanced encryption standard aes;power consumption;advanced encryption standard;low power consumption;high speed;power	Today most research involving the execution of the Advanced Encryption Standard (AES) algorithm falls into three areas: ultra-high-speed encryption, very low power consumption, and algorithmic integrity. This study's focus is on how to lower the power consumption of an FPGA-based encryption scheme with minimum effect on performance. Three novel FPGA schemes are introduced and evaluated. These schemes are compared in terms of architectural and performance differences, as well as the power consumption rates. The results show that the proposed schemes are able to reduce the logic and signal power by 60% and 27%, respectively on a Virtex 2 Pro FPGA while maintaining a high level of throughput.	algorithm;encryption;field-programmable gate array;throughput	Jason Van Dyken;José G. Delgado-Frias	2010	Journal of Systems Architecture - Embedded Systems Design	10.1016/j.sysarc.2009.12.001	advanced encryption standard;embedded system;parallel computing;real-time computing;performance;computer science;power;aes implementations;field-programmable gate array	EDA	8.839989468416004	45.52699848297409	191612
470d0e3b6139fbf43b88c67ef2046a50f4966c08	using prediction to conserve energy in recognition on mobile devices	prediction method;sensor systems;computational load;sensor phenomena and characterization;mobile device;embedded and mobile systems context recognition context prediction machine learning;turn off;context prediction;power aware computing mobile computing;context sensor phenomena and characterization accuracy sensor systems markov processes time series analysis;class sensor dependencies;context sensor dependency sets energy conservation prediction mobile devices power constraints context recognition power consumption reduction class sensor dependencies computational load context sequences;accuracy;power aware computing;energy conservation prediction;machine learning;time series analysis;embedded and mobile systems;context sequences;markov processes;quality of context;mobile systems;context sensor dependency sets;mobile computing;context recognition;context;mobile devices;power consumption reduction;power constraints	As devices are expected to be aware of their environment, the challenge becomes how to accommodate these abilities with the power constraints which plague modern mobile devices. We present a framework for an embedded approach to context recognition which reduces power consumption. This is accomplished by identifying class-sensor dependencies, and using prediction methods to identify likely future classes, thereby identifying sensors which can be temporarily turned off. Different methods for prediction, as well as integration with several classifiers is analyzed and the methods are evaluated in terms of computational load and loss in quality of context. The results indicate that the amount of energy which can be saved is dependent on two variables (the acceptable loss in quality of recognition, and the number of most likely classes which should be accounted for), and two scenario-dependent properties (predictability of the context sequences and size of the context-sensor dependency sets).	algorithm;capacitor plague;computation;embedded system;mobile device;sensor	Dawud Gordon;Stephan Sigg;Yong Ding;Michael Beigl	2011	2011 IEEE International Conference on Pervasive Computing and Communications Workshops (PERCOM Workshops)	10.1109/PERCOMW.2011.5766907	real-time computing;computer science;theoretical computer science;operating system;machine learning;mobile device;mobile computing;statistics	Robotics	1.844928432178279	34.688282360961125	191927
95732e0e5f21fd3b5c223cdce363ac7c6370f18e	multicore curve-based cryptoprocessor with reconfigurable modular arithmetic logic units over gf(2^n)	public key cryptography;instruction level parallel;processor architecture;public key cryptosystems;multiprocessor systems;superscaling feature multicore curve based cryptoprocessor reconfigurable modular arithmetic logic units scalar multiplication hyperelliptic curve cryptography instruction level parallelism;cosic;multiprocessing systems cryptography microprocessor chips;public key cryptosystem;cryptography elliptic curve cryptography magnetic cores computer architecture polynomials parallel processing public key cryptography;polynomials;reconfigurable modular arithmetic logic units;computer architecture;article letter to editor;elliptic curve cryptography;arithmetic and logic units;multicore curve based cryptoprocessor;cryptography;modular arithmetic;superscaling feature;cryptosystems;hyperelliptic curve cryptography;irreducible polynomial;hyperelliptic curve;multiprocessing systems;instruction level parallelism;magnetic cores;processor architectures;public key cryptosystems multiprocessor systems processor architectures reconfigurable hardware arithmetic and logic units;reconfigurable hardware;scalar multiplication;parallel processing;microprocessor chips	This paper presents a reconfigurable curve-based cryptoprocessor that accelerates scalar multiplication of Elliptic Curve Cryptography (ECC) and HyperElliptic Curve Cryptography (HECC) of genus 2 over GF(2n). By allocating a copies of processing cores that embed reconfigurable Modular Arithmetic Logic Units (MALUs) over GF(2n), the scalar multiplication of ECC/HECC can be accelerated by exploiting Instruction-Level Parallelism (ILP). The supported field size can be arbitrary up to a(n + 1) - 1. The superscaling feature is facilitated by defining a single instruction that can be used for all field operations and point/divisor operations. In addition, the cryptoprocessor is fully programmable and it can handle various curve parameters and arbitrary irreducible polynomials. The cost, performance, and security trade-offs are thoroughly discussed for different hardware configurations and software programs. The synthesis results with a 0.13-mum CMOS technology show that the proposed reconfigurable cryptoprocessor runs at 292 MHz, whereas the field sizes can be supported up to 587 bits. The compact and fastest configuration of our design is also synthesized with a fixed field size and irreducible polynomial. The results show that the scalar multiplication of ECC over GF(2163) and HECC over GF(283) can be performed in 29 and 63 mus, respectively.	cmos;clock signal;elliptic curve cryptography;exploit (computer security);fastest;genus (mathematics);hyperelliptic curve cryptography;instruction-level parallelism;irreducibility;irreducible polynomial;multi-core processor;on the fly;parallel computing;secure cryptoprocessor;speedup	Kazuo Sakiyama;Lejla Batina;Bart Preneel;Ingrid Verbauwhede	2007	IEEE Transactions on Computers	10.1109/TC.2007.1071	irreducible polynomial;arithmetic;parallel processing;modular arithmetic;parallel computing;computer science;cryptography;hyperelliptic curve cryptography;theoretical computer science;scalar multiplication;mathematics;elliptic curve cryptography;hyperelliptic curve;algebra	Arch	9.442725495067634	44.5489222163513	191930
2f4af662867c8102409debeeca9a266699d6e2ee	cheap noisy sensors can improve activity monitoring under stringent energy constraints	activity monitoring;monitoring hidden markov models microphones acoustics sensor phenomena and characterization acoustic sensors;energy constraints;resource allocation;energy constraints sensor management activity monitoring resource allocation;sensor management;comparators circuits;error rate cheap noisy sensors activity monitoring stringent energy constraints low power acoustic sensor off the shelf components energy consumption low power analog comparator adjustable thresholds snr adaptive sensor scheduling long term acoustic wildlife monitoring application;energy consumption;acoustic transducers;energy consumption acoustic transducers comparators circuits	This paper proposes a low-power acoustic sensor built using off-the-shelf components. To reduce energy consumption, the ADC and the microphone's signal-conditioning circuit are replaced by a low-power analog comparator with adjustable thresholds. Although the SNR of the proposed sensor is reduced, we demonstrate how recent advancements in adaptive sensor scheduling can utilize this sensing modality at the right times to deliver high estimation performance in the presence of extremely stringent device-energy constraints. In a long-term acoustic wildlife monitoring application, energy consumption is reduced by a factor of up to 15× over the scheduling policy that does not utilize the proposed sensor. Furthermore, when device energy is extremely scarce, optimal sensor management can reduce error rate from nearly 40% to 8%.	acoustic cryptanalysis;activity tracker;comparator;low-power broadcasting;microphone;modality (human–computer interaction);scheduling (computing);sensor web;signal-to-noise ratio	David M. Jun;Long Le;Douglas L. Jones	2013	2013 IEEE Global Conference on Signal and Information Processing	10.1109/GlobalSIP.2013.6736983	embedded system;electronic engineering;real-time computing;engineering	Mobile	3.558976958816242	34.936362353689255	192004
2590b0f5707f378599ac7672ba0243763da205c5	detection of chemicals used in explosive devices using mobile communication and satellite communication	explosives chemicals materials sensors mobile communication chemical sensors;chemicals;less economical and life losses explosive chemicals mobile communication satellite communication chemical sensors quick detection;satellite communication;sensors;quick detection;explosive devices;explosive detection;satellite communication chemical sensors explosive detection mobile communication;materials;chemical detection;explosive chemicals;explosives;mobile communication;less economical and life losses;mobile devices;bomb blasts;bomb blasts chemical detection explosive devices mobile communication satellite communication mobile devices;chemical sensors	This paper is aimed to present an idea of detecting explosives using mobile and satellite communication systems and thus paving a way for in depth research on formulating this idea into practice. The research in both electronics world and chemical world can lead to the development of sensors which can be embedded on the mobile devices. The present technology of satellite communication can be integrated with the mobile communication system to have easy and speedy detection of chemicals being used in explosive devices and therefore reducing the rate of heavy losses occurring due to sudden and frequently happening bomb blasts in various places around the world.	communications satellite;embedded system;mobile device;mobile phone;sensor	Sarita Bhan Zutshi;B. L. Koul	2013	2013 Tenth International Conference on Wireless and Optical Communications Networks (WOCN)	10.1109/WOCN.2013.6616248	chemical industry;explosive material;mobile telephony;telecommunications;computer science;sensor;mobile device;computer security;communications satellite	Robotics	4.8195247706062	33.34624282635832	192326
44807d716ac8f7a926bc02b134848c691c2e9924	aligner: a process-in-memory architecture for short read alignment in rerams		"""Genomics is the key to enable the personal customization of medical care. How to fast and energy-efficiently analyze the huge amounts of genomic sequence data generated by next generation sequencing technologies has become one of the most significant challenges facing genomics today. Existing hardware platforms achieve low genome sequencing throughput with significant hardware and power overhead. In this paper, we propose AligneR, a ReRAM-based process-in-memory architecture, to accelerate the bottleneck of genome sequencing, i.e., short read alignment. Compared to state-of-the-art accelerators, AligneR improves the short read alignment throughput per Watt per <inline-formula><tex-math notation=""""LaTeX"""">$mm^2$</tex-math><alternatives><inline-graphic xlink:href=""""jiang-ieq1-2854700.gif""""/></alternatives></inline-formula> by <inline-formula><tex-math notation=""""LaTeX"""">$13\times$</tex-math><alternatives><inline-graphic xlink:href=""""jiang-ieq2-2854700.gif""""/></alternatives></inline-formula>."""		Farzaneh Zokaee;Hamid R. Zarandi;Lei Jiang	2018	IEEE Computer Architecture Letters	10.1109/LCA.2018.2854700	genomics;parallel computing;architecture;throughput;memory architecture;bottleneck;computer science;dna sequencing;resistive random-access memory;watt	Arch	-0.16262971526963924	43.14309473350021	192463
f8dc90b0af92e919da330d36f7e8acc93b10af59	an adaptive parallel algorithm for computing connectivity		Computing connected components in undirected graphs is a fundamental problem in graph analytics. The size of graph data collections continues to grow in many different scientific domains, which motivates the need for high performance distributed memory parallel graph algorithms, especially for large networks that cannot fit into the memory of a single compute node. For a graph G(V,E) with n vertices and m edges, two vertices belong to the same connected component iff there is a path between the two vertices in G. Sequentially, this problem can be solved in linear O(m) time, e.g. by using one of the following two approaches. One approach is to use either Breadth First (BFS) or Depth First Search (DFS) for each component. Another technique is to use a union-find based algorithm, where each vertex is initially assumed to be a different graph component and components connected by an edge are iteratively merged. There are known work-optimal and practical parallel solutions for computing BFS traversals on distributed memory systems. While parallel BFS algorithms have been optimized for traversing a short diameter big graph component, they can still be utilized for finding connectivity using multiple executions, one per connected component. However, for graphs with large number of small components, parallel BFS needs to be executed one after another, because unless a component is identified, a vertex not in the component cannot be chosen to initiate search for the next connected component. On the contrary, the classic Shiloach-Vishkin (SV) algorithm [1], a widely known PRAM algorithm for computing connectivity, simultaneously computes connectivity of all vertices and promises convergence in logarithmic iterations making it suitable for components with large diameter, as well as for graphs with a large number of small sized components. Compared to the simple label propagation techniques, the SV algorithm bounds the number of iterations to O(log n) instead of O(n), where each iteration is equivalent to O(m) work.	connected component (graph theory);depth-first search;dijkstra's algorithm;disjoint-set data structure;distributed memory;graph (discrete mathematics);graph coloring;graph theory;iteration;parallel algorithm;software propagation;systemverilog;vertex (geometry)	Chirag Jain;Patrick Flick;Tony Pan;Oded Green;Srinivas Aluru	2016	CoRR		parallel computing;graph bandwidth;computer science;graph partition;connectivity;theoretical computer science;distributed computing	HPC	2.161206259512465	36.660993693601064	192481
1b7bdfda653197dc02bd0e88319d53a00617cd2a	designing coprocessors for hybrid compute systems	design principle;communication architectures;image coding;computational components;hardware software codesign;design engineering;application software;coprocessor design;buffered data processing;reconfigurable devices;bulk wise transfer;reed solomon codes;data processing;fpga;coprocessor;hw sw codesign;coprocessors;acceleration;hardware software codesign coprocessor design hybrid compute systems cpu reconfigurable devices fpga computational components bulk wise transfer buffered data processing reed solomon encoding coprocessor communication architectures;integrated circuit design;coprocessors field programmable gate arrays matrix decomposition acceleration data processing image coding application software bandwidth design engineering pipeline processing;matrix decomposition;design principles coprocessor hw sw codesign;reed solomon;hybrid compute systems;bandwidth;reed solomon codes coprocessors encoding field programmable gate arrays hardware software codesign integrated circuit design pipeline processing;field programmable gate arrays;cpu;encoding;pipeline processing;design principles;reed solomon encoding coprocessor	A hybrid compute system (HCS) combines standard CPUs and reconfigurable devices, usually FPGAs, in one system. These systems have become more attractive again, due to a closer and hence faster coupling of both computational components. From our work with several designs for the same application, we have found the communication between a CPU and a FPGA-based coprocessor to relate either to pipelining or to a bulk-wise transfer with buffered data processing. We identify conditions which determine whether the pipelined or the buffered style should be used in a design. A Reed/Solomon encoding coprocessor has been implemented for each of the communication architectures to serve as an example of how these conditions materialize and how they influence the performance.	central processing unit;clustered file system;computation;computer data storage;coprocessor;coupling (computer programming);data dependency;field-programmable gate array;hcs clustering algorithm;input/output;pipeline (computing)	Volker Hampel;Peter Sobe;Erik Maehle	2008	2008 IEEE International Symposium on Parallel and Distributed Processing	10.1109/IPDPS.2008.4536506	computer architecture;parallel computing;real-time computing;data processing;computer science;operating system;coprocessor;field-programmable gate array	Arch	7.02278506755829	42.01713244270151	192759
32cae7589c0e29357ed3600f52c4b0eb0b58665f	a fast dual-field modular arithmetic logic unit and its hardware implementation	carry logic;kernel;110 mhz;csa;elliptic curves;elliptic curve;clocks;cosic;logic;256 bit;integer arithmetic operations;carry save adder;modular arithmetic logic unit;malu;205 mbit s;elliptic curve cryptography;adders;cryptography;modular arithmetic;hyperelliptic curve cryptography;arithmetic logic hardware elliptic curve cryptography kernel elliptic curves scalability throughput clocks field programmable gate arrays;arithmetic;hyperelliptic curve;fpga prototyping;scalability;field programmable gate arrays;carry save adders;388 mbit s;hardware implementation;modular multiplication;galois fields;galois fields adders carry logic cryptography field programmable gate arrays;throughput;hardware;256 bit modular arithmetic logic unit hardware implementation malu carry save adders csa integer arithmetic operations elliptic curve hyperelliptic curve cryptography fpga prototyping 205 mbit s 388 mbit s 110 mhz	We propose a fast modular arithmetic logic unit (MALU) that is scalable in the digit size (d) and the field size (k). The datapath of MALU has chains of carry save adders (CSAs) to speed up the large integer arithmetic operations over GF(p) and GF(2m). It is well suited and very efficient for the modular multiplication and addition/subtraction which are the computational kernels of elliptic curve and hyperelliptic curve cryptography (H/ECC). While maintaining the scalability and multi-function, we obtain a throughput of 205 Mbps and 388 Mbps with a clock rate of 110 MHz for 256-bit GF(p) and GF(2239) respectively on FPGA prototyping	arithmetic logic unit;clock rate;data rate units;datapath;elliptic curve cryptography;fpga prototyping;field-programmable gate array;hyperelliptic curve cryptography;markov chain;scalability;throughput	Kazuo Sakiyama;Bart Preneel;Ingrid Verbauwhede	2006	2006 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2006.1692703	arithmetic;modular arithmetic;parallel computing;computer science;theoretical computer science;mathematics;elliptic curve	Arch	9.71899104098539	44.526852712885315	193103
5644af0ad6a75b4edaa75fe024aa6336f2926ede	efficient dynamic load balancing strategies for parallel active set optimization methods	offre service;distributed memory;distributed system;algoritmo paralelo;carga dinamica;optimisation;dynamic load balancing;systeme reparti;parallel algorithm;optimizacion;equilibrio de carga;memoria compartida;optimal method;heuristic method;equilibrage charge;metodo heuristico;optimization method;charge dynamique;dynamic load;metodo optimizacion;algorithme parallele;regime desequilibre;sistema repartido;regimen desequilibrado;methode optimisation;load balancing;optimization;load balance;methode heuristique;memoire repartie;unbalanced conditions;optimal algorithm;proposals	In this paper three strategies are described to restore dynamically the load balancing in parallel active set optimization algorithms. The efficiency of our proposals is shown by comparison with other heuristics described in related works, such as the classical Bestfit and Worstfit methods. The computational cost due to the load unbalancing in the parallel code and the communication overheads associated with the most efficient load balancing strategy are analyzed and compared in order to establish whether the distribution is convenient or not. Experimental results on a distributed memory system, the Fujitsu AP3000, highlight the accuracy of our estimations.	active set method;load balancing (computing);program optimization	Inmaculada Pardines;Francisco F. Rivera	2003		10.1007/978-3-540-45209-6_31	parallel computing;computer science;load balancing;distributed computing;computer security;algorithm	HPC	-3.1842940306559284	34.01345749475233	193139
a989ebac7b01e63116c2d05d96cf0b7ed423e32e	revisiting fpga implementation of montgomery multiplier in redundant number system for efficient ecc application in gf(p)		The fast implementations of ECC in GF(p) are generally implemented using specialized prime field, and henceforth, they are dependent on the structure of the prime. But, these implementations cannot be ported to generic curves which do not support such prime structures. Such generic curves are often used in various crypto-applications like pairing and post-quantum secure supersingular isogeny based key exchange. In those cases, modular multiplication is executed through Montgomery multiplier which is slower compared to modular multiplication using specialized primes. This work aims to reduce the speed gap between Montgomery multiplication and modular multiplication in specialized prime field by presenting an efficient implementation of Montgomery multiplier on FPGA using the redundant number system.		Debapriya Basu Roy;Debdeep Mukhopadhyay	2018	2018 28th International Conference on Field Programmable Logic and Applications (FPL)	10.1109/FPL.2018.00061	field-programmable gate array;parallel computing;prime (order theory);elliptic curve cryptography;computer science;key exchange;modular arithmetic;isogeny;adder;pairing	EDA	9.157432008351424	44.35537259821411	193286
44abdc07419ee08e12906658a6ab6c759f71ac8d	yay - an open-hardware energy measurement system for feedback and appliance detection based on the arduino platform		To analyse user behaviour and energy consumption data in contemporary and future households, we need to monitor electrical appliance features as well as ambient appliance features. For this purpose, a distributed measurement system is required, which measures the entire power consumption of the household, the power consumption of selected household appliances, and the effect of these appliances on their environment. In this paper we present a distributed measurement system that records and monitors electrical household appliances. Our low-cost measurement system integrates the YaY smart meter, a set of smart plugs, and several networked ambient sensors. In conjunction with energy advisor tools the presented measurement system provides an efficient low-cost alternative to commercial energy monitoring systems by surpassing them with machine learning techniques, appliance identification methods, and applications based on load disaggregation.	arduino;computer appliance;machine learning;motherboard;open-source hardware;open-source software;sensor;smart meter;software appliance;software deployment;steady state;system of measurement;thermal management (electronics);transient state	Christoph Klemenjak;Wilfried Elmenreich	2017	2017 13th Workshop on Intelligent Solutions in Embedded Systems (WISES)	10.1109/WISES.2017.7986924	embedded system;real-time computing;computer science;energy consumption;system of measurement;arduino;computer hardware;smart meter;intelligent sensor	Mobile	1.6657734006235223	32.36639079098932	193315
429859b91e2bcefe34f60200dcee5d70a87d9514	watershed parallel algorithm for asynchronous processors array	asynchronous processors array;flooding;minima detection;granularity level;parallel algorithm;image segmentation;picture segmentation system;watershed parallel algorithm;hardware description languages;asynchronous processors network;hardware description languages parallel algorithms parallel architectures image segmentation multimedia communication mobile handsets;segmentation transformation;global synchronization;multimedia systems;mobile phone;multimedia computing;joint algorithm architecture analysis;low power;parallel architectures;multimedia mobile terminal;pixel;multimedia communication;systemc library;pixels;mobile handsets;labelling;architecture analysis;parallel hill climbing algorithm watershed parallel algorithm asynchronous processors array joint algorithm architecture analysis picture segmentation system multimedia mobile terminal asynchronous processors network granularity level pixels global synchronization segmentation transformation minima detection flooding labelling multimedia mobile phones real time computing circuit systemc library vhdl synchronous prototyping;circuits;vhdl synchronous prototyping;floods;data flow;parallel hill climbing algorithm;real time computing circuit;algorithm design and analysis;multimedia mobile phones;real time computing;parallel algorithms image segmentation multimedia systems pixel algorithm design and analysis floods multimedia computing mobile handsets real time systems circuits;mobile terminal;real time systems;parallel algorithms	"""A joint algorithm-architecture analysis leads to a new version of picture segmentation system adapted to multimedia mobile terminal constraints. The asynchronous processors network, with a granularity level of one processor per pixel, based on data flow model, takes less than 10 /spl mu/s to segment a SQCIF $88*72 pixels - image (about 2000 times faster than the classical sequential watershed algorithms). The main originality of the proposed algorithm is only one global synchronization point is needed in order to complete the segmentation transformation, instead of the three (or more) classical points: minima detection, labelization and flooding. Our system tends to cope with multimedia mobile phones constraints, i.e. real time computing circuit, low power. We have simulated and validated this system thanks to """"SystemC"""" library; VHDL synchronous prototyping shows up results accordingly."""	central processing unit;parallel algorithm;watershed (image processing)	Bruno Galilée;Franck Mamalet;Marc Renaudin;Pierre-Yves Coulon	2002		10.1109/ICME.2002.1035901	real-time computing;computer science;theoretical computer science;operating system;distributed computing;parallel algorithm;pixel	HPC	9.980688357837751	39.26216691376952	193551
4568d44517c33736ce34e1f3946dbc309c641d3c	vectorized vbyte decoding		We consider the ubiquitous technique of VByte compression, which represents each integer as a variable length sequence of bytes. The low 7 bits of each byte encode a portion of the integer, and the high bit of each byte is reserved as a continuation flag. This flag is set to 1 for all bytes except the last, and the decoding of each integer is complete when a byte with a high bit of 0 is encountered. VByte decoding can be a performance bottleneck especially when the unpredictable lengths of the encoded integers cause frequent branch mispredictions. Previous attempts to accelerate VByte decoding using SIMD vector instructions have been disappointing, prodding search engines such as Google to use more complicated but faster-to-decode formats for performance-critical code. Our decoder (MASKED VBYTE) is 2 to 4 times faster than a conventional scalar VByte decoder, making the format once again competitive with regard to speed.	branch predictor;byte;continuation;encode;open-source software;simd;web search engine	Jeff Plaisance;Nathan Kurz;Daniel Lemire	2015	CoRR		parallel computing;computer science;theoretical computer science;algorithm	Arch	9.197554310946641	38.4595641918607	193567
c33e55de3e9745c09cf1bc46863da140e73ce7bd	compressed threshold pivoting for sparse symmetric indefinite systems	communication avoiding;65f05;pivoting;symmetric;indefinite;sparse;65f50;65y20	A key technique for controlling numerical stability in sparse direct solvers is threshold partial pivoting. When selecting a pivot, the entire candidate pivot column below the diagonal must be up-to-date and must be scanned. If the factorization is parallelized across a large number of cores, communication latencies can be the dominant computational cost. In this paper, we propose two alternative pivoting strategies for sparse symmetric indefinite matrices that significantly reduce communication by compressing the necessary data into a small matrix that can be used to select pivots. Once pivots have been chosen, they can be applied in a communication-efficient fashion. For an n×p submatrix on P processors, we show our methods perform a factorization using O(log P ) messages instead of the O(p logP ) for threshold partial pivoting. The additional costs in terms of operations and communication bandwidth are relatively small. A stability proof is given and numerical results using a range of symmetric indefinite matrices arising from practical problems are used to demonstrate the practical robustness. Timing results on large random examples illustrate the potential speedup on current multicore machines.	algorithmic efficiency;central processing unit;computation;multi-core processor;numerical analysis;numerical stability;parallel computing;pivot element;robustness (computer science);sparse matrix;speedup	Jonathan D. Hogg;Jennifer A. Scott	2014	SIAM J. Matrix Analysis Applications	10.1137/130920629	mathematical optimization;combinatorics;theoretical computer science;mathematics;symmetry	HPC	-1.8242796979989402	38.69715631839257	193896
75c5cbd5f1ca7dfc6563df03b4b9d1c99a67b3db	an efficient high performance scalar multiplication method with resistance against timing attacks	naf encoded private keys;elliptic curve;time complexity;elliptic curve cryptosystems timing attacks elliptic curve scalar multiplication method binary encoding naf encoded private keys time complexity;binary codes;timing attacks;cryptography binary codes computational complexity;timing elliptic curves elliptic curve cryptography public key cryptography galois fields security energy consumption high performance computing resists petroleum;computational complexity;elliptic curve scalar multiplication method;cryptography;high performance;binary encoding;scalar multiplication;elliptic curve cryptosystems;timing attack	This paper presents an efficient high performance elliptic curve scalar multiplication method with resistance against Timing Attacks. The main idea of the proposed method is to control the main scalar multiplication loop such that- either- a- single-point- addition' is- performed- or- a- number- of- consecutive point doublings that take the same time taken by a single point addition is- performed. The proposed method works with both binary-encoded as well as NAF-encodedprivate keys with NAF encoding yielding higher performance. It requires no extra fake computations and its time complexity is less than other recently reported countermeasures, especially when parallel multipliers are used.	computation;countermeasure (computer);elliptic curve cryptography;nato architecture framework;time complexity	Turki F. Al-Somani;Alaaeldin Amin	2008	2008 IEEE/ACS International Conference on Computer Systems and Applications	10.1109/AICCSA.2008.4493630	timing attack;computer science;theoretical computer science;statistics	EDA	9.21327017791977	43.676262719337075	193940
8f6943d487ade3c594bb329cde2477727b8e0529	parallel exact inference on a cpu-gpgpu heterogenous system	parallel exact inference;nvidia geforce gtx 260;double buffering based asynchronous data transfer;kernel;paper;multicore processor scheduling parallel exact inference cpu gpgpu heterogenous system probabilistic graphical models computational complexity lightweight scheduler conflict free potential table organization data layout coalescing memory access double buffering based asynchronous data transfer;heterogeneous systems;probabilistic graphical models;heterogeneous computing;bepress selected works;processor scheduling;exact inference heterogeneous computing gpgpu;computer graphic equipment;exact inference;conflict free potential table organization;bayesian methods;inference mechanisms;instruction sets kernel junctions particle separators message systems bayesian methods layout;layout;junctions;coprocessors;probabilistic graphical model;cpu gpgpu heterogenous system;cuda;statistical distributions computational complexity computer graphic equipment coprocessors inference mechanisms parallel processing processor scheduling;memory access;gpgpu;statistical distributions;multicore processor scheduling;computational complexity;message systems;nvidia;graphical model;coalescing memory access;particle separators;multicore processors;bayesian methods instruction sets junctions kernel layout message systems particle spearators;data layout;computer science;parallel processing;data transfer;instruction sets;lightweight scheduler	Exact inference is a key problem in exploring probabilistic graphical models. The computational complexity of inference increases dramatically with the parameters of the graphical model. To achieve scalability over hundreds of threads remains a fundamental challenge. In this paper, we use a lightweight scheduler hosted by the CPU to allocate cliques in junction trees to the GPGPU at run time. The scheduler merges multiple small cliques or splits large cliques dynamically so as to maximize the utilization of the GPGPU resources. We implement node level primitves on the GPGPU to process the cliques assigned by the CPU. We propose a conflict free potential table organization and an efficient data layout for coalescing memory accesses. In addition, we develop a double buffering based asynchronous data transfer between CPU and GPGPU to overlap clique processing on the GPGPU with data transfer and scheduling activities. Our implementation achieved 30X speedup compared with state-of-the-art multicore processors.	central processing unit;clique (graph theory);computational complexity theory;general-purpose computing on graphics processing units;graphical model;ibm notes;multi-core processor;multiple buffering;overhead (computing);run time (program lifecycle phase);scalability;scheduling (computing);speedup;string searching algorithm;vii;yang	Hyeran Jeon;Yinglong Xia;Viktor K. Prasanna	2010	2010 39th International Conference on Parallel Processing	10.1109/ICPP.2010.15	parallel processing;parallel computing;real-time computing;computer science;theoretical computer science;operating system;graphical model;programming language	HPC	0.33403213149249744	41.867824563540296	193954
5c49387aa4f23585d7360c99dc3341758f23d80d	highly efficient and accurate seizure prediction on constrained iot devices		In this paper we present an efficient and accurate algorithm for epileptic seizure prediction on low-power and portable IoT devices. State-of-the-art algorithms suffer from two issues: computation intensive features and large internal memory requirement, which make them inapplicable for constrained devices. We reduce the memory requirement of our algorithm by reducing the size of data segments (i.e. the window of input stream data on which the processing is performed), and the number of required EEG channels. To respect the limitations of the processing capability, we reduce the complexity of our exploited features by only considering the simple features, which also contributes to reducing the memory requirements. Then, we provide new relevant features to compensate the information loss due to the simplifications (i.e. less number of channels, simpler features, shorter segment, etc.). We measured the energy consumption (12.41 mJ) and execution time (565 ms) for processing each segment (i.e. 5.12 seconds of EEG data) on a low-power MSP432 device. Even though the state-of-art does not fit to IoT devices, we evaluate the classification performance and show that our algorithm achieves the highest AUC score (0.79) for the held-out data and outperforms the state-of-the-art.	algorithm;computation;computer data storage;electroencephalography;low-power broadcasting;mobile device;power semiconductor device;requirement;run time (program lifecycle phase);simple features;stream (computing)	Farzad Samie;Sebastian P. M. Paul;Lars Bauer;Jörg Henkel	2018	2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)	10.23919/DATE.2018.8342147	memory management;support vector machine;real-time computing;computer science;parallel computing;energy consumption;feature extraction;simple features;computation;internet of things;communication channel	EDA	3.49930801282682	42.61315902161469	194275
eb902c1c00c2c313e5637308a544a0d3f2de72bf	acceleration of nonnumeric operations using hardware support for the ordered table hashing algorithms	database operations;computer architecture file organisation sorting search problems;sorting;information retrieval;low complexity;hardware accelerator;hardware accelerators;searching;acceleration hardware database machines very large scale integration information retrieval sorting statistics time factors digital arithmetic logic;ordered table hashing algorithms nonnumeric operations database operations information retrieval operations low level primitives radix sorting hashing bit vector operations nonnumeric algorithms hardware accelerators other algorithms low complexity hardware support;nonnumeric processing;computer architecture;hashing;search problems;sorting algorithm;file organisation	This paper introduces a new approach to acceleration of nonnumeric, database, and information retrieval operations. While traditional techniques accelerate the most time-critical high-level software constructs, we propose novel low-level primitives and demonstrate how these primitives improve database operations. Radix sorting, hashing, and bit-vector operations are used to develop a new class of nonnumeric algorithms—OTHER (Ordered Table Hashing and Radix sort algorithms)—based on low-level hashing operations Init, Mark, and Scan. We have proposed and evaluated two hardware accelerators for OTHER algorithms. It is shown that a low complexity hardware support (less than 10K transistors) can significantly improve the performance of nonnumeric operations.	cost efficiency;dec alpha;database;hardware acceleration;hash function;hash table;high- and low-level;information retrieval;init;preprocessor;radix sort;simulation;sorting algorithm;transistor;window of opportunity;xfig	Emil Jovanov;Veljko M. Milutinovic;Ali R. Hurson	2002	IEEE Trans. Computers	10.1109/TC.2002.1032623	parallel computing;hash function;hardware acceleration;computer science;sorting;theoretical computer science;operating system;sorting algorithm;database;algorithm	DB	6.133017498621097	39.557720126985195	194353
9e0f9180c5886d39084ed3d1e34e63b729756810	monitoring household activities and user location with a cheap, unobtrusive thermal sensor array	thermal sensor;household monitoring;activity recognition	We demonstrate that a cheap (30USD) small, low power 8x8 thermal sensor array can by itself provide a broad range of information relevant for human activity monitoring in home and office environments. In particular the sensor can track people with an accuracy in the range of 1m (which is sufficient to recognize activity relevant regions), detect the operation mode of various appliances such as toaster, water cooker or egg cooker and actions such as opening a refrigerator, the oven or taking a shower. While there are sensing modalities for each of the above types of information (e.g. current sensors for appliances) the fact that they can all be detected by such a simple sensor is highly relevant for practical activity recognition systems. Compared to vision (or thermal imaging systems) the system has the advantage is being less privacy invasive allowing it for example to monitor bathroom activities (as shown in one of our evaluation scenarios). The paper describes the sensor, the methods used for activity detection and the evaluation.	activity recognition;sensor;water cooling	Peter Hevesi;Sebastian Wille;Gerald Pirkl;Norbert Wehn;Paul Lukowicz	2014		10.1145/2632048.2636084	embedded system;simulation;computer science;activity recognition	HCI	3.240605780650768	33.56303460743802	194407
6c3e19fcd6ca27bca312aa57e98f4ed713abab69	eigen-g: gpu-based eigenvalue solver for real-symmetric dense matrices		This paper reports the performance of Eigen-G, which is a GPU-based eigenvalue solver for real-symmetric matrices. We confirmed that Eigen-G outperforms state-of-the-art GPU-based eigensolvers such as magma_dsyevd and magma_dsyevd_2stage implemented in the MAGMA version 1.4.0. Applying the best-tuned CUDA BLAS libraries and the GPU-CPU hybrid DGEMM yields an even better performance improvement. We observe an approximately 2.3 times speedup over magma_ dsyevd on a Tesla K20c.	eigen (c++ library);graphics processing unit;solver;sparse matrix	Toshiyuki Imamura;Susumu Yamada;Masahiko Machida	2013		10.1007/978-3-642-55224-3_63	divide-and-conquer eigenvalue algorithm;inverse iteration	Vision	-3.117998850758131	39.271292866313715	194428
ab7e22b6e619c1594129c08de0e1798b930be28b	xmx: a firmware-oriented block cipher based on modular multiplications	smart card;oscillations;block cipher;ingenieria logiciel;software engineering;algorithme;algorithm;public key;criptografia;cryptography;genie logiciel;cryptographie;modular multiplication;algoritmo	This paper presents xmx, a new symmetric block cipher optimized for public-key libraries and microcontrollers with arithmetic coprocessors. xmx has no S-boxes and uses only modular multiplications and xors. The complete scheme can be described by a couple of compact formulae that offer several interesting time-space trade-offs (number of rounds/key-size for constant security). In practice, xmx appears to be tiny and fast: 136 code bytes and a 121 kilo-bits/second throughput on a Siemens SLE44CR80s smart-card (5 MHz oscillator).	block cipher;byte;coprocessor;firmware;key size;library (computing);microcontroller;public-key cryptography;s-box;smart card;throughput;xmx	David M'Raïhi;David Naccache;Jacques Stern;Serge Vaudenay	1997		10.1007/BFb0052344	embedded system;smart card;block cipher;computer science;cryptography;theoretical computer science;operating system;mathematics;computer security;algorithm	OS	8.801373689863384	43.93158693820369	194495
6c6a983725fbed1da0aa6b2bcfdf5901429d0045	collaborative (cpu + gpu) algorithms for triangle counting and truss decomposition on the minsky architecture: static graph challenge: subgraph isomorphism		In this paper, we present collaborative CPU + GPU algorithms for triangle counting and truss decomposition, the two fundamental problems in graph analytics. We describe the implementation details and present experimental evaluation on the IBM Minsky platform. The main contribution of this paper is a thorough benchmarking and comparison of the different memory management schemes offered by CUDA 8 and NVLink, which can be harnessed for tackling large problems where the limited GPU memory capacity is the primary bottleneck in traditional computing platform. We find that the collaborative algorithms achieve 28× speedup on average (180× max) for triangle counting, and 165× speedup on average (498× max) for truss decomposition, when compared with the baseline Python implementation provided by the Graph Challenge organizers.	algorithm;baseline (configuration management);cuda;central processing unit;graphics processing unit;memory management;nvlink;python;speedup;subgraph isomorphism problem	Ketan Date;Keven Feng;Rakesh Nagi;Jinjun Xiong;Nam Sung Kim;Wen-Mei W. Hwu	2017	2017 IEEE High Performance Extreme Computing Conference (HPEC)	10.1109/HPEC.2017.8091042	time complexity;parallel computing;architecture;python (programming language);theoretical computer science;parallel algorithm;approximation algorithm;speedup;cuda;algorithm;subgraph isomorphism problem;computer science	HPC	-3.2156210541956787	42.64696161503288	194504
53415e7571d49da9494c24f86777b7983b7b72ea	hash-based algorithms for discretized data	neighbor calculation;sorting;65k05;hash based algorithms;general purpose graphics processing units;68q25;mesh algorithms;remap;68q85;68w10	We explore the idea that all mesh operations in numerical methods can be implemented with efficient hash-based algorithms. The hash-based methods are presented with a view toward highly parallel implementations on both the CPU and GPU. A general set of applications, including sorting, neighbor calculation, remapping, and table look-up, demonstrate the practical value and several orders of magnitude speed-up of hash-based implementations.	algorithm;discretization	Rachel N. Robey;David Nicholaeff;Robert W. Robey	2013	SIAM J. Scientific Computing	10.1137/120873686	hash table;double hashing;parallel computing;hash function;perfect hash function;dynamic perfect hashing;quadratic probing;computer science;sorting;theoretical computer science;universal hashing;distributed computing;rolling hash;3sum;algorithm;hash tree	HPC	-1.1292182400298874	38.30355206612923	194656
32351ac4c7f81e8cb3cf8c35a47d20c82c6b7a26	a complexity o(1) priority queue for event driven molecular dynamics simulations	complexite;complejidad;molecular dynamics;calculation;complexity;sistema particulas;methode calcul;molecular dynamic simulation;dynamique moleculaire;technique calcul;priority queue;particle system;calculation methods;molecular dynamic;dinamica molecular;systeme particules	We propose and implement a priority queue suitable for use in event driven molecular dynamics simulations. All operations on the queue take on average O(1) time per collision. In comparison, previously studied queues for event driven molecular dynamics simulations require O(logN) time per collision for systems of N particles.	molecular dynamics;priority queue;simulation	Gerald Paul	2007	J. Comput. Physics	10.1016/j.jcp.2006.06.042	molecular dynamics;complexity;simulation;calculation;computer science;theoretical computer science;particle system;priority queue;algorithm	Theory	-3.5151866657124904	33.08069731411794	194720
3b450d8b5ccdb69831e881c1ff72bc7099da497d	a survey on big data processing infrastructure: evolving role of fpga	3d multi fpga system;big data processing;3d multi fpga systems;fpga;journal article;parallelism;pipelining;field programmable gate arrays;high performance;data analytics infrastructure	In today's commercial world, information is becoming a major economic resource thus leading to a statement - information is wealth. It is a technical challenge for computer systems in managing and analysing the large volumes of data coming from a variety of resources continuously over a period. Experts are in a mood of moving towards alternative hardware platforms for achieving high-speed data processing and analysis especially for streaming applications. In this paper: a) existing trends in big data processing and the necessary systems involved are studied by performing a survey on available platforms; b) recommended features and suitable hardware systems are proposed based on the operations involved in the processing. Investigation shows that, in combination with CPU and along with GPU, FPGA is a possible alternative. It can be a part of the heterogeneous platform featuring parallelism, pipelining and high performance for the operations involved in big data processing.	big data;field-programmable gate array	Krishna Chaitanya Nunna;Farhad Mehdipour;Antoine Trouvé;Kazuaki Murakami	2015	IJBDI	10.1504/IJBDI.2015.070599	parallel computing;real-time computing;computer science;theoretical computer science	ML	-0.645256020211597	44.87546790467698	194927
f2c49c9c2fdde2e2f5c01a40c7bd4bb926a2787b	simulation of systolic arrays on the connection machine	systolic arrays;systolic array;design optimization;algorithm verification;connection machine;design optimizaiton	The use of a programming model which extends naturally from the underlying hardware, greatly eases the design and implementation of simulators, especially for those systems that resemble the hardware in the paradigm of computation. Given the characteristics of systolic arrays, SIMD computers which employ the data parallel programming model provide an ideal environment. In this paper, we present a systolic array simulator, a simulation tool written for the Connection Machine *(model CM2), a SIMD machine with pomerful interprocessor communication capabilities. Especially as recent advances have automated the design, there is a need for a verification environment to prototype systolic arrays. Primarily a simulation tool, the systolic array simulator also helps identify inefficiencies and motivates optimal design prior to implementation in either custom VLSI or DSP systems. Currently, we are updating the tool to allow the simulation ofdynamic array reconfiguration algorithms under transient and permanent faul...	connection machine;simulation	Nariankadu D. Hemkumar;Joseph R. Cavallaro	1993	Simulation	10.1177/003754979306100302	computer architecture;parallel computing;real-time computing;multidisciplinary design optimization;systolic array;computer science;operating system;misd	Arch	1.6596069494206551	46.04242002007434	195026
88cf5b5445bbc50fc777cd8d588f4a8d5ddeb63f	boda-rtc: productive generation of portable, efficient code for convolutional neural networks on mobile computing platforms	libraries;linear algebra;convolutional codes;kernel;paper;neural networks;convolution;machine learning;deep learning;arm;mathematical software;computer science;mobile computing;opencl;hardware;cnn	The popularity of neural networks (NNs) spans academia [1], industry [2], and popular culture [3]. In particular, convolutional neural networks (CNNs) have been applied to many image based machine learning tasks and have yielded strong results [4]. The availability of hardware/software systems for efficient training and deployment of large and/or deep CNN models is critical for the continued success of the field [5] [1]. Early systems for NN computation focused on leveraging existing dense linear algebra techniques and libraries [6] [7]. Current approaches use low-level machine specific programming [8] and/or closed-source, purpose-built vendor libraries [9]. In this work, we present an open source system that, compared to existing approaches, achieves competitive computational speed while achieving significantly greater portability. We achieve this by targeting the vendor-neutral OpenCL platform [10] using a code-generation approach. We argue that our approach allows for both: (1) the rapid development of new computational kernels for existing hardware targets, and (2) the rapid tuning of existing computational kernels for new hardware targets. Results are presented for a case study of targeting the Qualcomm Snapdragon 820 mobile computing platform [11] for CNN deployment.	artificial neural network;autonomous car;computation;computer vision;convolutional neural network;high- and low-level;library (computing);linear algebra;machine learning;medical imaging;mobile computing;open-source software;opencl api;programmer;social media;software deployment;software portability;software system	Matthew W. Moskewicz;Forrest N. Iandola;Kurt Keutzer	2016	2016 IEEE 12th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)	10.1109/WiMOB.2016.7763217	convolutional code;parallel computing;kernel;simulation;computer science;theoretical computer science;linear algebra;operating system;machine learning;distributed computing;deep learning;convolution;programming language;mobile computing;arm architecture;algorithm	Mobile	2.096986759505183	42.95621164883133	195094
95ff12ab70c36b020ff82e926a556d53636ade3e	fast software implementation of quark on a 32-bit architecture	lightweight cryptography;intel galileo;quark family;hash functions;fast software implementation	Secure applications for the Internet of Things IoT are constantly increasing and many of them require some lightweight cryptographic algorithms. Most lightweight cryptographic algorithms were not designed to be efficient in software platforms. As a result the throughput in software of these algorithms is low on recent IoT devices. In this paper we present optimization techniques for improving the software implementation of the QUARK functions. QUARK is a family of lightweight hash functions that is efficient in hardware but its design was not oriented for software platforms. We obtained a reduction on the number of binary operations required in each iteration of QUARK, and by computing in parallel some internal functions we achieved a further speed up. In addition, we also present the results of our optimized implementations of S-QUARK and D-QUARK on the 32-bit Intel Galileo platform.	32-bit	Roberto Cabral;Julio López Hernandez	2015		10.1007/978-3-319-29078-2_7	security of cryptographic hash functions;parallel computing;real-time computing;computer science;theoretical computer science	Arch	8.161357851716566	44.861864640201304	195155
d22625863b71fa615f8f591f1aa482f017349d0e	connecting legendre with kummer and edwards		Scalar multiplication on Legendre form elliptic curves can be speeded up in two ways. One can perform the bulk of the computation either on the associated Kummer line or on an appropriate twisted Edwards form elliptic curve. This paper provides details of moving to and from between Legendre form elliptic curves and associated Kummer line and moving to and from between Legendre form elliptic curves and related twisted Edwards form elliptic curves. Further, concrete twisted Edwards form elliptic curves are identified which correspond to known Kummer lines at the 128-bit security level which provide very fast scalar multiplication on modern architectures supporting SIMD operations.	128-bit;computation;simd;scalar processor;twisted	Sabyasachi Karati;Palash Sarkar	2017	IACR Cryptology ePrint Archive		discrete mathematics;legendre polynomials;mathematics;computation;elliptic curve;security level;scalar multiplication;legendre form	Crypto	9.198033220430954	42.68861176721916	195241
ac0cd13ca7d0f5abe1482b6c857d35ce9971fc27	stacked filters stationary flow for hardware-oriented acceleration of deep convolutional neural networks		To address memory and computation resource limitations for hardware-oriented acceleration of deep convolutional neural networks (CNNs), we present a computation flow, stacked filters stationary flow (SFS), and a corresponding data encoding format, relative indexed compressed sparse filter format (CSF), to make the best of data sparsity, and simplify data handling at execution time. And we also propose a three dimensional Single Instruction Multiple Data (3D-SIMD) processor architecture which takes full advantage of these two features. Comparing with the state-of-the-art result (Han et al., 2016b), our method achieved 1.11× improvement in reducing the storage required by AlexNet, and 1.09× improvement in reducing the storage required by SqueezeNet, without loss of accuracy on the ImageNet dataset. Moreover, using this approach, chip area for logics handling irregular sparse data access can be saved.	artificial neural network;clustered file system;code;computation;computer memory;convolutional neural network;data access;data compression;han unification;imagenet;run time (program lifecycle phase);simd;sparse matrix;stationary process	Yuechao Gao;Nianhong Liu;Sheng Zhang	2018	CoRR		machine learning;convolutional neural network;parallel computing;chip;acceleration;computation;artificial intelligence;encoding (memory);sparse matrix;computer science;simd;microarchitecture	HPC	3.107534205650733	42.96705309397186	195327
00e0e28a33126dcaebcb359df0f86e81ee192d5f	high-performance fpga implementation of elliptic curve cryptography processor over binary field gf(2^163)		Elliptic curve cryptography (ECC) plays a vital role in passing secure information among different wireless devices. This paper presents a fast, high-performance hardware implementation of an ECC processor over binary field GF(2m) using a polynomial basis. A high-performance elliptic curve point multiplier (ECPM) is designed using an efficient finite-field arithmetic unit in affine coordinates, where ECPM is the key operation of an ECC processor. It has been implemented using the National Institute of Standards and Technology (NIST) recommended curves over the field GF(2163). The proposed design is synthesized in field-programmable gate array (FPGA) technology with the VHDL. The delay of ECPM in a modern Xilinx Kintex-7 (28-nm) technology is 1.06 ms at 306.48 MHz. The proposed ECC processor takes a small amount of resources on the FPGA and needs only 2253 slices without using any DSP slices. The proposed design provides nearly 50% better delay performance than recent implementations.	central processing unit;cost per mille;data security;elliptic curve cryptography;field-programmability;field-programmable gate array;mathematical optimization;multiplication algorithm;polynomial basis;vhdl	Md. Selim Hossain;Ehsan Saeedi;Yinan Kong	2016		10.5220/0005741604150422	elliptic curve digital signature algorithm;elliptic curve cryptography	EDA	8.999113278756266	44.65010765104398	195489
adbd51d82565f6d83a420a51ba7cf2931ada3d58	high-throughput cordic-based geometry operations for 3d computer graphics	arithmetic operation;kernel;computational geometry coprocessors computer graphics;3d computer graphics;application software;computer graphics;cordic;data stream;computational geometry;computer graphic equipment;index terms cordic;coprocessors;computer graphic;geometry transforms;computational modeling;streaming media;3d rotation;graphics processors;graphics processor index terms cordic 3d rotations vector normalization geometry transforms;arithmetic;bandwidth;graphics processor;high throughput;signal processing algorithms;computer graphic equipment computational geometry coprocessors computer graphics;cordic type primitive;vector normalization;geometry transforms graphics processor arithmetic operation 3d computer graphics cordic type primitive 3d rotation vector normalization;3d rotations;throughput	Graphics processors require strong arithmetic support to perform computational kernels over data streams. Because of the current implementation using the basic arithmetic operations, the algorithms are given in algebraic terms. However, since the operations are really of a geometric nature, it seems to us that more flexibility in the implementation is obtained if the description is given in a high-level geometrical form. As a consequence of this line of thought, this paper is an attempt to reconsider some kernels in a graphics processor to obtain implementations that are potentially more scalable than just replicating the modules used in conventional implementations. We present the formulation of representative 3D computer graphics operations in terms of CORDIC-type primitives. Then, we briefly outline a stream processor based on CORDIC-type modules to efficiently implement these graphic operations. We perform a rough comparison with current implementations and conclude that the CORDIC-based alternative might be attractive.	3d computer graphics;algorithm;cordic;central processing unit;graphics processing unit;high- and low-level;linear algebra;requirement;scalability;scheduling (computing);stream processing;throughput	Tomás Lang;Elisardo Antelo	2005	IEEE Transactions on Computers	10.1109/TC.2005.53	high-throughput screening;throughput;computer architecture;application software;parallel computing;kernel;computational geometry;computer science;unit vector;theoretical computer science;operating system;computer graphics;computational model;bandwidth;coprocessor;3d computer graphics;algebra;cordic	Visualization	6.790020164919292	41.775397237612005	195508
f5242ba2110228980ee0ee0be18d8a39165c46d5	powerai ddl		As deep neural networks become more complex and input data-sets grow larger, it can take days or even weeks to train a deep neural network to the desired accuracy. Therefore, distributed Deep Learning at a massive scale is a critical capability, since it offers the potential to reduce the training time from weeks to hours. In this paper, we present a software-hardware co-optimized distributed Deep Learning system that can achieve near-linear scaling up to hundreds of GPUs. The core algorithm is a multi-ring communication pattern that provides a good tradeoff between latency and bandwidth and adapts to a variety of system configurations. The communication algorithm is implemented as a library for easy use. This library has been integrated into Tensorflow, Caffe, and Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC servers (256 GPUs) in about 7 hours to an accuracy of 33.8% validation accuracy. Microsoft’s ADAM [10] and Google’s DistBelief [11] results did not reach 30% validation accuracy for Imagenet 22K. Compared to Facebook’s recent paper [1] on 256 GPU training, we use a different communication algorithm, and our combined software and hardware system offers better communication overhead for Resnet50. A PowerAI DDL enabled version of Torch completed 90 epochs of training on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC servers (256 GPUs).	algorithm;application domain;artificial neural network;automation;data parallelism;deep learning;graphics processing unit;image scaling;overhead (computing);recurrent neural network;tensorflow;torch;turnkey;usability	Minsik Cho;Ulrich Finkler;Sameer Kumar;David S. Kung;Vaibhav Saxena;Dheeraj Sreedhar	2017	CoRR			HPC	2.112753774532115	42.87771719585424	195734
7ed4c562251b7117b878c22e52bfc4d0bc28edf9	pipeline/parallel algorithms for the jacobian and inverse dynamics computations	parallel algorithm;concurrent computing;inverse dynamics;chaos;parallel algorithms jacobian matrices concurrent computing pipeline processing equations sampling methods manipulator dynamics delay effects end effectors chaos;delay effects;manipulator dynamics;time delay;parallel computer;sampling methods;jacobian matrices;pipeline processing;end effectors;parallel algorithms	Algorithms have been developed for the Jacobian and Inverse Dynamics analyses in order to implement them on pipeline/parallel computing arrays. The results indicate that the sampling rate in either case may be significantly increased by adding processors to a pipelined array while, on the other hand, the compute time delay decreases very little. The results further show that a parallel structure is needed if the compute time is to be significantly reduced.	computation;inverse dynamics;jacobian matrix and determinant;parallel algorithm	David E. Orin;H. H. Chao;Karl W. Olson;W. W. Schrader	1985		10.1109/ROBOT.1985.1087343	mathematical optimization;concurrent computing;computer science;theoretical computer science;distributed computing;parallel algorithm	Robotics	2.8041181271517495	38.30384664028704	195772
01cab5225c659369ce71a09152b119ca5e11df57	integrating software pipelining and graph scheduling for iterative scientific computations	directed acyclic graph;distributed memory;scientific application;scientific computing;software pipelining;task graphs;sparse matrix	Graph scheduling has been shown eeective for solving irregular problems represented as directed acyclic graphs(DAGs) on distributed memory systems. Many scientiic applications can also be modeled as iterative task graphs(ITGs). In this paper, we model the SOR computation for solving sparse matrix systems in terms of ITGs and address the optimization issues for scheduling ITGs when communication overhead is not zero. We present an approach that incorporates techniques of software pipelining and graph scheduling. We demonstrate the eeectiveness of our approach in mapping SOR computation and compare it with the multi-coloring method.	computation;directed acyclic graph;distributed memory;graph coloring;iterative method;mathematical optimization;overhead (computing);pipeline (computing);scheduling (computing);software pipelining;sparse matrix	Cong Fu;Tao Yang;Apostolos Gerasoulis	1995		10.1007/3-540-60321-2_11	software pipelining;parallel computing;theoretical computer science;distributed computing;graph	HPC	-3.137655351163729	39.11519939607112	195986
2e3de37d5135059c391c6601bdeb3f832b161f70	adaptive strassen's matrix multiplication	linear algebra;fast algorithms;perforation;data reuse;adaptive algorithm;matrix multiplications;numerical analysis;fast algorithm;matrix multiplication;memory hierarchy;high performance	Strassen's matrix multiplication (MM) has benefits with respect to any (highly tuned) implementations of MM because Strassen's reduces the total number of operations. Strassen achieved this operation reduction by replacing computationally expensive MMs with matrix additions (MAs). For architectures with simple memory hierarchies, having fewer operations directly translates into an efficient utilization of the CPU and, thus, faster execution. However, for modern architectures with complex memory hierarchies, the operations introduced by the MAs have a limited in-cache data reuse and thus poor memory-hierarchy utilization, thereby overshadowing the (improved) CPU utilization, and making Strassen's algorithm (largely) useless on its own.  In this paper, we investigate the interaction between Strassen's effective performance and the memory-hierarchy organization. We show how to exploit Strassen's full potential across different architectures. We present an easy-to-use adaptive algorithm that combines a novel implementation of Strassen's idea with the MM from automatically tuned linear algebra software (ATLAS) or GotoBLAS. An additional advantage of our algorithm is that it applies to any size and shape matrices and works equally well with row or column major layout. Our implementation consists of introducing a final step in the ATLAS/GotoBLAS-installation process that estimates whether or not we can achieve any additional speedup using our Strassen's adaptation algorithm. Then we install our codes, validate our estimates, and determine the specific performance.  We show that, by the right combination of Strassen's with ATLAS/GotoBLAS, our approach achieves up to 30%/22% speed-up versus ATLAS/GotoBLAS alone on modern high-performance single processors. We consider and present the complexity and the numerical analysis of our algorithm, and, finally, we show performance for 17 (uniprocessor) systems.	adaptive algorithm;analysis of algorithms;atlas autocode;central processing unit;code;computer performance;gotoblas;linear algebra;matrix multiplication;memory hierarchy;memory management;numerical analysis;speedup;strassen algorithm;uniprocessor system	Paolo D'Alberto;Alexandru Nicolau	2007		10.1145/1274971.1275010	parallel computing;matrix multiplication;computer science;theoretical computer science;linear algebra;coppersmith–winograd algorithm;algorithm;strassen algorithm;algebra	HPC	-3.9641171246937748	41.17090002827978	196051
3fc4d976b7237692fce00285542ff72c14cf78da	install-time system for automatic generation of optimized parallel sorting algorithms		Sorting is a fundamental algorithm used extensively in computer science as an intermediate step in many applications. The performance of sorting algorithms is heavily influenced by the type of data being sorted, and the machine being used. To assist in obtaining portable performance for sorting algorithms, we propose an install-time system for automatically constructing sequential and parallel sorts that are highly tuned for the target architecture. Our system has two steps: first a hybrid sequential divide-and-conquer sort is constructed and then this algorithm is parallelized using a shared work-queue model. To evaluate our system, we compare automatically generated sorting algorithms to sequential and parallel versions of the C++STL sort. The generated sorts are shown to be competitive with STL sort on sequential systems and to outperform the parallel STL sort on a 4 processor Xeon server.	adaptive sort;computer science;hyper-threading;parallel computing;server (computing);sorting algorithm;speedup	Marek Olszewski;Michael J. Voss	2004			computer science;parallel computing;distributed computing;sorting algorithm	Theory	-2.203409545912146	43.31848405094025	196442
47dc2f5100988ca8c9cf7b071671c7a9e7105084	a fast microprogrammed digital filter supporting early signal processing research	digital signal processors;bit sliced architecture;digital filters;low sensitivity realizations;microprogramming;computational efficiency	In the 1970s research and teaching of digital signal processing was started in several universities in Scandinavia. Special emphasis in the research was on digital filter structures implementable on emerging digital hardware. On the theoretical side, the development of computationally efficient, low-sensitivity digital filter structures was widely investigated. Realization of these novel transfer functions was also important in order to verify the efficiency of the filters in practice. The latest commercial microprocessors indicated that real time signal processing covering the audio range of signals was becoming realistic. A microprogrammable digital signal processor based on high speed bit-sliced architecture was designed at Helsinki University of Technology. With this hardware, the performance of computationally efficient, low-sensitivity digital filter structures was tested. Algorithm research with emphasis on implementation aspects had a great impact on the readiness of Finnish electronics and telecommunications industry to start using programmable digital hardware. Early DSP applications in telecommunications industry were released already in late 1970s. Ten years later, when the first GSM phones were under development, Finnish industry was on the leading edge of applying digital signal processors in a number of applications.		Olli Simula;Yrjö Neuvo	2014		10.1007/978-3-319-17145-6_37	electronic engineering;parallel computing;computer science;theoretical computer science	Robotics	8.16285882324068	40.89941508381215	196563
16a8f2b661d25f9edd34db5f853091ae542bc7bd	micmr: an efficient mapreduce framework for cpu-mic heterogeneous architecture	simd;many integrated core;phoenix;mapreduce;hadoop	With the high-speed development of processors, coprocessor-basedMapReduce is widely studied. In this paper, we propose micMR, an efficient MapReduce framework for CPU–MIC heterogeneous architecture. micMR mainly provides the following new features. First, the two-level split and the SIMD friendly map are designed for utilizing the Vector Process Units on MIC. Second, heterogeneous pipelined reduce is developed for improving the efficiency of resource utilization. Third, a memory management scheme is designed for accessing <key, value> pairs in both the host and the MIC memory efficiently. In addition, optimization techniques, including load balancing, SIMD hash, and asynchronous task transfer, are designed for achieving more speedups. We have developed micMR not only in a single node with CPU and MIC but also in a CPU–MIC heterogeneous cluster. The experimental results show that micMR is up to 8.4x and 45.8x faster than Phoenix++, a high-performance MapReduce system for symmetric multiprocessing system, and up to 2.0x and 5.1x faster than Hadoop in a CPU–MIC cluster. © 2016 Elsevier Inc. All rights reserved.	apache hadoop;central processing unit;coprocessor;load balancing (computing);mapreduce;mathematical optimization;memory management;pipeline (computing);simd;symmetric multiprocessing	Wenzhu Wang;Yusong Tan;Qingbo Wu;Yaoxue Zhang	2016	J. Parallel Distrib. Comput.	10.1016/j.jpdc.2016.04.007	parallel computing;real-time computing;simd;computer science;operating system	DB	-4.140692476082375	41.90488795067004	196624
e93ae55780ff020381971defde6fead41031566d	igmm-based co-localization of mobile users with ambient radio signals	clustering co location gaussian mixture model mobile computing;smart phones;internet of things;servers;mobile communication servers internet of things smart phones ieee 802 11 standard real time systems clustering algorithms;mobile communication;clustering algorithms;ieee 802 11 standard;real time systems	Co-localization of mobile users combines methods of detecting nearby users and providing them interesting and useful services or information. By exploiting the massive use of smartphones, nearby users can be co-localized using only their captured ambient radio signals. In this paper, we propose a real-time co-localization system, in a centralized manner, that leverages co-located users with high accuracy. We exploit the similarity of radio frequency measurements from users’ mobile terminal. We do not require any further information about them. Our co-localization system is based on a nonparametric Bayesian method called infinite Gaussian mixture model that allows the model parameters to change with observed input data. In addition, we propose a modified version of Gibbs sampling technique with an average similarity threshold to better fit user’s group. We design our system in a completely centralized manner. Hence, it enables the network to control and manage the formation of the users’ groups. We first evaluate the performance of our proposal numerically. Then, we carry out an extensive experiment to demonstrate the feasibility, and the efficiency of our approach with data sets from a real-world setting. Results on experiment favor our algorithm over the state-of-the-art community detection-based clustering method.	algorithm;bayesian network;centralized computing;cluster analysis;colocation centre;experiment;gibbs sampling;mixture model;mobile phone;numerical analysis;radio frequency;real-time clock;sampling (signal processing);sensor;server (computing);smartphone	Pedro M. Varela;Jihoon Hong;Tomoaki Ohtsuki;Xiaoqi Qin	2017	IEEE Internet of Things Journal	10.1109/JIOT.2016.2568258	real-time computing;mobile search;simulation;mobile telephony;human–computer interaction;telecommunications;computer science;operating system;cluster analysis;world wide web;computer security;internet of things;server;computer network	Mobile	0.23754275416262893	34.434445908847344	196811
e3bc4da566000f2fb9aeed6e548d3178460e496f	a fast pipelined multi-mode des architecture operating in ip representation	traitement pipeline;software;optimisation;systeme unix;calculateur embarque;ip representation;optimizacion;integrated circuit;logiciel;metodo operatorio;implementation;unix system;circuito integrado;coprocessor;tamper resistance;hardware architecture;etat actuel;temps calcul;smartcard;mode operatoire;coprocesador;data encryption standard;state of the art;coprocesseur;operating mode;boarded computer;logicial;estado actual;optimization;procesador;sistema unix;tiempo computacion;computation time;processeur;implementacion;data encryption standard des;modes of operation;calculador embarque;triple des;processor;pipeline;circuit integre;pipeline processing;software implementation	The Data Encryption Standard (DES) is a cipher that is still used in a broad range of applications, from smartcards, where it is often implemented as a tamperresistant embedded co-processor, to PCs, where it is implemented in software (for instance to compute crypt(3) on UNIX platforms.) To the authors’ knowledge, implementations of DES published so far are based on the straightforward application of the NIST standard. This article describes an innovative architecture that features a speed increase for both hardware and software implementations, compared to the state-of-the-art. For example, the proposed architecture, at constant size, is about twice as fast as the state-of-the-art for 3DES-CBC. The first contribution of this article is an hardware architecture that minimizes the computation time overhead caused by key and message loading. The second contribution is an optimal chaining of computations, typically required when “operation modes” are used. The optimization is made possible by a novel computation paradigm, called “IP representation”.	8-bit;block cipher mode of operation;cipher;computation;coprocessor;critical path method;crypt (c);embedded system;encryption;input/output;mathematical optimization;overhead (computing);parallel computing;password cracking;pipeline (computing);programming paradigm;smart card;time complexity;triple des;unix;very-large-scale integration	Sylvain Guilley;Philippe Hoogvorst;Renaud Pacalet	2007	Integration	10.1016/j.vlsi.2006.06.004	embedded system;smart card;triple des;parallel computing;computer science;operating system;integrated circuit;hardware architecture;implementation;pipeline;tamper resistance;coprocessor	Arch	7.3582253708135985	44.74275739479849	196950
5ee684795d9644f5c21d91380c60781bbb0d5482	memory support design for lu decomposition on the starbridge hyper-computer	linear algebra;storage management;computational chemistry;matrix decomposition hardware field programmable gate arrays equations algorithm design and analysis parallel processing design engineering aerospace engineering linear algebra physics;matrix decomposition;memory architecture;linear algebra algorithm;starbridge hyper computer;data transfer hardware memory support design starbridge hyper computer lu matrix decomposition linear algebra algorithm;field programmable gate arrays;linear equations;system of equations;memory support design;storage management field programmable gate arrays matrix decomposition memory architecture;hardware implementation;data transfer hardware;data transfer;lu matrix decomposition	LU matrix decomposition is a linear algebra algorithm used to reduce the complexity required to solve a large system of linear equations. Large systems of equations frequently need to be solved in physics, engineering, and computational chemistry. In the hardware implementation of such LU algorithms supporting modules must be included which handle the transfer of memory between the disk and processing nodes. This paper looks at the data transfer hardware which supports an implementation of a block-based LU algorithm on a multi-FPGA system. Preliminary results are provided which show the required areas and latencies of these designs	algorithm;block size (cryptography);central processing unit;computational chemistry;distributed memory;field-programmable gate array;lu decomposition;linear algebra;linear equation;memory management;system of linear equations;type system	Seth Young;Arvind Sudarsanam;Aravind Dasu;Thomas Hauser	2006	2006 IEEE International Conference on Field Programmable Technology	10.1109/FPT.2006.270307	system of linear equations;embedded system;parallel computing;computer science;theoretical computer science;linear algebra;operating system;linear equation;numerical linear algebra;matrix decomposition;algorithm;field-programmable gate array	EDA	-3.0703313450643375	38.63904000649266	197533
9e3da96f4d4915b6b55c08069572119c4a908bc9	compiling affine nested loops	calcul matriciel;modelizacion;eficacia sistema;optimisation;decomposition;systeme multiprocesseur memoire repartie;optimizacion;complexite calcul;programacion paralela;nested loops;performance systeme;mapping heuristic;parallel programming;system performance;algorithme;modelisation;algorithm;complejidad computacion;local community;computational complexity;sistema multiprocesador memoria distribuida;parallel computer;access graph;optimization;systeme parallele;distributed memory multiprocessor system;parallel system;matrix calculus;descomposicion;communication;modeling;comunicacion;calculo de matrices;sistema paralelo;programmation parallele;algoritmo	Minimizing communication overhead when mapping aane loop nests onto distributed memory parallel computers (DMPCs) is a key problem with regard to performance , and many authors have dealt with it. All communications are not equivalent. Local communications (translations), simple communications (horizontal or vertical ones), or structured communications (broadcasts, gathers, scatters, or reductions) are performed much faster than general aane communications onto DMPCs. In this paper, we recall the mapping heuristic given by Dion and Robert which consists in minimizing the number of nonlocal communications and we focus on the next step: as it is generally impossible to obtain a communication local mapping, we show how to optimize residual general communications using structured communications or decompositions into small sequences of simple communications.	computer;distributed memory;heuristic;overhead (computing);parallel computing;quantum nonlocality	Michèle Dion;Cyril Randriamaro;Yves Robert	1996	J. Parallel Distrib. Comput.	10.1006/jpdc.1996.0139	parallel computing;systems modeling;nested loop join;matrix calculus;computer science;artificial intelligence;theoretical computer science;computer performance;decomposition;computational complexity theory;algorithm	HPC	-2.173607889076186	36.38116117055755	197683
d3f1d185a99e43bb4bed99c3d34736ddddfe98f4	efficient time-space mappings of nested loops onto multidimensional systolic arrays with a flexible buffer scheme	processing element;fractals;iteration level mappings time space mappings nested loops multidimensional systolic arrays flexible buffer scheme processing elements data tokens valid mappings statement level mappings;time space mappings;chaos;routing;very large scale integration;systolic arrays;nested loops;valid mappings;systolic arrays iterative methods parallel algorithms;statement level mappings;systolic array;processing elements;iterative methods;computational modeling;conductivity;necessary and sufficient condition;multidimensional systems systolic arrays fault tolerance routing very large scale integration fractals chaos conductivity delay computational modeling;fault tolerance;iteration level mappings;multidimensional systolic arrays;flexible buffer scheme;data tokens;space mapping;multidimensional systems;parallel algorithms	The task of mapping a nested loop algorithm onto a multidimensional systolic array is considered. A buffer structure for the processing elements (PEs) that allows the data tokens to arrive at the PE earlier than when they are needed is proposed. Necessary and sufficient conditions for valid mappings using this buffer structure are then given. A refinement technique for deriving efficient statement level mappings from iteration level mappings is then proposed. >	systolic array	R. Varadarajan;F. Augustine	1993	IEEE Trans. VLSI Syst.	10.1109/92.250204	mathematical optimization;routing;fault tolerance;discrete mathematics;space mapping;nested loop join;fractal;multidimensional systems;systolic array;computer science;theoretical computer science;conductivity;mathematics;parallel algorithm;iterative method;very-large-scale integration;computational model	HPC	4.248702537922935	38.44545055204114	197918
44a967b84b538169a9fd8b13801d5ec131495ff7	high performance linear equation solver using nvidia gpus	linear algebra;random access memory;kernel;linear system of equations;instruction sets graphics processing unit equations mathematical model random access memory matrices kernel;storage management;computer graphic equipment;coprocessors;storage management computer graphic equipment coprocessors finite element analysis linear algebra parallel architectures shared memory systems;shared memory systems;matrices;parallel architectures;global memory system high performance linear equation solver linear algebra aerospace aeronautics solid mechanics fluid dynamics oil research gaussian elimination forward elimination back substitution massive parallelism nvidia gpu architecture memory feature nvidia tesla c1060 gpu shared memory system;mathematical model;graphic processing unit;gaussian elimination;fluid dynamics;finite element analysis;linear equations;graphics processing unit;shared memory system;high performance;direct method;instruction sets	The solution of a linear system of equations constitutes an important part in the field of linear algebra that is widely used in industries like aerospace, aeronautics, solid mechanics, fluid dynamics, oil research and numerous others. A direct method for solving these equations is Gaussian Elimination, which consists of forward elimination and back substitution. We have tailored this method to take advantage of the massive parallelism offered by NVIDIA GPU architectures. Thorough evaluations have been performed for variants of our implementation that exploit different memory features on an NVIDIA Tesla C1060 GPU. Compared to a serial implementation on an Intel Core I7, the execution time for forward elimination on the GPU is reduced by a factor of 183X when using both global and shared memory systems, and by a factor of 185X when using only global memory.	algorithm;cuda;direct method in the calculus of variations;finite element method;gaussian elimination;graphics processing unit;integrated development environment;iterative method;jacobi method;linear algebra;linear equation;linear system;nvidia tesla;parallel computing;parallel programming model;run time (program lifecycle phase);shared memory;solver;speedup;system of linear equations;throughput;triangular matrix	Yoon Kah Leow;Ali Akoglu;Ibrahim Guven;Erdogan Madenci	2011	2011 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)	10.1109/AHS.2011.5963960	direct method;system of linear equations;embedded system;gaussian elimination;parallel computing;kernel;computer hardware;computer science;theoretical computer science;linear algebra;operating system;finite element method;instruction set;mathematical model;linear equation;matrix;coprocessor;fluid dynamics	HPC	-4.225894405181906	38.56020046888224	197948
39dca7d250e5a72d0dddbf23aecc0d2f01998013	domain decomposition based high performance parallel computing	domain decomposition;distributed computing;navier stokes;finite element;machine vision;parallel computer;newton method;high performance	The study deals with the parallelization of finite element based Navier-Stokes codes using domain decomposition and state-ofart sparse direct solvers. There has been significant improvement in the performance of sparse direct solvers. Parallel sparse direct solvers are not found to exhibit good scalability. Hence, the parallelization of sparse direct solvers is done using domain decomposition techniques. A highly efficient sparse direct solver PARDISO is used in this study. The scalability of both Newton and modified Newton algorithms are tested.	additive schwarz method;algorithm;code;computation;domain decomposition methods;finite element method;http 404;langrisser schwarz;navier–stokes equations;network-attached storage;newton;newton's method;parallel computing;scalability;solver;sparse matrix;time complexity;utility functions on indivisible goods	Mandhapati P. Raju;Siddhartha Khaitan	2009	CoRR		mathematical optimization;parallel computing;machine vision;computer science;theoretical computer science;finite element method;sparse approximation;distributed computing;domain decomposition methods;newton's method	HPC	-3.421319366721252	37.99953820819962	198141
3d8ded08b5f72a9c8afb41da165d484cd4529e62	prngs for masking applications and their mapping to evolvable hardware		This paper proposes the use of evolutionary computation for the design and optimization of lightweight Pseudo Random Number Generators (PRNGs). In this work, we focus on PRNGs that are suitable for generating masks and secret shares. Such generators should be light-weight and have a high throughput with good statistical properties. As a proof-of-concept, we present three novel hardware architectures that have an increasing level of prediction resistance and an increasing level of reconfigurability at run-time. We evaluate the three architectures on Zynq, Virtex-6, and ASIC platforms and compare the occupied resources and the throughput of the obtained designs. Finally, we use the Spartan-6 platform for the evaluation of the masked implementation where the masks are obtained via our PRNG.	evolvable hardware	Stjepan Picek;Bohan Yang;Vladimir Rozic;Jo Vliegen;Jori Winderickx;Thomas De Cnudde;Nele Mentens	2016		10.1007/978-3-319-54669-8_13	evolutionary computation;throughput;application-specific integrated circuit;parallel computing;real-time computing;computer science;evolvable hardware;hardware architecture;block cipher;pseudorandom number generator;reconfigurability	Crypto	8.382870212859421	45.48866182588476	198229
5ff2469982459e7bf8b60139db31eb1a66864987	critical node detection problem solving on gpu and in the cloud	electronic mail;approximation algorithms;graphics processing units approximation algorithms runtime parallel algorithms data structures electronic mail;runtime;data structures;graphics processing units;parallel algorithms	The Critical Node Detection Problem (CNDP) is a well-known NP-complete, graph-theoretical problem with many real-world applications in various fields such as social network analysis, supply-chain network analysis, transport engineering, network immunization, and military strategic planning. We present the first parallel algorithms for CNDP solving in general, and for fast, approximated CND on GPU and in the cloud in particular. Finally, we discuss results of our experimental performance analysis of these solutions.	approximation algorithm;cloud computing;graphics processing unit;karp's 21 np-complete problems;np-completeness;parallel algorithm;problem solving;profiling (computer programming);social network analysis;supply chain network	Cholpon Degenbaeva;Matthias Klusch	2015	2015 IEEE 17th International Conference on High Performance Computing and Communications, 2015 IEEE 7th International Symposium on Cyberspace Safety and Security, and 2015 IEEE 12th International Conference on Embedded Software and Systems	10.1109/HPCC-CSS-ICESS.2015.8	parallel computing;data structure;computer science;theoretical computer science;operating system;analysis of parallel algorithms;distributed computing;parallel algorithm;programming language;approximation algorithm	Embedded	-1.9417449656256207	33.686622636791284	198385
5ed5d06542d7adc808440375c5248f23e59229ad	a brief survey of advances in particle swarm optimization on graphic processing units	particle swarm optimisation electronic engineering computing floating point arithmetic graphics processing units multi agent systems multiprocessing systems parallel processing;graphics processing units kernel pricing;multiagent methods particle swarm optimization graphic processing units gpu hardware environment parallel implementation bioinspired algorithms multicore cpu independent cores parallel single instruction multiple data simd devices floating point operations per second flops pso;multi agent systems;graphics processing units;electronic engineering computing;multiprocessing systems;floating point arithmetic;survey particle swarm optimization general purpose gpus research applications;particle swarm optimisation;parallel processing	In the last few years, the Graphic Processing Units (GPUs) emerged as an exciting new hardware environment available for a truly parallel implementation and execution of Nature and Bio-inspired Algorithms. In contrast to common multicore CPUs that contain up to tens of independent cores, the GPUs represent a massively parallel single-instruction multiple-data (SIMD) devices that can nowadays reach peak performance of hundreds and thousands of giga FLOPS (floating-point operations per second). Nature and Bio-inspired Algorithms often adopt populational problem solving approaches and implement parallel optimization strategies in which group or groups of candidate solutions search for optimal solutions. Swarm Intelligence and Particle Swarm Optimization (PSO) in particular can be seen as multiagent methods in which the interaction of simple independent agents yields intelligent collective behavior. Such algorithms especially fit to the architecture of the GPUs. This survey provides a brief overview of the latest state-of-the-art research on the design, implementation, and applications of PSO-based methods on the GPUs.	agent-based model;algorithm;central processing unit;flops;graphics processing unit;mathematical optimization;multi-core processor;paradiseo;particle swarm optimization;phase-shift oscillator;problem solving;simd;swarm intelligence	Pavel Krömer;Jan Platos;Václav Snásel	2013	2013 World Congress on Nature and Biologically Inspired Computing	10.1109/NaBIC.2013.6617859	parallel processing;computer architecture;parallel computing;computer science;floating point;theoretical computer science	HPC	-0.2788197819311488	40.66915322936349	198555
6a1059595135c80f7e12d632c0c2e7808b33afe3	bcyclic: a parallel block tridiagonal matrix cyclic solver	parallel computing;plasma temperature;classical and quantum mechanics general physics;high temperature;calculation;three dimensional;methode calcul;alta temperatura;tridiagonal matrix;dense blocks;equilibre mhd;algorithme;thomas algorithm;physics;factorization;mhd equilibrium;multiple right hand sides;technique calcul;storage block tridiagonal block solver cyclic reduction mpi;matriz tridiagonal;calculation methods;parallel computer;factorisation;haute temperature;algorithms;block matrix;temperature plasma;matrice tridiagonale;cyclic reduction	A block tridiagonal matrix is factored with minimal fill-in using a cyclic reduction algorithm that is easily parallelized. Storage of the factored blocks allows the application of the inverse to multiple right-hand sides which may not be known at factorization time. Scalability with the number of block rows is achieved with cyclic reduction, while scalability with the block size is achieved using multithreaded routines (OpenMP, GotoBLAS) for block matrix manipulation. This dual scalability is a noteworthy feature of this new solver, as well as its ability to efficiently handle arbitrary (non-powers-of-2) block row and processor numbers. Comparison with a state-of-the art parallel sparse solver is presented. It is expected that this new solver will allow many physical applications to optimally use the parallel resources on current supercomputers. Example usage of the solver in magneto-hydrodynamic (MHD), three-dimensional equilibrium solvers for high-temperature fusion plasmas is cited. 2010 Elsevier Inc. All rights reserved.	algorithm;block size (cryptography);cyclic reduction;giant magnetoresistance;gotoblas;openmp;parallel computing;scalability;solver;sparse matrix;supercomputer;thread (computing)	Steven P. Hirshman;Kalyan S. Perumalla;Vickie E. Lynch;Raul Sánchez	2010	J. Comput. Physics	10.1016/j.jcp.2010.04.049	combinatorics;theoretical computer science;mathematics;factorization;algorithm;algebra	HPC	-2.5838137456214536	38.136196643431205	198599
2ca07a35416eb0c7206aed93b35968710e0a1341	a parallel rectangle intersection algorithm on gpu+cpu	intersection checking phase;optimisation;kernel;parallel algorithm;iso oriented rectangles;computer graphic equipment;data partition;gpu;parallel rectangle intersection algorithm;pri gc;coprocessors;cuda;arrays;sequential algorithms;instruction sets graphics processing unit algorithm design and analysis partitioning algorithms heuristic algorithms kernel arrays;gpu plus cpu;graphics processing units parallel rectangle intersection algorithm gpu plus cpu iso oriented rectangles data partition massive output pri gc mapping phase intersection checking phase optimization techniques rectangles reordering data compressing encoding execution overlapping sequential algorithms;heuristic algorithms;graphics processing units;optimization techniques;rectangle intersection;optimisation computer graphic equipment coprocessors;parallel algorithms rectangle intersection cuda;mapping phase;cpu;graphics processing unit;execution overlapping;algorithm design and analysis;massive output;data compressing encoding;partitioning algorithms;instruction sets;parallel algorithms;rectangles reordering	In this paper, we investigate efficient algorithms and implementations using GPU plus CPU to solve the rectangle intersection problem on a plane. The problem is to report all intersecting pairs of iso-oriented rectangles, whose parallelization on GPUs poses two major computational challenges: data partition and the massive output. The algorithm we presented is called PRI-GC, Parallel Rectangle Intersection algorithm on GPU+CPU, which consists of two phases: mapping and intersection-checking. In the mapping phase, rectangles are hashed into different subspaces (called cells) to reduce the unnecessary intersection checking for far-apart rectangles. In the intersection-checking phase, pairs of rectangles within the same cell are examined in parallel, and the intersecting pairs of rectangles are reported. Several optimization techniques, including rectangles re-ordering, output data compressing/encoding, and the execution overlapping of GPU and CPU, are applied to enhance the performance. We had evaluated the performance of PRI-GC and the result shows over 30x speedup against two well-implemented sequential algorithms on single CPU. The effectiveness of each optimization technique for this problem was evaluated as well. Several parameters, including different degrees of rectangle coverage, different block sizes, and different cell sizes, were also experimented to explore their influences on the performance of PRI-GC.	block size (cryptography);central processing unit;computation;computer memory;data access;data compression;garbage collection (computer science);graphics processing unit;hash function;hybrid system;input/output;intersection algorithm;load balancing (computing);mathematical optimization;national supercomputer centre in sweden;parallel computing;scalability;shared memory;speedup	Shih-Hsiang Lo;Che-Rung Lee;Yeh-Ching Chung;I-Hsin Chung	2011	2011 11th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing	10.1109/CCGrid.2011.13	parallel computing;computer science;theoretical computer science;operating system;distributed computing;parallel algorithm	Arch	-0.7283236651236914	42.043967252510846	198896
b258491519955ae5131feb897f04a95907a019f4	an identity parareal method for temporal parallel computations		A new simplified definition of time-domain parallelism is introduced for explicit time evolution calculations, and is implemented on parallel machines with bucket-brigade type communications. By the use of an identity operator instead of introducing an approximate solver, a recurrence formula for the parareal-in-time algorithm is much simplified. In spite of such a simple definition, it is applicable to many of explicit time-evolution calculations. In addition, this approach overcomes several drawbacks known in the original parareal-in-time method. In order to implement this algorithm on parallel machines, a parallel bucket-brigade interface is introduced, which reduces programming and tuning costs for complicated space-time parallel programs.		Toshiya Takami;Daiki Fukudome	2013		10.1007/978-3-642-55224-3_7	parallel computing;computer science;theoretical computer science;algorithm	ML	-3.513735035986344	36.30099950432082	199343
fbe314bf147e21c5d7d18de945073c2995c2a019	efficient inverse kinematics algorithm based on conformal geometric algebra - using reconfigurable hardware	virtual human;hardware accelerator;computer algebra system;efficient implementation;inverse kinematics;computer animation;optimal algorithm;hardware implementation;reconfigurable hardware;geometric algebra	This paper presents a very efficient approach for algorithms developed based on conformal geometric algebra using reconfigurable hardware. We use the inverse kinematics of the arm of a virtual human as an example, but we are convinced that this approach can be used in a wide field of computer animation applications. We describe the original algorithm on a very high geometrically intuitive level as well as the resulting optimized algorithm based on symbolic calculations of a computer algebra system. The main focus then is to demonstrate our approach for the hardware implementation of this algorithm leading to a very efficient implementation.	algorithm;computer algebra system;computer animation;computer graphics;conformal geometric algebra;field-programmable gate array;high-level programming language;inverse kinematics;maple;mathematical optimization;program optimization;reconfigurable computing;symbolic computation;virtual actor	Dietmar Hildenbrand;Holger Lange;Florian Stock;Andreas Koch	2008			geometric algebra;computer architecture;hardware acceleration;reconfigurable computing;computer science;artificial intelligence;theoretical computer science;inverse kinematics;computer animation	Graphics	3.1262803232332215	39.620278212875576	199432
80790d1a5eb9e8c94b0641e5fbff4a6bf716b9d3	a multiplier-less fpga core for image algebra neighbourhood operations	multiplication operator;canonic signed digit;high level synthesis field programmable gate arrays electronic data interchange digital arithmetic;chip;high level synthesis;field programmable gate arrays algebra hardware pixel image processing parallel processing convolution computer science computer applications acceleration;design and implementation;word length;digital arithmetic;field programmable gate arrays;electronic data interchange;xilinx xc4000 chips multiplier less fpga core image algebra neighbourhood operations high level generator window size window coefficients input pixel word length image size canonical signed digit representation shift and add operations edif netlists high level descriptions	This paper presents the design and implementation of a high-level generator of optimised FPGA configurations for Image Algebra (IA) neighbourhood operations. These configurations are parameterised and scaleable in terms of the IA operation itself the window size, the window coefficients, the input pixel word length and the image size. The window coefficients of the neighbourhood operations are represented as sum/subtract of power of twos in Canonical Signed Digit (CSD) representation, which means that the usually costly multiplication operation can be easily implemented using a small number of simple shift-and-add operations, leading to considerable hardware savings. EDIF netlists are generated automatically from high-level descriptions of the IA operations in /spl sim/1 sec. These are specifically optimised for Xilinx XC4000 chips, although implementations for other targets can also be easily realised.	field-programmable gate array	Khaled Benkrid	2002		10.1109/FPT.2002.1188695	chip;embedded system;multiplication operator;computer hardware;computer science;theoretical computer science;operating system;electronic data interchange;high-level synthesis;field-programmable gate array	DB	7.001066345226149	46.20311167232352	199601
c8ada0c8bc9f026acf0dd653fe3b3f048725c1bb	oscillatory neural networks based on tmo nano-oscillators and multi-level rram cells	oscillators resistance neurons computer architecture cmos integrated circuits resistors electronic ballasts;phase locking oscillatory neural networks tmo nano oscillators multilevel rram cells parallel computation pattern recognition energy constrained environments neuromorphic computing onn cmos technology synaptic weights metal oxide based resistive switching rram structures logic gates phase initialization;random access storage cmos integrated circuits neural nets parallel processing;rram based nano oscillators metal oxide resistive memory neuromorphic computing oscillatory neural networks	In massively parallel computational tasks, such as pattern recognition, conventional computing architectures have insufficient power efficiency for energy constrained environments. This has made alternative architectures, such as neuromorphic computing, increasingly attractive. Oscillatory neural networks (ONNs) are one promising architecture, but efficient hardware implementations have been limited by shortcomings in CMOS technology, specifically in the efficient implementation of oscillators and synaptic weights. The authors have recently demonstrated that metal-oxide based resistive switching (RRAM) structures can be engineered to create low-power, scalable, voltage-controlled oscillators that utilize inherent meta-stability in the device. This work proposes an RRAM-based ONN that couples oscillatory “neurons” through weighted “synapses” using oscillator phase as the state-variable. This paper demonstrates a robust architecture using only a few logic gates per neuron to implement phase initialization and locking of these oscillators, and demonstrate their capability to identify stored patterns from noisy inputs. Using measured characteristics of RRAMs as oscillators and programmable resistors, compact models are derived and used to simulate both an 8-neuron and 20-neuron network.	artificial neural network;cmos;digital electronics;electronic circuit;gnu nano;image scaling;lock (computer science);logic gate;low-power broadcasting;network architecture;neuromorphic engineering;neuron;oscillator (cellular automaton);pattern recognition;performance per watt;resistive random-access memory;scalability;simulation;synapse;systems architecture	Thomas C. Jackson;Abhishek A. Sharma;James A. Bain;Jeffrey A. Weldon;Lawrence T. Pileggi	2015	IEEE Journal on Emerging and Selected Topics in Circuits and Systems	10.1109/JETCAS.2015.2433551	embedded system;electronic engineering;parallel computing;computer science	Arch	5.00147159675101	41.9493442052817	199661
226dd2dd184d072cd68d4064d6583506ad1a5711	an energy-efficient fpga-based matrix multiplier		Matrix multiplication is a fundamental operation of numerical linear algebra, and applied widely in high performance computing to solve scientific and engineering problems. It requires computer systems have huge computing capacity and data throughputs as problem size is increased, and consumes much more power. In this research, an OpenCL-based matrix multiplier is presented to improve energy efficiency. When data are single precision floating-point, and matrix dimension is 16384×16384, the matrix multiplier implemented by the FPGA board DE5a-NET achieves 240.34 GFLOPs in data throughput and 19.64 GFLOPs/W in energy efficiency, which are 296 times and 1964 times over the software simulation carried out on a PC with 32 GB DDR4 RAMs and an AMD processor Ryzen 7 1700 running at 3.0 GHz, respectively.	analysis of algorithms;density matrix;field-programmable gate array;matrix multiplication;numerical analysis;numerical linear algebra;opencl api;simulation;single-precision floating-point format;supercomputer;the matrix;throughput	Yiyu Tan;Toshiyuki Imamura	2017	2017 24th IEEE International Conference on Electronics, Circuits and Systems (ICECS)	10.1109/ICECS.2017.8292049	throughput;flops;field-programmable gate array;electronic engineering;software;matrix multiplication;computer science;numerical linear algebra;matrix (mathematics);supercomputer	HPC	-2.328957648616474	40.56150411018645	199984
