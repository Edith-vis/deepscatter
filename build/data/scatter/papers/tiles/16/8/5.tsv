id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
311741acfdd5996e3699c9f481d2a2d4e735a9cb	virtual environment trajectory analysis: a basis for navigational assistance and scene adaptivity	spatial behaviour;adaptive ves;multi agent system;dynamic reconfiguration;qa75 electronic computers computer science;trajectory classification;virtual environment;vector quantisation;self organising map;neural network	This paper describes the analysis and clustering of motion trajectories obtained while users navigate within a virtual environment (VE). It presents a neural network simulation that produces a set of five clusters which help to differentiate users on the basis of efficient and inefficient navigational strategies. The accuracy of classification carried out with a self-organising map algorithm was tested and improved to in excess of 85% by using learning vector quantisation. This paper considers how such user classifications could be utilised in the delivery of intelligent navigational support and the dynamic reconfiguration of scenes within such VEs. We explore how such intelligent assistance and system adaptivity could be delivered within a Multi-Agent Systems (MAS) context. © 2004 Elsevier B.V. All rights reserved.	algorithm;artificial neural network;cluster analysis;cognition;computer cluster;learning vector quantization;multi-agent system;offset binary;online and offline;real-time clock;real-time computing;self-organization;self-organizing map;simulation;virtual reality	Corina Sas;Gregory M. P. O'Hare;Ronan G. Reilly	2005	Future Generation Comp. Syst.	10.1016/j.future.2004.04.003	computer vision;simulation;computer science;virtual machine;artificial intelligence;operating system;multi-agent system;artificial neural network	Robotics	6.891995976033519	-29.685618993253378	45723
ce2932c0b831070e5761b50d7a8694287cbffd9c	the comparison of decision tree based insurance churn prediction between spark ml and spss		We have deployed a big data platform for the insurance company. One can select the data they need from database, do data analysis job and save the final model to the model pool using the platform. We completed churn prediction task on both SPSS [1] and Spark [7] using the data providing by X insurance company and carefully compared the execution flow, runtime, model evaluation, and model precision of each. Experimental results confirm that Spark ML [6] is easy to use and can cope with big data problems.	algorithm;apache spark;artificial intelligence;big data;decision tree;experiment;machine learning;norm (social);spss	Wei Zhang;Tong Mo;Weiping Li;Hanyu Huang;Xiaogang Tian	2016	2016 9th International Conference on Service Science (ICSS)	10.1109/ICSS.2016.25	data science;data modeling;big data;decision tree;spark (mathematics);data mining;engineering	Robotics	1.6622808520220942	-34.08171917252583	45787
b0f67d9e3fb662a765dbb338cff042003546107c	integrated feature analysis and fuzzy rule-based system identification in a neuro-fuzzy paradigm	feedforward systems;neuro fuzzy paradigm;fuzzy rule based system;feed forward;modulator function;fuzzy neural nets;fuzzy systems knowledge based systems neural networks fuzzy neural networks system identification robustness cooperative systems fuzzy sets feedforward systems testing;neural networks;knowledge acquisition fuzzy neural nets feedforward neural nets feature extraction knowledge based systems;integrated feature analysis;rule extraction;testing;indexing terms;fuzzy sets;online selection;nonnegative characteristic;cooperative systems;system identification;fuzzy rule based system identification;nonnegative characteristic integrated feature analysis fuzzy rule based system identification neuro fuzzy paradigm feature analysis neuro fuzzy system five layered feedforward network fuzzification modulator function online selection;feature extraction;knowledge acquisition;neuro fuzzy;neuro fuzzy system;fuzzification;five layered feedforward network;robustness;feedforward neural nets;feature analysis;fuzzy neural networks;fuzzy systems;knowledge based systems;fuzzy system	"""Most methods of fuzzy rule-based system identification (SI) either ignore feature analysis or do it in a separate phase. This paper proposes a novel neuro-fuzzy system that can simultaneously do feature analysis and SI in an integrated manner. It is a five-layered feed-forward network for realizing a fuzzy rule-based system. The second layer of the net is the most important one, which along with fuzzification of the input also learns a modulator function for each input feature. This enables online selection of important features by the network. The system is so designed that learning maintains the nonnegative characteristic of certainty factors of rules. The proposed network is tested on both synthetic and real data sets and the performance is found to be quite satisfactory. To get an """"optimal"""" network architecture and to eliminate conflicting rules, nodes and links are pruned and then the structure is retrained. The pruned network retains almost the same level of performance as that of the original one."""	conflict (psychology);ephrin type-b receptor 1, human;expert system;feed forward (control);feedforward neural network;fuzzy control system;fuzzy rule;fuzzy set;modulation;modulator device component;network architecture;neuro-fuzzy;programming paradigm;rule (guideline);rule-based system;synthetic intelligence;system identification;anatomical layer	Debrup Chakraborty;Nikhil R. Pal	2001	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.931526	computer science;artificial intelligence;machine learning;pattern recognition;data mining;fuzzy set;fuzzy control system	ML	6.7140400429545295	-28.002406260449334	45882
18c70942706faa5690ed71715d3d5f4cf6278b4e	end-to-end neural network architecture for fraud scoring in card payments		Abstract Millions of euros are lost every year due to fraudulent card transactions. The design and implementation of efficient fraud detection methods is mandatory to minimize such losses. In this paper, we present a neural network based system for fraud detection in banking systems. We use a real world dataset, and describe an end-to-end solution from the practitioner’s perspective, by focusing on the following crucial aspects: unbalancedness, data processing and cost metric evaluation. Our analysis shows that the proposed solution achieves comparable performance values with state-of-the-art proprietary and costly solutions.	artificial neural network;network architecture	Jon Ander Gómez;Juan Arévalo;Roberto Paredes;Jordi Nin	2018	Pattern Recognition Letters	10.1016/j.patrec.2017.08.024	mathematics;data mining;payment;architecture;artificial neural network;end-to-end principle;data processing	Vision	4.537274082031965	-36.088101522235156	45916
88caee0e4e727ffd86457a42fb3d34402a16faf0	a relaxation scheme for improving a convexity based clustering method	analyse amas;cluster;amas;funcion densidad probabilidad;confidence measure;probability density function;convexite;mode detection;convexidad;relajacion;algorithme;fonction densite probabilite;algorithm;cluster analysis;clustering method;relaxation;analisis cluster;relaxation scheme;monton;convexity;algoritmo	Abstract   In cluster analysis, modes can be detected as regions where the underlying probability density function is locally concave. This paper investigates the possibility of using relaxation in order to improve the discrimination between the modes which are concave and the valleys which are convex.	cluster analysis;linear programming relaxation	Jack-Gérard Postaire;S. Olejnik	1994	Pattern Recognition Letters	10.1016/0167-8655(94)90111-2	mathematical optimization;probability density function;combinatorics;topology;convexity;computer science;machine learning;logarithmically concave function;relaxation;mathematics;cluster analysis;algorithm;statistics;cluster	Vision	3.749655460581413	-37.681882980944366	46008
94a00079770e5ad6902efc7b990afbd81552da14	"""proof-of-concept: creating """"fuzzy"""" sorting algorithms"""		Sorting algorithms are common tools for manipulating data and used in both standalone circumstances and within larger more complex algorithms. Thus, it is highly desirable for sorting algorithms to be efficient in terms of storage and computation. By applying the concept of fuzzy logic (an abstract version of Boolean logic) to any well-known algorithm, it generates an abstract version (i.e., fuzzy algorithm) that often results in computational improvements. Although the algorithm may produce a less precise result, this is counteracted by gaining computational efficiency with minimal acceptable trade-offs (e.g., small increase in space requirements, loss of precision). Using an established three-step framework for fuzzification of an algorithm, the resulting new fuzzy algorithm goes beyond a simple conversion of data from raw to fuzzy data by converting the operators and concepts within the algorithm to their abstract equivalents. The purpose of this paper is to demonstrate, as a proof-of-concept, that sorting algorithms can be converted into their corresponding fuzzy sorting algorithms. This paper discusses the preliminary results of: (1) how to apply the general framework by developing the corresponding fuzzy algorithms for a variety of sort algorithms, (2) the success of applying the framework through the development of several fuzzy sort algorithms including fuzzy shell sort, fuzzy strand sort, and fuzzy bucket sort, and (3) the possible applications and benefits of these fuzzy sort algorithms.	boolean algebra;bucket sort;computation;fuzzy logic;fuzzy set;logical connective;requirement;shellsort;sorting algorithm;strand (programming language)	Stephany Coffman-Wolph	2017			proof of concept;fuzzy logic;machine learning;sorting algorithm;computer science;artificial intelligence	Theory	0.38811017412994686	-27.378330469965185	46114
9b25e4a44f4cde347629ff46f47ce12b6f66c522	classification strategies using certain and possible rules	systeme intelligent;rule induction;adquisicion del conocimiento;systeme aide decision;rough set theory;sistema inteligente;sistema ayuda decision;acquisition connaissance;classification;decision support system;knowledge acquisition;intelligent system;error rate;information system;rough set;ensemble approximatif;clasificacion;systeme information;sistema informacion	A typical real-life data set is affected by inconsistencies-- cases characterized by the same attribute values are classified as members of different concepts. The most apparent methodology to handle inconsistencies is offered by rough set theory. For every concept two sets are computed: the lower approximation and the upper approximation. From these two sets a rule induction system induces two rule sets: certain and possible.#R##N##R##N#The problem is how to use these two sets in the process of classification of new, unseen cases. For example, should we use only certain rules (or only possible rules) for classification? Should certain rules be used first and, when a case does not match any certain rule, should possible rules be used later? How to combine certain and possible rules with complete and partial matching of rules by a case? This paper presents experiments that were done to answer these questions. Different strategies were compared by classifying ten real-life data sets, using the error rate as a criterion of quality.		Jerzy W. Grzymala-Busse;Xihong Zou	1998		10.1007/3-540-69115-4_6	rough set;decision support system;computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm	NLP	-0.544797237990564	-30.645375168501577	46392
b874637a22b42d0e1f3776aaacde3c9750fd75ce	normalized decision functions and measures for inconsistent decision tables analysis	rough sets;normalized decision functions;decision table;ndf based decision reducts	We consider the family of normalized decision functions acting over conditional frequency distributions computed from data tables. We draw the connection between such functions and approaches to generating inexact decision rules for the new case classification. We also introduce the family of normalized decision measures corresponding to particular decision functions. They enable us to express efficiency of particular strategies of reasoning with respect to a given data. We show the properties of approximate decision rules and decision reducts based on normalized decision functions and measures. As a result, we obtain an intuitive and flexible tool for extracting approximate classification models from data.	decision table	Dominik Slezak	2000	Fundam. Inform.		decision table;rough set;optimal decision;influence diagram;decision tree learning;weighted product model;computer science;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;decision rule;mathematics;admissible decision rule;programming language;weighted sum model;dominance-based rough set approach;decision matrix	ML	-1.5524901151578163	-25.85529286819657	46451
2a56ca15bea4f9a911835bfd08a2f4526091d785	the evaluation of network anomaly detection systems: statistical analysis of the unsw-nb15 data set and the comparison with the kdd99 data set	unsw nb15 data set;multivariate analysis;nidss;feature correlations	Over the last three decades, Network Intrusion Detection Systems (NIDSs), particularly, Anomaly Detection Systems (ADSs), have become more significant in detecting novel attacks than Signature Detection Systems (SDSs). Evaluating NIDSs using the existing benchmark data sets of KDD99 and NSLKDD does not reflect satisfactory results, due to three major issues: (1) their lack of modern low footprint attack styles, (2) their lack of modern normal traffic scenarios, and (3) a different distribution of training and testing sets. To address these issues, the UNSW-NB15 data set has recently been generated. This data set has nine types of the modern attacks fashions and new patterns of normal traffic, and it contains 49 attributes that comprise the flow based between hosts and the network packets inspection to discriminate between the observations, either normal or abnormal. In this paper, we demonstrate the complexity of the UNSW-NB15 data set in three aspects. First, the statistical analysis of the observations and the attributes are explained. Second, the examination of feature correlations is provided. Third, five existing classifiers are used to evaluate the complexity in terms of accuracy and false alarm rates (FARs) and then, the results are compared with the KDD99 data set. The experimental results show that UNSW-NB15 is more complex than KDD99 and is considered as a new benchmark data set for evaluating NIDSs.	anomaly detection;attack (computing);benchmark (computing);cluster analysis;coefficient;data (computing);expectation–maximization algorithm;intrusion detection system;lr parser;naive bayes classifier;network traffic control;nonlinear system;numerical analysis;sensor;test set;version 6 unix	Nour Moustafa;Jill Slay	2016	Information Security Journal: A Global Perspective	10.1080/19393555.2015.1125974	computer science;machine learning;data mining;multivariate analysis	Security	5.989619288508163	-37.337986706571535	46619
5909b7a8f8d0e0f8f66afa407baf6028205377af	the k-means clustering architecture in the multi-stage data mining process	extraction information;modelizacion;distributed system;hierarchical clustering;association statistique;analyse amas;cluster computing;sistema experto;systeme reparti;partition donnee;analisis datos;information extraction;systeme aide decision;algorithme k moyenne;classification non supervisee;racimo calculadura;distributed computing;data partition;statistical association;base connaissance;sistema ayuda decision;prise decision;data mining;software engineering;processing time;modelisation;refinement method;data analysis;decision support system;grappe calculateur;hierarchical classification;asociacion estadistica;sistema repartido;cluster analysis;regle association;regla asociacion;association rule;fouille donnee;clasificacion no supervisada;decouverte connaissance;estructura datos;classification hierarchique;genie logiciel;unsupervised classification;calculo repartido;temps traitement;descubrimiento conocimiento;base conocimiento;algoritmo k media;analyse donnee;k means algorithm;analisis cluster;structure donnee;modele donnee;systeme expert;methode raffinement;toma decision;modeling;metodo afinamiento;clasificacion jerarquizada;data structure;ingenieria informatica;clustered data;busca dato;tiempo proceso;calcul reparti;extraccion informacion;k means clustering;particion dato;data models;knowledge base;knowledge discovery;expert system	In this paper, we used software engineering principles for the development of models and proposed the K-Means clustering architecture implemented on the multi-stage data mining process. We developed a modified architecture and expanded it by showing refinements on every process of the clustering and knowledge discovery stages. We used the mentioned hierarchical clustering model to partition the data into smaller groups of attributes so that we would determine the data structure before applying the data mining tools. The experiment shows that the model using the clustering resulted to an isolated but imperative association rules based on clustered data, which in return could be practically explained for decision making purposes. Shorter processing time had been observed in computing for smaller clusters implying faster and ideal processing period than dealing with the entire dataset.		Bobby D. Gerardo;Jaewan Lee;Yeon-Sung Choi;Malrey Lee	2005		10.1007/11424826_8	correlation clustering;constrained clustering;knowledge base;data stream clustering;decision support system;data structure;fuzzy clustering;flame clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;cluster analysis;brown clustering;expert system;information extraction;biclustering;algorithm;elasticity;affinity propagation;k-means clustering;clustering high-dimensional data;conceptual clustering	ML	-3.0551109519132678	-32.710699516345876	47554
25a9f0f901e2359d8b4745feab35ed4e35e43535	customer information visualization via customer map	carte credit;extraction information;cambio estado;analisis datos;tarjeta de credito;information extraction;information visualization;changement etat;data mining;data analysis;complex data;fouille donnee;decouverte connaissance;data visualization;credit card;descubrimiento conocimiento;analyse donnee;visualisation donnee;change of state;applications of visualization;value creation;busca dato;extraccion informacion;credit cards;knowledge discovery	Many data mining techniques which are non-visual methods have been proved their virtues on various customer data. However, there have been hardly applications of visualization methods onto the customer information in spite of their ability of quick and easy knowledge discovery. In this paper, we propose a data visualization method for customer information using a customer map. To develop the customer map, we integrate numerous customer data from various data sources, perform data analyses using data mining techniques and finally visualize the information derived by the former analyses. The customer map makes it possible to mange diverse and complex data sets under the unified goal of value creation through customers. It also affords the ability to make quick observation of current state and the change of customer distribution based on their information without preconception. We applied the customer map to the credit card company, and suggested managerial implications from the customer maps obtained from its data.	information visualization	Ji Young Woo;Sung Min Bae;Chong Un Pyon;Sang-Chan Park	2005		10.1007/11408079_76	phase transition;voice of the customer;information visualization;computer science;artificial intelligence;data science;attitudinal analytics;customer reference program;data mining;database;customer intelligence;data analysis;computer security;information extraction;data visualization;complex data type	HCI	-1.3513440450679792	-31.697195814759446	47900
0ae6a22ed3137e9bc41d36b8510f4ccba4a6ef3b	fuzzy-rough nearest neighbor algorithms in classification	engineering;fuzzy set;procesamiento informacion;semantics;conjunto difuso;ensemble flou;semantica;semantique;classification;crisp;ingenierie;algorithme;algorithm;fuzzy;nearest neighbor;information processing;k nearest neighbor;ingenieria;classifiers;rough fuzzy and fuzzy rough;sistema difuso;systeme flou;traitement information;clasificacion;fuzzy system;algoritmo;rough	In this paper, classification efficiency of the conventional K-nearest neighbor algorithm is enhanced by exploiting fuzzy-rough uncertainty. The simplicity and nonparametric characteristics of the conventional K-nearest neighbor algorithm remain intact in the proposed algorithm. Unlike the conventional one, the proposed algorithm does not need to know the optimal value of K. Moreover, the generated class confidence values, which are interpreted in terms of fuzzy-rough ownership values, do not necessarily sum up to one. Consequently, the proposed algorithm can distinguish between equal evidence and ignorance, and thus the semantics of the class confidence values becomes richer. It is shown that the proposed classifier generalizes the conventional and fuzzy KNN algorithms. The efficacy of the proposed approach is discussed on real data sets. © 2007 Elsevier B.V. All rights reserved.	k-nearest neighbors algorithm;nearest-neighbor interpolation;need to know;optimization problem;rough set	Manish Sarkar	2007	Fuzzy Sets and Systems	10.1016/j.fss.2007.04.023	best bin first;information processing;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;semantics;k-nearest neighbors algorithm;fuzzy control system	AI	8.505648071163636	-32.64368789156056	47977
2b70d6c4c4d507048a8f5de401c116b4d310f68a	a hybrid som-bpn approach to lot output time prediction in a wafer fab	wafer fab;topology;tecnologia electronica telecomunicaciones;raisonnement base sur cas;razonamiento fundado sobre caso;computacion informatica;fuzzy rules;self organization map;root mean square error;logique floue;topologie;erreur quadratique moyenne;logica difusa;classification;topologia;fuzzy logic;hybrid approach;wafer;back propagation network;ciencias basicas y experimentales;mean square error;backpropagation algorithm;prediction accuracy;autoorganizacion;algorithme retropropagation;self organization;self organized map;pastilla electronica;pastille electronique;error medio cuadratico;case based reasoning;tecnologias;grupo a;clasificacion;output time prediction;autoorganisation;algoritmo retropropagacion	Output time prediction is a critical task to a wafer fab (fabrication plant). To further enhance the accuracy of wafer lot output time prediction, the concept of input classification is applied to the back propagation network (BPN) approach in this study by pre-classifying input examples with the self-organization map (SOM) classifier before they are fed into the BPN. Examples belonging to different categories are then learned with different BPNs but with the same topology. Production simulation is also applied in this study to generate test examples. According to experimental results, the prediction accuracy of the proposed methodology was significantly better than those of three existing approaches, case-based reasoning (CBR), BPN without example classification, and evolving fuzzy rules (EFR), in most cases by achieving a 13–46% (and an average of 30%) reduction in the root-mean-squared-error (RMSE) over the comparison basis – BPN without example classification.	backpropagation;business process network;case-based reasoning;cluster analysis;enhanced full rate;full scale;magnetic-core memory;mean squared error;randomness;self-organization;semiconductor fabrication plant;simulation;software propagation;test data;turing test;wafer (electronics)	Toly Chen	2006	Neural Processing Letters	10.1007/s11063-006-9027-4	computer science;artificial intelligence;machine learning;mean squared error;algorithm	ML	8.868632770144398	-28.940080621697348	48111
178ea0f036111c3835207766020b7480621eddd5	a dynamic integration algorithm for an ensemble of classifiers	systeme intelligent;procesamiento informacion;adquisicion del conocimiento;systeme apprentissage;systeme aide decision;ensemble of classifiers;integration information;sistema inteligente;acquisition connaissances;sistema ayuda decision;data mining;learning systems;information integration;decision support system;general methods;machine learning;knowledge acquisition;information processing;integracion informacion;intelligent system;classification accuracy;traitement information	Numerous data mining methods have recently been developed, and there is often a need to select the most appropriate data mining method or methods. The method selection can be done statically or dynamically. Dynamic selection takes into account characteristics of a new instance and usually results in higher classification accuracy. We discuss a dynamic integration algorithm for an ensemble of classifiers. Our algorithm is a new variation of the stacked generalization method and is based on the basic assumption that each basic classifier is best inside certain subareas of the application domain. The algorithm includes two main phases: a learning phase, which collects information about the quality of classifications made by the basic classifiers into a performance matrix, and an application phase, which predicts the goodness of classification for a new instance produced by the basic classifiers using the performance matrix. In this paper we present also experiments made on three machine learning data sets, which show promising results.	algorithm;application domain;data mining;ensemble learning;experiment;machine learning	Seppo Puuronen;Vagan Y. Terziyan;Alexey Tsymbal	1999		10.1007/BFb0095148	random subspace method;decision support system;information processing;computer science;artificial intelligence;information integration;machine learning;data mining	AI	8.65352035787071	-33.186814475389895	48124
48779099d4febb90816cc0e74af5adc6cc05219a	rule induction partitioning estimator - a new deterministic algorithm for data dependent partitioning estimate			deterministic algorithm;rule induction	Vincent Margot;Jean-Patrick Baudry;Frédéric Guilloux;Olivier Wintenberger	2018		10.1007/978-3-319-96133-0_22	artificial intelligence;machine learning;pattern recognition;estimator;computer science;rule induction;deterministic algorithm	ML	3.004720521352882	-36.956320945537655	48303
279bd17bd4298bf1033c12d50dc1727c0280484d	dynamic reduct from partially uncertain data using rough sets	transferable belief model;belief function theory;attribute selection;uncertainty;rough sets;rough set;uncertain data;dynamic reduct;decision table;decision rule	In this paper, we deal with the problem of attribute selection from a sample of partially uncertain data. The uncertainty exists in decision attributes and is represented by the Transferable Belief Model (TBM), one interpretation of the belief function theory. To solve this problem, we propose dynamic reduct for attribute selection to extract more relevant and stable features for classification. The reduction of the uncertain decision table using this approach yields simplified and more significant belief decision rules for unseen objects.	rough set;uncertain data	Salsabil Trabelsi;Zied Elouedi;Pawan Lingras	2009		10.1007/978-3-642-10646-0_19	rough set;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;dominance-based rough set approach	DB	-0.6231258139355619	-28.570790270982055	48729
2735d718e562c670de7848125eccf9520a72ba41	hierarchical neuro-fuzzy systems part i		Neuro-fuzzy [Jang,1997][Abraham,2005] are hybrid systems that combine the learning capacity of neural nets [Haykin,1999] with the linguistic interpretation of fuzzy inference systems [Ross,2004]. These systems have been evaluated quite intensively in machine learning tasks. This is mainly due to a number of factors: the applicability of learning algorithms developed for neural nets; the possibility of promoting implicit and explicit knowledge integration; and the possibility of extracting knowledge in the form of fuzzy rules. Most of the well known neuro-fuzzy systems, however, present limitations regarding the number of inputs allowed or the limited (or nonexistent) form to create their own structure and rules [Nauck,1997][Nauck,19 98][Vuorimaa,1994][Zhang,1995]. This paper describes a new class of neuro-fuzzy models, called Hierarchical Neuro-Fuzzy BSP Systems (HNFB). These models employ the BSP partitioning (Binary Space Partitioning) of the input space [Chrysanthou,1996] and have been developed to bypass traditional drawbacks of neuro-fuzzy systems. This paper introduces the HNFB models based on supervised learning algorithm. These models were evaluated in many benchmark applications related to classification and time-series forecasting. A second paper, entitled Hierarchical Neuro-Fuzzy Systems Part II, focuses on hierarchical neuro-fuzzy models based on reinforcement learning algorithms. BACKGROUND	algorithm;artificial neural network;benchmark (computing);binary space partitioning;fuzzy control system;fuzzy logic;fuzzy rule;hybrid system;knowledge integration;machine learning;neuro-fuzzy;reinforcement learning;supervised learning;time series;whole earth 'lectronic link	Marley M. B. R. Vellasco;Marco Aurélio Cavalcanti Pacheco;Karla Figueiredo;Flávio Joaquim de Souza	2009				AI	4.818730762489633	-25.86693179209847	48748
ba4174110ea774ad4787d2b52aa34919e8c64818	evaluation of neural networks for software development effort estimation using a new criterion	automated testing;user interface pattern;web application;domain specific language	This paper evaluates the performance of three neural network models, namely, Radial Basis Function, General Regression, and Extreme Learning Machine, as applied to the problem of software development effort estimation. Unlike the conventional back propagation algorithm, these three neural network models provide output from the input in a single pass. Their performance is evaluated using data obtained in the context of small programs developed in an academic setting. The results are evaluated using standard statistical tests of significance. Additionally, effect size value is provided for comparison. A new evaluation criterion is proposed by fitting a linear regression line into the observed values and the predicted values. While the conventional practice is to use a single parameter for comparison of the performance, we use two parameters, namely, the intercept, and the slope of the regression line. In the ideal condition, the slope should be one and the intercept should be zero. While being simple to implement, the results indicate that the Extreme Learning Machine’s performance is comparable to those of Radial Basis Function and General Regression Network. While applying our criterion to the test data, it is observed that all the three network models suffer from the introduction of bias.	algorithm;artificial neural network;backpropagation;computer performance;cost estimation in software engineering;exponent bias;radial (radio);radial basis function;software development effort estimation;software propagation;test data;visual intercept	S. K. Pillai;M. K. Jeyakumar	2014	ACM SIGSOFT Software Engineering Notes	10.1145/2659118.2659141	web application;computer science;domain-specific language;theoretical computer science;data mining;programming language	SE	4.460377378187985	-33.98086493287027	49105
fde64e62399eee88670c8234f30eb10eab7996bb	an e-mail filtering approach using neural network	electronic mail;self organized feature map;principal component analysis;neural network	  The communication via electronic mail is one of the most popular services of Internet. The volume of emails that we get is  constantly growing. In particular, unsolicited messages or spam, flood our email boxes, and result in causing frustration, and wasting bandwidth and time. The paper presents a novel schema  to automatically filter spam emails by using the principal component analysis(PCA) and the Self Organized Feature Map (SOFM).  In our schema, each email is represented by a series of textual and non-textual features. To reduce the number of textual  features, PCA is used to select the most relevant features. Finally the output of the PCA and the non-textual features should  be inputted into a well-trained SOFM to classify (spam or normal). In comparison with some traditional classification methods, the experimental result denotes that the scheme will increase  the accuracy of filtering emails.    		Yukun Cao;Xiaofeng Liao;Yunfeng Li	2004		10.1007/978-3-540-28648-6_110	neural gas;probabilistic neural network;self-organizing map;computer science;machine learning;pattern recognition;data mining;time delay neural network;artificial neural network;principal component analysis	ML	4.854813562380257	-36.98283003862562	49168
9235f7307bfed7837675cc2a598b44b048fecc5f	modular neural networks optimization with hierarchical genetic algorithms with fuzzy response integration for pattern recognition	granular computing;modular neural networks;type 2 fuzzy logic;hierarchical genetic algorithms	In this paper a new model of a Modular Neural Network (MNN) with fuzzy integration based on granular computing is proposed. The topology and parameters of the MNN are optimized with a Hierarchical Genetic Algorithm (HGA). The proposed method can divide the data automatically into sub modules or granules, chooses the percentage of images and selects which images will be used for training. The responses of each sub module are combined using a fuzzy integrator, the number of the fuzzy integrators will depend of the number of sub modules or granules that the MNN has at a particular moment. The method was applied to the case of human recognition to illustrate its applicability with good results.	genetic algorithm;modular neural network;neural networks;pattern recognition	Daniela Sánchez;Patricia Melin;Oscar Castillo;Fevrier Valdez	2012		10.1007/978-3-642-37798-3_22	granular computing;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;algorithm	Vision	5.062318027268377	-27.239209269783185	49233
4d048400bd98689413205f3ec396f4a2f6dcf61d	ensembles of relational classifiers	bayes estimation;ensemble classification;autocorrelacion;extraction information;base relacional dato;entity relationship model;autocorrelation function;relational data;bayesian classifier;modele agrege;ensemble method;analisis datos;information extraction;sparse graph;grafo disperso;relational autocorrelation;modelo agregado;modelo entidad relacion;complex method;relational database;probabilistic approach;modele entite relation;data mining;classification;funcion autocorrelacion;empirical evidence;data analysis;estimacion bayes;metodo complex;fouille donnee;heterogeneidad;fonction autocorrelation;enfoque probabilista;approche probabiliste;methode complex;aggregate model;representacion parsimoniosa;base de donnees relationnelle;analyse donnee;sparse graphs;relational data mining;classification accuracy;sparse representation;graphe epars;busca dato;clasificacion;extraccion informacion;heterogeneity;heterogeneite;autocorrelation;estimation bayes;representation parcimonieuse	Relational classification aims at including relations among entities into the classification process, for example taking relations among documents such as common authors or citations into account. However, considering more than one relation can further improve classification accuracy. Here we introduce a new approach to make use of several relations as well as both, relations and local attributes for classification using ensemble methods. To accomplish this, we present a generic relational ensemble model that can use different relational and local classifiers as components. Furthermore, we discuss solutions for several problems concerning relational data such as heterogeneity, sparsity, and multiple relations. Especially the sparsity problem will be discussed in more detail. We introduce a new method called PRNMultiHop that tries to handle this problem. Furthermore we categorize relational methods in a systematic way. Finally, we provide empirical evidence, that our relational ensemble methods outperform existing relational classification methods, even rather complex models such as relational probability trees (RPTs), relational dependency networks (RDNs) and relational Bayesian classifiers (RBCs).	algorithm;bayesian network;categorization;ensemble learning;entity;experiment;han unification;machine learning;naive bayes classifier;relational data mining;relational model;sparse matrix	Christine Preisach;Lars Schmidt-Thieme	2007	Knowledge and Information Systems	10.1007/s10115-007-0093-3	relational model;statistical relational learning;autocorrelation;entity–relationship model;relational database;computer science;machine learning;pattern recognition;data mining;mathematics;information extraction;statistics	AI	8.280105019908888	-34.07341251711995	49307
1f1780d2efa6521a1cd6966aa62610fd55295147	the m2m pathfinding algorithm based on the idea of granular computing	granular computing;hierarchical structure;organization;intelligent agent data structures costs navigation heuristic algorithms iterative algorithms conferences power system modeling multiagent systems real time systems;iterative algorithms;transition;satisfiability;navigation;data structures;heuristic algorithms;intelligent agent;power system modeling;data structure;conferences;multiagent systems;real time systems	Macro-to-micro (M2M) model is an implementation model that inherits the GrC idea and extends it to some additional highly desirable characteristics. In this paper we introduce an effective pathfinding algorithm based on the M2M model. This algorithm takes O(n) time to preprocess, constructing the M2M data structure. Such hierarchical structure occupies O(n) bit memory space and can be updated in O(1) expected time to handle changes. Although the resulting path is not always the shortest one, it can make a trade-off between accuracy and time cost by adjusting a parameter - range value to satisfy various applications. At last, we will discuss the advantages of the M2M pathfinding algorithm (M2M-PF) and demonstrate the academic and applied prospect of M2M model.	algorithm;average-case complexity;dspace;data structure;granular computing;pathfinding;preprocessor	Haifeng Wan;Yingpeng Zhang;Shengzhou Luo;Ruijie Liu;Wensheng Ye	2009	2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WI-IAT.2009.208	navigation;simulation;data structure;computer science;pathfinding;organization;artificial intelligence;transition;theoretical computer science;machine learning;data mining;satisfiability	AI	-3.967451382348418	-36.14026558799976	49504
ea3248c9ab35305997bdd9f8e9cf4dd4a4e8a92a	boosting tuple propagation in multi-relational classification	classification algorithm;query optimization;classification;structural decomposition methods;hypergraphs;decomposition method;multi relational data mining;database management system	Multi-relational classification is a mining method aiming at building classifiers for the tuples in some target relation based on its own data as well as on the data possibly dispersed over other non-target relations, by exploiting the relationships among them formalized via foreign key constraints. While improving on the efficacy of the resulting classifiers, propagating data via the foreign key constraints deteriorates the scalability of the underlying algorithm. In the paper, various techniques are discussed to efficiently implement this propagation task, and hence to boost performances of current multi-relational classification algorithms. These techniques are based on suitable adaptations of state-of-the-art query optimization methods, and are conceived to be coupled with database management systems. A system prototype integrating all the techniques is illustrated, and results of experimental activity conducted on top of it are eventually discussed.	algorithm;bayesian network;boosting (machine learning);database;decision tree learning;foreign key;mathematical optimization;performance;prototype;query optimization;relational data mining;scalability;software propagation	Lucantonio Ghionna;Gianluigi Greco	2011		10.1145/2076623.2076637	query optimization;web query classification;decomposition method;biological classification;computer science;machine learning;data mining;database	DB	-2.438561612639347	-30.48765662046051	49523
7ab9002a54cb266fb3816dba02932ab080432ac4	nutri-expert, an educational software in nutrition	algorithm;expert system;inference rule;pattern recognition;fuzzy logic;artificial intelligence;educational software;decision tree	Nutri-Expert is an educational software which helps patients to improve their nutritional habits, by analyzing in detail their food intakes, and by suggesting changes that result in Ž well-balanced meals. The inherent imprecision of the data food composition, weight of . foods is represented by means of fuzzy intervals, according to the possibility theory. Fuzzy arithmetic is used to perform calculations, and fuzzy pattern matching to compare results with acceptable standards, yielding an assessment of the degree to which a meal is well balanced. Such degrees are translated into sentences by a simple but effective procedure. A tree search algorithm is then used to find out minimal sets of pertinent actions to perform on a meal in order to correct it. The search is guided by an evaluation function based on fuzzy pattern matching indices. In this real case, the possibility theory improves markedly the quality of the results and their resilience, for the price of a reasonable increase of calculation cost. Q 1997 John Wiley & Sons, Inc.	computation;effective method;evaluation function;expert system;food composition data;john d. wiley;pattern matching;possibility theory;relevance;search algorithm;tree traversal	Jean-Christophe Buisson	1997	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199711/12)12:11/12%3C915::AID-INT8%3E3.0.CO;2-%23		AI	0.7624638188903341	-28.203617445244294	49589
29395d8158c33ab5a5091d9322dffe3e46a6d767	quantitative concept analysis	semantic completion;universal property;concept analysis;enriched category	Formal Concept Analysis (FCA) begins from a context, given as a binary relation between some objects and some attributes, and derives a lattice of concepts, where each concept is given as a set of objects and a set of attributes, such that the first set consists of all objects that satisfy all attributes in the second, and vice versa. Many applications, though, provide contexts with quantitative information, telling not just whether an object satisfies an attribute, but also quantifying this satisfaction. Contexts in this form arise as rating matrices in recommender systems, as occurrence matrices in text analysis, as pixel intensity matrices in digital image processing, etc. Such applications have attracted a lot of attention, and several numeric extensions of FCA have been proposed. We propose the framework of proximity sets (proxets), which subsume partially ordered sets (posets) as well as metric spaces. One feature of this approach is that it extracts from quantified contexts quantified concepts, and thus allows full use of the available information. Another feature is that the categorical approach allows analyzing any universal properties that the classical FCA and the new versions may have, and thus provides structural guidance for aligning and combining the approaches.	digital image processing;formal concept analysis;pixel;recommender system	Dusko Pavlovic	2012		10.1007/978-3-642-29892-9_24	discrete mathematics;computer science;formal concept analysis;machine learning;data mining;mathematics;enriched category;algorithm;universal property	DB	-3.9896175492647123	-25.072930203902743	49658
366d9cb2f7d2041eb3c13f5f4ea93cd38d3a8c5c	completing fuzzy if-then rule bases by means of smoothing splines	fuzzy if then rule bases;fuzzy set;fuzzy transforms;smoothing splines;parameter space;fuzzy if then rules;smoothing spline	A fuzzy if-then rule base may be viewed as a partial function between universes of fuzzy sets. We show how this partial function determines a total one by means of the method of smooting splines. To this end, we identify the fuzzy sets with elements of a finite-dimensional real parameter space in an approximate way, using Perfilieva’s fuzzy transforms.	approximation algorithm;fuzzy logic;fuzzy set;rule-based system;smoothing spline;spline (mathematics)	Thomas Vetterlein;Martin Stepnicka	2006	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488506003960	fuzzy logic;mathematical optimization;mathematical analysis;discrete mathematics;membership function;defuzzification;smoothing spline;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;statistics	DB	0.8550009071571643	-23.953165143488572	49809
f51a2533e592962a5a03625a47c52aa100c3ebb9	bayesian learning and relevance vector machines for hydrologic applications (keynote speech of the session)	relevance vector machine;bayesian learning		relevance	Rao S. Govindaraju	2005			artificial intelligence;pattern recognition;machine learning;computer science;relevance vector machine;bayesian inference	NLP	9.834547999220147	-27.101176494951602	50106
c7e9e40557142aef95a62e48330939ae01a79062	indexed bit map (ibm) for mining frequent sequences	extraction information;occupation time;base donnee;bit map;storage access;analisis datos;information extraction;database;base dato;data mining;data analysis;temps occupation;fouille donnee;indexation;decouverte connaissance;carte memoire image;acces memoire;tiempo ocupacion;data access;time use;acceso memoria;descubrimiento conocimiento;analyse donnee;sequential pattern mining;synthetic data;busca dato;extraccion informacion;knowledge discovery	Sequential pattern mining has been an emerging problem in data mining. In this paper, we propose a new algorithm for mining frequent sequences. It processes only one scan of the database thanks to an indexed structure associated to a bit map representation. Thus, it allows a fast data access and a compact storage in main memory. This algorithm has been applied to activity sequences belonging to a population time-use survey. The experimental results show the efficiency of our method compared to existing algorithms.	algorithm;computer data storage;data access;data mining;sequential pattern mining	Lionel Savary;Karine Zeitouni	2005		10.1007/11564126_70	sequential pattern mining;data access;computer science;data mining;database;knowledge extraction;data stream mining;data analysis;bitmap;information extraction;synthetic data	ML	-3.362717698346309	-33.181975356313544	50456
9b43684a3baecb4218e0ed4570caa7adcea667eb	a rough set framework for learning in a directed acyclic graph	directed acyclic graph;grafo aciclico;learning algorithm;expression profile;graphe acyclique;intelligence artificielle;algorithme apprentissage;acyclic graph;gene expression;expression genique;learning methods;directed graph;graphe oriente;learning problems;artificial intelligence;grafo orientado;inteligencia artificial;rough set;gene function;algoritmo aprendizaje;ensemble approximatif;expresion genetica	Prediction of gene function from expression profiles introduces a new learning problem where the decision classes associated with the objects (i.e., genes) are organized in a directed acyclic graph (DAG). Standard learning methods such a Rough Sets assume that these classes are unrelated, and cannot handle this problem properly. To this end, we introduce an extended rough set framework with several new operators. We show how these operators can be used in an new learning algorithm.	directed acyclic graph;rough set	Herman Midelfart;Jan Komorowski	2002		10.1007/3-540-45813-1_18	feedback arc set;computer science;artificial intelligence;machine learning;mathematics;directed acyclic word graph;directed acyclic graph;algorithm;dominance-based rough set approach	ML	8.685409121138445	-31.418360638695578	50775
6ff12b10d2e230422e6027896e6143b9330c8b6e	methods for robust clustering of epileptic eeg spikes	graph theory;espiga positiva;systeme nerveux pathologie;fuzzy c mean;configuration electrode;analyse amas;electrodiagnostic;teoria grafo;electroencefalografia;epilepsia;algoritmo borroso;estudio comparativo;signal analysis;simulation;electrode geometry inverse computations spikes fuzzy c means algorithm graph theoretic algorithm automatic clustering manual clustering real life data sets human choice geometrically weighted feature extraction supplementary clustering dimension graph theoretic method outlier contamination eeg processing electrodiagnostics simulated signals;encephale pathologie;hombre;outlier;analisis de senal;simulacion;robustness epilepsy electroencephalography clustering algorithms signal processing algorithms contamination brain modeling humans pollution measurement feature extraction;theorie graphe;limit set;electroencephalographie;electrodiagnostico;pointe positive;etude comparative;spike;observacion aberrante;sistema nervosio central patologia;algorithms biomedical engineering cluster analysis computer simulation electroencephalography epilepsy humans;cerebral disorder;nervous system diseases;cluster analysis;configuracion electrodo;systeme nerveux central pathologie;feature extraction;fuzzy algorithm;robustesse;sistema nervioso patologia;medical signal processing feature extraction electroencephalography;human;comparative study;robust performance;algorithme flou;central nervous system disease;encefalo patologia;observation aberrante;robustness;analisis cluster;electroencephalography;electrodiagnosis;electroencephalogram;electrode configuration;medical signal processing;analyse signal;epilepsie;homme;robustez;epilepsy	The authors investigate algorithms for clustering of epileptic electroencephalogram (EEG) spikes. Such a method is useful prior to averaging and inverse computations since the spikes of a patient often belong to a few distinct classes. Data sets often contain outliers, which makes algorithms with robust performance desirable. The authors compare the fuzzy C-means (FCM) algorithm and a graph-theoretic algorithm. They give criteria for determination of the correct level of outlier contamination. The performance is then studied by aid of simulations, which show good results for a range of circumstances, for both algorithms. The graph-theoretic method gave better results than FCM for simulated signals. Also, when evaluating the methods on seven real-life data sets, the graph-theoretic method was the better method, in terms of closeness to the manual assessment by a neurophysiologist. However, there was some discrepancy between manual and automatic clustering and the authors suggest as an alternative method a human choice among a limited set of automatically obtained clusterings. Furthermore, the authors evaluate geometrically weighted feature extraction and conclude that it is useful as a supplementary dimension for clustering.	algorithm;centrality;class;cluster analysis;computation;discrepancy function;electroencephalography phase synchronization;epilepsy;feature extraction;fuzzy cognitive map;graph - visual representation;graph theory;patients;real life;simulation;statistical cluster	Patrik Wahlberg;Göran Lantz	2000	IEEE Transactions on Biomedical Engineering	10.1109/10.846679	limit set;outlier;electroencephalography;electrodiagnosis;feature extraction;computer science;artificial intelligence;graph theory;machine learning;comparative research;signal processing;mathematics;cluster analysis;algorithm;robustness	ML	7.039267488085525	-34.75329857158206	50791
c401d902bde8bcc084f08593e0799d92d5db9f63	robust ensemble learning for mining noisy data streams	modelizacion;decision models;concept drift;empirical study;streaming;methode empirique;modele agrege;ensemble learning;analisis datos;noisy data;data stream;metodo empirico;empirical method;apprentissage conceptuel;modelo agregado;prise de decision;data mining;classification;journal article;concept drifting;journal;modelisation;data analysis;transmission en continu;aprendizaje conceptual;fouille donnee;concept learning;aggregate model;analyse donnee;modele donnee;rapport signal bruit;prediction model;relacion senal ruido;transmision fluyente;signal to noise ratio;toma decision;data preprocessing;modeling;busca dato;clasificacion;noise;data models	a Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China b Centre for Quantum Computation & Intelligent Systems, University of Technology Sydney, Broadway, NSW 2007, Australia c Research Center on Fictitious Economy and Data Science, Chinese Academy of Sciences, Beijing, China d College of Information Science & Technology, Univ. of Nebraska at Omaha, Omaha, NE 68182, USA e School of Computer Science & Information Eng., Hefei University of Technology, Hefei 230009, China f Department of Computer Science, University of Vermont, Burlington, VT 05405, USA	academy;aggregate data;aggregate function;algorithm;broadway (microprocessor);computation;computer science;data mining;data pre-processing;data science;dynamic data;ensemble forecasting;ensemble learning;information science;machine learning;preprocessor;signal-to-noise ratio	Peng Zhang;Xingquan Zhu;Yong Shi;Li Guo;Xindong Wu	2011	Decision Support Systems	10.1016/j.dss.2010.11.004	concept learning;computer science;artificial intelligence;machine learning;data mining;ensemble learning;data stream mining;empirical research;statistics	Theory	5.621306916235072	-24.064129052631397	51266
1eb3a23051e4dfafd0e5c0207bdcd0c5cf8624dc	rough set based decision tree model for classification	complexite;model based reasoning;raisonnement base sur modele;model combination;decision tree;supervised learning;algorithme glouton;top down;rough set theory;teoria conjunto;complejidad;theorie ensemble;complexity;arbol decision;set theory;data mining;classification;fouille donnee;theorie ensemble approximatif;greedy algorithm;algoritmo gloton;feature selection;apprentissage supervise;information system;rough set;aprendizaje supervisado;busca dato;arbre decision;clasificacion;systeme information;sistema informacion	Decision tree, a commonly used classification model, is constructed recursively following a top down approach (from the general concepts to particular examples) by repeatedly splitting the training data set. ID3 is a greedy algorithm that considers one attribute at a time for splitting at a node. In C4.5, all attributes, barring the nominal attributes used at the parent nodes, are retained for further computation. This leads to extra overheads of memory and computational efforts. Rough Set theory (RS) simplifies the search for dominant attributes in the information systems. In this paper, Rough set based Decision Tree (RDT) model combining the RS tools with classical DT capabilities, is proposed to address the issue of computational overheads. The experiments compare the performance of RDT with RS approach and ID3 algorithm. The performance of RDT over RS approach is observed better in accuracy and rule complexity while RDT and ID3 are comparable.		Sonajharia Minz;Rajni Jain	2003		10.1007/978-3-540-45228-7_18	rough set;computer science;artificial intelligence;machine learning;mathematics;supervised learning;feature selection;algorithm	ML	8.506612085710152	-32.507251506522884	51462
8ceec06b5e17e63af36382f090544aacb99cd553	computational intelligence: retrospection and future	granular computing;neural networks;computational intelligence;fuzzy sets;synergy		computation;computational intelligence	Witold Pedrycz	2017	JACIII	10.20965/jaciii.2017.p0009	synergy;granular computing;computer science;artificial intelligence;machine learning;computational intelligence;data mining;fuzzy set;artificial neural network	AI	4.270871966444798	-25.943500930715615	51630
b018afccbed76c5de72f652b8bdb46523514c05e	a new approach to teaching fuzzy logic system design	starting;iterative method;systeme commande;sistema control;learning;concepcion sistema;logic design;top down;logique floue;educational software program;logica difusa;intelligence artificielle;didacticiel;metodo iterativo;aprendizaje;fuzzy logic;demarrage;control system;apprentissage;conception logique;arranque;methode iterative;system design;pattern classification;pattern recognition;fuzzy logic system;artificial intelligence;sistema difuso;enseignement;programa didactico;inteligencia artificial;reconnaissance forme;systeme flou;information system;reconocimiento patron;recherche scientifique;concepcion logica;scientific research;conception systeme;systeme information;fuzzy system;investigacion cientifica;teaching;approaches to learning;sistema informacion;ensenanza;classification forme	In order for Fuzzy Systems to continue to flourish in all scientific research areas, not only engineering, a more efficient and effective way of transmitting the relevant knowledge and skills of this discipline is necessary. Conventional tutorials in this area follow a fixed outline starting from the basic principles (set operations, relations, etc) and ending with current applications such as pattern recognition, information classification and system control. We argue that an iterative and concurrent top-down/down-top approach to learning would be more effective, and suggest that a plan to discover concepts first and map them afterwards is viable. Several recent publications and a case study currently under development in the field of medicine are used as examples to show evidence for the future of this new approach.	fuzzy logic	Emine Inelmen;Erol Inelmen;Ahmad Ibrahim	2003		10.1007/3-540-44967-1_8	fuzzy logic;logic synthesis;scientific method;computer science;control system;artificial intelligence;top-down and bottom-up design;iterative method;information system;algorithm;systems design	EDA	8.003336495951661	-29.984764234986557	51790
1a3ca76de84e7ab42c2c8c38a5d77fbd43242b98	time-series prediction: application to the short-term electric energy demand	metodo cuadrado menor;forecasting;methode moindre carre;prevision;base donnee;electric energy;least squares method;database;base dato;intelligence artificielle;time series;computation by abstract devices;energie electrique;serie temporelle;court terme;serie temporal;artificial intelligence incl robotics;artificial intelligence;modele donnee;mathematical logic and formal languages;inteligencia artificial;energia electrica;corto plazo;short term;data models	This paper describes a time-series prediction method based on the kNN technique. The proposed methodology is applied to the 24hour load forecasting problem. Also, based on recorded data, an alternative model is developed by means of a conventional dynamic regression technique, where the parameters are estimated by solving a least squares problem. Finally, results obtained from the application of both techniques to the Spanish transmission system are compared in terms of maximum, average and minimum forecasting errors.	k-nearest neighbors algorithm;least squares;time series	Alicia Troncoso Lora;Jesús Riquelme Santos;José Cristóbal Riquelme Santos;Antonio Gómez Expósito;José Luís Martínez Ramos	2003		10.1007/978-3-540-25945-9_57	data modeling;forecasting;computer science;artificial intelligence;electric energy;time series;short-term memory;operations research;least squares;algorithm;statistics	EDA	9.364148422461668	-24.59077978777274	51815
563e6b0d7a158146ebe5a83c7be248dea7ab6be3	efficiently maintaining moving micro clusters for clustering moving objects	databases;moving cluster feature tree;moving object;cluster algorithm;pattern clustering;object clustering;pediatrics;clustering algorithms software engineering acceleration algorithm design and analysis spatial databases cellular phones animals kinetic theory data structures merging;prediction algorithms;extrapolation;individual object;trees mathematics;vectors;moving cluster feature tree moving microcluster object clustering global clustering vector;clustering algorithms;moving microcluster;vector;global clustering;vectors pattern clustering trees mathematics;timing	Clustering moving objects is a challenging task, especially when space consumption must be flexibly and efficiently adjusted based on dynamic object movement. In this paper we develop an efficient approach to dynamically maintain a small set of moving micro clusters (MMCs) to represent groups of similar moving objects. Global clusters can then be generated by any clustering algorithm, whenever is needed, from these representative MMCs, not from individual objects. Under our approach, each MMC is represented by a vector that summarizes the position and velocity information of its member objects. Based on this summarized information, a set of simple formulas is developed to efficiently predict when the contents of MMCs must be updated. MMCs are also organized into a moving cluster feature (MCF) tree so they can be efficiently merged for conserving space and accelerating global clustering.	algorithm;cluster analysis;global serializability;memory management controller;meta content framework;velocity (software development)	Chih Lai;Edward A. Heuer	2008	2008 IEEE International Conference on System of Systems Engineering	10.1109/SYSOSE.2008.4724194	correlation clustering;prediction;vector;flame clustering;computer science;theoretical computer science;machine learning;data mining;cluster analysis;extrapolation;statistics	DB	-3.036170213050702	-37.78426287517656	51821
9df7f11e8ebda08489cc18c186ff4aecfb039dbc	pattern matching in high energy physics by using neural network and genetic algorithm	pattern matching neural networks data analysis pattern recognition concurrent computing parallel algorithms evolutionary computation genetic mutations testing image analysis;high energy physics instrumentation computing;parallel algorithm;soft computing;multilayer perceptrons;optical resonators;high energy;data analysis;high energy physics;cherenkov counters;pattern matching;pattern recognition;genetic algorithm;genetic algorithms;evolutionary algorithm;cherenkov counters high energy physics instrumentation computing pattern matching genetic algorithms parallel processing multilayer perceptrons optical resonators;cern pattern matching high energy physics neural network genetic algorithm ga data analysis soft computing parallel algorithms two layer neural network forward connections evolutionary algorithm elitistic strategy mutation cross over adaptive probability optical ring imaging cherenkov detector;parallel processing;neural network	In this paper two different approaches to provide information from events by high energy phisics experiments are shown. Usually the representations produced in such experiments are spot-composed and the classical algorithms to be needed for data analysis are time consuming. For this reason the possibility to speed up pattern recognition tasks by soft computing approach with parallel algorithms has been investigated. The first scheme shown in the following is a two layer neural network with forward connections, the second one consists of an evolutionary algorithm with elitistic strategy and mutation and cross-over adaptive probability. Test results of these approaches have been carried out analysing a set of images produced by an optical Ring imaging Cherenkov (RICH) detector at CERN.	artificial neural network;evolutionary algorithm;experiment;genetic algorithm;parallel algorithm;pattern matching;pattern recognition;soft computing	Marcello Castellano;Giuseppe Mastronardi;Vitoantonio Bevilacqua;E. Nappi	2000		10.1109/IJCNN.2000.857891	parallel processing;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;evolutionary algorithm;soft computing;artificial neural network	AI	4.076649126167628	-31.4485800565776	52271
06231f132fdd9b5478c2ecb71f66f50eeb27b961	a machine learning approach to predicting winning patterns in track cycling omnium	performance patterns;respubid20891;cycling;swinburne;cycling races;machine learning;970111 expanding knowledge in the medical and health sciences;cyclists;thermal conductivity;institute of sport exercise and active living iseal;artificial intelligence;0104 statistics;olympic games	This paper presents work on using Machine Learning approaches for predicting performance patterns of medalists in Track Cycling Omnium championships. The omnium is a newly introduced track cycling competition to be included in the London 2012 Olympic Games. It involves six individual events and, therefore, requires strategic planning for riders and coaches to achieve the best overall standing in terms of the ranking, speed, and time in each individual component. We carried out unsupervised, supervised, and statistical analyses on the men’s and women’s historical competition data in the World Championships since 2008 to find winning patterns for each gender in terms of the ranking of riders in each individual event. Our results demonstrate that both sprint and endurance capacities are required for both men and women to win a medal in the omnium. Sprint ability is shown to have slightly more influence in deciding the medalists of the omnium competitions.	machine learning;sprint (software development);supervised learning;unsupervised learning	Bahadorreza Ofoghi;John Zeleznikow;Clare MacMahon;Dan Dwyer	2010		10.1007/978-3-642-15286-3_7	simulation;engineering;operations management;operations research	NLP	2.607901533155543	-35.13370785913854	52727
94229f12639452f4faf457466167ec910729120c	rule extraction from boolean artificial neural networks	random access memory;information acquisition;neural nets;rule extraction;information acquisition rule extraction boolean artificial neural networks;data mining;software engineering;boolean algebra;artificial neural networks;boolean artificial neural networks;guidelines;neural nets boolean algebra knowledge acquisition;knowledge acquisition;neurons;artificial neural networks read write memory neurons data mining information analysis guidelines hardware software engineering boolean algebra random access memory;read write memory;information analysis;artificial neural network;hardware	Artificial neural networks (ANNs) are being widely used in many applications with competitive results. Few studies describe how the acquired information is represented by these structures, how we could extract that information and standard ways of analyzing the results. This work presents a guideline for extracting and modeling the knowledge acquired by Boolean ANNs (BANNs).	artificial neural network;rule induction	Marcelo M. de Barros;Jeferson L. F. Valadares;Teresa Bernarda Ludermir	1999		10.1109/IJCNN.1999.833457	boolean algebra;computer science;theoretical computer science;machine learning;data mining;data analysis;artificial neural network	EDA	5.859997730706173	-28.95006496448992	52731
b6cb9639ece61843827c333a2a244ac19cb3a316	adaptive layered approach using machine learning techniques with gain ratio for intrusion detection systems		Intrusion Detection System (IDS) has increasingly become a crucial issue for computer and network systems. Optimizing performance of IDS becomes an important open problem which receives more and more attention from the research community. In this work, A multi-layer intrusion detection model is designed and developed to achieve high efficiency and improve the detection and classification rate accuracy . we effectively apply Machine learning techniques (C5 decision tree, Multilayer Perceptron neural network and Naïve Bayes) using gain ratio for selecting the best features for each layer as to use smaller storage space and get higher Intrusion detection performance. Our experimental results showed that the proposed multi-layer model using C5 decision tree achieves higher classification rate accuracy, using feature selection by Gain Ratio, and less false alarm rate than MLP and naïve Bayes. Using Gain Ratio enhances the accuracy of U2R and R2L for the three machine learning techniques (C5, MLP and Naïve Bayes) significantly. MLP has high classification rate when using the whole 41 features in Dos and Probe layers.	artificial neural network;feature selection;intrusion detection system;layer (electronics);machine learning;memory-level parallelism;multilayer perceptron;naive bayes classifier;naivety;optimizing compiler	Heba Ezzat Ibrahim;Sherif M. Badr;Mohamed A. Shaheen	2012	CoRR		computer science;artificial intelligence;machine learning;pattern recognition;data mining	ML	7.3954067987592085	-37.65597542708017	52759
0943ad90b9b1e689e812f415059a21d3b1adeb82	a new data mining scheme using artificial neural networks	symbolic rules;neural networks;rule extraction;data mining;pruning;weight freezing;clustering;constructive algorithm;artificial intelligence;neural networks computer	Classification is one of the data mining problems receiving enormous attention in the database community. Although artificial neural networks (ANNs) have been successfully applied in a wide range of machine learning applications, they are however often regarded as black boxes, i.e., their predictions cannot be explained. To enhance the explanation of ANNs, a novel algorithm to extract symbolic rules from ANNs has been proposed in this paper. ANN methods have not been effectively utilized for data mining tasks because how the classifications were made is not explicitly stated as symbolic rules that are suitable for verification or interpretation by human experts. With the proposed approach, concise symbolic rules with high accuracy, that are easily explainable, can be extracted from the trained ANNs. Extracted rules are comparable with other methods in terms of number of rules, average number of conditions for a rule, and the accuracy. The effectiveness of the proposed approach is clearly demonstrated by the experimental results on a set of benchmark data mining classification problems.	artificial neural network;attention deficit hyperactivity disorder;benchmark (computing);black box;classification;connectionism;data mining;decision trees;decision tree;display resolution;experiment;extraction;machine learning;prunes;published database;recursion;rule (guideline);rule induction;trees (plant);verification of theories;algorithm	S. M. Kamruzzaman;A. M. Jehad Sarkar	2011		10.3390/s110504622	computer science;artificial intelligence;pruning;machine learning;data mining;cluster analysis;artificial neural network	ML	2.8204221707661383	-29.344233892157323	52833
a6324f6a6b4a6145e884fe89c98e322381772f67	an overview of the evolution of the concept of testor	discriminating features;fuzzy logic;pattern recognition;feature selection;feature combination;testor	"""In this paper, the historical evolution of the concept of testor is presented. Testors in a bivalued logic, in a k-valued logic, and also in a fuzzy logic are considered, particular considerations about each case are expressed. This concept evolution is presented to English readers for the """"rst time. The authors reviewed this history because testor, in each particular formulation, is an interesting tool for feature selection problems, especially when the descriptions of objects are non-classical. ( 2001 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved."""	column (database);evolution;feature selection;fuzzy logic;fuzzy set;pattern recognition;set theory;similarity measure	Manuel Lazo-Cortés;José Ruiz-Shulcloper;Eduardo Alba-Cabrera	2001	Pattern Recognition	10.1016/S0031-3203(00)00028-5	fuzzy logic;computer science;artificial intelligence;machine learning;pattern recognition;data mining;feature selection	AI	-0.16131800055395937	-26.260630398897337	52984
aca188c1ea84c258a736078a19223e9321d7e22a	mining the most reliable association rules with composite items	frequent pattern;trees mathematics;composite items association rules frequent pattern tree redundant rules;data mining;trees mathematics data mining;association rule;data mining association rules diseases itemsets transaction databases technology management algorithm design and analysis conferences	The issue of mining association rules with composite items was proposed several years ago. Algorithms with composite items have the potential to discover rules which cannot be found out by other algorithms without composite items. However, much redundant rules which are of trivial significance or even incorrect will be also discovered by these algorithms in certain cases. In this paper, the authors design a novel frequent-pattern tree for finding large composite items first. And then how to measure the reliability of these discovered rules with composite items in order to find out the most reliable association rules is discussed	algorithm;association rule learning	Ke Wang;James Nga-Kwok Liu;Weimin Ma	2006	Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)	10.1109/ICDMW.2006.117	association rule learning;computer science;data science;data mining;database	DB	-4.4643498632525995	-36.463580699050596	53083
60c89f7406069a9ca47976e35592e6407b2c9a0a	concept drift detection via competence models	concept drift;classification;incremental supervised learning;journal article;competence model;case base maintenance	Detecting changes of concepts, such as a change of customer preference for telecom services, is very important in terms of prediction and decision applications in dynamic environments. In particular, for case-based reasoning systems, it is important to know when and how concept drift can effectively assist decision makers to perform smarter maintenance operations at an appropriate time. This paper presents a novel method for detecting concept drift in a case-based reasoning system. Rather than measuring the actual case distribution, we introduce a new competence model that detects differences through changes in competence. Our competence-based concept detection method requires no prior knowledge of case distribution and provides statistical guarantees on the reliability of the changes detected, as well as meaningful descriptions and quantification of these changes. This research concludes that changes in data distribution do reflect upon competence. Eight sets of experiments under three categories demonstrate that our method effectively detects concept drift and highlights drifting competence areas accurately. These results directly contribute to the research that tackles concept drift in case-based reasoning, and to competence model studies.	case-based reasoning;concept drift;experiment;reasoning system;sensor	Ning Lu;Guangquan Zhang;Jie Lu	2014	Artif. Intell.	10.1016/j.artint.2014.01.001	biological classification;computer science;knowledge management;artificial intelligence;concept drift;machine learning;data mining	AI	-0.08322968886329843	-34.51933242555681	53271
ede2dcc59ffa9baf86ff2e48728d5e019df698cc	aggregation and disaggregation of fuzzy polygons for spatial-temporal modelling				Geoffrey Edwards	1994			fuzzy logic;polygon;artificial intelligence;mathematics;pattern recognition	AI	1.808765940657947	-24.336819669632483	53346
ef9a1b73c5922abc3e1c0997cf1acc5169d5a41b	knowledge reduction of evaluation dataset based on genetic algorithm and fuzzy rough set	information loss;satellite navigation system combat effectiveness knowledge reduction evaluation dataset genetic algorithm fuzzy rough set attribute reduction;evaluation dataset;fuzzy set theory;genetics;fuzzy rough set;attribute reduction;genetic algorithms fuzzy sets data engineering genetic engineering knowledge engineering satellite navigation systems biological cells computer science software engineering helium;genetic algorithm;genetic algorithms;optimization;approximation methods;navigation system;data handling;rough set;encoding;knowledge based systems;satellite navigation systems;genetic algorithms data handling fuzzy set theory;knowledge reduction fuzzy rough set genetic algorithm evaluation dataset;satellite navigation system combat effectiveness;knowledge reduction	In order to obtain better attribute reduction of the continuous dataset, genetic algorithm and fuzzy rough set were used. By making use of this method, the discretization process of continuous attributes was avoided, and the information loss was reduced, the reduction was quickened, the decision dependency was raised in comparison with the traditional rough set. Here a simulation example was used to test the efficiency of this method firstly. Then it was applied to the reduction of evaluation dataset about satellite navigation system combat effectiveness. The simulation result has shown that it can obtain those input features that are most predictive of a given outcome and realize the dataset preprocess, which is helpful to realize the combat effectiveness evaluation.	discretization;fuzzy set;genetic algorithm;preprocessor;rough set;satellite navigation;simulation	Chengxi Dong;Dewei Wu;Jing He	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1112	genetic algorithm;computer science;artificial intelligence;knowledge-based systems;machine learning;data mining	SE	3.3676620172590876	-29.516079392582018	53605
cab723e7b77983ba543d9909f0da982754b47c52	learning consistent, complete and compact sets of fuzzy rules in conjunctive normal form for regression problems	search space;fuzzy rules;flexible fuzzy rules;genetic fuzzy systems;regression problems;multiobjective optimization;conjunctive normal form;genetic fuzzy system;fuzzy model;multiobjective genetic algorithm;interpretability constrains	When a flexible fuzzy rule structure such as those with antecedent in conjunctive normal form is used, the interpretability of the obtained fuzzy model is significantly improved. However, some important problems appear related to the interaction among this set of rules. Indeed, it is relatively easy to get inconsistencies, lack of completeness, redundancies, etc. Generally, these properties are ignored or mildly faced. This paper, however, focuses on the design of a multiobjective genetic algorithm that properly considers all these properties thus ensuring an effective search space exploration and generation of highly legible and accurate fuzzy models.	conjunctive normal form;fuzzy rule;genetic algorithm	Jorge Casillas;Pedro Martínez;Alicia D. Benítez	2009	Soft Comput.	10.1007/s00500-008-0361-5	fuzzy logic;conjunctive normal form;mathematical optimization;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;multi-objective optimization;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	AI	0.9306849582425154	-27.835039715001837	53736
8cd3cb97b0520cb135539832059f8aba801c4fee	similarity of binary relations based on complete boolean lattices and related results		Rough set theory is concerned with the lower and upper approximations through a binary relation on the universe. Binary relations play an important role in this theory. This paper investigates similarity of binary relations based on complete Boolean lattices. First, rough approximations based on complete Boolean lattices are proposed through the predecessor neighborhood. Second, L-fuzzy topologies induced by binary relations are researched where L is a complete Boolean lattice. Thirdly, similarity of binary relations is introduced by using L-fuzzy topology and the fact that every binary relation is solely similar to some preorder relation is showed. Finally, as its theoretical expansion, a topological problem “when can the given L-fuzzy topology be coincided with the L-fuzzy topology induced by some binary relation where L is a complete Boolean lattice?” is considered and this problem is answered by introducing L-fuzzy approximating spaces.		Ningxin Xie;Gangqiang Zhang;Zhaowen Li	2017	Soft Comput.	10.1007/s00500-016-2086-1	combinatorics;discrete mathematics;boolean network;binary expression tree;topology;binary independence model;boolean algebras canonically defined;binary relation;relation algebra;mathematics;complete boolean algebra;binary decision diagram	Logic	-2.391528032579367	-24.831742151555414	53932
bfc4ea9fbc02ced920181537c8984df9fa2b7098	a new sampling technique for association rule mining	busqueda informacion;extraction information;evaluation performance;performance evaluation;information extraction;information retrieval;evaluacion prestacion;decision maker;data mining;association rule mining;experimental result;sampling;input output;algorithme;algorithm;extraction de donnees;sampling technique;recherche information;resultado experimental;profitability;data reduction;resultat experimental;parameterized sampling;extraccion informacion;algoritmo	Association Rule Mining (ARM) is one of the data mining techniques used to extract hidden knowledge from datasets, that can be used by an organization’s decision makers to improve overall profit. However, performing ARM requires repeated passes over the entire database. Obviously, for large database, the role of input/output overhead in scanning the database is very significant. A popular solution to improve the speed of ARM is to apply the mining algorithm on a sample instead of the entire database. In this paper, a parameterized sampling algorithm for ARM is presented. This algorithm extracts sample datasets based on three parameters: transaction frequency, transaction length and transaction frequency-length. To evaluate its performance and accuracy, a comparison against a two-phase sampling-based algorithm is performed using real and synthetic datasets. The experimental results show that the proposed sampling algorithm in some cases outperforms two-phase sampling algorithm, and achieves up to 98% accuracy.	arm architecture;algorithm;apriori algorithm;association rule learning;binary tree;data mining;database;dhrystone;experiment;information science;input/output;overhead (computing);sampling (signal processing);synthetic intelligence;tree structure;two-phase commit protocol	Basel A. Mahafzah;Amer F. Al-Badarneh;Mohammed Z. Zakaria	2009	J. Information Science	10.1177/0165551508100382	sampling;gsp algorithm;computer science;artificial intelligence;machine learning;data mining;database;fsa-red algorithm;information extraction;statistics	DB	-3.5918727097519367	-33.95997131715415	53938
0326bc549e19349174334a5f23f6c8916666552c	learning interestingness of streaming classification rules	sistema interactivo;streaming;learning algorithm;interet;interes;transmision continua;intelligence artificielle;algorithme apprentissage;classification;systeme conversationnel;transmission en continu;interactive system;voting;classification rules;artificial intelligence;voto;inteligencia artificial;vote;interest;user interaction;algoritmo aprendizaje;clasificacion	Inducing classification rules on domains from which information is gathered at regular periods lead the number of such classification rules to be generally so huge that selection of interesting ones among all discovered rules becomes an important task. At each period, using the newly gathered information from the domain, the new classification rules are induced. Therefore, these rules stream through time and are so called streaming classification rules. In this paper, an interactive rule interestingness-learning algorithm (IRIL) is developed to automatically label the classification rules either as “interesting” or “uninteresting” with limited user interaction. In our study, VFP (Voting Feature Projections), a feature projection based incremental classification learning algorithm, is also developed in the framework of IRIL. The concept description learned by the VFP algorithm constitutes a novel approach for interestingness analysis of streaming classification rules.	arm architecture;algorithm;distress (novel);flickr	Tolga Aydin;H. Altay Güvenir	2004		10.1007/978-3-540-30182-0_7	voting;biological classification;computer science;interest;artificial intelligence;machine learning;data mining;mathematics;electoral-vote.com	ML	7.479596301125097	-33.02312762256587	54073
924a59ae302bbe2a9fb2c27757c003d5e8af934d	a method to determine basic probability assignment in the open world and its application in data fusion and classification	dempster-shafer evidence theory;generalized evidence theory;basic probability assignment;information fusion;triangular fuzzy number	Under the circumstance of the complete frame of discernment, There are quantities sufficient researches applying the Dempster-Shafer evidence theory (D-S theory) to process uncertain information. However, in most real cases, the frame of discernment is not complete, and the classic evidence theory is not applicable in some degree, including the basic probability assignment (BPA) generation method. In this paper, under the assumption of the open world, the BPA determination issue is focused originally, and a new method based on triangular fuzzy member is proposed to determine BPA. First, the mean value, standard deviation, extreme values as well as the triangular membership function of each attribute can be determined. Then, a nested structure BPA function with an assignment for the null set can be constructed, using the intersection point of one test sample and the models above. Experiments are conducted with the proposed method to determine BPA, through which the classification accuracy rates are computed, analyzed and compared with those generated by other BPA determination methods, which demonstrates the efficiency of the proposed method both in the open world and in the closed world.	algorithm;experiment;fuzzy number;information source;multi-source;open world;oracle bpa suite;pattern recognition;source data;state space;statistical classification;test data	Jingfei Zhang;Yong Deng	2016	Applied Intelligence	10.1007/s10489-016-0877-9	artificial intelligence;data mining;algorithm	AI	0.06412299451212178	-28.12953923506385	54118
6f6a87f5ba89c2215f58532325cadd9bb355a306	grey relational effort analysis technique using regression methods for software estimation		Software project planning and estimation is the most important confront for software developers and researchers. It incorporates estimating the size of the software project to be produced, estimating the effort required, developing initial project schedules, and ultimately, estimating on the whole cost of the project. Numerous empirical explorations have been performed on the existing methods, but they lack convergence in choosing the best prediction methodology. Analogy based estimation is still one of the most extensively used method in industry which is based on finding effort from similar projects from the project repository. Two alternative approaches using analogy for estimation have been proposed in this study. Firstly, a precise and comprehensible predictive model based on the integration of Grey Relational Analysis (GRA) and regression has been discussed. Second approach deals with the uncertainty in the software projects, and how fuzzy set theory in fusion with grey relational analysis can minimize this uncertainty. Empirical results attained are remarkable indicating that the methodologies have a great potential and can be used as a candidate approaches for software effort estimation. The results obtained using both the methods are subjected to rigorous statistical testing using Wilcoxon signed rank test.	cost estimation in software engineering;endeavour (supercomputer);feature selection;fuzzy set;grey relational analysis;schedule (computer science);set theory;software developer;software development effort estimation;software project management	Geeta Nagpal;Moin Uddin;Arvinder Kaur	2014	Int. Arab J. Inf. Technol.		putnam model;software sizing;artificial intelligence;machine learning;analysis effort method;data mining;algorithm;software metric;statistics	SE	3.4148008637088947	-33.325791525899895	54198
a7024bf494bcbd7732bd2a157f082e757e3f8215	student survey by information-theoretic competitive learning	costs data mining principal component analysis feature extraction computer vision cybernetics data analysis neural networks information science laboratories;competitive learning;data analysis;educational administrative data processing;information theoretic competitive learning;data analyses;learning artificial intelligence data analysis educational administrative data processing information theory;learning artificial intelligence;information theoretic;student survey;information theory;information theoretic neural methods;student survey information theoretic competitive learning information theoretic neural methods data analyses	In this paper, we apply our information-theoretic method to a student survey. The information-theoretic method aims to extract main features in input patterns by condensing information contained in input patterns as much as possible. By using 2500 students' responses to the questionnaire, we could extract the main subjects in which the majority of students have interest. Especially, we could classify students into several groups by their interest. On the other hand, the conventional PCA could not demonstrate specific features in input patters. Thus, the information-theoretic neural methods can open up a new perspective for data analyses.	competitive learning;information theory	Ryotaro Kamimura	2006	2006 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2006.385123	information theory;computer science;artificial intelligence;data science;machine learning;data mining;competitive learning;data analysis;statistics	Robotics	5.797723743380745	-34.88621765400907	54251
80a00771dcbe1e6cc7adbc616434f1c0c8396419	a new type of simplified fuzzy rule-based system	recursive least square;data density and distribution;fuzzy rule based system;fuzzy set;takagi sugeno;fuzzy rule based systems;data association;data distribution;qa75 electronic computers computer science;fuzzy rule base;aggregation operator;spatial distribution;clustering;mamdani and takagi sugeno fuzzy systems;membership function;fuzzy system;recursive least square estimation	Over the last quarter of a century, two types of fuzzy rule-based (FRB) systems dominated, namely Mamdani and Takagi–Sugeno type. They use the same type of scalar fuzzy sets defined per input variable in their antecedent part which are aggregated at the inference stage by t-norms or co-norms representing logical AND/OR operations. In this paper, we propose a significantly simplified alternative to define the antecedent part of FRB systems by data Clouds and density distribution. This new type of FRB systems goes further in the conceptual and computational simplification while preserving the best features (flexibility, modularity, and human intelligibility) of its predecessors. The proposed concept offers alternative non-parametric form of the rules antecedents, which fully reflects the real data distribution and does not require any explicit aggregation operations and scalar membership functions to be imposed. Instead, it derives the fuzzy membership of a particular data sample to a Cloud by the data dens...	fuzzy rule;rule-based system	Plamen P. Angelov;Ronald R. Yager	2012	Int. J. General Systems	10.1080/03081079.2011.634807	discrete mathematics;membership function;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set;cluster analysis;fuzzy control system	Logic	1.0380404531856608	-25.815324256544745	54259
15bac53a7229c7cc9a4669188c76e6a3aecd5c06	garc: a new associative classification approach	critere selection;extraction information;association statistique;generalized association rule;analisis datos;information extraction;selection criterion;systeme aide decision;competitividad;statistical association;almacen dato;sistema ayuda decision;criterio seleccion;data mining;classification;regla seleccion;data analysis;selection rule;decision support system;asociacion estadistica;classifier;generic basis;association rule;associative classification;fouille donnee;generic association rules;classification rules;decouverte connaissance;competitiveness;descubrimiento conocimiento;analyse donnee;regle selection;entrepot donnee;data warehouse;classification accuracy;competitivite;busca dato;clasificacion;extraccion informacion;knowledge discovery	Many studies in data mining have proposed a new classification approach called associative classification. According to several reports associative classification achieves higher classification accuracy than do traditional classification approaches. However, the associative classification suffers from a major drawback: it is based on the use of a very large number of classification rules; and consequently takes efforts to select the best ones in order to construct the classifier. To overcome such drawback, we propose a new associative classification method called Garc that exploits a generic basis of association rules in order to reduce the number of association rules without jeopardizing the classification accuracy. Moreover, Garc proposes a new selection criterion called score, allowing to ameliorate the selection of the best rules during classification. Carried out experiments on 12 benchmark data sets indicate that Garc is highly competitive in terms of accuracy in comparison with popular associative classification methods.	association rule learning;benchmark (computing);categorization;data mining;discretization;document classification;executable;experiment;liverpool	Ines Bouzouita;Samir Elloumi;Sadok Ben Yahia	2006		10.1007/11823728_53	association;association rule learning;decision support system;classifier;biological classification;computer science;machine learning;data warehouse;pattern recognition;data mining;mathematics;selection rule;data analysis;information extraction	ML	8.594141613118754	-33.59395180533066	54621
7edbb93b5e88130f5120693b0adb91a91fbc01f8	rough-fuzzy granular computing, case based reasoning and data mining	granular computing;computability theory;case base reasoning;soft computing;fuzzy granulation;fuzzy integral;data mining;large scale;granular computation;pattern recognition;rough sets;case based reasoning;point of view;rough set;knowledge discovery	VHDL high level modelling and implementaiton of fuzzy systems p. 11 Some complexity results on fuzzy description logics p. 19 An evolutionary approach to ontology-based user model acquisition p. 25 Mathematical modeling of passage dynamic function p. 33 Bi-monotonic fuzzy sets lead to optimal fuzzy interfaces p. 39 Conversational agent model in intelligent user interface p. 46 A fuzzy frame-based knowledge representation formalism p. 55 Statistical analysis of the different operator involved in the fuzzy inference process p. 63 Concepts and fuzzy models for behavior-based robotics p. 72 Mathematical aspects of fuzzy control p. 80 Piecewise linear fuzzy sliding mode control p. 89 Application of fuzzy logic controllers for laser tracking with autonomous robot system p. 97 Fuzzy relational neural network for data analysis p. 103 A neuro-fuzzy system for the prediction of the vehicle traffic flow p. 110 On the use of neuro-fuzzy techniques for analyzing experimental surface electromyographic data p. 119 Linear regression model-guided clustering for training RBF networks for regression problems p. 127 An iterative algorithm for fuzzy quadratic programming problems p. 133 A general defuzzification method for fuzzy total cost in an inventory without backorder case p. 140 Fuzzy rough sets and multiple-premise gradual decision rules p. 148 Fuzzy spatial relationships for model-based pattern recognition in images and spatial reasoning under imprecision p. 164 Classification of digital terrain models through fuzzy clustering : an application p. 174 Evolutionary approach to inverse planning in coplanar radiotherapy p. 183 Soft pyramid symmetry transforms p. 191 Image file compression using approximation and fuzzy logic p. 200 Fuzzy information fusion scheme used to segment brain tumor from MR images p. 208 Out-of-core segmentation by deformable models p. 216 Rough set approach for classification of breast cancer mammogram images p. 224 Genetic Fourier descriptor for the detection of rotational symmetry p. 232 Fourier transform based column-block and row-block matching procedure for document image mosaicing p. 240 Object recognition by recursive learning of multiscale trees p. 255 An integrated fuzzy cells-classifer p. 263 A neural network for classification of chambers arrangement in foraminifera p. 271 Fuzzy concepts in vector quantization training p. 279 Some component analysis based on fuzzy relational structure p. 289 Fuzzy technique based recognition of handwritten characters p. 297	approximation;artificial neural network;autonomous robot;behavior-based robotics;case-based reasoning;cluster analysis;coplanar waveguide;data compression;data mining;defuzzification;description logic;dialog system;digital elevation model;electromyography;fast fourier transform;fuzzy clustering;fuzzy control system;fuzzy logic;fuzzy set;granular computing;high-level programming language;intelligent agent;intelligent user interface;iterative and incremental development;iterative method;knowledge representation and reasoning;mathematical model;neuro-fuzzy;out-of-core algorithm;outline of object recognition;pattern recognition;quadratic programming;radial basis function network;recursion;rough set;semantics (computer science);spatial–temporal reasoning;vhdl;vector quantization	Sankar K. Pal	2003		10.1007/10983652_1	rough set;granular computing;computer science;artificial intelligence;machine learning;data mining;soft computing	AI	3.688550569661452	-25.863408729000003	55028
b527ec7db2bfaf2b9349fcc7584dbb0aa65fb02c	on the idea of using granular rough mereological structures in classification of data	rough inclusions;granulation of knowledge;rough sets;granular classifiers;rough set	This paper is devoted to an exposition of the idea of using granular structures obtained from data in the classification tasks of these data into decision classes. Classifiers are induced from granular reflections of data sets.	granular computing;mereology;reflection (computer graphics)	Lech Polkowski	2008		10.1007/978-3-540-79721-0_32	rough set;computer science;artificial intelligence;machine learning;data mining;mathematics	ML	1.3997006880894287	-27.98424039835163	55234
e44da9928903702a77977c8ab2af124cff0f8e3b	critiques on some combination rules for probability theory based on optimization techniques	dissimilar sensor fusion probability theory combination rules similar sensor fusion;decision level identity fusion;optimisation;fusion rules probability theory optimization techniques decision level identity fusion objective function dissimilar sensor fusion;sensor phenomena and characterization;probability;fusion rules;optimization technique;uncertainty;fuses;dissimilar sensor fusion;fuzzy set theory;sensor fusion sensor phenomena and characterization fuses uncertainty decision support systems fusion power generation possibility theory fuzzy set theory optimization methods redundancy;objective function;redundancy;similar sensor fusion;decision support systems;optimization techniques;probability theory;sensor fusion optimisation probability;combination rules;fusion power generation;possibility theory;fusion rule;sensor fusion;optimization methods	A crucial point in the decision-level identity fusion is to combine information in an appropriate way to generate an optimal decision, according to the individual information coming from a set of different sensors. An interesting approach was developed for the decision- level identity fusion, which use optimization techniques to minimize an objective function which measure the dissimilarities between the combination result and the set of initial sensor reports. Several objective functions were already proposed for the similar sensor fusion (SSF) and the dissimilar sensor fusion (DSF) models. In this paper, we present these fusion methods, we raise some questions and make some improvements, and finally we study the behaviour of these fusion rules on several examples.	conjunctive query;end-to-end principle;loss function;mathematical optimization;optimization problem;partition type;scott continuity;sensor web;shortest seek first	Mihai Cristian Florea;Éloi Bossé	2007	2007 10th International Conference on Information Fusion	10.1109/ICIF.2007.4408062	machine learning;pattern recognition;data mining;mathematics	Robotics	-0.7784793655094012	-27.852923324578956	55749
622e9d8ca3077d0519f3a580bc4f1db63711b6d5	granular modeling: the synergy of granular computing and fuzzy logic	granular computing;approximation error;hybrid intelligent systems;multiplexing;fuzzy sets;fuzzy logic;network topology;fuzzy logic fuzzy sets clustering algorithms fuzzy systems network topology multiplexing approximation error hybrid intelligent systems;clustering algorithms;fuzzy systems		fuzzy logic;granular computing;synergy	Witold Pedrycz	2004		10.1109/ICHIS.2004.50	control engineering;fuzzy electronics;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;fuzzy number;theoretical computer science;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;fuzzy set operations;fuzzy control system	EDA	3.5510386615575174	-25.267735027825296	55841
3500f71dd487350aa33cb563069cac4a3495816e	an energy data-driven decision support system for high performance in industrial injection moulding and stamping systems	energy patterns energy data driven decision support system industrial injection moulding system stamping system dss real time energy measurements process operational states high performance manufacturing intelligent framework process identification framework haar transform empirical bayesian threshold power time series segmentation support vector machines power segment clustering process operational state identification;time series bayes methods decision support systems haar transforms injection moulding manufacturing industries pattern clustering production engineering computing support vector machines;decision support systems time series analysis injection molding support vector machines manufacturing decision making energy consumption	In this paper, a unified decision support system (DSS) is proposed, which uses real-time energy measurements and process operational states to make effective decisions, enabling high-performance manufacturing. To reduce the number of required sensors and amount of logged data, our proposed DSS includes an intelligent framework which identifies the process operational states based on energy measurements. This process identification framework uses Haar transform and empirical Bayesian threshold to segment the power time series and support vector machines to cluster the power segments into groups according to the underlying process operational states. To justify our proposed framework, comparative experiments with an existing framework are evaluated on two industrial applications, an injection moulding system and a stamping system. Experiment results show that our proposed framework is more effective in identifying the process operational states using the energy patterns.	decision support system;experiment;haar wavelet;next-generation network;real-time clock;real-time transcription;sensor;support vector machine;time series	Chee Khiang Pang;Cao Vinh Le	2014	11th IEEE International Conference on Control & Automation (ICCA)	10.1109/ICCA.2014.6871074	engineering;artificial intelligence;machine learning;data mining;engineering drawing	Robotics	1.1781186129979961	-33.01288777776641	56064
7c6a0a93db020eb55a8aeda2e10eec2ddd030400	a parallel neural network approach to prediction of parkinson's disease	parallel neural networks;feed forward neural network;rule based system;decision support system;engineering and technology;teknik och teknologier;parkinson s disease;networked systems;imbalanced data sets;neural network	Recently the neural network based diagnosis of medical diseases has taken a great deal of attention. In this paper a parallel feed-forward neural network structure is used in the prediction of Parkinson's Disease. The main idea of this paper is using more than a unique neural network to reduce the possibility of decision with error. The output of each neural network is evaluated by using a rule-based system for the final decision. Another important point in this paper is that during the training process, unlearned data of each neural network is collected and used in the training set of the next neural network. The designed parallel network system significantly increased the robustness of the prediction. A set of nine parallel neural networks yielded an improvement of 8.4% on the prediction of Parkinson's Disease compared to a single unique network. Furthermore, it is demonstrated that the designed system, to some extent, deals with the problems of imbalanced data sets.		Freddie Åström;Rasit Koker	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.04.028	stochastic neural network;nervous system network models;feedforward neural network;probabilistic neural network;decision support system;computer science;artificial intelligence;recurrent neural network;machine learning;data mining;time delay neural network;deep learning;artificial neural network	ML	8.138550933537053	-27.346165399905864	56387
042a15fe80c630f6a0d9f00135b35f7fd90ee7ff	integrating fuzziness with olap association rules mining	tratamiento datos;extraction information;metodo analitico;census;on line processing;analisis datos;information extraction;fuzzy data;logique floue;data processing;logica difusa;traitement donnee;intelligence artificielle;data mining;association rule mining;fuzzy logic;tratamiento en linea;data analysis;fouille donnee;analytical method;decouverte connaissance;methode analytique;artificial intelligence;descubrimiento conocimiento;analyse donnee;censo;recensement;inteligencia artificial;traitement en ligne;structural integrity;busca dato;extraccion informacion;on line analytical processing;knowledge discovery;fuzzy association rules	This paper handles the integration of fuzziness with On-Line Analytical Processing (OLAP) association rules mining. It contributes to the ongoing research on multidimensional online data mining by proposing a general architecture that uses a fuzzy data cube for knowledge discovery. Three different methods are introduced to mine fuzzy association rules in the constructed fuzzy data cube, namely single dimension, multidimensional and hybrid association rules mining; the third structure integrates the other two methods. To the best of our knowledge, this is the first effort in this direction. Experimental results obtained for each of the three methods on the adult data of the United States census in 2000 show the effectiveness and applicability of the proposed mining approach.	online analytical processing	Mehmet Kaya;Reda Alhajj	2003		10.1007/3-540-45065-3_31	fuzzy logic;census;association rule learning;data processing;computer science;artificial intelligence;data mining;data analysis;information extraction	DB	-2.3922721129012077	-32.182988151935746	56639
479c70ae7d5e9178679ef8d5a3c8ce0899ec80c8	a gmdh-based fuzzy modeling approach for constructing ts model	takagi sugeno model;group method of data handling gmdh;journal;fuzzy modeling;noise immunity	In this paper, a new learning algorithm based on group method of data handling (GMDH) is proposed for the identification of Takagi-Sugeno fuzzy model. Different from existing methods, the new approach, called TS-GMDH, starts from simple elementary TS fuzzy models, and then uses the mechanism of GMDH to produce candidate fuzzy models of growing complexity until the TS model of optimal complexity has been created. The main characteristic of the new approach is its ability to identify the structure of TS model automatically. Experiments on Box-Jenkins gas furnace data and UCI datasets have shown that the proposed method can achieve satisfactory results and is more robust to noise in comparison with other TS modeling techniques such as ANFIS.	group method of data handling	Bing Zhu;Changzheng He;Panos Liatsis;Xiaoyu Li	2012	Fuzzy Sets and Systems	10.1016/j.fss.2011.08.004	artificial intelligence;machine learning;data mining	SE	5.203856240194539	-27.494169481769884	56655
ac714815d150d3d9ca30ac4918877bbec0f6d255	a soft k-nearest neighbor voting scheme	image processing;algoritmo borroso;logique floue;procesamiento imagen;logica difusa;classification;traitement image;fuzzy logic;vecino mas cercano;voting;fuzzy algorithm;pattern recognition;algorithme flou;plus proche voisin;nearest neighbour;k nearest neighbor;voto;reconnaissance forme;vote;reconocimiento patron;clasificacion	Abstract#R##N##R##N#The K-Nearest Neighbor (K-NN) voting scheme is widely used in problems requiring pattern recognition or classification. In this voting scheme an unknown pattern is classified according to the classifications of its K nearest neighbors. If a majority of the K nearest neighbors have a given classification C*, then the unknown pattern is also given the classification C*. Although the scheme works well it is sensitive to the number of nearest neighbors, K, which is used. In this paper we describe a fuzzy K-NN voting scheme in which effectively the value of K varies automatically according to the local density of known patterns. We find that the new scheme consistently outperforms the traditional K-NN algorithm. © 2001 John Wiley & Sons, Inc.	k-nearest neighbors algorithm	H. B. Mitchell;Paul A. Schaefer	2001	Int. J. Intell. Syst.	10.1002/int.1018	fuzzy logic;voting;image processing;biological classification;computer science;artificial intelligence;machine learning;data mining;mathematics;k-nearest neighbors algorithm;electoral-vote.com	DB	8.842908433606555	-32.82999054730724	56712
f196899bd871f24e33366819f2555dc11c26dc53	gas identification based on committee machine for microelectronic gas sensor	pattern classification array signal processing chemical analysis gas sensors integrated circuits;density models;neural networks gas identification committee machine microelectronic gas sensor pattern recognition systems in house sensor array signals ensemble machine giem gaussian mixture models density models gas sensor array;neural networks;ensemble method;gas identification;committee machine;microelectronics gas detectors sensor arrays pattern recognition signal processing combustion gases neural networks assembly voting;chemical analysis;array signal processing;indexing terms;classification;microelectronic gas sensor;giem;gas sensor;gas sensor array;pattern recognition classification density models ensemble methods gas sensor array neural networks;gaussian mixture model;ensemble methods;gaussian mixture models;pattern classification;sensor array;pattern recognition;in house sensor array signals;ensemble machine;classification accuracy;pattern recognition systems;gas sensors;integrated circuits;neural network	Gas identification represents a big challenge for pattern recognition systems due to several particular problems such as nonselectivity and drift. The purpose of this paper is twofold: 1) to compare the accuracy of a range of advanced and classical pattern recognition algorithms for gas identification for the in-house sensor array signals and 2) to propose a gas identification ensemble machine (GIEM), which combines various gas identification algorithms, to obtain a unified decision with improved accuracy. An integrated sensor array has been designed with the aim of identifying combustion gases. The classification accuracy of different density models is compared with several neural network architectures. On the gas sensors data used in this paper, Gaussian mixture models achieved the best performance with higher than 94% accuracy. A committee machine is implemented by assembling the outputs of these gas identification algorithms through advanced voting machines using a weighting and classification confidence function. Experiments on real sensors' data proved the effectiveness of the system with an improved accuracy over the individual classifiers. An average performance of 97% was achieved using the proposed committee machine	algorithm;artificial neural network;best, worst and average case;committee machine;experiment;mixture model;pattern recognition;sensor	Minghua Shi;Amine Bermak;Sofiane Brahim-Belhouari;Philip C. H. Chan	2006	IEEE Transactions on Instrumentation and Measurement	10.1109/TIM.2006.880956	computer science;engineering;machine learning;pattern recognition;mixture model;data mining;artificial neural network	ML	7.911095232558401	-26.900725522307493	56831
9f57c7b8911e300364d81b03d612d51f9220e9e0	construction and assessment of classification rules, by d.j. hand	classification rules	  		Glenn W. Milligan	2000	J. Classification	10.1007/s003570000025	pattern recognition;mathematics	ML	9.05134122294926	-36.61791307374233	57075
37e041c9774f28b11ec65d96afcdf3365e22740d	on generalization of rough sets by using two different methods			rough set	Mona Hosny	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-172078	machine learning;mathematics;discrete mathematics;artificial intelligence;rough set	Robotics	2.0840808905156325	-24.900151684683966	57110
019140717db3d93f83c7ecb9e3c44406aeb567a0	instantiated first order qualitative choice logic for an efficient handling of alerts correlation		Intrusion Detection Systems (IDS) are necessary and important tools for monitoring information systems. However they produce a huge quantity of alerts. Alerts correlation is a process that reduces the number of alerts reported by intrusion detection systems. In this paper, we propose a new algorithm for a logical-based alerts correlation approach that integrates: security operator's knowledge and preferences. The representation and the reasoning on these knowledge and preferences are done using a new logic called Instantiated First Order Qualitative Choice Logic (IFO-QCL). Our modeling views an alert as an interpretation which allows us to have an efficient algorithm that performs the correlation process in a polynomial time. This paper also provides experimental results which are achieved on datasets issued from a real monitoring system.		Lydia Bouzar-Benlabiod;Salem Benferhat;Thouraya Bouabana-Tebibel	2015	Intell. Data Anal.	10.3233/IDA-140693	computer science;machine learning;data mining;computer security	AI	-3.2038017559896956	-29.806835457754165	57327
043c0b2ceffd6342193e972cdac748d553805f87	a fast geometric method for defuzzification of type-2 fuzzy sets	type 2 fuzzy sets;fuzzy sets fuzzy logic uncertainty computational complexity fuzzy systems geometry real time systems hardware solids iterative methods;type reduction;uncertainty;conference;geometric representation;geometry;computational geometry;fuzzy set theory computational geometry;fuzzy set theory;fuzzy sets;fuzzy logic;iterative methods;computational geometry defuzzification type 2 fuzzy sets geometric representation geometric operation membership function;computational complexity;membership function;type 2 fuzzy set;fuzzy logic system;type 2 fuzzy logic;geometric operation;article;type reduction computational geometry defuzzification type 2 fuzzy logic;fuzzy systems;defuzzification;solids;hardware;real time systems	Generalized type-2 fuzzy logic systems cannot currently be used for practical problems because the amount of computation required to defuzzify a generalized type-2 fuzzy set is too large. This paper presents a new method for defuzzifing a type-2 fuzzy set. The new much faster technique is based on geometric representations and operations. The results of a real world example contained in this paper show this new approach to be over 200,000 times faster than type-reduction. We present a new method for assessing the accuracy of the membership function of a type-2 fuzzy set. This method is used to show that the new representation used by the defuzzifier is not detrimental to the accuracy of the set. We also discuss the differences between the new approach and type-reduction, identifying the origin of this massive improvement in execution speed.	computation;defuzzification;design tool;discretization;expressive power (computer science);formal system;fuzzy control system;fuzzy set;membership function (mathematics);mobile robot;type-2 fuzzy sets and systems	Simon Coupland;Robert Ivor John	2008	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2008.924345	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;computational geometry;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	0.1916692577909948	-24.96650568245944	57618
6430bcfc57e7ff285e55fb9dd5b7a2c5a2451be8	learning bayesian belief network classifiers: algorithms and system	bayes estimation;extraction information;reseau croyance;belief;learning algorithm;analisis datos;information extraction;intelligence artificielle;algorithme apprentissage;data mining;classification;data analysis;estimacion bayes;croyance;fouille donnee;bayesian belief network;artificial intelligence;analyse donnee;inteligencia artificial;creencia;algoritmo aprendizaje;belief net;clasificacion;estimation bayes;extraction informacion	This paper investigates the methods for learning Bayesian belief network (BN) based predictive models for classification. Our primary interests are in the unrestricted Bayesian network and Bayesian multi-net based classifiers. We present our algorithms for learning these classifiers and also the methods for fighting the overfitting problem. A natural method for feature subset selection is also studied. Using a set of standard classification problems, we empirically evaluate the performance of various BN based classifiers. The results show that the proposed BN and Bayes multi-net classifiers are competitive with (or superior to) the best known classifiers, based on both BN and other formalisms; and that the computational time for learning and using these classifiers is relatively small. We also briefly introduce our BN classifier learning system – BN PowerPredictor. We argue that BN based classifiers deserve more attention in the data mining community.	algorithm;bayesian network;data mining;feature selection;fractal dimension;machine learning;overfitting;predictive modelling;subject-matter expert;time complexity	Jie Cheng;Russell Greiner	2001		10.1007/3-540-45153-6_14	random subspace method;cascading classifiers;biological classification;computer science;artificial intelligence;belief;machine learning;pattern recognition;bayesian network;data analysis;information extraction	ML	8.91469414005848	-32.68845885544737	57682
112414c7ff9d880d47a860c790b6e6e6b8574187	building classes in object-based languages by automatic clustering	systeme intelligent;procesamiento informacion;adquisicion del conocimiento;systeme apprentissage;dissimilarity measure;sistema inteligente;acquisition connaissances;classification;database management;learning systems;knowledge acquisition;information processing;intelligent system;image processing and computer vision;information system;traitement information;information storage and retrieval;clasificacion;systeme information;artificial intelligence incl robotics pattern recognition it in business;sistema informacion	The paper deals with clustering of objects described both by properties and relations. Relational attributes may make object descriptions recursively depend on themselves so that attribute values cannot be compared before objects themselves are. An approach to clustering is presented whose core element is an object dissimilarity measure. All sorts of object attributes are compared in a uniform manner with possible exploration of the existing taxonomic knowledge. Dissimilarity values for mutually dependent object couples are computed as solutions of a system of linear equations. An example of building classes on objects with self-references demonstrates the advantages of the suggested approach.	cluster analysis;linear equation;object-based language;recursion;system of linear equations	Petko Valtchev	1999		10.1007/3-540-48412-4_26	information processing;biological classification;computer science;artificial intelligence;machine learning;data mining;database;information system;algorithm	AI	-2.8501621434347366	-31.674087966373836	57698
1893f3f374185d900d419c757a7ce9f571ead095	privacy preserving sub-feature selection in distributed data mining	fuzzy random variable;fuzzy probability;feature selection;distributed data mining;privacy	This paper addresses the selection of sub-feature from each feature using fuzzy methodologies maintaining the privacy during collection of data from participating parties in distributed environment. Based on fuzzy random variables conditional expectation is used in which two fuzzy sets are generated using Borel set that helps to determine sub-feature within certain interval. The privacy and selection of sub-feature leading to a distinguished class is the main objective of this research work. These two problems are directly related to data mining problems of classification and characterization of feature. In many cases traditional techniques are not suitable for complex databases. However our methodology provides better way for selection of sub-features under different situations. The proposed model and techniques both presents extensive theoretical analysis and experimental results. The experiments show the effectiveness and performance based on real world data set. © 2015 Published by Elsevier B.V.	data mining;database;experiment;feature selection;fuzzy concept;fuzzy set;privacy;regular expression	Hemanta Kumar Bhuyan;Narendra Kumar Kamila	2015	Appl. Soft Comput.	10.1016/j.asoc.2015.06.060	computer science;machine learning;data mining;database;privacy;feature selection;fuzzy set operations;statistics	DB	0.22797495967431658	-37.55372179478003	57732
2c2f7f4508e688fdc05646c236968c11789e0225	housing market hedonic price study based on boosting regression tree			decision tree learning;hedonic regression	Guangtong Gu;Bing Xu	2017	JACIII	10.20965/jaciii.2017.p1040	hedonic index;computer science;pattern recognition;artificial intelligence;machine learning;boosting (machine learning);gradient boosting;decision tree	ECom	9.72347671199385	-25.944176448666546	57889
aa26c66678bfac5688e362cfcb25f21cc76dc3bc	hypersurface classifiers ensemble for high dimensional data sets	high dimensional dataset;dimensionalidad;base dato multidimensional;haute performance;hipersuperficie;classifier ensemble;high dimensionality;modele agrege;base de donnees multidimensionnelle;dimension reduction;base donnee tres grande;dimensionality;modelo agregado;multidimensional database;classification;three dimensional;reduction dimension;dimensionnalite;high dimensional data;alto rendimiento;aggregate model;reduccion dimension;very large databases;reseau neuronal;high performance;clasificacion;red neuronal;hypersurface;neural network	Based on Jordan Curve Theorem, a universal classification method called HyperSurface Classifier (HSC) has recently been proposed. Experimental results show that in three-dimensional space, this method works fairly well in both accuracy and efficiency even for large size data up to 10. However, what we really need is an algorithm that can deal with data not only of massive size but also of high dimensionality. In this paper, an approach based on the idea of classifiers ensemble by dimension dividing without dimension reduction for high dimensional data is proposed. The most important difference between HSC ensemble and the traditional ensemble is that the sub-datasets are obtained by dividing the features rather than by dividing the sample set. Experimental results show that this method has a preferable performance on high dimensional	algorithm;dimensionality reduction;experiment;naive bayes classifier;statistical classification	Xiu-Rong Zhao;Qing He;Zhongzhi Shi	2006		10.1007/11759966_193	three-dimensional space;hypersurface;combinatorics;curse of dimensionality;biological classification;computer science;machine learning;mathematics;geometry;artificial neural network;dimensionality reduction;clustering high-dimensional data	AI	9.899273687341568	-33.07309919199479	57967
c50b96ab6711df270b83a034d544033ec3158645	sw cost estimation: measuring model performance of arbitrary function approximators	empirical study;model performance;function approximation;software development;cross validation;cost estimation;working paper	Estimating software development cost with high accuracy is still a largely unsolved problem. Consequently, there is ongoing, high activity in this research field; a large number of different estimation models ranging from mathematical functions to arbitrary function approximators (AFA’s) have been proposed over the last 20+ years. Unfortunately, the studies do not converge with respect to the question “which model is best?” when functions and AFA’s are compared. So far, it has not been understood why this is so. In this empirical study, we show that this is due to inappropriate validation methods as far as the validation of AFA’s is concerned. In fact, the de facto validation method, crossvalidation combined with MMRE, will give completely arbitrary results for AFA’s. Obviously, other criteria are called for in order to appropriately assess the performance of AFA’s. This should be a topic of future research.	alternating finite automaton;converge;cost estimation in software engineering;norm (social);shattered world;software development	Ingunn Myrtveit;Erik Stensrud	2004			simulation;computer science;artificial intelligence;operations management	DB	3.6608538934966846	-32.699785620328875	57998
1a1e746afd0423bd6d58cf674483260becf38bcc	enhanced and hierarchical structure algorithm for data imbalance problem in semantic extraction under massive video dataset	semantic indexing;massive video dataset;data imbalance;enhanced and hierarchical structure ehs;surveillance event detection;trecvid	Data imbalance problem often exists in our real life dataset, especial for massive video dataset, however, the balanced data distribution and the same misclassification cost are assumed in traditional machine learning algorithms, thus, it will be difficult for them to accurately describe the true data distribution, and resulting in misclassification. In this paper, the data imbalance problem in semantic extraction under massive video dataset is exploited, and enhanced and hierarchical structure (called EHS) algorithm is proposed. In proposed algorithm, data sampling, filtering and model training are considered and integrated together compactly via hierarchical structure algorithm, thus, the performance of model can be improved step by step, and is robust and stability with the change of features and datasets. Experiments on TRECVID2010 Semantic Indexing demonstrate that our proposed algorithm has much more powerful performance than that of traditional machine learning algorithms, and keeps stable and robust when different kinds of features are employed. Extended experiments on TRECVID2010 Surveillance Event Detection also prove that our EHS algorithm is efficient and effective, and reaches top performance in four of seven events.	adaboost;algorithm;european home systems protocol;experiment;machine learning;real life;sampling (signal processing);sed	Zan Gao;Longfei Zhang;Ming-yu Chen;Alexander G. Hauptmann;Hua Zhang;Anni Cai	2012	Multimedia Tools and Applications	10.1007/s11042-012-1071-7	computer science;data science;machine learning;data mining	ML	-0.6816712414823388	-37.66558896656565	58028
514d456891729e196b871c9281415588b5b2a3ee	an extended anfis architecture and its learning properties for type-1 and interval type-2 models	least squares estimate method extended anfis architecture adaptive network based fuzzy inference system learning properties fuzzification process;fuzzy logic;computer architecture;computational modeling;fuzzy logic computer architecture fuzzy systems frequency selective surfaces computational modeling ieee 802 11 standard computer science;computer science;ieee 802 11 standard;fuzzy systems;least squares approximations adaptive systems estimation theory fuzzy reasoning learning artificial intelligence;frequency selective surfaces	In this paper, an extended ANFIS architecture is proposed. By incorporating an extra layer for the fuzzification process, the extended architecture is able to fit both type-1 and interval type-2 models. The learning properties of the proposed architecture based on the least-squares estimate method are studied on selected type-1 and interval type-2 ANFIS models. We show that the least-squares estimate method in general behaves differently for interval type-2 ANFIS models compared to type-1 ANFIS models, producing larger errors for interval type-2 ANFIS.	adaptive neuro fuzzy inference system;algorithm;fuzzy set;least squares;os-tan	Chao Chen;Robert Ivor John;Jamie Twycross;Jonathan M. Garibaldi	2016	2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2016.7737742	fuzzy logic;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;computational model;fuzzy control system	Robotics	6.393111578492953	-26.922821265813404	58151
d1bab497044e0cdd113b6756f0f422978e7ca072	soft computing in intrusion detection: the state of the art	swarm intelligence;fuzzy reasoning;decision trees dt;soft computing;dempster shafer d s;intrusion detection;artificial immune systems ais;ensemble combinations;artificial neural networks ann;bayes reasoning;intrusion detection systems ids;self organizing maps som;evolutionary computing ec;state of the art;hidden markov model hmm;feature selection;hybrid combinations	The state of the art is explored in using soft computing (SC) methods for network intrusion detection, including the examination of efforts in ten specific areas of SC as well as consecutive, ensemble, and hybrid combinations. Numerous comparisons of these methods are listed followed by a recommendation for future research. This paper can be used as a reference of strategies, and as a resource for planning future research.	data pre-processing;evolutionary computation;firewall (computing);fuzzy classification;intrusion detection system;logic programming;malware;mechatronics;network packet;no silver bullet;packet analyzer;programming paradigm;relevance;router (computing);sensor;soft computing;swarm intelligence;synergy;test data	Chet Langin;Shahram Rahimi	2010	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-010-0012-4	intrusion detection system;swarm intelligence;computer science;artificial intelligence;machine learning;data mining;soft computing;feature selection	Metrics	8.426222878229714	-37.39012698547754	58579
ad0e05208acd040a0865375b719f2235b937f432	the minimum worst case error of fuzzy approximators	fuzzy approximators;fuzzy system theory;approximation algorithms;uncertainty;computational complexity fuzzy set theory uncertainty handling knowledge based systems;uncertainty handling;fuzzy system theory minimum worst case error fuzzy approximators approximation capability fuzzy systems input output maps information based complexity ibc fuzzy rule bases;fuzzy set theory;approximation capability;input output;computer aided software engineering;fuzzy rule base;shape;ibc;computational complexity;computer aided software engineering fuzzy systems uncertainty shape computational complexity noise shaping councils automation humans approximation algorithms;minimum worst case error;input output maps;councils;noise shaping;information based complexity;humans;fuzzy rule bases;fuzzy systems;knowledge based systems;fuzzy system;automation	The approximation capability of fuzzy systems is an important topic of research when the systems are regarded as input-output maps. By using the notion of information-based complexity (IBC), we derive the minimum worst case error of a fuzzy approximator, which is independent of the detailed construction of the fuzzy rule bases.	best, worst and average case	Chung-Ping Kwong	2001	IEEE Trans. Systems, Man, and Cybernetics, Part A	10.1109/3468.983427	input/output;mathematical optimization;noise shaping;uncertainty;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;shape;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;automation;machine learning;control theory;mathematics;fuzzy set;computational complexity theory;computer-aided software engineering;fuzzy set operations;fuzzy control system	Embedded	4.236606863777282	-27.162917269685572	58723
007888147059d950272b1fc1ca4583289168f8e9	a new type of fuzzy logic system for adaptive modeling and control	unsupervised learning;fuzzy controller;fuzzy set;partition of unity;theoretical analysis;fuzzy logic system	Abstract We present a new type of fuzzy controller constructed with the B spline model and its applications in modelling and control Un like the other normalised parameterised set functions for de ning fuzzy sets B spline basis functions do not necessarily span from membership value to but possess the property partition of unity These B spline basis functions are automatically determined after the input space is partitioned By using product as fuzzy conjunction centroid as defuzzi cation fuzzy singletons for modelling output variables and adding marginal linguistic terms fuzzy controllers can be constructed which have advantages like smoothness automatic design and intuitive interpretation of controller parameters Furthermore both theoretical analysis and experimental results show the rapid convergence for tasks of data approximation and unsupervised learning with this type of fuzzy controller	approximation;b-spline;basis function;fuzzy logic;fuzzy set;marginal model;unsupervised learning	Jianwei Zhang;Alois Knoll;K. V. Le	1997		10.1007/3-540-62868-1_129	fuzzy logic;partition of unity;unsupervised learning;combs method;fuzzy electronics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;control theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	1.3382494756600083	-25.113161345719387	58893
1cd18dc95747fffbcfb64227074378ea0fe14610	integrating classification capability and reliability in associative classification: a beta-stronger model	rule based;β stronger relationship;swinburne;classification capability;decision support system;association rule;associative classification;rule discovery;pruning theorem;classification reliability	Mining class association rules is an important task for associative classification and plays a key role in rule-based decision support systems. Most of the existing methods try the best to mine rules with high reliability but ignore their capability for classifying potential objects. This paper defines a concept of -stronger relationship, and proposes a new method that integrates classification capability and classification reliability in rule discovery. The method takes advantage of rough classification method to generate frequent items and rules, and calculate their support and confidence degrees. We propose two new theorems to prune redundant frequent items and a concept of indiscernibility relationship between rules to prune redundant rules. The pruning theorems afford the associative classifier with good classification capability. The experiment shows that the proposed method generates a smaller frequent item set and significantly enhances the classification performance.	association rule learning;decision support system;experiment;logic programming;prune and search;rough set	Yuan-Chun Jiang;Yezheng Liu;Xiao Qiao Liu;Shanling Yang	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.11.021	association rule learning;decision support system;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;one-class classification	ML	-2.119319426276871	-27.989795464092502	59048
e34d41c8c3b4faf1e927444e6a1bcd09093c543f	typed linear chain conditional random fields and their application to intrusion detection	intrusion detection;computer network;domain knowledge;machine learning;conditional random field;reference data	Intrusion detection in computer networks faces the problem of a large number of both false alarms and unrecognized attacks. To improve the precision of detection, various machine learning techniques have been proposed. However, one critical issue is that the amount of reference data that contains serious intrusions is very sparse. In this paper we present an inference process with linear chain conditional random fields that aims to solve this problem by using domain knowledge about the alerts of different intrusion sensors represented in an ontology.	conditional random field;intrusion detection system;machine learning;sensor;sparse matrix	Carsten Elfers;Mirko Horstmann;Karsten Sohr;Otthein Herzog	2010		10.1007/978-3-642-15381-5_2	anomaly-based intrusion detection system;intrusion detection system;reference data;computer science;machine learning;pattern recognition;data mining;conditional random field;domain knowledge	Security	6.053451983974176	-36.96688094618072	59196
609151206a1546cf6b1c89d9c3c384676bcd509c	a novel algorithm for dynamic clustering: properties and performance	silicon;uninterruptible power systems;training;data mining;shape;heuristic algorithms;clustering algorithms	In this paper, we present a dynamic clustering algorithm that efficiently deals with data streams and achieves several important properties which are not generally found together in the same algorithm. The dynamic clustering algorithm operates online in two different time-scale stages, a fast distance-based stage that generates micro-clusters and a density-based stage that groups the micro-clusters according to their density and generates the final clusters. The algorithm achieves novelty detection and concept drift thanks to a forgetting function that allows micro-clusters and final clusters to appear, drift, merge, split or disappear. This algorithm has been designed to be able to detect complex patterns even in multi-density distributions and making no assumption of cluster convexity. The performance of the dynamic clustering algorithm is assessed theoretically through complexity analysis and empirically through a set of experiments.	analysis of algorithms;autocorrelation;cluster analysis;computer cluster;concept drift;experiment;novelty detection;rejection sampling;streaming algorithm;time series;unsupervised learning	Nathalie A. Barbosa;Louise Travé-Massuyès;Victor H. Grisales	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0099	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;shape;computer science;artificial intelligence;theoretical computer science;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;fsa-red algorithm;cluster analysis;silicon;k-medoids;dbscan;affinity propagation;statistics;hierarchical clustering of networks;clustering high-dimensional data	ML	-1.393586463169295	-37.72862473761018	59243
e5a7cf695f3fa980eaca18531286adf1bdff388e	detecting sim box fraud using neural network	classification;sim box fraud;multi layer perceptron;telecom fraud	One of the most severe threats to revenue and quality of service in telecom providers is fraud. The advent of new technologies has provided fraudsters new techniques to commit fraud. SIM box fraud is one of such fraud that has emerged with the use of VOIP technologies. In this work, a total of nine features found to be useful in identifying SIM box fraud subscriber are derived from the attributes of the Customer Database Record (CDR). Artificial Neural Networks (ANN) has shown promising solutions in classification problems due to their generalization capabilities. Therefore, supervised learning method was applied using Multi layer perceptron (MLP) as a classifier. Dataset obtained from real mobile communication company was used for the experiments. ANN had shown classification accuracy of 98.71 %.	algorithm;artificial neural network;computation;experiment;memory-level parallelism;neural networks;perceptron;quality of service;subscriber identity module;supervised learning	Abdikarim Hussein Elmi;Subariah Ibrahim;Roselina Sallehuddin	2012		10.1007/978-94-007-5860-5_69	data mining;world wide web;computer security	ML	4.649221922814482	-36.232712225205844	59303
c62caca39ae442c8a7191397b0a29afc4cfbba8b	using rough set to induce all kinds of positive region knowledge and it's use in sars data set	rough set		rough set	Honghai Feng;Baoyan Liu;Liyun He;Bingru Yang;Yumei Chen;Zhao Shuo	2006			rough set;mathematics;artificial intelligence;pattern recognition	AI	1.9137465539560454	-25.378259998454045	59350
4e95b1eb6657c5e7f78d5d12352e8fb361658b11	adaptive fuzzy interpolation based on general representative values of polygonal fuzzy sets and the shift and modification techniques		Abstract Adaptive fuzzy interpolative reasoning (AFIR) can overcome the limitation of the conventional fuzzy interpolation techniques because it can find and solve the contradictions of the fuzzy interpolative reasoning (FIR) results in order to ensure that the derived FIR results are consistent. In this paper, we propose a new AFIR method to solve the inconsistencies occurring after the FIR process. The proposed AFIR method is based on the general representative values of polygonal fuzzy sets and the proposed shift and modification techniques. The proposed AFIR method includes a new contradictions solving method to get a higher similarity degree between the AFIR results. In order to demonstrate the higher consistency of the AFIR results obtained by the proposed AFIR method, we apply the proposed AFIR method to deal with the diarrheal disease prediction problem. The experimental results show that the proposed AFIR method outperforms Yang and Shenu0027s AFIR method (2011) and Cheng et al.’s AFIR method (2016) in terms of the degree of similarity between the AFIR results.	fuzzy set;interpolation	Shyi-Ming Chen;Stenly Ibrahim Adam	2017	Inf. Sci.	10.1016/j.ins.2017.06.002	machine learning;artificial intelligence;interpolation;discrete mathematics;mathematics;fuzzy logic;polygon;fuzzy set;mathematical optimization	DB	-0.6089894581831498	-26.029457852112646	59546
3e6a48c2ddf99683556352b297949fe75fca2f58	reinforcement learning/spl i.bar/hierarchical neuro-fuzzy politree model for control of autonomous agents	fuzzy neural nets;fuzzy systems learning artificial intelligence fuzzy neural nets cooperative systems;cooperative systems;intelligent agents learning system reinforcement learning hierarchical autonomous agents hybrid neuro fuzzy politree model automatic learning;learning artificial intelligence;fuzzy systems;autonomous agents fuzzy neural networks learning systems telecommunication computing intelligent agent state space methods telecommunication control partitioning algorithms table lookup explosions	This work presents a new hybrid neuro-fuzzy model for automatic learning of actions taken by agents. The main objective of this new model is to provide an agent with intelligence, making it capable, by interacting with its environment, to acquire and retain knowledge for reasoning (infer an action). This new model, named reinforcement learning hierarchical neuro-fuzzy politree (RL-HNFP), descends from the reinforcement learing hierarchical neuro-fuzzy BSP (RL-HNFB) that uses binary space partitioning. By using hierarchical partitioning methods, together with the reinforcement learning (RL) methodology, a new class of neuro-fuzzy systems (SNF) was obtained, which executes, in addition to automatically learning its structure, the autonomous learning of the actions to be taken by an agent. These characteristics represent an important differential when compared with the existing intelligent agents learning systems. The obtained results demonstrate the potential of this new model, which operates without any prior information, such as number of rules, rules specification, or number of partitions that the input space should have.	agent-based model;algorithm;algorithmic efficiency;autonomous robot;autonomy;binary space partitioning;computation;fuzzy control system;intelligent agent;interaction;interpretation (logic);knowledge base;neuro-fuzzy;rl (complexity);reinforcement learning;skolem normal form	Karla Figueiredo;Marley M. B. R. Vellasco;Marco Aurélio Cavalcanti Pacheco;Flávio Joaquim de Souza	2004	Fourth International Conference on Hybrid Intelligent Systems (HIS'04)	10.1109/ICHIS.2004.80	unsupervised learning;control engineering;robot learning;instance-based learning;error-driven learning;computer science;artificial intelligence;neuro-fuzzy;machine learning;learning classifier system;competitive learning;reinforcement learning;active learning;hyper-heuristic;artificial neural network;intelligent control	AI	6.350221287864876	-27.99622226689338	59617
e1577429dcb833cc5a1589ceb96fe16e5fa306d7	a new visualization tool for data mining techniques		Clustering techniques and classification trees are two of the main techniques used in data mining but, at present, there is still a lack of visualization methods for these tools. Many graphs associated with clustering, also with hierarchical clustering, do not give any information about the values of the centroids’ attributes and the relationships among them. In classification trees, graphical procedures can also be developed to help simplify their interpretation and to obtain a better understanding, but more visualization methods to support this tool are needed. This paper presents a novel visualization technique called sectors on sectors (SonS), and an extended version called multidimensional sectors on sectors (MDSonS), for improving the interpretation of several data mining algorithms. These methods are applied for visualizing the results of: (a) hierarchical clustering, which makes it possible to extract all the existing relationships among centroids’ attributes at any hierarchy level; (b) growing hierarchical self-organizing maps (GHSOM), a variant of the well-known self-organizing maps (SOM), by means of which it is possible to visualize, simultaneously, the data information at each hierarchy level compactly and extract relationships among variables; (c) classification trees, in which the SonS is used for representing the input data information for each class presented in each terminal node of a classification tree providing extra information for a better understanding of the problem. These methods are tested by means of several data sets (real and synthetic). The achieved results show the suitability and usefulness of the proposed approaches.	algorithm;cluster analysis;data mining;decision tree learning;emoticon;graphical user interface;hierarchical clustering;organizing (structure);self-organization;self-organizing map;synthetic intelligence;whole earth 'lectronic link	José María Martínez-Martínez;Pablo Escandell-Montero;Emilio Soria-Olivas;José David Martín-Guerrero;Antonio J. Serrano	2015	Progress in Artificial Intelligence	10.1007/s13748-015-0079-4	computer science;bioinformatics;artificial intelligence;data science;machine learning;data mining	ML	1.09116634500306	-37.72811086787425	59749
30b2cc6c15c3062da2c79f22e9272c0ce15a202d	considering currency in decision trees in the context of big data	330 wirtschaft;ddc 330;big data mining;data quality;decision trees;currency	In the current age of big data, decision trees are one of the most commonly applied data mining methods. However, for reliable results they require up-to-date input data, which is not always given in reality. We present a two-phase approach based on probability theory for considering currency of stored data in decision trees. Our approach is efficient and thus suitable for big data applications. Moreover, it is independent of the particular decision tree classifier. Finally, it is context-specific since the decision tree structure and supplemental data are taken into account. We demonstrate the benefits of the novel approach by applying it to three datasets. The results show a substantial increase in the classification success rate as opposed to not considering currency. Thus, applying our approach prevents wrong classification and consequently wrong decisions.	big data;data mining;decision tree;tree structure;two-phase commit protocol	Diana Hristova	2014			data quality;decision tree learning;computer science;data science;machine learning;incremental decision tree;data mining;database;currency	ML	-0.08274097297315636	-34.098537918657904	59818
ed9f13aa1f1b1e7756d55c31784984e6eedd0f62	generalization of association rules through domain knowledge and generalized knoeledge evaliation			association rule learning	Veronica Oliveira de Carvalho	2007				DB	-0.8650536217936451	-32.694102328066414	59885
363eb60e448a32a51fd519e2c70503bba180c0a8	interval type-2 radial basis function neural network: a modeling framework	pragmatics;uncertainty;radial basis function networks backpropagation fuzzy logic fuzzy set theory granular computing knowledge based systems optimisation;fuzzy logic;radial basis function networks;computational modeling;uncertainty fuzzy systems radial basis function networks pragmatics fuzzy logic computational modeling;fuzzy systems;real world fls implementations interval type 2 radial basis function neural network modeling framework it2 rbf nn functional equivalence type 1 fuzzy logic systems t1 fls interval type 2 equivalent system type 2 fuzzy sets linguistic uncertainty system variables mamdani type interval weights karnik mendel type reduction process structural optimization parametric optimization hybrid approach initial rule base estimation footprint of uncertainty fou adaptive back propagation approach computational efficiency rbf based adaptive learning algorithms rbf based multiobjective optimization techniques granular computing based information capture techniques	In this paper, an interval type-2 radial basis function neural network (IT2-RBF-NN) is proposed as a new modeling framework. We take advantage of the functional equivalence of radial basis function neural networks (RBF-NNs) to a class of type-1 fuzzy logic systems (T1-FLS) to propose a new interval type-2 equivalent system; it is systematically shown that the type equivalence (between RBF and FLS) of the new modeling structure is maintained in the case of the IT2 system. The new IT2-RBF-NN incorporates interval type-2 fuzzy sets within the radial basis function layer of the neural network in order to account for linguistic uncertainty in the system's variables. The antecedent part in each rule in the IT2-RBF-NN is an interval type-2 fuzzy set, and the consequent part is of Mamdani type with interval weights, which are used for the Karnik and Mendel type-reduction process in the output layer of the network. The structural and parametric optimization of the IT2-RBF-NN parameters is carried out by a hybrid approach that is based on estimating the initial rule base and footprint of uncertainty (FOU) directly via a granular computing approach and an adaptive back propagation approach. The effectiveness of the new modeling framework is assessed in two parts. First, the IT2-RBF-NN is tested against a number of popular benchmark datasets, and second, it is demonstrated in a real-world industrial application that has particular challenges that are related to the uncertainty of the raw information. Via simulation results, it is shown that the proposed modeling framework performs well as compared with its T1 equivalent system. In addition, a very good computational efficiency is demonstrated as a result of the systematic and automatic creation of IT2 linguistic information and the FOU. Crucially, the proposed modeling framework opens up a host of opportunities for the academic community that already uses the popular T1-RBF-NN-based structure to try the new IT2-RBF-NN and take advantage of the numerous existing RBF-based adaptive learning algorithms, RBF-based multiobjective optimization techniques, granular computing-based information capture techniques, and real-world FLS implementations, and, in general, take advantage of the computational efficiency of the fusion of IT2-FLS and RBF-NN.	algorithm;approximation;artificial neural network;backpropagation;benchmark (computing);cluster analysis;computation;formal system;free library of springfield township;fuzzy logic;fuzzy set;governance, risk management, and compliance;granular computing;information capture;machine learning;mathematical optimization;multi-objective optimization;nn (newsreader);non-deterministic turing machine;radial (radio);radial basis function;radial basis function network;rule-based system;simulation;software propagation;turing completeness;type system	Adrian Rubio Solis;George Panoutsos	2015	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2014.2315656	fuzzy logic;uncertainty;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;computational model;algorithm;fuzzy control system;pragmatics	ML	5.892570820856444	-26.152685205135636	60235
597dcb96ec5b39549dedde284ac8b5888e5d03ad	identifying next relevant variables for segmentation by using feature selection approaches	filter methods;customer segmentation;clustering;feature selection;business analytics	This paper aims at identifying next relevant variables for segmentation.Different feature selection techniques are discussed and applied.The performance is assessed using four evaluation metrics for clustering.A new approach is proposed and compared with other candidates.A real application with data from the concert industry is reported in detail. Data mining techniques are widely used by researchers and companies in order to solve problems in a myriad of domains. While these techniques are being adopted and used in daily activities, new operational challenges are encountered concerning the steps following this adoption. In this paper, the problem of updating and improving an existing clustering model by adding relevant new variables is studied. A relevant variable is here defined as a feature which is highly correlated with the current structure of the data, since our main goal is to improve the model by adding new information to the current segmentation, but without modifying it significantly. For this purpose, a general framework is proposed, and subsequently applied in a real business context involving an event organizer facing this problem. Based on extensive experiments based on real data, the performance of the proposed approach is compared to existing methods using different evaluation metrics, leading to the conclusion that the proposed technique is performing better for this specific problem.	feature selection	Alex Seret;Sebastián Maldonado;Bart Baesens	2015	Expert Syst. Appl.	10.1016/j.eswa.2015.01.070	computer science;artificial intelligence;data science;machine learning;data mining;business analytics;cluster analysis;feature selection;market segmentation	Vision	0.8953790285612452	-33.18944272599401	60264
8f1baec0b9494497ad32d9f56255eaa3352a7cd4	contrapositive symmetry of distributive fuzzy implications revisited	distributive fuzzy implication;information science;boundary conditions;probability density function;contrapositive symmetry;equations fuzzy logic educational institutions mathematics information science boundary conditions explosions engines;satisfiability;data mining;fuzzy set theory;fuzzy logic;functional equation;fuzzy set theory functional equations fuzzy logic;mathematical model;functional equations;t norm;fuzzy implication distributive fuzzy implication contrapositive symmetry functional equation t norm;fuzzy implication	In this paper, we explore the solution of functional equations I(x,T(y,z)) = T(I(x,y),I(x,z)) and I(x,y) = I(N(y),N(x)) satisfied simultaneously, where T is a strict t-norm, I a fuzzy implication and N a strong negation. Under the assumptions of I continuous except the points (0, 0) and (1, 1), we get the full characterizations of the solutions for this functional equations.	complementarity (physics);stable model semantics;t-norm	Yang Li;Qin Feng	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277159	functional equation;mathematical analysis;discrete mathematics;information science;computer science;artificial intelligence;mathematics	Robotics	-0.8577100975037479	-24.256786094714727	60889
0744ad10a8b0c7f0cd70fe4592f4deaed34ba148	a soft computing model for physicians' decision process	decision models;diagnostic test;physical examination;fuzzy data;soft computing;fuzzy set theory;artificial intelligent;medical expert system;mathematical model;decision process	In this paper the author presents a kind of Soft Computing Technique, mainly an application of fuzzy set theory of Prof. Zadeh [16], on a problem of Medical Experts Systems. The choosen problem is on design of a physician’s decision model which can take crisp as well as fuzzy data as input, unlike the traditional models. The author presents a mathematical model based on fuzzy set theory for physician aided evaluation of a complete representation of information emanating from the initial interview including patient past history, present symptoms, and signs observed upon physical examination and results of clinical and diagnostic tests.	fuzzy logic;fuzzy set;mathematical model;set theory;soft computing	Siddharths Sankar Biswas	2010	CoRR		decision model;computer science;physical examination;artificial intelligence;machine learning;mathematical model;data mining;soft computing;fuzzy set;diagnostic test		2.165176962452125	-26.897428432655524	60904
01c4f1ffd4c4cc1ee5be25d75d310053c026294b	self-adjusting a genetic algorithm using fuzzy logic techniques.				Mario César López Locés;H. J. Fraire HéctorJ.Fraire;Rodolfo A. Pazos Rangel;Juan Javier González Barbosa;David Terán Villanueva	2017	IJCOPI			Logic	4.02901268294027	-25.102918557006635	61006
c46766c49ac35f656e0fc192522ea24326c77eec	a combinative method for decision tree construction	construction process;decision tree;decision trees classification tree analysis computer science statistics pattern recognition decision theory signal processing machine learning artificial neural networks np complete problem;data mining;contingency table aggregation combinative method decision tree construction splitting attribute olap;contingency table;data mining decision trees;decision trees	The selection of the splitting attribute in decision tree construction process is the key point for the size and quality of the tree. Although several criteria have been proposed and there are good papers that compare their results, no consensus have been adopted regarding the best method. In this paper we present a new approach in which each candidate attribute is evaluated using a set of available criteria and the attribute voted the best by most of the criteria will be selected as the winning splitting attribute. Each criterion will be evaluated based on the contingency table created at each splitting node. An approach based on OLAP is used for faster contingency table aggregation.	contingency table;decision tree;olap cube;online analytical processing;performance	Daniel Pop;Ciprian Jichici;Viorel Negru	2005	Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)	10.1109/SYNASC.2005.1	decision table;decision tree learning;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;id3 algorithm;grafting;decision stump	DB	4.583426356246535	-32.321226938452405	61083
969396ab23a6c9368f461368544f47de786c0cfa	one-shot procedure learning from instruction and observation	procedural learning;learning system;natural language understanding;machine learning;decision making process;natural language;collaborative problem solving	Learning tasks from a single demonstration presents a significant challenge because the observed sequence is inherently an incomplete representation of the procedure that is specific to the current situation. Observation-based machine-learning techniques are not effective without multiple examples. However, when a demonstration is accompanied by natural language explanation, the language provides a rich source of information about the relationships between the steps in the procedure and the decision-making processes that led to them. In this paper, we present a oneshot task learning system built on TRIPS, a dialogue-based collaborative problem solving system, and show how natural language understanding can be used for effective one-shot	computable function;executable;information source;machine learning;mathematical optimization;natural language understanding;problem solving;recursion	Hyuckchul Jung;James F. Allen;Nathanael Chambers;Lucian Galescu;Mary D. Swift;William Taysom	2006			natural language processing;language identification;robot learning;multi-task learning;instance-based learning;decision-making;collaborative learning;error-driven learning;algorithmic learning theory;sequence learning;double loop learning;computer science;artificial intelligence;machine learning;linguistics;active learning;natural language;stability;active learning;procedural memory	AI	6.2621595055057515	-30.822407898836374	61278
a2c528eb817cf45b7af2570dca5743d969c6c477	fuzzy-rough set based semi-supervised learning	unlabeled data;unsupervised learning;mammographic analysis;fuzzy set;approximate algorithm;supervised learning;approximation algorithms;approximation method;rough set theory;data collection;risk management;prediction algorithms;real world mammographic risk assessment problem fuzzy rough set unsupervised tasks data objects fuzzy rough based semisupervised self learning unlabelled data artificial data;approximation methods approximation algorithms prediction algorithms rough sets supervised learning training data labeling;gene expression data;indexing terms;semi supervised learning;fuzzy set theory;fuzzy sets;text classification;training data;semi supervised learning rough sets fuzzy sets mammographic analysis;data analysis;real world application;risk assessment;dk atira pure researchoutput researchoutputtypes contributiontobookanthology conference;rough sets;approximation methods;experimental evaluation;mammography;unsupervised learning data analysis fuzzy set theory mammography risk management rough set theory;rough set;labeling	Much work has been carried out in the area of fuzzy-rough sets for supervised learning. However, very little has been accomplished for the unsupervised or semi-supervised tasks. For many real-word applications, it is often expensive, time-consuming and difficult to obtain labels for all data objects. This often results in large quantities of data which may only have very few labelled data objects. This paper proposes a novel fuzzy-rough based semi-supervised self-learning or self-training approach for the assignment of labels to unlabelled data. Unlike other semi-supervised approaches, the proposed technique requires no subjective thresholding or domain information. An experimental evaluation is performed on artificial data and also applied to a real-world mammographic risk assessment problem with encouraging results.	algorithm;approximation;fuzzy set;horner's method;iteration;logical connective;risk assessment;rough set;semi-supervised learning;semiconductor industry;supervised learning;thresholding (image processing);unsupervised learning	Neil MacParthalain;Richard Jensen	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007483	unsupervised learning;rough set;risk management;computer science;machine learning;pattern recognition;data mining;mathematics;fuzzy set;supervised learning	Robotics	2.822077983149763	-30.50358923972018	61623
3696da952ee2f16793094455fdff394023b80e5c	comparing clusterings by the variation of information	comparing partitions;medida informacion;analyse amas;metric space;espace metrique;espacio metrico;mesure information;triangle inequality;informacion mutual;metric;intelligence artificielle;classification;information mutuelle;cluster analysis;particion;information measure;clustering;partition;mutual information;artificial intelligence;metrico;analisis cluster;inteligencia artificial;theorie information;point of view;measures of agreement;information theoretic;clasificacion;metrique;information theory;teoria informacion	This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering C to clustering C′. The criterion makes no assumptions about how the clusterings were generated and applies to both soft and hard clusterings. The basic properties of VI are presented and discussed from the point of view of comparing clusterings. In particular, the VI is positive, symmetric and obeys the triangle inequality. Thus, surprisingly enough, it is a true metric on the space of clusterings.	cluster analysis;social inequality;theory;variation of information	Marina Meila	2003		10.1007/978-3-540-45167-9_14	variation of information;combinatorics;information theory;machine learning;mathematics;cluster analysis;statistics	ML	7.0174677697608026	-34.484476322275675	61633
835db2a8fa968487a19556e25f59bebc87eeaa39	learning compound decision functions for sequential data in dialog with experts	decision sequentielle;complex objects;sequential decision;decision secuencial;decision function;rough set theory;fonction decision;prise decision;theorie ensemble approximatif;funcion decision;sequential pattern recognition;rough sets;information system;sequential pattern;rough set;toma decision;systeme information;sistema informacion	In this paper, we investigate the problem of learning the decision functions for sequential data describing complex objects that are composed of subobjects. The decision function maps sequence of attribute values into a relational structure, representing properties of the object described by the sequence. This relational structure is constructed in a way that allows us to answer questions from a given language. The decision function is constructed by means of rule system. The rules are learned incrementally in a dialog with an expert. We also present an algorithm that implements the rule system and we apply it to real life problems.	algorithm;decision theory;heuristic;level of measurement;map;numerical analysis;outline of object recognition;query language;real life;dialog	Wojciech Jaworski	2006		10.1007/11908029_65	rough set;computer science;artificial intelligence;machine learning;decision rule;mathematics;algorithm	AI	5.124594462058703	-31.54941207354516	61643
637b3ef7c9b1443e9941592e20faca506d9ae270	a link between k-nearest neighbor rules and knowledge based systems by sequence analysis	knowledge based system;k nearest neighbor;sequence analysis	Abstract   We propose, in this paper, non-parametric decision rules used in a statistical pattern recognition system with incomplete knowledge. These decision rules are based on the examination of membership sequences of  k -nearest neighbors. Beyond these parameters, we introduce the  non-decision . To apply the decision rules, an expert system is used, which is also described in this paper. Lastly, a numerical and simulated example is given.	k-nearest neighbors algorithm;sequence analysis	A. Joussellin;Bernard Dubuisson	1987	Pattern Recognition Letters	10.1016/0167-8655(87)90011-0	computer science;knowledge-based systems;machine learning;sequence analysis;pattern recognition;data mining;k-nearest neighbors algorithm	DB	0.9021883763270971	-29.754439389097215	61919
c7bb8516c2906e7340555e3093b86e1ff0100212	signatureminer: a fast anti-virus signature intelligence tool		This article presents SignatureMiner, a semisupervised security framework for Anti-Virus signatures featuring normalization, customization, clustering and knowledge discovery. SignatureMiner is based on MinHash and regular expressions and can be used both for malware label classification and signature-based analytics.	computer virus	Ignacio Martín;José Alberto Hernández;Sergio de los Santos	2018		10.1109/CNS.2018.8433141	computer science;computer security;knowledge extraction;personalization;cluster analysis;minhash;malware;data mining;normalization (statistics);regular expression;analytics	Logic	7.961205830492428	-36.60387951412554	61953
2292fa4698e2a0b0be0027248babb45e87de6d1a	dogma: a ga-based relational learner	representacion conocimientos;learning algorithm;apprentissage conceptuel;algorithme apprentissage;logical programming;algoritmo genetico;apprentissage relationnel;aprendizaje conceptual;inductive reasoning;programmation logique;algorithme genetique;concept learning;genetic algorithm;knowledge representation;representation connaissances;algoritmo aprendizaje;programacion logica;information gain;relational learning;fitness function;raisonnement inductif	We describe a GA-based concept learning/theory revision system DOGMA and discuss how it can be applied to relational learning. The search for better theories in DOGMA is guided by a novel tness function that combines the minimal description length and information gain measures. To show the e cacy of the system we compare it to other learners in three relational domains.	concept learning;dogma;information gain in decision trees;kullback–leibler divergence;software release life cycle;theory	Jukka Hekanaho	1998		10.1007/BFb0027324	knowledge representation and reasoning;genetic algorithm;concept learning;statistical relational learning;computer science;artificial intelligence;inductive reasoning;machine learning;mathematics;fitness function;algorithm	AI	8.180365557238899	-31.765093192510957	62167
46f8114443c1f82eeae28491067b4586f4130add	generalization of rough membership function based on \alpha -coverings of the universe	relation equivalence;theorie ensemble flou;generic model;probabilidad condicional;rough set theory;fuzzy relation;probabilite conditionnelle;fuzzy set theory;equivalence relation;real world application;particion;fonction appartenance;partition;membership function;funcion pertenencia;relation floue;rough set;relacion equivalencia;conditional probability;relacion difusa	In 1982, Pawlak proposed the concept of rough sets with practical purpose of representing indiscernibility of elements. Even it is easy to analyze, the rough set theory built on a partition induced by equivalence relation may not provide a realistic view of relationships between elements in the real-world application. Here, coverings of, or non-equivalence relations on, the universe can be considered to represent a more realistic model instead of partition in which a generalized model of rough sets was proposed. In this paper, based on a-coverings of the universe, a generalized concept of rough membership functions is proposed and defined into three values, minimum, maximum and average. Their properties are examined.	rough set	Rolly Intan;Masao Mukaidono	2002		10.1007/3-540-45631-7_18	combinatorics;discrete mathematics;rough set;computer science;artificial intelligence;machine learning;mathematics;dominance-based rough set approach	Crypto	-1.7612235546840243	-24.70053297450758	62317
2e646803dd06bc43c891c63407676a7a4c38af84	visualizing clusters in artificial neural networks using morse theory	diabetes;topological data analysis;clustering;morse theory;artificial neural network	This paper develops a process whereby a high-dimensional clustering problem is solved using a neural network and a lowdimensional cluster diagram of the results is produced using the Mapper method from topological data analysis. The lowdimensional cluster diagram makes the neural network's solution to the high-dimensional clustering problem easy to visualize, interpret, and understand. As a case study, a clustering problem froma diabetes study is solved using a neural network. The clusters in this neural network are visualized using the Mapper method during several stages of the iterative process used to construct the neural network. The neural network and Mapper clustering diagram results for the diabetes study are validated by comparison to principal component analysis.	artificial neural network	Paul T. Pearson	2013	Adv. Artificial Neural Systems	10.1155/2013/486363	neural gas;stochastic neural network;probabilistic neural network;computer science;artificial intelligence;machine learning;data mining;mathematics;cluster analysis;artificial neural network;morse theory	NLP	5.670609063087336	-34.461738117788066	62414
fed27de449c8c219170d388e599b064ed8b71dea	intersection search for a fuzzy petri net-based knowledge representation scheme	linguistic variable;search algorithm;intersection search;fuzzy petri net;inference procedure;knowledge representation;petri net;dynamic properties	This paper describes the intersection search as an inference procedure for a knowledge representation scheme based on the theory of Fuzzy Petri Nets. The procedure uses the dynamical properties of the scheme. The relationships between the concepts of interest, obtained by the intersection search algorithm, are accompanied by the value of the linguistic variable expressing the assurance for the relations. An illustrative example of the intersection search procedure is provided.	dynamical system;knowledge base;knowledge representation and reasoning;petri net;search algorithm;time complexity	Slobodan Ribaric;Nikola Pavesic;Valentina Zadrija	2009		10.1007/978-3-642-04595-0_1	combinatorics;discrete mathematics;machine learning;mathematics;petri net	AI	1.1897113261465395	-24.50140630400874	62859
35ad211f413f460a0c999c7a62e77722b57ce075	classifier-ensemble incremental-learning procedure for nuclear transient identification at different operational conditions	fuzzy c mean;classifier ensemble;difference operator;operant conditioning;ensemble of classifiers;bagging;boiling water reactor;classification;ensemble;incremental learning;fuzzy c means fcm clustering;nuclear power plant;majority voting;bwr nuclear power plant;transient identification	An important requirement for the practical implementation of empirical diagnostic systems is the capability of classifying transients in all plant operational conditions. The present paper proposes an approach based on an ensemble of classifiers for incrementally learning transients under different operational conditions. New classifiers are added to the ensemble where transients occurring in new operational conditions are not satisfactorily classified. The construction of the ensemble is made by bagging; the base classifier is a supervised Fuzzy C Means (FCM) classifier whose outcomes are combined by majority-voting. The incremental learning procedure is applied to the identification of simulated transients in the feedwater system of a Boiling Water Reactor (BWR) under different reactor power levels.	fuzzy cognitive map;reactor (software);transient (computer programming);water cooling	Piero Baraldi;Roozbeh Razavi-Far;Enrico Zio	2011	Rel. Eng. & Sys. Safety	10.1016/j.ress.2010.11.005	ensembl;majority rule;bootstrap aggregating;cascading classifiers;biological classification;computer science;engineering;machine learning;operant conditioning;pattern recognition;data mining;ensemble learning	AI	7.891205291449773	-27.4339300490181	62975
c0141c34064c894ca0a9fa476bad80ef1ab25257	performance assessment and uncertainty quantification of predictive models for smart manufacturing systems	manufacturing systems;smart manufacturing systems;uncertainty handling assembling big data learning artificial intelligence manufacturing data processing product quality statistical analysis;uncertainty quantification;supervised learning;support vector machines;uncertainty;biological system modeling;assembly line uncertainty quantification predictive models smart manufacturing systems statistical learning theory slt performance assessment big data analytics product quality forecasting;big data;statistical learning theory;statistical learning theory performance assessment uncertainty quantification smart manufacturing systems big data supervised learning;predictive models support vector machines uncertainty big data manufacturing systems data models biological system modeling;predictive models;performance assessment;data models	We review in this paper several methods from Statistical Learning Theory (SLT) for the performance assessment and uncertainty quantification of predictive models. Computational issues are addressed so to allow the scaling to large datasets and the application of SLT to Big Data analytics. The effectiveness of the application of SLT to manufacturing systems is exemplified by targeting the derivation of a predictive model for quality forecasting of products on an assembly line.	best, worst and average case;big data;binary classification;computation;image scaling;multiclass classification;predictive modelling;requirement;semi-supervised learning;semiconductor industry;statistical learning theory;uncertainty quantification;worst-case complexity	Luca Oneto;Ilenia Orlandi;Davide Anguita	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363904	engineering;data science;machine learning;data mining	Robotics	8.604352875473593	-25.883171853759364	63097
cb716b9f0d21bb36d02d9577c9c7b9cba742712b	a general framework for adaptive anomaly detection with evolving connectionist systems	indexing terms;a priori knowledge;concept drift;anomaly detection;false alarm rate;adaptive resonance theory;behavior change	A new adaptive anomaly detection framework, based on the use of unsupervised evolving connectionist systems, is proposed to address the issue of concept drift. It is designed to adapt to normal behavior changes while still recognizing anomalies. The evolving connectionist systems learn a subject’s behavior in an online, adaptive fashion without a priori knowledge of the underlying data distributions. Experiments with the KDD Cup 1999 network data and the Windows NT user profiling data show that our adaptive anomaly detection systems, based on Fuzzy Adaptive Resonance Theory (ART) and Evolving Fuzzy Neural Networks (EFuNN), can significantly reduce the false alarm rate while the attack detection rate remains high.	adaptive resonance theory;anomaly detection;concept drift;connectionism;data mining;evolving classification function;windows nt	Yihua Liao;V. Rao Vemuri;Alejandro Pasos	2004		10.1137/1.9781611972740.42	computer science;anomaly detection;machine learning;artificial intelligence;pattern recognition;adaptive resonance theory;concept drift;connectionism;constant false alarm rate	ML	6.988109609363957	-36.33975500793629	63107
af836866d9989b3e7a70f1c022936664e75f6ba1	advanced data mining and applications : 4th international conference, adma 2008, chengdu, china, october 8-10, 2008, proceedings	data mining	Keynotes.- An Introduction to Transfer Learning.- Autonomy-Oriented Computing (AOC), Self-organized Computability, and Complex Data Mining.- Regular Papers.- Improving Angle Based Mappings.- Mining Natural Language Programming Directives with Class-Oriented Bayesian Networks.- Boosting over Groups and Its Application to Acronym-Expansion Extraction.- A Genetic-Based Feature Construction Method for Data Summarisation.- Suicidal Risk Evaluation Using a Similarity-Based Classifier.- Gene Selection for Cancer Classification Using DCA.- FARS: A Multi-relational Feature and Relation Selection Approach for Efficient Classification.- Enhancing Text Categorization Using Sentence Semantics.- Mining Evolving Web Sessions and Clustering Dynamic Web Documents for Similarity-Aware Web Content Management.- Data Quality in Privacy Preservation for Associative Classification.- Timeline Analysis of Web News Events.- Analysis of Alarm Sequences in a Chemical Plant.- Speed Up SVM Algorithm for Massive Classification Tasks.- Mining Supplemental Frequent Patterns.- A Distributed Privacy-Preserving Association Rules Mining Scheme Using Frequent-Pattern Tree.- Dichotomy Method toward Interactive Testing-Based Fault Localization.- Maintaining the Maximum Normalized Mean and Applications in Data Stream Mining.- Identification of Interface Residues Involved in Protein-Protein Interactions Using Naive Bayes Classifier.- Negative Generator Border for Effective Pattern Maintenance.- CommTracker: A Core-Based Algorithm of Tracking Community Evolution.- Face Recognition Using Clustering Based Optimal Linear Discriminant Analysis.- A Novel Immune Based Approach for Detection of Windows PE Virus.- Using Genetic Algorithms for Parameter Optimization in Building Predictive Data Mining Models.- Using Data Mining Methods to Predict Personally Identifiable Information in Emails.- Iterative Reinforcement Cross-Domain Text Classification.- Extracting Decision Rules from Sigmoid Kernel.- DMGrid: A Data Mining System Based on Grid Computing.- S-SimRank: Combining Content and Link Information to Cluster Papers Effectively and Efficiently.- Open Domain Recommendation: Social Networks and Collaborative Filtering.- An Effective Approach for Identifying Evolving Three-Dimensional Structural Motifs in Protein Folding Data.- Texture Image Retrieval Based on Contourlet Transform and Active Perceptual Similarity Learning.- A Temporal Dominant Relationship Analysis Method.- Leakage-Aware Energy Efficient Scheduling for Fixed-Priority Tasks with Preemption Thresholds.- Short Papers.- Learning and Inferences of the Bayesian Network with Maximum Likelihood Parameters.- TARtool: A Temporal Dataset Generator for Market Basket Analysis.- Dimensionality Reduction for Classification.- Trajectories Mining for Traffic Condition Renewing.- Mining Bug Classifier and Debug Strategy Association Rules for Web-Based Applications.- Test the Overall Significance of p-values by Using Joint Tail Probability of Ordered p-values as Test Statistic.- Mining Interesting Infrequent and Frequent Itemsets Based on MLMS Model.- Text Learning and Hierarchical Feature Selection in Webpage Classification.- The RSO Algorithm for Reducing Number of Set Operations in Association Rule Mining.- Predictive Performance of Clustered Feature-Weighting Case-Based Reasoning.- Selecting the Right Features for Bipartite-Based Text Clustering.- Image Emotional Classification Based on Color Semantic Description.- A Semi-supervised Clustering Algorithm Based on Must-Link Set.- T-rotation: Multiple Publications of Privacy Preserving Data Sequence.- The Integrated Methodology of KPCA and Wavelet Support Vector Machine for Predicting Financial Distress.- Outlier Detection Based on Voronoi Diagram.- AWSum - Data Mining for Insight.- Integrative Neural Network Approach for Protein Interaction Prediction from Heterogeneous Data.- Rules Extraction Based on Data Summarisation Approach Using DARA.- A Rough-Apriori Technique in Mining Linguistic Association Rules.- Mining Causal Knowledge from Diagnostic Knowledge.- Modified Particle Swarm Optimizer with Adaptive Dynamic Weights for Cancer Combinational Chemotherapy.- MPSQAR: Mining Quantitative Association Rules Preserving Semantics.- Using Support Vector Regression for Classification.- Dynamic Growing Self-organizing Neural Network for Clustering.- A Design of Reward Function Based on Knowledge in Multi-agent Learning.- A Learning Method of Detecting Anomalous Pedestrian.- Moment+: Mining Closed Frequent Itemsets over Data Stream.- CDPM: Finding and Evaluating Community Structure in Social Networks.- Using Matrix Model to Find Association Rule Core for Diverse Compound Critiques.- Link-Contexts for Ranking.- DC-Tree: An Algorithm for Skyline Query on Data Streams.- Sequential Pattern Mining for Protein Function Prediction.- Improving Web Search by Categorization, Clustering, and Personalization.- JSNVA: A Java Straight-Line Drawing Framework for Network Visual Analysis.- Recognition of Data Records in Semi-structured Web-Pages Using Ontology and ? 2 Statistical Distribution.- Organizing Structured Deep Web by Clustering Query Interfaces Link Graph.- CBP: A New Efficient Method for Mining Multilevel and Generalized Frequent Itemsets.- Supporting Customer Retention through Real-Time Monitoring of Individual Web Usage.- A Comparative Study of Correlation Measurements for Searching Similar Tags.- Structure of Query Modification Process: Branchings.- Mining Top-n Local Outliers in Constrained Spatial Networks.- Mining Concept-Drifting Data Streams with Multiple Semi-Random Decision Trees.- Automatic Web Tagging and Person Tagging Using Language Models.- Real-Time Person Tracking Based on Data Field.		Changjie Tang	2008		10.1007/978-3-540-88192-6	computer science;machine learning;pattern recognition;data mining;data stream mining	HPC	1.2846099200567098	-35.879210490669955	63249
921489590f573a7350635dd63b2813459859a4b2	evolving a set of dtds according to a dynamic set of xml documents	document structure;extraction information;estructura de documento;red www;definicion tipo documento;analisis datos;information extraction;structure document;xml language;definition type document;heterogeneous environment;reseau web;data mining;classification;data analysis;internet;association rule;fouille donnee;xml document;world wide web;analyse donnee;busca dato;clasificacion;extraccion informacion;langage xml;lenguaje xml;document type definition	In this paper we address the problem of evolving a set of DTDs so to obtain a description as precise as possible of the structures of the documents actually stored in a source of XML documents. This problem is highly relevant in such a dynamic and heterogeneous environment as the Web. The approach we propose relies on the use of a classification mechanism based on document structure and on the use of data mining association rules to find out frequent structural patterns in data.	xml	Elisa Bertino;Giovanna Guerrini;Marco Mesiti;Luigi Tosetto	2002		10.1007/3-540-36128-6_4	xml validation;xml;computer science;data mining;database;world wide web;information extraction	DB	-3.8879195237475925	-33.09478013861871	63283
ef61d23215f2bd23168894e3d6de6d9e873b9a55	decision trees for mining data streams	medium data set;any-time property;vfdtc system;extended vfdt;accurate decision tree model;continuous data;mining data stream;concept drift;data stream;standard decision tree algorithm;decision tree;data streams	In this paper we study the problem of constructing accurate decision tree models from data streams. Data streams are incremental tasks that require incremental, online, and any-time learning algorithms. One of the most successful algorithms for mining data streams is VFDT. We have extended VFDT in three directions: the ability to deal with continuous data; the use of more powerful classification techniques at tree leaves, and the ability to detect and react to concept drift. VFDTc system can incorporate and classify new information online, with a single scan of the data, in time constant per example. The most relevant property of our system is the ability to obtain a performance similar to a standard decision tree algorithm even for medium size datasets. This is relevant due to the any-time property. We also extend VFDTc with the ability to deal with concept drift, by continuously monitoring differences between two class-distribution of the examples: the distribution when a node was built and the distribution in a time window of the most recent examples. We study the sensitivity of VFDTc with respect to drift, noise, the order of examples, and the initial parameters in different problems and demonstrate its utility in large and medium data sets.	decision tree learning	João Gama;Ricardo Fernandes;Ricardo Rocha	2006	Intell. Data Anal.		decision tree learning;computer science;concept drift;machine learning;decision tree;pattern recognition;incremental decision tree;data mining;data stream mining	ML	-1.4233468710719257	-36.47605045644791	63316
11d59a3067f0461f8360cfb4f1186b5f990bb8b8	association rule discovery with fuzzy decreasing support on syndrome differentiation in coronary heart disease	drugs;length decreasing support constraint syndrome differentiation coronary heart disease modern chinese medicine constant support constraint exponential complexity association rule mining framework fuzzy decreasing support confidence;itemsets;constant support constraint;heart;fuzzy logic cardiovascular system diseases drugs;coronary heart disease;association rule mining framework;data collection;association rules;satisfiability;association rule mining;fuzzy decreasing support confidence;modern chinese medicine;fuzzy logic;frequent itemset;association rule;syndrome differentiation;diseases;cardiovascular system;length decreasing support constraint;association rules cardiac disease itemsets data mining transaction databases hospitals cardiology cardiovascular diseases information science medical treatment;exponential complexity;algorithm design and analysis	Association rules represent a promising technique to search syndrome differentiation on modern Chinese medicine. Over the years, a variety of algorithms for finding frequent itemsets in very large transaction databases have been developed. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of problem. Long itemsets with low support can still be interesting but it is unable to find them. This paper presents a new association rules mining framework: fuzzy decreasing support-confidence to find all itemsets that satisfy a length-decreasing support constraint. We extract data about relevant factors of syndrome differentiation from the Coronary Heart Disease data collected from hospital. The experimental results show that the frameworks proposed in this paper can not only verify the existing Syndrome Differentiation, but also can discover Syndrome Differentiation with a combination of multiple factors.	algorithm;association rule learning;database;time complexity	Weiguo Yi;Mingyu Lu;Zhi Liu;Hao Xu	2009	2009 2nd International Conference on Biomedical Engineering and Informatics	10.1109/BMEI.2009.5304789	association rule learning;medicine;computer science;artificial intelligence;data science;machine learning;data mining	DB	-3.9995539342873827	-36.044855695212576	63340
904b5e5cb1bca4fa372e5e8cde7eb7cefcc2f32f	rough set-based sar analysis: an inductive method	decision tree;indirubins;rule based;support vector classifier;structure activity relationship;model building;feature selection;cross validation;rough set;data preprocessing;cyclin dependent kinase;leave one out;neural network	a Department of Organic Chemistry, China Pharmaceutical University, 24 Tongjia Xiang, Nanjing, 210009, PR China b Center for Instrumental Analysis, China Pharmaceutical University, 24 Tongjia Xiang, Nanjing 210009, PR China c Department of Pharmacy, Guangdong Vocational College of Chemical Engineering and Pharmaceutics, 321 North Longdong Road, Guangzhou 510520, PR China d Department of Chemistry, Zhengzhou University, 100 Science Road, Zhengzhou 450052, PR China	algorithm;cross-validation (statistics);feature selection;inductive reasoning;machine learning;nonlinear system;preprocessor;quantitative structure–activity relationship;rough set;test set;weka	Ying Dong;Bingren Xiang;Teng Wang;Hao Liu;Lingbo Qu	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.12.008	structure–activity relationship;model building;rough set;computer science;machine learning;decision tree;pattern recognition;data mining;mathematics;cyclin-dependent kinase;data pre-processing;feature selection;artificial neural network;cross-validation	ML	5.469005263261626	-24.410099383411023	63388
4f4073e670d4f48c14e79ca2be0c76cf1e39a5d9	artificial neural network and rough set for hv bushings condition monitoring	bushings;condition monitoring;multilayer perceptrons;power engineering computing;power system reliability;power transformers;radial basis function networks;rough set theory;hv bushings condition monitoring;artificial neural network;multilayer perceptron;oil filled bushing condition monitoring;radial basis function;rough set models;transformer failures	Most transformer failures are attributed to bushings failures. Hence it is necessary to monitor the condition of bushings. In this paper three methods are developed to monitor the condition of oil filled bushing. Multi-layer perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are developed and combined through majority voting to form a committee. The MLP performs better that the RBF and the RS is terms of classification accuracy. The RBF is the fasted to train. The committee performs better than the individual models. The diversity of models is measured to evaluate their similarity when used in the committee.	artificial neural network;filled julia set;memory-level parallelism;perceptron;quad flat no-leads package;radial (radio);radial basis function;reed–solomon error correction;resultant;rough set;transformer	J. L. Mpanza;Tshilidzi Marwala	2011	2011 15th IEEE International Conference on Intelligent Engineering Systems		rough set;computer science;machine learning;multilayer perceptron;artificial neural network	Robotics	7.703104367920926	-27.027172991481574	64006
8d0f7e0a695fa0f5e5914b29887de10d54c07e9f	fuzzification of discrete attributes from financial data in fuzzy classification trees	fuzzy classification;optimisation;optimization algorithm;fuzzy set;fuzzy classification trees;human expert;financial management;training;discrete values;classification tree analysis decision trees loans and mortgages fuzzy sets regression tree analysis humans financial management marketing management risk management banking;data mining;fuzzy set theory;fuzzy sets;financial data;fuzzy decision trees;classification algorithms;loans and mortgages;discrete attributes fuzzification;classification tree analysis;optimisation decision trees financial management fuzzy set theory;fuzzy decision tree;classification accuracy;decision trees;optimal algorithm;discrete values discrete attributes fuzzification financial data fuzzy classification trees fuzzy decision trees optimization algorithm human expert fuzzy sets;gallium	Fuzzy Decision Trees have been successfully applied to both classification and regression problems by allowing gradual transitions to exist between attribute values. Methodologies for fuzzification in fuzzy trees currently create such gradual transitions for continuous attributes. This is achieved by automatically creating fuzzy regions around tree nodes using an optimization algorithm or by using the knowledge of a human expert to create a series of fuzzy sets which are representative of the attributes domain. A problem occurs when trying to construct a fuzzy tree from real world data which comprises of only discrete or a mixture of discrete and continuous attributes. Discrete attribute values have no proximity to other values in the decision space, as there is no continuum between values. Consequently, within a fuzzy tree they are interpreted as crisp sets and contribute little towards the final outcome. This paper proposes a new approach for the fuzzification of discrete attributes in fuzzy decision trees. The approach ranks discrete values on the basis of their effect on the outcome rate and assigns a possibility of being a specific outcome. Experiments carried out on two real world financial datasets which contain a significant proportion of discrete attributes show improved classification accuracy compared with a crisp interpretation of such attributes within fuzzy trees.	algorithm;decision tree learning;fuzzy classification;fuzzy concept;fuzzy set;mathematical optimization;triune continuum paradigm	Keeley A. Crockett;Zuhair Bandar;James O'Shea	2009	2009 IEEE International Conference on Fuzzy Systems	10.1109/FUZZY.2009.5277400	statistical classification;financial management;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;fuzzy control system	AI	3.031790756825556	-29.58396942282526	64060
3a9066fa96a3a2865197201aae80d117a33d1d09	an axiomatic approach to linear explanations in data classification		In this work, we focus on local explanations for data analytics; in other words: given a datapoint ~x, how important was the i-th feature in determining the outcome for~x? The literature has seen a recent emergence of various analytical answers to this question. We argue for a linear influence measure explanation: given a datapoint ~x, assign a value φi(~x) to every feature i, which roughly corresponds to feature i’s importance in determining the outcome for ~x. We present a family of measures called MIM (monotone influence measures), that are uniquely derived from a set of axioms: desirable properties that any reasonable influence measure should satisfy. Departing from prior work on influence measures, we assume no knowledge — or access — to the underlying classifier labeling the dataset. In other words, our influence measures are based on the dataset alone and do not make any queries to the classifier. We compare MIM to other linear explanation models in the literature and discuss their underlying assumptions, merits, and limitations. ACM Classification	acm computing classification system;emergence;statistical classification;monotone	Jakub Sliwinski;Martin Strobel;Yair Zick	2018			data classification;mathematics;artificial intelligence;axiomatic system;pattern recognition	AI	-2.0307646300192714	-30.462767628248006	64129
06afbb7ed4c7e7d7fa7bfcbe2253a59bafd7fa6d	a fuzzy fusion algorithm to identify the traffic states by the floating car data	fusion algorithm floating car data identification of traffic states k means clustering neural network;traffic engineering computing automobiles fuzzy set theory multilayer perceptrons pattern clustering road traffic sensor fusion;pattern clustering;urban main road fuzzy fusion algorithm urban traffic state identification floating car data fuzzy character parameter threshold mlp neural network reality road network beijing blockage level congestion level slight congestion level free flow level k means clustering threshold value urban expressway;automobiles;roads neural networks clustering algorithms accuracy training educational institutions classification algorithms;road network;fusion algorithm;road traffic;multilayer perceptrons;fuzzy set theory;identification of traffic states;mlp neural network;clustering method;floating car data;traffic engineering computing;sensor fusion;k means clustering;neural network	Floating car data provided the speed of each section every five minutes. The clustering methods could reflect the fuzzy character of traffic states, but the parameter threshold would be man-made subjectivity somehow. The neural network could solve this problem, but the floating car data could not be used directly because of the training time as the data was too massive. Based on the reality road network of Beijing, this paper divided urban traffic states into four levels which were blockage, congestion, slight congestion and free flow. To overcome the problems above, this research used a fusion algorithm which connected k-means clustering with MLP neural network to identify the traffic states. This method had a high reliability and the result was consistent with the actual traffic condition. This paper finds a fusion algorithm to get the threshold value of different level urban roads by the floating car data which just needs a short training time. This paper also gets the conclusion that the dipartite degree is not inadequate between the urban expressway and the urban main road if the traffic state is classified into three types.	algorithm;artificial neural network;cluster analysis;k-means clustering;network congestion;open road tolling;quad flat no-leads package	Liying Wei;Mingjun Li	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6234279	simulation;floating car data;computer science;machine learning;sensor fusion;fuzzy set;computer security;artificial neural network;k-means clustering	Robotics	6.1145369552518165	-25.141462989024852	64389
e4f0b593da510ed85d3116ba313e175a4f780a03	new classifier based on fuzzy level set subgrouping	ingenierie connaissances;logique floue;level set;logica difusa;clasificador;intelligence artificielle;classification;fuzzy logic;classifier;classification system;classificateur;artificial intelligence;inteligencia artificial;clasificacion;knowledge engineering	We present new classification system which is based on fuzzy level sets subgrouping. This new classification system allow us fast classification method with quite accurate results.		Paavo Kukkurainen;Pasi Luukka	2006		10.1007/11893011_49	fuzzy logic;classifier;biological classification;computer science;artificial intelligence;level set;machine learning;knowledge engineering;algorithm	Vision	8.936891423449861	-31.268009259355537	64420
d3b9a134bae5f8d70e951197d6033fb6565c8ee4	adaptive fuzzy interpolation based on ranking values of interval type-2 polygonal fuzzy sets		Abstract In recent years, some adaptive fuzzy interpolative reasoning (AFIR) methods have been proposed to deal with contradictions occurred during fuzzy interpolation processes. However, the degrees of consistency of the AFIR results of the existing AFIR methods are not good enough. Moreover, the existing AFIR methods are based on type-1 fuzzy sets (T1FSs), which cannot deal with AFIR using interval type-2 fuzzy sets (IT2FSs), where IT2FSs are more suitable to represent the fuzziness of information than T1FSs. In this paper, we propose a new AFIR method based on the ranking values of interval type-2 polygonal fuzzy sets (IT2PFSs). We also apply the proposed AFIR method based on IT2PFSs to deal with the diarrheal disease prediction problem. The proposed AFIR method based on the ranking values of IT2PFSs can overcome the shortcomings of Yang and Shenu0027s AFIR method (2011) and Cheng et al.’s AFIR method (2016) because it gets a higher consistency of the AFIR results in terms of the degree of similarity between the AFIR results and it deals with AFIR based on IT2PFSs rather than T1FSs.	fuzzy set;interpolation	Shyi-Ming Chen;Stenly Ibrahim Adam	2018	Inf. Sci.	10.1016/j.ins.2018.01.003	artificial intelligence;machine learning;fuzzy logic;interpolation;mathematics;fuzzy set;polygon;ranking	DB	-0.6033781078333776	-25.900013702209517	64435
5dc4248005b58f553d11fffeb7b0198b106ea06f	probabilistic approaches to rough sets	granular computing;shannon entropy;rule induction;belief function;belief functions;rough set theory;probabilistic approach;decision theoretic;decision theoretic rough set model;rough set approximations;rough set;high order rules;information theoretic	This paper reviews probabilistic approaches to rough sets in granulation, approximation, and rule induction. The Shannon entropy function is used to quantitatively characterize partitions of a universe. Both algebraic and probabilistic rough set approximations are studied. The probabilistic approximations are defined in a decision-theoretic framework. The problem of rule induction, a major application of rough set theory, is studied in probabilistic and information-theoretic terms. Two types of rules are analyzed, the local, low order rules, and the global, high order rules.	approximation;information theory;linear algebra;rough set;rule induction;set theory;shannon (unit);turing completeness	Yiyu Yao	2003	Expert Systems	10.1111/1468-0394.00253	rough set;granular computing;probabilistic relevance model;computer science;machine learning;dominance-based rough set approach	ML	-2.493536846619689	-24.211216612110587	64492
cff4f36ad622094e817e0e0e7661fe218b689589	regression trees for multivalued numerical response variables		In the framework of regression trees, this paper provides a recursive partitioning methodology to deal with a non-standard response variable. Specifically, either multivalued numerical or modal response of the type histogram will be considered. These data are known as symbolic data, which special cases are classical data, imprecise data, conjunctive data as well as fuzzy data. In spite of pre-processing data in order to deal with standard regression tree methodology, this paper provides, as main contribution, a definition of the impurity measure and of the splitting criterion allowing for building the regression tree for multivalued numerical response variable. We analyze and evaluate the performance of our proposal, using simulated data as well as a real-world case studies.	decision tree;numerical analysis	Antonio D'Ambrosio;Massimo Aria;Carmela Iorio;Roberta Siciliano	2017	Expert Syst. Appl.	10.1016/j.eswa.2016.10.021	mathematical optimization;discrete mathematics;mathematics;statistics	Metrics	0.7574641964462956	-26.781344992900298	64502
1f3a87e1fa1fc5f7b95c05d3fef95547907e2e71	learning from multiple related data streams with asynchronous flowing speeds	arbitrary sliding window;relational data;lcie;learning experience;data stream;0801 artificial intelligence and image processing;training;real world data streams asynchronous flowing speed related data streams central processing unit arbitrary sliding window asynchronous data streams learning from complete and fixed examples lcfe framework synthetic data streams;school of engineering and science;learning from complete and fixed examples;respubid21684;accuracy;related data streams;data structures;learning artificial intelligence data handling;data stream mining;accuracy training lifting equipment data structures central processing unit data models predictive models;synthetic data streams;prediction accuracy;lcfe framework;asynchronous data streams;predictive models;lifting equipment;data handling;learning artificial intelligence;uci machine learning repository;learn from complete and incomplete examples;sliding window;central processing unit;data models;asynchronous flowing speed;real world data streams	Related data streams refer to data streams that can be joined together by matching their join attributes. Existing research on learning from related data streams is based on an assumption that all streams arrive at a central processing unit in a synchronous way, such that in an arbitrary sliding window, all tuples of the streams can be perfectly joined together. This assumption, however, does not hold when related data streams are generated or transferred at different speeds, and thus may arrive in the central processing unit in an asynchronous manner. In this paper, we argue that for asynchronous data streams, there exist a small portion of perfectly joined examples (i.e., complete examples) and a large portion of partially joined examples (i.e., incomplete examples). Accordingly, we present a new Learning from Complete and Fixed Examples (LCFE) framework that can fix incomplete examples to boost the learning. Experiments on both synthetic and real-world data streams demonstrate that LCFE is able to achieve a higher prediction accuracy for learning from related data streams than other simple solutions can offer.	central processing unit;existential quantification;experiment;synthetic intelligence	Zhi Qiao;Peng Zhang;Jing He;Jinghua Yan;Li Guo	2010	2010 Ninth International Conference on Machine Learning and Applications	10.1109/ICMLA.2010.47	sliding window protocol;data modeling;relational database;computer science;central processing unit;machine learning;group method of data handling;data mining;database;accuracy and precision;predictive modelling;data stream mining;lifting equipment	ML	-1.87999711267155	-36.21653210823456	64515
203144bf0ed230b7d3ba87718647128b1cb70d93	the adjusted analogy-based software effort estimation based on similarity distances	analogy based estimation;software effort estimation;software project management;effort estimation;linear model;problem solving method;genetic algorithm;cost estimation;similarity measure	Analogy-based estimation is a widely adopted problem solving method that has been evaluated and confirmed in software effort or cost estimation domains. The similarity measures between pairs of projects play a critical role in the analogy-based software effort estimation models. Such a model calculates a distance between the software project being estimated and each of the historical software projects, and then retrieves the most similar project for generating an effort estimate. Although there exist numerous analogy-based software effort estimation models in literature, little theoretical or experimental works have been reported on the method of deriving an effort estimate from the adjustment of the reused effort based on the similarity distance. The present paper investigates the effect on the improvement of estimation accuracy in analogy-based estimations when the genetic algorithm method is adopted to adjust reused effort based on the similarity distances between pairs of projects. The empirical results show that applying a suitable linear model to adjust the analogy-based estimations is a feasible approach to improving the accuracy of software effort estimates. It also demonstrates that the proposed model is comparable with those obtained when using other effort estimation methods.	cost estimation in software engineering;software development effort estimation	Nan-Hsing Chiu;Sun-Jen Huang	2007	Journal of Systems and Software	10.1016/j.jss.2006.06.006	putnam model;genetic algorithm;software sizing;software project management;computer science;software engineering;linear model;analysis effort method;data mining;management;cost estimate;software metric;statistics	SE	3.5395070753777356	-33.310751656035755	64840
22005395fd8a6ac2248bd6d592ce54bd17091aa6	boolean search for content-based retrieval using fuzzy logic	fuzzy logic		boolean algebra;fuzzy logic	Mingchun Liu;Chunru Wan	2002			fuzzy electronics;machine learning;computer science;t-norm fuzzy logics;boolean algebra;fuzzy set operations;artificial intelligence;fuzzy logic;pattern recognition;fuzzy associative matrix;combs method;standard boolean model	AI	2.1877951039909793	-25.235685331329588	64844
e7029242dede484d3e74d005ed3006a6aa3645fa	ranking of fuzzy numbers based on centroid point and spread	mathematics;computing	Centroid and spread are commonly used approaches in ranking fuzzy numbers. Some experts rank fuzzy numbers using centroid or spread alone while others tend to integrate them together. Although a lot of methods for ranking fuzzy numbers that are related to both approaches have been presented, there are still limitations whereby the ranking obtained is inconsistent with human intuition. This paper proposes a novel method for ranking fuzzy numbers that integrates the centroid point and the spread approaches and overcomes the limitations and weaknesses of most existing methods. Proves and justifications with regard to the proposed ranking method are also presented. 5	fuzzy number	Ahmad Syafadhli Abu Bakar;Alexander E. Gegov	2014	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-131082	computing;computer science;fuzzy number;theoretical computer science;machine learning;data mining;mathematics;ranking svm	AI	-1.0714826504752184	-26.72138841909302	64996
e9f91b103539087b2df1edf40d13de455417e270	the discovery of experts' decision rules from qualitative bankruptcy data using genetic algorithms	decision support;neural networks;qualitative data;data mining;discriminant analysis;financial ratios;system;decision rules;inductive learning;risk assessment;genetic algorithm;genetic algorithms;bankruptcy prediction;article;prediction;models;decision rule;problem solving;neural network	Numerous studies on bankruptcy prediction have widely applied data mining techniques to finding out the useful knowledge automatically from financial databases, while few studies have proposed qualitative data mining approaches capable of eliciting and representing experts’ problem-solving knowledge from experts’ qualitative decisions. In an actual risk assessment process, the discovery of bankruptcy prediction knowledge from experts is still regarded as an important task because experts’ predictions depend on their subjectivity. This paper proposes a genetic algorithm-based data mining method for discovering bankruptcy decision rules from experts’ qualitative decisions. The results of the experiment show that the genetic algorithm generates the rules which have the higher accuracy and larger coverage than inductive learning methods and neural networks. They also indicate that considerable agreement is achieved between the GA method and experts’ problemsolving knowledge. This means that the proposed method is a suitable tool for eliciting and representing experts’ decision rules and thus it provides effective decision supports for solving bankruptcy prediction problems. q 2003 Elsevier Ltd. All rights reserved.	artificial neural network;data mining;database;genetic algorithm;problem solving;risk assessment;software release life cycle	Myoung-Jong Kim;Ingoo Han	2003	Expert Syst. Appl.	10.1016/S0957-4174(03)00102-7	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;decision rule;artificial neural network	AI	-0.03337712848358446	-30.763649693975037	65024
c4b2def0b73349aa04ef4ae3f620c83129558e9f	a systematic review of software fault prediction studies	systematic review;expert systems;machine learning;automated fault prediction models;method level metrics;software life cycle;prediction model;public datasets;expert system	0957-4174/$ see front matter 2008 Elsevier Ltd. A doi:10.1016/j.eswa.2008.10.027 * Corresponding author. E-mail addresses: cagatay.catal@bte.mam.gov.tr (C (B. Diri). This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle. 2008 Elsevier Ltd. All rights reserved.	algorithm;machine learning;software release life cycle;systematic review	Cagatay Catal;Banu Diri	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.10.027	systematic review;computer science;artificial intelligence;data science;machine learning;data mining;predictive modelling;expert system;software development process	SE	3.6086144283415114	-33.67102520101917	65172
2ac1bb2918924841e1d13afe90f0a0f3ab9df326	study of definable subsets in covering approximation spaces of rough sets	covering;definable subset rough set covering approximation space;fuzzy set;definable subset;approximation method;uncertainty;computer model;rough set theory;approximation space;statistical method;fuzzy set theory;approximation theory;data analysis;computational modeling;machine learning;approximation operator definable subsets approximation spaces rough set theory intelligent decision system fuzzy set theory data analysis approximation representation probability;rough set theory approximation theory data analysis fuzzy set theory;rough sets;approximation methods;computer science;approximation methods rough sets data analysis computer science uncertainty computational modeling;rough set;uncertain data	Intelligent decision systems often need to deal with vague and uncertain data. Several approaches are commonly used to address this problem, such as statistical methods, machine learning, and fuzzy set. Overlapping with but different from the fuzzy set theory, rough set theory is a relatively new mathematical approach to vague data analysis. A rough set is basically an approximation representation of the given data. The representation is expressed in two subsets defined on the data set: the upper and lower approximations. The main difference between the rough set theory and other approaches is that it does not rely on preliminary information about the data such as membership probabilities of the data items required for fuzzy set. One of the open questions in rough set is to decide if a subset of a covering approximation space is definable. In this paper, we answer this question by investigating the approximation operator and conclude the relation of the inner definable, outer definable, and definable subsets of a covering approximation space under certain conditions.	approximation;fuzzy set;machine learning;rough set;set theory;uncertain data;vagueness	Nianbai Fan;Gongzhu Hu;Hui Liu	2011	2011 IEEE International Conference on Information Reuse & Integration	10.1109/IRI.2011.6009514	computer simulation;mathematical optimization;rough set;definable set;computer science;machine learning;k-approximation of k-hitting set;dominance-based rough set approach	DB	-1.574582054269479	-26.141003708115544	65250
ec56269cc8b877327e01def508cc9d63663baed6	making reliable diagnoses with machine learning: a case study	aide diagnostic;outil logiciel;inference motor;estimacion;informatica biomedical;software tool;biomedical data processing;sistema experto;learning algorithm;reliability estimation;aplicacion medical;coronary heart disease;mecanisme par etape;genie biomedical;stepwise mechanism;unbiased estimator;appareil circulatoire pathologie;informatique biomedicale;hombre;intelligence artificielle;algorithme apprentissage;aparato circulatorio patologia;coronary artery disease;motor inferencia;biomedical engineering;estimation;machine learning;estudio caso;cardiovascular disease;herramienta controlada por logicial;stepwise diagnostic process;human;etude cas;artificial intelligence;mecanismo a etapas;evaluation;ingenieria biomedica;medical application;inteligencia artificial;evaluacion;systeme expert;algoritmo aprendizaje;cardiopatia coronaria;cardiopathie coronaire;moteur inference;medical diagnosis;diagnostic aid;homme;application medicale;ayuda diagnostica;expert system	In the past decades Machine Learning tools have been successfully used in several medical diagnostic problems. While they often significantly outperform expert physicians (in terms of diagnostic accuracy, sensitivity, and specificity), they are mostly not used in practice. One reason for this is that it is difficult to obtain an unbiased estimation of diagnose’s reliability. We propose a general framework for reliability estimation, based on transductive inference. We show that our reliability estimation is closely connected with a general notion of significance tests. We compare our approach with classical stepwise diagnostic process where reliability of diagnose is presented as its post-test probability. The presented approach is evaluated in practice in the problem of clinical diagnosis of coronary artery disease, where significant improvements over existing techniques are achieved.	machine learning;sensitivity and specificity;stepwise regression;transduction (machine learning)	Matjaz Kukar	2001		10.1007/3-540-48229-6_12	estimation;computer science;artificial intelligence;evaluation;medical diagnosis;bias of an estimator;operations research;expert system;algorithm;statistics	ML	8.219150651495552	-30.22132408562702	65322
75afd9ef21b1833901304de7aba84177ea0374b7	simplifying a neuro-fuzzy model	neuro fuzzy modeling;neural networks;input output;inference rule;neuro fuzzy;neuro fuzzy system;membership function;fuzzy systems;fuzzy system;neural network	Neuro-fuzzy modeling allows a fuzzy system to be refined by neural training, thus avoiding lenghty trial-and-error phases in defining both membership functions and inference rules. An approach to obtain simple neuro-fuzzy models is proposed, which reduces the number of rules by means of a systematic procedure that consists in successively removing a rule and updating the remaining rules in such a way that the overall input-output behavior is kept approximately unchanged over the entire training set. A formulation of the proper update is described and a criterion for choosing the rules to be removed is also provided. Initial experimental results show the effectiveness of the proposed method in reducing the complexity of a neuro-fuzzy system by using its input-output data.	fuzzy control system;neuro-fuzzy;test set	Giovanna Castellano;Anna Maria Fanelli	1996	Neural Processing Letters	10.1007/BF00420616	input/output;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system;rule of inference	ML	7.020450215187158	-28.662871319211895	65323
65026b96354696447e25dbb81dc56b1f8e10da23	db-subdue: database approach to graph mining	estensibilidad;extraction information;base relacional dato;analisis datos;information extraction;sql;principio minimo;interrogation base donnee;interrogacion base datos;relational database;data mining;graph mining;data analysis;transaction data;fouille donnee;minimum principle;estructura datos;base donnee relationnelle;minimum description length principle;analyse donnee;structure donnee;extensibilite;scalability;relationship lending;data structure;busca dato;database query;extraccion informacion;structured data;principe minimum	In contrast to mining over transactional data, graph mining is done over structured data represented in the form of a graph. Data having structural relationships lends itself to graph mining. Subdue is one of the early main memory graph mining algorithms that detects the best substructure that compresses a graph using the minimum description length principle. Database approach to graph mining presented in this paper overcomes the problems – performance and scalability – inherent to main memory algorithms. The focus of this paper is the development of graph mining algorithms (specifically Subdue) using SQL and stored procedures in a Relational database environment. We have not only shown how the Subdue class of algorithms can be translated to SQL-based algorithms, but also demonstrated that scalability can be achieved without sacrificing performance.	algorithm;computer data storage;data mining;dynamic data;mdl (programming language);minimum description length;relational database;sql;scalability;stored procedure;structure mining	Sharma Chakravarthy;Ramji Beera;Ramanathan Balachandran	2004		10.1007/978-3-540-24775-3_42	spqr tree;sql;wait-for graph;scalability;data structure;data model;relational database;computer science;theoretical computer science;machine learning;transaction data;data mining;database;graph;data analysis;molecule mining;information extraction;graph database;algorithm;graph rewriting	DB	-3.476790819412505	-33.27608916125851	65427
13c8c1962837b472a2cf7ced388e29115925dac8	enhancements to the bayesian network for raster data (baynerd)		Bayesian Networks are powerful probabilistic method to make inferences based on evidences. However, this technique has been rarely applied to processing Remote Sensing data. The Bayesian Network for Raster Data – BayNeRD, implemented in the R software, was developed to be used for raster data analysis. This paper describes an enhanced version of the BayNeRD algorithm, stating what has been changed in terms of data preprocessing, user interaction, and outputs.	algorithm;algorithmic efficiency;bayesian network;computation;data pre-processing;geographic information system;graphical model;parallel computing;preprocessor;r language;raster data;usability	Alexsandro C. O. Silva;Marcio Pupin Mello;Leila Maria Garcia Fonseca	2014			data mining;computer science;software;data pre-processing;probabilistic method;raster data;bayesian network	AI	-1.0412892127241342	-33.247113778411425	65473
3be13364923c905e3f797f46811d206ebea8fee5	fuzzy knowledge- and data-based models of damage to reeds by grazing of greylag geese	fuzzy set;fuzzy data;fuzzy rules;data collection;fuzzy models of damage to reeds;fuzzy ecological modelling;fuzzy logic;neuro fuzzy;model development;greylag geese;knowledge based models;fuzzy model;ecological modelling;neural network;knowledge base	This paper describes a fuzzy and neuro-fuzzy approach to modelling feeding intensity of Greylag Geese on reed. As a consequence of the presence of some non-measurable or random factors and the heterogeneity of reed and goose behaviour, the relationships between the model variables are often not well known and the data collected have a high degree of uncertainty. A fuzzy approach was selected which can be applied with vague knowledge and data of high uncertainty. Fuzzy logic can be used to handle inexact reasoning in knowledge-based models with fuzzy rules and fuzzy sets to handle uncertainty in data. The neural network technique was applied to develop the fuzzy data-based models. For training, several dataset combinations of three lakes in North Germany were used. The generalisation capability of these models was checked for other lakes. The performance of these models was compared with the results of the fuzzy knowledge-based model developed in the next step. The knowledge base of this model contains the Mamdani-type rules formulated by a domain expert. All models were implemented using the Fuzzy Logic Toolbox of MATLAB ® .		A. Salski;B. Holsten	2009	Ecological Informatics	10.1016/j.ecoinf.2009.04.001	fuzzy logic;knowledge base;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system;data collection	NLP	3.2421489256504654	-26.17609235061871	65482
b663325f83b61410ecd56e9e9008277f3a38311e	multilayer in-place learning networks for modeling functional layers in the laminar cortex	top down method;methode descendante;unsupervised learning;modelizacion;representation;metodo adaptativo;learning algorithm;laminar cortex;high dimensionality;supervised learning;complexite calcul;top down;extraction forme;desarrolo cognitivo;invarianza;abstraction;methode adaptative;intelligence artificielle;multidimensional analysis;apprentissage non supervise;algorithme apprentissage;cognitive development;invariance;modelisation;complejidad computacion;regression;cortex structure;red multinivel;analyse n dimensionnelle;function approximation;extraccion forma;laminar flow;approximation d une fonction;computational complexity;developpement cognitif;metodo descendente;feature extraction;multi task learning;genome equivalents;adaptive method;genome;analisis n dimensional;pattern recognition;flujo laminar;invariante;artificial intelligence;layer 2;inteligencia artificial;reconnaissance forme;multilayer network;genoma;apprentissage supervise;extraction caracteristique;reseau multicouche;reseau neuronal;reconocimiento patron;principe equivalence;principio equivalencia;ecoulement laminaire;aprendizaje supervisado;algoritmo aprendizaje;modeling;aproximacion de funciones;pattern extraction;place learning;equivalence principle;red neuronal;invariant;neural network;intermediate representation	"""Currently, there is a lack of general-purpose in-place learning networks that model feature layers in the cortex. By """"general-purpose"""" we mean a general yet adaptive high-dimensional function approximator. In-place learning is a biological concept rooted in the genomic equivalence principle, meaning that each neuron is fully responsible for its own learning in its environment and there is no need for an external learner. Presented in this paper is the Multilayer In-place Learning Network (MILN) for this ambitious goal. Computationally, in-place learning provides unusually efficient learning algorithms whose simplicity, low computational complexity, and generality are set apart from typical conventional learning algorithms. Based on the neuroscience literature, we model the layer 4 and layer 2/3 as the feature layers in the 6-layer laminar cortex, with layer 4 using unsupervised learning and layer 2/3 using supervised learning. As a necessary requirement for autonomous mental development, MILN generates invariant neurons in different layers, with increasing invariance from earlier to later layers and the total invariance in the last motor layer. Such self-generated invariant representation is enabled mainly by descending (top-down) connections. The self-generated invariant representation is used as intermediate representations for learning later tasks in open-ended development."""	acoustic lobing;artificial intelligence;autonomous robot;backpropagation;binary prefix;cerebral cortex;class;computation;computational complexity theory;downstream (software development);experience;general-purpose modeling;impulse invariance;in-place algorithm;msu lossless video codec;machine learning;maple syrup urine disease;neuron;neurons;neuroscience discipline;nonlinear gameplay;numerical analysis;personality character;real-time locating system;rule (guideline);simulation;software propagation;source code;supervised learning;top-down and bottom-up design;turing completeness;unsupervised learning;web site;anatomical layer;frontal lobe;sensor (device)	Juyang Weng;Tianyu Luwang;Hong Lu;Xiangyang Xue	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2007.12.048	semi-supervised learning;unsupervised learning;multidimensional analysis;data link layer;feature learning;multi-task learning;instance-based learning;algorithmic learning theory;systems modeling;regression;wake-sleep algorithm;function approximation;feature extraction;computer science;artificial intelligence;laminar flow;invariant;machine learning;invariant;top-down and bottom-up design;mathematics;abstraction;equivalence principle;supervised learning;cognitive development;stability;competitive learning;intermediate language;computational complexity theory;computational learning theory;representation;artificial neural network;algorithm;genome	ML	9.465231971484007	-31.030604405970802	65541
0025c934c23f371b8b378aed75a9c3689c2ab15b	optimization of type-2 fuzzy systems based on the level of uncertainty, applied to response integration in modular neural networks with multimodal biometry	membership function optimizatoin type 2 fuzzy system optimisation modular neural networks multimodal biometry evolutionary method fuzzy response integration methods fuzzy inference systems;fuzzy response integration methods;fuzzy neural nets;evolutionary computation;fuzzy reasoning;biometrics access control;uncertainty;input variables;training;fuzzy inference systems;fuzzy integral;fuzzy set theory;type 2 fuzzy system optimisation;artificial neural networks;modular neural networks;fingerprint recognition;membership function;fuzzy inference system;uncertainty optimization artificial neural networks fingerprint recognition face training input variables;face;optimization;multimodal biometry;modular neural network;fuzzy system;membership function optimizatoin;fuzzy set theory biometrics access control evolutionary computation fuzzy neural nets fuzzy reasoning;evolutionary method	In this paper we describe an evolutionary method for the optimization of a modular neural network for multimodal biometry. The proposed evolutionary method produces the best architecture of the modular neural network (number of modules, layers and neurons) and fuzzy inference systems (memberships functions) as fuzzy integration methods. The integration of responses in the modular neural network is performed by using optimal interval type-2 fuzzy inference systems. The optimization of membership functions of the type-2 fuzzy systems is based on the level of uncertainty with application to fuzzy response integration.	artificial neural network;biostatistics;evolutionary algorithm;fuzzy control system;fuzzy logic;mathematical optimization;modular neural network;multimodal interaction;program optimization	Denisse Hidalgo;Patricia Melin;Oscar Castillo;Guillermo Licea Sandoval	2010	The 2010 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2010.5596495	face;uncertainty;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations;fingerprint recognition;artificial neural network;evolutionary computation	Robotics	4.918783058186775	-26.563285457204167	65665
f8e24f702781dee4964becdb016e064c426c14a2	a novel fuzzy neural approach to data reconstruction and failure prediction		Bank failure prediction is of great importance to a banku0027s clients, policy-makers and regulators. Various traditional models have been employed to study bank failures. Unfortunately, their performances are unsatisfactory. In this paper, the pseudo-outer product fuzzy neural network using the compositional rule of inference and singleton fuzzifier (POPFNN-CRI(S))-based bank failure prediction model is proposed. It employs computational bank failure analysis techniques coupled with reconstruction of missing financial data in financial covariates that are available from publicly available financial statements as inputs. The performance of the proposed model is assessed through the classification rate of 3636 US banks observed over a 21-year period. The effects of missing data reconstruction are investigated. Copyright © 2009 John Wiley u0026 Sons, Ltd.	protein structure prediction	Hiok Chai Quek;R. W. Zhou;C. H. Lee	2009	Int. Syst. in Accounting, Finance and Management	10.1002/isaf.299	computer science;artificial intelligence;data science;data mining;operations research;statistics	DB	2.1251616887867777	-34.31916537261399	65866
4838c058b0b865be42be3d4af1cf370faed1fb21	a safety evaluation method of mine pressure based on incomplete labeled data stream classification		Mine pressure monitoring data is essentially a data stream, and the safety label of mine pressure is difficult to obtain. In this case, the safety evaluation of mine pressure can be regarded as the incomplete labeled data stream classification, and classification labels are safety and unsafety. The safety evaluation method of mine pressure used in this paper is a incomplete labeled data stream classification algorithm based on K means clustering, which uses K means clustering to label unlabeled data, and uses support vector machine as the base classifier, and uses Bayesian classifier to filter noise data, and uses the double thresholds determined by Hoeffding Bounds inequality to detect concept drifts. Experimental results show the method can better label unlabeled data, and better detect concept drifts in data stream, and it has better classification accuracy for data stream, and it can be applied to the safety evaluation of mine pressure.	algorithm;cluster analysis;k-means clustering;naive bayes classifier;social inequality;support vector machine	Gang Sun;Jia Zhao;Zhongxin Wang;Hao Wang;Chuanyun Ni;Guobao Cai;Xiaolian Chen	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393348	data stream;labeled data;naive bayes classifier;support vector machine;artificial intelligence;machine learning;cluster analysis;k-means clustering;statistical classification;computer science	ML	0.2563878791902307	-34.358981848835974	65935
8e3449296732e46cba1a2f4354b63f88036fd381	belief rough set classifier	belief function;uncertainty;belief functions;classification;rough sets;rough set;decision rule	In this paper, we propose a new rough set classifier induced from partially uncertain decision system. The proposed classifier aims at simplifying the uncertain decision system and generating more significant belief decision rules for classification process. The uncertainty is reperesented by the belief functions and exists only in the decision attribute and not in condition attribute values.	decision support system;rough set	Salsabil Trabelsi;Zied Elouedi;Pawan Lingras	2009		10.1007/978-3-642-01818-3_37	rough set;computer science;machine learning;pattern recognition;data mining;evidential reasoning approach;dominance-based rough set approach	ML	-0.5668632922925368	-28.42674270496049	65946
14d284232ba21337faec0ade82832b8b491bc672	active diagnosis under persistent noise with unknown noise distribution: a rank-based approach		We consider a problem of active diagnosis, where the goal is to efficiently identify an unknown object by sequentially selecting, and observing, the responses to binary valued queries. We assume that query observations are noisy, and further that the noise is persistent, meaning that repeating a query does not change the response. Previous work in this area either assumed the knowledge of the query noise distribution, or that the noise level is sufficiently low so that the unknown object can be identified with high accuracy. We make no such assumptions, and introduce an algorithm that returns a ranked list of objects, such that the expected rank of the true object is optimized. Furthermore, our algorithm does not require knowledge of the query noise distribution.	algorithm;conditional entropy;experiment;greedy algorithm;ibm notes;jason;mutual information;noise (electronics);persistent data structure;quantum decoherence;stanley (vehicle);synthetic intelligence	Gowtham Bellala;Suresh K. Bhavnani;Clayton Scott	2011			control theory	ML	-0.3994837054841714	-34.960946723846796	65960
47590c6ec792ddc1f571f9f3870cecb4393a77be	a new interval type-2 fuzzy possibilistic c-means clustering algorithm	interval fuzzy sets interval type 2 fuzzy possibilistic c means clustering algorithm fpcm type 2 fuzzy logic techniques;uncertainty;fuzzy sets;fuzzy logic;classification algorithms;mathematical model;clustering algorithms;interval type 2 clustering algorithm fuzzy sets fuzzy c means possibilistic c;possibility theory fuzzy logic fuzzy set theory pattern clustering;clustering algorithms fuzzy sets mathematical model uncertainty fuzzy logic classification algorithms fuzzy systems;fuzzy systems	In this paper we are presenting the extension of the Fuzzy Possibilistic C-Means (FPCM) algorithm using Type-2 Fuzzy Logic techniques, with the goal of improving the performance of this algorithm. We also performed the comparison of this proposed algorithm against the Interval Type-2 Fuzzy C-means (IT2FCM) algorithm to observe if the proposed approach performs better than this algorithm. The proposed extension was realized considering both of the weight exponents (fuzzy and possibilistic) the m and η as interval fuzzy sets.	algorithm;cluster analysis;fuzzy logic;fuzzy set	Elid Rubio;Oscar Castillo;Patricia Melin	2015	2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC)	10.1109/NAFIPS-WConSC.2015.7284205	fuzzy logic;statistical classification;discrete mathematics;uncertainty;membership function;defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematical model;data mining;mathematics;fuzzy set;fuzzy associative matrix;cluster analysis;fuzzy set operations;fuzzy control system	EDA	0.39145937245534174	-26.899035623209237	66098
a2bd523fc9e0f1163b1b68c4d8bfb61457e698f0	all about fuzzy description logics and applications		The aim of this talk is to present a detailed, self-contained and comprehensive account of the state of the art in representing and reasoning with structured fuzzy knowledge. Fuzzy knowledge comes into play whenever one has to deal with concepts for which membership is a matter of degree (e.g., the degree of illness is a function of, among others, the body temperature). Specifically, we address the case of the fuzzy variants of conceptual languages of the OWL 2 family.	description logic;web ontology language	Umberto Straccia	2015		10.1007/978-3-319-21768-0_1	fuzzy logic;t-norm fuzzy logics;membership function;type-2 fuzzy sets and systems;fuzzy classification;artificial intelligence;mathematics;fuzzy set;algorithm	AI	0.4518630880321107	-24.830030700140952	66262
9018fb2ce7e29c2d252df25ff628b2069105ed0e	neuro-fuzzy controller for two-group pattern classification problems	neuro fuzzy		neuro-fuzzy	Mukul Jain;Manu Pratap Singh	2005			machine learning;neuro-fuzzy;control theory;computer science;artificial intelligence;fuzzy classification	Theory	4.434861669696495	-25.891112265306873	66279
d3e11955dd53634467037d0bc4bb43c578985c9f	a neuro-evolutive interval type-2 tsk fuzzy system for volatile weather forecasting	weather forecasting;fuzzy system	This paper presents an hybrid Neuro-Evolutive algorithm for a Firstorder Interval Type-2 TSK Fuzzy Logic System applied to a volatile weather forecasting case. All results are tested by statistical tests asGoldfeld-Quant, Ljung-Box, ARCH, Runs, Turning Points, Bayesian, Akaike and Hannan-Quin criteria. Some methodological aspects about a hybrid implementation among ANFIS, an Evolutive Optimizer and a First order Interval Type-2 TSK FLS are presented. The selected type-reduction algorithm is the IASCO algorithm proposed by Melgarejo in [1] since it presents better computing properties than other algorithms.	fuzzy control system	Dusko Kalenatic;Juan Carlos Figueroa García;César Amilcar López Bello	2010		10.1007/978-3-642-14922-1_19	weather forecasting;computer science;artificial intelligence;machine learning;operations research;fuzzy control system	NLP	4.899219047228231	-25.093444556418568	66285
c350116f669e8d46c184507d747d123361895403	webpage classification with aco-enhanced fuzzy-rough feature selection	dimensionalidad;invertebrata;swarm intelligence;categorisation;fuzzy set;information loss;ant colony optimization;arthropoda;electronically stored information;insecto social;redundancia;rough set theory;logique floue;dimensionality;conjunto difuso;logica difusa;ensemble flou;classification;user assistance;fuzzy logic;dk atira pure researchoutput researchoutputtypes contributiontoconference paper;categorizacion;assistance utilisateur;redundancy;optimizacion enjambre particula;theorie ensemble approximatif;perdida informacion;dimensionnalite;aculeata;insecta;asistencia usuario;optimisation essaim particule;feature selection;hymenoptera;insecte social;social insect;formicoidea;rough set;perte information;clasificacion;redondance;categorization	Due to the explosive growth of electronically stored information, automatic methods must be developed to aid users in maintaining and using this abundance of information effectively. In particular, the sheer volume of redundancy present must be dealt with, leaving only the information-rich data to be processed. This paper presents an approach, based on an integrated use of fuzzy-rough sets and Ant Colony Optimization (ACO), to greatly reduce this data redundancy. The work is applied to the problem of webpage categorization, considerably reducing dimensionality with minimal loss of information.	ant colony optimization algorithms;benchmark (computing);categorization;data redundancy;feature selection;heuristic;mathematical optimization;rough set;web content;web page	Richard Jensen;Qiang Shen	2006		10.1007/11908029_17	rough set;swarm intelligence;computer science;artificial intelligence;machine learning;data mining;feature selection;algorithm	AI	7.040257225126175	-33.260052523995896	66313
8e6ba01300cc54cf9a4843d22146299d3c4a01a4	learning classifier systems for adaptive learning of intrusion detection system		Relational databases contain information that must be protected such as personal information, the problem of intrusion detection of relational database is considered important. Also, the pattern of attacks evolves and it is difficult to grasp by rule-based method or general machine learning, so adaptive learning is needed. Learning classifier systems are system that combines supervised learning, reinforcement learning and evolutionary computation. It creates and updates classifiers according to data input. Learning classifier systems can learn adaptive because they generate and evaluate classifiers in real time. In this paper, we apply accuracy based learning classifier systems to relational database and confirm that adaptive learning is possible. Also, we confirmed their practical usability that they close to the best accuracy, though were not the best.	evolutionary computation;intrusion detection system;learning classifier system;logic programming;machine learning;personally identifiable information;reinforcement learning;relational database;supervised learning;usability	Chang-Seok Lee;Sung-Bae Cho	2017		10.1007/978-3-319-67180-2_54	supervised learning;computer science;machine learning;instance-based learning;anomaly detection;adaptive learning;relational database;unsupervised learning;reinforcement learning;learning classifier system;artificial intelligence;pattern recognition	ML	7.38483675830007	-36.0533724255474	66380
28bc8276232cbe19ee74af59934640546fd89530	fuzzy spatial relations for 2d scene	spatial relation	Different models for computing the spatial relations have been developed in the last decade. Separate methods are used for computing topological, directional and distance relations. These models have also been extended in fuzzy domains based on fuzzy or vague objects and less attention has been paid to fuzzy relations. In this paper 8 fuzzy topological relations are computed with the help of 1D Allen relations and directional relations are evaluated by using fuzzy membership functions. A matrix of fuzzy relations is developed which provides complete set of information about a 2D scene. This method deals with two types of fuzziness (i) fuzziness involved in the objects position (ii) fuzziness about directional relations.	fuzzy set;membership function (mathematics);natural language;numerical analysis;spatial–temporal reasoning;vagueness	Nadeem Salamat;El-hadi Zahzah	2010			fuzzy logic;discrete mathematics;matrix (mathematics);spatial relation;mathematics	AI	-1.6080035176222482	-24.622360359240627	66532
231ecab55c0a11508dd62f998e20d3377afe0db1	a sample classification algorithm based on inclusion degree	rough sets hair color classification algorithms cognition data mining medical diagnostic imaging;classification algorithm;color;rough set theory;data mining;set value vector inclusion degree sample classification algorithm decision tables rough set;classification;set value vector inclusion degree;cognition;classification algorithms;rough sets;sample classification algorithm;rough set;decision tables;decision table;rough set theory classification data mining decision tables;medical diagnostic imaging;hair	The purpose of this paper is to establish sample classification algorithms in consistent and inconsistent decision tables. First, according to the definition of inclusion degree and the idea of positive region in rough set, we give the definition of set value vector inclusion degree; then, according to the maximum inclusion degree principle, the sample classification algorithms are put forward with respect to consistent and inconsistent decision tables respectively; at last, the validity of the classification algorithms are accounted for through examples.	algorithm;decision table;rough set	Wenjun Liu;You Fei	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569215	statistical classification;rough set;computer science;machine learning;pattern recognition;data mining;mathematics	DB	-1.729372520013572	-26.443113069247822	66657
995d58b371a1eb18a8d551c1ff1a55d884cf4670	a novel incremental attribute reduction approach for dynamic incomplete decision systems		Abstract Attribute reduction is an important process in data mining and knowledge discovery. In dynamic data environments, the attribute reduction problem has three issues: variation of object sets, variation of attribute sets and variation of attribute values. For the first two issues, a few achievements have been made. For variation of the attribute values, current attribute reduction approaches are not efficient, because the method becomes a non-incremental or inefficient one in some cases. In order to address this, we first introduce the concept of an inconsistency degree in an incomplete decision system and prove that the attribute reduction based on the inconsistency degree is equivalent to that based on the positive region. Then, three update strategies of inconsistency degree for dynamic incomplete decision systems are provided. Finally, the framework of the incremental attribute reduction algorithm is proposed. Experiments on different data sets from UCI show the accuracy and feasibility of the proposed incremental reduction algorithms.	approximation algorithm;big data;dynamic problem (algorithms);experiment;feature selection;rough set	Xiaojun Xie;Xiaolin Qin	2018	Int. J. Approx. Reasoning	10.1016/j.ijar.2017.12.002	knowledge extraction;mathematics;artificial intelligence;machine learning;data set;dynamic data	SE	-2.717644856800088	-28.67487998456894	66706
602aebcaeb2318fddb9f6eda43f664c7b34f4eac	fundamentals of m-vague algebra and m-vague arithmetic operations	traditional approach;name m-vague algebra;m-vague algebraic property;m-vague algebraic approach;algebraic concept;strong fuzzy function;fuzzy equality;m-vague arithmetic operation;arithmetic operation;fuzzy arithmetic operation	In the present work, the development of a new and general theory of algebraic concepts based on *-fuzzy equalities and strong fuzzy functions is begun, and several fundamental results are established under the name M-vague algebra. As a natural implementation of the M-vague algebraic approach, new kinds of arithmetic operations, namely M-vague arithmetic operations, are introduced using an approach different from the traditional approach to fuzzy arithmetic operations, and various kinds of M-vague algebraic properties of M-vague arithmetic operations are investigated.	vagueness	Mustafa Demirci	2002	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems			DB	-0.834441917156056	-23.979807365107344	66720
8b1bcef0bcee231fd321d2260a6357cbb63453f8	efficient similarity search for time series data based on the minimum distance	search problem;adaptacion;base donnee;distance minimale;tiempo busqueda;database;base dato;stockage donnee;euclidean distance;time series;temps recherche;problema investigacion;data mining;minimal distance;data storage;stock price;indexing;fouille donnee;minimum distance;adaptation;indexation;serie temporelle;indizacion;serie temporal;time series data;almacenamiento datos;probleme recherche;sequence;search time;similarity measure;busca dato;similarity search;secuencia;distancia minima	We address the problem of efficient similarity search based on the minimum distance in large time series databases. Most of previous work is focused on similarity matching and retrieval of time series based on the Euclidean distance. However, as we demonstrate in this paper, the Euclidean distance has limitations as a similarity measurement. It is sensitive to the absolute offsets of time sequences, so two time sequences that have similar shapes but with different vertical positions may be classified as dissimilar. The minimum distance is a more suitable similarity measurement than the Euclidean distance in many applications, where the shape of time series is a major consideration. To support minimum distance queries, most of previous work has the preprocessing step of vertical shifting that normalizes each time sequence by its mean before indexing. In this paper, we propose a novel and fast indexing scheme, called the segmented mean variation indexing(SMV-indexing). Our indexing scheme can match time series of similar shapes without vertical shifting and guarantees no false dismissals. Several experiments are performed on real data(stock price movement) to measure the performance of our indexing scheme. Experiments show that the SMV-indexing is more efficient than the sequential scanning in performance.	autocorrelation;database;euclidean distance;experiment;pitch shift;preprocessor;similarity search;time series	Sangjun Lee;Dongseop Kwon;Sukho Lee	2002		10.1007/3-540-47961-9_27	computer science;machine learning;time series;data mining;mathematics;jaro–winkler distance;algorithm;statistics	DB	-2.9847512171434034	-34.66345706322399	66928
1fb8f39eee9391b837dfc9c658f08e3386da2169	temporal decision tree and interpretable temporal rules: j48 and fuzzy cognitive maps approach		Temporal data classification is relatively young in the area of machine learning and data mining, where the historical data are used to predict the future value. A nonlinear temporal data classification is proposed in this work, namely Temporal-J48. With its tree-based architecture, the implementation is relatively simple. The classification information is readable through the generated temporal rules. The interrelationship among the generated rules also made available to the user through Fuzzy Cognitive Maps (FCM).	cognitive map;decision tree	Shih Yin Ooi;Shing Chiang Tan;Wooi Ping Cheah	2014	Austr. J. Intelligent Information Processing Systems		architecture;temporal database;fuzzy cognitive map;decision tree;nonlinear system;artificial intelligence;machine learning;computer science;c4.5 algorithm	ML	3.001939703235584	-26.53797548416833	67008
7c8c472b4436d68cf5ad04bf30a6e5e40814e0be	maximizing classifier utility when there are data acquisition and modeling costs	tecnologia electronica telecomunicaciones;decision tree;active learning;data mining;utility based data mining;induction;machine learning;cost sensitive learning;tecnologias;grupo a;sampling strategy;decision trees;data acquisition;cost model	Classification is a well-studied problem in data mining. Classification performance was originally gauged almost exclusively using predictive accuracy, but as work in the field progressed, more sophisticated measures of classifier utility that better represented the value of the induced knowledge were introduced. Nonetheless, most work still ignored the cost of acquiring training examples, even though this cost impacts the total utility of the data mining process. In this article we analyze the relationship between the number of acquired training examples and the utility of the data mining process and, given the necessary cost information, we determine the number of training examples that yields the optimum overall performance. We then extend this analysis to include the cost of model induction—measured in terms of the CPU time required to generate the model. While our cost model does not take into account all possible costs, our analysis provides some useful insights and a template for future analyses using more sophisticated cost models. Because our analysis is based on experiments that acquire the full set of training examples, it cannot directly be used to find a classifier with optimal or near-optimal total utility. To address this issue we introduce two progressive sampling strategies that are empirically shown to produce classifiers with near-optimal total utility.	algorithm;analysis of algorithms;category utility;central processing unit;chart;computation;data acquisition;data mining;decision tree;experiment;iteration;machine learning;sampling (signal processing);statistical classification;test set	Gary M. Weiss;Ye Tian	2007	Data Mining and Knowledge Discovery	10.1007/s10618-007-0082-x	computer science;data science;machine learning;decision tree;data mining;statistics	ML	2.829264270026716	-34.92865224941377	67142
ff8d699f1767562a1a480b826883d5683bf3bf5c	granular reducts of formal fuzzy contexts	rough set-theory;concept lattices;attribute reduction;galois connection;operators;object	We introduce the notion of granular reduct in formal fuzzy contexts.We proposed methods of granular reduct in the sense of reducing attributes.We examined the relationship between granular reduct and classification reduct. Knowledge reduction is one of the key issues in knowledge discovery and data mining. During the construction of a concept lattice, it has been recognized that computational complexity is a major obstacle in deriving all the concept from a database. In order to improve the computational efficiency, it is necessary to preprocess the database and reduce its size as much as possible. Focusing on formal fuzzy contexts, we introduce in the paper the notions of granular consistent sets and granular reducts and propose granular reduct methods in the sense of reducing the attributes. With the proposed approaches, the attributes that are not essential to all the object concepts can be removed without loss of knowledge and, consequently, the computational complexity of constructing the concept lattice is reduced. Furthermore, the relationship between the granular reducts and the classification reducts in a formal fuzzy context is investigated.		Ming-Wen Shao;Yee Leung;Xizhao Wang;Wei-Zhi Wu	2016	Knowl.-Based Syst.	10.1016/j.knosys.2016.10.010	data mining;algorithm	DB	-3.010557632383984	-26.155015365301978	67280
f5f610cbbb6ac78c0913a3ccee8c7cac2530107a	neural network influence in group technology: a chronological survey and critical analysis	group technology;cellular manufacturing system;artificial intelligent;self organizing system;artificial neural network;neural network	This article portrays a chronological review of the influence of Artificial Neural Network in group technology applications in the vicinity of Cellular Manufacturing Systems. The research trend is identified and the evolvement is captured through a critical analysis of the literature accessible from the very beginning of its practice in the early 90's till the 2010. Analysis of the diverse ANN approaches, spotted research pattern, comparison of the clustering efficiencies, the solutions obtained and the tools used make this study exclusive in its class.	artificial neural network;cluster analysis	Sourav Sengupta;Tamal Ghosh;Pranab K. Dan;Manojit Chattopadhyay	2010	CoRR		nervous system network models;engineering;artificial intelligence;machine learning;physical neural network;time delay neural network;operations research	SE	7.045990303627517	-24.721438029202588	67413
2fb46d07ddf43dc2978071210ec1825e9c95f901	improved wind power forecasting using a combined neuro-fuzzy and artificial neural network model	modelizacion;forecasting;prevision;fuzzy neural nets;prediction error;analisis estadistico;intervalo confianza;wind power;logique floue;logica difusa;reseau neuronal flou;intelligence artificielle;forecasting model;probabilistic approach;fuzzy logic;modelisation;confidence interval;artificial neural networks;intermitencia;statistical analysis;power system;enfoque probabilista;approche probabiliste;neuro fuzzy;intervalle confiance;analyse statistique;intermittency;artificial intelligence;inteligencia artificial;physical model;reseau neuronal;intermittence;modeling;red neuronal;wind power forecasting;artificial neural network;neural network	The intermittent nature of the wind creates significant uncertainty in the operation of power systems with increased wind power penetration. Considerable efforts have been made for the accurate prediction of the wind power using either statistical or physical models. In this paper, a method based on Artificial Neural Network (ANN) is proposed in order to improve the predictions of an existing neuro-fuzzy wind power forecasting model taking into account the evaluation results from the use of this wind power forecasting tool. Thus, an improved wind power forecasting is achieved and a better estimation of the confidence interval of the proposed model is provided.	algorithm;artificial neural network;autonomous robot;ibm power systems;network model;neuro-fuzzy;scheduling (computing);sysop	Yiannis A. Katsigiannis;Antonis G. Tsikalakis;Pavlos S. Georgilakis;Nikos D. Hatziargyriou	2006		10.1007/11752912_13	fuzzy logic;wind power;systems modeling;confidence interval;forecasting;physical model;computer science;artificial intelligence;neuro-fuzzy;machine learning;mean squared prediction error;electric power system;operations research;intermittent energy source;artificial neural network;statistics	EDA	9.173582286625328	-24.642254403551505	67943
5817fe1a915884c51e96128a11ba33e5c2e50a55	using authorship analysis techniques in forensic analysis of electronic mails	electronic mail forensics data mining internet feature extraction ieee computer society support vector machines;support vector machines computer crime computer forensics data mining decision trees electronic mail neural nets;electronic mail;decision tree;computer forensics;support vector machines;neural nets;information technology;computer crime;data mining;ieee computer society;internet;feature extraction;support vector machine;f measure rate authorship analysis techniques forensic analysis electronic mails information technology communication tool malicious electronic mail cyber crimes artifical neural network ann support vector machines svm decision trees method data mining classfication weka;decision trees;forensics;neural network	As a result of rapid advances in information technology electronic mail has become one of today's most important communication tool. Electronic mail which provides conveniences to its user in many cases, is also an attractive environment for criminals. Malicious electronic mail whose actual owner is uncertain, is taking place in cyber crimes and authorship analysis has become necessary for determining the actual owner of this electronic mail. In this study 43 textual features were extracted from dataset of electronic mails which is obtained for 5 writers. These extracted textual features were processed with Artifical Neural Network (ANN), Support Vector Machines (SVM) and Decission Trees Method that are method of data mining classfication techniques in WEKA. As a result of the application. Decision Trees Method has been observed to be most succesful with F-measure rate of 83% in average for available dataset.	artificial neural network;cybercrime;data mining;decision tree;email;stylometry;support vector machine;weka	Ekin Ekinci;Hidayet Takçi	2012	2012 20th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2012.6204560	support vector machine;computer science;data science;machine learning;decision tree;data mining;world wide web;artificial neural network	ML	4.905311657543073	-37.13708863510575	67993
031012b4daf6938a0c515bfab33bfe3fba657597	a quick attribute reduction algorithm based on knowledge granularity	granular computing;time complexity quick attribute reduction algorithm knowledge granularity incomplete information systems rough set theory attribute frequency;information systems;time complexity;rough set theory;knowledge granularity;attribute reduction rough set incomplete information systems knowledge granularity discernibility matrix;rough set theory computational complexity granular computing information systems;attribute reduction;incomplete information;computational complexity;incomplete information systems;information systems complexity theory algorithm design and analysis educational institutions computers frequency shift keying machine learning algorithms;rough set;discernibility matrix	The research of knowledge granularity has been a hotspot at home and abroad. In incomplete information systems of the rough set, we give a formula, which calculates the attribute frequency directly without acquiring the discernibility matrix. Then applying it to the field of knowledge granularity, we give a quick calculation of the attribute reduction algorithm, which of the time complexity is O(|C| 2  |U |) in the worst case. The example result shows that the algorithm is correct and efficient.	algorithm	Zhangyan Xu;Wei Zhang;Xiaoyu Wang;Xiaoyu Li	2012		10.1109/FSKD.2012.6233802	rough set;attribute domain;granular computing;computer science;theoretical computer science;machine learning;data mining;mathematics	EDA	-3.9614303567475684	-36.07074235253043	68086
56594045b8a1852125b168cb1bac88e89043b8f7	a comparison of classification strategies in rule-based classifiers			logic programming;rule 90	Szymon Wojciechowski	2018	Logic Journal of the IGPL	10.1093/jigpal/jzx053	rule-based system;random subspace method;discrete mathematics;mathematics;probabilistic classification;pattern recognition;artificial intelligence	Vision	8.953944099685389	-36.740768315261036	68139
3c90d67db35bbdd9998ee824be451285f8a96178	pattern recognition techniques in handoff and service area determination	pattern recognition techniques;channel allocation pattern recognition techniques service area determination pattern recognition algorithms handoff field strength intensities base station isodata clustering algorithm fuzzy classifiers membership functions performance microcellular environment;isodata clustering algorithm;cluster algorithm;fuzzy classifiers;base stations;uncertainty;cellular radio;working environment noise;lakes;performance;service area determination;interference;fuzzy set theory;handoff;land mobile radio;base station;frequency allocation;data structures;membership functions;space stations;membership function;pattern classification;pattern recognition;clustering algorithms;frequency allocation cellular radio land mobile radio radio spectrum management pattern recognition fuzzy set theory pattern classification;radio spectrum management;microcellular environment;pattern recognition algorithms;channel allocation;field strength intensities;pattern recognition base stations clustering algorithms space stations working environment noise lakes interference uncertainty channel allocation data structures;fuzzy classifier	Pattern recognition algorithms are used to process the field strength intensities in order to select the base station which will serve the call. The algorithms are: the Isodata clustering algorithm and fuzzy classifiers based on membership functions. The performance and application of these algorithms, in a microcellular environment, are evaluated and discussed.	algorithm;cluster analysis;data recovery;pattern recognition	Héctor Maturino-Lozoya;David Muñoz Rodríguez;Hazem Tawfik	1994		10.1109/VETEC.1994.345157	data structure;telecommunications;computer science;base station;operating system;machine learning;data mining	Vision	3.582558223895781	-37.3186079134123	68536
df1748e9304a42a2d86c1ef32bbadbd7664872b3	dimension reduction in intrusion detection using manifold learning	intrusion detection robustness support vector machines data mining feature extraction computational intelligence information security manifolds support vector machine classification text categorization;support vector machines graph theory learning artificial intelligence security of data;graph theory;kdd dataset;kv lle;locally linear embedding;connected neighborhood graph;unm;manifolds;support vector machines;dimension reduction;kv isomap;intrusion detection dimension reduction manifold learning kv lle kv isomap;intrusion detection;manifold learning;data mining;k variable method;kv isomap algorithm;text classification intrusion detection manifold learning nonlinear dimension reduction locally linear embedding k variable method kv lle algorithm kv isomap algorithm connected neighborhood graph support vector machine kdd dataset unm dataset;text classification;kv lle algorithm;false positive rate;feature extraction;nonlinear dimension reduction;classification algorithms;detection rate;robustness;support vector machine;learning artificial intelligence;security of data;local linear embedding;unm dataset;singular point	Manifold learning is an emerging and promising approach in nonlinear dimension reduction. Representative methods include locally linear embedding (LLE) and Isomap. However, both methods fail to guarantee connectedness of the constructed neighborhood graphs. We propose k variable method called kv-LLE and kv-Isomap to build connected neighborhood graphs so as to enhance the robustness. The applicability of above two modified dimension reduction methods is examined by combining with the classifier of one class SVM. We evaluate new schemes with the KDD dataset and UNM dataset. Experiment results demonstrate higher detection rate and significant lower false positive rate on kv-Isomap method. The kv-LLE method is more suitable for kinds of text classification tasks. And the algorithm kv-Isomap performs much better than kv-LLE. We also have an unexpected gain in marking singular points.	algorithm;arc diagram;data mining;document classification;intrusion detection system;isomap;item unique identification;nonlinear dimensionality reduction;nonlinear system;support vector machine	Kai-mei Zheng;Xu Qian;Pei-chong Wang	2009	2009 International Conference on Computational Intelligence and Security	10.1109/CIS.2009.116	support vector machine;computer science;graph theory;machine learning;pattern recognition;data mining;mathematics	ML	5.633789998706698	-37.502169974258564	68800
fd4a7746abc6ca4fb8f7ee2887314cebb789ad21	introducing class-based classification priority in fuzzy rule-based classification systems	benign tumors;cancer;synthetic two dimensional problem;fuzzy rule based classification systems;fuzzy rules;fuzzy control;malignant tumors;fuzzy systems pattern classification malignant tumors costs cancer fuzzy sets medical diagnosis benign tumors fuzzy control knowledge based systems;fuzzy set theory;automatic generation;fuzzy sets;synthetic two dimensional problem class based classification priority fuzzy rule based classification systems fuzzy if then rules;fuzzy rule base;general methods;computer experiment;pattern classification fuzzy set theory;classification system;pattern classification;fuzzy if then rules;class based classification priority;fuzzy systems;knowledge based systems;medical diagnosis	In this paper we propose a fuzzy rule-generation method for pattern classification problems with classification priority. The assumption in this paper is that a classification priority is given a priori in relation to other classes. Our fuzzy rule-based classification system consists of a set of fuzzy if-then rules that are automatically generated from a set of given training patterns. The proposed method decides the consequent class of fuzzy if-then rules based on the number of covered training patterns for each class. In computational experiments we first show the effect of introducing classification priority for a synthetic two-dimensional problem. Then we show the effectiveness of the proposed method for several real-world pattern classification problems.	computation;experiment;fuzzy rule;logic programming;synthetic intelligence	Tomoharu Nakashima;Yasuyuki Yokota;Gerald Schaefer;Hisao Ishibuchi	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295632	type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;classification rule;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;one-class classification;fuzzy control system	Robotics	3.5880901728213104	-29.54435236902302	69179
f35f88a27298f2761c6c792c02a5d789c1a82672	an effective approach to software cost estimation based on soft computing techniques		Employing estimation models in software engineering help in envisaging some essential traits of future entities like software development effort, software reliability and programmers productivity. Of these models, the one that supports the estimation of software effort has drawn substantial attention currently to carry out researches. Estimation by analogy is one among the interesting techniques used for estimating the software effort. But, the process of estimating by analogy is unable to handle categorical data accurately. A novel technique that relies on reasoning by analogy, fuzzy logic and linguistic quantifiers is being proposed here for estimating effort, provided that the software project is represented either by categorical or numerical data. Use of fuzzy logic-based cost estimation models is more suitable if unclear or inaccurate information are considered. Fuzzy systems attempt to imitate the processes of the brain through a rule base. The proposed method utilizes Fuzzy logic based analogy approach to estimate the cost and the effort. The performance analysis of the proposed scheme is made using Mean Absolute Relative Error (MARE) and Mean Magnitude of Relative Error (MMRE) which is validated with other existing techniques.	approximation error;categorical variable;entity;fuzzy control system;fuzzy logic;level of measurement;mean squared error;numerical analysis;programmer;rule-based system;soft computing;software development effort estimation;software engineering;software project management;software reliability testing	Marappagounder Shanker;Keppanagounder Thanushkodi	2015	Int. Arab J. Inf. Technol.		machine learning;software quality;software;artificial intelligence;fuzzy control system;fuzzy logic;computer science;analogy;soft computing;software development;cost estimation models	SE	3.4250773319157264	-33.2089733361325	69520
b8a67c25e353cacf2309ef0693d7c98e585400e5	self-organizing maps with a time-varying structure	unsupervised learning;self organizing maps;neural networks	A number of research studies considering a self-organizing map have been developed since such a map was proposed by Kohonen [1982]. Some of these studies concern SOM-based models that do not use pre-defined structures to produce their mappings. We call these models Self-Organizing Maps with Time-Varying Structure (SOM-TVS). Despite the large number of SOM-TVS models there is not a standard way to describe them. In this article, we propose a framework to describe SOM-TVS models, which we use to describe some of these models and to compare their algorithms, and we present some real-world applications of the models presented.	algorithm;organizing (structure);self-organization;self-organizing map	Aluizio F. R. Araújo;Renata L. M. E. do Rego	2013	ACM Comput. Surv.	10.1145/2522968.2522975	unsupervised learning;self-organizing map;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	5.883143663348564	-34.022853134560734	69554
1f3372965745f75969d807198d1604cbc7ce94c1	layout-based approach for extracting constructive elements of bar-charts	analisis imagen;analisis forma;reconocimiento grafico;traitement document;pattern recognition;image analysis;pattern analysis;reconnaissance graphique;document processing;reconnaissance forme;network structure;reconocimiento patron;analyse image;analyse forme;graphical recognition;tratamiento documento	The analysis/recognition subject of chart structures aims to distinguish individual composite elements structurally, identify their constructive relationships logically, and extract the interpretative information semantically. Many of constructive elements are often allocated to appropriately predefined positions according to the illustrated structures of individual charts. Thus, we can construct a layout knowledge for chart structures with respect to the connective/neighboring relationships among some composite elements. We represent the layout konwledge of bar-chart structures, using a network structure called the layout network. This network contains all possible chart primitives in bar-charts and is composed by all possible relationships among them. In addition to this network, rules are introduced to make the interpretation of this layout network effectual and resolve the ambiguously duplicated representations stepwisely.	chart	Naoko Yokokura;Toyohide Watanabe	1997		10.1007/3-540-64381-8_47	image analysis;document processing;computer science;artificial intelligence;pattern recognition;algorithm	Crypto	-2.461597281427539	-31.228335336379562	69657
410ac9f40e47d6df59c31290cd477371715591b6	association rule based classifier built via direct enumeration, online pruning and genetic algorithm based rule decimation	supervised learning;set covering problem;association rule;genetic algorithm;classifiers;genetic algorithms;direct association rule mining	Direct rule generation is a possible alternative to tree building for classifiers. Here, we use the association rule framework to build classifiers. The rule generator performs direct enumeration (no generation of candidate sequences or so, and no preliminary enumeration of large sets) with online pruning to keep combinatorial explosion under control. The rule set thus generated is ultimately and drastically decimated so that a final non redundant rule system with reduced learning bias is produced. Decimation is modeled as a minimum cost and minimal set covering problem solved with a genetic algorithm. Experiment results are presented and compared to results obtained with a tree building based classifier (C4.5).	association rule learning;decimation (signal processing);genetic algorithm	Jean-Marc Adamo	2006			machine learning;pattern recognition;data mining;mathematics	ML	5.435940108945694	-32.425995272540725	69756
ed1af0ffa4e0a292cf14c50502f7be051a6ef25a	on-line incremental learning for unknown conditions during assembly operations with robots	learning criterion incremental learning unknown environments fuzzyartmap;robotic assembly fuzzy set theory learning artificial intelligence neurocontrollers;unknown environments;fuzzy set theory;robot sensing systems assembly artificial neural networks knowledge based systems vectors uncertainty;incremental learning;learning criterion;robotic assembly;fuzzyartmap;neurocontrollers;learning artificial intelligence;industrial robot online incremental learning assembly operations robots control algorithms fuzzy artmap pattern selection neural network weights peg in hole operation pih	To be effective in real operations where the environment is continuously changing, robots have to perceive the environment and to adapt accordingly. Unfortunately, there are uncertainties due to ageing of mechanisms, isturbances, backlash, etc. that limit the usage of current control algorithms. In this paper we propose an on-line incremental learning technique using Fuzzy ARTMAP and a pattern selection criterion. The technique starts by training the ANN with a primitive knowledge base. In the presence of new patterns, the criterion-based on the success of the current action-decides autonomously if the pattern should be learned, if the ANN has to recall, or if a recovery action must be performed. The incremental learning approach is based on the online update of the neural network weights and the defined criterion decides should the new pattern be learned. The peg in hole operation (PIH) is selected as the study case in order to evaluate the performance of the technique, which is described in detail as well as the basics of the peg in hole operation. Promising results obtained with real operations with an industrial robot without over training/forgetting is presented that validate the approach.	algorithm;artificial neural network;experiment;industrial robot;knowledge base;online and offline;stationary process	Jose Luis Navarro Gonzalez;Ismael Lopez-Juarez;Keny Ordaz-Hernández	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.101	robot learning;simulation;computer science;artificial intelligence;machine learning;fuzzy set	Robotics	6.595335975380159	-29.852218466368452	69800
1132c33886dee00fb7ea70f71b80d287117a4c63	intelligent feature selection method rooted in binary bat algorithm for intrusion detection	silicon;machine learning algorithms;naive bayes and bba feature selection svm;support vector machines;training;niobium;support vector machines machine learning algorithms silicon feature extraction intrusion detection niobium training;intrusion detection;nsl kdd dataset intelligent feature selection method binary bat algorithm intrusion detection complex cyber threats current dynamic threats machine learning algorithms;security of data learning artificial intelligence;feature extraction	The multitude of hardware and software applications generate a lot of data and burden security solutions that must acquire informations from all these heterogenous systems. Adding the current dynamic and complex cyber threats in this context, make it clear that new security solutions are needed. In this paper we propose a wrapper feature selection approach that combines two machine learning algorithms with an improved version of the Binary Bat Algorithm. Tests on the NSL-KDD dataset empirically prove that our proposed method can reduce the number of features with almost 60% and obtains good results in terms of attack detection rate and false alarm rate, even for unknown attacks.	bat algorithm;data mining;feature selection;intrusion detection system;lévy flight;machine learning;naive bayes classifier;run time (program lifecycle phase);support vector machine;threat (computer)	Adriana-Cristina Enache;Valentin Sgarciu;Alina Petrescu-Nita	2015	2015 IEEE 10th Jubilee International Symposium on Applied Computational Intelligence and Informatics	10.1109/SACI.2015.7208259	intrusion detection system;support vector machine;niobium;feature extraction;computer science;artificial intelligence;machine learning;pattern recognition;data mining;silicon;feature	Arch	6.926375487118149	-37.668043530920286	69844
ecc177cfe53d6dbe5ef4acf2eedf2c9f61930456	estimating the urban od matrix: a neural network approach	transportation network;transportation networks;zona urbana;red transporte;problema transporte;transportation problem;modele mathematique;probleme transport;zone urbaine;hopfield neural nets;intelligence artificielle;modelo matematico;hopfield neural network;reseau neuronal hopfield;mathematical model;urban area;artificial intelligence;inteligencia artificial;reseau transport;neural network	In this paper, the Hopfield Neural Network (HNN) model is used to estimate the urban Orientation-Destination (OD) distribution matrix from the link volumes of the transportation network, so as to promote the solving speed and precision.	artificial neural network	Zhejun Gong	1998	European Journal of Operational Research	10.1016/S0377-2217(97)00162-8	transportation theory;mathematical optimization;computer science;artificial intelligence;mathematical model;mathematics;operations research;artificial neural network	ML	9.847180249424424	-24.27111521351186	69872
77135400cd5ffecdf3de4cef8e978d792b5f6fc1	vtk: vertical mining of top-rank-k frequent patterns	databases;itemsets;frequent pattern;frequent pattern mining;frequent patterns;data mining;pattern mining;transaction databases;vtk;accidents;vertical mining;vertical pattern mining;vtk vertical pattern mining top rank k frequent patterns;scalability;top rank k frequent patterns;algorithm design and analysis;transaction databases data mining fuzzy systems laboratories knowledge engineering computer science electronic mail filtering;vertical mining data mining pattern mining frequent patterns	Mining top-rank-k frequent patterns is a new topic in frequent pattern mining. In this paper, we propose a new mining algorithm called VTK, vertical mining of Top-Rank-k frequent patterns, to mining Top-Rank-k frequent patterns using some vertical skills. Our performance study shows that the VTK method is more efficient and scalable for mining both synthetic datasets and real datasets than the algorithms proposed before.	algorithm;data mining;scalability;synthetic intelligence;vtk	Guo-Dong Fang;Zhi-Hong Deng	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.472	algorithm design;scalability;computer science;data science;data mining;database	ML	-4.450714553656713	-37.1642167030217	70317
86cd1f5c8a522cd28bd09d764d18fcc2ac67d378	the wm method completed: a flexible fuzzy system approach to data mining	steel rolling plant wang mendel method flexible fuzzy system approach data mining fuzzy rules generation data description data prediction fuzzy if then rules fuzzy predictive models continuous variables prediction piecewise constant output categorical variables prediction prediction accuracy fuzzy sets importance ranking pattern recognition;fuzzy logic data mining pattern recognition fuzzy set theory;fuzzy set;fuzzy systems data mining predictive models accuracy decision trees neural networks fuzzy sets input variables fuzzy logic pattern recognition;fuzzy rules;continuous variable;indexing terms;data mining;fuzzy set theory;fuzzy logic;prediction accuracy;pattern recognition;fuzzy if then rules;prediction model;fuzzy system	In this paper, the so-called Wang–Mendel (WM) method for generating fuzzy rules from data is enhanced to make it a comprehensive and flexible fuzzy system approach to data description and prediction. In the description part, the core ideas of the WM method are used to develop three methods to extract fuzzy IF–THEN rules from data. The first method shows how to extract rules for the user-specifed cases, the second method generates all the rules that can be generated directly from the data, and the third method extrapolates the rules generated by the second method over the entire domain of interest. In the prediction part, two fuzzy predictive models are constructed based on the fuzzy IF–THEN rules extracted by the methods of the description part. The first model gives a continuous output and is suitable for predicting continuous variables, and the second model gives a piecewise constant output and is suitable for predicting categorical variables. We show that by comparing the prediction accuracy of the fuzzy predictive models with different numbers of fuzzy sets covering the input variables, we can rank the importance of the input variables. We also propose an algorithm to optimalize the fuzzy predictive models, and show how to use the models to solve pattern recognition problems. Throughout this paper, we use a set of real data from a steel rolling plant to demonstrate the ideas and test the models. The core codes of the WM method are included in the Appendix.	algorithm;code;data mining;extrapolation;fuzzy concept;fuzzy control system;fuzzy set;pattern recognition;predictive modelling;wang tile	Li-Xin Wang	2003	IEEE Trans. Fuzzy Systems	10.1109/TFUZZ.2003.819839	fuzzy logic;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	ML	4.6652891099833855	-27.287368112366114	70460
a5d8fa76d9d7a57b108eef49fb994be29adc0458	on efficient factorization of standard fuzzy concept lattices and attribute-oriented fuzzy concept lattices		Abstract We propose two ways to make factorization of attribute-oriented fuzzy concept lattices recently studied by Ciobanu u0026 Văideanu more efficient. Firstly, we show that the blocks of the corresponding factor lattice are determined by a particular fuzzy interior operator. This allows us to compute an attribute-oriented fuzzy concept lattice by intents. Such a computation requires fewer traversals through data, as the number of the attributes is usually smaller than the number of objects. Secondly, we scale the input data in such a way that the factor lattice can be computed as a one-sided fuzzy-crisp concept lattice. Our experiments indicate that both methods lead to a significant improvement in efficiency of computation of the factor lattice. In addition, we improve and extend the theory provided by Ciobanu u0026 Văideanu.	fuzzy concept	Jan Konecny	2018	Fuzzy Sets and Systems	10.1016/j.fss.2018.01.012	fuzzy concept;discrete mathematics;operator (computer programming);fuzzy logic;mathematics;lattice (order);computation;factorization	Logic	-3.376683899807411	-25.692978391603784	70498
2d6a071e2ef820f7ce5ce51ec22e4bcd2a41766e	anomaly detection using weak estimators	databases;silicon;electronic mail;learning;network monitoring;handheld computer;anomaly detection;training;anomalous temperature patterns anomaly detection weak estimators parameterized statistical schemes training learning network monitoring;euclidean distance;electronic mail handheld computers silicon training euclidean distance databases;parameterized statistical schemes;weak estimators;statistical analysis;statistical analysis learning artificial intelligence parameter estimation security of data;parameter estimation;learning artificial intelligence;security of data;handheld computers;anomalous temperature patterns	Anomaly detection involves identifying observations that deviate from the normal behavior of a system. One of the ways to achieve this is by identifying the phenomena that characterize “normal” observations. Subsequently, based on the characteristics of data learned from the “normal” observations, new observations are classified as being either “normal” or not. Most state-of-the-art approaches, especially those which belong to the family parameterized statistical schemes, work under the assumption that the underlying distributions of the observations are stationary. That is, they assume that the distributions that are learned during the training (or learning) phase, though unknown, are not time-varying. They further assume that the same distributions are relevant even as new observations are encountered. Although such a “stationarity” assumption is relevant for many applications, there are some anomaly detection problems where stationarity cannot be assumed. For example, in network monitoring, the patterns which are learned to represent normal behavior may change over time due to several factors such as network infrastructure expansion, new services, growth of user population, etc. Similarly, in meteorology, identifying anomalous temperature patterns involves taking into account seasonal changes of normal observations. Detecting anomalies or outliers under these circumstances introduces several challenges. Indeed, the ability to adapt to changes in non-stationary environments is necessary so that anomalous observations can be identified even with changes in what would otherwise be classified as “normal” behavior. In this paper, we proposed to apply weak estimation theory for anomaly detection in dynamic environments. In particular, we apply this theory to detect anomaly activities in system calls. Our experimental results demonstrate that our proposal is both feasible and effective for the detection of such anomalous activities.	anomaly detection;estimation theory;sensor;stationary process;system call	Justin Zhijun Zhan;B. John Oommen;Johanna Crisostomo	2011	Proceedings of 2011 IEEE International Conference on Intelligence and Security Informatics	10.1109/ISI.2011.5984065	anomaly detection;computer science;artificial intelligence;machine learning;data mining;euclidean distance;silicon;estimation theory;computer security;network monitoring;statistics	Vision	0.6237303231678729	-36.338574371826596	70561
df403fc3407f79176399b19d0c9d761d5ff0c3fa	on classification of data by means of rough mereological granules of objects and rules	rough inclusions;granulation of knowledge;rough sets;rough set;data classification;decision rule	Granulation of knowledge has turned an effective tool in data classification. We propose the approach to classification of data which extends our earlier methods by considering granules of either objects or decision rules obtained either from the original training set or from its granular reflection. Members of a granule vote for the decision class of that object. We present results of tests which show that this method usually gives results at least as good as the exhaustive classifier built on rough set principles.	mereology;rough set	Piotr Artiemjew	2008		10.1007/978-3-540-79721-0_33	rough set;computer science;machine learning;pattern recognition;data mining;mathematics;dominance-based rough set approach	DB	1.4056060087066575	-28.162146268121685	70609
5f7e4948500433c378a1395cddf62bd0c54706bb	an interval set classification based on support vector machines	classification function;interval set classification;support vector machines;interval representation;prior knowledge;satisfiability;incomplete information;pattern classification;interval representation interval set classification support vector machines incomplete information classification function;support vector machine;support vector machines pattern classification;support vector machines support vector machine classification pattern recognition costs measurement standards helium educational institutions computer science information science algorithm design and analysis	The input vector of standard support vector machine (SVM) is n-array attributes. Before new patterns are classified by trained SVM, the measurement of all attribute values is always necessary. In order to make incomplete information patterns can be classified correctly by trained SVM, we extend the inputs vector of SVM to interval input vectors where each unmeasured attribute of input is represented by an interval which includes its possible value, and the operation in classification function was extended to interval operation correspondingly. For the incomplete information input, the value of classification function is the interval operation result. When the output of classification function satisfies the classification condition, the incomplete information input pattern can be classified correctly. Meanwhile the attribute value prior knowledge about interval representation can be utilized fully in the proposed algorithm. Both theory analyze and experiments result all show the present algorithm is practical and effective, and the input attribute measurement cost can also be reduced	algorithm;experiment;interval arithmetic;statistical classification;support vector machine	Yinggang Zhao;Qi Chen;Qinming He	2005	Joint International Conference on Autonomic and Autonomous Systems and International Conference on Networking and Services - (icas-isns'05)	10.1109/ICAS-ICNS.2005.20	support vector machine;least squares support vector machine;computer science;machine learning;pattern recognition;data mining;relevance vector machine;structured support vector machine	DB	2.823229067599647	-28.856220946822994	70616
2bafdfc9458f80561a33f02f2baf71ff0bbb9832	the quotient structure in granule computing	problem solving quotient structure granule computing theory quotient space theory rough set theory;rough set theory;artificial intelligence set theory problem solving humans switches cognitive science expert systems switching circuits technological innovation signal processing;problem solving rough set theory;rough set granule computing quotient space theory quotient structure;problem solving	The theory of granule computing based on the quotient space is one of the three main granule computing theories. The emphasis is on the structure of the quotient space theory in this paper. Comparing with rough set theory, the authors point out the importance of the structure in granule computing theory. A new method of constructing quotient space according to the structure is also presented in this paper. The differences between the quotient structure and structure-based method are proved. Finally, some examples show the rationality and feasibility of our methods.	granule (oracle dbms);rationality;rough set;set theory	Shu Zhao;Yanping Zhang;Ling Zhang;Jie Chen;Zhong Wan;Ying-chun Zhang;Chen-xi Zhang	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547304	combinatorics;discrete mathematics;rough set;quotient space;computer science;machine learning;mathematics	Robotics	-1.1485332062954163	-24.262545560055354	70712
b43f0d9f1ae8c7fbd058edaa6d092fc5ed58ba96	generating z-number based on owa weights using maximum entropy			principle of maximum entropy	Bingyi Kang;Yong Deng;Kasun Hewage;Rehan Sadiq	2018	Int. J. Intell. Syst.	10.1002/int.21995	principle of maximum entropy;mathematics;artificial intelligence;machine learning;pattern recognition	ML	2.3002871248895165	-24.6420852456772	70919
da4f2fcc99d6e1af47010f7affb4c631c79aacc8	fuzzy entropy based max-relevancy and min-redundancy feature selection	estimation theory;probability;probability density estimation;probability density;max relevance min redundancy algorithm;fuzzy entropy;horses;fuzzy set theory;max relevancy;probability entropy estimation theory feature extraction fuzzy set theory minimax techniques pattern classification;accuracy;classification algorithms accuracy entropy mutual information algorithm design and analysis indexes horses;indexes;minimax techniques;feature selection algorithm;feature extraction;pattern classification system;classification algorithms;pattern classification;mrmr algorithm;mutual information;min redundancy;feature selection;entropy;fuzzy entropy feature selection max relevancy min redundancy mrmr;mrmr;algorithm design and analysis;probability density estimation feature selection algorithm fuzzy entropy pattern classification system mrmr algorithm max relevance min redundancy algorithm	Feature selection is an important problem for pattern classification systems. Mutual information is a good indicator of relevance between variables, and has been used as a measure in several feature selection algorithms. Because the mutual information could not be calculated directly for continuous data sets in max-relevance and min-redundancy (mRMR) algorithm, here we combine the mRMR algorithm with fuzzy entropy, which avoids estimating probability density. We test our new algorithm using several different data sets and two different classifiers. According to the comparison between the new algorithm and max-dependency, max-dependency and min-redundancy (mDMR) algorithms, it is proven the new algorithm is feasible and valid.	aggregate data;algorithm;experiment;feature selection;maxima and minima;mutual information;relevance	Shuang An;Qinghua Hu;Daren Yu	2008	2008 IEEE International Conference on Granular Computing	10.1109/GRC.2008.4664740	database index;algorithm design;entropy;probability density function;feature extraction;computer science;machine learning;pattern recognition;probability;data mining;mathematics;accuracy and precision;fuzzy set;mutual information;estimation theory;feature selection;statistics	Robotics	2.354968274046905	-36.67888356908036	71060
698c403fb9b59d34aa9236a1250ba30fe15e0581	fuzzy evaluation on network security based on the new algorithm of membership degree transformation -m(1, 2, 3)	membership degree transformation;network security;information transmission;evaluation method;data mining;close relationships;information management;indexation;fuzzy evaluation;physical environment;object classification;3 model;2;m 1	Network security has close relationships with the physical environment of information carriers, information transmission, information storage, information management and so on, and these relations have much ambiguity. Therefore, it is reasonable and scientific to apply fuzzy comprehensive evaluation method for network security fuzzy evaluation. The core of network security fuzzy evaluation is membership degree transformation calculation. But the transformation methods should be discussed, because redundant data in index membership degree is also used to compute object membership degree, which is not useful for object classification. The new algorithm is: using data mining technology based on entropy to mine knowledge information about object classification hidden in every index, affirm the relationship of object classification and index membership, eliminate the redundant data in index membership for object classification by defining distinguishable weight and extract valid values to compute object membership. The new algorithm of membership degree transformation includes three calculation steps which can be summarized as “effective, comparison and composition”, which is denoted as M(1,2,3). The paper applied the new algorithm in network security fuzzy evaluation.	algorithm;computation;computer data storage;data mining;formal system;fuzzy set;information management;interference (communication);network security;nonlinear system	Hua Jiang;Junhu Ruan	2009	JNW	10.4304/jnw.4.5.324-331	square root of 2;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;network security;machine learning;data mining;fuzzy set;information management;computer security	DB	-3.5006109046623797	-24.952212382740647	71479
c3595d39812d57f5b476233390ceddf36d893657	a methodology to explain neural network classification	extraction information;evaluation performance;analyse amas;metodologia;performance evaluation;information extraction;saliency;multilayer perceptrons;evaluacion prestacion;knowledge extraction;neural network classifier;multilayer perceptron;data mining;classification;methodologie;perceptron multicouche;cluster analysis;fouille donnee;clustering;analisis cluster;reseau neuronal;methodology;busca dato;clasificacion;extraccion informacion;red neuronal;arsenic;neural network	Neural networks are still frustrating tools in the data mining arsenal. They exhibit excellent modelling performance, but do not give a clue about the structure of their models. We propose a methodology to explain the classification obtained by a multilayer perceptron. We introduce the concept of 'causal importance' and define a saliency measurement allowing the selection of relevant variables. Once the model is trained with the relevant variables only, we define a clustering of the data built from the hidden layer representation. Combining the saliency and the causal importance on a cluster by cluster basis allows an interpretation of the neural network classifier to be built. We illustrate the performances of this methodology on three benchmark datasets.	benchmark (computing);biological neural networks;causal filter;cluster analysis;data mining;multilayer perceptron;performance;statistical cluster	Raphaël Féraud;Fabrice Clérot	2002	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(01)00127-7	computer science;artificial intelligence;machine learning;data mining;knowledge extraction;cluster analysis;information extraction;artificial neural network	ML	9.34552390605196	-32.20907454274395	71556
b1fa1a7c72adf1ff33afb1e5b2f2c3d9036afac1	a visual approach for fuzzy rule induction	design process;fuzzy rules;pruning techniques visual approach fuzzy rule induction fine tuning fuzzy decision trees description accuracy design process;prior knowledge;knowledge based systems decision trees data visualisation fuzzy logic;fuzzy logic;humans decision trees process design industrial engineering manufacturing automation productivity continuous improvement continuous production sensor phenomena and characterization data visualization;data visualisation;visualization technique;membership function;fuzzy decision tree;building model;decision trees;knowledge based systems	Models are descriptions of real facts that serve us to think and reason. In building models, a compromise always exists between accuracy, that is, how precisely the model describes reality, and simplicity, without which the model would be useless. So, a good model must be simple and intuitive while being accurate enough. In this paper we propose a novel approach based on visual techniques aiming to help the human in fine-tuning fuzzy decision trees to enhance its interpretability and insightfulness with a minimal loss of accuracy. By involving the human in the design process, these techniques allow to include prior knowledge in the selection of membership functions as well as to assess the significance of rules in the model to help in the pruning stage.	fuzzy rule;rule 90;rule induction	Sergio R. Cuesta;Ignacio Díaz Blanco;Abel Alberto Cuadrado Vega;Alberto B. Diez	2003		10.1109/ETFA.2003.1248775	fuzzy logic;design process;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;knowledge-based systems;machine learning;decision tree;data mining;information fuzzy networks;fuzzy associative matrix;fuzzy set operations;data visualization;grafting	NLP	3.921411673864726	-27.996051145543728	71575
95eca6bff4230b8c96835626c7d949f68494289c	fuzzy region connection calculus in finite discrete space domains	geographic information system;spatial reasoning;road network;fuzzy set theory;fuzzy logic;geographical information systems;spatial relation;region connection calculus;binary relation;spatial information	Many practical applications involving spatial aspects work with finite discrete space domains, e.g. map grids, railways track layouts and road networks. Such space domains are computationally tractable and often include specialised forms of spatial reasoning. Moreover, in such applications, the spatial information naturally includes various forms of approximation, uncertainty or inexactness. Fuzzy representations are then appropriate. In this paper, we reformulate the region connection calculus (RCC) framework for finite, discrete space domains in simple set-theoretical terms. We generalise RCC framework and develop several fuzzy spatial concepts like fuzzy regions, fuzzy directions, fuzzy named distances. We propose a fuzzification of standard spatial relations in RCC. For this purpose, we enhance the fuzzy set theory to include fuzzy definitions for membership, subset and set equality crisp binary relations between sets (fuzzy or crisp). We illustrate the approach using a discrete finite two-dimensional map grid as the space domain. © 2003 Elsevier B.V. All rights reserved.	approximation algorithm;cobham's thesis;digital signal processing;fuzzy concept;fuzzy logic;fuzzy set;geographic information system;grid reference;query language;region connection calculus;set theory;simple set;spatial query;spatial–temporal reasoning;whole earth 'lectronic link	Girish Keshav Palshikar	2004	Appl. Soft Comput.	10.1016/j.asoc.2003.05.008	spatial relation;fuzzy logic;mathematical optimization;combinatorics;mathematical analysis;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;binary relation;mathematics;spatial analysis;geographic information system;fuzzy set;spatial intelligence;fuzzy associative matrix;fuzzy set operations	AI	-1.541370497663956	-24.734214060466858	71788
f8f95c271461ce9b61e05e62861d49f56b27ddb7	rough sets based rule generation from data with categorical and numerical values	numerical patterns;rough set theory;machine learning;utility programs;rule generation;rough sets;rough set;numerical values	Rough set theory has been mainly applied to data with categorical values. In order to handle data with nu- merical values in this theory, a familiar concept of 'wildcards' was employed, and a new framework of rough sets based rule generation has been proposed. Two characters @ and # were introduced into this framework, and numerical patterns were also defined for numerical values. The concepts of 'coarse' and 'fine' for rules were explicitly defined according to nu- merical patterns. This paper enhances the previous framework, and describes the implementation of an utility program. This utility program is applied to the data in UCI Machine Learning Repository, and some useful rules are obtained.	numerical method;rough set	Hiroshi Sakai;Kazuhiro Koba;Michinori Nakata	2008	JACIII	10.20965/jaciii.2008.p0426	rough set;computer science;machine learning;pattern recognition;data mining;dominance-based rough set approach	HPC	2.146370329184705	-28.832231194307575	71839
3c4350df48a37ea5fc8cd64bf312c97f21843a3c	discovering action rules that are highly achievable from massive data	naive bayes classifier;rule discovery;breadth first search	In this paper, we propose a novel algorithm which discovers a set of action rules for converting negative examples into positive examples. Unlike conventional action rule discovery methods, our method AARUDIA (Achievable Action RUle DIscovery Algorithm) considers the effects of actions and the achievability of the class change for disk-resident data. In AARUDIA, effects of actions are specified using domain rules and the achievability is inferred with Naive Bayes classifiers. AARUDIA takes a new breadth-first search method which manages actionable literals and stable literals, and exploits the achievability to reduce the number of discovered rules. Experimental results with inflated real-world data sets are promising and demonstrate the practicality of AARUDIA.		Einoshin Suzuki	2009		10.1007/978-3-642-01307-2_72	naive bayes classifier;breadth-first search;computer science;machine learning;pattern recognition;data mining;mathematics	ML	-4.423828023916184	-30.055052971518684	71918
a4871161eb3933a824a544a7fced17450d49a752	application of genetic algorithm to pattern extraction	population selection method genetic algorithm pattern extraction pattern recognition method time series data fitting data prediction;no response;time series;data mining;genetic algorithms data mining mathematics information science pattern recognition data analysis testing energy management parallel processing time series analysis;area of interest;learning artificial intelligence pattern recognition genetic algorithms time series data mining;pattern recognition;genetic algorithm;genetic algorithms;missing data;learning artificial intelligence;data fitting;work practice	The area of interest for this paper covers pattern recognition method, which can find and classify all useful relations between data entries in the time series. Genetic algorithm has been deployed to prepare and govern a set of independent patterns. For each pattern additional quality value has been added. This value corresponds to the level of certainty and is introduced in the work. Practical application of this solution consists of data fitting and prediction. Analyzed data can be non continuous and incomplete. In uncertain cases algorithm presents either no response at all or more than one answer to processed data. Architecture of the system offers possibility to interleave learning phase with use. Genetic algorithm applied in the method facilitates niche techniques as well as crowd factor and specialized population selection methods. Early testing results, which include prediction and fitting of simple time series with up to 50 percent of missing data, are presented at the end of the paper.	curve fitting;genetic algorithm;missing data;niche blogging;pattern recognition;time series	Marcin Borkowski	2005	5th International Conference on Intelligent Systems Design and Applications (ISDA'05)	10.1109/ISDA.2005.24	genetic algorithm;computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics;population-based incremental learning	Robotics	0.06414402118018052	-32.75392912834677	72040
185d390955438ea55b68a8839fead3e1bae1ad0f	multi-criteria fuzzy clustering problems based on vague set theory	pattern clustering;universe of discourse;fuzzy set theory set theory educational technology fuzzy sets mathematics cities and towns information science mathematical model educational institutions computer industry;set theory;data mining;fuzzy set theory;multi criteria fuzzy clustering problems;fuzzy clustering;vague set theory;membership function;pattern clustering fuzzy set theory;data mining multi criteria fuzzy clustering problems vague set theory decision making;similarity measure	Multi-criteria fuzzy clustering problems are kind of common problems in decision-making and data mining. A method for modeling multi-criteria problems based on vague set theory is used in this paper. Each object in a universe of discourse is represented by a vague set in this model, then similar objects can be clustered into a subgroup after similarity measure between corresponding vague sets. The proposed method can solve the multi-criteria fuzzy clustering problems in a reasonable and reliable way.	cluster analysis;data mining;domain of discourse;fuzzy clustering;set theory;similarity measure;vague set;vagueness	Qingbo Yang;Jinping Li;Weiyu Zhang;Ruiying Gong	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.413	fuzzy logic;correlation clustering;constrained clustering;discrete mathematics;membership function;defuzzification;fuzzy clustering;type-2 fuzzy sets and systems;flame clustering;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;domain of discourse;mathematics;fuzzy set;cluster analysis;fuzzy set operations;set theory	DB	-1.7921665551012558	-25.62101446963853	72171
50141b99217bca3962464c5a275c3198eca81867	automated generation of connectionist expert systems for problems involving noise and redundancy	expert system	When creating an expert system, the most difficult and expensive task is constructing a knowledge base. This is particularly true if the problem involves noisy data and redundant measurements. This paper shows how to modify the MACIE process for generating connectionist expert systems from training examples so .that it can accommodate noisy and redundant data. The basic idea is to dynamically generate appropriate training examples by constructing both a 'deep' model and a noise model for the underlying problem. The use of winner-take-all groups of variables is also discussed. These techniques are illustrated with a small example that would be very difficult for standard expert system approaches. TOPICS/	connectionism;expert system;knowledge base;redundancy (engineering);signal-to-noise ratio	Stephen I. Gallant	1988	Int. J. Approx. Reasoning		legal expert system;computer science;artificial intelligence;machine learning;data mining;expert system	AI	5.747601699513517	-30.957340234833403	72202
6a292a8e7f814ad213a9e2d24f2bc741e8e4224f	granular fuzzy modeling with evolving hyperboxes in multi-dimensional space of numerical data	conditional fuzzy c means;hyperboxes;differential evolution;granular fuzzy modeling;induced information granules	Clustering has been applied to numerous areas, including signal and image processing. Many approaches have been developed over the years to efficiently construct granular models on a basis of numerical experimental data. In this study, we propose a novel approach to construct a granular model that is fundamentally designed around information granules regarded as hyperboxes. Several studies have been focused on building a set of hyperboxes around data; one of them being a Min-Max Neural Network (NN) algorithm. Here we develop two different methods to construct these information granules, nevertheless some essential similarities to previous studies can be found. In particular, hyperboxes are constructed by using some reference data, and they are endowed with some parametric flexibility to facilitate controlling their size, whereas the construction of the hyperboxes involve elimination or reduction of possible overlaps between them. In the proposed approach, given a set of input and output data pairs, we construct interval-based information granules to partition the output space (viz. the space of the output variable). On a basis of these intervals, we carry out a so-called contextbased Fuzzy C-Means algorithm to construct cluster centers (prototypes) in the multivariable input space. These prototypes serve as hyperbox cores. To construct the information granules, two methods are studied: one develops a family of hyperboxes by realizing some constrictions, while the other one engages Differential Evolution (DE) to realize further optimization. To reduce overlap, two methods are tested: one being previously proposed for the min-max NN and a new one, which engages DE to optimize	acceptance testing;algorithm;amplifier;artificial neural network;cluster analysis;computational complexity theory;cyclic redundancy check;differential evolution;gigabyte;granule (oracle dbms);image processing;input/output;level of measurement;mathematical optimization;maxima and minima;mind;noise shaping;numerical analysis;population;random search;randomness;stochastic process;the offspring;venue (sound system);viz: the computer game	Orion Fausto Reyes-Galaviz;Witold Pedrycz	2015	Neurocomputing	10.1016/j.neucom.2015.05.102	differential evolution;computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm;statistics	ML	5.590613668639096	-26.338523824348975	72277
2f349ec19443523bc6c1e4b15fb677b1c188e253	finding time series motifs in disk-resident data	databases;sorting;time series data mining;time series;data mining;time series motifs;computer network;closest pair;disk aware algorithm time series motifs disk resident data data mining;time series analysis;exact algorithm;data mining algorithm;classification algorithms;closest pair time series motif exact algorithm;disk resident data;rule discovery;approximation methods;disk aware algorithm;data mining clustering algorithms usa councils classification algorithms image databases biomedical imaging dna multidimensional systems computer science data engineering;algorithm design and analysis;time series motif	Time series motifs are sets of very similar subsequences of a long time series. They are of interest in their own right, and are also used as inputs in several higher-level data mining algorithms including classification, clustering, rule-discovery and summarization. In spite of extensive research in recent years, finding exact time series motifs in massive databases is an open problem. Previous efforts either found approximate motifs or considered relatively small datasets residing in main memory. In this work, we describe for the first time a disk-aware algorithm to find exact time series motifs in multi-gigabyte databases which contain on the order of tens of millions of time series. We have evaluated our algorithm on datasets from diverse areas including medicine, anthropology, computer networking and image processing and show that we can find interesting and meaningful motifs in datasets that are many orders of magnitude larger than anything considered before.	approximation algorithm;central processing unit;cluster analysis;computer data storage;data mining;database;gigabyte;image processing;parallel computing;scalability;sequence motif;time series	Abdullah Mueen;Eamonn J. Keogh;Nima Bigdely Shamlo	2009	2009 Ninth IEEE International Conference on Data Mining	10.1109/ICDM.2009.15	statistical classification;computer science;data science;machine learning;time series;data mining;statistics	DB	-2.846215873745938	-37.46630244753431	72564
01c3146b5dd4c1f3c6aef61482803e278f4395b5	classification of welding flaw types with fuzzy expert systems	bootstrap method;fuzzy rules;classification;radiography;fuzzy expert system;fuzzy expert systems;genetic algorithm;k nearest neighbor;genetic algorithms;multi layer perceptron;welding flaws;classification accuracy;neural network	The fuzzy expert system approach is proposed for the classification of different types of welding flaws. The fuzzy rules are generated from available examples using two different methods. The classification accuracy of fuzzy expert systems using fuzzy rules generated by the two methods is evaluated and compared. In addition, the fuzzy expert system approach is also compared with two other approaches: the fuzzy  k -nearest neighbors algorithm and multi-layer perceptron neural networks, based on the bootstrap method. The results indicate that the fuzzy expert system approach outperforms all others in terms of classification accuracy.	expert system;flaw hypothesis methodology	T. Warren Liao	2003	Expert Syst. Appl.	10.1016/S0957-4174(03)00010-1	genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;neuro-fuzzy;machine learning;pattern recognition;data mining;fuzzy associative matrix;fuzzy set operations;artificial neural network	ML	9.577210435710862	-37.680787647235476	72702
77a634aace4c6eea5a2190ee8b2b8e847dd59283	learning classification rules using lattices (extended abstract)	search space;noisy data;galois lattice;learning system;learning methods;classification rules	This paper presents a novel induction algorithm, Rulearner, which induces classification rules using a Galois lattice as an explicit map through the search space of rules. The Rulearner system is shown to compare favorably with commonly used symbolic learning methods which use heuristics rather than an explicit map to guide their search through the rule space. Furthermore, our learning system is shown to be robust in the presence of noisy data. The Rulearner system is also capable of learning both decision lists and unordered rule sets allowing for comparisons of these different learning paradigms within the same algorithmic framework.	algorithm;decision list;formal concept analysis;heuristic (computer science);signal-to-noise ratio	Mehran Sahami	1995		10.1007/3-540-59286-5_83	semi-supervised learning;discrete mathematics;machine learning;pattern recognition;mathematics;learning classifier system;generalization error	ML	6.619013852637356	-31.415746094256672	72747
bff05932b550f8c74dc0c3358e1dec2a13cc1605	intelligent hybrid system for person identification using biometric measures and modular neural networks with fuzzy integration of responses	fuzzy integral;intelligent system;hybrid system;modular neural network;neural network	This paper presents an intelligent system for person identification with biometric measures such as signature, fingerprint  and face. We describe the neural network architectures used to achieve person identification based on the biometrics measures.  Simulation results show that the proposed method provides good recognition.  	biometrics;hybrid system;modular neural network	Magdalena Serrano;Erika Ayala;Patricia Melin	2009		10.1007/978-3-642-04516-5_6	control engineering;adaptive neuro fuzzy inference system;artificial intelligence;neuro-fuzzy;machine learning;time delay neural network;intelligent control	Vision	4.8205909017981945	-26.363278775132983	72946
6eeb5b48cfc34eb6736852403d94d7e124537e0f	related family: a new method for attribute reduction of covering information systems	covering;granular computing;related family;attribute reduct;feature selection;rough set	In terms of attribute reduction of covering based rough sets, the discernibility matrix is used as a conventional method to compute all attribute reducts. However, it is inapplicable to attribute reduction in certain circumstances. In this article, a new method, referred to as the related family, is introduced to compute all attribute reducts and relative attribute reducts for covering rough sets. Its core idea is to remove superfluous attributes while keeping the approximation space of covering information system unchanged. The related family method is more powerful than the discernibility matrix method, since the former can handle complicated cases that could not be handled by the latter. In addition, a simplified version of the related family and its corresponding heuristic algorithm are also presented.	algorithm;futures studies;heuristic;information system;level of detail;numerical analysis;rough set	Tian Yang;Qingguo Li;Bilei Zhou	2013	Inf. Sci.	10.1016/j.ins.2012.11.005	discrete mathematics;rough set;attribute domain;granular computing;computer science;machine learning;data mining;mathematics;feature selection;algorithm	AI	-3.2121122073430217	-28.49082148635174	73060
7be22a11235b5691515d964c861177aac4558ade	keepaway soccer: from machine learning testbed to benchmark	multiagent system;soccer;prior knowledge;robotics;machine learning;football;robotica;robotique;sistema multiagente;programming tool;systeme multiagent;futbol	Keepaway soccer has been previously put forth as a testbed for machine learning. Although multiple researchers have used it successfully for machine learning experiments, doing so has required a good deal of domain expertise. This paper introduces a set of programs, tools, and resources designed to make the domain easily usable for experimentation without any prior knowledge of RoboCup or the Soccer Server. In addition, we report on new experiments in the Keepaway domain, along with performance results designed to be directly comparable with future experimental results. Combined, the new infrastructure and our concrete demonstration of its use in comparative experiments elevate the domain to a machine learning benchmark, suitable for use by researchers across the field.	benchmark (computing);experiment;machine learning;testbed	Peter Stone;Gregory Kuhlmann;Matthew E. Taylor;Yaxin Liu	2005		10.1007/11780519_9	simulation;computer science;artificial intelligence;machine learning;robotics	ML	5.631399355477984	-33.123464624290555	73149
6c6663ebb1f16d2dc345ba41764073b6dee2e3e4	a fuzzy model-based neural network for adaptive regularization in image restoration	fuzzy neural nets;edge detection;nn fuzzy model based neural network adaptive regularization image restoration neural network learning approach local regularization parameter values network weights training examples noise masking capabilities edge texture characterization measure etc measure fuzzified form;image restoration;adaptive systems;fuzzy neural networks neural networks adaptive systems intelligent networks image restoration cost function least squares methods fuzzy sets pixel australia;learning artificial intelligence;fuzzy model;neural network;edge detection image restoration fuzzy neural nets adaptive systems learning artificial intelligence	We address the problem of adaptive regularization in image restoration by adopting a neural network learning approach. The local regularization parameter values are regarded as network weights which are then modified through the supply of appropriate training examples. We also consider the separate regularization of edges and textures due to their different noise masking capabilities, which in turn requires discrimination between these two feature types. A new edge-texture characterization (ETC) measure is derived and incorporated into a fuzzified form of the previous NN for the above purpose.	artificial neural network;circuit restoration;image restoration;matrix regularization	Hau-San Wong;Ling Guan	1999		10.1109/ICIP.1999.821637	image restoration;computer vision;edge detection;adaptive neuro fuzzy inference system;computer science;artificial intelligence;adaptive system;neuro-fuzzy;machine learning;pattern recognition;time delay neural network;mathematics;artificial neural network	ML	6.97732758300849	-28.433009490036817	73348
73af858a3e1056b0449af10ae7d05c020cef0c9d	approximate entropy reducts	decision reducts;approximate reduct;probabilities in data;entropy	We use information entropy measure to extend the rough set based notion of a reduct. We introduce the Approximate Entropy Reduction Principle (AERP). It states that any simplification (reduction of attributes) in the decision model, which approximately preserves its conditional entropy (the measure of inconsistency of defining decision by conditional attributes) should be performed to decrease its prior entropy (the measure of the model’s complexity). We show NP-hardness of optimization tasks concerning application of various modifications of AERP to data analysis.	approximate entropy;complexity;conditional entropy;entropy (information theory);level of detail;mathematical optimization;np-hardness;rough set	Dominik Slezak	2002	Fundam. Inform.		approximate entropy;entropy;joint entropy;combinatorics;discrete mathematics;information diagram;binary entropy function;transfer entropy;maximum entropy probability distribution;principle of maximum entropy;pattern recognition;mathematics;differential entropy;conditional entropy;sample entropy	ML	-2.6315300150918968	-26.93871367563	73379
d8a89d37920ea33b0672a9c4f4a06cd2853faf3a	a fuzzy approach to the game of chicken based on fuzzy expected value models	expected value		fuzzy logic	Raj Mathew;M. R. Kaimal	2003			mathematical optimization;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy number;machine learning;fuzzy measure theory;expected value	AI	1.8625624898712065	-24.281484983150737	73395
4077844d2bab6ef370965d33d3c1d6669b753e21	a fuzzy k-partitions model for categorical data and its comparison to the gom model	engineering;multinomial distribution;cluster algorithm;fuzzy k partitions fkp model;fuzzy set;procesamiento informacion;grade of membership;analisis datos;fonction repartition;ley n variables;grade of membership gom model;fuzzy centroids;conjunto difuso;ensemble flou;funcion n variables;classification;fuzzy clustering algorithms;ingenierie;algorithme;algorithm;funcion distribucion;fuzzy clustering;data analysis;distribution function;cluster analysis;particion;n variable function;fuzzy k modes;information processing;partition;fonction n variables;multivariate distribution;analyse donnee;ingenieria;sistema difuso;systeme flou;traitement information;computational efficiency;loi n variables;categorical data;likelihood function;clasificacion;fuzzy system;algoritmo	The grade of membership (GoM) model uses fuzzy sets as memberships of each individual to extreme profiles (or classes) on the likelihood function of multivariate multinomial distributions. The GoM clustering algorithm derived from the GoM model is used in cluster analysis for categorical data, but it is iterated with complicated calculations. In this paper we create another approach, termed a fuzzy k-partitions (FkP) model, which is also based on the likelihood function of multivariate multinomial distributions. However, the calculations of the FkP algorithm for clustering categorical data derived from the proposed FkP model are simpler. The proposed FkP clustering algorithm is not only easier in calculation than the GoM, but also has more accuracy and computation efficiency. To verify it, we employ real empirical data and also some simulation data. We find that FkP has superior results to GoM. We then apply these two algorithms to classification of pathology. The results show the superiority of the FkP clustering algorithm. Moreover, the proposed FkP algorithm can be used as a fuzzy clustering algorithm for categorical data. Some comparisons between FkP and two popular algorithms, fuzzy k-modes and fuzzy centroids, are made. These results show that the FkP clustering algorithm can be another useful tool in analyzing categorical data. © 2007 Elsevier B.V. All rights reserved.	algorithm;categorical variable;cluster analysis;computation;fuzzy clustering;fuzzy set;iteration;multinomial logistic regression;simulation	Miin-Shen Yang;Yu-Hsuan Chiang;Chiu-Chi Chen;Chien-Yo Lai	2008	Fuzzy Sets and Systems	10.1016/j.fss.2007.08.012	partition;multivariate normal distribution;categorical variable;information processing;fuzzy clustering;biological classification;computer science;distribution function;machine learning;data mining;mathematics;likelihood function;fuzzy set;cluster analysis;data analysis;fuzzy control system;multinomial distribution;statistics	ML	8.44244073153366	-34.36398983061008	73476
625c8bc573f9e14e76832666111fb5173180819e	a method for multistrategy task-adaptive learning based on plausible justifications	adaptive learning	Multistrategy task-adaptive learning (MTL) comprises a class of methods in which the learner determines by itself which strategy or combination of strategies is most appropriate for a given learning task defined by the learner’s goal, the learner's background knowledge (BK) and the input to the learning process. The paper presents a MTL method which is based on building a plausible justification that the learner’s input is a consequence of its BK. The method assumes a general learning goal of deriving any useful knowledge from a given input and integrates dynamically a whole range of learning strategies. It also behaves as a singlestrategy method when the relationship between the input and the BK satisfies the requirements of the single-strategy method, and the general learning goal of the MTL method is specialized to the goal of the single-strategy method.	common lisp;experiment;horner's method;mason;mathematical induction;natural deduction;requirement	Gheorghe Tecuci;Ryszard S. Michalski	1991			computer science;artificial intelligence;machine learning;adaptive learning	ML	6.293625745182664	-30.848303533574263	73588
c5a0a6e1d84dcc2a1d190f5f6190497e4aec4a2c	from logic descriptors to granular logic descriptors: a study in allocation of information granularity		Local sources of knowledge structured in the form of logic descriptors—constructs of fuzzy logic, are arranged together (structured) in the form of a global model coming as a high-level granular logic descriptor. The inherent granularity of the global descriptor of this nature arises as a manifestation of the diversity of the locally available descriptors. The granular descriptor can be expressed with the aid of any of the formal models of information granules including sets, fuzzy sets, rough sets, probabilistic granules and others. The architectural essence of the granular descriptor, which supports a quantification of the variability among the sources of knowledge, is realized through an optimal allocation of information granularity. Information granularity is treated as an important design asset and its allocation throughout the parameters of the logic descriptors helps quantify the diversity of individual sources of knowledge. Various protocols of allocation of information granularity along with an overall quantification of their effectiveness are discussed along with their numeric characterization.	cluster analysis;data descriptor;feedforward neural network;fuzzy logic;fuzzy set;granular computing;high- and low-level;mathematical optimization;rough set;spatial variability;type-2 fuzzy sets and systems	Witold Pedrycz	2013	J. Ambient Intelligence and Humanized Computing	10.1007/s12652-012-0127-x	theoretical computer science;data mining	AI	1.3326553793221319	-26.855990978646638	73627
8607bfe8759ce6176ee65bf466db8e2a2d1ca7fb	reconstructing x'-deterministic extended petri nets from experimental time-series data x'		This work aims at reconstructing Petri net models for biological systems from experimental time-series data X ′. The reconstructed models shall reproduce the experimentally observed dynamic behavior in a simulation. For that, we consider Petri nets with priority relations among the transitions and control-arcs, to obtain additional activation rules for transitions to control the dynamic behavior. The contribution of this paper is to present an integrative reconstruction method, taking both concepts, priority relations and control-arcs, into account. Our approach is based on previous works for special cases and shows how these known steps have to be modified and combined to generate the desired integrative models, called X ′-deterministic extended Petri nets.	biological system;experiment;petri net;simulation;time series	Marie C. F. Favre;Annegret Wagler	2013			discrete mathematics;time series;petri net;stochastic petri net;computer science	Logic	-4.150835903839986	-31.774456408472826	73706
5f537e1f9be822bf8c3b98cdc325c940521c69ad	a discretization process in accordance with a qualitative ordered output	supervised discretization;qualitative reasoning	This paper lies within the domain of supervised discretization methods. The methodology aims at identifying relevant interactions between input and output variables. A new supervised discretization algorithm that takes into account the qualitative ordinal structure of the output variable is proposed. Most existing supervised discretization methods are designed for pattern recognition problems and do not take into account this ordinal structure. A qualitative distance is constructed over the discrete structure of absolute orders of magnitude spaces. The algorithm presented implements a maximization process of this distance. A simple example allows interpretation of the process of choosing landmarks.	discretization	Francisco Javier Ruiz;Cecilio Angulo;Núria Agell;Xari Rovira;Mónica Sánchez;Francesc Prats	2005			discretization error;mathematical optimization;discrete mathematics;machine learning;discretization;mathematics;discretization of continuous features	Vision	9.598909923665707	-29.267768610088353	74085
7ba0d369922b95114c0cae06a1f0cd7958ed0aa6	an approach to dealing with missing values in heterogeneous data using k-nearest neighbors		Techniques such as clusterization, neural networks and decision making usually rely on algorithms that are not well suited to deal with missing values. However, real world data frequently contains such cases. The simplest solution is to either substitute them by a best guess value or completely disregard the missing values. Unfortunately, both approaches can lead to biased results. In this paper, we propose a technique for dealing with missing values in heterogeneous data using imputation based on the k-nearest neighbors algorithm. It can handle real (which we refer to as crisp henceforward), interval and fuzzy data. The effectiveness of the algorithm is tested on several datasets and the numerical results are promising. 1. Introduction Missing data occurs in many research areas such as data mining, machine learning and statistics, which often use algorithms that are not well suited to deal with missing values. In decision making, for instance, the presence of missing values may lead to invalid evaluation of the criteria, therefore biasing the result towards an inappropriate decision, as shown by Ma et al. [16]. A data-set is considered incomplete if an attribute of a feature is not observed. Many factors can lead to unobserved values, such as machine fault on sampling data, human refusal to provide full information and censorship. Heitjan and Rubin [9] have shown that data may also be neither perfectly present nor entirely missing, instead, it is coarsened by rounding, heaping, censoring or other factors. According to Little and Rubin [14], missing data can be categorized into three categories: (i) Missing completely at random (MCAR), if the probability of missing value in a variable is independent of the variable itself and on any other variable on the set; (ii) Missing at random (MAR), when the probability of a missing value on a sample X is independent of X but follows a pattern throughout the data-set, therefore it can be predicted based on other variables; (iii) Not missing at random (NMAR), when the probability of missing values on X depends solely on X itself. Therefore , MCAR and MAR are recoverable whereas NMAR is, that is, the former can be imputed based on observation of other samples in the data-set and maximization of the likelihood. Handling missing data while keeping data reliability may not be possible by disregarding the missing values or substituting by a best guess value. Given that, many imputation algorithms based on both …	artificial neural network;benchmark (computing);biasing;categorization;censoring (statistics);data mining;expectation–maximization algorithm;futures studies;fuzzy logic;geo-imputation;guess value;k-nearest neighbors algorithm;machine learning;missing data;numerical analysis;replication (computing);rounding;sampling (signal processing);schedule (computer science)	Davi E. N. Frossard;Igor O. Nunes;Renato A. Krohling	2016	CoRR		computer science;machine learning;data mining;imputation;statistics	ML	0.4495402022008621	-32.11864768078168	74099
779fc10e5b846deb1dba284b9559b43a0dc8805a	a fuzzy inference system for credit scoring using boolean consistent fuzzy logic			fuzzy logic;inference engine	Milica Latinovic;Ivana Dragovic;Vesna Bogojevic Arsic;Bratislav Petrovic	2018	Int. J. Comput. Intell. Syst.	10.2991/ijcis.11.1.31	artificial intelligence;machine learning;fuzzy logic;mathematics;inference	Logic	2.77282049003937	-25.751779109177622	74117
0d5c74bac718b52da0e9d65ab3e3cb58e8c6ec02	projection pursuit constructive neural networks based on quality of projected clusters	projection pursuit;high dimensionality;complex data;indexation;high efficiency;neural network	Linear projection pursuit index measuring quality of projected clusters (QPC) is used to discover non-local clusters in high-dimensional multiclass data, reduction of dimensionality, feature selection, visualization of data and classification. Constructive neural networks that optimize the QPC index are able to discover simplest models of complex data, solving problems that standard networks based on error minimization are not able to handle. Tests on problems with complex Boolean logic and a few real world datasets show high efficiency of this	artificial neural network;boolean algebra;feature selection;logical connective;quantum point contact	Marek Grochowski;Wlodzislaw Duch	2008		10.1007/978-3-540-87559-8_78	projection pursuit;computer science;artificial intelligence;machine learning;data mining;artificial neural network;complex data type	ML	3.258179543759167	-30.112212459547276	74629
e6bc44f7f9aa82f5ed62e23f325fb0c22fa87951	network anomalous attack detection based on clustering and classifier	data mining;feature space;network connectivity;network traffic	"""A new approach to detect anomalous behaviors in network traffic is presented. The network connection records were mapped into different feature spaces according to their protocols and services. Then performed clustering to group training data points into clusters, from which some clusters were selected as normal and known-attack profile. For those training data excluded from the profile, we used them to build a specific classifier. The classifier has two distinct characteristics: one is that it regards each data point in the feature space with the limited influence scope, which is served as the decisive bounds of the classifier, and the other is that it has the """"default"""" label to recognize those novel attacks. The new method was tested on the KDD Cup 1999 data. Experimental results show that it is superior to other data mining based approaches in detection performance, especially in detection of PROBE and U2R attacks."""		Hongyu Yang;Feng Xie;Yi Lu	2006		10.1007/978-3-540-74377-4_70	feature vector;computer science;machine learning;pattern recognition;data mining	AI	5.452769353175386	-37.44157816994365	74744
0739b7e39f3a52187a22d101590dc989a0e33ae6	comparison of hybrid intelligent systems, neural networks and interval type-2 fuzzy logic for time series prediction	fuzzy set;time series fuzzy logic fuzzy set theory fuzzy systems neural nets;neural nets;hybrid intelligent systems neural networks fuzzy logic uncertainty fuzzy sets fuzzy systems chaos intelligent networks signal processing algorithms economic forecasting;chaotic time series;time series;hybrid intelligent system;fuzzy set theory;fuzzy logic;incomplete information;real world application;intelligent system;type 2 fuzzy set;type 1 fuzzy set hybrid intelligent system neural network interval type 2 fuzzy logic time series prediction;interval type 2 fuzzy logic;type 1 fuzzy set;fuzzy systems;fuzzy system;time series prediction;neural network	Uncertainty is an inherent part of intelligent systems used in real-world applications. The use of new methods for handling incomplete information is of fundamental importance. Type-1 fuzzy sets used in conventional fuzzy systems cannot fully handle the uncertainties present in intelligent systems. Type-2 fuzzy sets can handle such uncertainties in a better way because they provide us with a more complete model of real-world uncertainty. Experimental results are also presented for forecasting chaotic time series in which interval type-2 fuzzy logic outperforms some hybrid intelligent approaches. Neural networks provide a comparable result with type-2 fuzzy systems.	artificial intelligence;fuzzy control system;fuzzy logic;fuzzy set;neural networks;neuro-fuzzy;simulation;time series;type-2 fuzzy sets and systems	Oscar Castillo;Patricia Melin	2007	2007 International Joint Conference on Neural Networks	10.1109/IJCNN.2007.4371453	fuzzy electronics;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;time series;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system;intelligent control	AI	4.71779364893653	-26.891260630249718	75024
93799a78155c10bc5045c29f668194fb0b04ddbc	advanced artificial neural networks		Artificial neural networks (ANNs) have been extensively applied to a wide range of disciplines, such as system identification and control, decision making, pattern recognition, medical diagnosis, finance, data mining, visualization, and others. With advances in computing and networking technologies, more complicated forms of ANNs are expected to emerge, requiring the design of advanced learning algorithms. This Special Issue is intended to provide technical details of the construction and training of advanced ANNs.	algorithm;artificial neural network;data mining;machine learning;neural networks;pattern recognition;system identification	Tin-Chih Toly Chen;Cheng-Li Liu;Hong-Dar Lin	2018	Algorithms	10.3390/a11070102	physical neural network	ML	6.803361773602902	-24.03201656366423	75064
5dc56f2c70b31c88575b307f9d0132e279ae011e	bagging of complementary neural networks with double dynamic weight averaging	diversity;feed forward;ensemble technique;neural networks;backpropagation neural network;weighted averaging;uncertainty;bagging;feedforward backpropagation neural networks;training;regression analysis backpropagation feedforward neural nets;distributed computing;bridges;double dynamic weight averaging;natural languages;backpropagation;back propagation neural network;software engineering;accuracy;ensemble;artificial neural networks;bagging neural networks natural languages buildings hybrid power systems ontologies software engineering artificial intelligence distributed computing bridges;hybrid power systems;regression problems;artificial intelligence;ontologies;regression analysis;feedforward neural nets;uci data set benchmarking;bagging technique;buildings;uci data set benchmarking complementary neural networks double dynamic weight averaging ensemble technique regression problems bagging technique feedforward backpropagation neural networks;complementary neural networks;concrete;neural network;hardware;ensemble backpropagation neural network complementary neural networks diversity bagging	Ensemble technique has been widely applied in regression problems. This paper proposes a novel approach of the ensemble of Complementary Neural Network (CMTNN) using double dynamic weight averaging. In order to enhance the diversity in the ensemble, different training datasets created based on bagging technique are applied to an ensemble of pairs of feed-forward back-propagation neural networks created to predict the level of truth and falsity values. In order to obtain more accuracy, uncertainties in the prediction of truth and falsity values are used to weight the prediction results in two steps. In the first step, the weight is used to average the truth and the falsity values whereas the weight in the second step is used to calculate the final regression output. The proposed approach has been tested with benchmarking UCI data sets. The results derived from our technique improve the prediction performance while compared to the traditional ensemble of neural networks which is predicted based on only the truth values. Furthermore, the obtained results from our novel approach outperform the results from the existing ensemble of complementary neural network.	backpropagation;neural networks;neural ensemble;object composition;software propagation	Sathit Nakkrasae;Pawalai Kraipeerapun;Somkid Amornsamankul;Lance Chun Che Fung	2010	2010 11th ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing	10.1109/SNPD.2010.34	ensembl;bootstrap aggregating;concrete;uncertainty;computer science;ontology;artificial intelligence;backpropagation;machine learning;data mining;accuracy and precision;natural language;feed forward;artificial neural network;regression analysis	AI	6.767668146911957	-26.607843628458415	75204
0ce429d47e868c7b04822cbfdc3a55f45ae1712a	application on intersection classification algorithm based on clustering analysis		Applying the clustering classification algorithm in data analysis to classify intersections in many modern cities, and then managing or allocating traffic management solutions for the intersections in classification is a feasible method to improve intersection efficiency. First of all, this article determines the number of classification under clustering analysis with the help of the multiple evaluation index method and the method of square error sum within the group. Then, by using the partition clustering algorithm (Partitioning Around Medoid, PAM), the 107 intersections in the Suzhou Industrial Park were classified based on the intersection traffic, and this article measures the similarity of intersections by the dissimilarity function. In the end, 107 intersections in the Suzhou Industrial Park were divided into three groups, making each intersection highly similar to the intersections within the ethnic group, and there were significant differences between the intersections outside the ethnic group. The center intersection of each ethnic group makes a good representation of the intersections included in this group, which embodies the characteristics of intersections in the ethnic group. And center intersections provide a good basis for designing reasonable traffic management plans for all types of intersections in the future.	algorithm;cluster analysis;medoid	Dong Chen;Hao Wang	2018	2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)	10.1109/COMPSAC.2018.10245	support vector machine;partition (number theory);cluster analysis;medoid;industrial park;algorithm;statistical classification;computer science	Visualization	0.2330158675900499	-29.813816113911265	75340
fb4e90fcce1800a1c3d342d9f02d00bc26968377	an efficient random decision tree algorithm for case-based reasoning systems		We present an efficient random decision tree algorithm for case-based reasoning systems. We combine this algorithm with a simple similarity measure based on domain knowledge to create a stronger hybrid algorithm. This combination is based on our general approach for combining lazy and eager learning methods. We evaluate the resulting algorithms on a case base of patient records in a palliative care domain. Our hybrid algorithm consistently produces a lower average error than the base algorithms.	case-based reasoning;constrained shortest path first;decision tree model;eager learning;hybrid algorithm;lazy evaluation;list of algorithms;remote digital terminal;similarity measure	Tor Gunnar Houeland	2011			computer science;machine learning;incremental decision tree;data mining;id3 algorithm;algorithm;population-based incremental learning	AI	5.188593245991715	-32.37102038730995	75343
d95e6085f730ab217d28a50520ff4615603c06d8	feature discovery in classification problems	trigonometric function;raisonnement base sur cas;razonamiento fundado sobre caso;teleenseignement;decision tree;analisis datos;multilayer perceptrons;representation fonction;base connaissance;hombre;intelligence artificielle;multilayer perceptron;arbol decision;algoritmo genetico;data mining;classification;polynome trigonometrique;proof of concept;selection automatique;perceptron multicouche;data analysis;a priori knowledge;learning methods;internet;fonction trigonometrique;fouille donnee;seleccion automatica;function representation;decouverte connaissance;funcion trigonometrica;human;representacion funcion;algorithme genetique;polinomio trigonometrico;artificial intelligence;descubrimiento conocimiento;base conocimiento;analyse donnee;genetic algorithm;teleensenanza;inteligencia artificial;case based reasoning;remote teaching;busca dato;arbre decision;clasificacion;trigonometric polynomial;automatic selection;homme;knowledge base;knowledge discovery	In most problems of Knowledge Discovery the human analyst previously constructs a new set of features, derived from the initial problem input attributes, based on a priori knowledge of the problem structure. These different features are constructed from different transformations which must be selected by the analyst. This paper provides a first step towards a methodology that allows the search for near-optimal representations in classification problems by allowing the automatic selection and composition of feature transformations from an initial set of basis functions. In many cases, the original representation for the problem data is not the most appropriate, and the search for a new representation space that is closer to the structure of the problem to be solved is critical for the successful solution of the problem. On the other hand, once this optimal representation is found, most of the problems may be solved by a linear classification method. As a proof of concept we present two classification problems where the class distributions have a very intricate overlap on the space of original attributes. For these problems, the proposed methodology is able to construct representations based on function compositions from the trigonometric and polynomial bases that provide a solution where some of the classical learning methods, e.g. multilayer perceptrons and decision trees, fail. The methodology consists of a discrete search within the space of compositions of the basis functions and a linear mapping performed by a Fisher discriminant. We play special emphasis on the first part. Finding the optimal composition of basis functions is a difficult problem because of its nongradient nature and the large number of possible combinations. We rely on the global search capabilities of a genetic algorithm to scan the space of function compositions.	basis function;decision tree;genetic algorithm;linear classifier;linear discriminant analysis;machine learning;multilayer perceptron;polynomial	Manuel del Valle;Beatriz Sánchez;Luis F. Lago-Fernández;Fernando J. Corbacho	2005		10.1007/11552253_44	case-based reasoning;knowledge base;the internet;a priori and a posteriori;genetic algorithm;trigonometric functions;biological classification;computer science;artificial intelligence;machine learning;decision tree;mathematics;function representation;multilayer perceptron;data analysis;proof of concept;algorithm;trigonometric polynomial	ML	8.651678751317784	-31.15906219822829	75402
461e49943caa9292ecff1b40d8348569a96cb660	finding associations in composite data sets: the cfarm algorithm	composite attributes;association rules;association rule mining;fuzzy association rules;quantitative attributes	A fuzzy association rule mining mechanism (CFARM), directed at identifying patterns in datasets comprised of composite attributes, is described. Composite attributes are defined as attributes that can take simultaneously two or more values that subscribe to a common schema. The objective is to generate fuzzy association rules using “properties” associated with these composite attributes. The exemplar application is the analysis of the nutrients contained in items found in grocery data sets. The paper commences with a review of the back ground and related work, and a formal definition of the CFARM concepts. The CFARM algorithm is then fully described and evaluated using both real and synthetic data sets.	algorithm;association rule learning;database schema;fuzzy logic;fuzzy measure theory;fuzzy set;mined;synthetic data	M. Sulaiman Khan;Maybin K. Muyeba;Frans Coenen;David Reid;Hissam Tawfik	2011	IJDWM	10.4018/jdwm.2011070101	association rule learning;computer science;machine learning;pattern recognition;data mining	DB	-2.7347653137266144	-30.067118699987564	75851
f84adb4cb0ab4f850a5e8446aa602db119a66e10	inductive learning for risk classification	financial data processing;commercial loans inductive learning risk classification credit risk analysis business loan evaluation bond rating bankruptcy prediction marble knowledge based decision support system;decision support system;decision support systems;inductive learning;financial risk;bankruptcy prediction;decision support systems pattern recognition bonding displays feature extraction business communication decision trees intelligent systems knowledge acquisition engines;knowledge based systems decision support systems financial data processing;knowledge based systems;decision rule;credit risk;knowledge base	The authors discuss the application of inductive learning to credit risk analysis. Three risk classification problems are addressed: business loan evaluation, bond rating, and bankruptcy prediction. The use of a system called Marble, a knowledge-based decision-support system that uses approximately 80 decision rules to evaluate commercial loans, is discussed. In particular, an aspect of Marble that uses inductive learning to classify financial risks is described, and the effectiveness of the technique is discussed.<<ETX>>	decision support system;inductive reasoning;knowledge-based systems;marble;risk management;statistical classification	Michael J. Shaw;James A. Gentry	1990	IEEE Expert	10.1109/64.50856	decision support system;intelligent decision support system;financial risk;decision engineering;credit risk;computer science;knowledge management;artificial intelligence;data mining;decision rule;business decision mapping	ML	-0.8142460676982406	-30.16542853250898	75898
afdb8a59ffc8786938245da99693e2de0bb32b68	grc and ann based fault diagnosis method of distribution network	granular computing;power engineering computing backpropagation fault tolerance granular computing heuristic programming neural nets power distribution faults;fault tolerant;neural nets;distributed networks;power distribution faults;heuristic programming;backpropagation;fault diagnosis granular computing relative granularity bp neural network distribution network;parameter identification;fault diagnosis circuit breakers computational modeling artificial neural networks circuit faults educational institutions information systems;power engineering computing;fault tolerance;neutral network;fault diagnosis;intelligent hybrid diagnosis system grc ann based fault diagnosis method distribution network granular computing theory fault tolerance learning capabilities heuristic information reduction algorithm bp neural network theory;neural network	In order to improve the accuracy and efficiency of fault diagnosis in distribution network, this paper puts forward an intelligent hybrid diagnosis system, which combines granular computing theory with neural network theory, to make the best use of rules reduction of granular computing and fault-tolerance learning capabilities of neutral network. In this paper, the concepts of relative granularity and significance of attributes based on binary granular computing are proposed to select reasonable input variables to form the most simplified rules, and are used as the heuristic information of reduction algorithm. The most simplified rule sets are called to make modeling and parameter identification with BP neural network, and then learning training is done by training samples. Compared with the result of fault diagnosis for one distribution network, it shows that the method can reduce the learning training time, improve accuracy of diagnosis and have better fault-tolerance.	algorithm;artificial neural network;decision table;fault tolerance;granular computing;heuristic;information system;interference (communication);network theory	Xiaoming Han	2011	Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology	10.1109/EMEIT.2011.6022856	fault tolerance;computer science;artificial intelligence;theoretical computer science;machine learning;artificial neural network	AI	7.3689294381965	-26.922683849376114	75900
ab72818b0bbbd07c4567ddf9189766448112d50b	anafs computation of h-component of the earth's magnetic field	magnetic field;anafs;magnetosphere;non additive fuzzy systems;magnetic storms	This paper presents a computational technique for modeling the Earth’s magnetic field using the framework of Adaptive Non-Additive Fuzzy System (ANAFS). The defuzzified output constructed from both the premise and consequent parts of the GFM rules takes the form of Choquet integral. Premise and consequent parameters of non-additive fuzzy model are then updated for each online data based on the estimation error to make the adaptive system. The resulting model is applied on the real data, i.e. H-component of the magnetic field obtained from the magnetometer and the training and prediction results are found to be better and can be used for modeling such a complex system whose mathematical or physical modeling is very difficult to obtain. Thus, this work makes an important contribution to the field of fuzzy modeling of real life complex systems.	adaptive system;additive model;complex system;complex systems;computation;defuzzification;expectation–maximization algorithm;fuzzy concept;fuzzy control system;google map maker;internet backbone;planetary scanner;real life;utility functions on indivisible goods	Nishchal K. Verma;Madasu Hanmandlu	2008	International Journal of Computational Intelligence and Applications	10.1142/S1469026808002132	magnetosphere;simulation;magnetic field;artificial intelligence;fuzzy number;machine learning	Robotics	5.278296013125295	-25.314041743048545	75977
4fb34978cbe28d8437d7e395a58179cfd1fcaf3d	targeted action rule discovery	action rules;data mining;recommendation;decision trees;decision tree classifiers;recommender systems;targeted action rule discovery	The task of learning action rules aims to provide recommendations to analysts seeking to achieve a specific change. An action rule is constructed as a series of changes, or actions, which can be made to the flexible characteristics of a given object that ultimately triggers the desired change. Existing action rule discovery methods utilize a generate-and-test approach in which candidate action rules are generated and those that satisfy the user-defined thresholds are returned. A shortcoming of this operational model is there is no guarantee all objects are covered by the generated action rules. In this paper, we define a new methodology referred to as Targeted Action Rule Discovery (TARD). This methodology represents an object driven approach in which an action rule is explicitly discovered per target object. A TARD method is proposed that effectively discovers action rules through the iterative construction of multiple decision trees. Experiments show the proposed method is able to provide higher quality rules than the well-known Association Action Rule (AAR) method.	action potential;association for automated reasoning;association rule learning;decision tree;experiment;iterative method;whole earth 'lectronic link	Tom Johnsten;Samy Alihamad;Ashwin Kannalath;Ryan G. Benton	2013	2013 12th International Conference on Machine Learning and Applications	10.1109/ICMLA.2013.71	decision model;computer science;artificial intelligence;machine learning;decision tree;data mining;decision rule;recommender system	ML	-4.529418275161379	-29.979158039187897	75999
f7041f1685d13bdc290cfc4dbd004c90e4dc8239	software effort estimation as a classification problem	software effort estimation	Software cost estimation is still an open challenge. Many researchers have proposed various methods that usually focus on point estimates. Software cost estimation, up to now, has been treated as a regression problem. However, in order to prevent over/under estimates, it is more practical to predict the interval of estimations instead of the exact values. In this paper, we propose an approach that converts cost estimation into a classification problem and classifies new software projects in one of the effort classes each corresponding to an effort interval. Our approach integrates cluster analysis with classification methods. Cluster analysis is used to determine effort intervals while different classification algorithms are used to find the corresponding effort classes. The proposed approach is applied to seven public data sets. Our experimental results show that hit rates obtained for effort estimation are around 90%-100%s. For point estimation, the results are also comparable to those in the literature.	algorithm;cluster analysis;cost estimation in software engineering;information privacy;software development effort estimation	Ayse Bakir;Burak Turhan;Ayse Basar Bener	2008			verification and validation;software sizing;computer science;software reliability testing;analysis effort method;software construction;software metric	SE	3.5622167206986184	-33.620620174588716	76244
77e9265a577705390922219fbd4b9f8cf5a8924c	fcm clustering algorithm for t-s fuzzy model identification	time series fuzzy logic least squares approximations;nonlinear modeling;cluster algorithm;fuzzy c mean;least squares approximations;mackey glass chaos time series fcm clustering algorithm t s fuzzy model identification fuzzy c mean clustering algorithm nonlinear modeling uniformed premise structure least square algorithm box jenkins gas furnace data;least square algorithm;t s fuzzy model;predictive models clustering algorithms data models solid modeling mathematical model adaptation model fuzzy sets;fuzzy identification;time series;fuzzy sets;fuzzy logic;uniformed premise structure;least square fuzzy c mean t s fuzzy model fuzzy identification;adaptation model;mackey glass chaos time series;solid modeling;t s fuzzy model identification;fcm clustering algorithm;least square;fuzzy c mean clustering algorithm;mathematical model;clustering algorithms;predictive models;fuzzy c means clustering;data models;nonlinear model;box jenkins gas furnace data	An approach for building T-S fuzzy model is proposed based on fuzzy c-mean clustering algorithm on the basis of nonlinear modeling experience. An alternative T-S fuzzy model is adapted, which has the uniformed premise structure, the premise parameter is decided by fuzzy c-mean clustering algorithm and the consequence parameters is calculated by least square algorithm, and the identification precision is enhanced. Finally the effectiveness and practicability of this method is demonstrated by the simulation result of Box-Jenkins gas furnace data and Mackey-Glass chaos time series.	algorithm;cluster analysis;fuzzy cognitive map;jenkins;nonlinear system;simulation;system identification;time series	Pu Han;Jian-Zhong Shi;Dongfeng Wang;Songming Jiao	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5580478	fuzzy logic;data modeling;defuzzification;fuzzy clustering;fuzzy classification;computer science;artificial intelligence;fuzzy number;canopy clustering algorithm;neuro-fuzzy;machine learning;time series;cure data clustering algorithm;mathematical model;data mining;mathematics;predictive modelling;fuzzy set;solid modeling;cluster analysis;least squares;fuzzy set operations	DB	5.570029603887175	-25.739363408695613	76417
cc4591df10d94d72d9e7e5efadb913af0c997a9f	knowledge-based defuzzification	clustering;clustering method;mountain method;defuzzification;knowledge base	Abstract   We suggest a new approach to the problem of defuzzification. This approach is a knowledge-based approach in that it uses knowledge in terms of combinability function to help to more intelligently guide the defuzzification process. This approach makes use of a view of the defuzzification process as a kind of clustering problem and uses the basic idea used in the mountain clustering method.	defuzzification;knowledge-based systems	Ronald R. Yager	1996	Fuzzy Sets and Systems	10.1016/0165-0114(95)00191-3	knowledge base;defuzzification;computer science;artificial intelligence;machine learning;data mining;mathematics;cluster analysis	Logic	2.3688247866953	-27.804054800566703	76429
f8d250c5806c048d9e88adce74d50c8aae77524a	optimal decompositions of matrices with entries from residuated lattices	general type;k object-factor matrix a;residuated lattice;general framework;number k;optimal factor;particular galois connection;matrix product;optimal decomposition;m object-attribute matrix	We describe optimal decompositions of matrices whose entries are elements of a residuated lattice L, such as L = [0, 1]. Such matrices represent relationships between objects and attributes with the entries representing degrees to which attributes represented by columns apply to objects represented by rows. Given such an n×m object-attribute matrix I, we look for a decomposition of I into a product A◦B of an n×k object-factor matrix A and a k×m factor-attribute matrix B with entries from L with the number k of factors as small as possible. We show that formal concepts of I, which play a central role in the Port-Royal approach to logic and which are the fixpoints of particular Galois connections associated to I, are optimal factors for decomposition of I in that they provide us with decompositions with the smallest number of factors. Moreover, we describe transformations between the space of original attributes and the space of factors induced by a decomposition I = A◦B. The paper contains illustrative examples demonstrating the significance of the presented results for factor analysis of relational data. In addition, we present a general framework for a calculus of matrices with entries from residuated lattices in which both the matrix products and decompositions discussed in this paper as well as triangular products and decompositions discussed elsewhere can be regarded as two particular cases of a general type of product and decomposition. We present the results for matrices, i.e. for relations between finite sets in terms of relations, but the arguments behind are valid for relations between infinite sets as well.	approximation algorithm;binary data;column (database);computational complexity theory;experiment;factor analysis;fuzzy set;greedy algorithm;residuated lattice;set theory;t-norm;the matrix	Radim Belohlávek	2012	J. Log. Comput.	10.1093/logcom/exr023	combinatorics;discrete mathematics;mathematics;algebra	AI	-2.9163958888072097	-25.391698105658794	76632
606e3d4bd57cec350145c6b518676c3313f16071	design and implementation of a data mining system for malware detection	data mining;streambased classification;stream based novel class detection;machine learning;malware detection	This paper describes the design and implementation of a data mining system called SNODMAL Stream based novel class detection for malware for malware detection. SNODMAL extends our data mining system called SNOD Stream-based Novel Class Detection for detecting malware. SNOD is a powerful system as it can detect novel classes. We also describe the design of SNODMAL++ which is an extended version of SNODMAL.	data mining;malware	Bhavani M. Thuraisingham;Tahseen Al-Khateeb;Latifur Khan;Mehedy Masud;Kevin W. Hamlen;Vaibhav Khadilkar;Satyen Abrol	2012	J. Integrated Design & Process Science	10.3233/jid-2012-0016	computer science;data mining;data stream mining;world wide web;computer security	ML	6.78055577092425	-37.12967657106806	77049
25dc9b167554b9f06a82dc542ea647147d2cd510	forecasting electricity market price spikes based on bayesian expert with support vector machines	bayes estimation;extraction information;modelizacion;forecasting;prevision;bayesian classification;analisis estadistico;bayesian statistics;determinacion precio;reseau electrique;analisis datos;information extraction;electrical network;numerical method;electricity prices;red electrica;analisis decision;prior distribution;probabilistic approach;electricity market;data mining;ley a priori;prix marche;decision analysis;modelisation;classification a vaste marge;data analysis;estimacion bayes;market price;price determination;posterior distribution;statistical analysis;fouille donnee;determination prix;enfoque probabilista;approche probabiliste;analyse statistique;ley a posteriori;analyse donnee;precio de mercado;support vector machine;maquina ejemplo soporte;vector support machine;modeling;loi a posteriori;busca dato;analyse decision;extraccion informacion;loi a priori;estimation bayes	This paper present a hybrid numeric method that integrates a Bayesian statistical method for electricity price spikes classification determination and a Bayesian expert (BE) is described for data mining with experience decision analysis approach. The combination of experience knowledge and support vector machine (SVM) modeling with a Bayesian classification, which can classify the spikes and normal electricity prices, are developed. Bayesian prior distribution and posterior distribution knowledge are used to evaluate the performance of parameters in the SVM models. Electricity prices of one regional electricity market (REM) in China are used to test the proposed method, experimental results are shown.		Wei Wu;Jianzhong Zhou;Li Mo;Chengjun Zhu	2006		10.1007/11811305_23	electrical network;support vector machine;econometrics;naive bayes classifier;systems modeling;prior probability;market price;variable-order bayesian network;electricity market;decision analysis;forecasting;numerical analysis;computer science;data mining;posterior probability;bayesian statistics;data analysis;information extraction;statistics	AI	9.39888443015733	-24.910366444721646	77158
f3592bdd47f56a940dc2721c2fe7e246f4fa0d1e	random aggregated and bagged ensembles of svms: an empirical bias?variance analysis	extraction information;traitement signal;methode empirique;bootstrap;modele agrege;ensemble method;analisis datos;loi probabilite;information extraction;ley probabilidad;variance component;metodo empirico;empirical method;variance analysis;error sistematico;modelo agregado;probabilistic approach;small samples;data mining;data analysis;bias;fouille donnee;analisis variancia;enfoque probabilista;approche probabiliste;signal processing;probability distribution;machine exemple support;aggregate model;analyse donnee;support vector machine;maquina ejemplo soporte;vector support machine;procesamiento senal;variance reduction;busca dato;extraccion informacion;quantitative evaluation;analyse variance;large data;erreur systematique;variance;variancia	Bagging can be interpreted as an approximation of random aggregating, an ideal ensemble method by which base learners are trained using data sets randomly drawn according to an unknown probability distribution. An approximate realization of random aggregating can be obtained through subsampled bagging, when large training sets are available. In this paper we perform an experimental bias-variance analysis of bagged and random aggregated ensembles of Support Vector Machines, in order to quantitatively evaluate their theoretical variance reduction properties. Experimental results with small samples show that random aggregating, implemented through subsampled bagging, reduces the variance component of the error by about 90%, while bagging, as expected, achieves a lower reduction. Bias-variance analysis explains also why ensemble methods based on subsampling techniques can be successfully applied to large data mining problems.		Giorgio Valentini	2004		10.1007/978-3-540-25966-4_26	probability distribution;random forest;support vector machine;econometrics;analysis of variance;computer science;machine learning;bias;signal processing;mathematics;variance;data analysis;empirical research;information extraction;statistics;variance reduction	NLP	9.543554892668878	-34.00254952997415	77232
b5cbe51811a0ee3c67300363d143acb0a4c65b46	from data to global generalized knowledge	discovery;association rules;data mining;algorithm;multiple level mining;relational databases;attribute oriented induction;generalized knowledge	The attribute-oriented induction (AOI) is a useful data mining method that extracts generalized knowledge from relational data and user's background knowledge. The method uses two thresholds, the relation threshold and attribute threshold, to guide the generalization process, and output generalized knowledge, a set of generalized tuples which describes the major characteristics of the target relation. Although AOI has been widely used in various applications, a potential weakness of this method is that it only provides a snapshot of the generalized knowledge, not a global picture. When thresholds are different, we would obtain different sets of generalized tuples, which also describe the major characteristics of the target relation. If a user wants to ascertain a global picture of induction, he or she must try different thresholds repeatedly. That is time-consuming and tedious. In this study, we propose a global AOI (GAOI) method, which employs the multiple-level mining technique with multiple minimum supports to generate all interesting generalized knowledge at one time. Experiment results on real-life dataset show that the proposed method is effective in finding global generalized knowledge.		Yen-Liang Chen;Yu-Ying Wu;Ray-I Chang	2012	Decision Support Systems	10.1016/j.dss.2011.08.005	association rule learning;relational database;computer science;data science;machine learning;data mining;database;knowledge extraction	ECom	-3.72480957489602	-30.211921089437027	77233
8f688845d36d09b8c33dbd613a0184c20511ea08	sets of contrasting rules: a supervised descriptive rule induction pattern for identification of trigger factors	itemsets;electronic mail;data mining;force;redundancy;algorithm design and analysis	"""Data mining, through association rules mining, is one of the best known approaches for patterns identification. However, it results most of the time in a huge set of patterns (rules), so their exploitation is not easy and often requires expert analysis. In this paper we describe a new pattern """"set of contrasting rules"""" which, contrary to most state-of-the-art patterns, has the characteristic of being made up of a set of rules. It has also the advantage of not only identifying a reduced set of rules, but also structuring it into sets. One main originality of this pattern is that it allows to automatically identify trigger factors: factors that can bring some event state changes. In this work we show that the proposed pattern methodologically belongs to the supervised descriptive rules induction paradigm. We also show through the experiments on a real dataset of census data that """"set of contrasting rules"""" can be considered as a way to filter the huge amount of association rules and can be used to identify trigger factors."""	association rule learning;data mining;experiment;programming paradigm;rule induction	Marharyta Aleksandrova;Armelle Brun;Oleg Chertov;Anne Boyer	2016	2016 IEEE 28th International Conference on Tools with Artificial Intelligence (ICTAI)	10.1109/ICTAI.2016.0072	algorithm design;computer science;machine learning;pattern recognition;data mining;redundancy;force;specification pattern	DB	-1.344926584017224	-34.68918286179407	77271
cae1baf589a498fc7be9fbf14a7dcb57db8b8d1a	performance comparison of similarity measurements for database correlation localization method	software tool;database correlation method;fingerprinting localization;user adaptation;performance comparison;euclidean distance;computational complexity;similarity measure;simulation model;indoor positioning;similarity measurements	User positioning is a very important feature in user adaptive system. The user position can be estimated by various positioning methods. This paper investigates an impact of similarity measurements on localization error in de- terministic database correlation method. It is also called fingerprinting. Main idea is to compare widely used Euclidean distance with other similarity meas- urements. Seven different similarity measurements are implemented to simula- tion model created in Matlab software tool. Computation complexity of each similarity measurement is investigated and impact of similarity measurements on localization error in normal and extreme conditions is shown.		Juraj Machaj;Peter Brida	2011		10.1007/978-3-642-20042-7_46	computer science;theoretical computer science;machine learning;simulation modeling;data mining;euclidean distance;computational complexity theory	Vision	3.521519641704964	-33.18246085920078	77444
40752651a39b605cc74115e9ba940ed21cff4311	fuzzy min-max neural networks. i. classification	pattern classification fuzzy min max neural networks fuzzy set theory pattern recognition supervised learning neural network classifier fuzzy set hyperboxes membership function fuzzy min max learning algorithm;learning algorithm;fuzzy set;learning;supervised learning;neural nets;algoritmo borroso;ensemble flou;neural network classifier;fuzzy set theory;aprendizaje;learning systems;minimax techniques;apprentissage;fuzzy neural networks neural networks fuzzy sets pattern classification fuzzy systems supervised learning aggregates sonar applications radar applications control systems;fuzzy algorithm;membership function;pattern classification;pattern recognition;algorithme flou;pattern recognition fuzzy set theory learning systems minimax techniques neural nets;conjunto borroso;reseau neuronal;red neuronal;neural network;classification forme	A supervised learning neural network classifier that utilizes fuzzy sets as pattern classes is described. Each fuzzy set is an aggregate (union) of fuzzy set hyperboxes. A fuzzy set hyperbox is an n-dimensional box defined by a min point and a max point with a corresponding membership function. The min-max points are determined using the fuzzy min-max learning algorithm, an expansion-contraction process that can learn nonlinear class boundaries in a single pass through the data and provides the ability to incorporate new and refine existing classes without retraining. The use of a fuzzy set approach to pattern classification inherently provides a degree of membership information that is extremely useful in higher-level decision making. The relationship between fuzzy sets and pattern classification is described. The fuzzy min-max classifier neural network implementation is explained, the learning and recall algorithms are outlined, and several examples of operation demonstrate the strong qualities of this new neural network classifier.		Patrick K. Simpson	1992	IEEE transactions on neural networks	10.1109/72.159066	fuzzy logic;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;mathematics;information fuzzy networks;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network	ML	7.145413591817751	-28.547141178972794	77703
8dadccf8bf6222d58370911f5b7cb772d55149d3	a fuzzy min-max neural network classifier with compensatory neuron architecture	neural network;fuzzy set;fuzzy set theory;pattern recognition	"""This work proposes a supervised learning neural network classifier with compensatory neuron architecture. The proposed """"fuzzy min-max neural network classifier with compensatory neurons"""" (FMCN) extends the principle of minimal disturbance. The new architecture consists of compensating neurons that are trained to handle the hyperbox overlap and containment. The FMCN is capable of learning data on-line, in a single pass through, with reduced classification and gradation error. One of the good features of FMCN is that its performance is almost independent of the expansion coefficient i.e. maximum hyperbox size. The paper demonstrates the performance of FMCN with several examples."""	artificial neural network;biological neural networks;coefficient;maxima and minima;neuron;online and offline;supervised learning	Abhijeet V. Nandedkar;Prabir Kumar Biswas	2004	Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004.	10.1109/ICPR.2004.1333832	computer science;artificial intelligence;machine learning;pattern recognition;fuzzy set;artificial neural network	ML	7.281967380641087	-28.514831634517474	77707
d07903d3b8c380e1b5165eff27bc462975cace87	the search of causal orderings: a short cut for learning belief networks	reseau croyance;learning algorithm;bayes net;relacion orden;base connaissance;ordering;intelligence artificielle;algorithme apprentissage;simulated annealing;reseau bayes;relation ordre;recuit simule;causalite;artificial intelligence;base conocimiento;recocido simulado;inteligencia artificial;algoritmo aprendizaje;belief net;causality;causalidad;knowledge base	Although we can build a belief network starting from any ordering of its variables, its structure depends heavily on the ordering being selected: the topology of the network, and therefore the number of conditional independence relationships that may be explicitly represented can vary greatly from one ordering to another. We develop an algorithm for learning belief networks composed of two main subprocesses: (a) an algorithm that estimates a causal ordering and (b) an algorithm for learning a belief network given the previous ordering, each one working over different search spaces, the ordering and dag space respectively.	algorithm;bayesian network;causal filter;directed acyclic graph;emoticon	Silvia Acid;Luis M. de Campos;Juan F. Huete	2001		10.1007/3-540-44652-4_20	knowledge base;causality;simulated annealing;order theory;computer science;artificial intelligence;machine learning;bayesian network;mathematics;algorithm	AI	8.535341272895693	-31.74222386851198	77846
4232e0b45d6eb978147507e2fb1d3639b8375e01	association rules mining with relative weighted support	q science general;least;weighted association rules;association rule mining;association rule;knowledge discovery	Mining weighted association rules are very important in a domain of knowledge discovery. Most of traditional association rules are focused on binary relationships rather than the mixture binary-weight relationships of items. As a result, the significance of weight of each item in a transaction is just ignored completely. Until this instance, only few studies are dedicated for weighted schemes as compared to existing binary association rules. Here, we propose a novel weighted association rules scheme called Relative Weighted Support (RWS). The result reveals that RWS can easily discover the significance of weighted association rules and surprisingly all of them are extracted from the least items.	association rule learning	Zailani Abdullah;Mustafa Mat Deris	2009		10.1145/1806338.1806433	association rule learning;machine learning;pattern recognition;data mining;mathematics	DB	-1.6895216031913243	-33.83403021381412	77902
b97cf6cab2039c702cdca72af4119f682795c2df	unity and diversity of fuzziness-from a probability viewpoint	mathematics;yarn;probability;information systems;fuzziness;history;uncertainty representation fuzziness probability fuzzy set theory;fuzzy control;measurement uncertainty;fuzzy set theory;probability statistics fuzzy set theory measurement uncertainty history mathematics information systems possibility theory fuzzy control;statistics;uncertainty representation;possibility theory;fuzzy set theory probability;probability and statistics	Like the fields of probability and statistics, fuzzy set theory is characterized by a variety of viewpoints. Adherents of fuzzy methods, however, consistently maintain that probability is not necessarily the optimal representation of uncertainty. We rebut this view. >	unity	Michael Laviolette;John W. Seaman	1994	IEEE Trans. Fuzzy Systems	10.1109/91.273123	probability and statistics;possibility theory;discrete mathematics;conditional probability;type-2 fuzzy sets and systems;machine learning;probability;mathematics;fuzzy set;information system;fuzzy control system;statistics;measurement uncertainty	Embedded	-1.40763524404589	-25.22850361351279	78516
eb883c0dfdd1740e2a3c7d76f6cedd075a4e3cea	recognizing images from ica filters and neural network ensembles with rule extraction	neural network ensemble;rule extraction;classification;filter;prediction accuracy;filtre;neural network model;reseau neuronal;filtro;clasificacion;red neuronal;neural network	This work presents ensembles of neural network models that learn to discriminate images from different categorical scenes. The basic idea was to use ICA filter energies and to train neural network ensem- bles. The presented results improved the predictive accuracy of previ- ously published work on the second classification problem. Finally, rules generated from ensembles in the less complex classification task showed that a few filters are sufficient to reach a good recognition rate, whereas many more filters are represented in the rule antecedents of the most difficult classification problem.	artificial neural network;independent computing architecture;rule induction	Guido Bologna;Christian Pellegrini	2003		10.1007/3-540-44869-1_69	biological classification;filter;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;artificial neural network	ML	9.880772980428404	-32.417444636118624	78664
8e4bbb7b3c40f905defb847a141cc344d6a07d36	feature selection based on relative attribute dependency: an experimental study	modelizacion;utilisation information;uso informacion;information use;computer model;rough set theory;heuristic method;metodo heuristico;langage java;intelligence artificielle;classification;modelisation;attribute reduction;machine learning;theorie ensemble approximatif;table decision;tabla decision;artificial intelligence;lenguaje java;feature selection;machine learning and data mining;data reduction;methode heuristique;inteligencia artificial;information entropy;algoritmo optimo;rough set;algorithme optimal;optimal algorithm;modeling;data classification;clasificacion;decision table;java language	Most existing rough set-based feature selection algorithms suffer from intensive computation of either discernibility functions or positive regions to find attribute reduct. In this paper, we develop a new computation model based on relative attribute dependency defined as the proportion of the projection of the decision table on a condition attributes subset to the projection of the decision table on the union of the condition attributes subset and the decision attributes set. To find an optimal reduct, we use information entropy conveyed by the attributes as the heuristic. A novel algorithm to find optimal reducts of condition attributes based on the relative attribute dependency is implemented using Java, and is experimented with 10 data sets from UCI Machine Learning Repository. We conduct the comparison of data classification using C4.5 with the original data sets and their reducts. The experiment results demonstrate the usefulness of our algorithm.	actionscript;apply;c4.5 algorithm;computational model;decision table;entropy (information theory);feature selection;heuristic;java;level of measurement;machine learning;model of computation;numerical analysis;programming language;rough set;stepwise regression	Jianchao Han;Ricardo Sanchez;Xiaohua Hu	2005		10.1007/11548669_23	variable and attribute;rough set;computer science;artificial intelligence;machine learning;pattern recognition;data mining;database;feature selection;algorithm	AI	7.942330577616497	-32.88673823974483	79015
55b374bc48275b7738d2967a3638bbbf9c5fc492	knowledge-base reconstruction	knowledge base	The aim of this paper is the reconstruction of a knowledge base from observational data. The method is concerned with the simplest but current case of relations binary relations. The knowledge base is then represented by a tree-type oriented graph. Both the structure and the node type are identified, the structure by means of mean mutual information and the node type according to the channel matrix. The relative entropy is used to determine the edges of orientation.	knowledge base;kullback–leibler divergence;mutual information;orientation (graph theory)	Petr Vysoký	1990	Knowl.-Based Syst.	10.1016/0950-7051(90)90034-F	knowledge base;computer science;artificial intelligence;machine learning;data mining	ML	-3.3582819915462956	-24.841481673505413	79106
debc5c304f003f7419ebaa72dfcb856a8d3939ee	the x-mu approach: fuzzy quantities, fuzzy arithmetic and fuzzy association rules	databases;eradual elements;standards;fuzzy numbers;remuneration;fuzzy intervals;computational intelligence;x μ approach;fuzzy association confidence calculation;association rules;gradual numbers;fuzzy association confidence calculation x mu approach fuzzy quantities fuzzy arithmetic fuzzy association rules fuzzy numbers fuzzy intervals gradual numbers x μ approach visualizing method calculating functions;data mining;fuzzy set theory;fuzzy sets;visualization;fuzzy arithmetic;calculating functions;fuzzy quantities;fuzzy set theory arithmetic data mining;x mu approach;x mu method;arithmetic;visualizing method;fuzzy sets association rules remuneration databases standards computational intelligence visualization;tuzzv association rules;eradual elements x mu method fuzzy quantities fuzzy numbers tuzzv association rules;fuzzy association rules	The use of so-called fuzzy numbers for approximate calculations leads to significant problems, because the underlying mathematical structure is weaker than ordinary arithmetic. Many of these problems arise from the fact that the fuzzy quantities are actually fuzzy intervals. Gradual numbers were recently proposed as a better representation for fuzzy quantities. In this paper, we describe the X-μ approach, a new method of visualizing and calculating functions of fuzzy quantities. In particular, we illustrate the calculation of fuzzy association confidence in cases where membership can be represented by a function or a table of values.	approximation algorithm;association rule learning;fuzzy number;mathematical structure;numerical analysis;sampling (signal processing)	Trevor P. Martin;Ben Azvine	2013	2013 IEEE Symposium on Foundations of Computational Intelligence (FOCI)	10.1109/FOCI.2013.6602451	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;computational intelligence;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Theory	-0.8059278277181069	-25.721062491575687	79219
14c2777639b9e53435187bb9499ab9b64dd70d2d	an intelligent market segmentation system using k-means and particle swarm optimization	k means;data mining;particle swarm optimizer;clustering;particle swarm optimization;market segmentation	With the development of information technology (IT), how to find useful information existed in vast data has become an important issue. The most broadly discussed technique is data mining, which has been successfully applied to many fields as analytic tool. Data mining extracts implicit, previously unknown, and potentially useful information from data. Clustering is one of the most important and useful technologies in data mining methods. Clustering is to group objects together, which is based on the difference of similarity on each object, and making highly homogeneity in the same cluster, or highly heterogeneity between each group. In this paper, we propose a market segmentation system based on the structure of decision support system which integrates conventional statistic analysis method and intelligent clustering methods such as artificial neural network, and particle swarm optimization methods. The proposed system is expected to provide precise market segmentation for marketing strategy decision making and extended application.	k-means clustering;mathematical optimization;particle swarm optimization	Chui-Yu Chiu;Yi-Feng Chen;I-Ting Kuo;He Chun Ku	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.05.029	multi-swarm optimization;computer science;artificial intelligence;machine learning;data mining;cluster analysis;particle swarm optimization;market segmentation;k-means clustering	ML	-0.9317123005878167	-34.12209029302175	79325
d44647349c2a4e3705ff347532a159c3f92ab531	low cost and power cnn/deep learning solution for automated driving		Automated driving functions, like highway driving and parking assist, are increasingly getting deployed in high-end cars with the ultimate goal of realizing self-driving car using Deep learning techniques like convolution neural network (CNN). For mass-market deployment, the embedded solution is required to address the right cost and performance envelope along with security and safety. In the case of automated driving, one of the key functionality is “finding drivable free space”, which is addressed using deep learning techniques like CNN. These CNN networks pose huge computing requirements in terms of hundreds of GOPS/TOPS (Giga or Tera operations per second), which seems beyond the capability of today's embedded SoC. This paper covers various techniques consisting of fixed-point conversion, sparse multiplication, fusing of layers and network pruning, for tailoring on the embedded solution. These techniques are implemented on the device by means of optimized Deep learning library for inference. The paper concludes by demonstrating the results of a CNN network running in real time on TI's TDA2X embedded platform producing a high-quality drivable space output for automated driving.	artificial neural network;automatic parking;autonomous car;convolution;deep learning;embedded system;flops;requirement;software deployment;sparse matrix;tops	Mihir Mody;Kumar Desappan;Pramod Swami;Manu Mathew;Soyeb Nagori	2018	2018 19th International Symposium on Quality Electronic Design (ISQED)	10.1109/ISQED.2018.8357325	convolutional neural network;software deployment;real-time computing;tera-;giga-;flight envelope;deep learning;computer science;multiplication;inference;artificial intelligence	Embedded	3.8723760872614985	-35.54333805089014	79438
6248160ef196ef221fa1fa6576ae52ca95b815a9	a mesh-based qos aware multicast routing protocol	fuzzy neural nets;decision tree;technological innovation;fuzzy uncertainty;top down;fuzzy rules;decision maker;innovation management;classification rules;neuro fuzzy;technological innovation project;mathematical model;innovation management decision trees fuzzy neural nets;decision trees technological innovation mathematical model classification tree analysis investments costs environmental management artificial intelligence fuzzy neural networks humans;fuzzy decision tree;classification accuracy;decision trees;neuro fuzzy decision tree;project selection;neural network neuro fuzzy decision tree technological innovation project fuzzy uncertainty;neural network	When the mathematical model of projects selection is established in the technological innovation, the conventional methods have deficiencies in dealing with the fuzzy uncertainty. To improve the mathematical model, in this paper, Neuro-fuzzy decision tree(Neuro-FDT) is introduced to the research on the innovation projects selection . Fuzzy decision trees are powerful, top-down, hierarchical search methodology to extract human interpretable classification rules. However, they are very poor in classification accuracy. Neural networks-fuzzy decision tree improves FDT's classification accuracy and extracts more accuracy human interpretable classification rules. The fuzzy rules enable a decision-maker to decide the optimal projects selection of technological innovation. Comparing with the conventional methods, the mathematical model of neuro-fuzzy decision tree can be easily established by fully utilizing the information of projects. The result of the positive research indicated that this mathematical model is very valid for innovation projects selection and it will have a good application prospect in this area.	multicast;quality of service;routing	Dayin Promkotwong;Ohm Sornil	2007		10.1109/SNPD.2007.527	innovation management;computer science;machine learning;decision tree;incremental decision tree;data mining;management science;artificial neural network	Networks	3.4703468679521037	-29.041620652304655	79698
e3963297f7b4ab7d8adb3cce9de137d9907a606f	a novel unsupervised time series discord detection algorithm in aircraft engine gearbox		Aircraft engine discord detection is an important way to ensure flight safety. Unsupervised algorithms will be relatively effective due to the lack of models and tagged discord data. For the aircraft time series data collected from sensors, this paper proposes a Trend Featured Dynamic Time Wrapping for J Distance Discord Discovery algorithm based on the J-Distance Discord anomaly definition which combined with the trend information of the data. The experiments on aircraft engine gearbox data show that the TFDTW for JDD Discovery algorithm is better than the normal J-Distance Discord Discovery algorithm and also better than some other classic time series data discord detection algorithms.		Zhongyu Wang;Dechang Pi;Ya Gao	2018		10.1007/978-3-030-05090-0_18	time series;data mining;machine learning;artificial intelligence;unsupervised learning;algorithm;computer science	ML	5.519541766870208	-36.303818368898746	79699
da3d96771c6871ac55357186485f0e04f4cd634c	impact of using information gain in software defect prediction models		Presence or absence of defective modules in software is an indicator of quality of the software. Every company aspires to deliver good quality software with minimum number of defective modules. To achieve this goal, defect prediction models are used in different phases of software lifecycle. These models have to deal with a large number software metrics (as input parameters to the models). These metrics have correlation issues that affect a model’s performance. Also, in some cases using all the metrics negatively impacts the models’ performances. In order to reduce size of input space and resolve the possible issues of correlation in input data, models reported in literature use Principal Component Analysis (PCA) and Information Gain (IG) based dimension reduction. PCA reduces the dimensions but keeps the representation of all the input variables intact. Use of PCA is not suitable where representation of all the metrics is declining a model’s performance. To handle such situations, this paper advocates use of Information Gain (IG) based technique to reduce size of input space by dropping the irrelevant metrics. Afterwards, only relevant metrics are used to develop a prediction model. This paper compares the PCA and IC based techniques to develop classification tree and fuzzy inferencing system based models. In order to study the impact of using IG, percentage improvement in Recall, Accuracy and Misclassification Rate have been calculated for the aforementioned models. The results show that use of IG improves the models’ performances more often than PCA does.	classification chart;decision tree learning;dimensionality reduction;experiment;inference engine;information gain in decision trees;integrated circuit;kullback–leibler divergence;mike lesser;performance;principal component analysis;relevance;serial ata;software bug;software development process;software metric	Zeeshan Ali Rana;Mian M. Awais;Shafay Shamail	2014		10.1007/978-3-319-09333-8_69	artificial intelligence;software bug;machine learning;predictive modelling;pattern recognition;software metric;computer science;principal component analysis;software development process;software;dimensionality reduction;decision tree learning	SE	3.499415346757478	-33.83642987958688	79871
72d8aa859f78d77d5a0ecf0c30620ac9b8025831	gear crack detection using kernel function approximation	aproximacion funcion;condiciones limites;one class classification;detection panne;condition aux limites;failure detection;diagnostico;kernel function;gear;outlier;panne;classification;outlier detection;approximation fonction;observacion aberrante;function approximation;condition monitoring;boundary condition;crack detection;funcion nucleo;fonction noyau;normal operator;pana;breakdown;observation aberrante;engrenage;reseau neuronal;deteccion falla;diagnosis;clasificacion;red neuronal;neural network;engranaje;diagnostic	Failure detection in machine condition monitoring involves a classification mainly on the basis of data from normal operation, which is essentially a problem of one-class classification. Inspired by the successful application of KFA (Kernel Function Approximation) in classification problems, an approach of KFA-based normal condition domain description is proposed for outlier detection. By selecting the feature samples of normal condition, the boundary of normal condition can be determined. The outside of this normal domain is considered as the field of outlier. Experiment results indicated that this method can be effectively and successfully applied to gear crack diagnosis.	approximation;kernel (operating system)	Weihua Li;Tielin Shi;Kang Ding	2006		10.1007/11893295_59	kernel;anomaly detection;outlier;function approximation;biological classification;boundary value problem;gear;computer science;machine learning;calculus;mathematics;geometry;normal operator;one-class classification;artificial neural network;algorithm	ML	9.846458570185801	-30.54231461931779	80168
13ad9edfa337a62b8f98f2e9504ea6f59b6f40ac	comparison of two methods for computing action values in xcs with code-fragment actions	boolean problems;learning classifier systems;xcs;pattern recognition;xcscfa;code fragments	XCS is a learning classifier system that uses accuracy-based fitness to learn a problem. Commonly, a classifier rule in XCS is encoded using a ternary alphabet based condition and a numeric action. Previously, we implemented a code-fragment action based XCS, called XCSCFA, where the typically used numeric action was replaced by a genetic programming like tree-expression. In XCSCFA, the action value in a classifier was computed by loading the terminal symbols in the action-tree with the corresponding binary values in the condition of the classifier rule. This enabled accurate, general and compact rule sets to be simply produced. The main contribution of this work is to investigate an intuitive way, i.e. using the environmental instance, to compute the action value in XCSCFA, instead of the condition of the classifier rule. The methods will be compared in five different Boolean problem domains, i.e. multiplexer, even-parity, majority-on, design verification, and carry problems. The environmental instance based XCSCFA approach had better classification performance than standard XCS as well as classifier condition based XCSCFA and solved all the problems experimented here. In addition it produced more general and compact classifier rules in the final solution. However, classifier condition based XCSCFA has the advantage of producing the optimal classifiers such that they are clearly separated from the sub-optimal ones in certain domains.	boolean algebra;genetic programming;learning classifier system;multiplexer;problem domain	Muhammad Iqbal;Will N. Browne;Mengjie Zhang	2013		10.1145/2464576.2482702	margin classifier;quadratic classifier;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;algorithm	AI	5.099009988199026	-32.97061704770291	80202
8ce8924c31b50213f9865b93c61c6e3a1c7ebaae	minerva: sequential covering for rule extraction	machine learning algorithms;systems;decision tree;neural networks;support vector machines;minerva;neural nets;algorithms artificial intelligence decision support techniques information storage and retrieval pattern recognition automated;model performance;rule extraction;neural networks support vector machines machine learning information management machine learning algorithms artificial neural networks immune system humans decision trees medical diagnosis;classification;support vector machines classification rule extraction;artificial neural networks;black box model minerva sequential covering rule extraction artificial neural networks support vector machines machine learning;machine learning;classification rules;information management;immune system;humans;support vector machine;support vector machines knowledge based systems neural nets;sequential covering;decision trees;black box model;knowledge based systems;medical diagnosis;artificial neural network;neural network	Various benchmarking studies have shown that artificial neural networks and support vector machines often have superior performance when compared to more traditional machine learning techniques. The main resistance against these newer techniques is based on their lack of interpretability: it is difficult for the human analyst to understand the reasoning behind these models' decisions. Various rule extraction (RE) techniques have been proposed to overcome this opacity restriction. These techniques are able to represent the behavior of the complex model with a set of easily understandable rules. However, most of the existing RE techniques can only be applied under limited circumstances, e.g., they assume that all inputs are categorical or can only be applied if the black-box model is a neural network. In this paper, we present Minerva, which is a new algorithm for RE. The main advantage of Minerva is its ability to extract a set of rules from any type of black-box model. Experiments show that the extracted models perform well in comparison with various other rule and decision tree learners.	algorithm;artificial neural network;black box;decision tree;experiment;extraction;machine learning;rule (guideline);rule induction;support vector machine;vacuum extraction, obstetrical	Johan Huysmans;Rudy Setiono;Bart Baesens;Jan Vanthienen	2008	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2007.912079	support vector machine;computer science;artificial intelligence;machine learning;decision tree;data mining;artificial neural network	DB	3.922803350621811	-28.869837832347077	80275
d3467b7994b228a8cd881b85762797b12ff398be	an interval type-2 fuzzy logic system for the modeling and prediction of financial applications	financial applications;type 2 fuzzy logic systems	In the recent years, there has been growing interest in developing tools for the modeling and prediction of financial applications. The problem of financial applications is that there are huge data sets available which are sometimes incomplete, and almost always affected by noise and uncertainty. Some techniques used in financial applications employ black box models which do not allow the user to understand the behavior and dynamics of the given application. In this paper, we present a type-2 Fuzzy Logic System (FLS) for the modeling and prediction of financial applications. The proposed system avoids the drawbacks of the existing type-2 fuzzy classification systems where the proposed system is able to carry prediction based on a pre-specified rule base size even if the incoming input vector does not match any rules from the given rule base. We have performed several experiments based on the London Stock Exchange data which was successfully used to spot ahead of time arbitrage opportunities. The proposed type-2 FLS has outperformed the existing type-2 fuzzy logic based classification systems and the type-1 FLSs counterparts when using pre-specified rule bases.	black box;experiment;free library of springfield township;fuzzy classification;fuzzy logic;fuzzy set;genetic algorithm;image noise;receiver operating characteristic;rule-based system;type-2 fuzzy sets and systems	Dario Bernardo;Hani Hagras;Edward P. K. Tsang	2012		10.1007/978-3-642-31368-4_12	fuzzy electronics;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;machine learning;data mining	ML	3.353586225341782	-26.10916343669079	80278
f85f4f6384ac4cf078e63d801ff28d06b43556ae	bellwether analysis: searching for cost-effective query-defined predictors in large databases	data cube;learning algorithm;first year;computational techniques;large dataset;data collection;database query processing;data mining;machine learning;prediction accuracy;cost effectiveness;predictive models;prediction model;massive datasets;scalable algorithms;olap queries;database query;cost effective prediction;bellwether	How to mine massive datasets is a challenging problem with great potential value. Motivated by this challenge, much effort has concentrated on developing scalable versions of machine learning algorithms. However, the cost of mining large datasets is not just computational; preparing the datasets into the “right form” so that learning algorithms can be applied is usually costly, due to the human labor that is typically required and a large number of choices in data preparation, which include selecting different subsets of data and aggregating data at different granularities. We make the key observation that, for a number of practically motivated problems, these choices can be defined using database queries and analyzed in an automatic and systematic manner. Specifically, we propose a new class of data-mining problem, called bellwether analysis, in which the goal is to find a few query-defined predictors (e.g., first week sales of Peoria, IL of an item) that can be used to accurately predict the result of a target query (e.g., first year worldwide sales of the item) from a large number of queries that define candidate predictors. To make a prediction for a new item, the data needed to generate such predictors has to be collected (e.g., selling the new item in Peoria, IL for a week and collecting the sales data). A useful predictor is one that has high prediction accuracy and a low data-collection cost. We call such a cost-effective predictor a bellwether.  This article introduces bellwether analysis, which integrates database query processing and predictive modeling into a single framework, and provides scalable algorithms for large datasets that cannot fit in main memory. Through a series of extensive experiments, we show that bellwethers do exist in real-world databases, and that our computation techniques achieve good efficiency on large datasets.	algorithm;computation;computer data storage;data mining;database;experiment;kerrison predictor;machine learning;predictive modelling;scalability	Bee-Chung Chen;Raghu Ramakrishnan;Jude W. Shavlik;Pradeep Tamma	2009	TKDD	10.1145/1497577.1497582	computer science;data science;machine learning;data mining;database;predictive modelling	DB	2.7835222046690293	-34.934375595389525	80304
ef96cce8c8aff101cc7055a5f992312c9aeabc21	detection of changes in group of services in soa system by learning algorithms		The objective of this chapter is to present the detection of some anomalies in SOA system by learning algorithms. Special model of SOA system was designed and implemented for the experimental purpose. In this systems several anomalies were introduced. The detection of one of them i.e. changes in the frequency of a group of services is presented. Three learning algorithms: Kohonen network, emerging patterns and k-means clustering were used in the anomalies detector. The effectiveness of algorithms in detecting anomaly were measured. The results of experiments may be used to select an efficient algorithm for the detection of anomaly.	algorithm	Ilona Bluemke;Marcin Tarka	2014		10.1007/978-3-319-10383-9_20	anomaly detection;detector;receiver operating characteristic;cluster analysis;self-organizing map;algorithm;intrusion detection system;computer science	Theory	7.010173572115791	-37.168125232275486	80308
aed8c04fdc9eef78e86551e88f79e416a1c63f3b	fuzzy rough sets for self-labelling: an exploratory analysis	science general;operators;nickel;training;aggregation;unsupervised learning fuzzy set theory pattern classification rough set theory;rough sets open wireless architecture training nickel stability analysis labeling robustness;open wireless architecture;supervised learning semisupervised learning semisupervised classification associated class labels unlabelled data instances class predictions fuzzy rough set theory stability analysis ordered weighted average based fuzzy rough model fuzzy rough self labelling technique unsupervised learning;stability analysis;rough sets;robustness;labeling	Semi-supervised learning incorporates aspects of both supervised and unsupervised learning. In semi-supervised classification, only some data instances have associated class labels, while others are unlabelled. One particular group of semi-supervised classification approaches are those known as self-labelling techniques, which attempt to assign class labels to the unlabelled data instances. This is achieved by using the class predictions based upon the information of the labelled part of the data. In this paper, the applicability and suitability of fuzzy rough set theory for the task of self-labelling is investigated. An important preparatory experimental study is presented that evaluates how accurately different fuzzy rough set models can predict the classes of unlabelled data instances for semi-supervised classification. The predictions are made either by considering only the labelled data instances or by involving the unlabelled data instances as well. A stability analysis of the predictions also helps to provide further insight into the characteristics of the different fuzzy rough models. Our study shows that the ordered weighted average based fuzzy rough model performs best in terms of both accuracy and stability. Our conclusions offer a solid foundation and rationale that will allow the construction of a fuzzy rough self-labelling technique. They also provide an understanding of the applicability of fuzzy rough sets for the task of semi-supervised classification in general.	approximation;design rationale;experiment;fuzzy set;heuristic (computer science);iteration;iterative method;rough set;semi-supervised learning;semiconductor industry;set theory;supervised learning;test set;unsupervised learning	Sarah Vluymans;Neil MacParthalain;Chris Cornelis;Yvan Saeys	2016	2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2016.7737788	nickel;labeling theory;von neumann stability analysis;rough set;fuzzy classification;computer science;operator;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set operations;robustness;dominance-based rough set approach	Robotics	5.246125470590029	-30.249310808257192	80504
2da4c1019643174728beebaecba39a413fdffec0	a constructive neural network to predict pitting corrosion status of stainless steel	pitting corrosion;austenitic stainless steel;constructive neural networks	The main consequences of corrosion are the costs derived from both the maintenance tasks as from the public safety protection. In this sense, artificial intelligence models are used to determine pitting corrosion behaviour of stainless steel. This work presents the C-MANTEC constructive neural network algorithm as an automatic system to determine the status pitting corrosion of that alloy. Several classification techniques are compared with our proposal: Linear Discriminant Analysis, k-Nearest Neighbor, Multilayer Perceptron, Support Vector Machines and Naive Bayes. The results obtained show the robustness and higher performance of the C-MANTEC algorithm in comparison to the other artificial intelligence models, corroborating the utility of the constructive neural networks paradigm in the modelling pitting corrosion problem.		Daniel Urda;Rafael Marcos Luque Baena;María Jesús Jiménez-Come;Ignacio Turias Domínguez;Leonardo Franco;José M. Jerez	2013		10.1007/978-3-642-38679-4_7	constructive;corrosion;artificial intelligence;naive bayes classifier;machine learning;robustness (computer science);pattern recognition;artificial neural network;multilayer perceptron;computer science;pitting corrosion;austenitic stainless steel	ML	9.0064779028937	-37.49513253982135	80575
7d4f8409917c3801040efdb2914b798fb816ffbc	memetic pareto differential evolutionary neural network for donor-recipient matching in liver transplantation	decisionmaking process;artificial neural network model;mathematical value;pareto differential evolutionary neural;liver transplantation;complex scenario;computational tool;right decision;multi-objective evolutionary algorithm;donor-recipient matching;inherent complexity	Donor-Recipient matching constitutes a complex scenario not easily modelable. The risk of subjectivity and the likelihood of falling into error must not be underestimated. Computational tools for decisionmaking process in liver transplantation can be useful, despite its inherent complexity. Therefore, a Multi-Objective Evolutionary Algorithm and various techniques of selection of individuals are used in this paper to obtain Artificial Neural Network models to assist in making decisions. Thus, the experts will have a mathematical value that enables them to make a right decision without deleting the principles of justice, efficiency	artificial neural network;computation;evolutionary algorithm;machine learning;memetics;pareto efficiency;rule-based system	Manuel Cruz-Ramírez;César Hervás-Martínez;Pedro Antonio Gutiérrez;Javier Briceño;Manuel de la Mata	2011		10.1007/978-3-642-21498-1_17	computer science;artificial intelligence;machine learning;management science	ML	1.3315555864023114	-27.50210154999734	80820
e5da65c609b5fcc6fe331f137f179ab05d3d8242	rule extraction from medical data without discretization of numerical attributes		Association rule mining is a popular technique used to find associations between attributes in a dataset. When using deterministic algorithms, if the attributes have numerical values the usual approach is to discretize them defining proper intervals. But the discretization can notably affect the quality of the rules generated. This work presents a method based on a deterministic exploration of the interval search space without a previous discretization of the numerical attributes. It has been applied to medical data from an atherosclerosis study. The quality of the obtained rules seems to support this method as a valid alternative for this kind of rule extraction.	algorithm;association rule learning;deterministic algorithm;discretization;numerical analysis;plan 9 from bell labs;rule induction	Juan L. Domínguez-Olmedo;Jacinto Mata Vázquez;Victoria Pachón;Manuel J. Maña López	2012			machine learning;pattern recognition;data mining;discretization of continuous features	DB	1.3772926148094602	-30.38266972570653	80884
68b098362cc0da10ce98afc02bbfbf3b5e241aa7	an effective approach for software project effort and duration estimation with machine learning algorithms		Abstract During the last two decades, there has been substantial research performed in the field of software estimation using machine learning algorithms that aimed to tackle deficiencies of traditional and parametric estimation techniques, increase project success rates and align with modern development and project management approaches. Nevertheless, mostly due to inconclusive results and vague model building approaches, there are few or none deployments in practice. The purpose of this article is to narrow the gap between up-to-date research results and implementations within organisations by proposing effective and practical machine learning deployment and maintenance approaches by utilization of research findings and industry best practices. This was achieved by applying ISBSG dataset, smart data preparation, an ensemble averaging of three machine learning algorithms (Support Vector Machines, Neural Networks and Generalized Linear Models) and cross validation. The obtained models for effort and duration estimation are intended to provide a decision support tool for organisations that develop or implement software systems.	algorithm;machine learning;software project management	Przemyslaw Pospieszny;Beata Czarnacka-Chrobot;Andrzej Kobylinski	2018	Journal of Systems and Software	10.1016/j.jss.2017.11.066	software deployment;estimation;artificial neural network;analysis effort method;project management;computer science;ensemble averaging;data mining;algorithm;computational learning theory;machine learning;software sizing;artificial intelligence	SE	3.4181978306063354	-33.47547308114726	80976
100c9fdba646fdd1f8fb9ed64e934a32eeae27ee	decision trees and transient stability of electric power systems	sensibilite;reseau multimachine;evaluation systeme;transient stability analysis;decision tree;stabilite dynamique;reseau electrique;analisis datos;commande;electrical network;sensitivity and control;evaluacion sistema;red electrica;estabilidad dinamica;power systems;intelligence artificielle;transient stability;arbol decision;stability;artificial intelligent;sensitivity;data analysis;system evaluation;electric power system;data reduction and analysis;machine learning;sistema transporte fuerzas multimaquina;decision theory;pattern recognition decision trees;pattern recognition;artificial intelligence;analyse donnee;control;data reduction;inteligencia artificial;reconnaissance forme;reconocimiento patron;multimachine power system;arbre decision;electric power systems;sensibilidad	Al~lraet--An inductive inference method for the automatic building of decision trees is investigated. Among its various tasks, the splitting and the stop splitting criteria successively applied to the nodes of a grown tree, are found to play a crucial role on its overall shape and performances. The application of this general method to transient stability is systematically explored. Parameters related to the stop splitting criterion, to the learning set and to the tree classes are thus considered, and their influence on the tree features is scrutinized. Evaluation criteria appropriate to assess accuracy are also compared. Various tradeoffs are further examined, such as complexity vs number of classes, or misclassifieation rate vs type of misclassification errors. Possible uses of the trees are also envisaged. Computational issues relating to the building and the use of trees are finally discussed.	complexity;computation;decision tree;ibm power systems;inductive reasoning;performance	Louis Wehenkel;M. Pavella	1991	Automatica	10.1016/0005-1098(91)90010-Y	engineering;artificial intelligence;mathematics;electric power system;operations research;algorithm;statistics	ML	8.604803404299526	-24.868527702989283	81472
5111f9289aa01e7e261b612c62038cd300396871	anomaly detection and diagnosis algorithms for discrete symbol sequences with applications to airline safety	anomaly;unsupervised learning;flotte;variable discreta;overhead line;modelo markov oculto;sequences;unsupervised clustering;analyse amas;sensor phenomena and characterization;discrete sequence data;switch sensors;discrete data;algorithm analysis;sistema critica;securite;modele markov cache;system with n degrees of freedom;hidden markov model;anomaly detection;systeme critique;ligne aerienne;metric;donnee discrete;outlier;discrete symbols;commercial airliners anomaly detection diagnosis algorithms discrete symbol sequences airline safety sequenceminer high dimensional symbol sequences switch sensors aircraft system wide health unsupervised clustering discrete sequence data;air transportation;multidimensional analysis;apprentissage non supervise;sequenceminer;anomalie;aerospace safety;classification;similitude;sistema reactivo;anomalia;airline safety;captador medida;observacion aberrante;transport aerien;transporte aereo;measurement sensor;integrated system health management ishm;cluster analysis;capteur mesure;innovation;disk recording;hidden markov models;critical system;analyse n dimensionnelle;detection defaut;diagnostic panne;monitoring;air safety;air safety switches clustering algorithms hidden markov models algorithm design and analysis fault detection vehicle safety disk recording sensor phenomena and characterization aircraft;diagnosis algorithms;fault diagnostic;systeme n degres liberte;health and safety;fault detection;fleet;safety;sequences anomaly detection diagnosis discrete symbols fault detection integrated system health management ishm;similarity;diagnostico pana;analisis n dimensional;reactive system;systeme reactif;discrete symbol sequences;clustering algorithms;travel industry aerospace safety health and safety;observation aberrante;metrico;analisis cluster;analyse algorithme;vehicle safety;high dimensional symbol sequences;monitorage;linea aerea;similitud;commercial airliners;innovacion;switches;sistema n grados libertad;monitoreo;diagnosis	We present a set of novel algorithms which we call sequenceMiner that detect and characterize anomalies in large sets of high-dimensional symbol sequences that arise from recordings of switch sensors in the cockpits of commercial airliners. While the algorithms that we present are general and domain-independent, we focus on a specific problem that is critical to determining the system-wide health of a fleet of aircraft. The approach taken uses unsupervised clustering of sequences using the normalized length of the longest common subsequence as a similarity measure, followed by detailed outlier analysis to detect anomalies. In this method, an outlier sequence is defined as a sequence that is far away from the cluster center. We present new algorithms for outlier analysis that provide comprehensible indicators as to why a particular sequence is deemed to be an outlier. The algorithms provide a coherent description to an analyst of the anomalies in the sequence when compared to more normal sequences. In the final section of the paper, we demonstrate the effectiveness of sequenceMiner for anomaly detection on a real set of discrete-sequence data from a fleet of commercial airliners. We show that sequenceMiner discovers actionable and operationally significant safety events. We also compare our innovations with standard hidden Markov models, and show that our methods are superior.	algorithm;anomaly detection;bayesian network;cluster analysis;coherence (physics);hidden markov model;longest common subsequence problem;markov chain;sensor;sequence clustering;similarity measure	Suratna Budalakoti;Ashok N. Srivastava;Matthew Eric Otey	2009	IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)	10.1109/TSMCC.2008.2007248	simulation;telecommunications;computer science;artificial intelligence;machine learning;cluster analysis;hidden markov model	ML	6.498961003497752	-34.89782379515183	81573
b8d67b32774f31fa87030343f81547c66dd73b27	decision-improved support vector machine and its application	unsupervised learning;kernel;data identification decision improved support vector machine pattern classification decision curve region classification accuracy unsupervised learning multiclass classification gear box fault diagnosis;support vector machines;decision improved support vector machine;decision curve region;gear box fault diagnosis;artificial neural networks;support vector machines fault diagnosis unsupervised learning space technology artificial neural networks kernel support vector machine classification pattern classification artificial intelligence intelligent networks;multi class classification;pattern classification;artificial intelligence;support vector machine classification;space technology;intelligent networks;support vector machine;classification accuracy;data identification;unsupervised learning decision making pattern classification support vector machines;multiclass classification;fault diagnosis	The practical applications of 1-SVM in pattern classification are limited due to the deficiency of its low classification precision. Aimed at solving this problem, first, a 1-DISVM is proposed, in which a coefficient is introduced to adjust decision curve region. Compared to 1-SVM, 1-DISVM inherits the ability to find outliers but gains improved classification accuracy by the coefficient adjusting. Based on 1-DISVM, an unsupervised learning multi-class classification model is also built. By means of small quantity of fault samples, the model improves classification performance by getting rid of the influence from wrong samples. Then the experiment is implemented by applying the considered approaches in gear-box fault diagnosis. Experimental results show that, the presented method achieved precise classification for two-class (normal and fault) data identification.	angular defect;coefficient;multiclass classification;support vector machine;unsupervised learning	G. J. Liu;X. M. Liu;Yongzhi Zhang;J. Qiu	2008	2008 IEEE International Conference on Networking, Sensing and Control	10.1109/ICNSC.2008.4525283	unsupervised learning;support vector machine;computer science;machine learning;linear classifier;multiclass classification;pattern recognition;data mining;one-class classification;artificial neural network	Robotics	9.895171669743226	-37.86132328646526	81624
854cdcec3f5f6ab8083bb7d91456382ed0b3ba76	a general grid-clustering approach	hierarchical clustering;hierarchical system;cluster algorithm;evaluation performance;analyse amas;performance evaluation;large dataset;evaluacion prestacion;estimation non parametrique;systeme hierarchise;core grid;locality;algorithme;algorithm;non parametric estimation;sistema jerarquizado;hierarchical classification;cluster analysis;clustering;signal classification;classification hierarchique;classification signal;analisis cluster;grid size;estimacion no parametrica;classification automatique;automatic classification;clasificacion automatica;clasificacion jerarquizada;algoritmo	Hierarchical clustering is an important part of cluster analysis. Based on various theories, numerous hierarchical clustering algorithms have been developed, and new clustering algorithms continue to appear in the literature. It is known that both divisive and agglomerative clustering algorithms in hierarchical clustering play a pivotal role in data-based models, and have been successfully applied in clustering very large datasets. However, hierarchical clustering is parameter-sensitive. When the user has no knowledge of how to choose the input parameters, the clustering results may become undesirable. In this paper, we propose a general grid-clustering approach (GGCA) under a common assumption about hierarchical clustering. The key features of the GGCA include: (1) the combination of the divisible and the agglomerative clustering algorithms into a unifying generative framework; (2) the determination of key input parameters: an optimal grid size for the first time; and (3) the application of a two-phase merging process to aggregate all data objects. Consequently, the GGCA is a non-parametric algorithm which does not require users to input parameters, and exhibits excellent performance in dealing with not well-separated and shape-diverse clusters. Some experimental results comparing the proposed GGCA with the existing methods show the superiority of the GGCA approach.	cluster analysis	Shihong Yue;Miaomiao Wei;Jeen-Shing Wang;Huaxiang Wang	2008	Pattern Recognition Letters	10.1016/j.patrec.2008.02.019	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;single-linkage clustering;brown clustering;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	Vision	8.989682177331106	-34.6077953484238	81704
b3df241a741fa5c84ec23d725c3ca91ed3d430ea	spark-parsketch: a massively distributed indexing of time series datasets		A growing number of domains (finance, seismology, internet-of-things, etc.) collect massive time series. When the number of series grow to the hundreds of millions or even billions, similarity queries become intractable on a single machine. Further, naive (quadratic) parallelization won't work well. So, we need both efficient indexing and parallelization. We propose a demonstration of Spark-parSketch, a complete solution based on sketches / random projections to efficiently perform both the parallel indexing of large sets of time series and a similarity search on them. Because our method is approximate, we explore the tradeoff between time and precision. A video showing the dynamics of the demonstration can be found by the link http://parsketch.gforge.inria.fr/video/parSketchdemo_720p.mov.	approximation algorithm;parallel computing;quadratic function;similarity search;time series	Oleksandra Levchenko;Djamel Edine Yagoubi;Reza Akbarinia;Florent Masseglia;Boyan Kolev;Dennis Shasha	2018		10.1145/3269206.3269226	information retrieval;search engine indexing;spark (mathematics);theoretical computer science;computer science;nearest neighbor search	DB	-2.5662371971703175	-37.36102237474983	81797
9f882df673f4c2e2bb1040cafda2c65581eba246	extension of genetic programming with multiple trees for agent learning		This paper proposes an extension of genetic programming (GP) with multiple trees. In order to improve the performance, GP with control node (GPCN) and its three kinds of modification have been proposed. In GPCN, an individual consists of several trees which have the number P of executions. In previous work, the two kinds of modification, the conditional probability and the cross-cultural island model are employed. This paper proposes two methods: the new island model that combines the conditional probability with two islands in the cross-cultural island model and a method exchanges multiple trees in an individual in a suitable order. Experiments are conducted to show the performance in the garbage collection problem and the Santa Fe Trail problem.		Takashi Ito;Kenichi Takahashi;Michimasa Inaba	2016	JCP		machine learning;genetic programming;artificial intelligence;computer science	ML	4.480122632991566	-30.56559759525677	81853
ac9193052cac26ca7478eafd89b40899e99beb51	hybrid multi-granulation rough sets of variable precision based on tolerance	rough set theory information systems;hybrid multi granulation rough sets model of variable precision;rough sets yttrium information systems computational modeling knowledge discovery periodic structures;variable precision rough sets;hybrid multi granulation rough set method hybrid multigranulation rough sets variable precision based on tolerance data processing mode rough set theory incomplete information system research object variable precision rough set method;hybrid multi granulation rough sets model of variable precision tolerance rough set hybrid multi granulation rough sets variable precision rough sets;hybrid multi granulation rough sets;tolerance rough set	Hybrid multi-granularity approach is a data processing mode in rough set theory. Taking the incomplete information system as the research object, variable precision rough set method and hybrid multi granulation rough set method are fused. And this paper proposes a hybrid multi-granulation rough sets based on variable precision tolerance relations. Basic properties of hybrid multi-granularity rough set of variable precision are discussed, which provides a new approach to deal with the incomplete information system.	approximation algorithm;disjunctive normal form;information system;recurrence relation;rough set;set theory	He Lin;Qianyi Wang;Xin Lu;Haibo Li	2015	2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)	10.1109/FSKD.2015.7381945	rough set;machine learning;pattern recognition;data mining;mathematics	DB	-1.7574161999560716	-27.67679390468515	81899
870abd71758632a7aa9a1c6575842e9057633a1e	learning to identify known and unknown classes: a case study in open world malware classification		In this paper we propose an open world malware classification. Our approach is not only able to identify known families of malware but is also able to distinguish them from malware families that were never seen before. Our proposed approach is more accurate and scales better on two evaluation datasets when compared to existing algorithms.	algorithm;anomaly detection;malware;open world;statistical classification	Mehadi Hassen;Philip K. Chan	2018			natural language processing;machine learning;malware;artificial intelligence;computer science	ML	6.217566864735033	-37.08669814106732	82246
29a89d2e61ecb1fc64fd7e75d59383ac6e007e82	clustering based stocks recognition	cluster algorithm;analyse amas;stock market;bolsa valores;algoritmo borroso;logique floue;logica difusa;long terme;feature space;classification;long term;stock markets;bourse valeurs;stock exchange;fuzzy logic;marche valeurs;cluster analysis;largo plazo;fuzzy algorithm;indexation;decouverte connaissance;estructura datos;court terme;algorithme flou;descubrimiento conocimiento;analisis cluster;structure donnee;data structure;clasificacion;corto plazo;short term;knowledge discovery	A new stocks analysis method based on clustering is presented in this paper, in which, six-dimension feature space is constructed according to the data structure of stock chief-index, and the constructed feature space is analyzed with a new fuzzy kern clustering algorithm. We use the Shanghai and Shenzhen’s stock index since 1997 to test our presented method. The results show that the method could intelligently recognizes some rules of essence trends of the stock markets and forecasts essence direction of the stock markets not only in short-term but also in long-term.	algorithm;artificial intelligence;cluster analysis;data structure;effective method;feature vector;pattern recognition;support vector machine	Yaoyuan Shi;Zhongke Shi	2006		10.1007/11881599_140	fuzzy logic;stock exchange;feature vector;data structure;fuzzy clustering;biological classification;computer science;artificial intelligence;machine learning;short-term memory;cluster analysis	Web+IR	-2.5341541772654486	-32.684424531855484	82288
014afdcd1eabf7fb750a3afc37c8cecd5e1899de	graph projection techniques for self-organizing maps	data analysis;visualization technique;nearest neighbor;self organized map;neural network model	The Self-Organizing Map is a popular neural network model for data analysis, for which a wide variety of visualization techniques exists. We present two novel techniques that take the density of the data into account. Our methods define graphs resulting from nearest neighborand radius-based distance calculations in data space and show projections of these graph structures on the map. It can then be observed how relations between the data are preserved by the projection, yielding interesting insights into the topology of the mapping, and helping to identify outliers as well as dense regions.	artificial neural network;data point;dataspaces;experiment;network model;organizing (structure);principal component analysis;self-organizing map;u-matrix	Georg Pölzlbauer;Andreas Rauber;Michael Dittenbach	2005			nearest neighbor graph;computer science;machine learning;pattern recognition;data mining;mathematics;data analysis;k-nearest neighbors algorithm;artificial neural network	ML	1.3928646833461207	-37.57716486989599	82328
bb74ab74f2e8a557976502c03508873566bd3d6c	sqr: balancing speed, quality and risk in online experiments		"""Controlled experimentation, also called A/B testing, is widely adopted to accelerate product innovations in the online world. However, how fast we innovate can be limited by how we run experiments. Most experiments go through a """"ramp up"""" process where we gradually increase the traffic to the new treatment to 100%. We have seen huge inefficiency and risk in how experiments are ramped, and it is getting in the way of innovation. This can go both ways: we ramp too slowly and much time and resource is wasted; or we ramp too fast and suboptimal decisions are made. In this paper, we build up a ramping framework that can effectively balance among Speed, Quality and Risk (SQR). We start out by identifying the top common mistakes experimenters make, and then introduce the four SQR principles corresponding to the four ramp phases of an experiment. To truly scale SQR to all experiments, we develop a statistical algorithm that is embedded into the process of running every experiment to automatically recommend ramp decisions. Finally, to complete the whole picture, we briefly cover the auto-ramp engineering infrastructure that can collect inputs and execute on the recommendations timely and reliably."""	a/b testing;algorithm;embedded system;experiment;minkowski portal refinement;ramp simulation software for modelling reliability, availability and maintainability;recommender system;sqr;test set	Ya Xu;Weitao Duan;Shaochen Huang	2018		10.1145/3219819.3219875	computer science;machine learning;causal inference;artificial intelligence;a/b testing;too slowly;inefficiency	SE	1.4365139624342866	-34.42222163081112	82440
f51f6ceccf81e91faf56b66826445dbf7a407dd5	the application of fuzzy logic in a hybrid fuzzy knowledge-based system for multiobjective optimization of power distribution system operations	knowledge based system;multiobjective optimization;fuzzy logic		fuzzy logic;knowledge-based systems;multi-objective optimization;program optimization	Ashu M. G. Solo;Robert J. Sárfi	2005			fuzzy electronics;fuzzy transportation;fuzzy set operations;logic optimization;adaptive neuro fuzzy inference system;fuzzy logic;neuro-fuzzy;fuzzy control system;control engineering;computer science	EDA	3.568256423034864	-24.899944743213617	82487
e4e75f8f18adcdb98631026333d094e742ec9371	generalized fuzzy petri nets as pattern classifiers	learning algorithm;fuzzy set;learning;continuous variable;fuzzy sets;classifier;pattern classification;optimization;t and s norms;numerical experiment;petri nets;petri net	In this study, we discuss a novel approach to pattern classi®cation using a concept of fuzzy Petri nets. In contrast to the commonly encountered Petri nets with their inherently Boolean character of processing tokens and ®ring transitions, the proposed generalization involves continuous variables. This extension makes the nets to be fully in rapport with the panoply of the real-world classi®cation problems. The introduced model of the fuzzy Petri net hinges on the logic nature of the operations governing its underlying behavior. The logic-driven eect in these nets becomes especially apparent when we are concerned with the modeling of its transitions and expressing pertinent mechanisms of a continuous rather than an on±o ®ring phenomenon. An interpretation of fuzzy Petri nets in the setting of pattern classi®cation is provided. This interpretation helps us gain a better insight into the mechanisms of the overall classi®cation process. Input places correspond to the features of the patterns. Transitions build aggregates of the generic features giving rise to their logical summarization. The output places map themselves onto the classes of the patterns while the marking of the places correspond to the class of membership values. Details of the learning algorithm are also provided along with an illustrative numeric experiment. Ó 1999 Elsevier Science B.V. All rights reserved.	aggregate data;algorithm;artificial neural network;computer vision;high-level programming language;item unique identification;pattern recognition;petri net;relevance;synthetic intelligence	Witold Pedrycz	1999	Pattern Recognition Letters	10.1016/S0167-8655(99)00073-2	fuzzy classification;computer science;artificial intelligence;machine learning;mathematics;process architecture;fuzzy set;petri net;algorithm	AI	2.3285681458009564	-28.64090996072264	82507
aa11e301608d980824d600cb9592ff756290866d	a three-phase knowledge extraction methodology using learning classifier system	genetique;base donnee;genetic operator;learning algorithm;fuzzy neural nets;decision tree;algoritmo busqueda;genetica;algorithme recherche;logique floue;search algorithm;knowledge extraction;database;chaine caractere;base dato;base connaissance;logica difusa;learning classifier system;reseau neuronal flou;intelligence artificielle;circuito logico;algorithme apprentissage;arbol decision;algoritmo genetico;genetics;fuzzy logic;optimal operation;machine learning;circuit logique;decouverte connaissance;cadena caracter;proceedings paper;algorithme genetique;funcionamiento optimo;artificial intelligence;descubrimiento conocimiento;base conocimiento;genetic algorithm;knowledge integration;inteligencia artificial;reseau neuronal;decision tree induction;algoritmo aprendizaje;logic circuit;article;arbre decision;red neuronal;fonctionnement optimal;character string;neural network;knowledge base;knowledge discovery	Machine learning methods such as fuzzy logic, neural networks and decision tree induction have been applied to learn rules but they may be trapped into local optimal. Based on the principle of natural evolution and global searching, a genetic algorithm is promising in obtaining better results. This article adopts learning classifier systems (LCS) technique to provide a three-phase knowledge extraction methodology, which makes continues and instant learning while integrates multiple rule sets into a centralized knowledge base. This paper makes three important contributions: (1) it represents various rule sets that are derived from different sources and encoded as a fixed-length bit string in the knowledge encoding phase; (2) it uses three criteria (accuracy, coverage, and fitness) to select an optimal set of rules from a large population in the knowledge extraction phase; (3) it applies genetic operations to generate optimal rule sets in the knowledge integration phase. The experiments prove the rule sets derived by the proposed approach is more accurate than other machine learning algorithm.	artificial neural network;binary prefix;bit array;centralized computing;decision tree;experiment;fuzzy logic;genetic algorithm;knowledge base;knowledge integration;learning classifier system;machine learning	An-Pin Chen;Kuang-Ku Chen;Mu-Yen Chen	2005		10.1007/11546924_84	fuzzy logic;genetic algorithm;logic gate;string;computer science;artificial intelligence;genetic operator;machine learning;decision tree;knowledge extraction;learning classifier system;algorithm;search algorithm	AI	7.695887558171946	-31.595306198368753	82630
38b4dccc0ba6099e918a5ce5fb0bfc60d1434caa	employing tsk fuzzy models to automate the revision stage of a cbr system	forecasting;biological system;prevision;raisonnement base sur cas;razonamiento fundado sobre caso;logique floue;logica difusa;intelligence artificielle;fuzzy logic;systeme biologique;artificial intelligence;inteligencia artificial;case based reasoning;sistema biologico	CBR systems are normally used to assist experts in the resolution of problems. During the last few years, researchers have been working in the development of techniques to automate the reasoning stages identified in this methodology. This paper presents a fuzzy logic based method that automates the review stage of case-based reasoning systems and aids in the process of obtaining an accurate solution. The proposed methodology has been derived as an extension of the Sugeno Fuzzy model, and evaluates different solutions by reviewing their score in an unsupervised mode. The method has been successfully used to completely automate the reasoning process of a biological forecasting system and to improve its performance.	belief revision;case-based reasoning	Florentino Fernández Riverola;Juan Manuel Corchado	2003		10.1007/978-3-540-25945-9_30	fuzzy logic;case-based reasoning;forecasting;computer science;artificial intelligence;neuro-fuzzy;operations research;algorithm	AI	8.131235176308932	-29.488428829689667	82836
06573acd7355ade42aaceb6c4bf87e5ad2530959	bayesian instance selection for the nearest neighbor rule	bayes estimation;multicriteria analysis;ajustamiento modelo;optimisation;fiabilidad;reliability;analisis estadistico;diagramme voronoi;optimizacion;supervised learning;instance selection;competitividad;multi criteria analysis;metric;tiling;classification;ajustement modele;plan classement;vecino mas cercano;estimacion bayes;posterior distribution;statistical analysis;fiabilite;model matching;nearest neighbor;analyse statistique;pavage;prediction accuracy;distance metric;competitiveness;ley a posteriori;pattern recognition;plus proche voisin;nearest neighbour;metrico;optimization;plan clasificacion;analisis multicriterio;reconnaissance forme;apprentissage supervise;analyse multicritere;maximum a posteriori;reconocimiento patron;diagrama voronoi;optimal algorithm;competitivite;aprendizaje supervisado;loi a posteriori;clasificacion;metrique;voronoi diagram;classification scheme;estimation bayes;voronoi tesselation	The nearest neighbors rules are commonly used in pattern recognition and statistics. The performance of these methods relies on three crucial choices: a distance metric, a set of prototypes and a classification scheme. In this paper, we focus on the second, challenging issue: instance selection. We apply a maximum a posteriori criterion to the evaluation of sets of instances and we propose a new optimization algorithm. This gives birth to Eva, a new instance selection method. We benchmark this method on real datasets and perform a multi-criteria analysis: we evaluate the compression rate, the predictive accuracy, the reliability and the computational time. We also carry out experiments on synthetic datasets in order to discriminate the respective contributions of the criterion and the algorithm, and to illustrate the advantages of Eva over the state-of-the-art algorithms. The study shows that Eva outputs smaller and more reliable sets of instances, in a competitive time, while preserving the predictive accuracy of the related classifier.	algorithm;benchmark (computing);bottom-up parsing;browsing;cell (microprocessor);cellular automaton;comparison and contrast of classification schemes in linguistics and metadata;computation;division by zero;eva (benchmark);eva conferences;experiment;feature selection;graph labeling;greedy algorithm;hamming distance;heuristic (computer science);image scaling;k-nearest neighbors algorithm;lazy evaluation;mathematical optimization;numerical analysis;pattern recognition;prototype;randomness;reduction (complexity);selection algorithm;software prototyping;synthetic intelligence;time complexity;top-down and bottom-up design;unsharp masking;variable neighborhood search;vienna development method;volume boot record;voronoi diagram	Sylvain Ferrandiz;Marc Boullé	2010	Machine Learning	10.1007/s10994-010-5170-2	metric;computer science;machine learning;pattern recognition;mathematics;supervised learning;statistics	ML	9.734383593990943	-35.033705255422504	83379
80caf0786daaaeea031967197abc4ac2ce5573fd	som-based anomaly intrusion detection system	anomaly based intrusion detection;cluster;intrusion detection;u matrix;reference value;som;high dimension;anomaly intrusion detection;neural network	In this paper, a SOM-based anomaly intrusion detection system is proposed, which can contract high-dimension data to lower, meanwhile keeping the primary relationship between clustering and topology. During the experiment, the theory of SOM is used to train three SOMs on the layers of system, process and network. Although our experiment environment is simpler than the real one, the result shows that it has its reference value for us to build intelligent IDSs. Through the analysis of the monitoring results on the three layers from the hacking tools (NMAP, HYDRA), it is suggested that the approach of detecting and the parameters chosen be correct and effective.	anomaly detection;cluster analysis;hydra (chess);intrusion detection system;nmap security scanner;sensor	Chundong Wang;He-feng Yu;Huai-bin Wang;Kai Liu	2007		10.1007/978-3-540-77092-3_31	anomaly-based intrusion detection system;intrusion detection system;u-matrix;computer science;artificial intelligence;machine learning;data mining;artificial neural network;cluster	ML	7.48772948744471	-36.89788049993501	83390
aaa32690ebb4da83e50631bcc97a27aea3e0884c	discovering prediction rules by a neuro-fuzzy modeling framework	fuzzy rules;competitive learning;chemical properties;fuzzy rule base;neuro fuzzy;electricity generation;prediction accuracy;feature selection;is success	In this paper, we propose a neuro-fuzzy modeling framework to discover fuzzy rules and its application to predict chemical properties of ashes produced by thermo-electric generators. The framework is defined by several sequential steps in order to obtain a good predictive accuracy and the readability of the discovered fuzzy rules. First, a feature selection procedure is applied to the available data by discarding the features possessing lowest ranking in terms of their predictive power. Then, a competitive learning scheme is adopted to initialize a fuzzy rule base, which is successively refined by a neuro-fuzzy network trained on the available data. To improve accuracy, we applied the process on each ash property to be predicted, hence obtaining a set of MISO models that are both accurate and transparent, as shown by the reported experimen-	artificial neural network;bayesian network;competitive learning;cybernetics;entity–relationship model;feature selection;fuzzy sets and systems;fuzzy rule;general-purpose modeling;neural networks;neuro-fuzzy;organizing (structure);rule-based system;self-organization;system analysis	Giovanna Castellano;Ciro Castiello;Anna Maria Fanelli;Corrado Mencar	2003		10.1007/978-3-540-45224-9_168	defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	ML	5.0689058194663295	-27.759785184529083	83438
6459be70be2e41e6fb11506ed0a30cc370619a88	the combinatorial neural network: a connectionist model for knowledge based systems	connectionist models;knowledge based system;fault tolerant;neural model;data representation;fuzzy set theory;incomplete data;incremental learning;feature selection;expert knowledge;high speed;neural network;expert system	This paper describes the Combinatorial Neural Model, a high order neural network suitable for classification tasks. The model is based on the fuzzy sets theory, neural sciences and expert knowledge analysis results. The model presents interesting properties such as: modularity, explanation capacity, concomitant knowledge and data representation, high speed of training, incremental learning, generalization capacity, feature selection, processing of uncertain and incomplete data, fault tolerance.	artificial neural network;connectionism;data (computing);fault tolerance;feature selection;fuzzy set	Ricardo José Machado;Armando Freitas da Rocha	1990		10.1007/BFb0028145	legal expert system;fault tolerance;computer science;artificial intelligence;neuro-fuzzy;knowledge-based systems;machine learning;pattern recognition;data mining;time delay neural network;external data representation;fuzzy set;feature selection;expert system	ML	4.0479099797645	-26.972267040479398	83441
5395cd50070b1bced4781448daefc764a795ad89	constructing a decision tree for graph-structured data and its applications	decision tree;evidence based medicine;graph mining;graphmining;beam search;evidence basedmedicine;graph based induction;beamsearch;structured data	A machine learning technique called Graph-Based Induction (GBI) efficiently extracts typical patterns from graph-structured data by stepwise pair expansion (pairwise chunking). It is very efficient because of its greedy search. Meanwhile, a decision tree is an effective means of data classification from which rules that are easy to understand can be obtained. However, a decision tree could not be constructed for the data which is not explicitly expressed with attribute-value pairs. This paper proposes a method called Decision Tree Graph-Based Induction (DT-GBI), which constructs a classifier (decision tree) for graph-structured data while simultaneously constructing attributes for classification using GBI. Substructures (patterns) are extracted at each node of a decision tree by stepwise pair expansion in GBI to be used as attributes for testing. Since attributes (features) are constructed while a classifier is being constructed, DT-GBI can be conceived as a method for feature construction. The predictive accuracy of a decision tree is affected by which attributes (patterns) are used and how they are constructed. A beam search is employed to extract good enough discriminative patterns within the greedy search framework. Pessimistic pruning is incorporated to avoid overfitting to the training data. Experiments using a DNA dataset were conducted to see the effect of the beam width and the number of chunking at each node of a decision tree. The results indicate that DT-GBI that uses very little prior domain knowledge can construct a decision tree that is comparable to other classifiers constructed using the domain knowledge. DT-GBI was also applied to analyze a real-world hepatitis dataset as a part of evidence-based medicine. Four classification tasks of the hepatitis data were conducted using only the time-series data of blood inspection and urinalysis. The Address for correspondence: Institute of Scientific and Industrial Research, Osaka University, JAPAN 8-1 Mihogaoka, Ibaraki, Osaka 567-0047, Japan 1002 Warodom Geamsakul et al. / Constructing a Decision Tree for Graph-Structured Data and its Applications preliminary results of experiments, both constructed decision trees and their predictive accuracies as well as extracted patterns, are reported in this paper. Some of the patterns match domain experts’ experience and the overall results are encouraging.	attribute–value pair;beam search;decision tree;embedded system;error message;experiment;feature vector;gene expression programming;graph (abstract data type);greedy algorithm;information gain in decision trees;machine learning;missing data;on the fly;overfitting;principle of good enough;rom cartridge;randomness;shallow parsing;stepwise regression;tilde;time series	Warodom Geamsakul;Tetsuya Yoshida;Kouzou Ohara;Hiroshi Motoda;Hideto Yokoi;Katsuhiko Takabayashi	2005	Fundam. Inform.		evidence-based medicine;beam search;influence diagram;decision tree learning;data model;computer science;artificial intelligence;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;interval tree;fractal tree index;search tree;id3 algorithm;tree traversal;algorithm;decision stump	ML	-1.990582455229869	-33.69308475822265	83684
1ba976a2eb335e9e231cdb365b149042e445fa42	using neural networks to improve classification: application to brain maturation	traitement signal;sistema experto;knowledge based system;aplicacion medical;multivariate analysis;expert systems;electroencefalografia;quantitative eeg;rule based system;supervised classification;medical decision making;base connaissance;prise decision;classification;electroencephalographie;artificial neural networks;biomedical engineering;medical expert system;data extraction;signal processing;knowledge acquisition;base conocimiento;medical application;systeme expert;electroencephalography;reseau neuronal;toma decision;procesamiento senal;clasificacion;red neuronal;knowledge based systems;artificial neural network;neural network;application medicale;knowledge base;expert system	The knowledge acquisition problem is one of the most difficult issues in elaborating a medical expert system. This is more true in the context of automated brain signal diagnosis. This kind of knowledge does not lend itself to be represented in a classical rule-based system and is not easily put in quantitative terms by the specialists. Artificial neural networks (ANNs) provide a useful alternative for capturing this information. In this work, an application of ANNs to brain maturation prediction is presented. The problem is essentially a supervised classification. A case data base consisting of data extracted from electroencephalographic (EEG) signals and diagnoses carried out by an expert neurologist serves to test the ability of several statistical classifiers and several kinds of ANNs in reproducing the expert results. There is also a discussion on how to integrate ANNs in a higher-level knowledge-based system for brain signal interpretation.	artificial neural network	Lorenzo Moreno;José D. Piñeiro;José L. Sánchez;Soledad Mañas;Juan J. Merino;Leopoldo Acosta;Alberto F. Hamilton	1995	Neural Networks	10.1016/0893-6080(94)00111-X	electroencephalography;biological classification;computer science;artificial intelligence;machine learning;data mining;multivariate analysis;artificial neural network	ML	8.120102525402158	-30.13744586688563	83758
8da59333b089c5c1604a1ed3183504238c52b713	modeling data uncertainty on electric load forecasting based on type-2 fuzzy logic set theory	type 1 fuzzy set and type 2 fuzzy set theories;data information uncertainty;interval type 2 fuzzy sets	Real applications based on type-2 (T2) fuzzy sets are rare. The main reason is that the T2 fuzzy set theory requires massive computation and complex determination of secondary membership function. Thus most real-world applications are based on one simplified method, i.e. interval type-2 (IT2) fuzzy sets in which the secondary membership function is defined as interval sets. Consequently all computations in three-dimensional space are degenerated into calculations in two-dimensional plane, computing complexity is reduced greatly. However, ability on modeling information uncertainty is also reduced. In this paper, a novel methodology based on T2 fuzzy sets is proposed i.e. T2SDSA-FNN (Type-2 Self-Developing and Self-Adaptive Fuzzy Neural Networks). Our novelty is that (1) proposed system is based on T2 fuzzy sets, not IT2 ones; (2) it tackles one difficult problem in T2 fuzzy logic systems (FLS), i.e. massive computing time of inference so as not to be applicable to solve real world problem; and (3) membership grades on third dimensional space can be automatically determined from mining input data. The proposed method is validated in a real data set collected from Macao electric utility. Simulation and test results reveal that it has superior accuracy performance on electric forecasting problem than other techniques shown in existing literatures.	electrical load;fuzzy logic;set theory	Chin Wang Lou;Ming Chui Dong	2012	Eng. Appl. of AI	10.1016/j.engappai.2012.07.006	fuzzy logic;mathematical optimization;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	AI	1.7849346131599944	-26.18108666727334	83820
db9278fe4fc940bc0b8f287fe0ccf96668f5f119	using weighted combination-based methods in ensembles with different levels of diversity	modele agrege;classifier system;modelo agregado;classification;aggregate model;reseau neuronal;clasificacion;red neuronal;neural network	There are two main approaches to combine the output of classifiers within a multi-classifier system, which are: combination-based and selection-based methods. This paper presents an investigation of how the use of weights in some non-trainable simple combination-based methods applied to ensembles with different levels of diversity. It is aimed to analyse whether the use of weights can decrease the dependency of ensembles on the diversity of their members.		Thiago Dutra;Anne M. P. Canuto;Marcílio Carlos Pereira de Souto	2006		10.1007/11893028_79	biological classification;computer science;artificial intelligence;machine learning;artificial neural network	NLP	9.88419097887639	-32.58609471956126	83849
0e2adc7055260de7bc590abe333c5c76bdb66aa3	redundant object and dependency of domain attributes in - goverings of the universe	fuzzy relational database;redundant object;electronic mail;probability;generic model;rough set theory;conditional probability relation;universe;α redundancy;set theory;data mining;functional dependency;fuzzy set theory;domain attributes;equivalence relation;fuzzy set theory rough set theory data mining relational databases probability;fuzzy functional dependency;α coverings;knowledge discovery and data mining;decision rules;pattern recognition;rough sets;relational databases;computer science;generalization model;standard rough sets;rough set;conditional probability;decision table;fuzzy relational database redundant object domain attributes spl alpha coverings universe conditional probability relation generalization model standard rough sets knowledge discovery data mining spl alpha redundancy decision rules decision table fuzzy functional dependency;fuzzy systems;rough sets data mining relational databases set theory computer science electronic mail pattern recognition concrete fuzzy systems;decision rule;concrete;knowledge discovery	/spl alpha/-coverings of the universe induced by a conditional probability relation offers a generalization model of standard rough sets which are mainly defined by a partition induced by an equivalence relation. The paper improves the concept of /spl alpha/-coverings of the universe by some properties and applications related to knowledge discovery and data mining (KDD). A concept of /spl alpha/-redundancy of objects based on /spl alpha/-coverings of the universe is proposed in order to reduce the number of decision rules in the presence of a decision table. Moreover, an important concept of dependency of domain attributes corresponding to the concept of fuzzy functional dependency in the presence of a fuzzy relational database is also introduced.		Rolly Intan;Masao Mukaidono	2001		10.1109/FUZZ.2001.1008931	discrete mathematics;rough set;computer science;machine learning;data mining;mathematics;knowledge extraction;fuzzy control system;statistics	AI	-1.8610860018724797	-26.539007685795987	84102
0911bd274cd311fa1d186997018fed7754797010	predicting the possibilistic score of owl axioms through support vector regression		Within the context of ontology learning, we consider the problem of selecting candidate axioms through a suitable score. Focusing on subsumption axioms, this score is learned coupling support vector regression with a special similarity measure inspired by the Jaccard index and justified by semantic considerations. We show preliminary results obtained when the proposed methodology is applied to pairs of candidate OWL axioms, and compare them with an analogous inference procedure based on fuzzy membership induction.	support vector machine	Dario Malchiodi;Célia da Costa Pereira;Andrea Tettamanzi	2018		10.1007/978-3-030-00461-3_28	support vector machine;fuzzy logic;ontology learning;artificial intelligence;machine learning;computer science;jaccard index;similarity measure;axiom;inference	ML	0.23549660072924422	-27.383434606096746	84308
f4f979e8dadf6a604d616e857c1efdb2f07462fd	incremental distributed weighted class discriminant analysis on interval-valued emitter parameters	fuzzy pattern mining;distributed computing;emitter identification;incremental learning;class discriminant analysis;signal processing	In the age of big data, the emitter parameter measurement data is generally characteristic of uncertainty in the form of normally-distributed intervals, enormous size and continuous growth. However, existing interval-valued data analysis methods generally assume a uniform distribution instead and are unable to adapt to the rapid growth of volume. To address the above problems, we have brought forward an incremental distributed weighted class discriminant analysis method on interval-valued emitter parameters. Extensive experiments indicate that our method is able to cope with these new characteristics effectively.	linear discriminant analysis	Xin Xu;Wei Wang;Jiaheng Lu;Jin Chen	2015		10.1007/978-3-319-25159-2_56	computer science;machine learning;signal processing;pattern recognition	Theory	-1.0503652429160037	-36.51763148042856	84313
01d6387f5d524b18b2821dc34f973f17a859bb3a	frct: fuzzy-rough classification trees	top down method;methode descendante;encapsulation;time average;base relacional dato;fuzzy set;classification tree;decision tree;generic algorithm;fuzzy data;top down;structure arborescente;sintesis mecanismo;algoritmo borroso;rough set theory;metodo arborescente;decision borrosa;logique floue;gear;conjunto difuso;logica difusa;promedio temporal;ensemble flou;encapsulacion;decision floue;general techniques;aprendizaje por ejemplos;relational database;arbol decision;synthese mecanisme;classification;functional dependency;fuzzy sets;fuzzy logic;induccion;learning by example;induction;theorie ensemble approximatif;estructura arborescente;dependance fonctionnelle;metodo descendente;fuzzy algorithm;tree structure;algorithme flou;base de donnees relationnelle;rough sets;tree structured method;methode arborescente;engrenage;fuzzy rough sets;dependencia funcional;mechanism synthesis;fuzzy decision tree;classification accuracy;rough set;decision trees;fuzzy rough classification trees;arbre decision;clasificacion;apprentissage a partir d exemple;fuzzy decision;functional dependence;moyenne temporelle;engranaje	Using fuzzy-rough hybrids, we have proposed a measure to quantify the functional dependency of decision attribute(s) on condition attribute(s) within fuzzy data. We have shown that the proposed measure of dependency degree is a generalization of the measure proposed by Pawlak for crisp data. In this paper, this new measure of dependency degree has been encapsulated into the decision tree generation mechanism to produce fuzzy-rough classification trees (FRCT); efficient, top-down, multi-class decision tree structures geared to solving classification problems from feature-based learning examples. The developed FRCT generation algorithm has been applied to 16 real-world benchmark datasets. It is experimentally compared with the five fuzzy decision tree generation algorithms reported so far, and the rough decomposition tree algorithm. Comparison has been made in terms of number of rules, average training time, and classification accuracy. Experimental results show that the proposed algorithm to generate FRCT outperforms existing fuzzy decision tree generation techniques and rough decomposition tree induction algorithm.	algorithm;benchmark (computing);decision tree;experiment;functional dependency;fuzzy logic;list of algorithms;qr decomposition;rough set;top-down and bottom-up design	Rajen B. Bhatt;M. Gopal	2007	Pattern Analysis and Applications	10.1007/s10044-007-0080-z	rough set;decision tree learning;classification tree method;computer science;artificial intelligence;machine learning;incremental decision tree;mathematics;id3 algorithm;algorithm	AI	8.272828777734096	-33.00174804558619	84358
e8162d12a7eb0978684271c142e0095bbea0e508	regression analysis for massive datasets	journal collection;期刊論文;minimum variance;best linear unbiased estimate;考試論文;optimal weight;weighted averaging;information technology;機構典藏;data analysis methods;論文;碩博士論文;report;研究計畫;thesis;ir;教育培訓;論文課件;best linear unbiased estimator;考古題;校園高校;simulation study;regression analysis;博碩士論文;massive datasets;pass exam;article	In the past decades, we have witnessed a revolution in information technology. Routine collection of systematically generated data is now commonplace. Databases with hundreds of fields (variables), and billions of records (observations) are not unusual. This presents a difficulty for classical data analysis methods, mainly due to the limitation of computer memory and computational costs (in time, for example). In this paper, we propose an intelligent regression analysis methodology which is suitable for modeling massive datasets. The basic idea here is to split the entire dataset into several blocks, applying the classical regression techniques for data in each block, and finally combining these regression results via weighted averages. Theoretical justification of the goodness of the proposed method is given, and empirical performance based on extensive simulation study is discussed.		Tsai-Hung Fan;Dennis K. J. Lin;Kuang-Fu Cheng	2007	Data Knowl. Eng.	10.1016/j.datak.2006.06.017	minimum-variance unbiased estimator;best linear unbiased prediction;computer science;data mining;database;data analysis;information technology;regression analysis	ML	2.096530752461858	-34.40778019835332	84449
20aaefcb7eb3cdacf32b4341d76fe382cec3f787	imprecise data classification based on fuzzy logic and possibility theory			fuzzy logic;possibility theory	Isabela Drummond	2007				AI	2.0177675858769852	-25.694876639242615	84506
b6f9ff686b05196f004ac6477fb5ab9826ca053e	the evolution of causal models: a comparison of bayesian metrics and structure priors	eficacia sistema;sample size;sistema experto;information retrieval;decision bayes;performance systeme;causal discovery;intelligence artificielle;bayes decision;algoritmo genetico;data mining;system performance;evaluation metric;minimum message length;recherche information;algorithme genetique;artificial intelligence;genetic algorithm;inteligencia artificial;recuperacion informacion;systeme gestion base donnee;information system;systeme expert;causal models;sistema gestion base datos;database management system;systeme information;kullback leibler distance;sistema informacion;expert system	We report the use genetic algorithms (GAs) as a search mechanism for the discovery of linear causal models when using two Bayesian metrics for linear causal models, a Minimum Message Length (MML) metric [10] and a full posterior analysis (BGe) [3]. We also consider two structure priors over causal models, one giving all variable orderings for models with the same arc density equal prior probability (P1) and one assigning all causal structures with the same arc density equal priors (P2). Evaluated with Kullback-Leibler distance prior P2 tended to produce models closer to the true model than P1 for both metrics, with MML performing slightly better than BGe. By contrast, when using an evaluation metric that better reflects the nature of the causal discovery task, namely a metric that compares the results of predictive performance on the effect nodes in the discovered model P1 outperformed P2 in general, with MML and BGe discovering models of similar predictive performance at various sample sizes. This supports our conjecture that the P1 prior is more appropriate for causal discovery.	causal filter	Julian R. Neil;Kevin B. Korb	1999		10.1007/3-540-48912-6_57	sample size determination;genetic algorithm;computer science;artificial intelligence;minimum message length;data mining;mathematics;kullback–leibler divergence;expert system;information system;statistics;causal model	Vision	9.792400953423403	-34.98681070904399	84591
9ffca8095030620af8b3e00606113ef4eee13a2f	discovering fuzzy association rules using fuzzy partition methods	association rules;relational database;data mining;human subjects;association rule;natural language;article;fuzzy system;fuzzy partition;fuzzy association rules	Fuzzy association rules described by the natural language are well suited for the thinking of human subjects and will help to increase the flexibility for supporting users in making decisions or designing the fuzzy systems. In this paper, a new algorithm named fuzzy grids based rules mining algorithm (FGBRMA) is proposed to generate fuzzy association rules from a relational database. The proposed algorithm consists of two phases: one to generate the large fuzzy grids, and the other to generate the fuzzy association rules. A numerical example is presented to illustrate a detailed process for finding the fuzzy association rules from a specified database, demonstrating the effectiveness of the proposed algorithm. q 2002 Elsevier Science B.V. All rights reserved.	algorithm;association rule learning;fuzzy control system;natural language;numerical analysis;relational database	Yi-Chung Hu;Ruey-Shun Chen;Gwo-Hshiung Tzeng	2003	Knowl.-Based Syst.	10.1016/S0950-7051(02)00079-5	fuzzy logic;fuzzy cognitive map;association rule learning;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	1.9254095163214664	-27.909335923724964	84807
08b635103e96d4d83f6acfa5092e555b9f2420d5	an unsupervised k-means based clustering method for geophysical post-earthquake diagnosis		After an earthquake takes place, the geophysical problem to solve has always been to determine the geodynamical behaviour of the rupture zone. Nonetheless, such geodynamical process is usually a result of the contribution of more than one enchained tectonic process. At this point, the geophysical clustering problem turns in a complex task from a pattern recognition approach, due to the dynamical behaviour of the problem to solve. In this regard, the use of unsupervised techniques is mandatory. Such techniques, however, present three important drawbacks for their application to the addressed problem: (1) the identification of the number of clusters and their spatial localization from a geophysical point of view, (2) the geodynamical sense of the performed clusters, and (3) the statistical dependence between features. To solve such limitations we developed the GEO K-means algorithm by incorporating precursor contextual information within the clustering process. In this respect, our study provides a novel approach to scientists for a more accurate post-earthquake diagnosis about the geodynamical behaviour of the affected area.	algorithm;cluster analysis;dynamical system;k-means clustering;pattern recognition;steam rupture;unsupervised learning	Fernando Mato;Theofilos Toulkeridis	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285216	geophysics;cluster analysis;k-means clustering;tectonics;mathematics	Vision	3.0948225936276335	-35.424836037444635	84886
2596df292ba0242ad941c9a1cb38d1338bfcd280	a bipolar view on medical diagnosis in ovaexpert system		In the paper we present OvaExpert a unique tool for supporting gynecologists in the diagnosis of ovarian tumor, combining classical diagnostic scales with modern methods of machine learning and soft computing. A distinguishing feature of the system is its comprehensiveness, which makes it usable at any stage of a diagnostic process. We gather all the results and solutions making up the system, some of which were described in our other publications, to provide an overall picture of OvaExpert and its capabilities. A special attention is paid to a property of supporting uncertainty modeling and processing, that is an essential part of the system.	canonical account;computational intelligence;decision theory;fuzzy rule;logic programming;machine learning;microsoft research;soft computing	Anna Stachowiak;Krzysztof Dyczkowski;Andrzej Wójtowicz;Patryk Zywica;Maciej Wygralak	2015		10.1007/978-3-319-26154-6_37	data mining;medical diagnosis;information retrieval;usable;uncertainty modeling;computer science;soft computing	AI	2.3985422018965337	-26.792947802823257	84955
2138c633f3040ec782b61aaaeb262d68d228e5a7	an extension to fuzzy support vector data description (fsvdd*)	to real data the experimental results show the ability of the proposed method in taiwanese tea evaluation;fuzzy numbers;fuzzy svdd;data description;the fuzzy svdd cannot be utilized in this paper;we extend the fuzzy svdd for the description of such training samples and then apply our proposed method;the well known support vector data description svdd is based on precise description of precise data when we know the features of training samples precisely and we are uncertain about their class labels;the fuzzy svdd can be used to obtain the data description but if the features of training samples are fuzzy numbers;called fsvdd;defuzzification;distance	The well-known support vector data description (SVDD) is based on precise description of precise data. When we know the features of training samples precisely and we are uncertain about their class labels, the fuzzy SVDD can be used to obtain the data description. But if the features of training samples are fuzzy numbers, the fuzzy SVDD cannot be utilized. In this paper, we extend the fuzzy SVDD for the description of such training samples and then apply our proposed method, called FSVDD*, to real data. The experimental results show the ability of the proposed method in Taiwanese tea evaluation.	fuzzy number;whole earth 'lectronic link	Yahya Forghani;Hadi Sadoghi Yazdi;Sohrab Effati	2011	Pattern Analysis and Applications	10.1007/s10044-011-0208-z	defuzzification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;distance;fuzzy set operations	AI	0.5864935020718858	-26.82814930993032	84960
b7426f0c5f36a8d9910195a114ad8b45f6e45353	fusing biomedical multi-modal data for exploratory data analysis	fondo marino;microarray data;fond marin;clinical data;texture;affichage;self organizing maps;pulga de dna;navegacion informacion;analyse exploratoire;aplicacion medical;image processing;neural networks;visualizacion;analisis datos;genie biomedical;puce a dna;navigation information;sous eau;information browsing;procesamiento imagen;bioinformatique;exploratory analysis;vertebrata;information visualization;underwater;data fusion;data mining;traitement image;data analysis;visualization;complex data;biomedical engineering;display;visualisation;visual inspection;fusion donnee;pisces;textura;subacueo;dna chip;visual control;controle visuel;analyse donnee;self organized map;ingenieria biomedica;medical application;bioinformatica;reseau neuronal;fusion datos;multi modal data;imagen color;semantic data integration;ocean floors;categorical data;red neuronal;image couleur;control visual;donnee categorielle;color image;dato categorico;analisis exploratorio;exploratory data analysis;neural network;application medicale;bioinformatics	Data analysis in modern biomedical research has to integrate data from different sources, like microarray, clinical and categorical data, so called multi-modal data. The reef SOM, a metaphoric display, is applied and further improved such that it allows the simultaneous display of biomedical multi-modal data for an exploratory analysis. Visualizations of microarray, clinical, and category data are combined in one informative and entertaining image. The U-matrix of the SOM trained on microarray data is visualized as an underwater sea bed using color and texture. The clinical data and category data are integrated in the form of fish shaped glyphs. The resulting images are intuitive, entertaining and can easily be interpreted by the biomedical collaborator, since specific knowledge about the SOM algorithm is not required. Visual inspection enables the detection of interesting structural patterns in the multi-modal data when browsing through and zooming into the image. Results of such an analysis are presented for the van’t Veer data set. keywords: data mining, exploratory data analysis, semantic data integration, information visualization, self organizing maps, neural networks, multi-modal data, complex data	algorithm;artificial neural network;categorical variable;data mining;exploratory testing;glyph;information visualization;microarray;modal logic;organizing (structure);self-organization;self-organizing map;structural pattern;u-matrix;visual inspection	Christian W. Martin;Harmen grosse Deters;Tim W. Nattkemper	2006		10.1007/11840930_83	computer vision;visualization;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	6.83548998204703	-33.09420317212839	84969
f15d0f5066539ac131b1bf4d025248871a3a07ac	self-adaptive fuzzification in fuzzy decision tree induction	som technology;self adaptive fuzzification algorithm;triangular membership functions;training;uci data set self adaptive fuzzification algorithm fuzzy decision tree induction learning self organizing map technology som technology triangular membership functions selected fuzzy decision tree heuristic algorithm;fuzzy decision tree induction learning;fuzzy set theory;fuzzy sets;triangular membership function;accuracy;machine learning;self organising feature maps;self organizing map technology;selected fuzzy decision tree heuristic algorithm;classification algorithms;self organizing map;membership function;uci data set;fuzzification;self organized map;decision trees accuracy training neurons classification algorithms machine learning fuzzy sets;self organizing map fuzzy decision tree fuzzification triangular membership function;neurons;fuzzy decision tree;learning artificial intelligence;self organising feature maps decision trees fuzzy set theory learning artificial intelligence;decision trees;heuristic algorithm	One of the most important issues in fuzzy decision tree learning is the fuzzification of input data. This paper proposes a self-adaptive data fuzzification algorithm based on the self-organizing map (SOM) technology, which can automatically determine the number and coordinates of centers in triangular membership functions. Then the membership degree of each sample to all fuzzy subsets can be calculated. Finally, a fuzzy decision tree can be learned from the fuzzified training samples by any selected fuzzy decision tree heuristic algorithm. Experimental results on UCI data set iris show that the new approach outperform the traditional fuzzification methods.	algorithm;decision tree learning;fuzzy set;heuristic (computer science);organizing (structure);self-organization;self-organizing map	Xiao-Dong Dai;Lin-Qing Gao;Chunru Dong	2010	2010 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2010.5581048	statistical classification;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;linear partial information;pattern recognition;incremental decision tree;data mining;mathematics;fuzzy set;fuzzy set operations	Robotics	4.808178339680328	-35.24759317274	85143
6409e5d247378bbe4fdd9e2d5f8431c8fc1454b5	reducing rankings of classifiers by eliminating redundant classifiers	prediction method;analisis datos;metodo reduccion;redundancia;classification supervisee;supervised classification;intelligence artificielle;data analysis;redundancy;clasificacion supervisada;ranking algorithm;artificial intelligence;analyse donnee;methode reduction;inteligencia artificial;reduction method;redondance	Several methods have been proposed to generate rankings of supervised classification algorithms based on their previous performance on other datasets [8,4]. Like any other prediction method, ranking methods will sometimes err, for instance, they may not rank the best algorithm in the first position. Often the user is willing to try more than one algorithm to increase the possibility of identifying the best one. The information provided in the ranking methods mentioned is not quite adequate for this purpose. That is, they do not identify those algorithms in the ranking that have reasonable possibility of performing best. In this paper, we describe a method for that purpose. We compare our method to the strategy of executing all algorithms and to a very simple reduction method, consisting of running the top three algorithms. In all this work we take time as well as accuracy into account. As expected, our method performs better than the simple reduction method and shows a more stable behavior than running all algorithms.		Pavel Brazdil;Carlos Soares;Rui Pereira	2001		10.1007/3-540-45329-6_5	computer science;artificial intelligence;machine learning;data mining;redundancy;data analysis	NLP	8.156255454087699	-33.09548963292666	85237
765305f72d5aaa645ab4d121dd40cf5a93d0fb43	knowledge acquisition via incremental conceptual clustering	inference;incremental conceptual clustering;hill climbing;conceptual clustering;incremental learning;knowledge acquisition;concept formation	Conceptual clustering is an important way of summarizing and explaining data. However, the recent formulation of this paradigm has allowed little exploration of conceptual clustering as a means of improving performance. Furthermore, previous work in conceptual clustering has not explicitly dealt with constraints imposed by real world environments. This article presents COBWEB, a conceptual clustering system that organizes data so as to maximize inference ability. Additionally, COBWEB is incremental and computationally economical, and thus can be flexibly applied in a variety of domains.	cluster analysis;conceptual clustering;knowledge acquisition;programming paradigm	Douglas H. Fisher	1987	Machine Learning	10.1007/BF00114265	concept learning;computer science;artificial intelligence;hill climbing;machine learning;data mining;conceptual clustering	AI	6.6248885396253545	-32.308885956484744	85575
761c4c3bbf1eda25c647d9bd436cd5a5dddc60bb	computing efficient features using rough set theory combined with ensemble classification techniques to improve the customer churn prediction in telecommunication sector	churn prediction;telecommunication;rough set theory;feature selection;ensemble classification;bagging;boosting;random subspace;00-general;03-mathematical logic and foundations;41-approximations and expansions	Rough set theory (RST) can be viewed as one of the classical set theory for handling with imprecision knowledge. The theory has discovered applications in numerous areas, for example, engineering, industries, environment and others. Churn in telecommunication sector, customer switching from one service provider to another. Predicting telecom customer churn is challenging due to the huge and inconsistent nature of the data. Churn prediction is crucial for telecommunication companies in order to build an efficient customer retention plan and apply successful marketing strategies. In this article, a methodology is proposed using RST to identify the efficient features for telecommunication customer churn prediction. Then the selected features are given to the ensemble-classification techniques such as Bagging, Boosting, Random Subspace. In this work the duke university-churn prediction data set is considered for performance evaluation and three sets of experiments are performed. Finally the performance of the proposed model is evaluated based on the following metrics such as true churn, false churn, specificity, precision and accuracy and it is identified that Proposed system designed with combining attribute selection with ensemble classification techniques works fine with classification accuracy of 95.13% compared to any single model.	backward induction;boosting (machine learning);bootstrap aggregating;discretization;ensemble learning;experiment;feature selection;information;intel matrix raid;kullback–leibler divergence;mandelbrot set;performance evaluation;plasma cleaning;preprocessor;rough set;sampling (signal processing);sensitivity and specificity;set theory;telecommunications network	J. Vijaya;E. Sivasankar	2018	Computing	10.1007/s00607-018-0633-6	boosting (machine learning);service provider;mathematics;feature selection;customer retention;accuracy and precision;subspace topology;telecommunications;set theory;rough set	ML	3.6083724897482554	-36.13945241325316	85979
2445091c33ff1269474d8bb062883497f8ef4b1d	active learning: applying rinscut thresholding strategy to uncertainty sampling	unlabeled data;text;haute performance;learning algorithm;threshold detection;supervised learning;incertidumbre;uncertainty;random sampling;active learning;hombre;intelligence artificielle;texte;algorithme apprentissage;probabilistic approach;classification;text classification;detection seuil;systeme incertain;deteccion umbral;enfoque probabilista;approche probabiliste;muestreo aleatorio;human;alto rendimiento;artificial intelligence;incertitude;inteligencia artificial;apprentissage supervise;sistema incierto;uncertain data;texto;aprendizaje supervisado;echantillonnage aleatoire;algoritmo aprendizaje;high performance;uncertain system;clasificacion;homme	In many supervised learning approaches to text classification, it is necessary to have a large volume of manually labeled documents to achieve a high level of performance. This manual labeling process is time-consuming, expensive, and will have some level of inconsistency. Two common approaches to reduce the amount of expensive labeled examples are: (1) selecting informative uncertain examples for human-labeling, rather than relying on random sampling, and (2) using many inexpensive unlabeled data with a small number of manually labeled examples. Previous research has been focused on a single approach and has shown the considerable reduction on the amount of labeled examples to achieve a given level of performance. In this paper, we investigate a new framework to combine both approaches for similarity-based text classification. By applying our new thresholding strategy, RinSCut, to the conventional uncertainty sampling, we propose a new framework which automatically selects informative uncertain data that should be presented to human expert for labeling and positive-certain data that could be directly used for learning without human-labeling. Extensive experiments have been conducted on Reuters-21578 dataset to compare our proposed scheme with random sampling and conventional uncertainty sampling schemes, based on micro and macro-averaged F 1 . The results showed that if both macro and micro-averaged measures are concerned, the optimal choice might be our new approach.		Kang Hyuk Lee;Judy Kay;Byeong Ho Kang	2003		10.1007/978-3-540-24581-0_79	semi-supervised learning;sampling;uncertainty;biological classification;computer science;artificial intelligence;machine learning;data mining;active learning;supervised learning;statistics	Vision	7.724254478846613	-31.995227204588552	86124
4b50806552865523f7ca89ac968f38ead861ee38	introduction to feature selection for atmospheric quality parameters forecasting	dimensionality reduction;feature extraction;pattern classification data mining feature selection forecasting theory knowledge management;classification algorithms principal component analysis feature extraction taxonomy training numerical models decision trees;feature selection;feature extraction dimensionality reduction feature selection;classification accuracy feature selection atmospheric quality parameters forecasting knowledge management dimensionality reduction data mining	Knowledge is only valuable when it can be used efficiently and effectively, therefore knowledge management is increasingly being recognized as a key element in extracting its value. Feature selection and dimensionality reduction can be used for that purpose, in order to reduce the time required to perform data mining and to increase the resulting classification accuracy. This paper presents an introduction to some of these algorithms that can be used to forecast atmospheric quality parameters.	algorithm;data mining;dimensionality reduction;emoticon;feature selection;heuristic;knowledge management;run time (program lifecycle phase)	George Papadourakis;Ioannis Kyriakidis	2015	2015 11th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)	10.1109/SITIS.2015.23	feature vector;feature extraction;computer science;machine learning;pattern recognition;data mining;feature selection;k-nearest neighbors algorithm;feature;dimensionality reduction	Robotics	2.6046085186347034	-33.44434581491821	86180
4762218a82f70fbd6bd4c9456eb80ddc63923d48	an approach to grid scheduling optimization based on fuzzy association rule mining	grid scheduling;fuzzy association rule mining;fuzzy set;association patterns;optimization technique;quantitative attribute value grid scheduling optimization fuzzy association rule mining knowledge discovery grid monitoring association patterns optimization logic data mining fuzzy sets;time window;grid monitoring;data mining;fuzzy set theory;fuzzy sets;association rule mining;optimization logic;association rules data mining monitoring processor scheduling grid computing fuzzy sets dynamic scheduling fuzzy logic delay stochastic systems;quantitative attribute value;scheduling;scheduling data mining fuzzy set theory grid computing;grid computing;grid scheduling optimization;knowledge discovery;fuzzy association rules	This paper presents a grid scheduling optimization technique based on knowledge discovery. The main idea is to transform the grid monitoring data into a performance data set, extract the association patterns of performance data through fuzzy association rule mining, then construct optimization logic according to the mining results, and finally optimize the grid scheduling. In the process of data mining, a method of association rule mining is proposed based on time-window and fuzzy set concepts, which can mine data for quantitative attribute value based on the attribute and time dimensions in grid performance data set	algorithm;association rule learning;data mining;fuzzy set;mathematical optimization;scheduling (computing);supercomputer	Jin Huang;Hai Jin;Xia Xie;Qin Zhang	2005	First International Conference on e-Science and Grid Computing (e-Science'05)	10.1109/E-SCIENCE.2005.16	computer science;operating system;machine learning;data mining;database;fuzzy set;knowledge extraction	HPC	-1.795496893056045	-27.939119007010753	86198
10ab74c48a61fa6cce0b163e426a696fd3aa5c4d	a general definition of an attribute reduct	evaluation function;monotonicity of evaluation function;attribute reducts;property preservation functions;attribute reduction	A reduct is a subset of attributes that are jointly sufficient and individually necessary for preserving a particular property of a given information table. A general definition of an attribute reduct is presented. Specifically, we discuss the following issues: First, there are a variety of properties that can be observed in an information table. Second, the preservation of a certain property by an attribute set can be evaluated by different measures, defined as different fitness functions. Third, by considering the monotonicity property of a particular fitness function, the reduct construction method needs to be carefully examined. By adopting different heuristics or fitness functions for preserving a certain property, one is able to derive most of the existing definitions of a reduct. The analysis brings new insight into the problem of reduct construction, and provides guidelines for the design of new algorithms.	algorithm;attribute-value system;evaluation function;fitness function;heuristic (computer science);subject reduction	Yan Zhao;Feng Luo;S. K. Michael Wong;Yiyu Yao	2007		10.1007/978-3-540-72458-2_12	discrete mathematics;computer science;artificial intelligence;evaluation function;data mining;reduct;mathematics;algorithm	DB	-3.3233231690511826	-27.098040646465943	86362
6030141bfd7b94f827001833e09a7450f51c1ed4	decreasing excess fuzziness in fuzzy outputs from neural networks for linguistic rule extraction	fuzzy set theory fuzzy neural nets feedforward neural nets learning artificial intelligence knowledge acquisition pattern classification;feedforward neural network;intelligent networks fuzzy neural networks neural networks arithmetic pattern classification industrial engineering electronic mail multi layer neural network computer simulation input variables;fuzzy neural nets;electronic mail;fuzziness;neural networks;fuzzy number;input variables;multilayer feedforward neural network;linguistic rule extraction;rule extraction;fuzzy set theory;multi layer neural network;fuzzy arithmetic;knowledge acquisition;pattern classification;arithmetic;fuzzy set theory fuzziness fuzzy arithmetic linguistic rule extraction pattern classification feedforward neural network;feedforward neural nets;intelligent networks;learning artificial intelligence;fuzzy neural networks;computer simulation;industrial engineering;neural network	"""Ishibuchi et al. (1996) have proposed a linguistic rule extraction method from trained neural networks for pattern classification problems. In the method, antecedent linguistic values such as """"small"""" and """"large"""" are used as inputs to a multilayer feedforward neural network for determining the consequent part of linguistic rules. Since the linguistic input values are handled as fuzzy numbers, the corresponding outputs from the neural network are also calculated as fuzzy numbers by fuzzy arithmetic. The accurate calculation of the fuzzy outputs is very important because the determination of the consequent part is based on the calculated fuzzy outputs. It is, however, well-known that fuzzy arithmetic involves excess fuzziness. In this paper, we illustrate how subdivision methods can decrease the excess fuzziness. We also examine the effect of those methods on the performance of our rule extraction method."""	artificial neural network;offset binary;rule induction	Hisao Ishibuchi;Manabu Nii;Kimiko Tanaka	1999		10.1109/IJCNN.1999.830842	fuzzy logic;feedforward neural network;intelligent network;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy set;fuzzy set operations;artificial neural network	ML	5.549464364690354	-27.623115421692706	86367
42fafac3307158492cffb0f08a0a16181c7beb9f	fingerprint recognition using the fuzzy sugeno integral for response integration in modular neural networks	fuzzy sugeno integral;fuzzy logic;modular neural networks;fingerprint recognition;pattern recognition;modular neural network;fuzzy logic method;sugeno integral	We describe in this paper a new approach for pattern recognition using modular neural networks with a fuzzy logic method for response integration. We proposed a new architecture for modular neural networks for achieving pattern recognition in the particular case of human fingerprints. Also, the method for achieving response integration is based on the fuzzy Sugeno integral. Response integration is required to combine the outputs of all the modules in the modular network. We have applied the new approach for fingerprint recognition with a real database of fingerprints from students of our institution.	artificial neural network;fingerprint recognition;modular neural network;sugeno integral	Patricia Melin;Diana Bravo;Oscar Castillo	2008	Int. J. General Systems	10.1080/03081070701321910	fuzzy logic;computer science;artificial intelligence;neuro-fuzzy;machine learning;mathematics;fingerprint recognition	Robotics	4.773243176934966	-26.39794563912435	86588
4a0856cb4373d0c42c9981715da2e60015181b58	a comparative study on sufficient conditions for takagi-sugeno fuzzy systems as universal approximators	fuzzy set theory takagi sugeno systems fuzzy systems universal approximation sufficient conditions rule consequent;fuzzy set;takagi sugeno;sufficient conditions takagi sugeno model fuzzy systems input variables fuzzy sets fuzzy control hybrid intelligent systems automation control systems;approximation theory fuzzy systems fuzzy set theory function approximation;indexing terms;fuzzy set theory;linear functionals;approximation theory;function approximation;comparative study;fuzzy systems;fuzzy system	Universal approximation is the basis of theoretical research and practical applications of fuzzy systems. Studies on the universal approximation capability of fuzzy systems have achieved great progress in recent years. In this paper, linear Takagi–Sugeno (T–S) fuzzy systems that use linear functions of input variables as rule consequent and their special case named simplified fuzzy systems that use fuzzy singletons as rule consequent are investigated. On condition that overlapped fuzzy sets are employed, new sufficient conditions for simplified fuzzy systems and linear T–S fuzzy systems as universal approximators are given, respectively. Then, a comparative study on existing sufficient conditions is carried out with numeric examples.	fuzzy control system;fuzzy set;linear function;universal approximation theorem	Ke Zeng;Nai-Yao Zhang;Wen-Li Xu	2000	IEEE Trans. Fuzzy Systems	10.1109/91.890337	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;control theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	5.044568763103964	-27.385604569982554	86675
31ddcb5137e4d3dcdd0ac0f11ff77954fa2e0998	comparative study on null boundary and periodic boundary 3-neighborhood multiple attractor cellular automata for classification	implicit memory;null boundary cellular automata nbca;boolean functions;maca null boundary periodic boundary cellular automata 3 neighborhood multiple attractor cellular automata classification technique pattern recognition pattern generation testing field fault diagnosis linear additive ca xor logic xnor logic nonlinear ca and logic or logic not logic rule vector graph linear time algorithms;boundary conditions;pattern search;pattern generation;linear time algorithm;polynomials;automata;rule vector graph rvg;pattern classification cellular automata formal logic;vectors;logic vectors educational institutions distributed computing poles and towers pattern recognition automatic test pattern generation test pattern generators automatic testing fault diagnosis;classification algorithms;comparative study;pattern classification;formal logic;pattern recognition;periodic boundary cellular automata pbca;cellular automata;theoretical foundation;rule vector graph rvg null boundary cellular automata nbca periodic boundary cellular automata pbca state transition;state transition;fault diagnosis	This paper reports a generic analysis on null boundary and periodic boundary 3-neighborhood multiple attractor cellular automata (MACA) for showing the comparative study in classification technique. Cellular automata (CA) is now-a-days an essential tool for researchers in the area of pattern recognition, pattern generation, testing field, fault diagnosis and so on. So, general knowledge on CA up to some extent is a must for researchers in these areas. A CA may be linear or non-linear in behavior. A linear/additive CA employs XOR/XNOR logic, while a non-linear CA employs AND/OR/NOT logic. This paper shows a graph analysis along with state transition behavior of CA cells. A rule vector graph (RVG) is generated from the rule vector (RV) of a CA. Linear time algorithms are reported for generation of RVG. MACA provides an implicit memory to store the patterns. Search operation to identify the class of a pattern out of several classes boils down to running a CA for one time step. This demands storage of RV and seed values. MACA is based on sound theoretical foundation of CA technology. This paper only concentrates on MACA since it is responsible for classifying the various types of patterns.	algorithm;automata theory;cellular automaton;exclusive or;nonlinear system;pattern recognition;state transition table;time complexity;utility functions on indivisible goods;xnor gate	Anirban Kundu;Alok Ranjan Pal;Tanay Sarkar;Moutan Banerjee;Sutirtha Kr. Guha;Debajyoti Mukhopadhyay	2008	2008 Third International Conference on Digital Information Management	10.1109/ICDIM.2008.4746805	statistical classification;cellular automaton;pattern search;boundary value problem;computer science;artificial intelligence;theoretical computer science;machine learning;comparative research;automaton;boolean function;logic;algorithm;polynomial;implicit memory	Robotics	3.532925305581361	-31.65140889879209	86754
350bafeb5c5f78903ea47383153990469d666c8a	credit scoring using artificial immune system algorithms: a comparative study	optimisation finance;immunos artificial immune system credit scoring classification airs clonalg;optimisation;credit industry;credit scoring;decision tree;artificial immune system;finance;statistical machine learning;airs;artificial immune systems machine learning algorithms artificial intelligence credit cards statistics machine learning artificial neural networks rough sets decision trees immune system;classification;artificial intelligent;accuracy;artificial neural networks;logistics;natural immune system processes;clonalg;classification algorithms;comparative study;immune system;immunos credit scoring artificial immune system algorithms credit industry artificial neural networks rough sets decision trees natural immune system processes airs clonalg;artificial immune system algorithms;rough sets;rough set;decision trees;artificial immune systems;algorithm design and analysis;credit cards;immunos;artificial neural network	Credit scoring has become a very important task in the credit industry and its use has increased at a phenomenal speed through the mass issue of credit cards since the 1960s. Credit scoring models have been widely studied in the areas of statistics, machine learning, and artificial intelligence (AI). Many novel approaches such as artificial neural networks (ANNs), rough sets, or decision trees have been proposed to increase the accuracy of credit scoring models. Artificial Immune Systems (AIS) which are algorithm developed with inspiration from natural immune system processes have been used to solve various kinds of real life processes with success. Various AIS algorithms like AIRS, CLONALG, Immunos etc have been proposed. This paper explores the possibility of application of various Artificial Immune System Algorithms to credit scoring problem and compares the results with other methodologies. Experiments are done against two benchmark data sets and results presented with respect to other algorithms to help credit analysts chose from various methodologies.	algorithm;artificial immune system;artificial intelligence;artificial neural network;benchmark (computing);decision tree;machine learning;real life;rough set	Antariksha Bhaduri	2009	2009 World Congress on Nature & Biologically Inspired Computing (NaBIC)	10.1109/NABIC.2009.5393671	rough set;computer science;artificial intelligence;machine learning;decision tree;data mining;artificial immune system;artificial neural network	AI	4.978290881903491	-29.280341627626708	86887
c1ed0fe988752ea423bc5d3035ec975e43021b6e	decision trees and reducts for distributed decision tables	relative reducts;greedy algorithms;distributed decision tables;decision trees	In the paper greedy algorithms for construction of decision trees and relative reducts for joint decision table generated by distributed decision tables are studied. Two ways for definition of joint decision table are considered: based on the assumption that the universe of joint table is the intersection of universes of distributed tables, and based on the assumption that the universe of joint table is the union of universes of distributed tables. Furthermore, a case is considered when the information about distributed decision tables is given in the form of decision rule systems.	decision table;decision tree;greedy algorithm	Mikhail Ju. Moshkov	2004		10.1007/3-540-32370-8_17	decision rule;data mining;decision table;decision tree;mathematics;greedy algorithm	AI	-1.7628024127849073	-26.548477074823854	86890
bd314c55967fd0b618da02dc4deaf44e46d7bf05	generating rare association rules using the minimal rare itemsets family		Rare association rules correspond to rare, or infrequent, itemsets, as opposed to frequent ones that are targeted by conventional pattern miners. Rare rules reflect regularities of local, rather than global, scope that can nevertheless provide valuable insights to an expert, especially in areas such as genetics and medical diagnosis where some specific deviations/illnesses occur only in a small number of cases. The work presented here is motivated by the long-standing open question of efficiently mining strong rare rules, i.e., rules with high confidence and low support. We also propose an efficient solution for finding the set of minimal rare itemsets. This set serves as a basis for generating rare association rules.		Laszlo Szathmary;Petko Valtchev;Amedeo Napoli	2010	Int. J. Software and Informatics		data mining;medical diagnosis;computer science;association rule learning;small number	SE	-1.740673529831608	-34.88412349146323	86922
629d32dbed551458d7b109629e5e2c37f87d22fe	further experimentation with hybrid immune inspired network intrusion detection	artificial immune system;network monitoring;anomaly detection;hybrid model;network intrusion detection	  This paper presents continued experimentation on the Network Threat Recognition with Immune Inspired Anomaly Detection, or  NetTRIIAD, model. This hybrid model combines established network monitoring methods with artificial immune system methods  to achieve improved performance. The paper presets experiments investigating the model’s performance in detecting novel threats  and the performance contribution of the individual components.    		Robert L. Fanelli	2010		10.1007/978-3-642-14547-6_21	anomaly-based intrusion detection system;anomaly detection;computer science;machine learning;network monitoring;artificial immune system	Security	7.004618026991229	-37.031156486088555	87210
176c93988fb8dbb23ec517926778833dba8ac3a6	corrective feedback and persistent learning for information extraction	extraction information;modelizacion;graphic method;feedback mechanism;unfolding;filtering;ajustamiento modelo;filtrage;base donnee;learning algorithm;mise a jour;electronic mail;sistema activo;statistical machine learning;information extraction;deploiement;bepress selected works;active learning;extraction forme;filtrado;information extraction active learning graphical models;database;despliegue;base dato;correo electronico;intelligence artificielle;aprendizaje probabilidades;algorithme apprentissage;probabilistic approach;classification;systeme actif;active system;actualizacion;ajustement modele;modelisation;graphical models;real world application;methode graphique;spam filtering;extraccion forma;enfoque probabilista;approche probabiliste;model matching;empirical validation;inferencia;graphical model;metodo grafico;apprentissage probabilites;artificial intelligence;inteligencia artificial;information system;algoritmo aprendizaje;modeling;pattern extraction;clasificacion;extraccion informacion;systeme information;inference;probability learning;updating;courriel;sistema informacion	To successfully embed statistical machine learning models in real world applications, two post-deployment capabilities must be provided: (1) the ability to solicit user corrections and (2) the ability to update the model from these corrections. We refer to the former capability as corrective feedback and the latter as persistent learning. While these capabilities have a natural implementation for simple classification tasks such as spam filtering, we argue that a more careful design is required for structured classification tasks. One example of a structured classification task is information extraction, in which raw text is analyzed to automatically populate a database. In this work, we augment a probabilistic information extraction system with corrective feedback and persistent learning components to assist the user in building, correcting, and updating the extraction model. We describe methods of guiding the user to incorrect predictions, suggesting the most informative fields to correct, and incorporating corrections into the inference algorithm. We also present an active learning framework that minimizes not only how many examples a user must label, but also how difficult each example is to label. We empirically validate each of the technical components in simulation and quantify the user effort saved. We conclude that more efficient corrective feedback mechanisms lead to more effective persistent learning.	algorithm;email filtering;information extraction;machine learning;population;simulation;software deployment;statistical classification	Aron Culotta;Trausti T. Kristjansson;Andrew McCallum;Paul A. Viola	2006	Artif. Intell.	10.1016/j.artint.2006.08.001	simulation;computer science;artificial intelligence;machine learning;graphical model;information extraction	AI	7.61095310971123	-31.919974629919558	87435
03a55e458135351e0d7a1533fc346c946c9782e4	st-alphabets: on the feasibility in the explicit use of extended relational alphabets in classifier systems	classifier system;learning classifier system;gene expression;information gain;neural network	It is proposed a way of increasing the cardinality of an alphabet used to write rules in a learning classifier system that extends the idea of relational schemata. Theoretical justifications regarding the possible reduction in the amount of rules for the solution of problems such extended alphabets ( st  -alphabets) imply are shown. It is shown that when expressed as bipolar neural networks, the matching process of rules over  st  -alphabets strongly resembles a gene expression mechanism applied to a system over {0,1,#}. In spite of the apparent drawbacks the explicit use of such relational alphabets would imply, their successful implementation in an information gain based classifier system (IGCS) is presented.		Carlos D. Toledo-Suárez	2009		10.1007/978-3-642-05258-3_41	gene expression;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;learning classifier system;kullback–leibler divergence;artificial neural network;algorithm	Logic	5.264602864075186	-30.755100451901534	87823
cca9160bc4a590d89a984e8e7a30af7e60018684	efficient neural network models and information theoretic feature extraction in improved e-mail spam categorization	multilayer perceptron;information theory;feature extraction;neural network model;naive bayes;linear model		categorization;feature extraction	Vasilios Zorkadis;Dimitris A. Karras;M. Panayotou	2005			information theory;naive bayes classifier;computer science;pattern recognition;artificial intelligence;machine learning;artificial neural network;feature extraction;linear model;categorization;multilayer perceptron;services marketing	ML	8.406341003051473	-36.942626765389704	87975
aa0e823357a70fda9415bbf4fad9c66babdb0030	on topological dominance-based rough set approach	computing;topological properties;dominance based rough set approach;economics;point of view;rough set;business information systems;theoretical foundation	In this article, we characterize the Dominance-based Rough Set Approach (DRSA) from the point of view of its topological properties. Using the concept of a bitopological space, we extend to DRSA the classical results known for the original rough set approach. Moreover, we introduce a topological approach to ordinal classification. These considerations intend to strengthen theoretical foundations of DRSA, giving a deeper knowledge of its specific characteristics.	dominance-based rough set approach	Salvatore Greco;Benedetto Matarazzo;Roman Slowinski	2010	Trans. Rough Sets	10.1007/978-3-642-14467-7_2	computing;rough set;computer science;machine learning;management information systems;dominance-based rough set approach	HCI	-1.8808546625905742	-24.28330005239879	88075
7e91d0774d5a2cfddc9d222f8b72bf0aa159ea7f	an extended two-phase architecture for mining time series data	extraction information;association statistique;analyse exploratoire;analisis datos;information extraction;analisis cuantitativo;statistical association;exploratory analysis;intelligence artificielle;time series;data mining;similitude;data analysis;asociacion estadistica;regle association;regla asociacion;association rule;analyse quantitative;fouille donnee;serie temporelle;similarity;proceedings paper;serie temporal;quantitative analysis;time series data;artificial intelligence;analyse donnee;accuracy analysis;phase ii;inteligencia artificial;similitud;article;busca dato;extraccion informacion;analisis exploratorio;exploratory data analysis	Time series data vary with time. In the past, most of the researches focused on the matching of feature points or measuring of the similarities. They could successfully represent the feature patterns in a visualized way. In the mean while, those researches did not sufficiently describe the results in simple and understandable words. In this research, a two-phase architecture for mining time series data is introduced. By combining some different mining techniques, the difficulties mentioned above may be overcome. This architecture mainly consists of Exploratory Data Analysis (EDA) and techniques related to mining association rules. After the phase I analysis, quantitative association rules are obtained by phase II. Meanwhile, the rules of the architecture are able to be verified by accuracy analysis. Finally, a result of comparison with the traditional data mining techniques and this architecture shows that the two-phase architecture is superior to traditional techniques to the time series data.	time series;two-phase locking	An-Pin Chen;Yi-chang Chen;Nai-Wen Hsu	2005		10.1007/11552413_169	computer science;artificial intelligence;time series;data mining;information extraction;statistics	DB	-2.8824505708740324	-32.43606143789602	88581
b7b1a41000dd1bb74e40b93635d39d90449d6895	linguistic variables determination using fuzzy clustering	fuzzy c-means.;membership functions;cluster analysis;fuzzy clustering;membership function;fuzzy set	The fuzzy sets defining the linguistic variable values can be seen as a fuzzy partition of the linguistic variable. The membership functions obtained using fuzzy clustering algorithms are defined with respect to the group prototypes, and they cannot be used to define the linguistic variable values. We introduce several criteria to pass from the clustering membership functions to the linguistic variable value membership functions. The data have been taken from the financial sector.	algorithm;cluster analysis;fuzzy clustering;fuzzy set;membership function (mathematics)	Antonio Flores-Sintas;José Manuel Cadenas;Fernando Martin	1999			conductor;sabot;fuse (electrical);cylinder;composite material;spark plug;artificial intelligence;mathematics;pattern recognition	NLP	-0.10576905769687966	-26.039809164373874	88606
341d1c832683445608cfad7b7a92ebd04452039c	incremental learning of tree augmented naive bayes classifiers	tratamiento datos;bayesian classifier;learning algorithm;heuristic method;naive bayes;branching;data processing;metodo heuristico;traitement donnee;clasificador;algorithme apprentissage;naive bayes classifier;reseau bayes;structure reseau;classifier;incremental learning;machine learning;red bayes;ramificacion;bayes network;classificateur;ramification;incremental algorithm;methode heuristique;network structure;learning artificial intelligence;tree augmented naive bayes;algoritmo aprendizaje;apprentissage intelligence artificielle	Machine learning has focused a lot of attention at Bayesian classifiers in recent years. It has seen that even Naive Bayes classifier performs well in many cases, it may be improved by introducing some dependency relationships among variables (Augmented Naive Bayes). Naive Bayes is incremental in nature but, up to now, there are no incremental algorithms for learning Augmented classifiers. When data is presented in short chunks of instances, there is an obvious need for incrementally improving the performance of the classifiers as new data is available. It would be too costly, in computing time and memory space, to use the batch algorithms processing again the old data together with the new one. We present in this paper an incremental algorithm for learning Tree Augmented Naive classifiers. The algorithm rebuilds the network structure from the branch which is found to be invalidated, in some sense, by data. We will experimentally demonstrate that the heuristic is able to obtain almost optimal trees while saving computing time.	algorithm;analysis of algorithms;dspace;dynamic problem (algorithms);experiment;heuristic;machine learning;naive bayes classifier	Josep Roure Alcobé	2002		10.1007/3-540-36131-6_4	naive bayes classifier;data processing;computer science;artificial intelligence;machine learning;pattern recognition	ML	8.599072549159592	-32.65972397928251	88758
c524cd5b847376adddd0b47e0ad4472eadb20171	a change detector for mining frequent patterns over evolving data streams	databases;change detector;count data;online algorithm;itemsets;change detection;frequent pattern;frequent pattern mining;statistical testing data mining;probability density function;data stream;statistical test;data mining;chi square test data streams change detector frequent pattern mining statistical tests;data streams;change detection data stream frequent pattern mining;detectors itemsets sampling methods testing databases change detection algorithms data mining information systems application software feeds;chi square test;statistical tests;statistical testing;conferences;change detection algorithms;data models	Mining data streams for frequent patterns is important in many applications. Unlike traditional static databases, the underlying process that generates the data streams evolves over time. Past data may become outdated and of little use when compared to the most recent one. When a significant change occurs, much harm is done to the mining result if it is not properly handled. In this paper, an online algorithm for change detection in frequent pattern mining is proposed. Although there have been several studies mainly for adapting to changes, we contend that this is not enough. The ability to detect and characterize change is essential in many applications. A novel test strategy is designed to gather the ldquoevidencerdquo sufficient to conclude on whether the current sample differ significantly from a reference sample. Different statistical tests are evaluated and our study shows that the chi-square test is the most suitable for enumerated or count data.	association rule learning;chi;clock drift;concept drift;count data;data mining;database;experiment;online algorithm;test strategy;translational drift	Willie Ng;Manoranjan Dash	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811655	statistical hypothesis testing;computer science;data science;pattern recognition;data mining;data stream mining;statistics	DB	-0.5900082361413052	-35.62726327636842	88809
4490c304ef1e3d508c1fc988d6998d9dbfb607bc	a classifier for quantitative feature values based on a region oriented symbolic approach	medida velocidad;replication;metodo monte carlo;capsula convexa;normal distribution;geometrie algorithmique;computational geometry;methode monte carlo;mesure vitesse;intelligence artificielle;curva gauss;matching function;classification;replicacion;enveloppe convexe;speed measurement;monte carlo method;prediction accuracy;loi normale;error rate;artificial intelligence;geometria computacional;inteligencia artificial;convex hull;monte carlo simulation;clasificacion;gaussian distribution	In this paper, a classifier for quantitative feature values (intervals and/or points) based on a region oriented symbolic approach is proposed. In the learning step, each class is described by a region (or a set of regions) in R p  defined by a convex hull. To affect a new object to a class a dissimilarity matching function compares the class description (a region or a set of regions) with a point in R p . Experiments with two artificial data sets generated according to bi-variate normal distributions have been performed in order to show the usefulness of this classifier. The evaluation of the proposed classifier is accomplished according to the calculation of the prediction accuracy (error rate), speed and storage measurements computed through of a Monte Carlo simulation method with 100 replications.		Simith T. D'Oliveira Junior;Francisco de A. T. de Carvalho;Renata M. C. R. de Souza	2004		10.1007/978-3-540-30498-2_46	normal distribution;margin classifier;econometrics;quadratic classifier;computational geometry;artificial intelligence;mathematics;statistics;monte carlo method	SE	9.4912368315732	-34.57442580820891	88830
fe505659202ef970d32355e894a2ab55ab2243bc	a load curve based fuzzy modeling technique for short-term load forecasting	metodo cuadrado menor;modelizacion;complexite;methode moindre carre;selection problem;problema seleccion;least squares method;classification floue;modelisation floue;complejidad;complexity;algoritmo genetico;fuzzy c regression method;b spline curve;genetics;model complexity;fuzzy modeling;modelisation;short term load forecasting;methode c regression floue;fuzzy clustering;model building;robustesse;least square;prediction accuracy;algorithme genetique;genetic algorithm;robustness;genetic algorithms;b spline;esplin cubico;spline cubique;reseau neuronal;modeling;red neuronal;b splin;fuzzy model;neural network;cubic spline;robustez;prevision demande court terme;probleme selection;cubic b splines	A modeling method is suggested in this paper that permits building fuzzy models for short-term load forecasting (STLF). The model building process is divided in three parts: (a) the structure identi6cation based on a fuzzy C-regression method, (b) selection of the proper model inputs which is achieved using a genetic algorithm based selection mechanism, and (c) 6ne tuning by means of a hybrid genetic=least squares algorithm. To obtain simple and e8cient models we employ two descriptions for the load curves (LC’s), namely, the feature description for the premise part and the cubic B-spline curve for the consequent part of the rules. The simulation results demonstrate that the suggested model exhibits very good forecast capabilities. The suggested model is favorably compared with the ANN model in terms of prediction accuracy, robustness and model complexity. c © 2002 Elsevier Science B.V. All rights reserved.	b-spline;cubic function;emoticon;genetic algorithm;least squares;load profile;long short-term memory;simulation;software release life cycle;spline (mathematics)	Stelios E. Papadakis;Ioannis B. Theocharis;A. G. Bakirtzis	2003	Fuzzy Sets and Systems	10.1016/S0165-0114(02)00211-7	econometrics;genetic algorithm;computer science;artificial intelligence;machine learning;mathematics;least squares;artificial neural network;algorithm	AI	9.261780892433254	-28.84438047615521	89213
5d49c85679dd050f45276edd4dad7b92eef70835	filtering of interval type-2 fuzzy time-delay systems under a unified frame				Guan-Hong Chen;Kai Liu;Tao Zhao	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-162338	mathematics;artificial intelligence;filter (signal processing);discrete mathematics;fuzzy logic;machine learning	Robotics	2.2565312236488353	-24.523426414006295	89251
a94432157a2c0d2a89bd38cafabbb1eaad93f5d6	a generic formulation of neural nets as a model of parallel and self-programming computation	formal model;neural net;local computation	In the same way the more conventional fields of computer science need some theory, including the mathematical foundations of the calculus and the establishment of formal models, neural computation also needs its own. The basic requirements of this model are modularity, “small grain”, high connectivity, parametric local computation and some capacity of self-programming by means of the adjustment of these parameters.	artificial neural network;computation	José Mira Mira;Juan Carlos Herrero;Ana E. Delgado	1997		10.1007/BFb0032477	discrete mathematics;computer science;theoretical computer science;machine learning;artificial neural network	ML	6.568034913711545	-29.045612970851767	89452
492f3d2de5ef6dd1723e969f69a2bd5366794641	a general approach to rule aggregation in fuzzy logic control	generic model;fuzzy logic control	We look at the representation and aggregation of individual rules in the fuzzy logic control system. Two extreme paradigms for rule representation are introduced, the Mamdani model and the logical model. We look at the characteristics of these approaches. We then combine these two approaches to get a general model for the representation of rules. From this general formulation we obtain two soft classes of rules aggregation, or-like and and-like aggregations.	aggregate function;control system;fuzzy logic;logic control	Ronald R. Yager	1992	Applied Intelligence	10.1007/BF00058650	fuzzy logic;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy associative matrix	AI	1.0501566598639176	-25.737327753920905	89506
bbd668dc7731240dc6d467c7a6901190a3a37c78	feature space theory - a mathematical foundation for data mining	relevance analysis;data mining;feature space;features;feature reduction;feature construction;feature selection;knowledge representation	In data mining, an important task in classification and prediction includes feature construction, feature description, feature selection, feature relevance analysis and feature reduction. In this paper, feature space theory is introduced as a mathematical foundation for feature related concepts and techniques in data mining.	data mining;feature vector	Hong-Xing Li;Lida Xu	2001	Knowl.-Based Syst.	10.1016/S0950-7051(01)00103-4	feature learning;feature vector;feature;feature extraction;computer science;machine learning;pattern recognition;data mining;feature selection;feature;feature model;dimensionality reduction	ML	8.37651288015559	-36.41621429287151	89696
cf6e0edea15741738693bbb5bed7750e74dbdf41	an approach to knowledge reduction based on relative partition granularity	decision attribute;heuristic knowledge reduction algorithm;complexity theory;conditional attribute;relative classification ability;rough set theory;quantitative representation;algorithm design and analysis partitioning algorithms classification algorithms heuristic algorithms set theory complexity theory computer science;set theory;data mining;relative partition granularity;heuristic algorithms;classification algorithms;pattern classification;data mining relative partition granularity rough set theory heuristic knowledge reduction algorithm relative classification ability quantitative representation decision attribute conditional attribute;computer science;rough set;algorithm design and analysis;rough set theory data mining pattern classification;partitioning algorithms	Knowledge and classifications are related together by the theory of rough sets which claim is that knowledge is deep-seated in the classification abilities of human beings. In this paper, relative partition granularity, a quantitative representation for the relative classification ability of conditional attributes relative to decision attribute was defined. The equivalence between some basic concepts in rough set theory and relative partition granularity was proved. A heuristic knowledge reduction algorithm was designed based on relative partition granularity. Finally, we show that this algorithm is effective through an example.	algorithm;decision table;heuristic;partition type;rough set;set theory;turing completeness	Qinrong Feng;Duoqian Miao;Yi Cheng	2008	2008 IEEE International Conference on Granular Computing	10.1109/GRC.2008.4664639	statistical classification;rough set;computer science;machine learning;pattern recognition;data mining;mathematics	Robotics	-2.21494618043702	-27.251300211830674	89921
a69560fa40c6dc9e19bad293b3684327f55e4b1f	a new approach to fuzzy wavelet system modeling	transformation ondelette;fuzzy neural nets;fuzzy system models;transformacion discreta;computacion informatica;discrete wavelet transform;learning;system modeling;series expansion;wavelet frame;logique floue;ondita discreta;logica difusa;reseau neuronal flou;approximation fonction;aprendizaje;fuzzy logic;identificacion sistema;apprentissage;wavelet transform;function approximation;system identification;ciencias basicas y experimentales;ondelette floue;discrete wavelet;discrete transformation;ondelette discrete;takagi sugeno kang;sistema difuso;transformacion ondita;systeme flou;grupo a;fuzzy wavelet;transformation discrete;identification systeme;fuzzy system;wavelet transformation;fuzzy model	In this paper, we propose simple but effective two different fuzzy wavelet networks (FWNs) for system identification. The FWNs combine the traditional Takagi–Sugeno–Kang (TSK) fuzzy model and discrete wavelet transforms (DWT). The proposed FWNs consist of a set of if–then rules and, then parts are series expansion in terms of wavelets functions. In the first system, while the only one scale parameter is changing with it corresponding rule number, translation parameter sets are fixed in each rule. As for the second system, DWT is used completely by using wavelet frames. The performance of proposed fuzzy models is illustrated by examples and compared with previously published examples. Simulation results indicate the remarkable capabilities of the proposed methods. It is worth noting that the second FWN achieves high function approximation accuracy and fast convergence. 2005 Elsevier Inc. All rights reserved.	algorithm;approximation;discrete wavelet transform;fuzzy control system;fuzzy logic;fuzzy rule;network model;nonlinear system;series expansion;simulation;system identification;systems modeling	Engin Karatepe;Musa Alci	2005	Int. J. Approx. Reasoning	10.1016/j.ijar.2005.06.003	fuzzy logic;systems modeling;second-generation wavelet transform;system identification;series expansion;function approximation;computer science;fuzzy number;machine learning;calculus;control theory;mathematics;wavelet packet decomposition;stationary wavelet transform;discrete wavelet transform;fuzzy set operations;algorithm;fuzzy control system;wavelet transform	AI	8.613861157574304	-28.797922320532077	90109
a2a38d50c1a3402e5d788ac6b4fb1be96df1108c	using apriori with weka for frequent pattern mining		Knowledge exploration from the large set of data, generated as a result of the various data processing activities due to data mining only. Frequent Pattern Mining is a very important undertaking in data mining. Apriori approach applied to generate frequent item set generally espouse candidate generation and pruning techniques for the satisfaction of the desired objective. This paper shows how the different approaches achieve the objective of frequent mining along with the complexities required to perform the job. This paper demonstrates the use of WEKA tool for association rule mining using Apriori algorithm. Keywords— Data Mining, Apriori, Frequent Pattern Mining, WEKA	apriori algorithm;association rule learning;data mining;weka	Paresh Tanna;Yogesh Ghodasara	2014	CoRR	10.14445/22315381/IJETT-V12P223	data science;machine learning;data mining;fsa-red algorithm;apriori algorithm;k-optimal pattern discovery	ML	-4.134027933575965	-35.21824361549673	90349
c7dedef1141e7ab95b412feaf1e7cbf8ad87c8b7	cumulative learning techniques in production rules with fuzzy hierarchy (prfh) system	data stream;cumulative learning;knowledge acquisition;production rules with fuzzy hierarchy;cumulant;knowledge representation;production rule;knowledge base	Cumulative learning, a promising route for automated knowledge acquisition and adaptation, involves using the results of prior learning to facilitate further learning. Achieving this objective would largely depend upon an enriched knowledge representation scheme. One of such efficient representations is Production Rules with Fuzzy Hierarchy (PRFHs) system. A PRFH, a standard production rule augmented with generality and specificity information, is of the form:  [image omitted] where P is the set of preconditions  = (Ppub)k  ∪ (Pspl)k  ∪ (Ppvt)k and the specificity element Dki(di) means that Dki is a specific class of Dk with degree of subsumption di. In this paper, a set of related PRFHs is called a cluster and is represented by a PRFH-tree. The proposed scheme incrementally incorporates new knowledge into set of clusters obtained from previous episodes and also maintains summary of clusters to be used in the future episodes. Using the Cumulative_growth algorithm, a new rule is added to the system, the Restructure_cluster algorithm restructures a cluster so as to minimize redundancy, and Merging_clusters algorithm enables merging of two related PRFHs clusters. The proposed system would be particularly useful in mining data streams and dynamic restructuring of knowledge bases.	multi-task learning	Kamal Kant Bharadwaj;Rekha Kandwal	2008	J. Exp. Theor. Artif. Intell.	10.1080/09528130701524117	knowledge base;computer science;artificial intelligence;machine learning;data mining;cumulant	ML	4.653416079839387	-31.18107007893178	90412
2603698077b101d48324ec20e8d45f9ad3f5b1a4	comparison of reduction in formal decision contexts	reduction;comparison;rule acquisition;formal decision context;formal concept analysis	Abstract In formal concept analysis, many reduction methods have recently been proposed for formal decision contexts, and each of them was to reduce formal decision contexts with a particular purpose. However, little attention has been paid to the comparison of their differences from various aspects. In fact, this problem is very important because it can provide evidence to select an appropriate reduction method for a given specific case. To address this problem, our study mainly focuses on clarifying the relationship among the existing reduction methods in formal decision contexts. Firstly, we give a rule-based review of the existing reduction methods, revealing the type of rules that each of them can preserve. Secondly, we analyze the relationship among the consistencies introduced by the existing reduction methods. More specifically, Weiu0027s first consistency (see [39] ) is stronger than others, while her second one is weaker than the remainder except Wuu0027s consistency (see [43] ). Finally, we make a comparison of the existing reductions, concluding that Liu0027s reduction (see [14] ) maintaining the non-redundant decision rules of a formal decision context is coarser than others. The results obtained in this paper are beneficial for users to select an appropriate reduction method for meeting their requirements.		Jinhai Li;Cherukuri Aswani Kumar;Changlin Mei;Xizhao Wang	2017	Int. J. Approx. Reasoning	10.1016/j.ijar.2016.08.007	reduction;decision analysis;computer science;formal concept analysis;artificial intelligence;machine learning;data mining;mathematics;algorithm	Logic	-3.25185303359466	-26.027736070068407	90427
fc9163d678e3916df3c4f57ee95113aa498f2fb3	a method for designing flexible neuro-fuzzy systems	discretisation;soft computing;logique floue;discretization;logica difusa;discretizacion;intelligence artificielle;calculo flexible;fuzzy logic;neuro fuzzy system;calcul souple;artificial intelligence;inteligencia artificial;reseau neuronal;red neuronal;neural network	In the paper we develop a new method for designing and reduction of flexible neuro-fuzzy systems. The method allows to reduce number of discretization points in the defuzzifier, number of rules, number of inputs, and number of antecedents. The performance of our approach is illustrated on a typical benchmark.	fuzzy control system;neuro-fuzzy	Krzysztof Cpalka	2006		10.1007/11785231_23	computer science;artificial intelligence;machine learning;discretization;soft computing;artificial neural network;algorithm	EDA	8.63021458522217	-28.731630325441806	90525
dad194cd52357c01afb2efd379226830ea01429e	fuzzy perceptron with pocket algorithm in postoperative patient data set	postoperative patient data set fuzzy pocket fuzzy perceptron;vectors support vector machine classification classification algorithms educational institutions equations mathematical model pragmatics;pattern classification fuzzy set theory;fuzzy perceptron decomposition theorem fuzzy vectors pattern recognition postoperative patient data set pocket algorithm	Classification is one of the problems in pattern recognition. Most of the time this problem will deal with data sets that are in numeric form and represented by vectors of numbers. Since there might be uncertainties embedded in a data set, it is more natural to represent the data set as fuzzy vectors. Hence, in this paper, we develop a fuzzy perceptron with pocket algorithm for fuzzy vectors. This algorithm is based on the extension principle and the decomposition theorem. We implement this algorithm on both synthetic and a real-world data set, i.e., the postoperative patient data. We also compare the result from the fuzzy perceptron with pocket algorithm with that from the regular perceptron with pocket algorithm. The comparison is done on the fuzzy perceptron with and without pocket as well.	algorithm;defuzzification;embedded system;fuzzy logic;pattern recognition;perceptron;synthetic data	Suwannee Phitakwinai;Sansanee Auephanwiriyakul;Nipon Theera-Umpon	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891891	fuzzy classification;computer science;machine learning;pattern recognition;data mining;fuzzy set operations	Robotics	3.743768109784813	-28.897131982993244	90529
3fd9cb6fdd4dcaabc0e87524bbae9175a72970f9	a hybrid model combining case-based reasoning and fuzzy decision tree for medical data classification	case base reasoning;breast cancer diagnosis;hybrid model;data clustering;machine learning;clustering method;genetic algorithm;liver disorders classification;fuzzy decision tree;case based reasoning;data classification;breast cancer;medical diagnosis;forecast accuracy;decision rule;hybrid intelligence	In this research, a hybrid model is developed by integrating a case-based data clustering method and a fuzzy decision tree for medical data classification. Two datasets from UCI Machine Learning Repository, i.e., liver disorders dataset and Breast Cancer Wisconsin (Diagnosis), are employed for benchmark test. Initially a case-based clustering method is applied to preprocess the dataset thus a more homogeneous data within each cluster will be attainted. A fuzzy decision tree is then applied to the data in each cluster and genetic algorithms (GAs) are further applied to construct a decision-making system based on the selected features and diseases identified. Finally, a set of fuzzy decision rules is generated for each cluster. As a result, the FDT model can accurately react to the test data by the inductions derived from the case-based fuzzy decision tree. The average forecasting accuracy for breast cancer of CBFDT model is 98.4% and for liver disorders is 81.6%. The accuracy of the hybrid model is the highest among those models compared. The hybrid model can produce accurate but also comprehensible decision rules that could potentially help medical doctors to extract effective conclusions in medical diagnosis.	case-based reasoning;decision tree	Chin-Yuan Fan;Pei-Chann Chang;Jyun-Jie Lin;Jui-Chien C. Hsieh	2011	Appl. Soft Comput.	10.1016/j.asoc.2009.12.023	case-based reasoning;genetic algorithm;decision tree learning;computer science;artificial intelligence;breast cancer;machine learning;medical diagnosis;incremental decision tree;data mining;decision rule	AI	3.1751088328134554	-31.124082163333952	90557
09c0c1191deec58ffe0414f38067a569be0a6f90	generating positive and negative exact rules using formal concept analysis: problems and solutions	inference rule;association rule;concept lattice;mixing rule;formal concept analysis	The objective of this article is to investigate the problem of generating both positive and negative exact association rules when a formal context K of (positive) attributes is provided. A straightforward solution to this problem consists of conducting an apposition of the initial context K with its complementary context K, construct the concept lattice B(K|K) of apposed contexts and then extract rules. A more challenging problem consists of exploiting rules generated from each one of the contexts K and K to get the whole set of rules for the context K|K.#R##N##R##N#In this paper, we analyze a set of identified situations based on distinct types of input, and come out with a set of properties. Obviously, the global set of (positive and negative) rules is a superset of purely positive rules (i.e., rules with positive attributes only) and purely negative ones since it generally contains mixed rules (i.e., rules in which at least a positive attribute and a negative attribute coexist).#R##N##R##N#The paper presents also a set of inference rules to generate a subset of all mixed rules from positive, negative and mixed ones. Finally, two key conclusions can be drawn from our analysis: (i) the generic basis containing negative rules, ΣK, cannot be completely and directly inferred from the set ΣK of positive rules or from the concept lattice B(K), and (ii) the whole set of mixed rules may not be completely generated from ΣK alone, ΣK ∪ ΣK alone, or B(K) alone.	formal concept analysis	Rokia Missaoui;Lhouari Nourine;Yoan Renaud	2008		10.1007/978-3-540-78137-0_13	discrete mathematics;association rule learning;computer science;formal concept analysis;machine learning;data mining;mathematics;algorithm;rule of inference	Logic	-3.480369581949789	-26.902877743030206	90582
451c83a034a8e332dd0155d62370db5effdc50b0	back-propagation of imprecision in a cheese ripening fuzzy model based on human sensory evaluations	engineering;food processing;takagi sugeno fuzzy model;fuzzy set;procesamiento informacion;hombre;conjunto difuso;imprecise sensory measurements;ensemble flou;sensory quality;satisfiability;ingenierie;control proceso;sensory evaluation;food products;imprecision back propagation;information processing;human;process control;ingenieria;sistema difuso;systeme flou;cheese ripening process;traitement information;back propagation;commande processus;fuzzy system;fuzzy model;homme	In the food processes, taking human sensory evaluations into account is relevant for the online control of the sensory quality of food products. However, the operator sensory measurements are subjected to higher imperfections than the conventional sensory measurements. Therefore, to determine the tolerated imprecision on the sensory evaluations according to the specified imprecision for the control variable is an important challenge for food processes. Thus, from a Takagi-Sugeno fuzzy model developed on an application dealing with the control of a cheese ripening process, the proposed approach allows to determine the maximum imprecision tolerated on the cheese coat and consistency input sensory measurements in order to satisfy a specified tolerance for the ripening degree inferred with the TS fuzzy model. Results show that for some ripening periods, the maximum imprecision tolerated on the input measurements is inferior to the human imprecision on these measurements, thus the model must be used carefully or measurements have to be repeated in these periods.	software propagation	Irina Ioannou;Gilles Mauris;Gilles Trystram;Nathalie Perrot	2006	Fuzzy Sets and Systems	10.1016/j.fss.2005.12.016	information processing;computer science;artificial intelligence;food processing;backpropagation;process control;fuzzy set;algorithm;fuzzy control system;satisfiability	Robotics	8.82227757169395	-27.784228393767336	90786
eaa44f69843a32b65368a6eacb59faa0883017ce	scalable clustering for mining local-correlated clusters in high dimensions and large datasets	local correlated cluster;image processing;large dataset;morphology;approximated clustering;high dimension	Clustering is useful for mining the underlying structure of a dataset in order to support decision making since target or high-risk groups can be iden t fi d. However, for high dimensional datasets, the result of traditional clustering methods can be meaningless as clusters may only be depicted with respect to a small part of features. Taking cus tomer datasets as an example, certain customers may correlate with their salaryandeducation, and the others may correlate with their job andhouse location. If one uses all the features of a customer for clustering, th ese local-correlated clusters may not be revealed. In addition, processing high d imensions and large datasets is a challenging problem in decision making. Searching all the combi nations of every feature with every record to extract local-correlated clusters is infeasible , which is in exponential scale in terms of data dimensionality and cardinality. In this paper, we prop ose a scalable 2-Leveled Approximated Hyper-Image-based Clustering framework, referred as 2L-H IC A, for mining local-correlated clusters, where each level clustering process requires only one scan of the original dataset. Moreover, the data-processing time of 2L-HIC-A can be independent of t he input data size. In 2L-HIC-A, various well-developed image processing techniques can be exploited for mining clusters. In stead of proposing a new clustering algorithm, our framework can a ccommodate other clustering methods for mining local-corrected clusters, and to shed new light o n he existing clustering techniques.	approximation algorithm;cluster analysis;computer cluster;decision support system;digital image processing;enea ose;granular computing;greedy algorithm;hic;lu decomposition;scalability;time complexity;yang	Kun-Che Lu;Don-Lin Yang	2010	Fundam. Inform.	10.3233/FI-2010-214	correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;morphology;k-medians clustering;fuzzy clustering;image processing;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;linguistics;cluster analysis;single-linkage clustering;brown clustering;biclustering;affinity propagation;clustering high-dimensional data;conceptual clustering	ML	-2.338889955011317	-37.906445131844876	90887
17eb8f91cfc880e59ce9d3bd552abe898dca8737	evaluation of the performance of clustering algorithms for a high voltage industrial consumer	load profiles;demand side management;unsupervised machine learning;conditional entropy minimization clustering;load modeling	Load profiling refers to a procedure which leads to the formulation of daily load curve clusters based on the similarity of the curves shapes. This paper focuses on the investigation of the consumption patterns of an existing high voltage industrial consumer. The profiling process involves stages like the normalization of the recorded load data, the utilization of pattern recognition algorithms, the selection of the appropriate validation scheme and the exploitation of the profiling findings. Certain improvements are proposed for each of these stages. More specifically, the most common algorithms of the related literature are implemented and a detailed investigation of their performance is presented. A new algorithm is proposed, presenting, in the majority of the cases, the best performance. Additionally, all the clustering validity indicators of the literature are considered to evaluate the clustering results. After the formulation of the load curve clusters, the load profiles are extracted and based on specific indices conclusions are drawn regarding the implementation of suitable demand side management schemes. & 2014 Elsevier Ltd. All rights reserved.	algorithm;cluster analysis;data validation;load profile;pattern recognition	Ioannis Panapakidis;Minas Alexiadis;Grigoris Papagiannis	2015	Eng. Appl. of AI	10.1016/j.engappai.2014.10.013	unsupervised learning;simulation;computer science;machine learning;data mining	HPC	1.0210907415672728	-33.09715271247833	91042
393e0d42ff7cc76cc8bc9526f729cbf59ef9c3fc	robust textual data streams mining based on continuous transfer learning	conference proceeding	In textual data stream environment, concept drift can occur at any time, existing approaches partitioning streams into chunks can have problem if the chunk boundary does not coincide with the change point which is impossible to predict. Since concept drift can occur at any point of the streams, it will certainly occur within chunks, which is called random concept drift. The paper proposed an approach, which is called chunk level-based concept drift method (CLCD), that can overcome this chunking problem by continuously monitoring chunk characteristics to revise the classifier based on transfer learning in positive and unlabeled (PU) textual data stream environment. Our proposed approach works in three steps. In the first step, we propose core vocabulary-based criteria to justify and identify random concept drift. In the second step, we put forward the extension of LELC (PU learning by extracting likely positive and negative microclusters)[1], called soft-LELC, to extract representative examples from unlabeled data, and assign a confidence score to each extracted example. The assigned confidence score represents the degree of belongingness of an example towards its corresponding class. In the third step, we set up a transfer learning-based SVM to build an accurate classifier for the chunks where concept drift is identified in the first step. Extensive experiments have shown that CLCD can capture random concept drift, and outperforms state-of-the-art methods in positive and unlabeled textual data stream environments.	concept drift;experiment;naive bayes classifier;one-class classification;shallow parsing;text corpus;vocabulary	Longbing Cao;Zhifeng Hao;Bo Liu;Yanshan Xiao;Philip S. Yu	2013		10.1137/1.9781611972832.81	computer science;data science;data mining;database	AI	-0.1623293312607011	-35.79152241815108	91088
4864baabf693ebc529448a89d24fcf57b6d1a93e	application of pruning techniques for propositional learning to progol	inductive logic;arbre recherche;base donnee;learning algorithm;algoritmo busqueda;search space;algorithme recherche;search algorithm;database;base dato;inductive logic programming;algorithme apprentissage;logical programming;data mining;logique inductive;arbol investigacion;programmation logique;fouille donnee;decouverte connaissance;search tree;algoritmo aprendizaje;programacion logica;knowledge discovery	Since learning with Inductive Logic Programming (ILP) can be regarded as the search problem through the hypotheses space, it is essential to reduce the search space in order to improve the efficiency. In the propositional learning framework, an efficient admissible search algorithm called OPUS (Optimized Pruning for Unordered Search) has been developed. OPUS employed the effective pruning techniques for unordered search and succeeded in improving the efficiency. In this paper, we propose an application of OPUS to an ILP system Progol. However, because of the difference of representation language, it is not applicable to ILP directly. We make the conditions clear under which the pruning techniques in OPUS can be applied in the framework of Progol. In addition, we propose a new pruning criterion, which can be regarded as inclusive pruning. Experiments are conducted to assess the effectiveness of the proposed algorithms. The results show that the proposed algorithms reduce the number of candidate hypotheses to be evaluated as well as the computational time for a certain class of problems.	progol	Tomonobu Ozaki;Koichi Furukawa	2001		10.1007/3-540-44797-0_17	computer science;artificial intelligence;machine learning;principal variation search;data mining;mathematics;knowledge extraction;search tree;algorithm;search algorithm;pruning	AI	7.440542872611919	-32.10687329940358	91188
bf667e99236e52ede7932efa9148b9891aa04473	mining ordinal patterns for data cleaning	pattern recognition data mining;data mining;pattern mining;pattern recognition;ordinal pattern sequential pattern mining data cleaning;data cleaning;sequential pattern mining;sequential pattern;cleaning itemsets data mining diseases educational institutions computer science educational technology laboratories knowledge engineering computer science education;high efficiency	It is well recognized that sequential pattern mining plays an essential role in many scientific and business domains. In this paper, a new extension of sequential pattern, ordinal pattern, is proposed. An ordinal pattern is an ordinal sequence of attributes, whose values commonly occur in ascending order over data set. Ordinal pattern mining requests that values of different attributes must be comparable and ordinal. After each record in data set is transformed into an ordinal sequence of attributes according to their ordinal values, ordinal patterns can be mined by means of mining sequential patterns. But our work is different from sequential pattern mining. One use of ordinal patterns is to identify possible error records in data cleaning, in which the values of attributes break the ordinal patterns which most of the data conform to. Experiments verify the high efficiency of the method presented.	algorithm;data mining;experiment;mined;ordinal data;plasma cleaning;sequential pattern mining;sorting	Ya-Bo Liu;Dayou Liu	2004	Proceedings of the 2004 IEEE International Conference on Information Reuse and Integration, 2004. IRI 2004.	10.1109/IRI.2004.1431500	sequential pattern mining;computer science;data science;pattern recognition;data mining	DB	-4.512679061867547	-36.1101558011308	91267
08f671f330a4dd4fac551a305905cc919d369973	hourly forecasting of so2 pollutant concentration using an elman neural network	forecasting;prevision;pollutant;elman neural network;surveillance;gradiente;contaminante;polluant;gradient;time series;vigilancia;monitoring;wind velocity;serie temporelle;serie temporal;cipa;monitorage;reseau neuronal;monitoreo;red neuronal;neural network	In this paper the first results produced by an Elman neural network for hourly SO2 ground concentration forecasting are presented. Time series has been recorded between 1998 and 2001 and are referred to a monitoring station of SO2 in the industrial site of Priolo, Syracuse, Italy. Data has been kindly provided by CIPA (Consorzio Industriale per la Protezione dell’Ambiente, Siracusa, Italia). Time series parameters are the horizontal and vertical wind velocity, the wind direction, the stability classes of Thomas, the base level of the layer of the atmospheric stability, the gradient of the potential temperature and the difference of the potential temperature of reference.	algorithm;artificial neural network;backpropagation;coefficient;gradient;linear algebra;rprop;time series;velocity (software development)	U. Brunelli;V. Piazza;L. Pignato;Filippo Sorbello;Salvatore Vitabile	2005		10.1007/11731177_10	wind speed;pollutant;forecasting;time series;gradient;artificial neural network;statistics	ML	9.934117438231757	-24.071652930949217	91301
2802421249fd79eef1e8c5213dea8677a97df3a5	the improved fuzzy classification model on disaster loss	analytical models;fuzzy classification;fuzzy theory;disaster;orthogonalization theory;earthquakes;data mining;research method;fuzzy set theory;pattern classification fuzzy set theory genetic algorithms;matrix decomposition;fuzzy classification model;membership functions;indexation;natural disaster;membership function;pattern classification;fuzzy systems risk analysis pattern analysis pattern recognition earthquakes knowledge engineering genetic algorithms sun mathematical model hazards;genetic algorithm;genetic algorithms;optimization;quantitative assessment;ga method;correlation;membership fuzzy theory disaster ga method orthogonalization theory;membership;fuzzy theory fuzzy classification model disaster loss quantitative assessment membership functions orthogonalization theory genetic algorithm;disaster loss	The aim of this paper is to solve the next two questions: (1) The quantitative assessment and classification of the disaster loss is a complex and difficult work. It often encounters the problem that different people will arbitrarily create different membership functions for the same issue, so the results of different model can not be compared. (2) The evaluation factors may have mutual relationships between each other, which will affect the results. The research methods of this paper are as follow: Firstly, this paper decomposes the related indexes based on orthogonalization theory, the aim of this step is to solve mutual relations between the factors leading to disaster losses; then confirms membership degree with the genetic algorithm (GA) method, getting the best membership of the factors; finally, this paper uses the fuzzy theory to recognize the damage degree of natural disaster, and evaluates the damage degree of mud-rock flow in Bijie area Guizhou province. The result is that the grade of disaster loss of Bijie area is low. The conclusion shows that the improved fuzzy classification model on disaster loss effectively solves the arbitrary and relevant problem of factors.	evaluation function;fuzzy classification;fuzzy logic;genetic algorithm;software release life cycle	Jing Li;Jian-yun Chen;Qiang Xu	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.37	genetic algorithm;computer science;artificial intelligence;machine learning;data mining;mathematics	AI	-2.2204812471801776	-26.511284323915625	91339
e468cde695cd4c96534f962c419a8be8604d1641	linguistic decision trees for fusing tidal surge forecasting models		The use of linguistic decision trees as represented within the la- bel semantics framework, is proposed for the fusion of multiple forecasting models. The learning algorithm LID3 is applied to infer a decision tree with branches representing a set of rules each identifying a probability distribution on the available models and where the constraints in each rule are generated from fuzzy labels describing the relevant input attributes. The resulting ag- gregated forecast for a given vector of input attributes x ,i s then taken to be the mean value of the forecasts from each model relative to a probability distribution on models conditional on x as determined from the linguistic decision tree. The potential of this approach is then investigated through its application to the fusion of tidal surge forecasting models for the east coast of the UK.	decision tree	Jonathan Lawry;Hongmei He	2010		10.1007/978-3-642-14746-3_50	artificial intelligence;machine learning;data mining;mathematics	ML	3.416411967917089	-25.92466679957344	91367
a83c301bfce70fcca88fe937187beae684a8d64c	weighted decision tables - an alternative solution for ambiguity	decision table		decision table	M. L. Schneider	1985	Comput. J.	10.1093/comjnl/28.4.366	decision table;weighted product model;computer science;decision tree;data mining;mathematics;programming language;algorithm;weighted sum model;statistics	ECom	0.8902485978226233	-28.57857786226082	91394
c27fa0b3ff7bf40e05a8744dceebcd4bfb794229	finding the complete set of minimal solutions for fuzzy max-archimedean t-norm relational equations	bottom up;image resolution;complete set;fuzzy relation;complete set fuzzy max archimedean t norm relational equations binding matrix irredundant coverings;fuzzy sets equations matrix converters fuzzy systems intelligent systems information management image converters knowledge engineering watermarking np hard problem;matrix algebra;fuzzy set theory;max archimedean t norm composition;irredundant coverings;indexes;fuzzy relational equations;intelligent systems;mathematical model;matrix converters;matrix algebra fuzzy set theory;max archimedean t norm composition fuzzy relational equations;fuzzy max archimedean t norm relational equations;binding matrix;knowledge engineering	This study considers the problem of generating all minimal solutions of a system of fuzzy relational equations (FREs) with max-Archimedean t-norm composition. It defines the binding matrix of a system of FREs, and then shows that an irredundant covering of the binding matrix corresponds to a minimal solution of the FREs. Consequently, the problem of finding all minimal solutions of the FREs can be transformed into the problem of finding all irredundant coverings of the binding matrix. We propose an algorithm to solve the transformed problem. This algorithm builds all irredundant coverings in a bottom-up manner, i.e., all irredundant coverings of the binding matrix is built from the irredundant coverings of the submatrices of the binding matrix. Once all irredundant coverings have been found, they can be easily converted into the minimal solutions of original FREs.	algorithm;bottom-up parsing;name binding;t-norm;top-down and bottom-up design;variable (computer science)	Jun-Lin Lin	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.80	database index;combinatorics;discrete mathematics;image resolution;computer science;artificial intelligence;knowledge engineering;top-down and bottom-up design;mathematical model;mathematics;fuzzy set;algebra	EDA	-4.31137159629233	-25.827680102340494	91729
f0793e7b96595360410a5edae3170d322d0ba9fc	improving direct counting for frequent itemset mining	base donnee;database;base dato;almacen dato;data mining;frequent itemset;fouille donnee;decouverte connaissance;frequent itemset mining;descubrimiento conocimiento;entrepot donnee;data warehouse;busca dato;knowledge discovery	During the last ten years, many algorithms have been proposed to mine frequent itemsets. In order to fairly evaluate their behavior, the IEEE/ICDM Workshop on Frequent Itemset Mining Implementations (FIMI’03) has been recently organized. According to its analysis, kDCI++ is a state-of-the-art algorithm. However, it can be observed from the FIMI’03 experiments that its efficient behavior does not occur for low minimum supports, specially on sparse databases. Aiming at improving kDCI++ and making it even more competitive, we present the kDCI-3 algorithm. This proposal directly accesses candidates not only in the first iterations but specially in the third one, which represents, in general, the highest computational cost of kDCI++ for low minimum supports. Results have shown that kDCI-3 outperforms kDCI++ in the conducted experiments. When compared to other important algorithms, kDCI-3 enlarged the number of times kDCI++ presented the best behavior.	algorithm;algorithmic efficiency;association rule learning;computation;database;experiment;iteration;sparse matrix	Adriana Prado;Cristiane Targa;Alexandre Plastino	2004		10.1007/978-3-540-30076-2_37	computer science;data science;data warehouse;data mining;database	ML	-3.0220243411194194	-33.89251835436708	91799
61d75d3f9ff9b5c2bdde7ca4a9f178391443ab17	fuzzy technique-based identification of close and distant clusters in clustering	fuzzy set theory;fuzzy clustering;linguistic interface;clustering;cluster identification	Due to advances in hardware performance, user-friendly interfaces are becoming one of the major concerns in information systems. Linguistic conversation is a very natural way of human communications. Fuzzy techniques have been employed to liaison the discrepancy between the qualitative linguistic terms and quantitative computerized data. This paper deals with linguistic queries using clustering results on data sets, which are intended to retrieve the close clusters or distant clusters from the clustering results. In order to support such queries, a fuzzy technique-based method is proposed. The method introduces distance membership functions, namely, close and distant membership functions which transform the metric distance between two objects into the degree of closeness or farness, respectively. In order to measure the degree of closeness or farness between two clusters, both cluster closeness measure and cluster farness measure which incorporate distance membership function and cluster memberships are considered. For the flexibility of clustering, fuzzy clusters are assumed to be formed. This allows us to linguistically query close or distant clusters by constructing fuzzy relation based on the measures.	centrality;cluster analysis;computer cluster;discrepancy function;entity;fuzzy concept;information system;membership function (mathematics);phylogenetics;usability	Kyung Mi Lee;Keon-Myung Lee	2011	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2011.11.3.165	complete-linkage clustering;correlation clustering;discrete mathematics;membership function;k-medians clustering;fuzzy clustering;flame clustering;fuzzy classification;fuzzy number;machine learning;data mining;mathematics;fuzzy set;cluster analysis;single-linkage clustering	Web+IR	-0.9753652236771406	-25.75438023158135	91875
efd24ceca914759623a7fd36fc9108f1548d66c2	an intelligent system for pattern recognition and time series prediction using modular neural networks	software tool;neural nets;intelligent systems intelligent networks pattern recognition neural networks intelligent structures software tools mathematical model graphics system testing bayesian methods;sugeno integral formulas intelligent system pattern recognition time series prediction modular neural networks software tool neural multinet structures user friendly graphic enviroment;time series;intelligent system;pattern recognition;time series knowledge based systems neural nets pattern recognition software tools;software tools;modular neural network;knowledge based systems;time series prediction;sugeno integral	On this research work we present a software tool to experiment with new neural multi-net structures, incuding ensemble and modular approaches. This tool allow us to draw models, set parameters, save as project and generate files with results, always in a user friendly graphic enviroment. Other feature is the implementation of the Sugeno Integral formulas, this program was developed to allow the combination of any number of elements.	algorithm;artificial intelligence;artificial neural network;backpropagation;computer vision;modular neural network;neural network software;pattern recognition;programming tool;sugeno integral;time series;usability	Patricia Melin;Olivia Mendoza;Miguel Soto;Maribel Gutierez;Daniel Solano	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246932	computer science;artificial intelligence;knowledge-based systems;machine learning;time series;data mining;time delay neural network;artificial neural network;statistics	Robotics	6.074185869099478	-28.589935798423625	91999
17a13e2f26250f950020de904ad92a6a32599b57	analysis of credit risk prediction using arsknn		Credit risk is characterized as the risk that borrowers will neglect to pay its advance commitments and loan obligations. It is very hard to predict the outcomes (risky borrower) manually as the evaluation of large features set is quite time consuming. That’s why, we need some good predictor as classifier. The traditional k-NN is one pre-established classifier used in various domains along with credit risk predictions. The newly conceptualized ARSkNN is another such classification which reduces the runtime in predicting the outcomes and improves overall accuracy percentage of the predicted classes over Traditional k-NN. The method adopt the similarity measure which is based on the Mass estimation rather than distance estimation for predicting the K- nearest neighbor. The results were compared using WEKA 3.7.10 as tool and found significant improvement vis-a-vis the evaluation parameters by the ARSkNN method.		Ashish Kumar;Roheet Bhatnagar;Sumit Srivastava	2018		10.1007/978-3-319-74690-6_63	neglect;similarity measure;credit risk;loan;machine learning;classifier (linguistics);k-nearest neighbors algorithm;computer science;artificial intelligence	NLP	3.406039834786512	-34.029778548071285	92004
24fdc0c85462949b533ba8715d12d24ff9420cf5	granularity adaptive density estimation and on demand clustering of concept-drifting data streams	grain size;concept drift;analyse amas;streaming;medicion densidad;estimacion densidad;density measurement;systeme aide decision;transmision continua;estimation densite;data stream;almacen dato;sistema ayuda decision;classification;density estimation;transmission en continu;decision support system;cluster analysis;grosor grano;clustering method;decouverte connaissance;mesure densite;descubrimiento conocimiento;analisis cluster;entrepot donnee;data warehouse;estimacion adaptativa;clustered data;clasificacion;adaptive estimation;estimation adaptative;knowledge discovery;grosseur grain	Clustering data streams has found a few important applications. While many previous studies focus on clustering objects arriving in a data stream, in this paper, we consider the novel problem of on demand clustering concept drifting data streams. In order to characterize concept drifting data streams, we propose an effective method to estimate densities of data streams. One unique feature of our new method is that its granularity of estimation is adaptive to the available computation resource, which is critical for processing data streams of unpredictable input rates. Moreover, we can apply any clustering method to on demand cluster data streams using their density estimations. A performance study on synthetic data sets is reported to verify our design, which clearly shows that our method obtains results comparable to CluStream [3] on clustering single stream, and much better results than COD [8] when clustering multiple streams.	cluster analysis;computation;effective method;scalability;synthetic data	Weiheng Zhu;Jian Pei;Jian Yin;Yihuang Xie	2006		10.1007/11823728_31	correlation clustering;constrained clustering;data stream clustering;density estimation;decision support system;fuzzy clustering;biological classification;flame clustering;computer science;concept drift;canopy clustering algorithm;machine learning;data warehouse;cure data clustering algorithm;data mining;database;cluster analysis;statistics;grain size;clustering high-dimensional data	DB	-3.092780584962402	-33.27258212359089	92177
4f0948823ea9d918a1460502234d1e33ae79628c	evolutionary fuzzy function with support vector regression for the prediction of concrete compressive strength	pattern clustering;fuzzy c mean;neural network ann;mechanical engineering computing;anfis;support vector regression svr;support vector machines;compressive strength evolutionary fuzzy function support vector regression svr evolutionary algorithm anfis neural network ann concrete;evolutionary fuzzy function;support vector regression;system modelling;support vector machines compressive strength concrete fuzzy set theory mechanical engineering computing pattern clustering regression analysis;fuzzy set theory;hybrid model;least squared estimation evolutionary fuzzy function concrete compressive strength support vector regression model fuzzy system modelling membership value fuzzy c mean clustering adaptive neural fuzzy inference system;compressive strength;least squares estimate;training concrete support vector machines mathematical model kernel vectors predictive models;adaptive neural fuzzy inference system;regression analysis;evolutionary algorithm;fuzzy system;artificial neural network;concrete;neural network;generalization capability	The main purpose of this paper is to develop an evolutionary fuzzy function with support vector regression (EFF-SVR) model to predict the compressive strength of concrete. Fuzzy functions alter conventional fuzzy system modelling methods structurally. They take advantage of utilizing membership values calculated by fuzzy c-mean (FCM) clustering, and their possible transformations, as additional explanatory variables augmented to the original input space. Since support vector regression (SVR) methods have considerable capability of minimizing both empirical and complexity risks simultaneously, the hybrid model of EFF-SVR is expected to yield robust results. Finally, the generalization capability and robustness of EFF-SVR are compared with some existing system modelling methods, i.e., artificial neural network (ANN), adaptive neural-fuzzy inference system (ANFIS), fuzzy function with least squared estimation (FF-LSE), and improved FF-LSE. The results show that EFF-SVR has a great ability as a feasible tool for prediction of the concrete compressive strength.	adaptive neuro fuzzy inference system;aggregate function;algorithm;artificial neural network;blast;benchmark (computing);cluster analysis;eff des cracker;error detection and correction;fuzzy cognitive map;fuzzy control system;inference engine;mix;nonlinear system;support vector machine	Siamak Safarzadegan Gilan;Alireza Mashhadi Ali;Ali Akbar Ramezanianpour	2011	2011 UKSim 5th European Symposium on Computer Modeling and Simulation	10.1109/EMS.2011.28	defuzzification;adaptive neuro fuzzy inference system;engineering;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition	AI	6.5205228694831545	-26.2431946742074	92302
b4f77199eb47e91280311ec3362f614d66e6bcf4	extreme learning machines with heterogeneous data types		Abstract Current advances in communication, sensor and computing technologies are generating information in never before seen amounts and at constantly increasing rates (i.e. the information explosion, the Internet of Things). From the point of view of data analytics, the information is composed of a diversity of data types and it contains uncertainties and incompleteness of different degrees, which add an extra component to the original heterogeneity. Many data mining and machine learning methods do not handle heterogeneity well. Extreme learning machines (ELM) are interesting computational algorithms because of their simplicity, their good performance and their speed. They can be extended for processing information composed of heterogeneous data types (HT-ELM), capable of addressing classification and regression problems with complex data. Two approaches are discussed: one works directly with the heterogeneous data and the other one transforms the information into simpler homogeneous spaces that preserve structural properties. In them, standard learning methods can be applied, including classical ELMs among others. Both approaches are illustrated using real world examples involving heterogeneous predictor variables composed of mixtures of nominal, ordinal, interval, ratio, fuzzy variables, and entire empirical probability distributions. In all cases HT-ELM and ELM models produced results that compare favorably with well-established methods.		Julio J. Valdés	2018	Neurocomputing	10.1016/j.neucom.2017.02.103	machine learning;fuzzy logic;information explosion;information processing;data type;pattern recognition;empirical probability;artificial intelligence;intrinsic dimension;complex data type;data analysis;computer science	ML	2.209417964688477	-30.197163251327257	92552
843c596824c549a598105c3408969a5569ebcf79	new supervision architecture based on on-line modelling of non-stationary data	unsupervised learning;classification automatique statistiques;calcul neuronal;neural computation;information extraction;diagnostico;autoadaptative system;62m45;sistema autoadaptativo;apprentissage non supervise;non stationary data;dynamical classification;dynamical system;systeme dynamique;dynamic clustering;monitoring system;monitoring;62h30;clustering;auto adaptation;dynamic modelling;37xx;monitorage;systeme autoadaptatif;sistema dinamico;cluster analysis statistics;reseau neuronal;donnee non stationnaire;monitoreo;diagnosis;red neuronal;computacion neuronal;neural network;diagnostic	A new supervision system consisting of three modules is presented. The main novelty is the first module that corresponds to a modelling task. This module, which uses the auto-adaptive and dynamical clustering (AUDyC) neural network, allows us to continuously analyse and classify the functioning state of the monitored system using a dynamical modelling of all known modes (good/bad functioning modes represent different classes). The second module exploits these models of the functioning modes in order to detect “fast” and “slow” deviations. From membership degrees and from the information extracted by the monitoring module, the third module, dedicated to the diagnostics, informs the user about the functioning conditions of the system. In this paper, the main characteristics of the AUDyC and its abilities to model on-line non-stationary data are presented. Then, the description of the supervision system is given and some experimental results stemmed from a supervision application of a hydraulic system are discussed.	adaptive architecture;algorithm;artificial neural network;cluster analysis;complex systems;dynamical system;experiment;exploit (computer security);failure cause;institute for operations research and the management sciences;online and offline;pattern recognition;software prototyping;stationary process;unsupervised learning	Stéphane Lecoeuche;Christophe Lurette;Sylvain Lalot	2004	Neural Computing & Applications	10.1007/s00521-004-0427-y	unsupervised learning;simulation;computer science;artificial intelligence;dynamical system;machine learning;cluster analysis;information extraction;artificial neural network;models of neural computation	AI	9.222515836620941	-29.56721090057668	92566
7efe95f3e62cef72bd6e9e31f03829845a1fac86	closed form fuzzy interpolation with interval type-2 fuzzy sets	g400 computer science;interpolation;fuzzy rules;conference;membership functions;fuzzy sets interpolation uncertainty equations mathematical model educational institutions fuzzy logic;knowledge uncertainty closed form fuzzy rule interpolation fuzzy inference sparse rule bases system complexity reduction convex interpolation precise valued membership functions interval type 2 fuzzy sets data uncertainty;interpolation fuzzy set theory;interval type 2 fuzzy sets fuzzy rule interpolation closed form interpolation;h600 electronic and electrical engineering	Fuzzy rule interpolation enables fuzzy inference with sparse rule bases by interpolating inference results, and may help to reduce system complexity by removing similar (often redundant) neighbouring rules. In particular, the recently proposed closed form fuzzy interpolation offers a unique approach which guarantees convex interpolated results in a closed form. However, the difficulty in defining the required precise-valued membership functions still poses significant restrictions over the applicability of this approach. Such limitations can be alleviated by employing type-2 fuzzy sets as their membership functions are themselves fuzzy. This paper extends the closed form fuzzy rule interpolation using interval type-2 fuzzy sets. In this way, as illustrated in the experiments, closed form fuzzy interpolation is able to deal with uncertainty in data and knowledge with more flexibility.	experiment;fuzzy logic;fuzzy rule;fuzzy set;interpolation;membership function (mathematics);set theory;sparse matrix;type-2 fuzzy sets and systems	Longzhi Yang;Chengyuan Chen;Nanlin Jin;Xin Fu;Qiang Shen	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891643	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;interpolation;fuzzy mathematics;fuzzy classification;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;statistics	Robotics	-0.27868349264389647	-25.3889620349409	92964
f8825da1424a26335ce077384238e4fbc3fe471a	smartfd: a real big data application for electrical fraud detection		The main objective of this paper is the application of big data analytics to a real case in the field of smart electric networks. Smart meters are not only elements to measure consumption, but they also constitute a network of millions of sensors in the electricity network. These sensors provide a huge amount of data that, once analyzed, can lead to significant advances for the society. In this way, tools are being developed in order to reach certain goals, such as obtaining a better consumption estimation (which would imply a better production planning), finding better rates based on the time discrimination or the contracted power, or minimizing the non-technical losses in the network, whose actual costs are eventually paid by end-consumers, among others. In this work, real data from Spanish consumers have been analyzed to detect fraud in consumption. First, 1 TB of raw data was preprocessed in a HDFS-Spark infrastructure. Second, data duplication and outliers were removed, and missing values handled with specific big data algorithms. Third, customers were characterized by means of clustering techniques in different scenarios. Finally, several key factors in fraud consumption were found. Very promising results were achieved, verging on 80% accuracy.	big data	David Gutiérrez-Avilés;J. A. Fábregas;J. Tejedor;Francisco Martínez-Álvarez;Alicia Troncoso Lora;A. Arcos;José Cristóbal Riquelme Santos	2018		10.1007/978-3-319-92639-1_11	data mining;machine learning;raw data;production planning;artificial intelligence;missing data;big data;cluster analysis;outlier;computer science;data deduplication	ML	1.3308024101933367	-33.53492318926182	93009
8ad0b925275d9a63606a89a522f3e9215fe23522	the granular extension of sugeno-type fuzzy models based on optimal allocation of information granularity and its application to forecasting of time series	granular computing;information granularity;the sugeno type fuzzy model;time series;the sugeno type granular model;modeling and prediction	Graphical abstractThe schematic diagram of the design process for extending the Sugeno-type fuzzy model to its granular model (granular counterpart) by an optimal allocation of information granularity. Display Omitted HighlightsWe proposed a novel Sugeno-type granular model in which the output is an information granular, which facilitates further interpretation..Information granularity, which is regarded as an important and practically useful design asset, is fully exploited for designing Sugeno-type granular model.The proposed approach of constructing the Sugeno-type granular model is of general nature as it could be applied to various fuzzy models and realized by invoking different formalisms of information granules.The proposed Sugeno-type granular model can provide much more flexibility than the Sugeno-type numeric fuzzy model. The Sugeno-type fuzzy models are used frequently in system modeling. The idea of information granulation inherently arises in the design process of Sugeno-type fuzzy model, whereas information granulation is closely related with the developed information granules. In this paper, the design method of Sugeno-type granular model is proposed on a basis of an optimal allocation of information granularity. The overall design process initiates with a well-established Sugeno-type numeric fuzzy model (the original Sugeno-type model). Through assigning soundly information granularity to the related parameters of the antecedents and the conclusions of fuzzy rules of the original Sugeno-type model (i.e. granulate these parameters in the way of optimal allocation of information granularity becomes realized), the original Sugeno-type model is extended to its granular counterpart (granular model). Several protocols of optimal allocation of information granularity are also discussed. The obtained granular model is applied to forecast three real-world time series. The experimental results show that the method of designing Sugeno-type granular model offers some advantages yielding models of good prediction capabilities. Furthermore, those also show merits of the Sugeno-type granular model: (1) the output of the model is an information granule (interval granule) rather than the specific numeric entity, which facilitates further interpretation; (2) the model can provide much more flexibility than the original Sugeno-type model; (3) the constructing approach of the model is of general nature as it could be applied to various fuzzy models and realized by invoking different formalisms of information granules.	mathematical optimization;time series	Wei Lu;Liyong Zhang;Witold Pedrycz;Jianhua Yang;Xiaodong Liu	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.01.021	granular computing;computer science;artificial intelligence;machine learning;time series;data mining;mathematics;statistics	AI	1.5942182302256303	-26.88448142770762	93085
154566e339cbd68f41931275cc031f858317b718	an active learning system for mining time-changing data streams	concept drift;time change;data stream;active learning;increasing iterative step size;significant changes;ming time changing data streams;uncertainty sampling	Mining time-changing data streams is of great interest. The fundamental problems are how to effectively identify the significant changes and organize new training data to adjust the outdated model. In this paper, we propose an active learning system to address these issues. Without need knowing any true labels of the new data, we devise an active approach to detecting the possible changes. Whenever the suspected changes are indicated, it exploits a light-weight uncertainty sampling algorithm to choose the most informative instances to label. With these labeled instances, it further tests the truth of the suspected changes. If the changes indeed cause significant performance deterioration of the current model, it evolves the old model. Thus, our method is sensitive to significant changes and robust to noisy changes, and can quickly adapt to concept-drift. Experimental results from both synthetic and real-world data confirm the advantages of our system.	active learning (machine learning)	Shucheng Huang;Yisheng Dong	2007	Intell. Data Anal.		daylight saving time;computer science;data science;concept drift;machine learning;data mining;active learning;statistics	ML	-0.25734818013005734	-35.43757671637234	93119
1a3297e057f5b3fd90565c2b113efc020d97dc47	on the robustness of dempster's rule of combination	belief function;expert systems;uncertainty;knowledge management;bayesian methods;fuzzy logic;discount factor;fuzzy logic artificial intelligence;highly conflicting belief functions;discounted belief functions;dempster s rule of combination;discount factor robustness dempster s rule of combination highly conflicting belief functions discounted belief functions perturbations;perturbations;data systems;artificial intelligence;robustness;robustness uncertainty bayesian methods expert systems knowledge management data systems artificial intelligence	The robustness of Dempster s rule demonstrated by example that Dempster's rule of combination is not robust when combining highly conflicting belief functions. Shafer's discounted belief functions also suffers from this lack of robustness with respect to small perturbations in the discount factor. A modified version of Dempster's rule is proposed to remedy this difficulty. combination is discussed. It ::	perturbation theory	Hai-Yen Hau;Rangasami L. Kashyap	1989		10.1109/TAI.1989.65370	fuzzy logic;perturbation;uncertainty;dempster–shafer theory;bayesian probability;computer science;artificial intelligence;machine learning;discounting;data mining;data system;expert system;robustness	ML	-0.6597090966723789	-28.50938169751936	93164
be3f9ba65e655022ef45c0828b54509e84b94c4d	a first approach to solve classification problems based on functional networks	modelizacion;linear independence;metodo formal;methode formelle;intelligence artificielle;classification;formal method;modelisation;artificial intelligence;inteligencia artificial;reseau neuronal;modeling;clasificacion;red neuronal;neural network	In this paper the ability of the functional networks approach to solve classification problems is explored. Functional networks were introduced by Castillo et al. [1] as an alternative to neural networks. They have the same purpose, but unlike neural networks, neural functions are learned instead of weights, using families of linear independent functions. This is illustrated by applying several models of functional networks to a set of simulated data and to the well-known Iris data and Pima Indian data sets.		Rosa Eva Pruneda;Beatriz Lacruz;Cristina Solares	2005		10.1007/11550907_50	linear independence;formal methods;systems modeling;biological classification;computer science;artificial intelligence;machine learning;artificial neural network;algorithm	ML	9.988482548477682	-30.863291642620432	93616
74e5c9786a32c800242b23bdce47364ce532d42f	uncovering hierarchical structure in data using the growing hierarchical self-organizing map	hierarchical clustering;hierarchical structure;high dimensionality;information retrieval;data collection;data mining;feature space;three dimensional;data analysis;self organizing map som;data mining application;tree structure;unsupervised hierarchical clustering;self organized map;document classification;neural network model;growing hierarchical self organizing map;exploratory data analysis;neural network	Discovering the inherent structure in data has become one of the major challenges in data mining applications. It requires stable and adaptive models that are capable of handling the typically very high-dimensional feature spaces. In particular, the representation of hierarchical relations and intuitively visible cluster boundaries are essential for a wide range of data mining applications. Current approaches based on neural networks hardly fulfill these requirements within a single model.#R##N##R##N#In this paper we present the growing hierarchical self-organizing map (GHSOM), a neural network model based on the self-organizing map. The main feature of this novel architecture is its capability of growing both in terms of map size as well as in a three-dimensional tree-structure in order to represent the hierarchical structure present in a data collection during an unsupervised training process. This capability, combined with the stability of the self-organizing map for high-dimensional feature space representation, makes it an ideal tool for data analysis and exploration.#R##N##R##N#We demonstrate the potential of the GHSOM with an application from the information retrieval domain, which is prototypical both of the high-dimensional feature spaces frequently encountered in today's applications as well as of the hierarchical nature of data.	organizing (structure);self-organization;self-organizing map	Michael Dittenbach;Andreas Rauber;Dieter Merkl	2002	Neurocomputing	10.1016/S0925-2312(01)00655-5	three-dimensional space;feature vector;computer science;machine learning;pattern recognition;data mining;hierarchical clustering;tree structure;data analysis;exploratory data analysis;artificial neural network;statistics;data collection	Robotics	5.641418092715495	-34.23665706019161	93806
021eae6e174c894f258cfaf2a0ec503581ca5e2d	multi-granulation rough filters and rough fuzzy filters in pseudo-bci algebras			brain–computer interface	Songtao Shao;Xiaohong Zhang;Chunxin Bo;Choonkil Park	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-18144	discrete mathematics;machine learning;fuzzy logic;mathematics;granulation;artificial intelligence	Robotics	2.052613172774837	-24.54503074380774	93952
ac3ba3d567a360cb86aef070807677cd67221c63	on scaling of fuzzy fca to pattern structures		FCA is a mathematical formalism having many applications in data mining and knowledge discovery. Originally it deals with binary data tables. However, there is a number of extensions that enrich standard FCA. In this paper we consider two important extensions: fuzzy FCA and pattern structures, and discuss the relation between them. In particular we introduce a scaling procedure that enables representing a fuzzy context as a pattern structure. Studying the relation between different extensions of FCA is of high importance, since it allows migrating methods from one extension to another. Moreover, it allows for more simple implementation of different extensions within a software.	binary data;data mining and knowledge discovery;formal concept analysis;fuzzy set;image scaling;semantics (computer science)	Aleksey Buzmakov;Amedeo Napoli	2016			data mining;computer science;machine learning;fuzzy logic;artificial intelligence;knowledge extraction;theoretical computer science;software;formalism (philosophy of mathematics);binary data;scaling	ML	-4.185041868996234	-27.007386875157074	94136
33741a2f374302f6b8fd7d5deeede8154508ae09	a self-tuning fuzzy rule-based classifier for data streams	online rule weighting;data stream;classification;concept change	In recent years, tremendous amounts of data streams are generated in different application areas. The new challenges in these data need fast and online data processing, especially in classification problems. One of the most challenging problems in field of data streams that reduces the performance of traditional methods is concept change. To handle this problem, it is necessary to update the classifier system after every alteration of the concept of data. However, updating a classifier can often be a time consuming and expensive process. In this paper, an efficient method is proposed for quickly and easily updating of a fuzzy rule-based classifier by setting a weight for each rule. Then, two online procedures for online adjustment of the rule weights are proposed. The experimental results show the high performance of these methods against a non-weighted approach.	computation;concept drift;fuzzy rule;learning classifier system;logic programming;self-tuning;sensor;simpson's rule;statistical classification;time complexity	Homeira Shahparast;Sam Hamzeloo;Mansoor Zolghadri Jahromi	2014	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488514500147	biological classification;computer science;machine learning;pattern recognition;data mining	AI	-0.9399536176759924	-36.37974808210764	94256
05ebb6ee547830290e43709c705a9e8afec31c72	time series forecasting through rule-based models obtained via rough sets	forecasting;time series;rule based models;rough sets	Prediction models based on artificial intelligence techniques have been widely used in Time Series Forecasting in several areas. They are often fuzzy models or neural networks. However, the use of rough sets based models have not yet been explored. The aim of this work is to introduce a new approach which uses rough set concepts to obtain rule-based models capable to perform time series forecasting.	artificial intelligence;artificial neural network;discretization;experiment;fuzzy concept;logic programming;rough set;rule 90;statistical model;time series	Claudio Paulo Faustino;Carlos A. M. Pinheiro;Otávio Augusto S. Carpinteiro;Isaías Lima	2011	Artificial Intelligence Review	10.1007/s10462-011-9215-0	rough set;forecasting;computer science;machine learning;time series;data mining;dominance-based rough set approach	AI	3.4477307520627822	-26.05325047222321	94264
534e518b1595401b20e0c28a8853fe62fc1fc881	adaptive neurofuzzy anfis modeling of laser surface treatments	systeme temps reel;fonction green;traitement;tratamiento;systeme commande;sistema control;calcul neuronal;neural computation;treatment;sistema experto;anfis;materials processing;aplicacion;funcion green;development;neurofuzzy modeling of nonlinear processes;concepcion sistema;laser;real time;62f35;analysis and design;34b27;desarrollo;control system;surface treatment;65d17;system design;developpement;37n20;surface heat treatments;heat treatment;modele simulation;superficie;surface;real time system;sistema tiempo real;modelo simulacion;systeme expert;reseau neuronal;process monitoring and control;application;simulation model;supervision;red neuronal;computacion neuronal;monitoring and control;conception systeme;green function;neural network;expert system;laser materials processing applications	This paper introduces a new ANFIS adaptive neurofuzzy inference model for laser surface heat treatments based on the Green’s function. Due to its high versatility, efficiency and low simulation time, this model is suitable not only for the analysis and design of control systems, but also for the development of an expert real time supervision system that would allow detecting and preventing any failure during the treatment.	adaptive neuro fuzzy inference system;control system;sensor;simulation	José Antonio Pérez;Manuel González;Daniel Dopico	2009	Neural Computing and Applications	10.1007/s00521-009-0259-x	laser;adaptive neuro fuzzy inference system;computer science;control system;artificial intelligence;simulation modeling;heat treating;green's function;surface;expert system;artificial neural network;models of neural computation;systems design	Robotics	9.286689577077103	-24.946778190445595	94443
20916095c25faadcff4d8e5f6230c3515c2bc478	comparison of classifiers efficiency on missing values recovering: application in a marketing database with massive missing data	bayesian classifier;marketing database;decision tree;marketing data processing;pattern classification data mining database management systems marketing data processing;database management systems;database preprocessing;massive missing data;databases data mining artificial neural networks clustering algorithms bayesian methods classification tree analysis decision trees delta modulation backpropagation algorithms computational intelligence;record classification;record recovery;data mining;record selection;data mining application;missing value recovery;imbalanced databases;pattern classification;roc curve;missing data;missing values;record selection missing value recovery marketing database massive missing data data mining application imbalanced databases record classification database preprocessing record recovery;artificial neural network;value prediction	Missing data in databases are considered to be one of the biggest problems faced on data mining application. This problem can be aggravated when there is massive missing data in the presence of imbalanced databases. Several techniques as samples deletion, values imputation, values prediction through classifiers and approximation of patterns have been proposed and compared, but these comparisons do not consider adverse conditions found in real databases. In this work, it is presented a comparison of techniques used to classify records from a real imbalanced database with massive missing data, where the main objective is the database pre-processing to recover and select records completely filled for further techniques application. It was compared with other algorithms such as clustering, decision tree, artificial neural networks and Bayesian classifier, expressing their efficiency through ROC curves. Through the results, it can be verified that the problem characterization and database understanding are essential steps for a correct techniques comparison in a real problem. It was observed that artificial neural networks are an interesting alternative for this kind of problem since it was capable to obtain satisfactory results even when dealing with real-world problems.	approximation;artificial neural network;bayesian network;c4.5 algorithm;cluster analysis;data mining;data structure;database;decision tree;geo-imputation;missing data;naive bayes classifier;preprocessor;problem solving;receiver operating characteristic;robustness (computer science);subject-matter expert	Bruno Costa e Silva Nogueira;Tadeu R. A. Santos;Luis E. Zárate	2007	2007 IEEE Symposium on Computational Intelligence and Data Mining	10.1109/CIDM.2007.368854	missing data;computer science;data science;machine learning;pattern recognition;data mining;imputation;artificial neural network	DB	-0.7818863437806461	-32.27581485363307	94580
45d309a3ef66f212dddb4859bbefa162a309e743	an anomaly intrusion detection model based on limited labeled instances	density based clustering algorithm intrusion detection k means;cluster algorithm;kernel;optics;k means;intrusion detection;prior knowledge;density based clustering algorithm;data mining;kdd cup 1999 data set;anomaly intrusion detection model;false positive rate;classification algorithms;detection rate;clustering algorithms;clustering algorithms intrusion detection kernel data models optics data mining classification algorithms;network intrusion detection system;security of data;anomaly intrusion detection;kdd cup 1999 data set anomaly intrusion detection model network intrusion detection system;data models	Unsupervised or supervised anomaly intrusion detection techniques have great utility with the context of network intrusion detection system. However, large amount of labeled attack instances used by supervised approaches are difficult to obtain. And this makes most existing supervised techniques hardly be implemented in the real world. Unsupervised methods are superior in their independency on prior knowledge, but it is also very difficult for these methods to achieve high detection rate as well as low false positive rate. In this paper, we proposed an anomaly intrusion detection model based on small labeled instances that outperform existing unsupervised methods with a detection performance very close to that of the supervised one. We evaluated our methods by conducting experiments with network records from the KDD CUP 1999 data set. The results showed that our algorithm is an efficient method in detecting both known and unknown attacks.	algorithm;anomaly detection;cups;data breach;data mining;experiment;intrusion detection system;sensor;supervised learning;unsupervised learning	Shan-Qing Guo;Zhong-Hua Zhao	2008	2008 International Symposium on Electronic Commerce and Security	10.1109/ISECS.2008.26	anomaly-based intrusion detection system;statistical classification;intrusion detection system;data modeling;anomaly detection;kernel;false positive rate;computer science;machine learning;pattern recognition;data mining;cluster analysis;k-means clustering	Security	6.200677036125829	-37.32797456854785	94668
fdaad9bd234d4f7e542aa79911251ce4e74a4e4c	term clustering in texts based on fuzzy neighborhoods and kernel functions	cluster algorithm;kernel function;information system	A fuzzy neighborhood model for analyzing information systems having topological structures on occurrences of keywords is proposed and associated kernel functions are derived. Sufficient conditions when a neighborhood defines a kernel are described. Clustering algorithms with and without a kernel function are developed. Illustrative examples are given.		Sadaaki Miyamoto;Satoshi Hayakawa;Yuichi Kawasaki	2007		10.1007/978-3-540-74827-4_65	kernel;kernel method;discrete mathematics;string kernel;kernel embedding of distributions;radial basis function kernel;mean-shift;kernel principal component analysis;machine learning;pattern recognition;graph kernel;mathematics;tree kernel;variable kernel density estimation;polynomial kernel;kernel smoother	ML	-1.5970680821868608	-30.302346393445408	94714
612feaf81476cd0b128d92d3d3a533abda3ac7c7	flexible design of image classification rules using extended fuzzy oriented classifier evolution	optimisation directed graphs fuzzy set theory image classification inference mechanisms knowledge based systems;mf optimization image classification rule fuzzy oriented classifier evolution force method directed graph membership function optimization	We have previously proposed Fuzzy ORiented Classifier Evolution (FORCE), a method to construct fuzzy classification rules automatically with evolution of a directed graph composed of fuzzy conditions. In this work, we introduce (1) flexible optimization of Membership Functions (MFs) for each condition and (2) comparison operators between two input variables into FORCE. The original FORCE has used fixed MFs because optimizing MFs can decrease interpretability of rules. However optimizing MFs can also improve accuracy and compactness of rules. Hence we introduce flexible optimization of MFs into FORCE to investigate its effectiveness. The comparison operators between two input variables enable rules to consider relationships between two input variables easily, and can also improve performance of FORCE. We apply the proposed model to three different image classification tasks to investigate its performance in comparison with conventional methods.	computer vision;directed graph;evolution;fuzzy classification;fuzzy rule;genetic algorithm;kinetic data structure;mathematical optimization;membership function (mathematics);relational operator	Junji Otsuka;Tomoharu Nagao	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505082	mathematical optimization;fuzzy classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;algorithm	Robotics	3.792874793593814	-29.71121936934068	94889
d5a10499ecd771d4937a04417aaaed23c895d37e	3d function approximation with rgcs classifier system	random access memory;classifier system;learning classifier system;satisfiability;argon;grammar based classifier system;genetic algorithm 3d function approximation rgcs classifier system grammar based classifier system context free grammar sentences;ear;function approximation;3d function approximation;three dimensional displays;context free grammar;context free grammar sentences;pattern classification;function approximation intelligent robots context modeling genetic algorithms benchmark testing space exploration intelligent systems application software control engineering computing design engineering;on the fly;genetic algorithm;genetic algorithms;classification tree analysis;pattern classification function approximation genetic algorithms;rgcs classifier system;gallium	Extended classifier systems (XCS) introduced by Wilson became popular instrument used to solve many real-world problems possible to express with ternary representation. Our new model of real-valued LCS - the rGCS - is designed to classify real valued data. rGCS is based on grammar-based classifier system (GCS), which was originally used to process context free grammar sentences. To improve 3D function approximation a covering technique was developed. This procedure replenishes the population of system classifiers with new ones created on the fly to satisfy current state of grammar evolution. As a result there is no need to employ a genetic algorithm.	approximation;context-free grammar;genetic algorithm;learning classifier system;on the fly	Lukasz Cielecki;Olgierd Unold	2008	2008 Eighth International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2008.179	natural language processing;genetic algorithm;computer science;artificial intelligence;machine learning;pattern recognition;learning classifier system	Robotics	6.488924959370883	-30.861809063946954	94900
0f1dd7e297bbbc28469fb19f4135f0a1beac3fbd	transforming between propositions and features: bridging the gap	bayesian network modeling;structural analogy;terrorist activity;fully-specified probabilistic model;automatic knowledge acquisition;arbitrary propositional case description;probabilistic generalization;terrorist action;structural representation;structural data	It is notoriously difficult to simultaneously deal with both probabilistic and structural representations in A.I., particularly because probability necessitates a uniform representation of the training examples. In this paper, we show how to build fully-specified probabilistic models from arbitrary propositional case descriptions about terrorist activities. Our method facilitates both reasoning and learning. Our solution is to use structural analogy to build probabilistic generalizations about those cases. We use these generalizations as a framework for mapping the structural representations, which are well-suited for reasoning, into features, which are well-suited for learning, and back again. Finally, we demonstrate how probabilistic generalizations are an excellent bridge for joining reasoning and learning by using them to perform a traditional machine learning technique, Bayesian network modeling, over arbitrarily high order structural data about terrorist actions, and further, we discuss how this might be used to facilitate automatic knowledge acquisition.	bayesian network;bridging (networking);knowledge acquisition;machine learning	Daniel T. Halstead;Kenneth D. Forbus	2005			computer science;artificial intelligence;machine learning;data mining	AI	5.987208407746414	-31.33219965431845	94954
d1f6fd83292e3476d2935c22abde9b4da7a55823	investigation of owa operator weights for estimating fuzzy validity of geometric shapes		The estimation of fuzzy validity (f-validity) of complex fuzzy objects (f-objects) by using fuzzy geometry (f-geometry) may be a useful tool in reveal- ing unknown links or patterns e.g. finger prints, shoe print, face sketch of a criminal etc. at crime site. The Extended Fuzzy Logic (FLe) is a combination of Fuzzy Logic (FL) and Unprecisiated Fuzzy Logic (FLu). Whenever a precise solution of any problem is either impossible or bit costlier, then we opt for the concept of FLe. The f-geometry is an example of Unprecisiated Fuzzy Logic (FLu). The f-geometry has different f-objects like f-point, f-line, f -circle, f-triangle, etc. The aggregation models can be used for aggregating the compo- nent of f-objects. The Minimizing Distance from extreme Point (MDP), which is a nonlinear ordered weighted averaging (OWA) objective model, is used to estimate f-validity of fuzzy objects. The results generated by the MDP model are found closer to degree of OR-ness. The objective of this paper is to lay the foundation and encourage further discussion on the development ofmethods for defining as well as estimating f-validity of some more complex f-objects for forensic investigation services.		Abdul Rahman;M. M. Sufyan Beg	2013		10.1007/978-3-319-03674-8_2	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Theory	-1.881874356230306	-24.28994590941337	95129
61c7d6104dee9a5dc4b008c983da95e7ee344c9a	modeling sensor reliability in fault diagnosis based on evidence theory	evidential conflict;biological patents;biomedical journals;belief function;text mining;europe pubmed central;deng entropy;citation search;citation networks;sensor data fusion;research articles;sensor reliability;abstracts;open access;life sciences;clinical guidelines;dempster shafer evidence theory;full text;rest apis;orcids;europe pmc;biomedical research;fault diagnosis;bioinformatics;literature search	Sensor data fusion plays an important role in fault diagnosis. Dempster-Shafer (D-R) evidence theory is widely used in fault diagnosis, since it is efficient to combine evidence from different sensors. However, under the situation where the evidence highly conflicts, it may obtain a counterintuitive result. To address the issue, a new method is proposed in this paper. Not only the statistic sensor reliability, but also the dynamic sensor reliability are taken into consideration. The evidence distance function and the belief entropy are combined to obtain the dynamic reliability of each sensor report. A weighted averaging method is adopted to modify the conflict evidence by assigning different weights to evidence according to sensor reliability. The proposed method has better performance in conflict management and fault diagnosis due to the fact that the information volume of each sensor report is taken into consideration. An application in fault diagnosis based on sensor fusion is illustrated to show the efficiency of the proposed method. The results show that the proposed method improves the accuracy of fault diagnosis from 81.19% to 89.48% compared to the existing methods.	conflict (psychology);entity name part qualifier - adopted;hearing aid, shafer cortical;handling (psychology);mathematical model;matthews correlation coefficient;numerous;sensor web;weight	Kaijuan Yuan;Fuyuan Xiao;Liguo Fei;Bingyi Kang;Yong Deng	2016		10.3390/s16010113	text mining;medical research;soft sensor;computer science;bioinformatics;engineering;artificial intelligence;data science;data mining;statistics	AI	-0.6000103375238391	-27.670985719289177	95286
ae494385583aab56371a8c142d34ed9a0a18f823	using auto-associative neural networks to compress and visualize multidimensional data	neural networks;training;oils;principal component analysis;data visualization;neurons;iris	To interpret the information hidden in multidimensional data can be considered as challenging and complicated task. Usually, dimension reduction or data compression is considered as the first step to data analysis and exploration of multidimensional data. Here, the focus is given to study Auto-Associative Neural Networks (AANNs) technique for data compression and visualization. AANNs have the ability to deal with linear and nonlinear correlation among variables. This technique is often referred to as nonlinear Principal Component Analysis (NLPCA) or could also be known as Bottleneck Neural Networks (BNNs) due to their specific structures that consist of combination of compression and decompression networks. The trained AANNs can reduce high dimensional data onto lower dimensional data by compressing them on its bottleneck layer that later can be used for data visualization. In this paper, the technique of AANNs are described, developed using high level computer language and applied on two different multidimensional datasets. The results have shown that AANNs are able to compress multidimensional data into only two non linear principal components at its bottleneck layer and these compressed data can provide visualization of different clusters of data.	algorithm;artificial neural network;c++;cluster analysis;computer language;data compression;data visualization;dimensionality reduction;heart rate variability;high-level programming language;multidimensional signal processing;nonlinear system;norm (social);principal component analysis	Zalhan Mohd Zin	2014	2014 11th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)	10.1109/URAI.2014.7057451	computer science;artificial intelligence;theoretical computer science;machine learning;data mining;data visualization;principal component analysis	DB	5.469280707957092	-34.45228887660233	95329
111dc2f3ffdcdcc7dddb9b08c0f2a0c539380799	improved genetic algorithm for the context-free grammatical inference		Inductive learning of formal languages, often called grammatical inference, is an active area in machine learning and computational learning theory. By learning a language we understand finding the grammar of the language when some positive (words from language) and negative examples (words that are not in language) are given. Learning mechanisms use the natural language learning model: people master a language, used by their environment, by the analysis of positive and negative examples. The problem of inferring context-free languages (CFG) has both theoretical and practical motivations. Practical applications include pattern recognition (for example finding DTD or XML schemas for XML documents) and speech recognition (the ability to infer context-free grammars for natural languages would enable speech recognition to modify its internal grammar on the fly). There were several attempts to find effective learning methods for context-free languages (for example [1,2,3,4,5]). In particular, Y.Sakakibara [3] introduced an interesting method of finding a context-free grammar in the Chomsky normal form with a minimal set of nonterminals. He used the tabular representation similar to the parse table used in the CYK algorithm, simultaneously with genetic algorithms. In this paper we present several adjustments to the algorithm suggested by Sakakibara. The adjustments are concerned mainly with the genetic algorithms used and are as follows: – we introduce a method of creating the initial population which makes use of characteristic features of context-free grammars, – new genetic operations are used (mutation with a path added, ‘die process’, ‘war/disease process’), – different definition of the fitness function, – an effective compression of the structure of an individual in the population is suggested. These changes allow to speed up the process of grammar generation and, what is more, they allow to infer richer grammars than considered in [3].	cyk algorithm;chomsky normal form;computational learning theory;context-free grammar;context-free language;fitness function;formal language;genetic algorithm;grammar induction;lr parser;machine learning;natural language;on the fly;parsing;pattern recognition;speech recognition;table (information);xml schema	Adrianna Gietka	2007	Annales UMCS, Informatica		computer science;context-sensitive grammar;affix grammar;ambiguous grammar;programming language;extended affix grammar;natural language processing;grammar induction;artificial intelligence;context-free grammar;context-free language;stochastic context-free grammar	NLP	6.553911734540391	-31.286800725829806	95633
350718bacaa710eefc40ed62904fe5d6aec4d258	fault diagnosis of an air-handling unit system using a dynamic fuzzy-neural approach	fuzzy neural network;error reduction;diagnostic tool;functional equivalence;building automation systems;takagi sugeno kang;air handling unit;fault detection and diagnosis;fuzzy model;fault diagnosis;neural network	This paper presents a diagnostic tool to be used to assist building automation systems for sensor heath monitoring and fault diagnosis of an Air-Handling Unit (AHU). The tool employs fault detection and diagnosis (FDD) strategy based on an Efficient Adaptive Fuzzy Neural Network (EAFNN) method. EAFNN is a Takagi-Sugeno-Kang (TSK) type fuzzy model which is functionally equivalent to the Ellipsoidal Basis Function (EBF) neural network neurons. An EAFNN uses the combined pruning algorithm where both Error Reduction Ratio (ERR) method and a modified Optimal Brain Surgeon (OBS) technology are used to remove the unneeded hidden units. Simulation works show the proposed diagnosis algorithm is very efficient which can not only reduce the complexity of the network but also accelerate the learning speed.	fuzzy concept	Juan Du;Meng Joo Er;Leszek Rutkowski	2010		10.1007/978-3-642-13208-7_8	building automation;computer science;artificial intelligence;machine learning;control theory;artificial neural network	Robotics	8.00543120186207	-26.231659469895213	95748
8c8301adfbc211c3b8c3399954a8c2f17edec183	exploring different rule quality evaluation functions in aco-based classification algorithms	evaluation function;pareto optimisation;classification algorithm;ant colony optimization;pareto frontier;training;prediction algorithms;data mining;aco based classification algorithm;accuracy;quality assessment;pheromone update;quality evaluation;classification rules;multiple pheromone type;prediction accuracy;pattern classification;predictive models;quality management knowledge engineering pareto optimisation pattern classification;entropy;prediction model;probabilistic logic;rule quality evaluation function;training accuracy predictive models prediction algorithms entropy data mining probabilistic logic;μant miner algorithm;rule quality evaluation functions;ant miner classification rule discovery algorithm;quality management;ant colony optimization rule quality evaluation function aco based classification algorithm μant miner algorithm ant miner classification rule discovery algorithm multiple pheromone type rule quality evaluation functions quality assessment pheromone update pareto frontier;knowledge engineering	The μAnt-Miner algorithm is an extension of the well-known Ant-Miner classification rule discovery algorithm. μAnt-Miner utilizes multiple pheromone types, one for each permitted rule class. An ant would first select the rule class and then deposit the corresponding type of pheromone. In this paper, we explore the use of different rule quality evaluation functions for rule quality assessment prior to pheromone update. The aim of this investigation is to discover how the use of different evaluation function affects the output model in terms of predictive accuracy and model size. In our experimental results, we use 10 different rule quality evaluation functions on 13 benchmark datasets, and identify a Pareto frontier of 4 evaluation functions.	algorithm;association rule learning;benchmark (computing);evaluation function;pareto efficiency;whole earth 'lectronic link	Khalid M. Salama;Ashraf M. Abdelbar	2011	2011 IEEE Symposium on Swarm Intelligence	10.1109/SIS.2011.5952574	engineering;machine learning;pattern recognition;data mining	AI	3.4027856731185975	-29.78794798316136	96104
4bbe0ff9c64e1a3ffee44a5f302c1a2277db1d06	type-1 or interval type-2 fuzzy logic systems — on the relationship of the amount of uncertainty and fou size	type 2 fuzzy logic systems fou size type 2 flss chaotic time series prediction very wide interval type 2 fuzzy sets footprint of uncertainty type 1 fuzzy sets type 1 fuzzy logic system;noise fuzzy set interval type 2 uncertainty footprint of uncertainty quantification of uncertainty;uncertainty frequency selective surfaces noise fuzzy sets testing time series analysis fuzzy logic;time series fuzzy logic fuzzy reasoning fuzzy set theory	"""A recurring theme in research employing type-2 fuzzy sets is the question of how much uncertainty in a given context warrants the application of type-2 fuzzy sets and systems over their type-1 counterparts. In this paper we provide insight into this challenging question through a detailed investigation into the ability of both types of Fuzzy Logic Systems (FLSs) to capture and model different levels of uncertainty/noise through varying the size of the Footprint Of Uncertainty (FOU) of the underlying fuzzy sets from type-1 fuzzy sets to very """"wide"""" interval type-2 fuzzy sets. By applying the study in the well-controlled context of chaotic time-series prediction, we show how, as uncertainty/noise increases, type-2 FLSs with fuzzy sets with FOUs of increasing size become more and more viable. While the work in this paper is focused on a specific application, we believe it provides crucial insight into the challenging question of the viability of interval type-2 over type-1 FLSs."""	formal system;fuzzy logic;fuzzy set;time series;type-1 owa operators;type-2 fuzzy sets and systems;whole earth 'lectronic link	Jabran Hussain Aladi;Christian Wagner;Jonathan M. Garibaldi	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891593	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;control theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Robotics	-0.09732506329812828	-24.67959502331972	96618
cc0d77c49930ae3edddf0964ef46ead6edd00ee2	an architecture for constructing fuzzy regression tree forests using opt-ainet	artificial immune system fuzzy regression tree forests fuzzy inference system fuzzy regression tree data mining machine learning evolutionary algorithms;optimisation;fuzzy regression;fuzzy reasoning;artificial immune system;training;prediction algorithms;trees mathematics artificial intelligence fuzzy reasoning fuzzy set theory optimisation regression analysis;trees mathematics;data mining;fuzzy regression tree;fuzzy set theory;vegetation;machine learning;opt ainet modified elgasir fuzzy regression tree algorithm trapezoidal membership functions fuzzification takagi sugeno fuzzy inference artificial immune network model simultaneous optimization boston housing abalone uci repository;fuzzy inference system;evolutionary algorithms;vegetation regression tree analysis optimization prediction algorithms training inference algorithms;artificial intelligence;regression analysis;inference algorithms;optimization;fuzzy regression tree forests;evolutionary algorithm;optimal prediction;regression tree analysis	This paper presents a new approach to combining multiple fuzzy regression trees, which are induced by applying the modified Elgasir fuzzy regression tree algorithm. This method utilises Trapezoidal membership functions for fuzzification and the Takagi-Sugeno fuzzy inference to obtain the final predicted values. A modified version of Artificial Immune Network model (opt-aiNet) is used for the simultaneous optimization of the membership functions across all trees within the forest. Boston housing and Abalone are two real-world datasets from the UCI repository used to evaluate the proposed approach. The empirical results have showed that fuzzy regression tree forests reduce the error rate compared with single fuzzy regression tree.	abalone;algorithm;decision tree learning;fuzzy set;list of algorithms;mathematical optimization;network model;the forest	Fathi Gasir;Zuhair Bandar;Keeley A. Crockett	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007523	prediction;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;machine learning;evolutionary algorithm;pattern recognition;data mining;mathematics;fuzzy set;artificial immune system;vegetation;regression analysis	Robotics	6.823869095550203	-26.294796696896682	96792
2c9dc361ea29d872c5df9703929802210055970c	a model for the detection of underlying trends in temporal data		"""Trend detection in financial temporal data is a significant problem, with far-reaching applications, that presents researchers with many challenges. Existing techniques require users to choose a given interval, and then provide an approximation of the data on that interval; they always produce some approximation, namely, a member of a class of candidate functions that is """"best"""" according to some criteria. Moreover, financial analysis can be performed from different perspectives, at different levels, from short term to long term; it is therefore very desirable to be able to indicate a scale that is suitable and adapted to the analysis of interest. Based on these considerations, our objective was to design a method that lets users input a scale factor, determines the intervals on which an approximation captures a significant trend as a function of the scale factor, and proposes a qualification of the trend. The method we use combines various machine-learning and statistical techniques, a key role being played by a change-point detection method. We describe the architecture of a system that implements the proposed method. Finally, we report on the experiments we ran and use their results to stress how they differ from the results than can be obtained from alternative approaches."""	approximation;decision support system;experiment;hysteresis;machine learning;pattern recognition;sensor;time series	Ity Kaul;Eric Martin;Vishal Puri	2017	2017 12th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)	10.1109/ISKE.2017.8258812	scale factor;data mining;time series;market research;data pre-processing;financial analysis;temporal database;architecture;computer science	Robotics	0.7344436111869331	-33.07545741533044	96840
32b9f60cfe8e0d13d3856ec23bd8d881768e0c57	a dynamic adaptation of ad-trees for efficient machine learning on large data sets	learning algorithm;decision tree;large data sets;machine learning;association rule;tree structure;dynamic adaptation;empirical evaluation;data structure	This paper has no novel learning or statistics: it is concerned with making a wide class of preexisting statistics and learning algorithms computationally tractable when faced with data sets with massive numbers of records or attributes. It briefly reviews the static AD-tree structure of Moore and Lee (1998), and offers a new structure with more attractive properties: (1) the new structure scales better with the number of attributes in the data set; (2) it has zero initial build time; (3) it adaptively caches only statistics relevant to the current task; and (4) it can be used incrementally in cases where new data is frequently being appended to the data set. We provide a careful explanation of the data structure, and then empirically evaluate the performance under varying access patterns induced by different learning algorithms such as association rules, decision trees and Bayes net structures. We conclude by discussing the longer term benefits of the new structure: the eventual ability to apply AD-trees to data sets with real-valued attributes. 1. Description of AD-trees 1.1 What is an AD-tree? Table 1 shows a tiny data set with M 3 symbolic (i.e., categorical) attributes (the columns), and R 6 records (the rows). A counting query has the form C(a1 2 a2 a3 1), and is a request to count the number of records matching the query, with asterisks interpreted as “don’t cares”. C(a1 2 a2 a3 1)=3 in our example. Moore and Lee (1998) and Anderson and Moore (1998) introduced a new data structure for representing the cached counting statistics for a categorical data set, called an AllTable 1. Sample data set with three attributes and six records. ATTRIBUTES: a1 a2 a3 RECORD1 1 1 1 RECORD2 2 3 1 RECORD3 2 4 2 RECORD4 1 1 1 RECORD5 2 3 1 RECORD6 2 3 1 a2=1 MCV a2=2 Null a2=3 Null a2=4 Null a3=1 MCV a3=2 Null a3=1 MCV a3=2 Null a3=1 Null a3=2 MCV Vary a2 Vary a3 Vary a3 Vary a3 c=2 a2=1 a2=2 Null a2=3 MCV c=1 a2=4 c=2 a1=1 a1=2 MCV Vary a1 Vary a2 count=6 a3=* a2=* a1=*	algorithm;association rule learning;bayesian network;categorical variable;cobham's thesis;column (database);data structure;decision tree;machine learning;mobile television;tree structure	Paul Komarek;Andrew W. Moore	2000			semi-supervised learning;association rule learning;data structure;decision tree learning;computer science;data science;online machine learning;machine learning;decision tree;data mining;tree structure;active learning;statistics	ML	-1.8919792667968807	-36.25832701313359	97036
f20567734d1af58cd77dfb16b3543ac53236f356	probabilistic coherence weighting for optimizing expert forecasts	probabilistic coherence;linear opinion pool;forecast aggregation;fifty fifty blip;practice;crowdsourcing	"""Methods for eliciting and aggregating expert judgment are necessary when decision-relevant data are scarce. Such methods have been used for aggregating the judgments of a large, heterogeneous group of forecasters, as well as the multiple judgments produced from an individual forecaster. This paper addresses how multiple related individual forecasts can be used to improve aggregation of probabilities for a binary event across a set of forecasters. We extend previous efforts that use probabilistic incoherence of an individual forecaster’s subjective probability judgments to weight and aggregate the judgments of multiple forecasters for the goal of increasing the accuracy of forecasts. With data from two studies, we describe an approach for eliciting extra probability judgments to (i) adjust the judgments of each individual forecaster, and (ii) assign weights to the judgments to aggregate over the entire set of forecasters. We show improvement of up to 30% over the established benchmark of a simple equal-weighted averaging of forecasts. We also describe how this method can be used to remedy the “fifty–fifty blip” that occurs when forecasters use the probability value of 0.5 to represent epistemic uncertainty."	optimizing compiler	Christopher W. Karvetski;Kenneth C. Olson;David R. Mandel;Charles Twardy	2013	Decision Analysis	10.1287/deca.2013.0279	econometrics;artificial intelligence;data mining;crowdsourcing	HPC	0.5753592991998969	-32.05294633808868	97493
cf603ef160c09ce1e051e2820e7e3a2cb276c1bf	mining uncertain data streams using clustering feature decision trees	any-time property;clustering feature vector;cfdtu model;uncertain data stream;complete data;clustering feature decision tree;uncertain attribute value;uncertain clustering algorithm;deterministic method;uncertain example;data stream	During the last decade, classification from data streams is based on deterministic learning algorithms which learn from precise and complete data. However, a multitude of practical applications only supply approximate measurements. Usually, the estimated errors of the measurements are available. The development of highly efficient algorithms dealing with uncertain examples has emerged as an new direction. In this paper, we build a CFDTu model from data streams having uncertain attribute values. CFDTu applies an uncertain clustering algorithm that scans the data stream only once to obtain the sufficient statistical summaries. The statistics are stored in the Clustering Feature vectors, and are used for incremental decision tree induction. The vectors also serve as classifiers at the leaves to further refine the classification and reinforce any-time property. Experiments show that CFDTu outperforms a purely deterministic method in terms of accuracy and is highly scalable on uncertain data streams.	cluster analysis;decision tree learning;uncertain data	Wenhua Xu;Zheng Qin;Hao Hu;Nan Zhao	2011		10.1007/978-3-642-25856-5_15	computer science;machine learning;pattern recognition;data mining;statistics	ML	-1.4066405635154609	-36.68329546694365	97664
a30798ef57ae93c12522ea2cd68611721a81bcf5	a truth space diagram temporal linguistic rule extraction procedure using multiple objective evolutionary algorithm	water mixer multiple objective evolutionary algorithm temporal linguistic rule extraction procedure truth space diagram decision support systems fuzzy controllers data preprocessing rule set postprocessing;fuzzy controller;evolutionary computation;data collection;rule extraction;multiple objectives;decision support system;computational linguistics evolutionary computation;computational linguistics;evolutionary algorithm;evolutionary computation expert systems automatic control control systems humans application software fuzzy systems process control decision support systems fuzzy control	Autonomous temporal linguistic rule extraction is an application of growing interest for its relevance to both decision support systems and fuzzy controllers. In the presented work, rules are evaluated using three qualitative metrics based on their representation on the truth space diagram. Each metric is then treated as a conflicting optimization goal and multiple objective evolutionary algorithm is used to obtain a set of optimal non-dominant rules. Novel techniques for data pre-processing and rule set post-processing are designed. Data collected from a simulated hot and cold water mixer is used to validate the proposed procedure.	data pre-processing;decision support system;diagram;evolutionary algorithm;fuzzy logic;mathematical optimization;preprocessor;relevance;rule induction;video post-processing	Pedro G. DeLima;Gary G. Yen	2004	2004 IEEE International Conference on Fuzzy Systems (IEEE Cat. No.04CH37542)	10.1109/FUZZY.2004.1375795	decision support system;computer science;artificial intelligence;computational linguistics;machine learning;evolutionary algorithm;data mining;evolutionary computation;data collection	Robotics	2.830972660626198	-27.900661989018076	97893
2375ec32deace9727a87caec0644243c05796407	a study on the behaviour of the algorithm for finding relevant attributes and membership functions	membership function		algorithm;membership function (mathematics)	Madara Gasparovica;Ludmila Aleksejeva	2009	J. Riga Technical University	10.2478/v10143-010-0010-1	membership function;computer science;machine learning;data mining	Theory	2.062511923219322	-26.15664070848725	98010
b0175f2602256e71c4f40b96b6997422db38f39c	analysis and understanding of multi-class invoices	structure logique;document analysis;layout structure;layout problem;knowledge construction;probleme agencement;connaissance domaine classe dependante;base connaissance;logical structure;class dependent domain knowledge cddk;domain knowledge;physical object;logical object;traitement document;problema disposicion;class independent domain knowledge cidk;base conocimiento;document processing;class knowledge ck;connaissance domaine classe independante;tratamiento documento;knowledge base	In this paper a system for processing documents that can be grouped into classes is illustrated. We have considered invoices as a case-study. The system is divided into three phases: document analysis, classification, and understanding. We illustrate the analysis and understanding phases. The system is based on knowledge constructed by means of a learning procedure. The experimental results demonstrate the reliability of our document analysis and understanding procedures. They also present evidence that it is possible to use a small learning set of invoices to obtain reliable knowledge for the understanding phase.	bottom-up proteomics;document;experiment;pixel;statistical classification;test set;top-down and bottom-up design	Francesca Cesarini;Enrico Francesconi;Marco Gori;Giovanni Soda	2003	Document Analysis and Recognition	10.1007/s10032-002-0084-6	knowledge base;document processing;computer science;artificial intelligence;data mining;structure;domain knowledge;algorithm	NLP	-2.429252903492616	-31.243189615313558	98464
1efa73ed375198137dc75d1d0a70913da0c10eaf	locating matching rules by mining software change log	software maintenance;data mining;neural network	A software system maintenance activity is typically performed under an environment of lacking knowledge about how to process it. This scarcity of knowledge may be caused by various factors, such as the large size and complexity of the systems, high staff turnover, poor documentation and long-term system maintenance. The study applies Apriori algorithm to extract information from software change logs. Unfortunately, the software change logs generate many rules. Because searches the suitable rule from many rules is difficult and important matter, especially. This study focuses on the software co-change dependency and proposes a classification model based on association mining, to deal with such kind of dependency. The model combines data mining technologies, the traditional decision-tree and neural learning capabilities, to handle the complicated and real cases, and then improve the rule searching efficiency and the matching accuracy.	apriori algorithm;artificial neural network;association rule learning;complexity;data mining;decision tree;documentation;software system	Ming-Shi Wang;Jung-Te Weng	2006		10.2991/jcis.2006.303	software mining;computer science;pattern recognition;data mining;database	SE	4.607146920901112	-31.46424739269033	98510
3fef492d112aed1374a40eadca6805eb2fd2ca79	some ideas of informational deep neural networks structural organization		Deep Learning (DL) is a branch of machine learning based on multiple processing layers with complex structure, or otherwise composed of multiple non-linear transformations. Diverse DL models are used for solving different tasks, but have some common features and identic problems. Eight ideas for DL features organization and problems solving are outlined, the DL main goal is specified. The elements and structure of deep learning neuro informational model are discussed.	deep learning;neural networks	Vladimir Smolin	2016		10.1007/978-3-319-40663-3_66	management science;operations research	Logic	7.241959214149223	-24.79116837843322	98630
d4e214e66ca77e13da8d08bce367d148118fcda0	geometric associative memories and their applications to pattern classification	quadratic program;inner product;pattern classification;associative memory;geometric algebra;problem solving	Associative memories (AMs) were proposed as tools usually used in the restoration and classification of distorted patterns. Many interesting models have emerged in the last years with this aim. In this chapter a novel associative memory model (Geometric Associative Memory, GAM) based on Conformal Geometric Algebra (CGA) principles is described. At a low level, CGA provides a new coordinate-free framework for numeric processing in problem solving. The proposed model makes use of CGA and quadratic programming to store associations among patterns and their respective class. To classify an unknown pattern, an inner product is applied between it and the obtained GAM. Numerical and real examples to test the proposal are given. Formal conditions are also provided that assure the correct functioning of the proposal.		Benjamín Cruz;Ricardo Barrón;Juan Humberto Sossa Azuela	2010		10.1007/978-1-84996-108-0_11	discrete mathematics;theoretical computer science;machine learning;mathematics	ML	1.91030103376304	-28.924290456574465	98675
3075ddace4e517dd57de4689252af289f6418360	constructing fuzzy ontology by using method of concept distance clustering	lattices;lattices ontologies educational institutions clustering methods clustering algorithms buildings context;fuzzy ontology constructing conceptual distance clustering fuzzy concept lattice;clustering algorithms;ontologies;clustering methods;context;buildings	With the wide application of ontology in the field of computer, ontology aroused people's concern. In order to conduct nubilous object, Fuzzy ontology is introduced. How to construct the fuzzy ontology becomes a meaningful work. It is known that the inaccuracy of information is high, which leads to inefficiency in the manual construction of fuzzy ontology. To solve this problem, conceptual distance clustering method based fuzzy ontology construction is proposed to meet the user need to the maximum extent, which is combined with fuzzy set theory and fuzzy concept lattice. Firstly, fuzzy concept lattice is constructed by gradual mode method. Secondly, fuzzy parameters are used to process conceptual distance clustering, and fuzzy concept hierarchy is obtained. Finally, a fuzzy ontology is mapped and gotten. The experimental results show that both usability and effectiveness of the proposed method are better than the former methods.	algorithm;cluster analysis;formal concept analysis;fuzzy concept;fuzzy logic;fuzzy set;information needs;set theory;usability	Ning Liu;Feng-Jiao Sun;Bo Ning;Qiang-Qiang Li	2013	2013 IEEE 10th International Conference on High Performance Computing and Communications & 2013 IEEE International Conference on Embedded and Ubiquitous Computing	10.1109/HPCC.and.EUC.2013.180	defuzzification;fuzzy clustering;flame clustering;fuzzy classification;computer science;ontology;fuzzy number;theoretical computer science;neuro-fuzzy;machine learning;lattice;data mining;fuzzy set;fuzzy associative matrix;cluster analysis;ontology-based data integration;fuzzy set operations;conceptual clustering	Robotics	-2.0038456362582258	-26.099409686713695	99015
bc7dfc5e840fe58fc129e26ab172797977372835	description, composition, and decision support for multiagent computational systems	stress;computational intelligence intelligent agents multiagent systems neural networks;decision support;multiagent system;software agents data mining decision making decision support systems genetic algorithms multi agent systems neural nets;neural networks;neural nets;belief desire intention;computational intelligence;vertically layered architecture;data mining;software agents;computer architecture;accuracy;multi agent systems;intelligent agents;logic gates;monitoring;belief desire intention architecture decision support multiagent computational systems decision making intelligent agents computational intelligence neural networks genetic algorithms data mining problem description vertically layered architecture;decision support systems;multiagent computational systems;intelligent agent;genetic algorithm;genetic algorithms;belief desire intention architecture;computer architecture multiagent systems artificial intelligence computational intelligence intelligent agent neural networks genetic algorithms data mining humans artificial neural networks;multiagent systems;neural network;problem description	Abstract—A decision making architecture for intelligent agents is presented in this paper. It allows agents to perform autonomous behavior with respect to its own goals and cooperation with other agents. We focus on computational agents that encapsulate various methods of computational intelligence, such as neural networks, genetic algorithms, and similar methods. The goal of these agents is typically to solve a data mining problem characterized by a data set together with problem description. The architecture is based on the vertically-layered and belief-desire-intention architectures. Several experiments with computational agents were conducted to demonstrate the benefits of the architecture.	agent-based model;artificial neural network;autonomous robot;computation;computational intelligence;data mining;experiment;expressive power (computer science);genetic algorithm;intelligent agent;mathematical optimization;ontology (information science);search algorithm;software agent;time complexity	Roman Neruda	2009	2009 21st IEEE International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2009.126	agent architecture;genetic algorithm;computer science;artificial intelligence;machine learning;data mining;artificial neural network	AI	5.776595677549131	-28.40890979441512	99029
a14aa4f6fc7e9f4e9e3b83ae95325c22e576e38f	self-organizing feature map for cluster analysis in multi-disease diagnosis	vegetable disease;tomato;search algorithm;model performance;euclidean distance;multi disease diagnosis;cluster analysis;self organized feature map;self organizing map;self organized map;optimal algorithm	Aiming at the multi-disease diagnosis, a self-organizing map (SOM) is developed. In this paper the tomato disease features are extracted and a mapping relationship between the diseases and the features is created. The inaccurate clustering of traditional SOM algorithm is analyzed. According to the analysis, Euclidean distance is taken as the main discrimination, and the adjacent-searching algorithm is optimized. Using the optimized algorithm, the cluster results of input samples are obtained, features of diseases are mapped, and a multi-disease diagnosis model is developed. The proposed SOM-based model has two layers. The feature array of diseases can be accurately and rapidly sorted and clustered using this model. This model can achieve an accurate diagnosis of multi-diseases. The simulation results show that the proposed model performs well and the proposed multi-disease diagnosis is effective. 2010 Elsevier Ltd. All rights reserved.	cluster analysis;euclidean distance;organizing (structure);search algorithm;self-organization;self-organizing map;simulation	Ke Zhang;Yi Chai;Simon X. Yang	2010	Expert Syst. Appl.	10.1016/j.eswa.2010.02.084	self-organizing map;computer science;bioinformatics;machine learning;data mining;euclidean distance;cluster analysis;search algorithm	AI	4.7907697414537616	-35.303633947803405	99366
ce395498cf90f60c04dfb6bf1ce677dd51a72e8f	fuzzy qmd algorithm for mining fuzzy association rules		Association rules mining is to find associations efficiently among the different items of a transaction database. In order to help decision-makers conduct sound and timely solutions, we apply fuzzy partition method and combine QMD (Quick Modulized Decomposition) to propose a novel fuzzy data mining method. The proposed method is mainly generated fuzzy itemsets by MAP modulized, and uses fuzzy minimal fuzzy support and minimum fuzzy confidence to generate fuzzy association rules. The method only needs to scan whole transaction database once and uses this modulized method to increase the performance of mining process. Furthermore, in fuzzy partition, the linguistic values of each fuzzy grid were obtained easily and the decision maker makes correct business decisions for marketing strategies.	algorithm;association rule learning;data mining	Chien-Hua Wang;Wei-Hsuan Lee;Chia-Hsuan Yeh;Chin-Tzong Pang	2017		10.1145/3162957.3162986	grid;fuzzy logic;data mining;association rule learning;computer science;database transaction	DB	-3.0822380350526672	-29.17815190150315	99549
bddee9df84e9ae6caffb33e65a487a9f78d3a9e2	back analysis based on som-rst system	computational intelligence;rough set theory;soft computing;finite difference method;artificial intelligent;self organized map	This paper describes application of information granulation theory, on the back analysis of Jeffrey mine southeast wall Quebec. In this manner, using a combining of Self Organizing Map (SOM) and rough set theory (RST), crisp and rough granules are obtained. Balancing of crisp granules and sub rough granules is rendered in close-open iteration. Combining of hard and soft computing, namely finite difference method (FDM) and computational intelligence and taking in to account missing information are two main benefits of the proposed method. As a practical example, reverse analysis on the failure of the southeast wall Jeffrey mine is accomplished.	computational intelligence;finite difference method;intel matrix raid;iteration;rough set;self-organizing map;set theory;soft computing	Hamed Owladeghaffari;Hossein Aghababaei	2009	CoRR		rough set;computer science;finite difference method;artificial intelligence;machine learning;computational intelligence;data mining;soft computing;algorithm	NLP	1.852626164805487	-27.93171538489216	99554
02c992eae157f768bf69231aeefb8717f6c52ec2	applications of machine learning	software engineering;real world application;machine learning;message passing;system development	During the last 10 years, machine learning has been successfully applied. Most often, the applications are confidential. Therefore, only few publications about real world applications exist. In this paper, an overview of machine learning applications is given with their scenarios. Some typical applications are described. Then, future directions of machine learning applications are proposed. It is argued that machine learning is now mature enough to be incorporated into standard systems as well as algorithms. The integration of learning modules into database and retrieval systems is one of the trends. Another trend is to automatically select an appropriate learning tool out of a toolbox. The third trend, which is even more challenging, no longer requires a distinguished learning module, but offers methods of machine learning to bc applied by programmers in their regular system development. Software engineers of the future can use inductive techniques as they now use message passing, for instance. Then, any program can be enhanced by some learning ability. 1 E x p e r i e n c e w i t h M a c h i n e L e a r n i n g In the past 10 years, machine learning (ML) had several applications of two types of algorithms, namely top-down induction of decision trees, a family of algorithms from which ID3 [Quinlan, 1983] is the most famous one. conceptual clustering, a family of algorithms from which AQ [Michalski and Stepp, 1983] is the most famous one. The first break-through of applying machine learning was achieved by exploiting conceptual clustering for the building of a rule base on soy bean diseases [Michalski and Chilauski, 1980]. Both algorithms learn from examples which are represented by attribute values. Current research enhances the algorithms to deal with relations [Quinlan, 1990] and restricted first-order predicate logic [Michalski and Stepp, 1983]. Other, logicoriented approaches have been developed, which use background knowledge for learning and even learn the background knowledge itself [Morik, 1987], [Morik, 1990], [Kietz and Wrobel, 1991], [Bisson, 1991], [Muggleton and Feng, 1990]. The new, more powerfull algorithms are not yet products on the market. So, the following description of applications refers go learning from attribute-value representations. A log of knowledge processing can be performed using this representation [Morales, 1990]. There are two scenarios for applying ML:	algorithm;cluster analysis;conceptual clustering;confidentiality;database;decision tree;first-order logic;first-order predicate;machine learning;message passing;oracle advanced queuing;programmer;ross quinlan;rule-based system;software engineer;top-down and bottom-up design	Katharina Morik	1992		10.1007/3-540-55546-3_31	robot learning;message passing;computer science;theoretical computer science;distributed computing;active learning	AI	5.899993013999827	-29.463202308526917	99607
30a19950a5e3a9872c14d82a7d668946dd6bacc0	mining supervised classification performance studies: a meta-analytic investigation	classification automatique statistiques;analyse multivariable;computacion informatica;stochastic process;multivariate analysis;neural networks;analisis datos;05c05;estudio comparativo;supervised classification;62m45;etude methode;meta analysis;estudio metodo;tree classifiers;data mining;logistic regression;nearest neighbor method;discriminant analysis;analyse discriminante;etude comparative;data analysis;large scale;analisis discriminante;regresion logistica;62h30;ciencias basicas y experimentales;classification rules;matematicas;regression logistique;comparative study;processus stochastique;analisis multivariable;analyse donnee;method study;estimation statistique;reseau neuronal;proceso estocastico;58a25;grupo a;estimacion estadistica;statistical estimation;red neuronal;neural network;bradley terry	There have been many comparative studies of classification methods in which real datasets are used as a gauge to assess the relative performance of the methods. Since these comparisons often yield inconclusive or limited results on how methods perform, it is often believed that a broader approach combining these studies would shed some light on this difficult question. This paper describes such an attempt: we have sampled the available literature and created a dataset of 5807 classification results. We show that one of the possible ways to analyze the resulting data is an overall assessment of the classification methods, and we present methods for that particular aim. The merits and demerits of such an approach are discussed, and conclusions are drawn which may assist future research: we argue that the current state of the literature hardly allows large-scale investigations.	machine learning	Adrien Jamain;David J. Hand	2008	J. Classification	10.1007/s00357-008-9003-y	nearest neighbour algorithm;econometrics;meta-analysis;computer science;artificial intelligence;machine learning;comparative research;mathematics;logistic regression;multivariate analysis;data analysis;artificial neural network;statistics	Metrics	9.030096138606714	-34.214404518726695	99694
2a290709155b8f3c47f35af8e319d64eeabddde9	reduced ifam weight matrix representation using sparse matrices		The implicative fuzzy associative memories (IFAM) is a tool used to store patterns in a database and to recall desired pattern upon a presentation. The original IFAM model has been later updated to simplify the weight matrix construction. As a result of this improvement, model internally contains only significant values. This article describes how sparse matrix used to capture model’s weight matrix can be used to reduce memory-space consumption.	matrix representation;sparse matrix	Marek Vajgl	2017		10.1007/978-3-319-66827-7_42	fuzzy logic;matrix representation;sparse matrix;mathematics;matrix (mathematics);artificial intelligence;associative property;pattern recognition	Vision	-3.673366718549373	-25.992631911193925	99844
0bc76907334b54d33db5297cedf6cf3f21420915	prefacespecial section on mathematical programming in data mining and machine learning			data mining;machine learning;mathematical optimization	Katya Scheinberg;Jiming Peng	2008	Optimization Methods and Software	10.1080/10556780802102362	algorithmic learning theory;computer science;data science;machine learning;pattern recognition;data mining;inductive programming;computational learning theory;active learning	ML	9.192697993021538	-36.405265606228774	99923
5f5c8a2764aa9bef0e819aae8820ee75a78154f5	improving support vector solutions by selecting a sequence of training subsets	280213 other artificial intelligence;analisis estadistico;analisis datos;intelligence artificielle;algoritmo genetico;classification;support vector;data analysis;performance improvement;statistical analysis;machine exemple support;analyse statistique;algorithme genetique;artificial intelligence;analyse donnee;genetic algorithm;inteligencia artificial;700101 application packages;support vector machine;maquina ejemplo soporte;vector support machine;clasificacion	In this paper we demonstrate that it is possible to gradually improve the performance of support vector machine (SVM) classifiers by using a genetic algorithm to select a sequence of training subsets from the available data. Performance improvement is possible because the SVM solution generally lies some distance away from the Bayes optimal in the space of learning parameters. We illustrate performance improvements on a number of benchmark data sets.	support vector machine	Tom Downs;Jianxiong Wang	2004		10.1007/978-3-540-28651-6_103	support vector machine;computer science;artificial intelligence;machine learning;data mining	Vision	9.744550344907505	-33.374039364385055	100015
744f14392fe92742766008a3e9991135f07f6a5d	proactive failure management by integrated unsupervised and semi-supervised learning for dependable cloud systems	unsupervised learning;learning algorithms;learning algorithm;decision tree;supervised learning;bayes methods;failure detection;anomaly detection;data collection;bayesian methods;dynamic system;semi supervised learning;bayesian method;data model;health monitoring;cloud systems;large scale;dependable systems;system recovery;monitoring;execution environment;probability distribution;decision trees data models bayesian methods cloud computing mathematical model mutual information monitoring;pattern classification;mathematical model;unsupervised learning bayes methods cloud computing decision trees pattern classification system recovery;bayesian detector;mutual information;decision tree classier proactive failure management unsupervised learning semi supervised learning dependable cloud computing systems failure prediction mechanism unsupervised failure detection method bayesian models;decision trees;decision tree cloud systems dependable systems learning algorithms bayesian detector;cloud computing;data models;bayesian model;failure prediction	Cloud computing systems continue to grow in their scale and complexity. They are changing dynamically as well due to the addition and removal of system components, changing execution environments, frequent updates and upgrades, online repairs and more. In such large-scale complex and dynamic systems, failures are common. In this paper, we present a failure prediction mechanism exploiting both unsupervised and semi-supervised learning techniques for building dependable cloud computing systems. The unsupervised failure detection method uses an ensemble of Bayesian models. It characterizes normal execution states of the system and detects anomalous behaviors. After the anomalies are verified by system administrators, labeled data are available. Then, we apply supervised learning based on decision tree classier to predict future failure occurrences in the cloud. Experimental results in an institute-wide cloud computing system show that our proposed method can forecast failure dynamics with high accuracy.	bayesian network;cloud computing;decision tree;dynamical system;proactive parallel suite;semi-supervised learning;semiconductor industry;supervised learning;system administrator;unsupervised learning	Qiang Guan;Ziming Zhang;Song Fu	2011	2011 Sixth International Conference on Availability, Reliability and Security	10.1109/ARES.2011.20	computer science;machine learning;pattern recognition;data mining	HPC	6.0009821251886555	-35.604304246758026	100026
534ca8ad0dc23d8078da280f39068d789e112b53	a target value control while training the perceptrons in changing environments	computerized systems;perceptrons training;perceptron based classifiers;sample size;multi agent system;neural networks;learning;multilayer perceptrons feedback multi agent systems;multilayer perceptrons;training;dimensionality;feedback chain;data mining;genetics;target value control;pattern recognition data mining algorithm design and analysis frequency control systems mathematics informatics computer security information security multiagent systems;artificial neural networks;feedback chain target value control perceptrons training computerized systems perceptron based classifiers multiagent systems;multi agent systems;feedback;adaptation model;sample size generalization dimensionality evolution learning multi agent systems neural networks;classification algorithms;generalization;algorithm design and analysis;multiagent systems;neural network;evolution	To ensure fast adaptation and security of social and computerized systems to changing environments, targets of perceptron based classifiers ought to vary during training process. To determine optimal differences between target values (stimulation, arousal) we suggest using genetically evolving multi-agent systems aimed to extract necessary information from sequences of the changes. A specially designed additional feedback chain allows updating the target values faster.	algorithm;multi-agent system;perceptron	Sarunas Raudys	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.891	computer science;artificial intelligence;machine learning;data mining	Robotics	9.066556705671323	-26.78829266138587	100321
162b759ba0976e5c094edef5bd9d1b1fe67da064	a new coupled metric learning for real-time anomalies detection with high-frequency field programmable gate arrays	software;measurement field programmable gate arrays vectors neural networks real time systems intrusion detection software;measurement;neural networks;false alarm rate metric learning real time anomalies detection high frequency field programmable gate arrays fpga based intrusion detection system netflow based ids dataset internet end users device to device connections real time online fraud detection system security data analytic techniques bayesian classifier support vector machines classifier;netflow;intrusion detection;support vector machines data analysis field programmable gate arrays internet security of data;metric learning;vectors;intrusion detection systems;field programmable gate arrays;real time systems	Billions of internet end-users and device to device connections contribute to the significant data growth in recent years, large scale, unstructured, heterogeneous data and the corresponding complexity present challenges to the conventional real-time online fraud detection system security. With the advent of big data era, it is expected the data analytic techniques to be much faster and more efficient than ever before. Moreover, one of the challenges with many modern algorithms is that they run too slowly in software to have any practical value. This paper proposes a Field Programmable Gate Array (FPGA) -based intrusion detection system (IDS), driven by a new coupled metric learning to discover the inter- and intra-coupling relationships against the growth of data volumes and item relationship to provide a new approach for efficient anomaly detections. This work is experimented on our previously published NetFlow-based IDS dataset, which is further processed into the categorical data for coupled metric learning purpose. The overall performance of the new hardware system has been further compared with the presence of conventional Bayesian classifier and Support Vector Machines classifier. The experimental results show the very promising performance by considering the coupled metric learning scheme in the FPGA implementation. The false alarm rate is successfully reduced down to 5% while the high detection rate (=99.9%) is maintained.	algorithm;anomaly detection;big data;categorical variable;field-programmable gate array;intrusion detection system;naive bayes classifier;real-time clock;real-time computing;real-time transcription;sensor;statistical classification;support vector machine	Frank Jiang;Dan Luo	2014	2014 IEEE International Conference on Data Mining Workshop	10.1109/ICDMW.2014.203	intrusion detection system;real-time computing;computer science;machine learning;data mining;computer security;artificial neural network	ML	6.001018989406815	-36.32389453607067	100535
5cfcce8d9f5bc248ae3226a11a97eef2750ac868	compressing and improving fuzzy rules using genetic algorithm and its application to fault detection	fuzzy reasoning;gaussian processes;redundancy art neural nets fault diagnosis fuzzy reasoning gaussian processes genetic algorithms pattern classification;redundancy;pattern classification;genetic algorithms;fuzzy rule improvement gaussian art ordering algorithm fuzzy inference systems grnn artificial neural networks classification performance ann models dc ga dont care antecedent time consuming prediction process interpretability redundancy generalized regression neural network adaptive resonance theory o egart pr fis hybrid neural network model fuzzy rule sets fault detection genetic algorithm fuzzy rule compression;art neural nets;fault detection genetic algorithm fuzzy inference system rule extraction;genetic algorithms accuracy neural networks training subspace constraints fuzzy logic quantization signal;fault diagnosis	The fuzzy rule sets, which have been derived from the hybrid neural network model, called the O-EGART-PR-FIS, is an integration of the Adaptive Resonance Theory (ART) into Generalized Regression Neural Network (GRNN), display substantial redundancy and low interpretability that leads to time-consuming prediction process. The O-EGART-PR-FIS approach can achieve the highest accuracy rate among all, however the extracted rules are less compact. Hence, in this paper, we propose a genetic algorithm based method with the inclusion of the “Don't Care” antecedent (hereafter denoted as DC-GA) to the foundation of the O-EGART-PR-FIS, with the aim of further optimizing the existing fuzzy rules. The improved model is applied to two benchmark problems, and the rules extracted are analyzed, discussed and compared with other published methods. From the comparison results, it is observed that the improved model is attested to be statistically superior to other ANN models. Therefore, it reveals the efficacy of DC-GA in eliciting a set of compact and yet easily comprehensible rules while sustaining a high classification performance.	adaptive resonance theory;artificial neural network;benchmark (computing);fault detection and isolation;fuzzy rule;genetic algorithm;hybrid neural network;network model;pr/sm;serial ata;software release life cycle	Keem Siah Yap;Sheng Yuong Wong;Sheih Kiong Tiong	2013	2013 IEEE 18th Conference on Emerging Technologies & Factory Automation (ETFA)	10.1109/ETFA.2013.6648106	genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;gaussian process;fuzzy associative matrix;redundancy;fuzzy set operations	AI	7.91401260923376	-25.75650020197782	100590
626757eaea201fbd08a79c802c9d68359bad7b23	on combining fuzzy c-regression models and fuzzy c-means with automated weighting of the explanatory variables		This paper presents a fuzzy clusterwise regression method aiming to provide linear regression models that are based on homogeneous clusters of observations with respect to the explanatory variables and that are well fitted with respect to the response variable. To achieve this aim, this method combines Fuzzy C-Regression Models and Fuzzy C-Means with automatic computation of relevance weights to the explanatory variables. Because it learns simultaneously a prototype and a linear regression model for each cluster it is able to provide an appropriated regression model for unknown observations based on their description by the explanatory variables. We also discussed both, a heuristic procedure to automatically tune one of the hyperparameters of the proposed method in order to obtain more useful (explanation) models in a prediction task, and a way of making a fuzzy combination of each intra-cluster fitted model as a more natural and appropriate response to the problem of choosing the best regression model for a prediction task. Experiments with synthetic and real datasets corroborate the usefulness of the proposed method.		Ricardo A. M. da Silva;Francisco de A. T. de Carvalho	2018		10.1109/FUZZ-IEEE.2018.8491476	fuzzy logic;statistics;artificial intelligence;machine learning;regression analysis;task analysis;hyperparameter;linear regression;computer science;data modeling;heuristic;weighting	NLP	2.009196436915918	-30.91486468163264	100725
6e023d132b76741ab04b7587cb152147f429c600	coevolutionary multi-task learning for feature-based modular pattern classification		Abstract Due to modular knowledge representation in biological neural systems, the absence of certain sensory inputs does not hinder decision-making processes. For instance, damage to an eye does not result in loss of oneu0027s entire vision. In our earlier work, we presented coevolutionary multi-task learning that featured a synergy between multi-task learning and coevolutionary algorithms. In this paper, we extend this method for robust decision making in pattern classification problems given incomplete information. The method trains a cascaded neural network architecture to autonomously address the absence of certain input features and disruptions to neural connections. The results show that the method is comparable to conventional learning methods whilst having the advantage decision making given incomplete information. Moreover, the method provides a way for developmental learning and simultaneously quantifies feature contribution.	computer multitasking;multi-task learning	Rohitash Chandra;Sally Cripps	2018	Neurocomputing	10.1016/j.neucom.2018.08.011	architecture;machine learning;knowledge representation and reasoning;complete information;artificial neural network;robust decision-making;artificial intelligence;modular design;multi-task learning;mathematics;pattern recognition	ML	8.643496717855047	-27.176575915535313	100806
09d1c5c26ee4f668c17609fa4795fc987397bc4c	improving similarity search in time series using wavelets	time series;data mining;time sequences;similarity algorithms;text and place;similarity search;wavelet transformation;memory	Sequences constitute a large portion of data stored in databases. Data mining applications require the ability to process similarity queries over a large amount of time series data. The query processing performance is an important factor that needs to be taken into consideration. This article proposes a similarity retrieval algorithm for time series. The proposed approach utilizes wavelet transformation in order to reduce the dimensionality of the time series. The transformed series are indexed using X-Trees, which is a spatial indexing technique able to efficiently index high-dimensional data. The article proves that this technique outperforms the usage of the Fourier transformation, since the wavelet transformation provides better approximation of the time series. Through the experiments, it can be concluded that the optimum performance is obtained using 16 to 20 wavelet coefficients. Furthermore, a novel mechanism for reducing the complexity of the calculation for the false alarms removal is proposed. Storing the approximation coefficients of the penultimate level of the decomposition tree, the Euclidean distance between the two sequences is calculated, thus reducing further the number of false alarms before calculating the actual Euclidean distance using the complete time series. The article concludes with a detailed performance evaluation of the proposed similarity retrieval algorithm using data from the Greek stock market and the temperature measurements from Athens. The comparison is done with techniques that use the Haar transform and the R*-Tree, and the proposed algorithm is shown to outperform them.	algorithm;approximation;coefficient;data mining;database;euclidean distance;experiment;haar wavelet;information retrieval;performance evaluation;similarity search;time series;wavelet transform	Ioannis Liabotis;Babis Theodoulidis;Mohammad Saraee	2006	IJDWM	10.4018/jdwm.2006040103	computer science;machine learning;time series;pattern recognition;data mining;database;memory;statistics	DB	-2.8675492113849548	-34.976045433194706	101032
19ad6451a9ab1afafb96f4cf374e19270a4281ee	a rule-extraction framework under multigranulation rough sets		The multigranulation rough set (MGRS) is becoming a rising theory in rough set area, which offers a desirable theoretical method for problem solving under multigranulation environment. However, it is worth noticing that how to effectively extract decision rules in terms of multigranulation rough sets has not been more concerned. In order to address this issue, we firstly give a general ruleextraction framework through including granulation selection and granule selection in the context of MGRS. Then, two methods in the framework (i.e. a granulation selection method that employs a heuristic strategy for searching a minimal set of granular structures and a granule selection method constructed by an optimistic strategy for getting a set of granules with maximal covering property) are both presented. Finally, an experimental analysis shows the validity of the proposed rule-extraction framework in this paper. keywords Multigranulation rough set Rule extraction Granulation selection Granule selection	granule (oracle dbms);heuristic;information system;intension;maximal set;multi-source;problem solving;rough set;rule induction;selection algorithm	Xin Liu;Yuhua Qian;Jiye Liang	2014	Int. J. Machine Learning & Cybernetics	10.1007/s13042-013-0194-0	artificial intelligence;data mining;mathematics;algorithm	AI	-2.64544041921478	-28.07241468399905	101054
54ff39800c1aee3182fe50f1697c87170a92153d	an anomaly detection algorithm based on lossless compression	telecommunication security information theory markov processes security of data;anomaly detection;grammar based compression anomaly detection data mining;data mining;grammar compression algorithms statistical analysis hidden markov models markov processes training entropy;telecommunication security;grammar based compression;markov processes;security of data;information theory;bug detection anomaly detection algorithm lossless compression network security anomaly detection method markovian method markovian model information theory information theoretic measures information theoretic anomaly detection framework information quantity compression algorithm grammar compression data structure intrusion detection	Anomaly detection is essential in network security. It has been researched for decades. Many anomaly detection methods have been proposed. Because of the simplicity of principles, statistical and Markovian methods dominate these approaches. However, their effectiveness is constrained by specific preconditions, which make them work for only appropriate data sets which satisfy their premises. Other than statistical and Markovian model, information theory provides a different perspective about anomaly detection. However, the computation of information theoretic measures is still based on statistics. In this paper, we present a novel, information theoretic anomaly detection framework. Instead of statistics, it employs lossless compression for measuring the information quantity, and detects outliers according to compression result. We also discuss the selection of underlying compression algorithm, and choose a grammar compression for utilizing the structure of data. With grammar compression, our method overcomes the shortcomings of statistical and Markovian methods. In addition, the implementation and operation of our method is even simpler than traditional approaches. We test our method on four data sets about text analyzing, host intrusion detection and bug detection. Experimental results show that, even traditional methods fail in some situations, our simple method works well in all cases.	algorithm;anomaly detection;computation;information theory;intrusion detection system;lossless compression;network security;precondition;software bug	Nan Wang;Jizhong Han;Jinyun Fang	2012	2012 IEEE Seventh International Conference on Networking, Architecture, and Storage	10.1109/NAS.2012.8	anomaly detection;information theory;computer science;theoretical computer science;machine learning;data mining;markov process;grammar-based code;statistics	ML	4.5927674476330465	-36.7984438999819	101121
3d3c92460955f69fbfb015211ada9c34b7db06a2	reasoning under uncertainty with fir methodology	modelizacion;sistema experto;computacion informatica;apprentissage inductif;uncertainty;fuzzy rules;rule based;simulation;logique floue;pattern rule base;fuzzy inference systems;base connaissance;logica difusa;intelligence artificielle;linear system;fuzzy logic;modelisation;fuzzy rule base;aprendizaje por induccion;ciencias basicas y experimentales;robustesse;inferencia;inductive learning;reasoning under uncertainty;fuzzy inference system;artificial intelligence;base conocimiento;fuzzy inductive reasoning;robustness;inteligencia artificial;systeme expert;grupo a;modeling;prediction;non linear system;inference;robustez;knowledge base;expert system	The aim of this research is to develop a new methodology called UNFIR (uncertainty in FIR) as an extension of the fuzzy inductive reasoning (FIR) technique. The main idea behind UNFIR is to expand the modeling capacity of the FIR methodology allowing it to work with classical fuzzy rules. On the one hand, UNFIR is able to automatically construct fuzzy rules starting from a set of pattern rules obtained by FIR. On the other hand, UNFIR affords the prediction of systems behavior by using a mixed pattern/fuzzy inference system that takes advantage of the uncertainty inherent to the data. The pattern rule base that the FIR methodology generates can be very large, obstructing the prediction process and reducing its efficiency. The new methodology preserves as much as possible the knowledge of the pattern rules in a compact fuzzy rule base. In this process some precision is lost but the robustness is considerably increased. The performance of UNFIR methodology as a systems' prediction tool is also studied in this work. Three different applications are used for this purpose, i.e., a linear system, a non-linear system and an industrial process.	finite impulse response	Francisco Mugica;Àngela Nebot	2006	Int. J. Approx. Reasoning	10.1016/j.ijar.2005.07.001	fuzzy logic;knowledge base;systems modeling;uncertainty;prediction;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;linear system;expert system;algorithm;robustness	AI	8.308082510590687	-28.649775472118712	101498
e78aafd11ead9d305e39a4e623780063c1bcdc73	a bayesian spatial autoregressive model with k-nn optimization for modeling the learning outcome of the junior high schools in west java			autoregressive model;java;k-nearest neighbors algorithm;mathematical optimization	I. Gede Nyoman Mindra Jaya;Toni Toharudin;Atje Setiawan Abdullah	2018	MASA	10.3233/MAS-180435	machine learning;autoregressive model;artificial intelligence;bayesian probability;computer science;java	ML	9.623229770679961	-27.149394748505056	101568
b75d3562f1b9fc26cdb5a34e05f358962192d0d8	learning explanatory rules from noisy data (extended abstract)				Richard Evans;Edward Grefenstette	2018		10.24963/ijcai.2018/792	artificial intelligence;noisy data;machine learning;computer science	Theory	9.134816199475596	-36.15273734123949	102044
55440c592aa95f0b257d63f3779378745ee032e5	when neural networks meet decisional dna: a promising new perspective for knowledge representation and sharing	neural networks;decisional dna;deep learning;knowledge representation;set of experience knowledge structure	In this article, we introduce a novel concept combining neural network technology and Decisional DNA for knowledge representation and sharing. Instead of using traditional machine learning and knowledge discovery methods, this approach explores the way of knowledge extraction through deep learning processes based on a domain’s past decisional events captured by Decisional DNA. We compare our approach with kNN k-nearest neighbors, logistic regression, and AdaBoost in classification tasks, and the results show that our approach is very promising with regard to the enhancement of the accuracy of knowledge-based predictions required in complex decision-making problems.	artificial neural network;dna computing;knowledge representation and reasoning	Haoxi Zhang;Cesar Sanín;Edward Szczerbicki	2016	Cybernetics and Systems	10.1080/01969722.2016.1128776	computer science;artificial intelligence;machine learning;data mining;deep learning;artificial neural network	AI	6.010924397014397	-29.494384894565503	102190
3e75c2276e5c1ac1af73b8d8c90317c1e2e9c5c3	synergies of operations research and data mining	modelizacion;preconditionnement;optimisation;optimizacion;analisis datos;optimization technique;preconditioning;operations research;interseccion;data mining;classification;integration;modelisation;data analysis;fouille donnee;mathematical programming;recherche operationnelle;integracion;analyse donnee;precondicionamiento;optimization;process model;intersection;modeling;programmation mathematique;busca dato;programacion matematica;clasificacion;investigacion operacional	In this contribution we identify the synergies of Operations Research and Data Mining. Synergies can be achieved by integration of optimization techniques into Data Mining and vice versa. In particular, we define three classes of synergies and illustrate each of them by examples. The classification is based on a generic description of aims, preconditions as well as process models of Operations Research and Data Mining. It serves as a framework for the assessment of approaches at the intersection of the two procedures. 2009 Elsevier B.V. All rights reserved.	data mining;mathematical optimization;operations research;precondition;synergy	Stephan Meisel;Dirk C. Mattfeld	2010	European Journal of Operational Research	10.1016/j.ejor.2009.10.017	mathematical optimization;systems modeling;biological classification;computer science;artificial intelligence;intersection;process modeling;data mining;mathematics;preconditioner;data analysis;operations research	ML	8.771236227208782	-33.70287132928119	102419
124a9dccadbf2f530ab111533128f0fe43cfcfe4	a neuro-fuzzy approach to data analysis of pairwise comparisons	data analysis;neuro fuzzy	Abstract   Artificial neural networks provide iterative on-line learning schemes for modeling non-linear systems. An iterative learning algorithm in fuzzy models, which is called neuro-fuzzy, has been recently developed within the framework of fuzzy modeling in the sense of M. Sugeno. In this paper, using neuro-fuzzy approach, two quantification methods of pairwise comparisons are presented in order to derive the associated weights of different objects. The proposed methods can be applied even in the case of incomplete pairwise comparisons. A simplified fuzzy reasoning model is obtained in the form of Gaussian radial basis functions.  The psychological sensation responses of human beings to minute vibrations are analyzed by the newly proposed neuro-fuzzy approach. The proposed approach is compared with Guttman's method and Saaty's analytic hierarchy process (AHP). In our two neuro-fuzzy approaches, psychological values are obtained with the interval and the ratio scale properties. They are represented by smooth functions of class C ∞ .	fuzzy logic;neuro-fuzzy	Hidetomo Ichihashi;I. Burhan Türksen	1993	Int. J. Approx. Reasoning	10.1016/0888-613X(93)90011-2	computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;mathematics;data analysis;statistics	AI	2.96139320878936	-26.491324942883715	102558
b978d6e6ad483f2604005a59ceb139273cb83b52	generalisation of rough set for rule induction in incomplete system	incomplete information system;similarity relation;tolerance relation;decision rules;limited tolerance relation;iis;rough set	Rough set models based on the tolerance and similarity relations, have been widely used to deal with incomplete information systems. However, tolerance and similarity relations have their own limitations because the former is too loose while the latter is too strict in classification analysis. To make a reasonable and flexible classification in incomplete information system, a new binary relation is proposed in this paper. Such binary relation is only reflexive and it is a generalisation of tolerance and similarity relations. Furthermore, rough set models based on the above three different binary relations are compared. Finally, the direct approach to rules induction is investigated by using the proposed rough set, some illustrative examples are analysed to substantiate the conceptual arguments.	effective method;information system;intel matrix raid;internet information services;mined;rough set;rule induction	Xibei Yang;Xiaoning Song;Xiaohua Hu	2011	IJGCRSIS	10.1504/IJGCRSIS.2011.041459	rough set;computer science;machine learning;pattern recognition;data mining;decision rule;mathematics;dominance-based rough set approach	AI	-2.3456711534328862	-26.45097291624606	102560
524a3b2af0d5ba305701adbcc84dda627dd2869b	a probabilistic fuzzy logic system for modeling and control	probabilistic fuzzy set;stochastic modeling;fuzzy set;fuzzy control;inference mechanisms;three dimensional;fuzzy logic;robotic control;probabilistic model;control problem;random noise;robot control;function approximation;stochastic modeling probabilistic fuzzy logic system pfls probabilistic fuzzy set robotic control;stochastic processes;robots;membership function;fuzzy logic system;stochastic model;function approximation problem probabilistic fuzzy logic system fuzzification inference engine defuzzification operation stochastic modeling three dimensional membership function;probabilistic fuzzy logic system pfls;robots fuzzy control fuzzy logic inference mechanisms stochastic processes function approximation;fuzzy logic uncertainty stochastic resonance fuzzy systems fuzzy sets robot control stochastic processes engines function approximation power system modeling	In this paper, a probabilistic fuzzy logic system (PFLS) is proposed for the modeling and control problems. Similar to the ordinary fuzzy logic system (FLS), the PFLS consists of the fuzzification, inference engine and defuzzification operation to process the fuzzy information. Different to the FLS, it uses the probabilistic modeling method to improve the stochastic modeling capability. By using a three-dimensional membership function (MF), the PFLS is able to handle the effect of random noise and stochastic uncertainties existing in the process. A unique defuzzification method is proposed to simplify the complex operation. Finally, the proposed PFLS is applied to a function approximation problem and a robotic system. It shows a better performance than an ordinary FLS in stochastic circumstance.	approximation;defuzzification;free library of springfield township;fuzzy logic;fuzzy set;inference engine;membership function (mathematics);noise (electronics);robot;stochastic modelling (insurance)	Zhi Liu;Han-Xiong Li	2005	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2005.859326	fuzzy logic;robot;stochastic process;statistical model;three-dimensional space;mathematical optimization;membership function;defuzzification;function approximation;computer science;artificial intelligence;stochastic modelling;machine learning;mathematics;robot control;fuzzy set;stochastic;fuzzy control system	Robotics	5.164447759577439	-25.49839051595955	102965
c62e92c805da8d8c42d8f948ed1cb81fc7b9a97f	hierarchical system modeling	information fusion hierarchical models granular computing information granules of higher type principle of justifiable granularity fuzzy rule based models;hierarchical models granular computing information granules of higher type principle of justifiable granularity fuzzy rule based models information fusion;computational modeling fuzzy sets numerical models computer architecture optimization computers upper bound	In this study, we present a methodology of building a hierarchical framework of system modeling by engaging concepts and design methodology of granular computing. We demonstrate that it arises as a result of designing and using locally constructed models to develop a model of a global nature. Two main categories of development of hierarchical models are proposed and discussed. In the first one, given a collection of local models, designed is a granular output space and the ensuing hierarchical model produces information granules of the corresponding type depending upon the depth of the hierarchy of the overall hierarchical structure. The crux of the second category of modeling is about selecting one of the original models and elevating its level of information granularity so that it becomes representative of the entire family of local models. The formation of the most “promising” granular model identified in this way involves mechanisms of allocation of information granularity. The focus of the study is on information granules represented as intervals and fuzzy sets (which in case of type-2 information granules lead to so-called granular intervals and interval-valued fuzzy sets) while the detailed models come as rule-based architectures and neural networks. A series of experiments is presented along with a comparative analysis.	artificial neural network;experiment;fuzzy set;granular computing;hierarchical database model;logic programming;qualitative comparative analysis;systems modeling	Rami Al-Hmouz;Witold Pedrycz;Abdullah Saeed Balamash;Ali Morfeq	2018	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2017.2649581	artificial intelligence;theoretical computer science;machine learning;mathematics	ML	1.3869099063387316	-26.786852231821634	103270
e741ea738f22e5921cdc43acd1032f1b8b70f028	a hybrid fuzzy-neural expert system for diagnosis	fuzzy neural network;fuzzy rules;fuzzy logic;electroencephalogram;neural network;knowledge base;expert system	Fuzzy Logic, a neural network and an expert system are combined to build a hybrid diagnosis system. With this system we introduce a new approach to the acquisition of knowledge bases. Our system consists of a fuzzy expert system with a dual source knowledge base. Two sets of rules are acquired, inductively from given examples and deductively formulated by a physician. A fuzzy neural network serves to learn from sample data and allows to extract fuzzy rules for the knowledge base. The diagnosis of electroencephalograms by interpretation of graphoelements serves as visualization for our approach. Preliminary results demonstrate the promising possibilities offered by our method. 1 Introduction Repetitively applied cognitive tasks of recognizing and evaluating certain phenomena, called diagnostic tasks, are among the main applications for Artificial Intelligence (AI). As there exists a vast variety of such diagnostic tasks in medicine, it has always belonged to the spectrum of potential users of Artificial Intelligence. Most popular among AI methods in medicine are knowledge based systems [Buchanan and Shortliffe, 1985], modeling the diagnostic behaviour of experts. A variety of such expert systems is being used in everyday p ractice of physicians since Shortliffe introduced MYCIN Shortliffe, 1976], an expert system designed to diagnose infections of the human blood. One of the greatest difficulties in designing a convenient expert system is acquiring the knowledge base. We introduce a new approach where a dual source knowledge base is generated by de-ductive and inductive learning. Neural networks have also made their way into diagnosis. They are able to learn relationships between data sets by simply having sample data represented to their input and output layers. In the field of pattern recognition in medical data, neural network based approaches have led to quite remarkable results, for example in processing MRI pictures [Hall et a/., 1992] or EEG traces [Mamelak et a/., 1991; Jando et a/., 1993]. For the task of acquiring knowledge bases, which is a part of our hybrid approach, neural networks have been proposed recently [Thrun and Mitchell, 1993]. Fuzzy logic [Zadeh, 1965] also makes its appearance in medicine, dealing with the uncertainty of verbal expressions [Kuncheva, 1991; Nishimura et a/., 199l]. Terms like many, few or probably are hard to model with conventional logic. The linguistic variables offered by fuzzy representations allow pseudo-verbal descriptions close to natural human expressions. All of the above methods bear advantages as well as disadvantages as will …	artificial intelligence;artificial neural network;electroencephalography;expert system;fuzzy logic;inductive reasoning;input/output;knowledge base;mitchell corporation;mycin;neural network software;neuro-fuzzy;pattern recognition;tracing (software)	Christoph Siegfried Herrmann	1995			fuzzy logic;legal expert system;knowledge base;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system	AI	2.671455282319639	-27.2626687592613	103384
06abbf16a73f4a0fced7220e4ef8921f0ee90129	using neural nets to learn weights of rules for compositional expert systems	representacion conocimientos;sistema experto;learning algorithm;rule extraction;algorithme apprentissage;systeme base connaissances;neural net;systeme expert;reseau neuronal;knowledge representation;representation connaissances;algoritmo aprendizaje;red neuronal;knowledge based systems;credit risk;neural network;knowledge base;expert system	"""Knowledge base for a compositional expert system consists of a set of IF THEN rules with uncertainties expressed as weights. During consultation for a particular case, all aplicable rules are combined and weigths of goals ((nal diagnoses or recommendations) are computed. The main problem when eliciting such knowledge base from an expert is the question of \correct"""" weights of rules. Our idea is, to combine the structure of knowledge obtained from expert with weights learned from data. We choose the topology and initial settings of the neural net (number of neurons, prohibited links) according to the rules obtained from expert. Then, after learning such network, we try to interpret the weights of connections as uncertainty of the original rules. The paper shows some experimental results of this approach on a knowledge base for credit risk assessment."""	artificial neural network;expert system;knowledge base;risk assessment	Petr Berka;Marek Sláma	1998		10.1007/3-540-64582-9_782	legal expert system;credit risk;computer science;artificial intelligence;machine learning;data mining;artificial neural network;algorithm	AI	8.405562099922378	-30.15949244060665	103421
5d0adc5112e82ab9d964c84070e78dff07c7db2e	exploring logical rules based on causal semantics analysis of relational data	data analysis decision trees information theory classification tree analysis data mining information analysis entropy machine learning testing artificial intelligence;stopping criteria;relational data;empirical study;uncertain knowledge causal semantics analysis;general information theory;multi level semantic knowledge;rule based;training;decision forest;tree data structures;data mining;generalization ability;tree structures;generalization ability logical rules relational data uncertain knowledge causal semantics analysis multi level semantic knowledge tree structures decision forest stopping criteria;general information theory semantic knowledge logical rules decision forest;semantic knowledge;tree structure;classification algorithms;logical rules;entropy;classification tree analysis;information theory;semantic analysis;tree data structures knowledge engineering;knowledge engineering	For reasoning with uncertain knowledge causal semantics analysis is investigated to propose logical rules, which can represent multi-level semantic knowledge of the relationship between the data and information implicated.These rules constitutes several tree structures named decision forest, the number of trees and stopping criteria can be set automatically. Empirical studies on a set of natural domains show that decision forest has clear advantages with respect to the generalization ability.	causal filter;causality	YaoFu Cao;LiMin Wang;Xin Zuo	2009	2009 International Joint Conference on Artificial Intelligence	10.1109/JCAI.2009.95	information theory;computer science;artificial intelligence;machine learning;knowledge engineering;pattern recognition;data mining;tree structure;well-founded semantics	AI	-2.296794985574793	-27.24750405345916	103592
197d1de7abb03e0a65d459b32908cdb2557f29ab	distributed pasting of small votes	bootstrap;ensemble method;analisis datos;intelligence artificielle;aprendizaje probabilidades;classification;scaling up;data analysis;distributed environment;voting;apprentissage probabilites;artificial intelligence;analyse donnee;voto;massive datasets;inteligencia artificial;vote;clasificacion;probability learning	Bagging and boosting are two popular ensemble methods that achieve better accuracy than a single classifier. These techniques have limitations on massive datasets, as the size of the dataset can be a bottleneck. Voting many classifiers built on small subsets of data (“pasting small votes”) is a promising approach for learning from massive datasets. Pasting small votes can utilize the power of boosting and bagging, and potentially scale up to massive datasets. We propose a framework for building hundreds or thousands of such classifiers on small subsets of data in a distributed environment. Experiments show this approach is fast, accurate, and scalable to massive datasets.	bootstrap aggregating;bottleneck (engineering);ensemble learning;experiment;scalability;statistical classification	Nitesh V. Chawla;Lawrence O. Hall;Kevin W. Bowyer;Thomas E. Moore;W. Philip Kegelmeyer	2002		10.1007/3-540-45428-4_5	voting;biological classification;computer science;artificial intelligence;data science;machine learning;data mining;data analysis;electoral-vote.com;distributed computing environment	ML	9.116372208358543	-33.310499188424274	103670
a59a6bbe8856068989b736d0719a1ecc755492ce	comparison between svm and back propagation neural network in building ids		Recently, applying the novel data mining techniques for anomaly detection-an element in Intrusion Detection System has received much research alternation. Support Vector Machine (SVM) and Back Propagation Neural (BPN) network has been applied successfully in many areas with excellent generalization results, such as rule extraction, classification and evaluation. In this paper, we use an approach that is entropy based analysis method to characterize some common types of attack like scanning attack. A model based on SVM with Gaussian RBF kernel is also proposed here for building anomaly detection system. BPN network is considered one of the simplest and most general methods used for supervised training of multilayered neural network. The comparative results show that with attack scenarios that we create and through the differences between the performance measures, we found that SVM gives higher precision and lower error rate than BPN method.	backpropagation;software propagation	Nguyen Dai Hai;Nguyen Linh Giang	2013		10.1007/978-94-007-6738-6_138	word error rate;computer vision;artificial intelligence;support vector machine;computer science;anomaly detection;artificial neural network;backpropagation;intrusion detection system;gaussian;pattern recognition;radial basis function kernel	ML	7.733941493282053	-37.37765892628508	103862
80b4f4479ebacf09586cba523ebd5ec6a7e60d0a	neural networks for data mining: constrains and open problems	neural model;data mining;neural network	When we talk about using neural networks for data mining we have in mind the original data mining scope and challenge. How did neural networks meet this challenge? Can we run neural networks on a dataset with gigabytes of data and millions of records? Can we provide explanations of discovered patterns? How useful that patterns are? How to distinguish useful, interesting patterns automatically? We aim to summarize here the state-of-the-art of the principles beyond using neural models in data mining. 1 What is special in data mining applications? Data mining (DM) is the nontrivial extraction of implicit, previously unknown, interesting, and potentially useful information (usually in the form of knowledge patterns or models) from data. Historically data mining has grown from large business database applications, such as finding patterns in customer purchasing activities from transactions databases. Original DM problems were to adjust known methods such as decision trees and neural networks (NN) to large datasets (100,000 and more records) and relational database structures. Later methods such as association rules were developed specifically motivated by DM challenge. The most vehiculated DM problems are reduced to traditional statistical and machine leaning methods: classification, prediction, association rule extraction, and sequence detection. The techniques used in DM are very heterogeneous: statistical methods, case-based reasoning, NN, decision trees, rule induction, Bayesian networks, fuzzy sets, rough sets, genetic algorithms/evolutionary programming. The following are the major stages in solving a DM problem [7]: 1. Define the problem. 2. Collect and select data, such as deciding which data to collect and how to collect them. ESANN'2004 proceedings European Symposium on Artificial Neural Networks Bruges (Belgium), 28-30 April 2004, d-side publi., ISBN 2-930307-04-8, pp. 449-458	approximation algorithm;association rule learning;bayesian network;case-based reasoning;computer architecture;data mining;decision tree;distributed computing;evolutionary programming;fuzzy set;genetic algorithm;gigabyte;international standard book number;network architecture;neural networks;nonlinear system;purchasing;relational database;rough set;rule induction	Razvan Andonie;Boris Kovalerchuk	2004			computer science;data science;machine learning;data mining;artificial neural network	ML	-0.013484689927508814	-32.9003154169766	103865
851ca22d766e30acdd34be1c9d679f60c1764514	new fuzzy singleton distance measurement by convolution			convolution	Rodrigo Naranjo;Matilde Santos Peñas	2018		10.1007/978-3-030-03493-1_84		Metrics	2.2846854209864254	-24.520959523999522	104021
668eafb108ee9467da06fb89537b00f27afd809c	fault diagnosis of marine 4-stroke diesel engines using a one-vs-one extreme learning ensemble	extreme learning machines;ensemble learning;diesel engine;machine learning;fault detection;marine engine;multi class decomposition;fault diagnosis	This paper proposes a novel approach for intelligent fault diagnosis for stroke Diesel marine engines, which are commonly used in on-road and marine transportation. The safety and reliability of a ship's work rely strongly on the performance of such an engine; therefore, early detection of any type of failure that affects the engine is of crucial importance. Automatic diagnostic systems are of special importance because they can operate continuously in real time, thereby providing efficient monitoring of the engine's performance. We introduce a fully automatic machine learning-based system for engine fault detection. For this purpose, we monitor various signals that are emitted by the engine, and we use them as an input for a pattern classification algorithm. This action is realized by an ensemble of Extreme Learning Machines that work in a decomposition mode. Because we address 14 different faults and a correct operation mode, we must handle a 15-class problem. We tackle this task by binarization in one-vs-one mode, where each Extreme Learning Machine is trained on a pair of classes. Next, Error-Correcting Output Codes are used to reconstruct the original multi-class task. The results from experiments that were conducted on a real-life dataset demonstrate that the proposed approach delivers superior classification accuracy and a low response time in comparison with a number of state-of-the-art methods and thus is a suitable choice for a real-life implementation on board a ship.	diesel	Jerzy Kowalski;Bartosz Krawczyk;Michal Wozniak	2017	Eng. Appl. of AI	10.1016/j.engappai.2016.10.015	real-time computing;simulation;computer science;machine learning;ensemble learning;fault detection and isolation	AI	8.623098420571424	-23.987754262681968	104344
043307131a0d4d46ee30235d07f49241f49ebdd7	application of reduction of the set of conditional attributes in the process of global decision-making	global decision;reduction of set of conditional attributes;decision making system;rough set	The paper includes a discussion of issues related to the process of global decision-making on the basis of information stored in several local knowledge bases. The local knowledge bases contain information on the same subject, but are defined on different sets of conditional attributes that are not necessarily disjoint. A decision-making system, which uses a number of knowledge bases, makes global decisions on the basis of a set of conditional attributes specified for all of the local knowledge bases used. The paper contains a description of a multi-agent decision-making system with a hierarchical structure. Additionally, it briefly overviews methods of inference that enable global decision-making in this system and that were proposed in our earlier works. The paper also describes the application of the conditional attributes reduction technique to local knowledge bases. Our main aim was to investigate the effect of attribute reduction on the efficiency of inference in such a system. For a measure of the efficiency of inference, we mean mainly an error rate of classification, for which a definition is given later in this paper. Therefore, our goal was to reduce the error rate of classification.		Malgorzata Przybyla-Kasperek;Alicja Wakulicz-Deja	2013	Fundam. Inform.	10.3233/FI-2013-793	rough set;computer science;machine learning;pattern recognition;data mining;mathematics	Vision	-2.504864314463987	-26.763066880000363	104409
561756448402e41294f582b7e8d2c101d7dbfb7b	generating fuzzy rules from training instances for fuzzy classification systems	fuzzy classification;fuzzy set;fuzzy rules;fuzzy sets;iris data;membership functions;membership function;fuzzy classification systems;classification accuracy;data classification	In recent years, many methods have been proposed to generate fuzzy rules from training instances for handling the Iris data classification problem. In this paper, we present a new method to generate fuzzy rules from training instances for dealing with the Iris data classification problem based on the attribute threshold value @a, the classification threshold value @b and the level threshold value @c, where @a@?[0,1], @b@?[0,1] and @c@?[0,1]. The proposed method gets a higher average classification accuracy rate than the existing methods.	fuzzy classification	Shyi-Ming Chen;Fu-Ming Tsai	2008	Expert Syst. Appl.	10.1016/j.eswa.2007.07.013	membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	ML	3.623678478180749	-29.276480185127483	104582
cfeae7fee271381f52e7cb1706e1f3d4cc99dcff	a computational knowledge representation model for cognitive computers	cognitive computing;cognitive computers;real time process algebra;knowledge processing;denotational mathematics;knowledge representation	The accumulating data are easy to store but the ability of understanding and using it does not keep track with its growth. So researches focus on the nature of knowledge processing in the mind. This paper proposes a semantic model (CKRMCC) based on cognitive aspects that enables cognitive computer to process the knowledge as the human mind and find a suitable representation of that knowledge. In cognitive computer, knowledge processing passes through three major stages: knowledge acquisition and encoding, knowledge representation, and knowledge inference and validation. The core of CKRMCC is knowledge representation, which in turn proceeds through four phases: prototype formation phase, discrimination phase, generalization phase, and algorithm development phase. Each of those phases is mathematically formulated using the notions of real-time process algebra. The performance efficiency of CKRMCC is evaluated using some datasets from the well-known UCI repository of machine learning datasets. The acquired datasets are divided into training and testing data that are encoded using concept matrix. Consequently, in the knowledge representation stage, a set of symbolic rule is derived to establish a suitable representation for the training datasets. This representation will be available in a usable form when it is needed in the future. The inference stage uses the rule set to obtain the classes of the encoded testing datasets. Finally, knowledge validation phase is validating and verifying the results of applying the rule set on testing datasets. The performances are compared with classification and regression tree and support vector machine and prove that CKRMCC has an efficient performance in representing the knowledge using symbolic rules.	algorithm;binary classification;categorization;cognitive computer;data validation;decision tree learning;knowledge acquisition;knowledge representation and reasoning;machine learning;mind;missing data;performance;process calculus;prototype;real-time clock;simulation;support vector machine;test case;test set;verification and validation;whole earth 'lectronic link;zero-knowledge proof	Mona Nagy Elbedwehy;Mohamed Elsayed Ghoneim;Aboul Ella Hassanien;Ahmad Taher Azar	2014	Neural Computing and Applications	10.1007/s00521-014-1614-0	computer science;artificial intelligence;cognitive computing;knowledge-based systems;machine learning;open knowledge base connectivity;data mining;mathematics;procedural knowledge;knowledge extraction;statistics	AI	2.9016242972049477	-30.761738386867357	104793
79dfa54176b63c988c1ede0e1ec8cb4c9dbdc8ec	a hybrid ga-based fuzzy classifying approach to urinary analysis modeling	fuzzy classification;fuzzy rules;fuzzy rule learning;genetics;learning system;urinalysis;urinary tract;genetic algorithm;genetic algorithms;feature selection;classification accuracy;fuzzy classifier	Automatically analyzing urine samples is a very important issue in laboratory practice. In this paper, a hybrid GA-based fuzzy classification technique is proposed to create fuzzy rules for further identifying and monitoring diseases of the kidney and urinary tract. Fuzzy genetic learning has proven to be a promising approach and widely used to carry out medical diagnoses today. We have evaluated the classification performance of the different genetic fuzzy rule learning approaches. Results show that our proposed hybrid GA-based fuzzy learning system provides better classification accuracy and generates symbolic rules which outperform the previous GA-based fuzzy approaches.	fuzzy classification;fuzzy rule;software release life cycle;tract (literature)	Ping Wu;Erik D. Goodman;Tang Jiang;Min Pei	2009		10.1145/1570256.1570381	genetic algorithm;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;pattern recognition;feature selection;fuzzy set operations	AI	3.973430983148277	-29.269943426120708	104897
d03cee23c8936399b0d4a931316bd5f0608ff70d	an effective clustering approach to stock market prediction	financial report;document clustering;stock market;hierarchical agglomerative clustering;feature vector;stock price;clustering method;profitability;financial reporting;stock price prediction;k means clustering	In this paper, we propose an effective clustering method, HRK (Hierarchical agglomerative and Recursive K-means clustering), to predict the short-term stock price movements after the release of financial reports. The proposed method consists of three phases. First, we convert each financial report into a feature vector and use the hierarchical agglomerative clustering method to divide the converted feature vectors into clusters. Second, for each cluster, we recursively apply the K-means clustering method to partition each cluster into sub-clusters so that most feature vectors in each subcluster belong to the same class. Then, for each sub-cluster, we choose its centroid as the representative feature vector. Finally, we employ the representative feature vectors to predict the stock price movements. The experimental results show the proposed method outperforms SVM in terms of accuracy and average profits.	cluster analysis;feature vector;hierarchical clustering;k-means clustering;recursion (computer science);support vector machine	Anthony J. T. Lee;Ming-Chih Lin;Rung-Tai Kao;Kuo-Tay Chen	2010			financial economics;feature vector;document clustering;computer science;machine learning;financial system;profitability index;k-means clustering	ML	-4.510438619521172	-34.96402660910493	104944
d6e47773c0fe6d136b5be1d70717280328d48307	typicality based on soft aggregations in fuzzy object oriented databases	topological space	"""In this paper a method is proposed to compute typical objects for the classes in the scheme of a fuzzy object oriented database. Typical objects are considered as """"representatives"""" of a fuzzy majority of the class instances. The instances of a class are represented in a topological space and the typical object is derived as """"closest"""" to the fuzzy majority of the class instances."""	database;instance (computer science)	Gloria Bordogna;Gabriella Pasi	1999			dielectric;fuzzy logic;electromagnetic coil;manifold;coolant;coating;database;foil method;object-oriented programming;mathematics	DB	-1.4657082818695977	-25.308752212731044	105427
e3276e6e9a4055b3428e491b07155512002c1653	computational concepts in classification: neural networks, statistical pattern recognition, and model-based vision	mathematical analysis;statistical pattern recognition;neural network model;neural network	A large number of algorithms have been developed for classification and recognition. These algorithms can be divided into three major paradigms: statistical pattern recognition, neural networks, and model-based vision. Neural networks embody an especially rich field of approaches based on a variety of architectures, learning mechanisms, biological and algorithmic motivations, and application areas. Mathematical analysis of these approaches and paradigms reveals that there are only a few computational concepts permeating all the diverse approaches and serving as a basis for all paradigms and algorithms for classification and recognition. These basic computational concepts are reviewed in this paper with the purposes of (i) providing a mathematical continuity to seemingly disparate techniques, (ii) establishing basic mathematical limitations on applicability of existing techniques, (iii) discerning fundamental questions facing the classification field, and (iv) searching for directions in which answers to these questions may be found.	algorithm;artificial neural network;computation;neural network software;pattern recognition;scott continuity;statistical classification	Leonid I. Perlovsky	1994	Journal of Mathematical Imaging and Vision	10.1007/BF01250006	cellular neural network;feature;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network;artificial neural network	Vision	6.790927055633688	-24.717111945205282	105632
13481f20b6029e69b46de4bb4f1ede2462cb9916	forecasting stock market performance using hybrid intelligent system	forecasting;evaluation performance;prevision;systeme intelligent;control difusa;stock market;performance evaluation;sistema hibrido;bolsa valores;transparence;systeme discret;algoritmo borroso;evaluacion prestacion;sistema inteligente;fuzzy control;datos financieros;logique floue;swinburne;logica difusa;continuous system;donnee financiere;circuito logico;algoritmo genetico;transparencia;hybrid intelligent system;financial data;bourse valeurs;systeme continu;stock exchange;artificial intelligent;fuzzy logic;circuit logique;sistema continuo;fuzzy algorithm;intelligent system;prediction accuracy;hybrid system;algorithme genetique;fuzzy logic system;algorithme flou;genetic algorithm;transparency;sistema difuso;systeme flou;sistema discreto;reseau neuronal;computational efficiency;logic circuit;red neuronal;fuzzy system;discrete system;commande floue;neural network;systeme hybride	"""Predicting the future has always been one of mankind’s desires. In recent years, artificial intelligent techniques such as Neural Networks, Fuzzy Logic, and Genetic Algorithms have gained popularity for this kind of applications. Much research effort has been made to improve the prediction accuracy and computational efficiency. In this paper, a hybridized neural networks and fuzzy logic system, namely the FeedForward NeuroFuzzy (FFNF) model, is proposed to tackle a financial forecasting problem. It is found that, by breaking down a large problem into manageable """"chunks"""", the proposed FFNF model yields better performance in terms of computational efficiency, prediction accuracy and generalization ability. It also overcomes the black art approach in conventional NNs by incorporating """"transparency"""" into the system."""	computation;feed forward (control);fuzzy logic;genetic algorithm;hybrid intelligent system;neural networks	Xiaodan Wu;Ming Fung;Andrew Flitman	2001		10.1007/3-540-45718-6_49	fuzzy logic;stock exchange;genetic algorithm;logic gate;forecasting;computer science;artificial intelligence;neuro-fuzzy;hybrid intelligent system;discrete system;transparency;operations research;algorithm;fuzzy control system;hybrid system	AI	8.833100125557111	-28.697243955407956	105693
6543244c2df12945986b41f003bf8d48722b2d77	an embedded simplified fuzzy artmap implemented on a microcontroller for food classification	fuzzy artmap;neural networks;computer graphics;articulo;equipment failure analysis;fuzzy logic;equipment design;honey classification;algorithms;conductometry;pattern recognition automated;miniaturization;food analysis;user computer interface;honey;microcomputers;microcontroller	In the present study, a portable system based on a microcontroller has been developed to classify different kinds of honeys. In order to do this classification, a Simplified Fuzzy ARTMAP network (SFA) implemented in a microcontroller has been used. Due to memory limits when working with microcontrollers, it is necessary to optimize the use of both program and data memory. Thus, a Graphical User Interface (GUI) for MATLAB® has been developed in order to optimize the necessary parameters to programme the SFA in a microcontroller. The measures have been carried out by potentiometric techniques using a multielectrode made of seven different metals. Next, the neural network has been trained on a PC by means of the GUI in Matlab using the data obtained in the experimental phase. The microcontroller has been programmed with the obtained parameters and then, new samples have been analysed using the portable system in order to test the model. Results are very promising, as an 87.5% recognition rate has been achieved in the training phase, which suggests that this kind of procedures can be successfully used not only for honey classification, but also for many other kinds of food.	artificial neural network;biological neural networks;embedding;graphical user interface;honey;matlab;metals;microcontroller;neural network simulation;potentiometer	Eduardo García-Breijo;José Garrigues;Luis Gil-Sánchez;Nicolás Laguarda-Miró	2013		10.3390/s130810418	fuzzy logic;microcontroller;embedded system;computer hardware;computer science;artificial intelligence;microcomputer;miniaturization;computer graphics;artificial neural network;conductometry	AI	6.240347265974688	-29.594186454660978	105694
6705c27d9400c5e75a986284f0ce20b38a8c1559	the influence of arima-garch parameters in feed forward neural networks prediction	prediccion;topology;levenberg marquardt;calcul neuronal;feed forward;neural computation;garch model;linearity;metodo monte carlo;feed forward neural network;65c05;capacite;modelo autorregresivo;transfer functions;62m10;62m20;linearite;levenberg marquardt algorithm;62e17;topologie;methode monte carlo;processus autoregressif;loi conditionnelle;62m45;ley condicional;time series;capacidad;autoregressive model;topologia;linearidad;algorithme;modelo arima;algorithm;modele arima;artificial neural networks;modele garch;autoregressive integrated moving average model;prediction theory;heteroscedasticidad;autoregressive processes;monte carlo method;heteroscedasticite;analyse performance;serie temporelle;performance analysis;serie temporal;heteroscedasticity;reseau neuronal;theorie prediction;capacity;monte carlo simulation;modele autoregressif;prediction;red neuronal;computacion neuronal;conditional heteroscedasticity;conditional distribution;reseau neuronal artificiel;artificial neural network;neural network;algoritmo;analisis eficacia	The objective of this article is to find out the influence of the parameters of the ARIMA-GARCH models in the prediction of artificial neural networks (ANN) of the feed forward type, trained with the Levenberg–Marquardt algorithm, through Monte Carlo simulations. The paper presents a study of the relationship between ANN performance and ARIMA-GARCH model parameters, i.e. the fact that depending on the stationarity and other parameters of the time series, the ANN structure should be selected differently. Neural networks have been widely used to predict time series and their capacity for dealing with non-linearities is a normally outstanding advantage. However, the values of the parameters of the models of generalized autoregressive conditional heteroscedasticity have an influence on ANN prediction performance. The combination of the values of the GARCH parameters with the ARIMA autoregressive terms also implies in ANN performance variation. Combining the parameters of the ARIMA-GARCH models and changing the ANN’s topologies, we used the Theil inequality coefficient to measure the prediction of the feed forward ANN.	activation function;autoregressive integrated moving average;autoregressive model;conditional entropy;contour line;feed forward (control);levenberg–marquardt algorithm;matthews correlation coefficient;monte carlo method;neural networks;series and parallel circuits;sigmoid function;simulation;social inequality;stationary process;theil index;time series;volatility	Mauri Aparecido de Oliveira	2010	Neural Computing and Applications	10.1007/s00521-010-0410-8	econometrics;levenberg–marquardt algorithm;artificial intelligence;artificial neural network;statistics;monte carlo method	ML	9.738792722678642	-24.692121047563486	105770
fd81e32385382e5056073ebe66e45bb69710d5a0	mining fuzzy association rules from sequence databases with quantitative data and inter-transaction intervals			database	Hsin-Tai Yang;Shie-Jue Lee	2002			fuzzy logic;pattern recognition;computer science;artificial intelligence;data mining;association rule learning;fuzzy classification;database transaction	DB	-0.7863470361871409	-32.43281953263926	105783
d387fca4e030591489d5d26ca108025e289315f0	a comparative study on the performance of fuzzy logic, bayesian logic and neural network towards decision-making	bayesian logic;soft computing;fuzzy logic;perceptron neural network;membership functions;membership function;bell shaped function;perceptron neural networks	Soft computing models play an important role in the field of recognition, classification, data prediction, etc., and also in various application fields towards decision-making. Soft computing models include fuzzy logic, neural, network, genetic algorithm, particle swarm optimisation, tabu search, harmonie search, clustering, etc. The performance of a particular soft computing model can be ascertained using a particular dataset for the purpose of decision-making. Here, an effort has been made to make a comparison on the performance of fuzzy logic, Bayesian logic and neural network. The model with minimum error has been given preference for selection towards decision-making of information. The same method has been cross-checked based on the residual analysis to verify the earlier proposed observation. The said models have also been cross-checked based on other dataset. Under neural network, perceptron neural network model has been used.	artificial neural network;fuzzy logic	Dharmpal Singh;Jagannibas Paul Choudhury;Mallika De	2012	IJDATS	10.1504/IJDATS.2012.046792	fuzzy logic;probabilistic neural network;membership function;bayesian probability;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;neural modeling fields;data mining;time delay neural network;soft computing;intelligent control	AI	4.2388665838622135	-26.243632319584712	105851
747d0f21792b25cf0c6b4c64017d14e399b10791	self-organizing map artificial neural network application in multidimensional soil data analysis	kohonen self organizing feature maps;pattern analysis;soil properties;k means clustering	Because of the complex nonlinear relationships between soil variables and their multivariable aspects, classical analytic, deterministic, or linear statistical methods are unreliable and cause difficulty to present or visualize the results. Using intelligent techniques, which have ability to analyze the multidimensional soil data with an intricate visualization technique, is crucial for nutrient and water management in soil, consequently, for sustainable agriculture and groundwater management. In this study, first, the Kohonen self-organizing feature maps (KSOFM) neural network was applied to analyze the effects of soil physical properties on soil chemical/hydraulic processes, and to diagnose the inter-relationships of the multivariable soil data in vadose zone. The inter-relationships among the soil variables were extracted and interpreted using the pattern analysis visualized in component planes. Then K-means clustering algorithm was used to determine the optimal number of clusters by using the Silhouette clustering validity index, resulting in six clusters or groups for soil variables. In conclusion, the KSOFM technique is an effective tool for analyzing and diagnosing the dynamics in soil and extracting information from the multidimensional soil data. These results suggest that this technique has a potential to monitor and diagnose not only soil physical/chemical/hydraulic processes, but also soil morphological and microbiological processes.	algorithm;artificial neural network;cluster analysis;k-means clustering;nonlinear system;organizing (structure);pattern recognition;self-organization;self-organizing map;silhouette (clustering);teuvo kohonen	Hasan Merdun	2010	Neural Computing and Applications	10.1007/s00521-010-0425-1	computer science;machine learning;k-means clustering	ML	3.105401843990261	-35.26212981891633	106031
ebd1179a8e88284f3dbb04629e21eb2a49d4295f	mining and prioritization of association rules for big data: multi-criteria decision analysis approach	data mining;association rules;pfp-growth;big data;apache spark;road accident;multi-criteria decision analysis	Data mining techniques and extracting patterns from large datasets play a vital role in knowledge discovery. Most of the decision makers encounter a large number of decision rules resulted from association rules mining. Moreover, the volume of datasets brings a new challenge to extract patterns such as the cost of computing and inefficiency to achieve the relevant rules. To overcome these challenges, this paper aims to build a learning model based on FP-growth and Apache Spark framework to process and to extract relevant association rules. We also integrate the multi-criteria decision analysis to prioritize the extracted rules by taking into account the decision makers subjective judgment. We believe that this approach would be a useful model to follow, particularly for decision makers who are suffering from conflicts between extracted rules, and difficulties of building only the most interesting rules. Experimental results on road accidents analysis show that the proposed approach can be efficiently achieved more association rules with a higher accuracy rate and improve the response time of the proposed algorithm. The results make clear that the proposed approach performs well and can provide useful information that could help the decision makers to improve road safety.	algorithm;apache spark;association rule learning;big data;data mining;decision analysis;response time (technology)	Addi Ait-Mlouk;Tarik Agouti;Fatima Gharnati	2017	Journal of Big Data	10.1186/s40537-017-0105-4	multiple-criteria decision analysis;data mining;data science;decision rule;computer science;decision analysis;knowledge extraction;association rule learning;inefficiency;big data;prioritization	ML	-3.7769116020115483	-30.499712744182418	106288
e876557c82e3b5babf6654966ba293bfdce373db	rule extraction and validity domain on a multilayer neural network	constraint propagation;validity domain;neural nets;rule extraction;validity domain constraints expert system expertise domain logical constraints logical rules mathematical constraints multilayer neural network rule extraction;neural nets formal logic;logical rules;formal logic;expertise domain;mathematical constraints;logical constraints;multilayer neural network;constraints;neural network;expert system	The authors discuss the extraction of logical rules from a multilayer neural network when building a validity domain for the network is possible. This approach appears to be efficient when the expertise domain is restricted and when explicit rules are not easy to formulate. The validity domain is described as a set of mathematical and logical constraints. The constraints specifying the validity domain are then included in the constraint-propagation methods that can be used for extracting equivalent logical rules from the neural network. A corpus of jurisprudence illustrates these conditions very effectively. The logical rules that are extracted can be used either to explain the functioning of the neural network or as a first step for designing an expert system	artificial neural network;rule induction	Laurent Bochereau;Paul Bourgine	1990		10.1109/IJCNN.1990.137552	computer science;artificial intelligence;machine learning;mathematics;logic;artificial neural network;algorithm;local consistency	NLP	2.4549219282852275	-28.97378416952757	106315
d3cd4d7e5117d0cee140d4eb3d9fb29179529b96	learning association rules from data through domain knowledge and automation		An approach to automated data mining with association rules based on domain knowledge is introduced. Association rules are understood as interesting pairs of general Boolean attributes. Items of domain knowledge corresponding to various relations of non-Boolean attributes are used to formulate reasonable analytical questions. Particular items of knowledge are mapped to sets of association rules which can be considered their consequences. The sets of consequences are then used to interpret sets of association rules resulting from a data mining procedure.	automation	Jan Rauch;Milan Simunek	2014		10.1007/978-3-319-09870-8_20	knowledge management;data science;body of knowledge;data mining;domain knowledge	DB	-3.8036414150809423	-29.99362426393093	106397
a079ce11e98c26508496494b9e53cf97434581c7	an integrated approach to learning bayesian networks of rules	structure learning;integrated approach;bayesian network;learning algorithm;algorithme glouton;inductive logic programming;logique propositionnelle;algorithme apprentissage;rule learning;reseau bayes;red bayes;propositional logic;bayes network;greedy algorithm;algoritmo gloton;logica proposicional;algoritmo aprendizaje;programmation logique inductive	Inductive Logic Programming (ILP) is a popular approach for learning rules for classification tasks. An important question is how to combine the individual rules to obtain a useful classifier. In some instances, converting each learned rule into a binary feature for a Bayes net learner improves the accuracy compared to the standard decision list approach [3,4,14]. This results in a two-step process, where rules are generated in the first phase, and the classifier is learned in the second phase. We propose an algorithm that interleaves the two steps, by incrementally building a Bayes net during rule learning. Each candidate rule is introduced into the network, and scored by whether it improves the performance of the classifier. We call the algorithm SAYU for Score As You Use. We evaluate two structure learning algorithms Naive Bayes and Tree Augmented Naive Bayes. We test SAYU on four different datasets and see a significant improvement in two out of the four applications. Furthermore, the theories that SAYU learns tend to consist of far fewer rules than the theories in the two-step approach.		Jesse Davis;Elizabeth S. Burnside;Inês de Castro Dutra;David Page;Vítor Santos Costa	2005		10.1007/11564096_13	naive bayes classifier;computer science;artificial intelligence;machine learning;pattern recognition;bayesian network;mathematics;algorithm	ML	8.409292017092174	-31.990616513482298	106484
84e77d16cae91ef37dc916d285c622911698be5b	advances in computation and intelligence : second international symposium, isica 2007, wuhan, china, september 21-23, 2007 : proceedings	simulation and modeling;algorithm analysis;data mining;artificial intelligent;internet computing;problem complexity;pattern recognition;information system;computational biology;knowledge discovery	Multiobjective Evolutionary Optimization.- A New Evolutionary Decision Theory for Many-Objective Optimization Problems.- A Multi-Objective Genetic Algorithm Based on Density.- Interplanetary Trajectory Optimization with Swing-Bys Using Evolutionary Multi-objective Optimization.- A Hybrid Evolutionary Multi-objective and SQP Based Procedure for Constrained Optimization.- Study on Application of Multi-Objective Differential Evolution Algorithm in Space Rendezvous.- The Multi-objective ITO Algorithms.- An Evolutionary Algorithm for Dynamic Multi-Objective TSP.- The Construction of Dynamic Multi-objective Optimization Test Functions.- An Effective Dynamical Multi-objective Evolutionary Algorithm for Solving Optimization Problems with High Dimensional Objective Space.- Evolutionary Algorithms and Operators.- Operator Adaptation in Evolutionary Programming.- A Comparison of GAs Using Penalizing Infeasible Solutions and Repairing Infeasible Solutions on Average Capacity Knapsack.- About the Limit Behaviors of the Transition Operators Associated with EAs.- Differential Evolution Algorithm Based on Simulated Annealing.- A Novel Memetic Algorithm for Global Optimization Based on PSO and SFLA.- Building on Success in Genetic Programming: Adaptive Variation and Developmental Evaluation.- A Granular Evolutionary Algorithm Based on Cultural Evolution.- A Self-adaptive Mutations with Multi-parent Crossover Evolutionary Algorithm for Solving Function Optimization Problems.- Evolutionary Optimization.- A Quantum Genetic Simulated Annealing Algorithm for Task Scheduling.- Optimized Research of Resource Constrained Project Scheduling Problem Based on Genetic Algorithms.- An Evolutionary Agent System for Mathematical Programming.- Agent-Based Coding GA and Application to Combat Modeling and Simulation.- A Two-Stage Genetic Algorithm for the Multi-multicast Routing.- A Novel Lower-Dimensional-Search Algorithm for Numerical Optimization.- Performance Evaluation of Three Kinds of Quantum Optimization.- An Efficient Multilevel Algorithm for Inverse Scattering Problem.- Evolutionary Learning.- A New Evolutionary Neural Network and Its Application for the Extraction of Vegetation Anomalies.- Dynamic System Evolutionary Modeling: The Case of SARS in Beijing.- An Tableau Automated Theorem Proving Method Using Logical Reinforcement Learning.- Gene Expression Programming with DAG Chromosome.- Neural Networks.- Zhang Neural Network for Online Solution of Time-Varying Sylvester Equation.- A Global Optimization Algorithm Based on Novel Interval Analysis for Training Neural Networks.- Approximate Interpolation by Neural Networks with the Inverse Multiquadric Functions.- Decomposition Mixed Pixel of Remote Sensing Image Based on Tray Neural Network Model.- A Novel Kernel Clustering Algorithm Based Selective Neural Network Ensemble Model for Economic Forecasting.- An Evolutionary Neural Network Based Tracking Control of a Human Arm in the Sagittal Plane.- Ant Colony, Particle Swarm Optimization and Artificial Immune Systems.- New Ant Colony Optimization for Optimum Multiuser Detection Problem in DS-CDMA Systems.- A Fast Particle Swarm Optimization Algorithm with Cauchy Mutation and Natural Selection Strategy.- Fast Multi-swarm Optimization with Cauchy Mutation and Crossover Operation.- Particle Swarm Optimization Using Levy Probability Distribution.- Re-diversification Based Particle Swarm Algorithm with Cauchy Mutation.- An Improved Multi-Objective Particle Swarm Optimization Algorithm.- Dynamic Population Size Based Particle Swarm Optimization.- An Improved Particle Swarm Optimization for Data Streams Scheduling on Heterogeneous Cluster.- A Steepest Descent Evolution Immune Algorithm for Multimodal Function Optimization.- A Hybrid Clonal Selection Algorithm Based on Multi-parent Crossover and Chaos Search.- Pattern Recognition.- Spatial Clustering Method Based on Cloud Model and Data Field.- Diversity Analysis of Information Pattern and Information Clustering Algorithm.- Instant Message Clustering Based on Extended Vector Space Model.- Continuous K-Nearest Neighbor Queries for Moving Objects.- Texture Classification of Aerial Image Based on Bayesian Networks with Hidden Nodes.- Human Motion Recognition Based on Hidden Markov Models.- Data Mining.- Parameter Setting for Evolutionary Latent Class Clustering.- Automatic Data Mining by Asynchronous Parallel Evolutionary Algorithms.- Texture Image Retrieval Based on Contourlet Coefficient Modeling with Generalized Gaussian Distribution.- Heterogeneous Spatial Data Mining Based on Grid.- A Clustering Scheme for Large High-Dimensional Document Datasets.- Intelligent Systems.- Self-tuning PID Control of Hydro-turbine Governor Based on Genetic Neural Networks.- Adaptive Rate Selection Scheme Based on Intelligent Learning Algorithm in Wireless LANs.- The Research on Generic Project Risk Element Network Transmission Parallel Computing Model.- On the Performance of Metamodel Assisted MOEA/D.- A High Precision OGC Web Map Service Retrieval Based on Capability Aware Spatial Search Engine.- Analysis of the Performance of Balance of Digital Multi-value Based on Chebyshev Chaotic Sequence.- The Transformation Between Fuzzy Cognitive Maps and a Class of Simplified Dynamical Cognitive Networks.- Evolutionary Design.- Cryptanalysis of Two-Round DES Using Genetic Algorithms.- A Novel Artistic Image Generation Technique: Making Relief Effects Through Evolution.- Data Genome: An Abstract Model for Data Evolution.- Intrinsic Evolution of Frequency Splitter with a New Analog EHW Platform.- Towards the Role of Heuristic Knowledge in EA.- Using Instruction Matrix Based Genetic Programming to Evolve Programs.- Fuzzy Pattern Rule Induction for Information Extraction.- An Orthogonal and Model Based Multiobjective Genetic Algorithm for LEO Regional Satellite Constellation Optimization.		Lishan Kang;Yong Liu;Sanyou Zeng	2007		10.1007/978-3-540-74581-5	artificial architecture;marketing and artificial intelligence;computer science;data science;machine learning;computational intelligence;data mining	Arch	1.3329714777329773	-35.75183494947954	106609
bb2705e084832b690ecffe1655889307c91f289a	reusable knowledge from symbolic regression classification	classification algorithm;regression analysis data mining knowledge management knowledge representation pattern classification;complexity theory;pattern recognition classification symbolic regression knowledge management data mining;hidden markov model;learning model;supervised classification;knowledge management;level set;data mining;classification;mathematical model hidden markov models humans classification algorithms spirals complexity theory equations;hidden markov models;spirals;classification algorithms;pattern classification;mathematical model;pattern recognition;mathematical formulas reusable knowledge symbolic regression classification regression method supervised classification labeled training set separation surface model function respective equation;symbolic regression;regression analysis;humans;knowledge representation;analytical model	In this paper we generalize the well known regression method to fulfill supervised classification aiming to produce a learning model which best separates the class members of a labeled training set. The separation surface is represented by the level set of a model function and it is defined by the respective equation. The model is represented by mathematical formulas and composed of an optimum set of expressions of a given superset. We show that this property gives human experts additional insight in the application domain. Furthermore the representation in terms of mathematical formulas (e.g. the analytical model and its first and second derivative) adds additional value to the classifier and enables to answer questions which other classifier approaches cannot.	application domain;machine learning;statistical classification;supervised learning;symbolic regression;test set;whole earth 'lectronic link	Ingo Schwab;Norbert Link	2011	2011 Fifth International Conference on Genetic and Evolutionary Computing	10.1109/ICGEC.2011.34	biological classification;computer science;level set;machine learning;pattern recognition;mathematical model;data mining;mathematics;hidden markov model;regression analysis;spiral	Vision	4.996972408766623	-30.260013629260747	106718
aa999f6ccdb1e151c8b1bdd86f7616b97dbec1fb	digging in the details: a case study in network data mining	irregularity;statistical approach;extraction information;modelizacion;human cognition;tratamiento transaccion;alarm;medicion automatica;analisis estadistico;visualizacion;analisis datos;information extraction;securite informatique;hombre;intelligence artificielle;automatic measurement;probabilistic approach;mesure automatique;data mining;journal article;irregularite;computer security;modelisation;data analysis;visualization;detecteur phase;statistical analysis;visualisation;fouille donnee;enfoque probabilista;approche probabiliste;seguridad informatica;alarme;network model;decouverte connaissance;cognition;analyse statistique;irregularidad;human;cognicion;artificial intelligence;descubrimiento conocimiento;analyse donnee;phase detector;inteligencia artificial;transaction processing;detector fase;alarma;modeling;busca dato;extraccion informacion;traitement transaction;homme;knowledge discovery	Network Data Mining builds network linkages (network models) between myriads of individual data items and utilizes special algorithms that aid visualization of ‘emergent' patterns and trends in the linkage. It complements conventional and statistically based data mining methods. Statistical approaches typically flag, alert or alarm instances or events that could represent anomalous behavior or irregularities because of a match with pre-defined patterns or rules. They serve as ‘exception detection' methods where the rules or definitions of what might constitute an exception are able to be known and specified ahead of time. Many problems are suited to this approach. Many problems however, especially those of a more complex nature, are not well suited. The rules or definitions simply cannot be specified; there are no known suspicious transactions. This paper presents a human-centered network data mining methodology. A case study from the area of security illustrates the application of the methodology and corresponding data mining techniques. The paper argues that for many problems, a ‘discovery' phase in the investigative process based on visualization and human cognition is a logical precedent to, and complement of, more automated ‘exception detection' phases.	data mining	John Galloway;Simeon J. Simoff	2005		10.1007/11427995_2	cognition;visualization;computer science;artificial intelligence;data mining;operations research;information extraction	ML	-4.13236735941188	-31.66914397447163	106964
33974c27f26c2a5844e82b4954e16a941afa4089	fuzzy logic applied in remote sensing image classification	remote sensing image;analisis imagen;teledetection;hierarchical system;fuzzy classification;statistical data;sistema experto;distribution donnee;red local;expert systems fuzzy logic image classification remote sensing;image processing;expert systems;maximum likelihood;hierarchized structure;systeme hierarchise;logique floue;maximum vraisemblance;new mexico;land cover classification;procesamiento imagen;base connaissance;logica difusa;image classification;structure hierarchisee;image;logical programming;distribucion estadistica;classification;back propagation neural network;fuzzy logic remote sensing image classification control systems statistical distributions image analysis training data hybrid intelligent systems system testing satellites;traitement image;medida sin contacto;data distribution;fuzzy logic;local network;sistema jerarquizado;control system;landsat 7 etm;fuzzy expert system;distribution statistique;programmation logique;backpropagation algorithm;remote sensing;classification image;teledeteccion;donnee statistique;algorithme retropropagation;base conocimiento;image analysis;non contact measurement;sistema difuso;expert knowledge;systeme flou;information system;systeme expert;dato estadistico;reseau neuronal;reseau local;programacion logica;analyse image;distribucion dato;mesure sans contact;statistical distribution;red neuronal;hierarchical fuzzy expert system fuzzy logic knowledge based method expert knowledge training data remote sensing image classification landsat 7 etm rio rancho area new mexico;systeme information;estructura jerarquizada;maxima verosimilitud;maximum likelihood classifier;fuzzy system;neural network;sistema informacion;knowledge base;expert system;algoritmo retropropagacion	Fuzzy logic, a knowledge-based method and widely used in control systems, is proposed to be applied in remote sensing image classification. Fuzzy logic makes no assumption about statistical distribution of the data and it provides more complete information for a thorough image analysis, such as fuzzy classification results. It is interpretable and can use expert knowledge and training data at the same time. In this paper, a hierarchical fuzzy expert system is developed for the remote sensing image classification and tested on the land cover classification of Landsat 7 ETM+ over the Rio Rancho area, New Mexico incorporated area. The classification result is compared with that of maximum likelihood classifier and back-propagation neural network classification and it can get better classification performance.	artificial neural network;backpropagation;computer vision;control system;expert system;fuzzy classification;fuzzy logic;image analysis;software propagation;winsock	Yan Wang;Mo M. Jamshidi	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401402	local area network;fuzzy logic;probability distribution;contextual image classification;biological classification;fuzzy classification;computer science;artificial intelligence;backpropagation;neuro-fuzzy;image;data mining;maximum likelihood;hierarchical control system;expert system;one-class classification;information system	Robotics	9.273568444521844	-30.606195935929435	107006
23d6f4ff82f20dcf17c7f145849821ef5191dd4c	a case based method to predict optimal k value for k-nn algorithm			k-nearest neighbors algorithm	Yang Zhongguo;Hongqi Li;Zhu Liping;Liu Qiang;Sikandar Ali	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-161062	artificial intelligence;mathematics;machine learning	Robotics	9.115626554147493	-36.78860792725676	107017
abdd99a8641c882eb400458ee3e8d11aefff4931	pattern recognition via projection-based knn rules	k;proyeccion;classification automatique statistiques;analyse multivariable;estimator robustness;multivariate analysis;analisis datos;high dimensional data knn rules pattern recognition robustness;discriminant analysis;analyse discriminante;data analysis;analisis discriminante;robustez estimador;62h30;projection;high dimensional data;statistical computation;nearest neighbor;calculo estadistico;pattern recognition;analisis multivariable;analyse donnee;robustness;calcul statistique;reconnaissance forme;reconocimiento patron;random projection;nearest neighbor classification;knn rules;robustesse estimateur	A new procedure for pattern recognition is introduced based on the concepts of random projections and nearest neighbors. It can be considered as an improvement of the classical nearest neighbor classification rules. Besides the concept of neighbors, the notion of district, a larger set into which the data will be projected, is introduced. Then a one-dimensional kNN method is applied to the projected data on randomly selected directions. Thismethod,which ismore accurate to handle high-dimensional data, has some robustness properties. The procedure is also universally consistent. Moreover, the method is challenged with the Isolet data set where a very high classification score is obtained. © 2009 Elsevier B.V. All rights reserved.	k-nearest neighbors algorithm;pattern recognition;random projection;randomness	Ricardo Fraiman;Ana Justel;Marcela Svarc	2010	Computational Statistics & Data Analysis	10.1016/j.csda.2009.12.009	projection;machine learning;pattern recognition;mathematics;multivariate analysis;linear discriminant analysis;data analysis;k-nearest neighbors algorithm;statistics;robustness;clustering high-dimensional data	AI	8.98108463150132	-34.359927686159345	107041
324483dffaad68b5a77bf0d4d17acec5e611b198	zslices — towards bridging the gap between interval and general type-2 fuzzy logic	fuzzy sets fuzzy logic fuzzy systems equations uncertainty complexity theory conferences;complexity theory;type 2 sets zslices fuzzy logic systems;uncertainty;fuzzy logic systems;fuzzy systems fuzzy logic;fuzzy sets;fuzzy logic;type 2 sets;qa75 electronic computers computer science;computational complexity;zslices;councils;fuzzy systems;embedded computing;conferences	Higher order fuzzy logic systems such as interval type-2 fuzzy logic systems have been shown to be very well suited to dealing with the large amounts of uncertainties present in the majority of real world applications. General type-2 fuzzy logic systems are expected to further extend this capability. However, the complexity as well as the immense computational requirements have generally prevented a foray into general type-2 fuzzy logic research. This paper introduces an alternative approach termed zSlices for representing general type-2 sets based on interval type-2 sets. Thus, this will lead to a smooth transition from interval to general type-2 fuzzy systems. The proposed approach will lead to a significant reduction in both the complexity and the computational requirements for general type-2 fuzzy logic systems. Hence, this will lead to facilitating the application of general type-2 fuzzy logic to many real world applications.	apache axis;bridging (networking);computation;defuzzification;formal system;fuzzy control system;fuzzy logic;fuzzy set;norm (social);requirement;type-2 fuzzy sets and systems	Christian Wagner;Hani Hagras	2008	2008 IEEE International Conference on Fuzzy Systems (IEEE World Congress on Computational Intelligence)	10.1109/FUZZY.2008.4630413	fuzzy logic;fuzzy electronics;discrete mathematics;uncertainty;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;theoretical computer science;neuro-fuzzy;mathematics;fuzzy set;fuzzy associative matrix;computational complexity theory;fuzzy set operations;fuzzy control language;algorithm;fuzzy control system	Robotics	0.08209235857039633	-24.80881082264747	107249
7cdae5f91f0914e1e6549b4f92754c201c9d33a8	effect of similar behaving attributes in mining of fuzzy association rules in the large databases	extraction information;association statistique;tratamiento transaccion;analisis datos;information extraction;redundancia;base donnee tres grande;logique floue;statistical association;logica difusa;data mining;association rule mining;fuzzy logic;data analysis;asociacion estadistica;redundancy;fouille donnee;linguistic terms;analyse donnee;very large databases;transaction processing;busca dato;extraccion informacion;traitement transaction;redondance;fuzzy association rules	Association rule mining is an active data mining research area. Recent years have witnessed many efforts on discovering fuzzy associations. The key strength of fuzzy association rule mining is its completeness. This strength, however, comes with a major drawback. It often produces a huge number of fuzzy associations. This is particularly true for datasets whose attributes are highly correlated. The huge number of fuzzy associations makes it very difficult for a human user to analyze them. Existing research has shown that most of the discovered rules are actually redundant or insignificant. In this paper, we propose a novel technique to overcome this problem.The approach is effective because experiment results show that the set of produced rules is typically very small. Our solution also reduces the size of average transactions and dataset. Our performance study shows that this solution has a superior performance over the other algorithms.	database	Zahra Farzanyar;Mohammad Reza Kangavari;Sattar Hashemi	2006		10.1007/11751540_120	fuzzy logic;association;association rule learning;transaction processing;computer science;artificial intelligence;data science;data mining;database;redundancy;data analysis;information extraction	DB	-3.2162486532150756	-33.63051491690346	107436
2a2f76362d9e5afb0323ca57bec9cfaea4e4c647	granular computing in the development of fuzzy controllers	granular computing	This study elaborates on the role of information granularity in the development of fuzzy controllers. As opposed to numeric data being commonly accepted by fuzzy controllers, we discuss a general processing framework involving data-information granules exhibiting various levels of information granularity. The paper analyzes an impact of information granularity on the performance of the controller. We study a way in which information granules arise in control problems, elaborate on a way of describing these granules as well as provide a way of quantifying the level of information granularity. A number of analysis and design issues are studied including robustness of the fuzzy controller, representation of linguistic information and quantification of its granularity. Nonlinear characteristics of the compiled version of the fuzzy controller operating in presence of granular information are discussed in detail. Illustrative numerical examples are provided as well. © 1999 John Wiley u0026 Sons, Inc.	granular computing	Witold Pedrycz;George Vukovich	1999	Int. J. Intell. Syst.	10.1002/(SICI)1098-111X(199904)14:4%3C419::AID-INT5%3E3.0.CO;2-5	fuzzy logic;controller;granular computing;computer science;artificial intelligence;control theory;estimation theory;information technology;expert system	AI	1.5814730221536437	-26.901988393821973	107507
0ba1d20b79bfbd4b3c45e29790056d2660948ed6	value of ranked voting methods for estimation by analogy	ordinary least square regression;jack knifing procedure;stepwise regression;estimation by analogy;regular k based eba;categorical regression tree;ranked voting method;similarity measure	Background: One long-standing issue in Estimation by Analogy (EBA) is finding closest analogies. Prior studies revealed that existing similarity measures are easily influenced by extreme values and irrelevant features. Aims: Instead of identifying closest projects based on the aggregated similarity degrees, we propose to use Ranked Voting Methods that rank projects per feature and then aggregate those ranks over all features using voting count rules. The project(s) with highest score will be the winners and form new estimate for the target project. This also enables us to automatically come up with the preferred number of analogies for each target project since the winner set may contain more than a single winner. Method: Empirical evaluation with Jack-knifing procedure has been carried out in which nine datasets come from two repositories (PROMISE & ISBSG) were used for benchmarking. The proposed models are compared to some well-known estimation methods: regular K based EBA, Stepwise Regression, Ordinary Least Square Regression, and Regression tree (CART). Results & Conclusions: The performance figures of the proposed models were promising. The use of voting methods present some useful advantages: (1) saving time in finding appropriate K number of analogies for each individual project, (2) No need for project pruning, and (3) No data standardization is required.	aggregate data;decision tree learning;ordinary least squares;relevance;stepwise regression	Mohammad Azzeh;Marwan Alseid	2013	IET Software	10.1049/iet-sen.2012.0119	computer science;stepwise regression;data mining	AI	3.890876837848339	-34.6146884618507	107511
44a3fb5a61b79d39ac8597e09a949ffbfc035abb	projective art with buffers for the high dimensional space clustering and an application to discover stock associations	cluster algorithm;bayes rule;high dimensionality;part;buffer management;data mining;stock concurrence association;very large database;association rule;clustering;neural network;projective adaptive resonance theory	Unlike to traditional hierarchical and partitional clustering algorithms which always fail to deal with very large databases, a neural network architecture, Projective Adaptive Resonance Theory (PART), is developed for the high dimensional space clustering. However, the success of the PART algorithm depends on both accurate parameters and satisfied orders of input data sets. These disadvantages prevent PART from being applied to realtime databases. In this paper, we propose an improved method, Projective ART with buffer management, to overcome these disadvantages. The major contributions of our method are introducing a buffer management and a new similar degree function and buffer checkout process. The buffer management mechanism allows data sets not to be immediately clustered to one cluster. The purpose of the average similar degree is to successfully work with high similar noise data sets and partly achieve an order-independent objective without correct parameters. And the average similar degree has a good attribute, the parameter-tolerance. Namely, the clustering result doesn’t depend on the precise choice of input parameters, and different parameter values have close clustering results including dimensions associated with clusters. The buffer checkout process can handle a huge amount of input data sets by a small buffer space. Also, simulations and comparisons in high dimensional spaces are reported, and an application by using our algorithm to find stock concurrence association rules is given finally. ∗Research supported by the National Science Foundation of China (10371034), the Specialized Research Fund for the Doctoral Program of Higher Education (20050532023) and ”985 Project”. †Corresponding author E-mail: lhhuang@hnu.cn.	adaptive resonance theory;algorithm;artificial neural network;association rule learning;cluster analysis;database;network architecture;noise (electronics);point of sale;simulation	Lian Liu;Lihong Huang;Mingyong Lai;Chaoqun Ma	2009	Neurocomputing	10.1016/j.neucom.2008.01.020	association rule learning;fuzzy clustering;computer science;artificial intelligence;canopy clustering algorithm;machine learning;cure data clustering algorithm;data mining;mathematics;cluster analysis;bayes' theorem;artificial neural network;statistics;very large database	DB	6.831160523020903	-35.466115119683145	108006
48aeb5a2a96dff254771fceb63ee25fe4db716f5	construction of efficient rulesets from fuzzy data through simulated annealing	learning;fuzzy data;simulated annealing;classification;aprendizaje;apprentissage;recuit simule;classification system;recocido simulado;sistema difuso;systeme flou;donnee floue;clasificacion;generation regle;fuzzy system;decision rule	This paper proposes a simulated annealing-based approach for obtaining compact efficient classification systems from fuzzy data. Different methods for generating decision rules from fuzzy data share a problem in multidimensional spaces: their high cardinality. In order to solve it, the method of simulated annealing is proposed. This approach is illustrated with two well-known learning sets.	fuzzy logic;simulated annealing	Francisco Botana	2000		10.1007/3-540-45331-8_27	simulated annealing;biological classification;computer science;artificial intelligence;machine learning;data mining;decision rule;mathematics;fuzzy set operations;adaptive simulated annealing;fuzzy control system	Robotics	8.452889162538916	-31.295970307344675	108296
09ff91fda98d35583ba667e1858a24ad6613cfbb	the exploration of machine learning for abnormal prediction model of telecom business support system		Companies leverage plenty of monitoring tools collecting performance metrics of Telecom BSS to assure it is in good status. The influence of the system failure is variance, depending on the length of time to finish the system reparation. The metrics collected by monitoring tools may have the indication of the system failure, and the maintainers have chances to foresee a system failure from those metrics. However, the metrics collected by the monitoring tools are too much, and some hints may hide in combinations of multiple metrics. We leverage machine learning approaches to address this problem. We used several machine learning tools and algorithms to explore the configuration of the machine learning models to obtain the model performing the best to our dataset. We compared many algorithms like linear SVM, SVM with RBF kernel, random forest and fully connected neural network. We also introduced an anomaly detection learning technique to see if better performance can be achieved. We found SVM with RBF kernel can achieve the best performance to our dataset, and we conducted a comprehensive grid search of the hyperparameters of the RBF SVM to found the best configuration to our dataset. We achieve F-score 21 in the final explored result and the model can predict 15% of the system failure 60 minutes in advance.	algorithm;anomaly detection;artificial neural network;encode;f1 score;feature vector;machine learning;one-hot;overfitting;radial basis function kernel;random forest;relevance;web page;on-line system	Jen-Hao Chen;Chao-Wen Huang;Chia Chun Shih	2017	2017 19th Asia-Pacific Network Operations and Management Symposium (APNOMS)	10.1109/APNOMS.2017.8094039	anomaly detection;hyperparameter optimization;online machine learning;support vector machine;artificial neural network;computer science;telecommunications;random forest;data mining;machine learning;business support system;radial basis function kernel;artificial intelligence	ML	6.578855685036353	-36.12449672835213	108592
66f0cd2665f3af83e7776cce985436d937620a04	ensemble learning classifier system and compact ruleset	legibility;decision tree;modele agrege;ensemble learning;classifier system;modelo agregado;learning classifier system;clasificador;intelligence artificielle;almacen dato;arbol decision;classification;classifier;classificateur;aggregate model;artificial intelligence;legibilidad;inteligencia artificial;lisibilite;entrepot donnee;reseau neuronal;data warehouse;arbre decision;clasificacion;red neuronal;neural network	The aim of this paper is twofold, to improve the generalization ability, and to improve the readability of learning classifier system. Firstly, an ensemble architecture of LCS (LOSE) is described in order to improve the generalization ability of the original LCS. Secondly, an algorithm is presented for compacting the final classifier population set in order to improve the readability of LCSE, which is an amendatory version of CRA brought by Wilson. Some test experiments are conducted based on the benchmark data sets of UCI repository. The experimental results show that LCSE has better generalization ability than single LCS, decision tree, neural network and their bagging methods. Comparing with the original population rulesets, compact rulesets have readily interpretable knowledge like decision tree, whereas decrease the prediction precision lightly.		Yang Gao;Lei Wu;Joshua Zhexue Huang	2006		10.1007/11903697_6	classifier;biological classification;computer science;artificial intelligence;machine learning;decision tree;data warehouse;data mining;ensemble learning;learning classifier system	ML	9.389298321900858	-32.69210353615837	108764
55e2539a4d7436e51be2708655c473cf0d35fcb3	considering main memory in mining association rules	systeme intelligent;procesamiento informacion;adquisicion del conocimiento;algoritmo adaptativo;base donnee tres grande;sistema inteligente;acquisition connaissances;adaptive algorithm;regle association;algorithme adaptatif;association rule;knowledge acquisition;information processing;intelligent system;information system;very large databases;traitement information;systeme information;sistema informacion	We propose a family of large itemset counting algorithms which adapt to the amount of main memory available. By using historical or sampling data, the potential large itemsets (candidates) and the false candidates are identified earlier. Redundant computation is reduced(thus overall CPU time reduced) by counting different sizes of candidates together and the use of a dynamic trie. By counting candidates earlier and counting more candidates in each scan, the algorithms reduce the overall number of scans required.	computer data storage	Yongqiao Xiao;Margaret H. Dunham	1999		10.1007/3-540-48298-9_23	association rule learning;information processing;computer science;artificial intelligence;machine learning;data mining;information system;algorithm	DB	-3.484255912627723	-33.819622312898204	108855
0ee6012d015ddcc4cb75f7cc977da2be935276ac	medical diagnosis prediction using genetic programming	mitral valve prolapse;genetic program;evolutionary computation;decision tree;evolutionary programming;medical decision making;automatic programming;information overload;medical computing;decision support system;diagnostic expert systems;decision support systems;information processing;medical diagnosis genetic programming medical diagnostic imaging decision making decision support systems automatic programming intelligent systems information processing decision trees valves;intelligent systems;evolutionary computation diagnostic expert systems decision support systems automatic programming medical computing decision trees;mitral valve prolapse classification medical diagnosis prediction genetic programming decision support systems intelligent information processing decision trees automatic programming;decision trees;medical diagnosis	Medical decision making is one of the most demanding tasks in modern medicine. Based on a huge amount of solved cases and rapidly growing number of new findings, medical decision making is supposed to become more and more reliable and effective. However, physicians and medical experts are facing a new arising problem - information overload which they are not able to overcome. A solution might be found in decision support systems with intelligent information processing abilities that help to process a huge amount of data and suggest a possible decision for each new case. Regarding the simplicity of decision trees and effectiveness of evolutionary programming techniques we developed a decision support system based on automatic programming and used it to solve the mitral valve prolapse classification problem.	genetic programming	Vili Podgorelec;Peter Kokol;Jernej Zavrsnik	1999		10.1109/CBMS.1999.781271	decision support system;intelligent decision support system;decision engineering;computer science;artificial intelligence;machine learning;decision tree;data mining;business decision mapping;medical algorithm	AI	2.7726222275517243	-27.423764827819546	108996
fc0d0e3451dceb9d5028b39a91e64d49a91d6bc1	a swarm intelligence based clustering approach for outlier detection	swarm intelligence;large dataset;intrusion detection;credit card fraud detection;data mining;hierarchical agglomerative clustering;particle swarm optimisation data mining;computer network;outlier detection;conference item;particle swarm optimizer;stock price;particle swarm optimization;merging;hierarchical particle swarm optimization based clustering;recurrent neural networks;data mining particle swarm optimization merging breast cancer iris recurrent neural networks;abnormal health condition;iris;particle swarm optimisation;breast cancer;hierarchical agglomerative clustering swarm intelligence based clustering outlier detection data mining knowledge discovery intrusion detection computer network credit card fraud detection abnormal stock price change abnormal health condition hierarchical particle swarm optimization based clustering;fraud detection;credit cards;abnormal stock price change;swarm intelligence based clustering;knowledge discovery	Outlier detection is an important field in data mining and knowledge discovery, which aims to identify abnormal observations in a large dataset. Common application areas of outlier detection are intrusion detection in computer networks, credit cards fraud detection, detecting abnormal changes in stock prices, and identifying abnormal health conditions. We propose the use of a novel swarm intelligence based clustering technique called Hierarchical Particle Swarm Optimization Based Clustering (HPSO-clustering) for outlier detection. The proposed technique is able to perform Hierarchical Agglom-erative Clustering (HAC) as well as outlier detection. In the proposed approach a swarm of particles evolves through different stages to identify outliers and normal clusters. The experimentation of the proposed approach is performed on benchmark datasets which show that the efficiency of the approach is better than some other popular outlier detection techniques.	anomaly detection;benchmark (computing);cluster analysis;data mining and knowledge discovery;experiment;high-availability cluster;image scaling;intrusion detection system;particle swarm optimization;sensor;swarm intelligence	Shafiq Alam;Gillian Dobbie;Patricia Riddle;M. Asif Naeem	2010	IEEE Congress on Evolutionary Computation	10.1109/CEC.2010.5586152	intrusion detection system;swarm intelligence;computer science;data science;recurrent neural network;breast cancer;machine learning;data mining;particle swarm optimization	ML	4.810731938159998	-37.958554817389526	109019
93121da5d43f22dd6504b07f03a9d03720d72319	some new features for protein fold prediction	genetique;hydrophobic compound;level 2;folding;genetica;amino acid;multilayer perceptrons;teoria conjunto;fonction base radiale;theorie ensemble;intelligence artificielle;multilayer perceptron;set theory;pliage;classification;genetics;compose hydrophobe;perceptron multicouche;radial basis function;radial basis function network;compuesto hidrofobo;doblado;artificial intelligence;protein folding;difference set;inteligencia artificial;reseau neuronal;funcion radial base;clasificacion;red neuronal;level 1;neural network	In this paper we propose several sets of new features for protein fold prediction. The first feature set consisting of 47 features uses only the sequence information. We also define four different sets of features based on hydrophobicity of amino acids. Each such set has 400 features which are motivated by folding energy modeling. To define these features we have considered pair-wise amino acids (AA) interaction potential. The effectiveness of the proposed feature sets is tested using multilayer perceptron and radial basis function networks to solve the 4 class (level 1) and 27 class (level 2) prediction problems as defined in the context of SCOP classification. Our investigation shows that such features have good discriminating powers in predicting protein folds.	cpu cache;connectionism;energy modeling;feature selection;linear discriminant analysis;multilayer perceptron;radial (radio);radial basis function network;scop	Nikhil R. Pal;Debrup Chakraborty	2003		10.1007/3-540-44989-2_140	protein folding;radial basis function;amino acid;biological classification;computer science;artificial intelligence;folding;machine learning;mathematics;multilayer perceptron;radial basis function network;artificial neural network;algorithm;difference set;set theory	ML	9.92872478026755	-32.60108452761683	109065
1140e26d541f36df74c47d1112d510a31428a75f	creation of a fuzzy knowledge base for adaptive security systems	fuzzy knowledge base;rules monitoring fuzzy knowledge adaptive security systems matlab fuzzy logic toolbox neural network weights;fuzzy set theory;matlab fuzzy logic toolbox adaptive security system adaptive security rules fuzzy knowledge base;adaptive systems;security of data adaptive systems fuzzy set theory knowledge based systems;adaptive security rules;knowledge based systems fuzzy logic security matlab adaptive systems adaptation models biological system modeling;adaptive security system;matlab fuzzy logic toolbox;security of data;knowledge based systems	To design next generation adaptive security systems the powerful intelligent components should be developed. The paper describes the fuzzy knowledge base specifying relationships between threats and protection mechanisms by Mathworks MATLAB Fuzzy Logic Toolbox. The goal is to increase the effectiveness of the system reactions by minimization of neural network weights. We demonstrate a technique for creation of a fuzzy knowledge base to improve the system protection via rules monitoring and correction.	artificial neural network;decision support system;fuzzy logic;knowledge base;matlab;production (computer science);protection mechanism;security information and event management;threat (computer)	Philipp Nesteruk;Lesya Nesteruk;Igor V. Kotenko	2014	2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing	10.1109/PDP.2014.115	fuzzy electronics;fuzzy cognitive map;adaptive neuro fuzzy inference system;computer science;theoretical computer science;adaptive system;neuro-fuzzy;knowledge-based systems;machine learning;data mining;fuzzy set;fuzzy set operations;fuzzy control system	EDA	3.673734019055666	-27.201716849547893	109440
690f4f67939e9f96bd20a45ab50d55ce4e17e3d8	outlier detection using rough set theory	rough set theory;base connaissance;outlier;integrite;integridad;outlier detection;observacion aberrante;integrity;theorie ensemble approximatif;observation aberrante;base conocimiento;rough set;knowledge base	In this paper, we suggest to exploit the framework of rough set for detecting outliers — individuals who behave in an unexpected way or feature abnormal properties. The ability to locate outliers can help to maintain knowledge base integrity and to single out irregular individuals. First, we formally define the notions of exceptional set and minimal exceptional set. We then analyze some special cases of exceptional set and minimal exceptional set. Finally, we introduce a new definition for outliers as well as the definition of exceptional degree. Through calculating the exceptional degree for each object in minimal exceptional sets, we can find out all outliers in a given dataset.	anomaly detection;approximation;knowledge base;rough set;sensor;set theory	Feng Jiang;Yuefei Sui;Cungen Cao	2005		10.1007/11548706_9	knowledge base;rough set;computer science;machine learning;pattern recognition;data mining;mathematics;statistics	ML	-0.544779789596241	-30.83330953344931	109796
1631843a9d88bc8529c7ad6abf228ad00b7a034e	classifier construction by graph-based induction for graph-structured data	arbre graphe;industria farmaceutica;decision tree;analisis datos;tree graph;call graph;compose chimique;intelligence artificielle;toxicidad;toxicite;arbol decision;classification;industrie pharmaceutique;construction graphe;data analysis;compuesto quimico;machine learning;prediction accuracy;on the fly;artificial intelligence;analyse donnee;toxicity;inteligencia artificial;pharmaceutical industry;arbol grafo;data classification;graph construction;arbre decision;chemical compound;clasificacion;construccion grafo;structured data	A machine learning technique called Graph-Based Induction (GBI) efficiently extracts typical patterns from graph-structured data by stepwise pair expansion (pairwise chunking). It is very efficient because of its greedy search. Meanwhile, a decision tree is an effective means of data classification from which rules that are easy to understand can be obtained. However, a decision tree could not be produced for the data which is not explicitly expressed with attribute-value pairs. In this paper, we proposes a method of constructing a classifier (decision tree) for graph-structured data by GBI. In our approach attributes, namely substructures useful for classification task, are constructed by GBI on the fly while constructing a decision tree. We call this technique Decision Tree - Graph-Based Induction (DT-GBI). DT-GBI was tested against a DNA dataset from UCI repository. Since DNA data is a sequence of symbols, representing each sequence by attribute-value pairs by simply assigning these symbols to the values of ordered attributes does not make sense. The sequences were transformed into graph-structured data and the attributes (substructures) were extracted by GBI to construct a decision tree. Effect of adjusting the number of times to run GBI at each node of a decision tree is evaluated with respect to the predictive accuracy. The results indicate the effectiveness of DT-GBI for constructing a classifier for graph-structured data.		Warodom Geamsakul;Takashi Matsuda;Tetsuya Yoshida;Hiroshi Motoda;Takashi Washio	2003		10.1007/3-540-36175-8_6	call graph;segment tree;decision tree learning;biological classification;data model;computer science;artificial intelligence;machine learning;decision tree;toxicity;incremental decision tree;data mining;interval tree;search tree;data analysis;id3 algorithm;tree;algorithm;decision stump	NLP	-2.633606422121284	-33.48050951202334	109943
e72ac78f88e139bb455c3918d3aec4ea0d702f18	neuro fuzzy classification and detection technique for bioinformatics problems	biology computing;cluster algorithm;pattern clustering;fuzzy c mean;science and technology;mathematics computing;fuzzy reasoning;matlab software neuro fuzzy classification bioinformatics clustering techniques sugeno type neuro fuzzy model adaptive neuro fuzzy inference system recurrent classification fuzzy c means;bioinformatics fuzzy systems mathematical model mathematics physics computer science behavioral science biology computing information analysis pattern analysis;clustering techniques;qa mathematics;fuzzy set theory;neuro fuzzy classification;sugeno type neuro fuzzy model;behavioral science;computational science and engineering;neuro fuzzy;fuzzy c means;neuro fuzzy system;pattern classification;recurrent neural nets biology computing fuzzy reasoning fuzzy set theory mathematics computing pattern classification pattern clustering;recurrent classification;recurrent neural nets;biological data;matlab software;adaptive neuro fuzzy inference system;tk electrical engineering electronics nuclear engineering;bioinformatics	Bioinformatics is an emerging science and technology which has lots of research potential in the future. It involves multi-interdisciplinary approaches such as mathematics, physics, computer science and engineering, biology, and behavioral science. Computers are used to gather, store, analyze as well as integration of patterns and biological data information which can then be applied to discover new useful diagnosis or information. In this study, the focus was directed to the classification or clustering techniques which can be applied in the bioinformatics fields based on the Sugeno type neuro fuzzy model or ANFIS (adaptive neuro fuzzy inference system). It is very important to identify new integration of classification or clustering algorithm especially in neuro fuzzy domain as compared to conventional or traditional method. This paper explores the suitability and performance of recurrent classification technique, fuzzy c means (FCM) act as classifier in neuro fuzzy system compared to subclustering method. A package of software based on neuro fuzzy model (ANFIS) has been developed using MATLAB software and optimization were done with the help from WEKA. A set diabetes data based on real diagnosis of patient was used	adaptive neuro fuzzy inference system;algorithm;bioinformatics;cluster analysis;computer science;fuzzy classification;fuzzy cognitive map;fuzzy control system;inference engine;matlab;mathematical optimization;neuro-fuzzy;weka	Mohd Fauzi Bin Othman;Thomas Moh Shan Yau	2007	First Asia International Conference on Modelling & Simulation (AMS'07)	10.1109/AMS.2007.70	adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;data mining	SE	5.584540389031721	-34.5877818728743	110143
997bb96491ec8fed9c1b2ed822837d197a8b3690	a multiple svr modeling of hot rolling process combined with kernel clustering and grey relational grade	pattern clustering;support vector machines;abstracts mechanical factors;production engineering computing;steel manufacture;cluster center multiple svr modeling grey relational grade industrial process robustness generalization capability prediction problem mechanical property steel hot rolling process multiple support vector regression model training sample data set partitioning kernel k means clustering algorithm evaluation strategy submodel prediction performance model adaptive ability;regression analysis;mechanical property kernel k means multiple support vector regression model grey relational grade;support vector machines hot rolling pattern clustering production engineering computing regression analysis steel manufacture;hot rolling	In view of the complexity of the industrial process, the robustness and the generalization capability are two important criteria to evaluate a model. Aimed to solve the prediction problem of mechanical property in steel hot rolling process, we proposed a new multiple support vector regression (SVR) models approach by combining modified kernel k-means clustering with grey relational grade. In this paper, a whole training sample data set is partitioned into several subsets by using modified kernel k-means clustering algorithm, and the individual support vector regression is trained by each subset to construct the sub-model respectively. An evaluation strategy of the prediction performance of sub-model with sliding time window is further proposed for improving the prediction performance and adaptive ability of model. In order to correct the model, the grey relational grades are used for combining the outputs of multiple sub-models to obtain the final result, which are gained from relationship between a new input sample data and each cluster center. Simulation results in actual application demonstrate that this model has better generalization and prediction accuracy than the other three models.	algorithm;cluster analysis;k-means clustering;kernel (operating system);simulation	Ling Wang;Dong-Mel Fu;Wei-dong Yang	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6358945	support vector machine;computer science;machine learning;data mining;regression analysis	ML	4.793006596183875	-28.674721477489925	110448
5b0647776028070488f92521762c48bb95f0cc04	from reinforcement learning to deep reinforcement learning: an overview		This article provides a brief overview of reinforcement learning, from its origins to current research trends, including deep reinforcement learning, with an emphasis on first principles.	reinforcement learning	Forest Agostinelli;Guillaume Hocquet;Sameer Singh;Pierre Baldi	2017		10.1007/978-3-319-99492-5_13	reinforcement learning;deep learning;machine learning;artificial intelligence;computer science	ML	7.4857158782433055	-24.833838828602175	110523
f2f22f35dd75ceb98d1319373bab6352f9cb4644	deriving support threshold values and membership functions using the multiple-level cluster-based master-slave ifg approach	cluster based master slave technique;level crossing fuzzy association rules;multiple level ifg mining	Today, development of e-commerce has provided many transaction databases with useful information for investigators exploring dependencies among the items. In data mining, the dependencies among different items can be shown using an association rule. The new fuzzy-genetic (FG) approach is designed to mine fuzzy association rules from a quantitative transaction database. Three important advantages are associated with using the FG approach: (1) the association rules can be extracted from the transaction database with a quantitative value; (2) extracting proper membership functions and support threshold values with the genetic algorithm will exert a positive effect on the mining process results; (3) expressing the association rules in a fuzzy representation is more understandable for humans. In this paper, we design a comprehensive and fast algorithm that mines level-crossing fuzzy association rules on multiple concept levels with learning support threshold values and membership functions using the cluster-based master---slave integrated FG approach. Mining the fuzzy association rules on multiple concept levels helps find more important, useful, accurate, and practical information.		Mojtaba Asadollahpour Chamazi;Behrouz Minaei-Bidgoli;Mahdi Nasiri	2013	Soft Comput.	10.1007/s00500-012-0973-7	association rule learning;artificial intelligence;machine learning;data mining;mathematics	OS	-3.490327115649044	-30.280047589894764	110551
1b53eadadad127cccb01bcee0881b23a158159d9	the ersatz brain project: neural inspiration, cognitive application	neural nets;biology computing neurons application software hardware computer architecture cerebral cortex humans computer networks software performance buildings;software engineering;software engineering cognition neural nets;cognition;software development;biological constraints ersatz brain project neural inspiration cognitive application brain like computer software development	"""There is a complex relationship between the architecture of a computer, the software it is to run, and the tasks it is designed to perform. The most difficult aspect of building a brain-like computer may not be in its construction, but in its use: How can it be programmed? What can it do well? What does it do poorly? Software development in the past has proved far more difficult and far slower than hardware development. There is no reason to expect a brain like computer to be any different. We suggest some operations a brain like computer might do well. Our system is not so much """"neuromorphic"""" as """"cognomorphic"""", that is, biological constraints suggest new ways to perform cognitive tasks."""	computer;expect;neuromorphic engineering;software development	James A. Anderson;Paul Allopenna	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246711	simulation;cognition;artificial brain;computer science;artificial intelligence;software development;artificial neural network	Visualization	8.010145697271403	-25.03643680397151	110713
9808813fbea34576de924073d211c4ae9ce93cc6	an attribute reduction method based on rough set and svm and with application in oil-gas prediction	oil gas prediction;geophysics;rough set theory attribute reduction method svm oil gas prediction support vector machine machine learning method;support vector machines;application software;rough set theory;attribute reduction method;set theory;production engineering computing;learning systems;attribute reduction;training data;support vector machines gas industry petroleum industry production engineering computing rough set theory;machine learning;support vector machines set theory support vector machine classification machine learning training data equations application software computer science geophysics learning systems;petroleum industry;support vector machine classification;machine learning method;svm;computer science;support vector machine;rough set;gas industry	With greater generalization performance support vector machine (SVM) is a new machine learning method. Rough set theory is a new powerful tool h dealing with vagueness and uncertainty information. By combining the advantages of two approaches, an original attribute reduction method is proposed in the paper. Moreover, it is applied into oil-gas prediction to solve the problems when support vector machine is directly employed. Experiments and results show the validity and feasibility of the algorithm suggested in the paper.	algorithm;experiment;machine learning;rough set;set theory;support vector machine;vagueness	Ru Nie;Jianhua Yue	2007	6th IEEE/ACIS International Conference on Computer and Information Science (ICIS 2007)	10.1109/ICIS.2007.53	computer science;machine learning;pattern recognition;data mining;ranking svm;structured support vector machine	Robotics	3.634533755367622	-28.960412339059285	110767
70d5a8146d7cba181af592e8c4ee67bb14b741c6	gng3d - a software tool for mesh optimization based on neural networks	unsupervised learning;cluster algorithm;software tool;neural nets;best approximation;growing neural gas;mesh optimization;data visualization software tool mesh optimization neural network neural gas model unsupervised incremental clustering algorithm approximation theory face reconstruction;approximation theory;data visualisation;3d model;face recognition;image reconstruction;software tools neural networks artificial neural networks clustering algorithms optimization methods brain modeling visualization surface reconstruction software systems neural network hardware;mesh generation;optimal algorithm;unsupervised learning approximation theory data visualisation face recognition image reconstruction mesh generation neural nets solid modelling;solid modelling;neural network	A new software tool-denoted as GNG3D for mesh optimization is presented. This tool has been implemented taking as a basis a new method which is based on neural networks and consists on two differentiated phases: an optimization phase and a reconstruction phase. The optimization phase is developed applying an optimization algorithm based on the Growing Neural Gas model, which constitutes an unsupervised incremental clustering algorithm. The primary goal of this phase is to obtain a simplified set of vertices representing the best approximation of the original 3D object. In the reconstruction phase we use the information provided by the optimization algorithm to reconstruct the faces obtaining in such a way the optimized mesh. Finally, we will report some experimental results and examples for some 3D models using the different options implemented in the GNG3D tool.	3d modeling;algorithm;approximation;artificial neural network;cluster analysis;mathematical optimization;neural gas;programming tool;vertex (graph theory)	Rafael Álvarez;Jose Noguera;Leandro Tortosa;Antonio Zamora	2006	The 2006 IEEE International Joint Conference on Neural Network Proceedings	10.1109/IJCNN.2006.246923	iterative reconstruction;mesh generation;multi-swarm optimization;meta-optimization;computer science;theoretical computer science;machine learning;data mining;continuous optimization;artificial neural network;approximation theory	Robotics	5.657889892302615	-34.43844864823987	110779
d70dda61776b57a58698324a035eb0b2a78d86d4	a method for organizing knowledge bases in the hierarchical form	unsupervised learning;organizing lattices humans learning systems classification tree analysis sparse matrices;theoretical model;top down;set theory;hierarchical classification;object classification knowledge base organisation hierarchical form osham unsupervised concept formation method set theoretic model conceptual hierarchies human approach hierarchical classification;knowledge based systems;knowledge engineering unsupervised learning knowledge based systems set theory;concept formation;knowledge base;knowledge engineering	In this paper we propose OSHAM, a novel topdown, unsupervised co,ncept f o r m a t i o n method based o n a set-theoretic model f o r conceptual hierarchies. T h e method i s close t o the h u m a n approach to hierarchical classification and empirical results state i t s high ability t o correctly classify new objects.	organizing (structure);set theory	Tu Bao Ho	1994		10.1109/TAI.1994.346394	unsupervised learning;knowledge base;computer science;artificial intelligence;knowledge-based systems;machine learning;knowledge engineering;pattern recognition;top-down and bottom-up design;data mining;set theory;conceptual clustering	Robotics	-2.777599961981043	-31.41863691697373	110981
241a26e7cfd885ac6916433d1179d39e3601e91a	fuzzy systems of logical inference and their applications		The approaches to the solution of various problems of artificial intelligence methods are proposed. In particular, the problem of knowledge representation by means of fuzzy specifications in expert systems, the problem of recognizing the structures of the proteins of different organization levels and the problem of building linguistic models in fuzzy Boolean variables logic are considered. All methods are based on the ideas of inductive mathematics. To investigate a reliability of these methods is possible only with the help of the theory of probability or possibility theory.	artificial intelligence;expert system;fuzzy concept;fuzzy control system;knowledge representation and reasoning;possibility theory	Oleksandr Provotar	2015			opportunistic reasoning;fuzzy set operations;fuzzy control system;machine learning;fuzzy logic;adaptive neuro fuzzy inference system;fuzzy electronics;possibility theory;artificial intelligence;computer science;fuzzy classification	AI	1.6536697575623511	-25.518370284292594	111083
f45667d70cdc8ccca15e63a05cb3f574cb3bee76	tree-growth based sequential and associative pattern discovery	sequential pattern;association;pattern discovery;pattern mining;domain knowledge	Mining for frequent patterns is an active research area in which numerous algorithms have been proposed to discover different types of patterns based on associative and sequential structures. However, almost all methods are designed to discover a single type of pattern rather than a user selected combination. Within this paper, a novel, flexible algorithm called S&AD is presented, which is capable of discovering a multitude of inter-field and inter-record patterns. In addition the discovery of sequential and associative patterns is supported where the methods proposed for discovery only consist of a recognition phase, thus avoiding the time expensive task of candidate generation. The structure further allows the incorporation of user-driven constraints and domain knowledge at different levels of the discovery process.	algorithm;extensibility;process patterns;radiation pattern;tree structure	Matthias Baumgarten;Alex G. Büchner;John G. Hughes	2003			business process discovery;data mining;domain knowledge;computer science;k-optimal pattern discovery;associative property;machine learning;artificial intelligence;pattern recognition	ML	-2.5226333063907633	-35.62713067148257	111565
651a0f3f7174924bc11017774474b3208027a4d3	disjunctive learning with a soft-clustering method	analyse amas;algorithme glouton;search space;hypothese;apprentissage conceptuel;inductive logic programming;intelligence artificielle;classification;hipotesis;aprendizaje conceptual;cluster analysis;learning by example;learning methods;clustering method;logique ordre 1;recouvrement ensemble;concept learning;greedy algorithm;artificial intelligence;algoritmo gloton;analisis cluster;inteligencia artificial;set covering;hypothesis;cubierta conjunto;similarity measure;clasificacion;programmation logique inductive;first order logic;apprentissage a partir d exemple;logica orden 1	In the case of concept learning from positive and negative examples, it is rarely possible to find a unique discriminating conjunctive rule; in most cases, a disjunctive description is needed. This problem, known as disjunctive learning, is mainly solved by greedy methods, iteratively adding rules until all positive examples are covered. Each rule is determined by discriminating properties, where the discriminating power is computed from the learning set. Each rule defines a subconcept of concept to be learned with these methods. The final set of sub-concepts is then highly dependent from both the learning set and the learning method. In this paper, we propose a different strategy: we first build clusters of similar examples thus defining subconcepts, and then we characterize each cluster by a unique conjunctive definition. The clustering method relies on a similarity measure designed for examples described in first order logic. The main particularity of our clustering method is to build “soft clusters”, i.e. allowing some objects to belong to different groups. Once clusters have been built, we learn first-order rules defining the clusters, using a general-to-specific method: each step consists in adding a literal that covers all examples of a group and rejects as many negative examples as possible. This strategy limits some drawbacks of greedy algorithms and induces a strong reduction of the hypothesis space: for each group (subconcept), the search space is reduced to the set of rules that cover all the examples of the group and reject the negative examples of the concept.	cluster analysis;computer cluster;concept learning;disjunctive normal form;experiment;first-order logic;first-order predicate;greedy algorithm;literal (mathematical logic);numerical analysis;recursion;similarity measure;symbolic-numeric computation	Guillaume Cleuziou;Lionel Martin;Christel Vrain	2003		10.1007/978-3-540-39917-9_7	greedy algorithm;hypothesis;concept learning;biological classification;computer science;artificial intelligence;machine learning;first-order logic;mathematics;cluster analysis;unique negative dimension;algorithm	AI	8.246729332592084	-32.09750398132039	111615
41c48e13a7b09d2a033e9161e0802c513e4ee2ea	fuzzy health, illness, and disease sadegh-zadeh's framework and a program to identify diseases	patient diagnosis;computer program;decision support;fuzzy disease;fuzzy set theory;medical computing;sadegh zadeh framework;diseases medical diagnostic imaging fuzzy sets fuzzy set theory medical diagnosis statistics biomedical informatics set theory humans electrical engineering;decision support systems;fuzzy health;diseases;fuzzy illness;medical diagnosis;medical diagnosis sadegh zadeh framework fuzzy health fuzzy illness fuzzy disease decision support;patient diagnosis decision support systems diseases fuzzy set theory medical computing	"""Dealing with notions of health, illness and disease contains dealing with fuzziness. As the paper will demonstrate, states of these notions do not only exist or not exist. The medical philosopher and physician Sadegh-Zadeh introduced the notions of fuzzy health, fuzzy illness and fuzzy disease. A closer look will be taken on the concept of fuzzy disease. Because there are different possibilities to interpret the concept of disease, amongst others by linguistic and social backgrounds, Sadegh Zadeh indroduced potential candidates: complex rdquohuman conditions"""". This notion can be taken as pre-stage of decision support in medical diagnosis. For demonstration, a computer program has been implemented and its contents are summarised."""	computer program;decision support system;fuzzy concept	Julia Limberg;Rudolf Seising	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295467	decision support system;computer science;artificial intelligence;medical diagnosis;data mining;mathematics;management science;fuzzy set	SE	2.0948664429792414	-26.86917901321262	111863
3761af0288153b02d0282905a3a2dea98cb5fdfb	a self learning rough fuzzy neural network classifier for mining temporal patterns	fuzzy neural network;neural networks;temporal patterns;fuzzy rules;rough set theory;lower approximations;decision support system;fuzzy rule base;temporal pattern;fuzzy rough sets;rough set;decision table;neural network	This paper proposes a new approach that integrates neural networks with the fuzzy rough set to build a Rough Fuzzy Neural Network Classifier (RFNNC) in order to mine temporal patterns in clinical databases. The lower approximation hypothesis and fuzzy decision table with the fuzzy features are used to acquire the fuzzy decision classes for deciding on the attributes. By contemplating a subset of attributes, comprising of the temporal intervals, the lower approximations are devised in this work. Moreover the basic sets are attained from lower approximations are sorted into the decision classes. The discernibility of the decision classes is designed to delineate the temporal consistency degree between the objects of the sets, from which the reducts are acquired. Next, the attribute subset from the reducts is used for training the fuzzy neural network to infer fuzzy rules. The induced rules will result with temporal patterns for classification. The fuzzy neural network has completely used the competence of fuzzy rough set theory to condense huge quantity of superfluous data. The effectiveness of this method is compared with other classifiers such as fuzzy rule based classifier to evaluate the accuracy of the proposed fuzzy neural network classifier. Experiments have been performed on the diabetic dataset and the simulation results induced proves that the proposed fuzzy neural network classifier on medical diabetic dataset stays as a corroboration for predicting the severity of the disease and exactness in decision support system.	approximation;artificial neural network;database;decision support system;decision table;fuzzy rule;fuzzy set;neuro-fuzzy;rough set;set theory;simulation	R. Sethukkarasi;U. Keerthika;Arputharaj Kannan	2012		10.1145/2345396.2345415	fuzzy logic;rough set;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network;dominance-based rough set approach	ML	3.3339431649601834	-28.881108970160305	111995
91ad9dc8175f5747f98ba6fa046fba551c96571e	a simple procedure for pattern pre-recognition based on fuzzy logic analysis	fuzzy logic;pattern recognition	In this paper a simple procedure for pattern pre-recognition is presented. The task is to recognize whether a given pattern should be interesting for further analysis or not. The description of the procedure presented in this paper is based on formal theory of fuzzy logic in narrow sense so that the procedure – being general – can be used for many other purposes where patterns are to be recognized.	fuzzy logic	Vilém Novák;Alessandro Zorat;Mario Fedrizzi	1997	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S021848859700004X	fuzzy logic;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;fuzzy control language;algorithm	Robotics	0.30985986588043424	-25.759357958283257	112119
0851876dc78da291b572e901a9379a62cb3d04bb	a comparison of case-based reasoning approaches	web hypermedia metrics;case base reasoning;numerical technique;linear regression;stepwise regression;euclidean distance;software engineering;effort estimation;machine learning;prediction accuracy;web effort prediction;prediction model;case based reasoning;web hypermedia;prediction models	Over the years software engineering researchers have suggested numerous techniques for estimating development effort. These techniques have been classified mainly as algorithmic, machine learning and expert judgement. Several studies have compared the prediction accuracy of those techniques, with emphasis placed on linear regression, stepwise regression, and Case-based Reasoning (CBR). To date no converging results have been obtained and we believe they may be influenced by the use of the same CBR configuration.The objective of this paper is twofold. First, to describe the application of case-based reasoning for estimating the effort for developing Web hypermedia applications. Second, comparing the prediction accuracy of different CBR configurations, using two Web hypermedia datasets.Results show that for both datasets the best estimations were obtained with weighted Euclidean distance, using either one analogy (dataset 1) or 3 analogies (dataset 2). We suggest therefore that case-based reasoning is a candidate technique for effort estimation and, with the aid of an automated environment, can be applied to Web hypermedia development effort prediction.	algorithm;case-based reasoning;cost estimation in software engineering;euclidean distance;hypermedia;machine learning;stepwise regression	Emilia Mendes;Nile Mosley;Ian D. Watson	2002		10.1145/511446.511482	computer science;data science;machine learning;data mining;database;predictive modelling	Web+IR	3.7556593178677042	-33.79498904484488	112175
9458d0c6c3517b3b14d89d0edf27778ee1ded9c0	online forecasting of stock market movement direction using the improved incremental algorithm	modelizacion;forecasting;prevision;learning algorithm;stock market;modele agrege;supervised learning;bolsa valores;multilayer perceptrons;echantillonnage;apprentissage conceptuel;modelo agregado;intelligence artificielle;algorithme apprentissage;classification;stock markets;bourse valeurs;stock exchange;sampling;perceptron multicouche;modelisation;marche valeurs;aprendizaje conceptual;red multinivel;incremental learning;predictability;concept learning;aggregate model;artificial intelligence;multi layer perceptron;predictabilidad;incremental algorithm;inteligencia artificial;multilayer network;apprentissage supervise;reseau multicouche;reseau neuronal;muestreo;aprendizaje supervisado;predictabilite;algoritmo aprendizaje;modeling;clasificacion;red neuronal;probably approximately correct;neural network	In this paper we present a particular implementation of the Learn++ algorithm: we investigate the predictability of financial movement direction with Learn++ by forecasting the daily movement direction of the Dow Jones. The Learn++ algorithm is derived from the Adaboost algorithm, which is denominated by sub-sampling. The goal of concept learning, according to the probably approximately correct weak model, is to generate a description of another function, called the hypothesis, which is close to the concept, by using a set of examples. The hypothesis which is derived from weak learning is boosted to provide a better composite hypothesis in generalizing the establishment of the final classification boundary. The framework is implemented using multi-layer Perceptron (MLP) as a weak Learner. First, a weak learning algorithm, which tries to learn a class concept with a single input Perceptron, is established. The Learn++ algorithm is then applied to improve the weak MLP learning capacity and introduces the concept of online incremental learning. The proposed framework is able to adapt as new data are introduced and is able to classify.	adaboost;algorithm;concept learning;memory-level parallelism;perceptron;probably approximately correct learning;sampling (signal processing)	Dalton Lunga;Tshilidzi Marwala	2006		10.1007/11893295_49	sampling;stock exchange;simulation;systems modeling;concept learning;predictability;forecasting;biological classification;computer science;artificial intelligence;machine learning;supervised learning;multilayer perceptron;probably approximately correct learning;artificial neural network;algorithm	ML	9.505684192999192	-32.4091975747803	112721
3192b11f09a6edc752f6e20e72d17961d2a2e505	a decision support model used in teaching estimation system for bachelor course based on rst		To build a decision support model used in teaching estimation system(TES) from the viewpoint of knowledge engineering, this paper uses the rough set theory(RST) to discover useful rules from the big data of course materials (books, multi-medias, videos, documents etc.), bachelor student information and responses, teacher information stored in a TES. A heuristic clustering model based on self-organizing mapping is used to map the course references, student and teacher characteristics into conditional attributes, and map student score characteristics into decision attribute respectively. Then both the conditional attributes and decision attributes are discretized into symbolic numbers. Further, a novel decision support model based on rough set theory is built. This model is used for supporting decisions during teaching estimation in the example of bachelor course. Thereby this paper has the important theoretical value and application significance to support the scientific decision used in teaching estimation system for bachelor courses.	big data;book;cluster analysis;decision support system;discretization;heuristic;intel matrix raid;knowledge engineering;organizing (structure);rough set;self-organization;self-organizing map;set theory	Rongyong Zhao;Cuiling Li;Xiangke Tian;Dong Wang;Qianshan Hu;Qin Zhang;Xiaojuan Luo	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393144	knowledge engineering;big data;machine learning;artificial intelligence;cluster analysis;bachelor;decision support system;data modeling;computer science;heuristic;rough set	DB	2.442131888926707	-32.9432150884517	112723
08b36acc65260b7d9c29bc943cceb0eaa8449c03	leader-independent nonparametric consistent algorithms for incremental learning	base donnee;learning;informacion incompleta;database;base dato;base connaissance;intelligence artificielle;proceso adquisicion;acquisition process;algorithme;aprendizaje;algorithm;incomplete information;apprentissage;incremental learning;information incomplete;artificial intelligence;base conocimiento;inteligencia artificial;processus acquisition;algoritmo;knowledge base	Two algorithms useful for incremental machine learning are presented. The first algorithm is based on distribution-free, minimum-error-rate consistent estimators of discrimination functions. The second involves a Bayes-risk-consistent classification criterion with vague and Dirichlet priors incorporating expert-level knowledge heuristics. These algorithms are also extended to the situations with incomplete information. Application to a real medical data base is given as an example. Comparison with existing algorithms and the importance of the present contribution are discussed.	algorithm;database;heuristic (computer science);machine learning;vagueness	Rajani R. Joshi	1991	Inf. Sci.	10.1016/0020-0255(91)90046-W	knowledge base;computer science;artificial intelligence;machine learning;data mining;complete information	ML	7.855148529906637	-31.218959966481254	112735
563064b7377c966f4764fc72c68a447160df56e1	detecting anomalies in backbone network traffic: a performance comparison among several change detection methods	reversible sketch;backbone network traffic;heavy hitters;multi chart non parametric cusum algorithm;anomaly detection;heavy changes;forecasting algorithms	In the last years, the ever increasing number of network attacks has brought the research attention to the design and development of effective anomaly detection systems. To this aim, the main target is to develop efficient algorithms able to detect abrupt changes in the data, with the smallest detection delay. In this paper, we present a novel method for network anomaly detection, based on the idea of discovering heavy change (HC) in the distribution of the Heavy Hitters in the network traffic, by applying several forecasting algorithms. To assess the validity of the proposed method, we have performed an experimental evaluation phase, during which our system performance have been compared to more 'classical' approaches, such as a standard HC method and the promising CUSUM method. The performance analysis, presented in this paper, demonstrates the effectiveness of the proposed method, showing how it is able to outperform the 'classical' approaches.		Christian Callegari;Stefano Giordano;Michele Pagano;Teresa Pepe	2012	IJSNet	10.1504/IJSNET.2012.047149	anomaly detection;simulation;computer science;machine learning;data mining;computer security	Metrics	1.3709026639882929	-32.25608060707956	112993
81f273f1c8a15d5739b65f82c26ac10d135933c0	enhancing principal direction divisive clustering	dimensionalidad;cluster algorithm;metodo estadistico;analisis componente principal;estimacion densidad;methode noyau;kernel density estimation;estimation densite;dimensionality;statistical method;algorithme;data clustering;algorithm;density estimation;hierarchical classification;clustering;methode statistique;dimensionnalite;principal component analysis;metodo nucleo;signal classification;kernel density estimate;number of clusters;analyse composante principale;classification hierarchique;classification signal;kernel method;classification automatique;automatic classification;clasificacion automatica;clasificacion jerarquizada;algoritmo;principal component	While data clustering has a long history and a large amount of research has been devoted to the development of numerous clustering techniques, significant challenges still remain. One of the most important of them is associated with high data dimensionality. A particular class of clustering algorithms has been very successful in dealing with such datasets, utilising information driven by the principal component analysis. In this work, we try to deepen our understanding on what can be achieved by this kind of approaches. We attempt to theoretically discover the relationship between true clusters in the data and the distribution of their projection onto the principal components. Based on such findings, we propose appropriate criteria for the various steps involved in hierarchical divisive clustering and develop compilations of them into new algorithms. The proposed algorithms require minimal user-defined parameters and have the desirable feature of being able to provide approximations for the number of clusters present in the data. The experimental results indicate that the proposed techniques are effective in simulated as well as real data scenarios.	cluster analysis;hierarchical clustering	Sotiris K. Tasoulis;Dimitris K. Tasoulis;Vassilis P. Plagianakos	2010	Pattern Recognition	10.1016/j.patcog.2010.05.025	kernel density estimation;constrained clustering;econometrics;fuzzy clustering;computer science;machine learning;pattern recognition;mathematics;cluster analysis;brown clustering;statistics;principal component analysis;clustering high-dimensional data	Vision	8.943306486582081	-34.82076405327756	113064
265847c7bde1ab22e58e587aa173172170559953	takagi-sugeno-kang type collaborative fuzzy rule based system	system modeling;training testing engines computational modeling prototypes;prototypes;training;big data takagi sugeno kang type collaborative fuzzy rule based system tsk type collaborative fuzzy rule based system knowledge learning ability collaborative fuzzy clustering cfc collaborative mechanism;system modeling prediction and identification problem fuzzy c means fcm collaborative mechanism big data;testing;computational modeling;engines;prediction and identification problem;big data;collaborative mechanism;proceedings paper;pattern clustering big data fuzzy set theory knowledge based systems learning artificial intelligence;fuzzy c means fcm	In this paper, a Takagi-Sugeno-Kang (TSK) type collaborative fuzzy rule based system is proposed with the help of knowledge learning ability of collaborative fuzzy clustering (CFC). The proposed method split a huge dataset into several small datasets and applying collaborative mechanism to interact each other and this process could be helpful to solve the big data issue. The proposed method applies the collective knowledge of CFC as input variables and the consequent part is a linear combination of the input variables. Through the intensive experimental tests on prediction problem, the performance of the proposed method is as higher as other methods. The proposed method only uses one half information of given dataset for training process and provide an accurate modeling platform while other methods use whole information of given dataset for training.	big data;cluster analysis;computation;fuzzy clustering;fuzzy rule;inference engine;mathematical model;rule-based system;time complexity	Kuang-Pen Chou;Mukesh Prasad;Y. Y. Lin;S. Joshi;Chin-Teng Lin;Jamie Yi-Ting Chang	2014	2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)	10.1109/CIDM.2014.7008684	systems modeling;big data;fuzzy classification;computer science;artificial intelligence;machine learning;data mining;prototype;software testing;computational model;fuzzy set operations	AI	4.677308885356774	-28.584580851266292	113255
0c79e053f1129a7212a3d7d1993bb143b78a475c	a weighted belief entropy-based uncertainty measure for multi-sensor data fusion	deng entropy;uncertainty measure;weighted belief entropy;sensor data fusion;dempster shafer evidence theory	In real applications, how to measure the uncertain degree of sensor reports before applying sensor data fusion is a big challenge. In this paper, in the frame of Dempster-Shafer evidence theory, a weighted belief entropy based on Deng entropy is proposed to quantify the uncertainty of uncertain information. The weight of the proposed belief entropy is based on the relative scale of a proposition with regard to the frame of discernment (FOD). Compared with some other uncertainty measures in Dempster-Shafer framework, the new measure focuses on the uncertain information represented by not only the mass function, but also the scale of the FOD, which means less information loss in information processing. After that, a new multi-sensor data fusion approach based on the weighted belief entropy is proposed. The rationality and superiority of the new multi-sensor data fusion method is verified according to an experiment on artificial data and an application on fault diagnosis of a motor rotor.	benign occipital epilepsy;information processing;numerical analysis;numerous;r.o.t.o.r.;rationality;rényi entropy;state space	Yongchuan Tang;Deyun Zhou;Shuai Xu;Zichang He	2017		10.3390/s17040928	joint entropy;dempster–shafer theory;artificial intelligence;pattern recognition;data mining	Robotics	-0.5537227302901107	-27.58718865736015	113435
9a7d0a8f1ecb1530adcbb55b3310a4f9600b8227	partition validity and defuzzification	probabilite appartenance;analyse amas;validite amas;algorithme partition;amas hyper ellisoidal;c means;similarity relation;matrice covariance;matriz covariancia;validite;fuzzy minimals;algorithme;algorithm;cluster analysis;validity;particion;validez;mesure probabilite;fonction appartenance;partition;membership function;membership probability;analisis cluster;hyperellipsoidal clustering;partion algorithm;sistema difuso;cluster validity;systeme flou;funcion pertenencia;hyperellipsoidal cluster;probability measure;medida probabilidad;fuzzy system;covariance matrix;algoritmo	The fuzzy partition validity is a necessary measure to fix the number of groups in the fuzzy algorithms which need to know this data. We propose a measure based on the membership probabilities. Moreover, we propose a new concept of the hard partition based on the metric of the groups. The hard partition can be obtained from the fuzzy partition, from which it is possible to find the hard membership probabilities and the hard partition validity. A similarity relation is proposed over the set of groups belonging to the hard partition.	defuzzification	Antonio Flores-Sintas;José Manuel Cadenas;Fernando Martin	2000	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00004-9	partition;covariance matrix;combinatorics;discrete mathematics;partition refinement;membership function;probability measure;computer science;graph partition;calculus;mathematics;cluster analysis;fuzzy control system;validity;statistics	Logic	7.736714634983709	-34.40806251750584	113533
0437bbd34c548858d3767a174c26b9d05403cc98	classification by means of fuzzy analogy-related proportions — a preliminary report	reverse analogy;fuzzy analogy related proportions;unsolicited electronic mail;boolean functions;testing;classification;fuzzy logic;accuracy;machine learning;blood;pattern classification boolean functions fuzzy logic learning artificial intelligence multivalued logic;analogy;classification method;pattern classification;machine learning benchmarks fuzzy analogy related proportions boolean logic multiple valued logic formal proportions reverse analogy paralogy classification method;machine learning benchmarks;classification analogy multi valued;learning artificial intelligence;iris;multivalued logic;boolean logic;paralogy;multi valued;conference proceeding;accuracy blood unsolicited electronic mail iris testing equations;formal proportions;multiple valued logic	Boolean logic interpretations, as well as multiple-valued logic extensions, have been recently proposed for analogical proportions (i.e. statements of the form “a is to b as c is to d”), and for two other related formal proportions named reverse analogy (“what a is to b is the reverse of what c is to d”), and paralogy (“what a and b have in common c and d have it also”). These proportions relate items a, b, c, and d on the basis of their differences, or of their similarities. This may provide a basis for proposing a plausible classification for an object d described in terms of a set of features, on the basis of three other already classified objects described on the same features, considering that if some proportion holds for a sufficiently large number of features, it may hold on the allocation of the classes as well. This is the basis of a classification method which is tested on machine learning benchmarks for binary or multiple class problems with objects that have numerical features.	bitwise operation;boolean algebra;experiment;homology (biology);level of measurement;logical connective;machine learning;mathematical optimization;numerical analysis;statistical classification	Henri Prade;Gilles Richard;Bing Yao	2010	2010 International Conference of Soft Computing and Pattern Recognition	10.1109/SOCPAR.2010.5686636	fuzzy logic;boolean algebra;analogy;biological classification;computer science;artificial intelligence;machine learning;mathematics;accuracy and precision;software testing;boolean function;algorithm;statistics	SE	1.425703766435583	-29.041690352366953	113636
2d59c76f5d93ac1a3630c9746d34b48146025155	indexing possibilistic numerical data: the interval b ^+ -tree approach	technology and engineering			Guy De Tré;Robin De Mol;Antoon Bronselaer	2016		10.1007/978-3-319-40581-0_25	computer science;machine learning;data mining;database	Robotics	1.42057839447741	-25.61801329760199	113968
284fd59c309d98151f9d70efa557d5051b0426d0	equipment's prognostics using logical analysis of data		This paper demonstrates the implementation of Logical Analysis of Data (LAD) methodology in the field of prognostics in Condition Based Main- tenance (CBM). In this paper the LAD classification methodology, based on Sensitive Discriminating and Equipartitioning methods for data binarization, Mixed Integer Linear Programming (MILP) and Hybrid Greedy methods for pattern generation, is used. Using the generated patterns, two methods of calcu- lating the survival function are introduced. The methodology is applied on Prognostics and Health Management Challenge dataset, which is a condition monitoring dataset provided by NASA Ames Prognostics Data Repository. The results obtained by using LAD methodology, are compared with that obtained by using the Proportional Hazards Model (PHM).		Alireza Ghasemi;Sasan Esmaeili;Soumaya Yacout	2012		10.1007/978-3-642-40361-3_31	reliability engineering;engineering;data science;data mining	NLP	2.015807128267217	-32.158434320711216	114097
a6f18ca6730dd34c8d4c3a0a08a59e5ab278767d	an improved edp algorithm to privacy protection in data mining	complexity;privacy protection;decision tree pruning	In this paper, we propose an improved pruning algorithm with memory, which we call improved EDP algorithm. This method provides the better trade-off between data quality and privacy protection against classification attacks. The proposed algorithm reduces the time complexity degree significantly, especially in the case of the complete binary tree of which worst-case time complexity is of order O(M logM), where M is the number of internal nodes of the complete tree. The experiments also show that the proposed algorithm is feasible and more efficient especially in the case of large and more complex tree structure with more internal nodes, etc. From a practical point of view, the improved EDP algorithm is more applicable and easy to implement.	algorithm;data mining;electronic data processing	Mingzheng Wang;Na Ge	2011		10.1007/978-3-642-23605-1_27	complexity;computer science;theoretical computer science;machine learning;incremental decision tree;data mining;pruning	ML	-3.855329631384513	-36.70593848627738	114203
053eebef9eda57907d7afdd57943ff47d406e1ac	speeding-up pittsburgh learning classifier systems: modeling time and accuracy	genetique;modelizacion;parallelisme;evaluation performance;learning algorithm;performance evaluation;genetica;evaluacion prestacion;learning classifier system;intelligence artificielle;algorithme apprentissage;classification;genetics;resolucion problema;modelisation;parallelism;paralelismo;biomimetique;artificial intelligence;genetics based machine learning;inteligencia artificial;algoritmo aprendizaje;modeling;learning strategies;clasificacion;problem solving;resolution probleme;biomimetics	Windowing methods are useful techniques to reduce the computational cost of Pittsburghstyle genetic-based machine learning techniques. If used properly, they additionally can be used to improve the classification accuracy of the system. In this paper we develop a theoretical framework for a windowing scheme called ILAS, developed previously by the authors. The framework allows us to approximate the degree of windowing we can apply to a given dataset as well as the gain in run-time. The framework sets the first stage for the development of a larger methodology with several types of learning strategies in which we can apply ILAS, such as maximizing the learning performance of the system, or achieving the maximum run-time reduction without significant accuracy loss.	algorithmic efficiency;approximation algorithm;machine learning	Jaume Bacardit;David E. Goldberg;Martin V. Butz;Xavier Llorà;Josep Maria Garrell i Guiu	2004		10.1007/978-3-540-30217-9_103	biomimetics;systems modeling;biological classification;computer science;artificial intelligence;machine learning;learning classifier system;algorithm	ML	9.4916936993681	-31.613851668660168	114591
80d331a22696465f008970bb2146e0dd3128368f	a cognitive data stream mining technique for context-aware iot systems		IoT systems deployed in industrial and smart factory settings generate large volumes of data at high velocity. Context awareness is mandatory for knowledge discovery and actionable insights from such high-velocity, high-volume IoT data streams. Changes to the context of a data stream are represented in the underlying data distribution. Research in concept drift aims to detect and adapt to such changes in a data distribution. Concept drift detection can be extended to suit ad hoc Big Data streams generated by IoT systems, by introducing the cognitive principles of learning. This paper proposes an unsupervised incremental learning algorithm for detection and adaption of concept drift based on the cognitive principles of learning. It executes in automated time windows, detects concept drift using movement in space and determines the type of concept drift using movement in time. The algorithm was applied to a Big Data set representing an IoT system for urban vehicular movement and traffic. Results confirm that the proposed algorithm generates context-awareness by detection and adaptation to concept drift in high-volume, high-velocity IoT systems.	acclimatization;algorithm;big data;concept drift;context awareness;data stream mining;hoc (programming language);increment;industry 4.0;mandatory - hl7definedroseproperty;microsoft windows;velocity (software development)	Dinithi Nallaperuma;Daswin De Silva;Damminda Alahakoon;Xinghuo Yu	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8216824	streams;concept drift;knowledge extraction;control engineering;engineering;big data;cluster analysis;data stream mining;real-time computing;unsupervised learning;context awareness	Robotics	-0.16893330972825762	-35.658640162643636	114722
4fad82908b0fdecb93a8b2fec86daabada9d495f	two algorithms for generating structured and unstructured monotone ordinal data sets	monotone decision trees;decision tree;monotone classification;silicon wafer;ordinal data;artificial data;data structure;ordinal classification;structured data	Monotone constraints are very common while dealing with multi-attribute ordinal problems. Grinding wheels hardness selection, timely replacements of costly laser sensors in silicon wafer manufacturing, and the selection of the right personnel for sensitive production facilities, are just a few examples of ordinal problems where monotonicity makes sense. In order to evaluate the performance of various ordinal classifiers one needs both artificially generated as well as real world data sets. Two algorithms are presented for generating monotone ordinal data sets. The first can be used for generating random monotone ordinal data sets without an underlying structure. The second algorithm, which is the main contribution of this paper, describes for the first time how structured monotone data sets can be generated.	algorithm;ordinal data;monotone	Rob Potharst;Arie Ben-David;Michiel C. van Wezel	2009	Eng. Appl. of AI	10.1016/j.engappai.2009.02.004	ordinal regression;ordinal data type;data structure;data model;computer science;machine learning;decision tree;data mining;ordinal optimization;ordinal data;wafer	AI	-0.7403769890342222	-29.070986343521188	115462
310de14f384f3b2f1f41703c679d33aeb429ae96	fault diagnosis of power systems based on triangular fuzzy spiking neural p systems		Based on triangular fuzzy spiking neural P systems (TFSNP systems, in short), a fault diagnosis method for power system is presented in this paper. First, triangular fuzzy number (TFN) is integrated into spiking neural P systems (SNP systems, in short) to propose the TFSNP systems. Afterward, modeling and fuzzy reasoning methods based on TFSNP systems are developed. Finally, TFSNP systems are used for fault diagnosis in power system. A fault diagnosis example for ring network of the voltage level with 220 kV is used to demonstrate the availability and effectiveness of the proposed fault diagnosis model.	ibm power systems;p system	Chengyu Tao;Wenping Yu;Jun Wang;Hong Peng;Ke Chen;Jun Ming	2016		10.1007/978-981-10-3611-8_32	snp;fuzzy logic;electric power system;machine learning;artificial intelligence;computer science;ring network;fuzzy number	AI	3.8482283165372992	-24.44297451765408	115584
df8419abaab5201961fab1239c453fdba9ed4c76	predictive models of hard drive failures based on operational data		Hard drives are an essential component of modern data storage. In order to reduce the risk of data loss, hard drive failure prediction methods using the Self-Monitoring, Analysis and Reporting Technology attributes have been proposed. However, these methods were developed from datasets not necessarily representative of operational systems. In this paper, we consider the Backblaze public dataset, a recent operational dataset from over 47,000 drives, exhibiting hard drive heterogeneity with 81 models from 5 manufacturers, an extremely unbalanced ratio of 5000:1 between healthy and failure samples and a realworld loosely controlled environment. We observe that existing predictive models no longer perform sufficiently well on this dataset. We therefore selected machine learning classification methods able to deal with a very unbalanced training set, namely SVM, RF and GBT, and adapted them to the specific constraints of hard drive failure prediction. Our results reach over 95% precision and 67% recall on a one year real-world public dataset of over 12 million records with only 2586 failures.	backblaze;computer data storage;failure;government and binding theory;hard disk drive;machine learning;operational system;predictive modelling;radio frequency;s.m.a.r.t.;test set;unbalanced circuit	Nicolas Aussel;Samuel Jaulin;Guillaume Gandon;Yohan Petetin;Eriza Fazli;Sophie Chabridon	2017	2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2017.00-92	artificial intelligence;support vector machine;machine learning;pattern recognition;computer science;feature extraction;data loss;training set;statistical classification;hidden markov model;computer data storage	SE	1.67464239245737	-33.77881633123679	115639
28d3cb9cfa68597a649a344ef573155a2f07050a	missing or absent? a question in cost-sensitive decision tree	decision tree;induction;machine learning;knowledge acquisition;conference proceeding	One common source of error in data is the existence of missing value fields. Imputation method has been a widely used technique in preprocessing phase of data mining, in which missing values are replaced by some estimated values. Previous work is trying to seek the “original” values according to specific criteria, such as statistics measure. However, in domain of cost-sensitive learning, minimal overall cost is the most important issue, i.e. a value which can minimize total cost is prefer than the “best” value upon common sense. For example, in medical domains, some data fields usually are left as absent and known information is enough for a decision. In this paper, we proposed a new method to study the problem of “missing or absent values?” in the domain cost-sensitive learning. Experiment results show some improvements with distinguished missing and absent data in cost-sensitive decision tree.	decision tree	Zhenxing Qin;Shichao Zhang;Chengqi Zhang	2006			missing data;decision tree learning;computer science;artificial intelligence;machine learning;decision tree;incremental decision tree;data mining;imputation;statistics	ML	-1.5461811097487739	-31.813902471570135	115888
b38a9cb014d7ab6a8938294d346167f420fe5ce7	mutually-inversistic logic with uncertainty	uncertainty;equi_strength_of_support line;uncertainty fuzzy logic bayesian methods fuzzy systems information technology explosions;information technology;training;bayesian methods;fuzzy logic;vagueness;roads;vagueness mutually inversistic logic uncertainty equi_strength_of_support line fuzzy logic;mutually inversistic logic;uncertainty mutually inversistic logic fuzzy logic	"""In mutually-inversistic logic, T denotes truth, F denotes falsity, n denotes """"need not determine whether it is true or false"""". In this paper, the truth values of mutually-inversistic logic are augmented in order to deal with uncertainty. Real numbers in the closed interval [0, 1] are used to represent the degrees of truth of propositions, 1 represents absolute truth, 0 absolute falsity, 0.5 """"need not determine whether it is true or false"""". So, mutually-inversistic logic is a special case of mutually-inversistic logic with uncertainty. The diagram of equi_strength_ofsupport line is used to establish the truth value of fuzzy mutually inverse implicational propositions. In fuzzy logic, is defined as min, v as max, they are insensitive to the changes in parameters. Mutually-inversistic logic with uncertainty is sensitive to the changes in parameters. Fuzzy logic deals with vagueness with subjective method, mutually-inversistic logic with uncertainty deals with vagueness with objective method."""	degree of truth;diagram;fuzzy logic;logic gate;maxima and minima;vagueness	Xunwei Zhou	2008	2008 Fifth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2008.82	predicate logic;fuzzy logic;zeroth-order logic;t-norm fuzzy logics;higher-order logic;uncertainty;bayesian probability;many-valued logic;intuitionistic logic;computer science;intermediate logic;artificial intelligence;fuzzy number;machine learning;mathematics;truth value;information technology;probabilistic logic network;algorithm;statistics;subjective logic	AI	-0.5418323893944735	-24.543241270414462	116102
6e79b73ddc01ea097aadbc3ff1445e0fa8a9e4e8	scaling up support vector machines using nearest neighbor condensation	estensibilidad;tiempo respuesta;reponse temporelle;nearest neighbor searches;quadratic programming;nearest neighbor rule;analisis estadistico;nearest neighbor condensation support vector machines fcnn svm classifier condensation classification rule;support vector machines;support vector machines nearest neighbor searches support vector machine classification quadratic programming management training training data multidimensional systems testing delay pattern classification;fcnn svm classifier;base donnee tres grande;database;base dato;large data sets;multidimensional data;response time;support vector machines svms classification large data sets training set condensation nearest neighbor rule;testing;classification;support vector;scaling up;temps reponse;training data;classification a vaste marge;vecino mas cercano;statistical analysis;time response;classification rules;nearest neighbor;analyse statistique;nearest neighbor condensation;base de donnees;pattern classification;support vector machines learning artificial intelligence pattern classification;plus proche voisin;support vector machine classification;nearest neighbour;extensibilite;scalability;support vector machine;condensation classification rule;maquina ejemplo soporte;training set condensation;very large databases;vector support machine;learning artificial intelligence;reseau neuronal;respuesta temporal;management training;support vector machines svms;red neuronal;multidimensional systems;neural network	In this brief, we describe the FCNN-SVM classifier, which combines the support vector machine (SVM) approach and the fast nearest neighbor condensation classification rule (FCNN) in order to make SVMs practical on large collections of data. As a main contribution, it is experimentally shown that, on very large and multidimensional data sets, the FCNN-SVM is one or two orders of magnitude faster than SVM, and that the number of support vectors (SVs) is more than halved with respect to SVM. Thus, a drastic reduction of both training and testing time is achieved by using the FCNN-SVM. This result is obtained at the expense of a little loss of accuracy. The FCNN-SVM is proposed as a viable alternative to the standard SVM in applications where a fast response time is a fundamental requirement.	collections (publication);experiment;response time (technology);single linkage cluster analysis;support vector machine	Fabrizio Angiulli;Annabella Astorino	2010	IEEE Transactions on Neural Networks	10.1109/TNN.2009.2039227	support vector machine;computer science;machine learning;pattern recognition;data mining;mathematics;quadratic programming;artificial neural network	ML	9.766653262556167	-35.63800024363695	116139
cf73861b3eba4a3d83abe3130767e6fd1e74216e	qualitative approach to gradient based learning algorithms	learning algorithm;reinforcement learning;backpropagation;qualitative modeling;qualitative reasoning;point of view;neural network	This work is concerned with the establishment of a relation between the fields of qualitative reasoning (QR) and neural networks. We explore how well-known backpropagation learning algorithm can be studied from the point of view of QR Qualitative models are based on the discretization of their parameters and the use of closed operators on the sets induced by the discretization. Henceforth, a qualitative version of backpropagation is an algorithm in which the variables involved in it belong to one of the finite classes defined. We analyse the algorithms resulting from this transformation and test their performance with a set of four problems. The results are encouraging and provide an empirical basis for a deeper, theoretical study, which can be very useful to realize physical implementations of the algorithm or as a starting point for the development of reinforcement learning algorithms.	algorithm;gradient	Bernardo Morcego Seix;Andreu Català;Núria Piera Carreté	1995		10.1007/3-540-59497-3_212	unsupervised learning;instance-based learning;qualitative reasoning;wake-sleep algorithm;computer science;artificial intelligence;backpropagation;machine learning;pattern recognition;leabra;learning classifier system;reinforcement learning;active learning;artificial neural network;generalization error	ML	9.739052191687291	-29.33055526563022	116694
e2696c996dd49149b1f0ff97be4eee7988afeb61	neural computation in paleoclimatology: general methodology and a case study	neural networks;climate reconstruction;paleoclimatology	In this paper we present the general methodology and main issues related to the application of neural networks to paleoclimatic reconstruction problems. We establish the basic methodological framework, data selection, organization and their relation to neural networks' features. We also describe a skill score to compare regressors' performance and finally the paleoclimatic variable's reconstruction. We show a multi-layer perceptrons, and the comparison of the obtained results to that of the existing alternative methodologies. & 2013 Elsevier B.V. All rights reserved.	artificial neural network;computation;multilayer perceptron;relevance	Leopoldo Carro-Calvo;Sancho Salcedo-Sanz;Jürg Luterbacher	2013	Neurocomputing	10.1016/j.neucom.2012.12.045	computer science;artificial intelligence;machine learning;paleoclimatology;operations research;artificial neural network	AI	6.280127931898775	-24.334614156329444	116757
91dd0c726c5d4512ca04fecd49d8930a6041fbed	fuzzy set theoretical analysis of human membership values on the color triangle	fuzzy set;membership value;vague color;fuzzy set theory;color triangle;theoretical analysis;center of gravity;membership function;three additive primary colors;rgb system	The present study considers a fuzzy color system in which three membership functions are constructed on the RGB color triangle. This system can process a fuzzy input (as the membership values of subjects) to an RGB system and output the center of gravity of three weights associated with respective grades. Three membership functions are applied to the RGB color triangle relationship. By treating three membership functions of redness, greenness, and blueness on the RGB color triangle, an average color value can be easily obtained as the center of gravity of the fuzzy output. The differences between fuzzy input and inference output are described, and the relationship between the centers of gravity of fuzzy inputs and inference outputs for fuzzy inputs are shown in the present paper. A technique for obtaining expressions of the RGB color triangle using the fuzzy set theoretical method has been reported [5] and improved [6]. In the previous study, the relationship between input fuzzy sets with a plateau on the RGB triangle and fuzzy inputs of conical membership functions was examined. The RGB color triangle (plane) represents the hue and saturation of a color [8]. The six fundamental colors and white can be represented on the same color triangle (See Fig. 1). Vague colors on the RGB color triangle were clarified. In the present study, the membership value on the RGB triangular system are examined to determine the average color value as the center of gravity of the attribute information of vague colors. This fuzzy set theoretical approach is useful for vague color information processing, color-naming systems, and similar applications.	color;cross industry standard process for data mining;experiment;fuzzy logic;fuzzy set;human–computer interaction;information processing;membership function (mathematics);vagueness;xfig	Naotoshi Sugano;Shou Komatsuzaki;Hiroyuki Ono;Yuko Chiba	2009	JCP	10.4304/jcp.4.7.593-600	color histogram;computer vision;icc profile;color model;hsl and hsv;3d lookup table;membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;mathematics;geometry;fuzzy set;color space	AI	-1.284371807460541	-24.48258473658045	117084
6a77c307c9829946867422b284fb014af0d53e84	fuzzy least squares twin support vector machines		Least Squares Twin Support Vector Machine (LSTSVM) is an extremely efficient and fast version of SVM algorithm for binary classification. LSTSVM combines the idea of Least Squares SVM and Twin SVM in which two non-parallel hyperplanes are found by solving two systems of linear equations. Although the algorithm is very fast and efficient in many classification tasks, it is unable to cope with two features of real-world problems. First, in many real-world classification problems, it is almost impossible to assign data points to a single class. Second, data points in realworld problems may have different importance. In this study, we propose a novel version of LSTSVM based on fuzzy concepts to deal with these two characteristics of real-world data. The algorithm is called Fuzzy LSTSVM (FLSTSVM) which provides more flexibility than the binary classification of LSTSVM. Two models are proposed for the algorithm. In the first model, a fuzzy membership value is assigned to each data point and the hyperplanes are optimized based on these fuzzy samples. In the second model we construct fuzzy hyperJ. Salimi Sartakhti Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran E-mail: javad.salimi@ec.iut.ac.ir N. Ghadiri Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran E-mail: nghadiri@cc.iut.ac.ir H. Afrabandpey Department of Electrical and Computer Engineering, Isfahan University of Technology, Isfahan 84156-83111, Iran E-mail: h.afraei@ec.iut.ac.ir N. Yousefnezhad Department of Computer Engineering, Sharif University of Technology E-mail: yousefnezhad@ce.sharif.edu planes to classify data. Finally, we apply our proposed FLSTSVM to an artificial as well as three real-world datasets. Results demonstrate that FLSTSVM obtains better performance than SVM and LSTSVM.	algorithm;artificial intelligence;binary classification;computer engineering;data point;experiment;fuzzy concept;fuzzy set;image analysis;intrusion detection system;least squares;linear equation;mail (macos);mathematical optimization;multiclass classification;noise reduction;nonlinear system;pattern recognition;real life;support vector machine;system of linear equations;vagueness	Javad Salimi Sartakhti;Nasser Ghadiri;Homayun Afrabandpey	2015	CoRR		machine learning;pattern recognition;data mining;mathematics	ML	7.791562071054302	-37.40998084228375	117154
9751a1154f5aa18b13a7ba19585aac269525ffab	regression learning based on incomplete relationships between attributes		In recent years, machine learning researchers have focused on methods to construct flexible and interpretable regression models. However, the method of obtaining complete knowledge from incomplete and fuzzy prior knowledge and the trade-off between the generalization performance and the interpretability of the model are very important factors to consider. In this paper, we propose a new regression learning method. Complete relationships are obtained from the incomplete fuzzy relationships between attributes by using Markov logic networks. The complete relationships are then applied to constrain the shape of the regression model in the optimization procedure to solve the trade-off problem. Finally, the benefits of our approach are illustrated on benchmark data sets and in real-world experiments.	benchmark (computing);experiment;machine learning;markov chain;markov logic network;mathematical optimization	Jinwei Zhao;Xinhong Hei;Zhenghao Shi;Longlei Dong;Yu Liu;Ruiping Yan;Xiuxiu Li	2018	Inf. Sci.	10.1016/j.ins.2017.09.023	mathematics;machine learning;fuzzy logic;regression analysis;regression;data set;interpretability;artificial intelligence;markov chain;pattern recognition	AI	2.189041158400739	-30.660485822655904	117190
ba3a5ff4702fabb14ffcc37fe6afb92f157121a4	software cost estimation using svr based on immune algorithm	software;machine learning algorithms;optimisation;software cost estimation;machine learning technique;support vector machines;statistical algorithm;immune algorithm;support vector regression;linear regression;memory cells;costs software algorithms immune system machine learning algorithms machine learning software quality software engineering artificial intelligence computer science information systems;estimation;machine learning;linear regression method;immune system;memory cells software cost estimation immune algorithm information system regression analysis statistical algorithm machine learning technique support vector regression linear regression method;software algorithms;regression analysis;support vector machines optimisation regression analysis software cost estimation;information system	Increasing use of information system has led to larger amount of developing expenses and demands on software.Until recent days, the model using regression analysis based on statistical algorithm has been used.However, Machine learning is more being investigated now. This paper estimates the software cost using SVR (Support Vector Regression), a sort of machine learning technique. It, also, finds the best set of parameters applying immune algorithm.In this paper, Software cost estimation is performed by SVR based on immune algorithm while changing populations, memory cells, and the number of allele.Also, this paper analyzes and compares the result with existing linear regression method and other machine learning methods.	algorithm;artificial neural network;genetic programming;information system;machine learning;mathematical optimization;population;processor affinity;software development effort estimation;support vector machine;zero suppression	Joon-kil Lee;Ki-Tae Kwon	2009	2009 10th ACIS International Conference on Software Engineering, Artificial Intelligences, Networking and Parallel/Distributed Computing	10.1109/SNPD.2009.35	support vector machine;computer science;machine learning;pattern recognition;data mining	SE	8.017142302238895	-25.676177030134035	117420
e0382e75658923e449b9dcbae212738f120d2271	a regularized pairwise multi-classification knowledge-based machine and applications	metodo cuadrado menor;modelizacion;iterative method;optimal solution;methode moindre carre;solution optimale;pronostic;regularisation;polyedre;analisis estadistico;base de connaissances;least squares method;cancer;tumor maligno;piping;pipe;poliedro;modele lineaire;linear least square;polyhedron;clasificador;prior knowledge;modelo lineal;caneria;regularization;metodo iterativo;modelisation;classification a vaste marge;methode matricielle;classifier;pronostico;statistical analysis;linear classification;canalizacion;mathematical programming;least squares problem;methode iterative;solucion optima;analyse statistique;linear model;knowledge based model;matrix method;tuyauterie;classificateur;metodo matriz;canalisation;unconstrained optimization;base conocimiento;tumeur maligne;optimizacion sin restriccion;regularizacion;support vector machine;maquina ejemplo soporte;tikhonov regularization;vector support machine;ecoulement diphasique;iteration method;optimisation sans contrainte;discriminacion;flujo difasico;modeling;programmation mathematique;prognosis;breast cancer;programacion matematica;discrimination;knowledge based model linear classification tikhonov regularization;malignant tumor;two phase flow;knowledge base	This paper presents a novel knowledge-based linear classification model for multi-category discrimination of sets or objects with prior knowledge. The prior knowledge is in the form of multiple polyhedral sets belonging to one or more categories or classes and it is introduced as additional constraints into the formulation of the Tikhonov linear least squares multi-class support vector machine model. The resulting formulation leads to a least squares problem that can be solved using matrix methods or iterative methods. Investigations include the development of a linear knowledge-based classification model extended to the case of multi-categorical discrimination and expressed as a single unconstrained optimization problem. Advantages of this formulation include explicit expressions for the classification weights of the classifier(s) and its ability to incorporate and handle prior knowledge directly to the classifiers. In addition it can provide fast solutions to the optimal classification weights for multi-categorical separation without the use of specialized solver-software. To evaluate the model, data and prior knowledge from the Wisconsin breast cancer prognosis and two-phase flow regimes in pipes were used to train and test the proposed formulation.		Olutayo O. Oladunni;Theodore B. Trafalis	2009	European Journal of Operational Research	10.1016/j.ejor.2007.11.024	knowledge base;computer science;artificial intelligence;calculus;mathematics;iterative method;algorithm;cancer	HPC	9.701908657692464	-28.872152077757665	117661
39ba3963b7d3d334a6c6b3c0e4cc61e6d62bdbe2	topological approach to multivalued information system	topology;nondeterministic information systems;rough sets;deterministic information systems	Rough set is a powerful tool for analyzing deterministic information systems. Problems arise when a nondeterministic information system (multivalued information system) is used to make a classification. In this paper, we introduce a new method to solve this problem by using topological bases. Firstly, we define the characteristic sets for multivalued information systems. These characteristic sets are used to generate topological bases which decompose data. We carry out the reduct and the core of this classification by using the topological bases based on these characteristic sets. New pre-topological approximations are introduced and some of their properties are proved. Finally, we define the accuracy measure and the membership function with respect to pre-topological approximations.	approximation;attribute–value pair;information system;rough set	R. Mareay	2014	Theory of Computing Systems	10.1007/s00224-014-9551-y	combinatorics;discrete mathematics;rough set;computer science;mathematics	Theory	-1.5144098178349683	-24.15197351925765	117701
d8b096a823ba77dcc77009faf5b1e462c2440bfd	case base building with similarity relations	case base reasoning;bepress selected works;e commerce;similarity relation;fuzzy similarity relation;partition;case based reasoning;case based reasoning case base similarity relation fuzzy similarity relation partition e commerce;possible worlds;case base	This paper has two main contributions. Firstly, it shows that similarity relations are an adequate means of formalization not only for case retrieval but also for case base building. Secondly, this paper provides a theoretical formalization for building case bases in case-based reasoning and presents three algorithms for case base building. The proposed approach argues that case base building can be based on both similarity relations and fuzzy similarity relations, which are both defined on the possible world of problems and solutions respectively. Thus case base building is a form of similaritybased reasoning. This approach is an extension for the logical and fuzzy approach to case based reasoning. The proposed methods and algorithms can be applied to reduction of case base size. 2003 Elsevier Inc. All rights reserved.	algorithm;case-based reasoning;fuzzy logic;fuzzy set;possible world	Zhaohao Sun;Gavin R. Finnie;Klaus Weber	2004	Inf. Sci.	10.1016/j.ins.2003.09.020	partition;e-commerce;case-based reasoning;discrete mathematics;computer science;artificial intelligence;data mining;mathematics	AI	-3.148362871876877	-24.67839648029813	117925
f9fd2c6f468474a2628027a4f04b3e07b82b736b	a fast iterative rule-based linguistic classifier for hyperspectral remote sensing tasks	pragmatics;input variables;fuzzy rules;rule based;training;hyperspectral satellite image iterative rule based linguistic classifier hyperspectral remote sensing task genetic fuzzy rule based classification system highly dimensional feature space iterative rule learning rule extraction algorithm genetic tuning;hyperspectral sensors;rule extraction;rule learning;feature space;genetics;genetic fuzzy rule based classification system gfrbcs;hyperspectral remote sensing task;highly dimensional feature space;iterative methods;hyperspectral satellite image;fuzzy rule base;iterative rule based linguistic classifier;hyperspectral image classification genetic fuzzy rule based classification system gfrbcs local feature selection remote sensing;local features;feature extraction;remote sensing;classification algorithms;classification system;pattern classification;hyperspectral remote sensing;satellite image;genetic algorithms;genetic tuning;pragmatics input variables training genetics classification algorithms feature extraction hyperspectral sensors;iterative rule learning;computational linguistics;remote sensing computational linguistics fuzzy systems genetic algorithms iterative methods knowledge based systems pattern classification;local feature selection;hyperspectral image classification;rule extraction algorithm;hyperspectral image;genetic fuzzy rule based classification system;fuzzy systems;knowledge based systems	This paper introduces a genetic fuzzy rule-based classification system (GFRBCS), specifically designed to effectively handle highly-dimensional features spaces. The proposed methodology follows the principles of the iterative rule learning (IRL) approach, whereby a rule extraction algorithm (REA) is invoked in an iterative fashion, producing one fuzzy rule at a time. The REA is performed in two successive steps: the first one selects the relevant features of the currently extracted rule, whereas the second one decides the antecedent part of the fuzzy rule, using the previously selected subset of features. The performance of the classifier is finally optimized through a genetic tuning post-processing stage. Comparative results using a hyperspectral satellite image indicate the effectiveness of the proposed methodology in handling highly-dimensional classification problems, compared to other GFRBCSs.	computation;feature selection;fuzzy rule;genetic algorithm;iterative method;logic programming;requirement;rule induction;video post-processing	Dimitris G. Stavrakoudis;Georgia N. Galidaki;Ioannis Z. Gitas;Ioannis B. Theocharis	2011	2011 IEEE 5th International Workshop on Genetic and Evolutionary Fuzzy Systems (GEFS)	10.1109/GEFS.2011.5949501	computer science;machine learning;pattern recognition;data mining	Vision	3.8490919634020826	-30.480004050477355	117943
8ee2eb33b8e2d01cab0489570d749844e70394cf	dominance-based rough set approach for possibilistic information systems	fuzzy rough set;new decision case;fuzzy dominance relation;dominance-based fuzzy rough set;decision rule;possibilistic information system;dominance-based rough set approach;decision class;fuzzy subsets;decision analysis;finite set;dominance principle	In this paper, we propose a dominance-based fuzzy rough set approach for the decision analysis of a preference-ordered possibilistic information systems, which is comprised of a finite set of objects described by a finite set of criteria. The domains of the criteria may have ordinal properties that express preference scales. In the proposed approach, we first compute the degree of dominance between any two objects based on their possibilistic evaluations with respect to each criterion. This results in a fuzzy dominance relation on the universe. Then, we define the degree of adherence to the dominance principle by every pair of objects and the degree of consistency of each object. The consistency degrees of all objects are aggregated to derive the quality of the classification, which we use to define the reducts of an information system. In addition, the upward and downward unions of decision classes are fuzzy subsets of the universe. The lower and upper approximations of the decision classes based on the fuzzy dominance relation are thus fuzzy rough sets. By using the lower approximations of the decision classes, we can derive two types of decision rules that can be applied to new decision cases..	dominance-based rough set approach;information system	Tuan-Fang Fan;Churn-Jung Liau;Duen-Ren Liu	2011		10.1007/978-3-642-21881-1_20	combinatorics;discrete mathematics;machine learning;mathematics;dominance-based rough set approach	Logic	-2.2218659527370423	-25.00286728273366	117990
4e3499f8077812ac36c627b78ebbbba7d64b76a2	discrimination of benign from malignant breast lesions using statistical classifiers	feedforward neural network;classifier combination;analisis estadistico;modele agrege;supervised learning;approximation plus proche voisin;diagnostic accuracy;mammary gland;modelo agregado;useful information;informacion util;intelligence artificielle;probabilistic approach;classification;glandula mamaria;statistical analysis;enfoque probabilista;approche probabiliste;analyse statistique;aggregate model;artificial intelligence;k nearest neighbor;glande mammaire;inteligencia artificial;statistical techniques;apprentissage supervise;reseau neuronal;discriminacion;aprendizaje supervisado;clasificacion;red neuronal;fine needle aspiration;information utile;nearest neighbor approximation;discrimination;neural network	The objective of this study is to investigate the discrimination of benign from malignant breast lesions using: the linear, the feedforward neural network, the k-nearest neighbor and the boosting classifiers. Nuclear morphometric parameters from cytological smears taken by Fine Needle Aspiration (FNA) of the breast, have been measured from 193 patients. These parameters undergo an appropriate transformation and then, the classifiers are performed on the raw and on the transformed data. The results show that in terms of the raw data set all classifiers exhibit almost the same performance (overall accuracy ≡ 87%), Thus the linear classifier suffices for the discrimination of the present problem. Also, based on the previous results, one can conjecture that the use of these classifiers combined with image morphometry and statistical techniques for feature transformation, may offer useful information towards the improvement of the diagnostic accuracy of breast FNA.	statistical classification	Konstantinos Koutroumbas;Abraham Pouliakis;Tatiana Mona Megalopoulou;John Georgoulakis;Anna-Eva Giachnaki;Petros Karakitsos	2006		10.1007/11752912_64	feedforward neural network;discrimination;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;supervised learning;k-nearest neighbors algorithm;artificial neural network	NLP	9.678057658218439	-32.78975346681666	118144
afb460b342e5196ec69bf25900f34274a8006be4	a new two-phase approach to fuzzy modeling for nonlinear function approximation	simulation ordinateur;modelizacion;agregacion;evaluation performance;optimisation;tecnologia electronica telecomunicaciones;performance evaluation;optimizacion;numero difuso;systeme aide decision;fuzzy number;algoritmo borroso;evaluacion prestacion;partial encoding;nombre flou;non linear model;modele non lineaire;sistema complejo;sistema ayuda decision;algoritmo genetico;sintonizacion fase;aggregation;approximation fonction;fuzzy modeling;modelisation;codificacion;decision support system;modelo no lineal;systeme complexe;function approximation;complex system;fonction appartenance;fuzzy algorithm;signal classification;coding;membership function;algorithme genetique;agregation;classification signal;algorithme flou;algorithme evolutionniste;genetic algorithm;algoritmo evolucionista;optimization;sistema difuso;hiperplano;simulacion computadora;systeme flou;funcion pertenencia;evolutionary algorithm;classification automatique;tecnologias;phase tuning;grupo a;automatic classification;modeling;hyperplane;hyperplane shaped clustering;clasificacion automatica;two phase approach;computer simulation;fuzzy system;hyperplan;fuzzy model;codage;accord phase	Nonlinear modeling of complex irregular systems constitutes the essential part of many control and decision-making systems and fuzzy logic is one of the most effective algorithms to build such a nonlinear model. In this paper, a new approach to fuzzy modeling is proposed. The model considered herein is the well-known Sugeno-type fuzzy system. The fuzzy modeling algorithm suggested in this paper is composed of two phases: coarse tuning and fine tuning. In the first phase (coarse tuning), a successive clustering algorithm with the fuzzy validity measure (SCFVM) is proposed to find the number of the fuzzy rules and an initial fuzzy model. In the second phase (fine tuning), a moving genetic algorithm with partial encoding (MGAPE) is developed and used for optimized tuning of membership functions of the fuzzy model. Two computer simulation examples are provided to evaluate the performance of the proposed modeling approach and compare it with other modeling approaches.	approximation;two-phase locking	Wooyong Chung;Euntai Kim	2006	IEICE Transactions	10.1093/ietisy/e89-d.9.2473	computer simulation;simulation;systems modeling;genetic algorithm;decision support system;membership function;defuzzification;adaptive neuro fuzzy inference system;function approximation;computer science;artificial intelligence;fuzzy number;hyperplane;neuro-fuzzy;evolutionary algorithm;mathematics;fuzzy associative matrix;coding;fuzzy set operations;algorithm;fuzzy control system	DB	8.69305688059529	-28.738800920950066	118168
9987112b53db9528ef42e07d27187478f53f9c73	hybrid pattern recognition method to diagnose dynamic systems		Abstract   We use the supervised classification method Fuzzy Pattern Matching (FPM) to realize the diagnosis of dynamic systems. FPM is decentralized, i.e., its global decision is based on the selection of one of the intermediate decisions. Each intermediate decision is based on one attribute. Thus, FPM does not take into account the correlation between attributes. Additionally, FPM does not respect the shape of classes if this shape is non convex. These drawbacks can be solved by using the expert knowledge based on a set of rules which provide the information about attributes correlation and the shape of classes. However this information is imprecise. Thus in this paper, we propose to combine FPM with the expert rules in order to obtain efficient and precise decision about the class of new patterns. We call this combination Hybrid FPM (HFPM). Three examples are used to show the performances of HFPM with respect to the classical FPM.	dynamical system;pattern recognition	Mohamed Saïd Bouguelid;Moamar Sayed Mouchaweh;Patrice Billaudel;Bernard Riera	2007		10.3182/20070904-3-KR-2922.00009	machine learning;pattern recognition;data mining;mathematics	Vision	1.716009666834841	-30.037110142128725	118173
3c4e44b9139111fe09fd5d185d3048ed242c7bf2	a fuzzy neural architecture for customer satisfaction assessment	customer satisfaction	We present a fuzzy neural network for assessing customer satisfaction. It is a feedforward multilayer network with a fuzzy controller and utilizes back-propagation learning. The controller will measure customer satisfaction levels, thereby assessing current strategies and providing a means for evaluating possible changes. Preliminary results are promising and have provided new paths for future research.		Cecilia Temponi;Ying-Feng Kuo;Herbert W. Corley	1999	Journal of Intelligent and Fuzzy Systems		voice of the customer;computer science;knowledge management;customer intelligence;customer satisfaction;service quality	Robotics	5.296461071037403	-24.338761806700894	118211
d99e8acf2f8c72bbb848ff18ad880a0363099640	"""rule set complexity for incomplete data sets with many attribute-concept values and """"do not care"""" conditions"""				Patrick G. Clark;Cheng Gao;Jerzy W. Grzymala-Busse	2016		10.1007/978-3-319-47160-0_6	discrete mathematics;data mining;mathematics;statistics	ML	2.0177753044981697	-24.02211858276171	118362
8eb7ba954e88031510dd3be890057e157f6c944d	qbsq: a quad-tree based algorithm for skyline query	query processing;multi criteria decision making;algorithm data mining skyline decision making;query processing data mining decision making;user preferences;data mining;skyline;algorithm;data mining algorithm;partitioning algorithms decision making data mining computer science software engineering software algorithms information science data engineering data visualization scalability;user preference queries qbsq quad tree based algorithm skyline query multicriteria decision making data mining data visualization	Skyline has been proposed as an important operation for multi-criteria decision making, data mining and visualization, and user preference queries. In this paper, we systematically explore a quad-tree based approach for computing skyline points which contributes to a better performance than traditional ones for skyline queries. Based on this approach, we present a novel algorithm QBSQ for finding the set of global skyline points. QBSQ partitions data points dynamically by means of the configuration characters of quad-tree, and then deletes points dominated by other(s) while constructing the tree. The amount of work for domination checking is minimized and the efficiency of our algorithm is thus improved. Extensive experiments prove the efficiency and the scalability of QBSQ algorithm.	algorithm;data mining;data point;dominating set;experiment;pareto efficiency;quadtree;scalability	Zhixin Ma;Sheng Lijun;Xu Yusheng;Li Lian	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.682	computer science;data science;data mining;database;algorithm	DB	-4.411483107895502	-37.91269313508707	118667
3e8fcc784aa4452409f649ab7c9dc41a2247e89f	a distance-based comparison of basic voting rules	computational complexity;voting rule;computer simulation	In this paper we provide a comparison of different voting rules in a distance-based framework with the help of computer simulations. Taking into account the informational requirements to operate such voting rules and the outcomes of two well-known reference rules, we identify the Copeland rule as a good compromise between these two reference rules. It will be shown that the outcome of the Copeland rule is “close” to the outcomes of the reference rules, but it requires less informational input and has lower computational complexity.	computational complexity theory;computer simulation;qualitative comparative analysis;requirement	Daniel Eckert;Christian Klamler;Johann Mitlöhner;Christian Schlötterer	2006	CEJOR	10.1007/s10100-006-0011-x	computer simulation;computer science;artificial intelligence;machine learning;data mining;mathematics;cardinal voting systems;computational complexity theory;anti-plurality voting	AI	1.1117723803564055	-27.236667400231372	118890
bad8674ab4ad8c2288f2686e7923f501a55733da	embedding learning in a general frame-based architecture	explanation;learning algorithm;inference mechanisms;learning systems;machine learning;knowledge acquisition;software framework;search control knowledge frame based architecture machine learning capabilities chunker explanation based chunking mechanism theo software framework self modifying problem solving systems chains of inference;problem solving explanation inference mechanisms knowledge acquisition knowledge based systems knowledge representation learning systems;problem solving knowledge representation computer architecture state space methods costs computer science inference algorithms application software learning systems control systems;knowledge representation;knowledge based systems;problem solving	An effort to incorporate machine learning capabilities within a general-purpose frame-based architecture is discussed. The authors describe Chunker, an explanation-based chunking mechanism built on top of Theo, a software framework to support development of self-modifying problem-solving systems. Chunker forms rules that improve problem-solving efficiency by generalizing and compressing the chains of inference which Theo produces during problem solving. After presenting the learning algorithm used by Chunker, the authors illustrate its application to learning search control knowledge, discuss its relationship to Theo's other three learning mechanisms, and consider the relationship between architectural features of Theo and the effectiveness of Chunker. >	frame language;general frame	Toshikazu D Tanaka;Tom M. Mitchell	1989		10.1109/TAI.1989.65305	natural language processing;knowledge representation and reasoning;computer science;artificial intelligence;software framework;knowledge-based systems;machine learning	Vision	6.061316550247622	-30.259031936670503	118977
725d29a7e80a5e17453d2ba4a5444a664a7797c3	cluster validity based on the hard tendency of the fuzzy classification	analyse multivariable;fuzzy classification;fuzzy set;multivariate analysis;bootstrap;analisis datos;ensemble flou;validite;algorithme;algorithm;data analysis;validity;validez;pattern recognition;analisis multivariable;conjunto flojo;analyse donnee;reconnaissance forme;cluster validity;classification automatique;reconocimiento patron;automatic classification;clasificacion automatica;algoritmo	Abstract   We present two new fuzzy cluster validity functionals (minimum and mean hard tendencies), based on the analysis of the hard tendency of the fuzzy classification generated by the fuzzy  c -means algorithm. We have used the bootstrap technique, to avoid the possible influence of local minimums, obtained by the fuzzy  c -means algorithm.	fuzzy classification	Francisco F. Rivera;Emilio L. Zapata;José María Carazo	1990	Pattern Recognition Letters	10.1016/0167-8655(90)90050-C	defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;mathematics;fuzzy set;multivariate analysis;data analysis;algorithm;validity;statistics	Vision	8.321126370821196	-34.28509722194887	119041
02abba98bbe0ebc3af426af23e81702740437c0f	guiding genetic program based data mining using fuzzy rules	arbre graphe;tratamiento datos;extraction information;automatic;experimental method;genetic program;control algorithm;convergence;decision tree;mathematical formula;analisis datos;tree graph;information extraction;metodo experimental;structure programme;information retrieval;structure arborescente;fuzzy rules;cooperation;metodo arborescente;calcul formel;adaptive control systems;decision borrosa;logique floue;resource management;data processing;logica difusa;drones;traitement donnee;intelligence artificielle;genetic programs;robotics;decision floue;automatico;prise decision;arbol decision;algoritmo genetico;data mining;comparison;convergence numerique;calculo formal;fuzzy logic;numerical convergence;symposia;data analysis;innovation;estructura programa;algebra proceso;fouille donnee;estructura arborescente;formule mathematique;data mining algorithm;tree structure;formula matematica;algebre processus;algorithme genetique;automatique;artificial intelligence;tree structured method;analyse donnee;genetic algorithm;genetic algorithms;autonomous navigation;methode arborescente;inteligencia artificial;fuzzy decision tree;arbol grafo;innovacion;planning algorithms;toma decision;process algebra;computer algebra;program structure;convergencia numerica;busca dato;arbre decision;extraccion informacion;control algorithms;fuzzy decision;methode experimentale;multiagent systems	A data mining procedure for automatic determination of fuzzy decision tree structure using a genetic program is discussed. A genetic program (GP) is an algorithm that evolves other algorithms or mathematical expressions. Methods for accelerating convergence of the data mining procedure are examined. The methods include introducing fuzzy rules into the GP and a new innovation based on computer algebra. Experimental results related to using computer algebra are given. Comparisons between trees created using a genetic program and those constructed solely by interviewing experts are made. Connections to past GP based data mining procedures for evolving fuzzy decision trees are established. Finally, experimental methods that have been used to validate the data mining algorithm are discussed.	aerial photography;data mining;database;decision tree;fitness function;fuzzy rule;genetic algorithm;mined;symbolic computation;tree (data structure);tree structure;unmanned aerial vehicle	James F. Smith;ThanhVu Nguyen	2006		10.1007/11875581_159	genetic algorithm;data processing;computer science;artificial intelligence;resource management;machine learning;robotics;information extraction;algorithm	ML	0.6177124307036851	-30.68321597588815	119100
0eb6b306e533f1c05dcf203584b8f30ca098b3a3	on-line status assessment of wind turbines based on improved fuzzy comprehensive evaluation method				Ruiming Fang;Mingling Wu;Shunhui Jiang	2016	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-169163	artificial intelligence;wind power;machine learning;mathematics;fuzzy logic	Robotics	4.633202230108361	-24.13249930239646	119113
d997a849c83c8d4bdfc1ae9a1df7d5072517bb69	multiclass cascades for ensemble-based boosting algorithms			algorithm;multiclass classification	Teo Susnjak;Andre L. C. Barczak;Napoleon H. Reyes;Kenneth A. Hawick	2012		10.3233/978-1-61499-096-3-330	boosting (machine learning);machine learning;computer science;artificial intelligence;pattern recognition	ML	9.780984547744863	-36.962737076537806	119144
b2500c7d42a5b774a902a0b215bcba92c6eaae35	on combining dissimilarity representations	learning algorithm;image processing;analisis datos;mesure distance;caracter manuscrito;dissimilarity measure;manuscript character;procesamiento imagen;algorithme apprentissage;classification;traitement image;distance measurement;data analysis;pattern recognition;analyse donnee;reconnaissance forme;reconocimiento patron;algoritmo aprendizaje;caractere manuscrit;clasificacion	For learning purposes, representations of real world objects can be built by using the concept of dissimilarity (distance). In such a case, an object is characterized in a relative way, i.e. by its dissimilarities to a set of the selected prototypes. Such dissimilarity representations are found to be more practical for some pattern recognition problems. When experts cannot decide for a single dissimilarity measure, a number of them may be studied in parallel. We investigate two possibilities of combining either dissimilarity representations themselves or classifiers built on each of them separately. Our experiments conducted on a handwritten digit set demonstrate that when the dissimilarity representations are of different nature, a much better performance can be obtained by their combination than on individual representations.	experiment;pattern recognition	Elzbieta Pekalska;Robert P. W. Duin	2001		10.1007/3-540-48219-9_36	image processing;biological classification;computer science;artificial intelligence;machine learning;mathematics;data analysis;algorithm	ML	9.95497784119945	-33.80425604033624	119166
1868f402c4888ddd0a9781f4ef089f32aaf5efc9	a comparison of positive, boundary, and possible rules using the mlem2 rule induction algorithm	rule induction;probability;approximation algorithms;rough set theory;probabilistic logic approximation methods error analysis rough sets data mining approximation algorithms;data mining;probabilistic rules;approximation theory;error analysis;probabilistic rules data mining rough set theory rule induction algorithm mlem2;boundary rules;rough set theory data mining probability;probability theory;rule induction algorithm mlem2;ten fold data cross validation;rough sets;approximation methods;cross validation;probabilistic logic;mlem2 rule induction algorithm;data mining mlem2 rule induction algorithm rough set theory probability theory approximation theory ten fold data cross validation boundary rules	We explore an extension of rough set theory based on probability theory. Lower and upper approximations, the basic ideas of rough set theory, are generalized by adding two parameters, denoted by alpha and beta. In our experiments, for different pairs of alpha and beta, we induced three types of rules: positive, boundary, and possible. The quality of these rules was evaluated using ten-fold cross validation on five data sets. The main results of our experiments are that there is no significant difference in quality between positive and possible rules and that boundary rules are the worst.	algorithm;approximation;bell test experiments;cross-validation (statistics);data pre-processing;experiment;mathematical induction;rough set;rule 110;rule induction;set theory	Jerzy W. Grzymala-Busse;Shantan R. Marepally;Yiyu Yao	2010	2010 10th International Conference on Hybrid Intelligent Systems	10.1109/HIS.2010.5601064	machine learning;pattern recognition;data mining;mathematics	Robotics	1.3613544222690548	-30.37113010578799	119210
0061990ccfc4b9b6c5999ae2defb998aa33d646f	a general approach to mining quality pattern-based clusters from microarray data	microarray data;commerce electronique;model based reasoning;analyse amas;raisonnement base sur modele;pulga de dna;comercio electronico;generic model;analisis datos;puce a dna;segmentation;data mining;classification;data analysis;cluster analysis;fouille donnee;comportement utilisateur;dna chip;analyse donnee;analisis cluster;user behavior;busca dato;clasificacion;segmentacion;comportamiento usuario;electronic trade	Pattern-based clustering has broad applications in microarray data analysis, customer segmentation, e-business data analysis, etc. However, pattern-based clustering often returns a large number of highlyoverlapping clusters, which makes it hard for users to identify interesting patterns from the mining results. Moreover, there lacks of a general model for pattern-based clustering. Different kinds of patterns or different measures on the pattern coherence may require different algorithms. In this paper, we address the above two problems by proposing a general quality-driven approach to mining top-k quality pattern-based clusters. We examine our quality-driven approach using real world microarray data sets. The experimental results show that our method is general, effective and efficient.	algorithm;cluster analysis;electronic business;microarray;regular expression	Daxin Jiang;Jian Pei;Aidong Zhang	2005		10.1007/11408079_18	microarray analysis techniques;dna microarray;fuzzy clustering;biological classification;computer science;bioinformatics;model-based reasoning;consensus clustering;data mining;cluster analysis;data analysis;segmentation;affinity propagation;clustering high-dimensional data	ML	-3.612060812624056	-33.167376875355714	119359
268181db1de591f0df869230685f955ac1227573	sequential pattern mining based on a new criteria and attribute constraints	satisfiability;data mining;apriori property;pattern analysis computer networks data analysis information analysis association rules testing statistical analysis time factors;pattern classification;attribute constraints sequential pattern mining apriori property;attribute constraints;sequential pattern mining;sequential pattern;pattern classification data mining	This paper proposes the sequential interestingness as a new evaluation criterion that evaluates a sequential pattern corresponding to the interests of analysts. The sequential pattern is composed of rows of item sets. The criterion satisfies the apriori property. Also, this paper proposes three attribute constraints. These constraints can naturally evaluate relationships of attributes both in an item set and between continuous item sets. In addition, this paper proposes a mining method incorporating the criterion and the constraints. The method can efficiently discover all sequential patterns whose sequential interestingness is larger than or equal to a threshold and that satisfy the constraints. Lastly, this paper verifies the effectiveness of the proposed method by applying the method to medical examination data.	apriori algorithm;data mining;discrete mathematics;level of measurement;sequential pattern mining	Shigeaki Sakurai;Youichi Kitahara;Ryohei Orihara	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413602	sequential pattern mining;computer science;machine learning;pattern recognition;data mining;satisfiability	DB	-3.568852806907678	-30.550805098065236	119505
2081b83b22b907423f526352be43cd5db79f35e7	symbolic knowledge extraction from trained neural networks: a sound approach	empirical analysis;search space;knowledge extraction;rule extraction;qa75 electronic computers computer science;artificial neural networks;real world application;nonmonotonic reasoning;neural symbolic integration;extraction method;artificial neural network;neural network;partial order	Although neural networks have shown very good performance in many application domains, one of their main drawbacks lies in the incapacity to provide an explanation for the underlying reasoning mechanisms. The “explanation capability” of neural networks can be achieved by the extraction of symbolic knowledge. In this paper, we present a new method of extraction that captures nonmonotonic rules encoded in the network, and prove that such a method is sound. We start by discussing some of the main problems of knowledge extraction methods. We then discuss how these problems may be ameliorated. To this end, a partial ordering on the set of input vectors of a network is defined, as well as a number of pruning and simplification rules. The pruning rules are then used to reduce the search space of the extraction algorithm during a pedagogical extraction, whereas the simplification rules are used to reduce the size of the extracted set of rules. We show that, in the case of regular networks, the extraction algorithm is sound and complete. We proceed to extend the extraction algorithm to the class of non-regular networks, the general case. We show that non-regular networks always contain regularities in their subnetworks. As a result, the underlying extraction method for regular networks can be applied, but now in a decompositional fashion. In order to combine the sets of rules extracted from each subnetwork into the final set of rules, we use a method whereby we are able to keep the soundness of the extraction algorithm. Finally, we present the results of an empirical analysis of the extraction system, using traditional examples and real-world application problems. The results have shown that a very high fidelity between the extracted set of rules and the network can be achieved.  2001 Elsevier Science B.V. All rights reserved.	algorithm;application domain;artificial neural network;benchmark (computing);best, worst and average case;correctness (computer science);encode;expressive power (computer science);franz lisp;fuzzy set;genetic algorithm;hector levesque;level of detail;mitchell corporation;numerical analysis;regular expression;requirement;rule induction;stochastic optimization;subnetwork;text simplification;time complexity	Artur S. d'Avila Garcez;Krysia Broda;Dov M. Gabbay	2001	Artif. Intell.	10.1016/S0004-3702(00)00077-1	partially ordered set;computer science;artificial intelligence;non-monotonic logic;machine learning;data mining;artificial neural network	AI	2.227664189068005	-29.010961723051565	119544
bc6294b807c43a627a6de64edb098a56935b3983	empirical results on data dimensionality reduction using the divided self-organizing map	analyse parole;systeme intelligent;analisis palabra;multilayer perceptrons;speech processing;sistema inteligente;speech analysis;tratamiento palabra;traitement parole;perceptron multicouche;intelligent system;self organized map;dimensional reduction	In order to reduce the complexity of the classification task and decrease the quantisation error, we propose a speech data classification framework based on a two-stage neural network model comprising the divided self organizing map (DSOM) and multilayer perceptrons (MLP).	dimensionality reduction;organizing (structure);self-organizing map	Takamasa Koshizen;Hiroaki Ogawa;John Fulcher	1998		10.1007/3-540-64383-4_38	speech recognition;computer science;artificial intelligence;machine learning;speech processing	Robotics	9.944201728786116	-32.193994785088854	119561
d6ad662865668f1aa7c9bfee6ab9465f853e68b0	mining incomplete survey data through classification	extraction information;incomplete survey data;analisis datos;information extraction;dato que falta;informacion incompleta;prise de decision;data mining;classification;donnee manquante;incomplete information;incomplete data;data analysis;fouille donnee;decouverte connaissance;information incomplete;descubrimiento conocimiento;analyse donnee;survey data;missing data;missing values;toma decision;subject areas;busca dato;clasificacion;extraccion informacion;knowledge discovery	Data mining with incomplete survey data is an immature subject area. Mining a database with incomplete data, the patterns of missing data as well as the potential implication of these missing data constitute valuable knowledge. This paper presents the conceptual foundations of data mining with incomplete data through classification which is relevant to a specific decision making problem. The proposed technique generally supposes that incomplete data and complete data may come from different sub-populations. The major objective of the proposed technique is to detect the interesting patterns of data missing behavior that are relevant to a specific decision making, instead of estimation of individual missing value. Using this technique, a set of complete data is used to acquire a near-optimal classifier. This classifier provides the prediction reference information for analyzing the incomplete data. The data missing behavior concealed in the missing data is then revealed. Using a real-world survey data set, the paper demonstrates the usefulness of this technique.	association rule learning;data mining;database;decision tree;high- and low-level;knowledge engineering;mathematical induction;missing data;population;rule induction;sas;statistical classification;utility	Hai Wang;Shouhong Wang	2009	Knowledge and Information Systems	10.1007/s10115-009-0245-8	missing data;computer science;data science;pattern recognition;data mining;data pre-processing;information extraction;statistics	ML	-0.6335101600195732	-31.338446529737595	119701
86ec8f0f38207cbea336dd6101977cda8ae19cf7	batch classification with applications in computer aided diagnosis	modelizacion;concepcion asistida;correlacion;computer aided design;distribution donnee;computer aided diagnosis;batch production;diagnostico;procede discontinu;intelligence artificielle;probabilistic approach;classification;data distribution;modelisation;produccion por lote;mathematical programming;probabilistic analysis;enfoque probabilista;approche probabiliste;production par lot;batch process;conception assistee;artificial intelligence;procedimiento discontinuo;inteligencia artificial;correlation;diagnosis;modeling;programmation mathematique;distribucion dato;programacion matematica;clasificacion;diagnostic	Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this a ssumption, we consider the case where batches or groups of samples may hav e internal correlations, whereas the samples from different batches may be con sidered to be uncorrelated. Two algorithms are developed to classify all the samples in a batch jointly, one based on a probabilistic analysis and another based on a mathem atical programming approach. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significan tly more accurate than a naive SVM which ignores the correlations among the samp les.	algorithm;colon classification;computer-aided design;emoticon;naruto shippuden: clash of ninja revolution 3;probabilistic analysis of algorithms;real life	Volkan Vural;Glenn Fung;Balaji Krishnapuram;Jennifer G. Dy;R. Bharat Rao	2006		10.1007/11871842_43	probabilistic analysis of algorithms;systems modeling;biological classification;computer science;artificial intelligence;computer aided design;operations research;correlation;algorithm;batch processing	AI	9.242916945546432	-33.558337160542145	119862
428079257c660f04c57c036f56615b39f3f37c93	a fuzzy neural network applied in the speech recognition system	cluster algorithm;pattern clustering;fuzzy rules t s fuzzy neural network speech recognition;fuzzy neural network;fuzzy neural nets;fuzzy rules;training;subtraction clustering algorithm;radial basis function networks;indexes;artificial neural networks;radial basis function;speech recognition system;rbf neural network;radial basis function neural network;k means clustering algorithm t s fuzzy neural network speech recognition system subtraction clustering algorithm network reasoning radial basis function neural network;cognition;clustering algorithms;speech recognition;k means clustering algorithm;network reasoning;t s fuzzy neural network;fuzzy neural networks speech recognition clustering algorithms artificial neural networks hidden markov models fuzzy systems robustness fuzzy logic humans fuzzy reasoning;fuzzy neural networks;speech recognition fuzzy neural nets pattern clustering radial basis function networks;k means clustering	There are two problems when conditional T-S fuzzy neural network is used directly in speech recognition system. One is the rule disaster problem, that is, the rule number will increase exponentially with the increase of input dimensions. Another problem is the network reasoning failure resulted from input dimensions too large. The paper presented an improved algorithm of T-S fuzzy neural network. The subtraction clustering algorithm was used to make certain rule number to escape the rule disaster. The network reasoning can correctly work by adding a compensated factor on membership. The improved algorithm was used in speech recognition system. The experimental results showed that the recognition results of improved algorithm are better than the ones of radial basis function (RBF) neural network using K-means clustering algorithm to select the centroid. And it has much better robustness.	algorithm;artificial neural network;cluster analysis;k-means clustering;neuro-fuzzy;radial (radio);radial basis function;regular expression;robustness (computer science);speech recognition	Xueying Zhang;Gaoyun Li;Wenjun Hou	2008	2008 Fourth International Conference on Natural Computation	10.1109/ICNC.2008.404	neural gas;computer science;artificial intelligence;machine learning;pattern recognition;time delay neural network	ML	5.880627916475068	-27.802882324991447	120018
326aa2f59d0fdc3b30134e2b5a4239fe1c74ec94	intelligent frameworks for encoding xml elements using mining algorithm	arbre graphe;tratamiento datos;extraction information;base relacional dato;association statistique;record format;tratamiento transaccion;analisis datos;tree graph;information extraction;ingenierie connaissances;structure arborescente;format enregistrement;xml language;metodo arborescente;data processing;statistical association;traitement donnee;stockage donnee;intelligence artificielle;relational database;data mining;2 dimensional;association rule mining;semistructured data;data analysis;data storage;codificacion;asociacion estadistica;semi structured data;research and development;dato semi estructurado;association rule;indexing;fouille donnee;estructura arborescente;data mining algorithm;indexation;tree structure;coding;indizacion;base de donnees relationnelle;almacenamiento datos;artificial intelligence;tree structured method;analyse donnee;methode arborescente;inteligencia artificial;formato grabacion;transaction processing;arbol grafo;busca dato;extraccion informacion;langage xml;lenguaje xml;traitement transaction;codage;donnee semistructuree;knowledge engineering	Mining of association rules is to find associations among data items that appear together in some transactions or business activities. As of today, algorithms for association rule mining, as well as for other data mining tasks, are mostly applied to relational databases. As XML being adopted as the universal format for data storage and exchange, mining associations from XML data becomes an area of attention for researchers and developers. The challenge is that the semi-structured data format in XML is not directly suitable for traditional data mining algorithms and tools. In this paper we present an intelligent encoding method to encode XML tree-nodes. This method is used to store the XML data in 2-dimensional tables that can be easily accessed via indexing using knowledge. The hierarchical relationship in the original XML tree structure is embedded in the encoding. We applied this method in some applications, such as mining of association rules.		Haeng-Kon Kim	2006		10.1007/11893004_96	xml catalog;xml validation;binary xml;xml encryption;simple api for xml;xml;xml schema;predictive model markup language;streaming xml;computer science;document structure description;xml framework;data mining;xml database;xml schema;database;data stream mining;xml signature;world wide web;xml schema editor;efficient xml interchange	DB	-3.40555589002997	-33.08145312308273	120043
d3d1bbf6897ba0a178a03b82407a2eefb841cdeb	a parallel genetic programming based intelligent miner for discovery of censored production rules with fuzzy hierarchy	censored production rules with fuzzy hierarchy;hierarchical structure;genetic program;genetic operator;variable precision logic;genetic programming;data mining;approximate reasoning;knowledge structure;island model;production rule;fitness function;knowledge discovery	Automated discovery of rules with exceptions and hierarchical structures is an important problem in data mining. A knowledge structure based on Censored Production Rules with Fuzzy Hierarchy (CPRFH) not only provides an excellent mechanism for handling exceptions but also captures the hierarchical relationship among the classes in the dataset. Moreover, CPRFHs are able to exhibit variable precision logic for approximate reasoning. This paper proposes discovery of knowledge in the form of CPRFHs using island model of genetic programming with two advanced genetic operators, namely; fission and fusion. The fission and fusion operators impart intelligence to the system as these operators discover new classes/concepts which are not present explicitly in the data set being mined. A suitable encoding with syntactic constraints is designed and an appropriate fitness function is suggested to measure the goodness of the hierarchies. The experimental results confirm that the island model with fission and fusion outperforms the sequential as well as the island models without fission and fusion in terms of correctness of the solution arrived and size of the trees evolved.	genetic programming	K. K. Bharadwaj;Saroj Ratnoo	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.12.048	genetic programming;computer science;artificial intelligence;genetic operator;machine learning;data mining;mathematics;knowledge extraction;fitness function	AI	4.160567138780281	-30.364325635024862	120145
e22e3266e5cdd912616dd7a7a8f41d6e693b597b	deep learning approaches for predictive masquerade detection		In computer security, masquerade detection is a special type of intrusion detection problem. Effective and early intrusion detection is a crucial factor for computer security. Although considerable work has been focused on masquerade detection for more than a decade, achieving a high level of accuracy and a comparatively low false alarm rate is still a big challenge. In this paper, we present a comprehensive empirical study in the area of anomaly-based masquerade detection using three deep learning models, namely, Deep Neural Networks (DNN), Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN), and Convolutional Neural Networks (CNN). In order to surpass previous studies on this subject, we used three UNIX command line-based datasets, with six variant data configurations implemented from them. Furthermore, static and dynamic masquerade detection approaches were utilized in this study. In a static approach, DNN and LSTM-RNNmodels are used along with a Particle SwarmOptimization-based algorithm for their hyperparameters selection. On the other hand, a CNN model is employed in a dynamic approach. Moreover, twelve well-known evaluation metrics are used to assess model performance in each of the data configurations. Finally, intensive quantitative and ROC curves analyses of results are provided at the end of this paper. The results not only show that deep learning models outperform all traditional machine learning methods in the literature but also prove their ability to enhance masquerade detection on the used datasets significantly.	algorithm;anomaly detection;bernard greenberg;best, worst and average case;command-line interface;computer security;convolutional neural network;deep learning;evaluation function;experiment;feature extraction;high-level programming language;ibm systems network architecture;internet;intrusion detection system;long short-term memory;machine learning;neural networks;particle swarm optimization;phase-shift oscillator;profiling (computer programming);random neural network;receiver operating characteristic;recurrent neural network;unix	Wisam Elmasry;Akhan Akbulut;Abdul Halim Zaim	2018	Security and Communication Networks	10.1155/2018/9327215	convolutional neural network;computer science;computer network;receiver operating characteristic;artificial neural network;deep learning;constant false alarm rate;machine learning;intrusion detection system;recurrent neural network;artificial intelligence;particle swarm optimization	ML	7.323828127128865	-37.399297001923834	120309
a50bbcf7db1aad16e110cdb9709a29a527c1a890	intuitionistic fuzzy rough sets: at the crossroads of imperfect knowledge	science general;intuitionistic fuzzy set theory;lower and upper approximation;rough set theory;l fuzzy set theory;incomplete information;rough set	Just like rough set theory, fuzzy set theory addresses the topic of dealing with imperfect knowledge. Recent investigations have shown how both theories can be combined into a more flexible, more expressive framework for modelling and processing incomplete information in information systems. At the same time, intuitionistic fuzzy sets have been proposed as an attractive extension of fuzzy sets, enriching the latter with extra features to represent uncertainty (on top of vagueness). Unfortunately, the various tentative definitions of the concept of an ‘intuitionistic fuzzy rough set’ that were raised in their wake are a far cry from the original objectives of rough set theory. We intend to fill an obvious gap by introducing a new definition of intuitionistic fuzzy rough sets, as the most natural generalization of Pawlak’s original concept of rough sets.	byte;expert system;fuzzy set;information system;intuitionistic logic;martine kempf;national fund for scientific research;rapid refresh;rough set;set theory;vagueness	Chris Cornelis;Martine De Cock;Etienne E. Kerre	2003	Expert Systems	10.1111/1468-0394.00250	fuzzy logic;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;machine learning;fuzzy set;fuzzy set operations;algorithm;dominance-based rough set approach	AI	-2.5364119460861043	-23.959938004978458	120349
4e67aa8ebe9fa05f4f4c6fb6298d2146a09f5c7d	salient segmentation of medical time series signals	sliding window extraction;medical time series signals;image segmentation;medical administrative data processing;motif discovery;segmentation;high entropy;time series segment;time series;data mining;database index;data searching;time series analysis redundancy electrocardiography indexing;electrocardiography;redundancy;time series analysis;indexing;multidimensional datasets;feature extraction;salient segmentation;indexation;data mining time series signals segmentation indexing;time series signals;entropy;medical time series database;probabilistic segmentation;time series data mining entropy feature extraction image segmentation medical administrative data processing medical signal processing;motif discovery algorithm;database index salient segmentation medical time series signals data searching data mining medical time series database high entropy multidimensional datasets sliding window extraction time series segment probabilistic segmentation motif discovery algorithm;medical signal processing;sliding window	"""Searching and mining medical time series databases is extremely challenging due to large, high entropy, and multidimensional datasets. Traditional time series databases are populated using segments extracted by a sliding window. The resulting database index contains an abundance of redundant time series segments with little to no alignment. This paper presents the idea of """"salient segmentation"""". Salient segmentation is a probabilistic segmentation technique for populating medical time series databases. Segments with the lowest probabilities are considered salient and are inserted into the index. The resulting index has little redundancy and is composed of aligned segments. This approach reduces index sizes by more than 98% over conventional sliding window techniques. Furthermore, salient segmentation can reduce redundancy in motif discovery algorithms by more than 85%, yielding a more succinct representation of a time series signal."""	algorithm;alignment;database index;databases;extraction;greater than;image segmentation;motif;population;time series;biologic segmentation	Jonathan Woodbridge;Mars Lan;Majid Sarrafzadeh;Alex A. T. Bui	2011	2011 IEEE First International Conference on Healthcare Informatics, Imaging and Systems Biology	10.1109/HISB.2011.41	computer science;data science;pattern recognition;data mining;scale-space segmentation	DB	-3.2319749621507072	-37.414026680748975	120452
d512ab85dc79586e784a11a57caadaf5a7e21923	fuzzy-weighted distance and its applications in pattern recognition and classification	object recognition;pattern recognition fuzzy set theory fuzzy sets statistics control engineering hamming distance extraterrestrial measurements error analysis ice marine vehicles;object recognition pattern classification pattern recognition fuzzy weighted distance feature odds fuzzy set theory human thinking;pattern recognition fuzzy set theory;fuzzy set theory;pattern recognition	The fuzzy-weighted distance presented here shows that every feature of every sample has different effect on distance in pattern recognition and classification. The differece is described by using the concept o f feature odds defined here which is drived from fuzzy s e t theory and is determined by the way o f people's thinking in recognizing things. The reasonableness of this new distance is discussed in this paper. The experiment results show the advantages of this new distance.	pattern recognition;statistical classification	Yu Lu;Xi-lu Fan	1988		10.1109/ICPR.1988.28442	feature;fuzzy classification;computer science;fuzzy number;cognitive neuroscience of visual object recognition;machine learning;pattern recognition;data mining;mathematics;fuzzy set;3d single-object recognition	Vision	0.3511363081058609	-26.41472021576749	120613
c606747aa97a773542928ad3041116bfc626bb10	local neighborhood rough set		With the advent of the age of big data, a typical big data set called limited labeled big data appears. It includes a small amount of labeled data and a large amount of unlabeled data. Some existing neighborhood-based rough set algorithms work well in analyzing the rough data with numerical features. But, they face three challenges: limited labeled property of big data, computational inefficiency and over-fitting in attribute reduction when dealing with limited labeled data. In order to address the three issues, a combination of neighborhood rough set and local rough set called local neighborhood rough set (LNRS) is proposed in this paper. The corresponding concept approximation and attribute reduction algorithms designed with linear time complexity can efficiently and effectively deal with limited labeled big data. The experimental results show that the proposed local neighborhood rough set and corresponding algorithms significantly outperform its original counterpart in classical neighborhood rough set. These results will enrich the local rough set theory and enlarge its application scopes.	algorithm;approximation;big data;numerical analysis;overfitting;rough set;set theory;time complexity	Qi Wang;Yuhua Qian;Xinyan Liang;Qian Guo;Jiye Liang	2018	Knowl.-Based Syst.	10.1016/j.knosys.2018.04.023	machine learning;time complexity;labeled data;artificial intelligence;computer science;big data;inefficiency;rough set	ML	-1.1783482756094974	-35.4600654033823	121103
05dfca73bdb954de8faf7852277a64cf6e1677ab	cocoa: compressed continuity analysis for temporal databases	journal collection;期刊論文;tratamiento transaccion;考試論文;analisis datos;base donnee temporelle;機構典藏;data mining;論文;碩博士論文;report;data analysis;temporal database;研究計畫;thesis;fouille donnee;ir;教育培訓;decouverte connaissance;論文課件;考古題;校園高校;descubrimiento conocimiento;analyse donnee;temporal databases;博碩士論文;pass exam;transaction processing;article;busca dato;traitement transaction;knowledge discovery	A continuity is a kind of inter-transaction association which describes the relationships among different transactions. Since it breaks the boundaries of transactions, the number of potential itemsets and the number of rules will increase drastically. In this paper we consider the problem of discovering frequent compressed continuity patterns, which have the same power as mining the complete set of frequent continuity patterns. We devised a three-phase algorithm, COCOA, for frequent compressed continuity mining.	scott continuity	Kuo-Yu Huang;Chia-Hui Chang;Kuo-Zui Lin	2004		10.1007/978-3-540-30116-5_49	computer science;data mining;database;temporal database;knowledge extraction;algorithm	DB	-3.4944989707688308	-32.98928825184197	121285
1e30d5229db01d051799a8d3a4c15cbfb3cb5554	preserving output-privacy in data stream classification		Privacy-preservation has emerged to be a major concern in devising a data mining system. But, protecting the privacy of data mining input does not guarantee a privacy-preserved output. This paper focuses on preserving the privacy of data mining output and particularly the output of classification task. Further, instead of static datasets, we consider the classification of continuously arriving data streams: a rapidly growing research area. Due to the challenges of data stream classification such as vast volume, a mixture of labeled and unlabeled instances throughout the stream and timely classifier publication, enforcing privacy-preservation techniques becomes even more challenging. In order to achieve this goal, we propose a systematic method for preserving output-privacy in data stream classification that addresses several applications like loan approval, credit card fraud detection, disease outbreak or biological attack detection. Specifically, we propose an algorithm named Diverse and k-Anonymized HOeffding Tree (DAHOT) that is an amalgamation of popular data stream classification algorithm Hoeffding tree and a variant of k-anonymity and l-diversity principles. The empirical results on real and synthetic data streams verify the effectiveness of DAHOT as compared to its bedrock Hoeffding tree and two other techniques, one that learns sanitized decision trees from sampled data stream and other technique that uses ensemble-based classification. DAHOT guarantees to preserve the private patterns while classifying the data streams accurately.	algorithm;concept drift;credit card fraud;data mining;decision tree;differential privacy;privacy;synthetic data;synthetic intelligence	Radhika Kotecha;Sanjay Garg	2017	Progress in Artificial Intelligence	10.1007/s13748-017-0114-8	computer science;artificial intelligence;data science;machine learning;data mining;database;data stream mining	ML	-1.443858871828168	-36.70094173394203	121302
c9723c81b2fd028365c9bf1c0f290967dd2ef51c	a framework for use of imprecise categorization in developing intelligent systems	categories;imprecise categorization;prototypes;intelligent systems development;pattern recognition fuzzy set theory fuzzy systems;satisfiability;data mining;fuzzy set theory;fuzzy categorization;machine intelligence;intelligent systems;intelligent system;partitions;statistics;pattern recognition;clustering algorithms;concept hierarchy;partitions categories concept hierarchy fragmentation;humans;intelligent systems prototypes humans laboratories machine intelligence context data mining clustering algorithms statistics pattern recognition;fragmentation;context;imprecise hierarchies;fuzzy systems;fragmentation imprecise categorization intelligent systems development imprecise hierarchies fuzzy categorization	In this paper, we develop characterizations of the properties of categories. We introduce an approach to representing imprecise hierarchies based on the idea of fragmentation. In the case of fuzzy categories, measures of how well fuzzy categorization satisfies the concept of a partitioning for such categories are developed. Finally, issues that are involved in the formulation of fuzzy categorization of a domain using approaches based on the use of prototypes are considered.	artificial intelligence;categorization;category theory;fragmentation (computing);fuzzy logic	Frederick E. Petry;Ronald R. Yager	2010	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2010.2041782	intelligent decision support system;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;fragmentation;prototype;fuzzy set;cluster analysis;fuzzy control system;categorization;satisfiability	Visualization	-0.6539119783174105	-26.96332276499548	121473
51d09a638622691ea35767db144bd4386fde1905	a neural network for tornado diagnosis: managing local minima	mlp;weather forecast;prevision meteorologica;tornado;systeme apprentissage;performance;diagnostico;weather forecasting;multicouche;tornade;multilayer perceptron;prevision meteorologique;multiple layer;local minimum;perceptron multicouche;learning systems;minimum local;capa multiple;multi layer perceptron;rendimiento;perceptron;reseau neuronal;local minima;diagnosis;algorithm design;red neuronal;neural network;diagnostic	There exist radar-based algorithms designed to detect circulations in the atmosphere. Not all detected circulations, however, are associated with tornados on the ground. Outlined herein is the development of a multi-layered perceptron designed to classify the two types of circulations – nontornadic and tornadic – based on various attributes of the circulations. Special emphasis is placed on the role of local minima in determining the optimal architecture via bootstrapping, and on the performance of the network in terms of probabilistic measures.	algorithm;artificial neural network;bootstrapping (compilers);maxima and minima;memory-level parallelism;mitchell corporation;perceptron;radar;tornado	C. Marzban	2000	Neural Computing & Applications	10.1007/s005210070024	simulation;computer science;artificial intelligence;machine learning;maxima and minima;multilayer perceptron;artificial neural network	Vision	9.539498519981981	-29.66304709137042	121631
cb70b00d22b97023716608ee15b8c8aad335a9c8	a hybrid system ensemble based time series signal classification on driver alertness detection	belief networks;bayesian network;support vector machines;bayesian network hybrid system ensemble time series signal classification driver alertness detection ijcnn 2011 ford challenge ii problem physiological data environmental data vehicular data temporal processing feature creation feature extraction neural networks random forest support vector machine machine learning;neural nets;feature extraction support vector machines training artificial neural networks time series analysis training data;traffic engineering computing belief networks feature extraction learning artificial intelligence neural nets signal classification support vector machines time series;time series;learning system;feature extraction;random forest;signal classification;hybrid system;temporal processing;traffic engineering computing;support vector machine;learning artificial intelligence;neural network	This paper presents the methodologies developed for solving IJCNN 2011's Ford Challenge II problem, where the driver's alertness is to be detected employing physiological, environmental and vehicular data acquired during driving. The solution is based on a thorough four-fold framework consisting of temporal processing, feature creation and extraction, and the training and ensemble of several learning systems, such as neural networks, random forest, support vector machine, trained from diverse features. The selection of input features to a learning machine has always been critique on signal classification. In our approach, the employment of Bayesian network filtered out a set of features and has been proved by the ensemble to be effective. The ensemble technique enhanced the performance of individual systems dramatically. The performance acquired on 30% of the test samples reached an accuracy of 78.34%. These results are significant for a real-world vehicular problem and we are quite confident this solution will become one of the top ones on the competition test data.	algorithmic efficiency;artificial neural network;bayesian network;computation;data acquisition;ensemble learning;experiment;feature extraction;hybrid system;learning classifier system;logic programming;pattern recognition;radio frequency;random forest;statistical classification;statistical model;support vector machine;test data;time series	Shen Xu;Ruoqian Liu;Dai Li;Yi Lu Murphey	2011	The 2011 International Joint Conference on Neural Networks	10.1109/IJCNN.2011.6033486	support vector machine;computer science;machine learning;pattern recognition;data mining;ensemble learning;artificial neural network	AI	8.756748465459335	-23.999676009410557	121904
6017fd4134dd8f86fbaf77375373b384a18b25d4	membership functions generation based on density function	neuro fuzzy systems;pattern clustering;fuzzy membership function;fuzzy set;expert knowledge acquisition density function membership functions generation fuzzy membership functions fuzzy systems clustering technique neuro fuzzy systems;core fuzzy sets membership function density function clustering;expert knowledge acquisition;fuzzy membership functions;fuzzy set theory;fuzzy sets;density functional theory;distance measurement;core;fuzzy rule base;marine vehicles;general methods;density function;clustering;knowledge acquisition;membership functions generation;neuro fuzzy system;membership function;clustering algorithms;expert knowledge;magnetic cores;clustering methods;clustering technique;density functional;fuzzy systems;fuzzy system;density functional theory fuzzy sets fuzzy systems fuzzy set theory clustering methods knowledge acquisition computational intelligence security fuzzy neural networks reflection;pattern clustering fuzzy set theory knowledge acquisition	Fuzzy membership functions are considered as a key element in fuzzy systems. In order to generate a fuzzy membership function, there are two potential sources: expert knowledge and real data. However expert knowledge acquisition is a difficult issue, on the other hand using real data needs a methodology to translate real data to membership function. Most previous approaches considered membership function design highly dependent of fuzzy rule base and require the specification of membership functions¿ number. This paper attempts to overcome these problems and proposes an automatic membership function generation method. Our approach is based on a clustering technique and a density function for deriving cores of fuzzy sets. Experimental results show that our approach generates large core region which is more preferable than small core region in the context of membership function generation for neuro-fuzzy systems.	cluster analysis;expert system;fuzzy control system;fuzzy rule;fuzzy set;knowledge acquisition;membership function (mathematics);neuro-fuzzy;rule-based system	Imen Derbel;Narjes Hachani;Habib Ounelli	2008	2008 International Conference on Computational Intelligence and Security	10.1109/CIS.2008.211	membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy set;fuzzy control system	EDA	2.9909046390413616	-28.245987340216544	121943
f31a0eb75576ec6c4708443d5261e16161da3743	on the description of relative position of fuzzy patterns	relative position;fuzzy set;ensemble flou;conjunto flojo	Abstract   A method is summarized which describes the relative position of two 2D crisp patterns by a fuzzy degree. Then, this method is generalized for 2D fuzzy patterns in a manner that fuzzy patterns are first transformed to crisp function and then this is handled as the cross section function of a crisp pattern. For this the above method is used.		László T. Kóczy	1988	Pattern Recognition Letters	10.1016/0167-8655(88)90019-0	fuzzy logic;discrete mathematics;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations	Vision	-0.8363734149755685	-24.66756941847494	122091
d4018cfa33675c90446b774ce4d725d670a1762f	inconsistency in fuzzy rulebase: measure and optimization	commonality measure;fuzzy rule inconsistency;fuzzy rule base optimization;fuzzy rule base	Rule inconsistency is an important issue that is needed to be addressed while designing efficient and optimal fuzzy rule bases. Automatic generation of fuzzy rules from data sets, using machine learning techniques, can generate a significant number of redundant and inconsistent rules. In this study we have shown that it is possible to provide a systematic approach to understand the fuzzy rule inconsistency problem by using the proposed measure called the Commonality measure. Apart from introducing this measure, this paper describes an algorithm to optimize a fuzzy rule base using it. The optimization procedure performs elimination of redundant and/or inconsistent fuzzy rules from a rule base.	program optimization	Shounak Roychowdhury;Bo-Hyeun Wang	2001	Int. J. Fuzzy Logic and Intelligent Systems		discrete mathematics;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	Logic	1.0387647773197348	-27.787110768246762	122212
51f52dda13eabc6fc989db522f1e2acb0025e557	a rough set framework for data mining of propositional default rules	data mining;rough set;decision rule	As the amount of information in the world is steadily increasing, there is a growing demand for tools for analysing the information. In this paper we investigate the problem of data mining, that is, constructing decision rules from a set of primitive input data. The main contention of the present work is that there is a need to be able to reason also in presence of inconsistencies, and that more general, possibly unsafe rules should be made available through the data mining process. Such rules are typically simpler in structure, and allow the user to reason in absence of information. A framework is suggested for the generation of propositional default rules that reeect normal intradependencies in the data.	data mining;rough set	Torulf Mollestad;Andrzej Skowron	1996		10.1007/3-540-61286-6_169	rough set;computer science;machine learning;pattern recognition;data mining;decision rule;dominance-based rough set approach	ML	-4.367647593259791	-29.363575532072048	122316
efba8167f99be86d779d16b372349282f4a54f92	a review on machine learning and data mining techniques for residential energy smart management	support vector machines;home appliances;data mining;hidden markov models;monitoring;energy consumption;load modeling	In this paper, the different machine learning and data mining approaches used for Residential Energy Smart Management (RESM) will be discussed and classified according to some meaningful criteria. The proposed classification is an attempt to highlight the advantages and limitations of each category. Moreover, we emphasize the complementarity between approaches belonging to different categories and we point out the main challenges that still face RESM.	complementarity theory;data mining;machine learning	Hajer Salem;Moamar Sayed Mouchaweh;Ahlem Ben Hassine	2016	2016 15th IEEE International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2016.0195	support vector machine;computer science;data science;machine learning;data mining;hidden markov model	Robotics	1.2994421210976557	-33.181750791150364	122324
8aa39825bbea376eb8c0cfd2da5b31d7fd1e4f6f	case-based reasoning with optimized weight derived by particle swarm optimization for software effort estimation		Software effort estimation (SEE) is the process of forecasting the effort required to develop a new software system, which is critical to the success of software project management and plays a significant role in software management activities. This study examines the potentials of the SEE method by integrating particle swarm optimization (PSO) with the case-based reasoning (CBR) method, where the PSO method is adopted to optimize the weights in weighted CBR. The experiments are implemented based on two datasets of software projects from the Maxwell and Desharnais datasets. The effectiveness of the proposed model is compared with other published results in terms of the performance measures, which are MMRE, Pred(0.25), and MdMRE. Experimental results show that the weighed CBR generates better software effort estimates than the unweighted CBR methods, and PSO-based weighted grey relational grade CBR achieves better performance and robustness in both datasets than other popular methods.	case-based reasoning;cost estimation in software engineering;mathematical optimization;particle swarm optimization;software development effort estimation	Dengsheng Wu;Jianping Li;Chunbing Bao	2018	Soft Comput.	10.1007/s00500-017-2985-9	robustness (computer science);artificial intelligence;software system;machine learning;computer science;software;case-based reasoning;particle swarm optimization;software project management	EDA	3.438080048382982	-33.370040407764016	122569
0a26263972a2ce4cf335689d8d0b2074f32ad4c0	the effects of variable selection methods on linear regression-based effort estimation models	software metrics;input variables estimation linear regression software training testing;software cost estimation;software selection regression analysis software cost estimation software metrics;stepwise regression;variable selection;predictive performance linear regression based effort estimation models stepwise regression variable selection methods promise repository lasso based selection;effort estimation;lasso effort estimation variable selection stepwise regression;regression analysis;lasso;software selection	Stepwise regression has often been used for variable selection of effort estimation models. However it has been criticized for inappropriate selection, and another method is recommended. We thus examined the effects of Lasso, which is one of such variable selection methods. An experiment with datasets from PROMISE repository revealed that Lasso-based selection stably selected better variables than stepwise in predictive performance. We thus concluded Lasso-based selection is preferable to stepwise regression.	cost estimation in software engineering;experiment;feature selection;lasso;selection (user interface);software quality;stepwise regression	Sousuke Amasaki;Tomoyuki Yokogawa	2013	2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement	10.1109/IWSM-Mensura.2013.24	econometrics;computer science;pattern recognition;feature selection;statistics	SE	4.085763973738503	-33.737159600499076	122754
684beb56908671df251226c437611ce993a997ec	the kohonen algorithm: a powerful tool for analyzing and representing multidimensional quantitative and qualitative data	qualitative data;multidimensional data;non linear model;data analysis;linear model;general linear model;kohonen map;neural network	The simultaneous analysis of quantitative and qualitative variables is not an easy task in general. When a linear model is appropriate, the Generalized Linear Models are commonly used with success. But when the intrinsic structure of the data is not at all linear, they give very poor and confusing results. In this paper, we extensively study how to use the (non linear) Kohonen maps to solve some of the interesting problems which are encountered in data analysis: how to realize a rapid and robust classification based on the quantitative variables, how to visualize the classes, their differences and homogeneity, how to cross the classification with the remaining qualitative variables to interpret the classification and put in evidence the most important explanatory variables.	algorithm;teuvo kohonen	Marie Cottrell;Patrick Rousset	1997		10.1007/BFb0032546	econometrics;qualitative property;computer science;machine learning;linear model;data mining;data analysis;artificial neural network;statistics;general linear model	Logic	8.951045560120477	-34.568890907297764	122821
1c45ef5b2f939ad2cd4bc9181468360197ee83ef	an intelligent customer retention system	modelizacion;automatic;ontologie;base donnee;abonado;volatility;systeme intelligent;competition;decision tree;validacion cruzada;telecommunication sans fil;relation client fournisseur;retention;handling;sistema inteligente;abonne;database;base dato;intelligence artificielle;automatico;manutention;customer retention;manutencion;probabilistic approach;arbol decision;modelisation;test preliminaire;preliminary test;subscriber;enfoque probabilista;telecomunicacion sin hilo;approche probabiliste;validation croisee;intelligent system;automatique;relacion cliente proveedor;artificial intelligence;ontologia;volatilite;test preliminar;prediction model;cross validation;inteligencia artificial;modele amas;cluster model;volatibilidad;modeling;modem;ontology;arbre decision;retencion;competencia;wireless telecommunication;supplier customer relationship	This paper proposes an intelligent system for handling the customer retention task, which is getting important due to keen competition among companies in many modern industries. Taking wireless telecommunication industry as a target of research, our system first learns an optimized churn predictive model from a historical services database by the decision tree-based technique to support the prediction of defection probability of customers. We then construct a retention policy model which maps clusters of churn attributes to retention policies structured in a retention ontology. The retention policy model supports automatic proposing of suitable retention policies to retain a possible churner provided that he or she is a valuable subscriber. Our experiment shows the learned churn predictive model has around 85% of correctness in tenfold cross-validation. And a preliminary test on proposing suitable package plans shows the retention policy model works equally well as a commercial website. The fact that our system can automatically propose proper retention policies for possible churners according to their specific characteristics is new and important in customer retention study.	artificial intelligence;correctness (computer science);cross-validation (statistics);decision tree;ontology (information science)	Bong-Horng Chu;Kai-Chung Hsiao;Cheng-Seen Ho	2006		10.1007/11779568_133	simulation;competition;systems modeling;volatility;computer science;artificial intelligence;operating system;machine learning;decision tree;ontology;database;predictive modelling;customer retention;automatic transmission;computer security;cross-validation	ML	6.632069819521903	-32.912500125127885	122868
23e9095ba950e6047fa022534ce5803e406a99fb	ensemble neural networks with fuzzy logic integration for complex time series prediction	neural networks;weighted averaging;simulation;time series;fuzzy integral;fuzzy logic;weighted average;fuzzy integration;time series prediction;neural network;ensemble neural model	In this paper the application of an ensemble neural network model for complex time series prediction is presented. The Mackey-Glass time series is considered for testing the ensemble model. Simulation results of the ensemble neural network and its integration with the methods of average, weighted average and fuzzy integration are presented. Simulation results show very good prediction of the ensemble neural network with the method of fuzzy integration.	artificial neural network;fuzzy logic;time series	Martha Pulido;Alejandra Mancilla;Patricia Melin	2010	IJIEI	10.1504/IJIEI.2010.033531	adaptive neuro fuzzy inference system;computer science;neuro-fuzzy;machine learning;time series;pattern recognition;data mining;time delay neural network;artificial neural network;statistics	ML	4.8437514373384625	-25.929083370795432	122948
bf0d74d2c2a7612e84bbf94901b14e21f72e25b4	a structural learning algorithm and its application to predictive toxicology evaluation	structure learning;modelizacion;learning algorithm;intelligence artificielle;algorithme apprentissage;classification;modelisation;pattern recognition;artificial intelligence;inteligencia artificial;reconnaissance forme;reconocimiento patron;algoritmo aprendizaje;modeling;clasificacion	A common problem encountered in structural pattern recognition is the difficulty of constructing classification models or rules from a set of examples, due to the complexity of the structures needed to represent the patterns. In this paper we present an extension of a method for structural learning applied to predictive toxicology evaluation.	algorithm	Pasquale Foggia;Michele Petretta;Francesco Tufano;Mario Vento	2005		10.1007/11565123_28	systems modeling;biological classification;computer science;artificial intelligence;machine learning;algorithm	NLP	8.755939631304953	-31.565516920037883	123107
60a33e32f30740fa0ed33a33387aaa5d00b0dc22	analyzing intrusion detection system: an ensemble based stacking approach	intrusion;probes;stacking;security of data pattern classification program verification;classifiers;cross validation;tv;cross validation intrusion detection system analysis application software intrusive element hostile element security threat ids model stacking classifier intrusion attack kdd cup 99 dataset;cross validation intrusion classifiers stacking;tv probes	Intrusion Detection System (IDS) is an application software which detects the presence of hostile or intrusive elements inside the system. As the nature and type of the intrusions are continuously changing, a simple IDS cannot completely tackle the security threat. In this paper, we have proposed an IDS model, which classifies different types of intrusion attacks based on Stacking classifier. Stacking is an ensemble based classifier. We have achieved good accuracy while classifying the KDD-Cup 99 dataset and that has been achieved with 10 fold cross validation.	cross-validation (statistics);intrusion detection system;stacking;statistical classification	Sanjiban Sekhar Roy;P. Venkata Krishna;Sumanth Yenduri	2014	2014 IEEE International Symposium on Signal Processing and Information Technology (ISSPIT)	10.1109/ISSPIT.2014.7300605	anomaly-based intrusion detection system;intrusion;stacking;data mining;world wide web;computer security;cross-validation	Embedded	6.734294428106454	-37.420403588332825	123149
1e59e5848f5588aa19253b3d94008738cf370f51	nonstationary time series analysis by temporal clustering	pattern clustering;time varying;time series;fuzzy set theory;temporal membership matrix time series analysis temporal clustering nonstationary time series fuzzy clustering continuous drift;fuzzy clustering;nonstationary time series;time series analysis hidden markov models clustering algorithms electroencephalography parameter estimation probability distribution pattern analysis epilepsy data mining clustering methods;temporal pattern;cluster validity;probability distribution function;fuzzy set theory time series pattern clustering	The object of this paper is to present a model and a set of algorithms for estimating the parameters of a nonstationary time series generated by a continuous change in regime. We apply fuzzy clustering methods to the task of estimating the continuous drift in the time series distribution and interpret the resulting temporal membership matrix as weights in a time varying, mixture probability distribution function (PDF). We analyze the stopping conditions of the algorithm to infer a novel cluster validity criterion for fuzzy clustering algorithms of temporal patterns. The algorithm performance is demonstrated with three different types of signals.		Shai Policker;Amir B. Geva	2000	IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society	10.1109/3477.836381	correlation clustering;probability density function;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;canopy clustering algorithm;machine learning;time series;consensus clustering;pattern recognition;cure data clustering algorithm;mathematics;fuzzy set;cluster analysis;biclustering;statistics;clustering high-dimensional data	DB	2.0926672822512833	-36.81741021531196	123197
0c0bf9cecd978a22e8b51d202d88f3a47c1c7bf1	a novel hybrid intelligent system: genetic algorithm and rough set incorporated neural fuzzy inference system	dynamic change;optimal solution;fuzzy neural nets;fuzzy membership function;evolutionary computation;hybrid intelligent systems;rough set theory;rough set system;inference mechanisms;fuzzy membership functions;hybrid intelligent system;fuzzy set theory;fuzzy sets;neural fuzzy inference system;fuzzy membership functions hybrid intelligent system genetic algorithm rough set system neural fuzzy inference system rough set clustering technique knowledge reduction rough set approximations structural interpretability;fuzzy inference system;genetic algorithm;genetic algorithms;structural interpretability;rough set approximations;network structure;rough set;rough set clustering technique;fuzzy systems;knowledge based systems;fuzzy system;rough set theory fuzzy neural nets fuzzy set theory genetic algorithms inference mechanisms knowledge based systems;knowledge reduction	This paper proposes a novel hybrid intelligent system denoted as genetic algorithm and rough set incorporated neural fuzzy inference system (GARSINFIS). Its network structure dynamically changes along with the evolving genetic algorithm based rough set clustering (GARSC) technique. When input data set is applied, only the most essential information is retained in the clustering result, as knowledge reduction is done using rough set approximations and the most optimal solution is selected by genetic algorithm. The system not only obtains promising accuracy but also possesses a great level of interpretability to meet the increasing need of understanding the inference process. In terms of TSK type of fuzzy inference system, better structural interpretability is typically manifested as employing less number of input features, less number of rules, less number of fuzzy membership functions in each feature, and less complex rules in both antecedent and consequent parts. Extensive simulations on various data sets were conducted, and the performance of GARSINFIS was benchmarked against other well established neural and neural-fuzzy systems. Experimental results have shown that GARSINFIS performs well in both accuracy and interpretability.	approximation;artificial intelligence;benchmark (computing);cluster analysis;fuzzy control system;genetic algorithm;hybrid intelligent system;inference engine;rough set;simulation	Di Wang;Geok See Ng;Hiok Chai Quek	2008	2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence)	10.1109/CEC.2008.4631140	genetic algorithm;rough set;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy control system	AI	4.413437688646339	-29.20205068532192	123313
92936f58aaeafe5dd04cca035d7f0df1950f3d02	modified art1 neural networks for cell formation using production data	production data;bridges usa councils automation conferences;cluster algorithm;art1 neural network algorithm;pattern clustering;grouping efficiency cell formation adaptive resonance theory networks k means clustering;machine allocation;cellular manufacturing art1 neural network algorithm adaptive resonance theory disjoint machine cell formation production data machine allocation membership index modified grouping efficiency k means clustering algorithm genetic algorithm;bridges;usa councils;production engineering computing;grouping efficiency;disjoint machine cell formation;single machine;modified grouping efficiency;cell formation;indexation;membership index;adaptive resonance theory networks;k means clustering algorithm;genetic algorithm;production engineering computing art neural nets cellular manufacturing pattern clustering;art neural nets;k means clustering;cellular manufacturing;conferences;adaptive resonance theory;neural network;automation	In the present work, an attempt has been made to form disjoint machine cells using modified ART1 (adaptive resonance theory) to handle the real valued workload matrix. The methodology first allocates the machines to various machine cells and then parts are assigned to those cells with the aid of degree of belongingness through a membership index. The proposed algorithm uses a supplementary procedure to effectively take care of the problem of generating cells with single machine that may be encountered at times. A modified grouping efficiency (MGE) is proposed to measure the performance of the clustering algorithm. The results of modified ART1 algorithm are compared with the results obtained from K-means clustering and genetic algorithm. The modified ART1 results are also compared with the literature results in terms of number of exceptional elements. The performance of the proposed algorithm is tested with genetic algorithm and K-means clustering algorithm. The results distinctly indicate that the proposed algorithm is quite flexible, fast and efficient in computation for cell formation problems and can be applied in industries with convenience.	adaptive resonance theory;artificial neural network;care-of address;cluster analysis;computation;genetic algorithm;k-means clustering	S. G. Ponnambalam;R. Sudhakara Pandian;Siba Sankar Mahapatra;Saravana S. Sankar	2008	2008 IEEE International Conference on Automation Science and Engineering	10.1109/COASE.2008.4626507	computer science;artificial intelligence;canopy clustering algorithm;machine learning;data mining;fsa-red algorithm;k-medoids;population-based incremental learning	Robotics	4.514784539489024	-35.296610894841734	123417
9ad59743196c227d976869765ae7131777d99067	learning fuzzy classification rules from labeled data	linguistic model;fuzzy classification;compact fuzzy classifier;wine data;rule based;linguistic modeling;fuzzy rule base;classification system;genetic algorithm;similarity driven rule base reduction;parameter optimization;fuzzy classifier	The automatic design of fuzzy rule-based classification systems based on labeled data is considered. It is recognized that both classification performance and interpretability are of major importance and effort is made to keep the resulting rule bases small and comprehensible. For this purpose, an iterative approach for developing fuzzy classifiers is proposed. The initial model is derived from the data and subsequently, feature selection and rule base simplification are applied to reduce the model, while a Genetic Algorithm is used for parameter optimization. An application to the Wine data classification problem is shown.	feature selection;fuzzy classification;fuzzy concept;fuzzy rule;genetic algorithm;iterative method;level of detail;logic programming;mathematical optimization;rule-based system;software release life cycle;statistical classification	Johannes A. Roubos;Magne Setnes;János Abonyi	2003	Inf. Sci.	10.1016/S0020-0255(02)00369-9	rule-based system;genetic algorithm;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy associative matrix;fuzzy set operations	AI	4.487184067902621	-27.98175402114763	123476
11e025bd36449344add5466e1f3ea68534f63404	discretization: an enabling technique	tecnologia electronica telecomunicaciones;discretization;data mining;classification;machine learning;prediction accuracy;tecnologias;grupo a;continuous feature;knowledge discovery	Discrete values have important roles in data mining and knowledge discovery. They are about intervals of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from discretization: rules with discrete values are normally shorter and more understandable and discretization can lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require discrete features. All these prompt researchers and practitioners to discretize continuous features before or during a machine learning or data mining task. There are numerous discretization methods available in the literature. It is time for us to examine these seemingly different methods for discretization and find out how different they really are, what are the key components of a discretization process, how we can improve the current level of research for new development as well as the use of existing methods. This paper aims at a systematic study of discretization methods with their history of development, effect on classification, and trade-off between speed and accuracy. Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical framework to categorize the existing methods and pave the way for further development, concise discussions of representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to choose a discretization method under various circumstances. We also identify some issues yet to solve and future research for discretization.	c4.5 algorithm;categorization;computer cluster;data mining and knowledge discovery;decision tree;discretization;experiment;id3 algorithm;machine learning;mathematical induction;parallel computing;relevance;supervised learning;thresholding (image processing);time complexity	Huan Liu;Farhad Hussain;Chew Lim Tan;Manoranjan Dash	2002	Data Mining and Knowledge Discovery	10.1023/A:1016304305535	biological classification;computer science;data science;machine learning;discretization;data mining;knowledge extraction;discretization of continuous features;statistics	ML	0.5605310791771385	-33.05288921373788	123499
8bdb0ccb0c6eaa6cb92d110e2652bbeed6531a54	hybrid electric vehicles: application of fuzzy clustering for designing a tsk-based fuzzy energy flow management unit	gestion energia;fuzzy c mean;analyse amas;transportes;environmental impact;conduccion vehiculo;fuzzy rules;logique floue;conduite vehicule;logica difusa;vehicle driving;hybrid vehicle;desir;classification;deseo;fuzzy logic;identificacion sistema;energy flow;transports;fuzzy clustering;flux puissance;cluster analysis;first order;gestion energie;route;system identification;vehicule electrique;vehicule hybride;hybrid electric vehicle;transportation;fuzzy inference;inferencia;fuzzy inference system;carretera;non linearite;no linealidad;takagi sugeno kang;nonlinearity;analisis cluster;vehiculo electrico;power flow;vehiculo hibrido;highway;flujo potencia;electric vehicle;desire;clasificacion;identification systeme;inference;fuzzy model;energy management	Today, the satisfaction of the desire for personal transportation requires developing vehicles that minimize the consequences on the environment and maximize highway and fuel resources. Hybrid electric vehicles (HEVs) could be an answer to this demand. Their use can contribute significantly to reduce their environmental impact, achieving at the same time a rational energy employment. Controlling an HEV requires a lot of experimentations. Experts and training engineers can ensure the good working of the powertrains, but the research of optimality for some criteria combining fuel needs and power requirements is mainly empirical due to the nonlinearity of the driving conditions and vehicle loads. Consequently, in the paper a fuzzy modeling identification approach is applied for modeling the power flow management process. Amongst the various methods for the identification of fuzzy model structure, fuzzy clustering is selected to induce fuzzy rules. With such an approach the fuzzy inference system (FIS) structure is generated from data using fuzzy C-Means (FCM) clustering technique. As model type for the FIS structure a first order Takagi-Sugeno-Kang (TSK) model is considered. From this architecture a fuzzy energy flow management unit based on a TSK-type fuzzy inference is derived. Further, some interesting comparisons and simulations are discussed to prove the validity of the methodology.	fuzzy clustering	Lucio Ippolito;Pierluigi Siano	2003		10.1007/3-540-44967-1_62	fuzzy logic;route;transport;power-flow study;hybrid vehicle;defuzzification;system identification;fuzzy clustering;adaptive neuro fuzzy inference system;nonlinear system;fuzzy transportation;biological classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;first-order logic;energy flow;cluster analysis;fuzzy set operations;fuzzy control system;environmental impact assessment;energy management	HCI	8.681629505589033	-24.748675374523902	123784
b74d6f2ad5a3f01a5e82d9d8878b3bcec9810e80	a comparison of machine learning techniques for phishing detection	phishing;cart;regression tree;nnet;logistic regression;classification;random forests;machine learning;random forest;comparative study;prediction accuracy;svm;bart;neural network	There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.	additive model;artificial neural network;bayesian network;decision tree learning;email;lr parser;logistic regression;machine learning;phishing;radio frequency;random forest;spamming;statistical classification;support vector machine	Saeed Abu-Nimeh;Dario Nappa;Xinlei Wang;Suku Nair	2007		10.1145/1299015.1299021	computer science;machine learning;pattern recognition;data mining	ML	7.616178442537545	-37.412243978091055	123956
2e111087a044c8b00a43835766b6a5729bee39e4	design of fuzzy systems using neurofuzzy networks	control application;fuzzy reasoning principles;fuzzy reasoning;learning;etude theorique;brazil council;neural nets;noisy data;fuzzy rules;performance comparison;base connaissance;conception;inference mechanisms;complexity;indexing terms;design optimization;fuzzy sets;universal approximation capability;input output;aprendizaje;fuzzy logic;general neuron model;accuracy;apprentissage;if then fuzzy rules;shape;design knowledge;function approximation;computer experiment;fuzzy system design;computational complexity;system design;neurofuzzy networks;fonction appartenance;membership functions;estudio teorico;membership function;fuzzy systems fuzzy reasoning fuzzy neural networks design methodology shape function approximation fuzzy sets brazil council design optimization neurons;diseno;complexity neurofuzzy networks fuzzy system design general neuron model if then fuzzy rules fuzzy reasoning principles membership functions universal approximation capability accuracy;base conocimiento;design;sistema difuso;neurons;systeme flou;funcion pertenencia;network structure;theoretical study;reseau neuronal;point of view;fuzzy neural networks;red neuronal;fuzzy systems;knowledge based systems;neural nets fuzzy systems function approximation computational complexity knowledge based systems fuzzy logic inference mechanisms;fuzzy system;neural network;design methodology;knowledge base	This paper introduces a systematic approach for fuzzy system design based on a class of neural fuzzy networks built upon a general neuron model. The network structure is such that it encodes the knowledge learned in the form of if-then fuzzy rules and processes data following fuzzy reasoning principles. The technique provides a mechanism to obtain rules covering the whole input/output space as well as the membership functions (including their shapes) for each input variable. Such characteristics are of utmost importance in fuzzy systems design and application. In addition, after learning, it is very simple to extract fuzzy rules in the linguistic form. The network has universal approximation capability, a property very useful in, e.g., modeling and control applications. Here we focus on function approximation problems as a vehicle to illustrate its usefulness and to evaluate its performance. Comparisons with alternative approaches are also included. Both, nonnoisy and noisy data have been studied and considered in the computational experiments. The neural fuzzy network developed here and, consequently, the underlying approach, has shown to provide good results from the accuracy, complexity, and system design points of view.	approximation;biological neuron model;computation;experiment;fuzzy control system;fuzzy rule;input/output;linguistics;neurons;rule (guideline);signal-to-noise ratio;systems design	Maurício Figueiredo;Fernando A. C. Gomide	1999	IEEE transactions on neural networks	10.1109/72.774229	fuzzy logic;design;complexity;computer experiment;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;function approximation;fuzzy transportation;shape;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;fuzzy set operations;artificial neural network;algorithm;fuzzy control system	ML	4.963961854418264	-27.517192883973006	124089
184afe50a0270a023ee66e4aab1e8530fa8962d0	analytic study of fuzzy-based model for software cost estimation		The need for successful software projects has been a major area of discourse amongst researchers and software developers in academia and software industry respectively. Failure of software projects has been tied to flawed estimation at the early stages of software development life cycle. Recently, soft computing techniques such as Fuzzy logic models has been seen as an alternative to handle uncertainties and vagueness of input parameters to the early software estimation models. In order to analyze the various conditions which affect estimation accuracy of fuzzy-based models, a sample of 93 COCOMO NASA projects was used to develop two groups of fuzzy models. One was the controlled group while the other was the experimental group varying in conditions of model structure, linguistic variables, parameters of input and output variables. A comparative analysis of the Mean Magnitude of Relative Error (MMRE) and Prediction accuracy Pred(l) evaluation criteria for the models was made and findings recorded. Results from the experiments show that the performance of a fuzzy-based software cost estimation model utilizing Takagi-Sugeno inference, Gaussian/Sigmoid membership function with more number of input variables and linguistics variables is more efficient. CCS Concepts • Software and its engineering ➝Software system structures ➝Software system models ➝Model-driven software engineering • Computing methodology➝Artificial Intelligence Knowledge representation and reasoning➝Vagueness and fuzzy logic	approximation error;artificial intelligence;cocomo;common criteria;cost estimation in software engineering;defuzzification;experiment;fuzzy concept;fuzzy logic;in-house software;inference engine;input/output;knowledge representation and reasoning;membership function (mathematics);model-driven engineering;qualitative comparative analysis;serial ata;sigmoid function;soft computing;software developer;software development effort estimation;software development process;software industry;software system;vagueness	John Chibuike Nwaiwu;Samuel Adebayo Oluwadare	2016			fuzzy logic;data mining;software;cocomo;cost estimate;reliability engineering;computer science	SE	3.398644887137058	-33.28156720863435	124319
6f42193de52c530388de53425a970169ca676387	hierarchical temporal memory method for time-series-based anomaly detection	measurement;detection algorithms;prediction algorithms;robustness;neurons;rocks;algorithm design and analysis	Time-series-based anomaly detection is a quite important field that has been researched over years. Many techniques have been developed and applied successfully for certain application domains. However, there are still some challenges, such as continuously learning, tolerance to noise and generalization. This paper present Hierarchical Temporal Memory, a novel biological neural network, to time-series-based anomaly detection. HTM is able to learn the changing pattern of the data and incorporate contextual information from the past to make accurate prediction. We have evaluated HTM on real and artificial datasets. The experiment results show that HTM can successfully discover anomalies in time-series data.	anomaly detection;hierarchical temporal memory	Jia Wu;Weiru Zeng;Zhe Chen;Xue-Fei Tang	2016		10.1109/ICDMW.2016.0168	algorithm design;prediction;computer science;artificial intelligence;machine learning;data mining;mathematics;measurement;statistics;robustness	ML	5.45799449181461	-35.81248802121202	124326
2e8bcb8c0edcba1044f68913974030030036c54b	a hybrid particle swarm optimization and neural network with fuzzy membership function technique for epileptic seizure classification	epileptic seizure detection;particle swarm optimization;fuzzy membership;eeg signal;neural network		artificial neural network;particle swarm optimization	Khaled A. Abuhasel;Abdullah M. Iliyasu;Chastine Fatichah	2015	JACIII	10.20965/jaciii.2015.p0447	computer science;artificial intelligence;machine learning;pattern recognition;particle swarm optimization;artificial neural network	ML	4.839861425730743	-26.141980602139693	124377
e1e40efcfc9f6666176df9cc6264493fb9e462c7	a framework for learning rules from multiple instance data	multiple instance;learning algorithm;decision tree;supervised learning;error sistematico;algorithme apprentissage;arbol decision;rule learning;resolucion problema;feature vector;induccion;first order;induction;bias;apprentissage supervise;learning artificial intelligence;algoritmo aprendizaje;arbre decision;problem solving;resolution probleme;erreur systematique;apprentissage intelligence artificielle	"""This paper proposes a generic extension to propositional rule learners to handle multiple-instance data. In a multiple-instance representation, each learning example is represented by a \bag"""" of fixed-length \feature vectors"""". Such a representation, lying somewhere between propositional and first-order representation, offers a tradeoff between the two. Naive-RipperMi is one implementation of this extension on the rule learning algorithm Ripper. Several pitfalls encountered by this naive extension during induction are explained. A new multiple-instance search bias based on decision tree techniques is then used to avoid these pitfalls. Experimental results show the benefits of this approach for solving propositionalized relational problems in terms of speed and accuracy."""	algorithm;decision tree;field (computer science);first-order predicate;first-order reduction;machine learning;mathematical induction;mobile robot;multiple-instance learning;naive bayes classifier;progol;programming paradigm;refinement (computing);relational data mining;ripper;tilde;whole earth 'lectronic link	Yann Chevaleyre;Jean-Daniel Zucker	2001		10.1007/3-540-44795-4_5	feature vector;computer science;artificial intelligence;machine learning;decision tree;bias;first-order logic;mathematics;supervised learning;algorithm	AI	8.397077446610664	-32.260740863430264	124448
02b37d1d5a407041e18c064a9f89f7c6fc417026	fuzzy sets and similarity measure between fuzzy concepts in semantic nets	fuzzy set		fuzzy set;petri net;semantic network;similarity measure	Mohamed Nazih Omri;Mohamed Ali Mahjoub	2002			neuro-fuzzy;fuzzy logic;fuzzy set operations;fuzzy set;fuzzy associative matrix;defuzzification;machine learning;pattern recognition;fuzzy classification;artificial intelligence;membership function;mathematics	DB	2.3053871116107856	-25.37046294634144	124492
0d626028184f67039edf31e039aa1aea2a8735e7	a metalevel architecture for knowledge-based neural network design	it strategy;knowledge lean domains metalevel architecture knowledge based neural network design neural network configuration training scandal multi agent systems domain knowledge performance experiments knowledge based techniques search based configuration algorithm learning generalization time consuming iterative search;performance evaluation;neural networks;learning;knowledge acquisition neural net architecture knowledge based systems learning artificial intelligence cooperative systems software agents performance evaluation search problems generalisation artificial intelligence;training;performance;knowledge lean domains;gratings;search based configuration algorithm;neural net architecture;prior knowledge;knowledge based neural network design;metalevel architecture;data mining;neural network configuration;domain knowledge;software agents;network topology;neural networks machine learning network topology gratings data mining learning systems production decision trees training data buildings;learning systems;training data;multi agent systems;cooperative systems;machine learning;knowledge acquisition;experiments;knowledge based techniques;production;search problems;generalisation artificial intelligence;learning artificial intelligence;decision trees;time consuming iterative search;generalization;knowledge based systems;buildings;scandal;neural network;knowledge base	In this paper, the term knowledge-based neural network (NN) design is used to refer to all efforts at exploiting prior knowledge in neural network configuration and training. A variety of techniques have been proposed for this purpose; SCANDAL provides a workbench for evaluating and integrating these techniques. After a quick overview of three main approaches to NN design, we describe SCANDALS multi-agent, metalevel architecture as well as its strategies for maximizing the use of domain knowledge. To assess the impact of prior knowledge on NN performance, experiments were conducted comparing knowledge-based techniques with a search-based configuration algorithm. Results show that the use of prior knowledge in neural network design leads to both faster learning and improved generalization. More interestingly, this appears to hold even when domain knowledge and data are deficient; in such cases, knowledge is extracted from the available data and is used both to configure the network and to generate artificial training instances. This leads us to hope that time-consuming iterative search can be avoided even in knowledge-lean domains.	artificial neural network	Melanie Hilario;Ahmed Rida;Christian Pellegrini	1997		10.1109/TAI.1997.632273	generalization;training set;performance;computer science;artificial intelligence;software agent;knowledge-based systems;machine learning;decision tree;data mining;knowledge extraction;domain knowledge;artificial neural network;network topology	Arch	5.877038299766151	-30.930207936901173	124545
d2ad36b145926cc20d0c25b3f2b260f44840bf58	fpga hardware implementation of smart home autonomous system based on deep learning		The use of deep learning algorithms, as a core element of artificial intelligence, has attracted increased attention from industrial and academic institutes recently. One important use of deep learning is to predict the next user action inside an intelligent home environment that is based on Internet of Things (IoT). Recent researcher discusses the benefit of using deep learning based on different datasets to assist their result. However, assuring the best performance to satisfy real-time applications leads us to use a real-world dataset to make sure that the designed system meets the requirements of real-time applications. This paper uses the MavPad dataset which was gathered from distributed sensors and actuators in a real-world environment. The authors use simulation to investigate the performance of a multilayer neural network that predicts future human actions. The authors also present a hardware implementation of the deep learning model on an FPGA. The results showed that the hardware implementation demonstrated similar accuracy with significantly improved performance compared to the software-based implementation due to the exploitation of parallel computing and using optimization techniques to map the designed system into the target device. Additionally, our implementation of FPGA-based neural network system supports its future utilization for other applications.	autonomous system (internet);deep learning;field-programmable gate array;home automation	Basman M. Hasan Alhafidh;Amar I. Daood;Mohammed M. Alawad;William H. Allen	2018		10.1007/978-3-319-94370-1_9	computer hardware;field-programmable gate array;deep learning;software;artificial neural network;home automation;smart environment;autonomous system (mathematics);computer science;artificial intelligence;middleware	EDA	3.7954222547477783	-35.479819744934716	124630
59b5c6aa10e8377081420d1cdf029cd9306a1a54	mining frequent itemsets in data streams based on genetic algorithm	itemsets;memory space data type big data data streams applications frequent pattern mining data set frequent itemsets mining nonlinear optimization problem genetic algorithm bitmap representation 0 1 programming apriori algorithm time complexity;itemsets genetic algorithms sociology statistics biological cells data mining;frequent itemsets big data data streams genetic algorithm;data mining;biological cells;statistics;genetic algorithms;nonlinear programming big data computational complexity data mining genetic algorithms;sociology	Stream data is a very common data type in big data and in many data streams applications, users tend to pay more attention to the mode information of the data streams. So mining frequent patterns in data streams is a significative work. Meanwhile, finding frequent itemests in a data set with predefined fixed support threshold could be seen as an optimization problem. In this paper, the problem of frequent itemsets mining is derived as a non-linear optimization problem, then genetic algorithm is adopted to solve it. Through the formal and bitmap representation of frequent itemsets, the non-linear optimization problem is transformed to 0-1 programming. A set of experimental results show that unlike typical Apriori algorithm, the complexity of time and memory space grows exponentially as the support decrease, our proposed algorithm has a high time and space efficiency even with a very low support.	apriori algorithm;big data;bitmap;dspace;data mining;fitness function;genetic algorithm;global variable;iteration;linear programming;mathematical optimization;nonlinear programming;nonlinear system;optimization problem;population;randomness;requirement;text mining	Chong Han;Lijuan Sun;Jian Guo;Xiaodong Chen	2013	2013 15th IEEE International Conference on Communication Technology	10.1109/ICCT.2013.6820474	computer science;data science;data mining;database;fsa-red algorithm;data stream mining	DB	-3.691710224243155	-36.76282945636984	124643
bbaea8e2785ffde5687204b6a89b8442acc0cf19	record-level peculiarity-based data analysis and classifications	bayes estimation;densite probabilite;bayesian classifier;analisis estadistico;probability density;modele agrege;supervised learning;lpf bayes classifier;analisis datos;methode noyau;competitividad;probabilidad condicional;anomaly detection;probabilite conditionnelle;new record;modelo agregado;outlier;informacion fisher;peculiarity factor;data mining;densidad probabilidad;observacion aberrante;classification a vaste marge;vecino mas cercano;data analysis;estimacion bayes;statistical analysis;fouille donnee;kernel fisher discriminant;metodo nucleo;nearest neighbor;analyse statistique;competitiveness;aggregate model;local peculiarity factor;observation aberrante;plus proche voisin;analyse donnee;nearest neighbour;kernel method;support vector machine;apprentissage supervise;maquina ejemplo soporte;vector support machine;peculiarity analysis;bayes classifier;competitivite;aprendizaje supervisado;conditional probability;busca dato;information fisher;fisher information;estimation bayes	Peculiarity-oriented mining is a data mining method consisting of peculiar data identification and peculiar data analysis. Peculiarity factor and local peculiarity factor are important concepts employed to describe the peculiarity of a data point in the identification step. One can study the notions at both attribute and record levels. In this paper, a new record LPF called distance-based record LPF (D-record LPF) is proposed, which is defined as the sum of distances between a point and its nearest neighbors. The authors prove that D-record LPF can characterize the probability density of a continuous m-dimensional distribution accurately. This provides a theoretical basis for some existing distance-based anomaly detection techniques. More importantly, it also provides an effective method for describing the class-conditional probabilities in a Bayesian classifier. The result enables us to apply D-record LPF to solve classification problems. A novel algorithm called LPF-Bayes classifier and its kernelized implementation are proposed, which have some connection to the Bayesian classifier. Experimental results on several benchmark datasets demonstrate that the proposed classifiers are competitive to some excellent classifiers such as AdaBoost, support vector machines and kernel Fisher discriminant.	adaboost;algorithm;anomaly detection;bayesian network;bayesian programming;benchmark (computing);data mining;data point;discriminant;effective method;experiment;kernel (operating system);kernel method;linear programming;loss function;low-pass filter;multiclass classification;naive bayes classifier;portable document format;support vector machine;synthetic intelligence	Jian Yang;Ning Zhong;Yiyu Yao;Jue Wang	2010	Knowledge and Information Systems	10.1007/s10115-010-0315-y	support vector machine;kernel method;bayes classifier;probability density function;anomaly detection;outlier;naive bayes classifier;conditional probability;computer science;fisher information;machine learning;pattern recognition;mathematics;supervised learning;data analysis;k-nearest neighbors algorithm;statistics	ML	9.417176069470825	-33.99438046662593	124705
2efdb761a4fbc0cc50c26a7e482994c871657ed3	degrees of conditional (in)dependence: a framework for approximate bayesian networks and examples related to the rough set-based feature selection	conditional independence;bayesian network;condition dependence;approximate independence;data analysis;data discretization;multi valued dependencies;rough sets;mutual information;degree of approximation;feature selection;rough set;information theoretic;bayesian networks	Bayesian networks provide the means for representing probabilistic conditional independence. Conditional independence is widely considered also beyond the theory of probability, with linkages to, e.g. the database multi-valued dependencies, and at a higher abstraction level of semi-graphoid models. The rough set framework for data analysis is related to the topics of conditional independence via the notion of a decision reduct, to be considered within a wider domain of the feature selection. Given probabilistic version of decision reducts equivalent to the data-based Markov boundaries, the studies were also conducted for other criteria of the rough-set-based feature selection, e.g. those corresponding to the multi-valued dependencies. In this paper, we investigate the degrees of approximate conditional dependence, which could be a topic corresponding to the well-known notions such as conditional mutual information and polymatroid functions, however, with many practically useful approximate conditional independence models unmanageable within the information theoretic framework. The major paperu0027s contribution lays in extending the means for understanding the degrees of approximate conditional dependence, with appropriately generalized semi-graphoid properties formulated and with the mathematical soundness of the Bayesian network-like representation of the approximate conditional independence statements thoroughly proved. As an additional contribution, we provide a case study of the approximate conditional independence model, which would not be manageable without the above-mentioned extensions.	approximation algorithm;bayesian network;feature selection;rough set	Dominik Slezak	2009	Inf. Sci.	10.1016/j.ins.2008.09.007	conditional probability distribution;rough set;computer science;machine learning;pattern recognition;bayesian network;chain rule;mathematics;feature selection;statistics	AI	-3.039700397207316	-27.12493745808071	125129
ba176454d1ade52e6eec74e3f9eed5f61179761a	using sampling and queries to extract rules from trained neural networks	concept learning;neural network	Concepts learned by neural networks are dif-cult to understand because they are represented using large assemblages of real-valued parameters. One approach to understanding trained neural networks is to extract symbolic rules that describe their classiica-tion behavior. There are several existing rule-extraction approaches that operate by searching for such rules. We present a novel method that casts rule extraction not as a search problem, but instead as a learning problem. In addition to learning from training examples, our method exploits the property that networks can be eeciently queried. We describe algorithms for extracting both conjunctive and M-of-N rules, and present experiments that show that our method is more eecient than conventional search-based approaches.	ap computer science;algorithm;cargo cult programming;experiment;gibbs sampling;ibm notes;neural networks;rule induction;search problem;type conversion	Mark Craven;Jude W. Shavlik	1994			concept learning;computer science;artificial intelligence;machine learning;data mining	ML	5.895671587303092	-31.216014459756753	125170
3db395ebc2c6cb7bc47b0474a9880aae363e6cb3	approximate reduct computation by rough sets based attribute weighting	discernibility matrix reduct computation attribute weighting rough set theory knowledge reduction reduct set computation np hard problem;attribute weighting;rough sets set theory costs np hard problem algorithm design and analysis computer science information technology data analysis heuristic algorithms genetic algorithms;rough set theory;information technology;matrix algebra;set theory;reduct computation;np hard problem;data analysis;computational complexity;rough set theory attribute weighting reduct computation;heuristic algorithms;rough sets;genetic algorithms;computer science;rough set;weight reduction;discernibility matrix;algorithm design and analysis;reduct set computation;knowledge based systems;knowledge based systems rough set theory computational complexity matrix algebra;knowledge reduction	Rough set theory provides the reduct and the core concepts for knowledge reduction. The cost of reduct set computation is highly influenced by the attribute set size of the dataset where the problem of finding reducts has been proven as an NP-hard problem. This paper proposes an approximate approach for reduct computation. The approach uses the discernibility matrix concept and a weighting mechanism to determine the significance of an attribute to be considered in the reduct. A second supplementary weight is used to break the tie when several attributes have the same significance. The approach is extensively experimented and evaluated on various standard domains.	approximation algorithm;computation;heuristic (computer science);np-hardness;rough set;set theory;technical standard	Qasem A. Al-Radaideh;Md Nasir Sulaiman;Mohd Hasan Selamat;Hamidah Ibrahim	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547317	discrete mathematics;rough set;computer science;machine learning;data mining;mathematics;information technology	DB	-3.965829758956387	-36.23717071792154	125543
9f6cd231afbedad0ac2b7d37ef1f16ca32448821	feature grouping for intrusion detection system based on hierarchical clustering	hierarchical clustering;intrusion detection;feature grouping;mutual information	Intrusion detection is very important to solve an increasing number of security threats. With new types of attack appearing continually, traditional approaches for detecting hazardous contents are facing a severe challenge. In this work, a new feature grouping method is proposed to select features for intrusion detection. The method is based on agglomerative hierarchical clustering method and is tested against KDD CUP 99 dataset. Agglomerative hierarchical clustering method is used to construct a hierarchical tree and it is combined with mutual information theory. Groups are created from the hierarchical tree by a given number. The largest mutual information between each feature and a class label within a certain group is then selected. The performance evaluation results show that better classification performance can be attained from such selected features.	computer cluster;hierarchical clustering;intrusion detection system	Jingping Song;Zhiliang Zhu;Chris J. Price	2014		10.1007/978-3-319-10975-6_21	computer science;machine learning;pattern recognition;data mining;hierarchical clustering;single-linkage clustering;brown clustering;hierarchical clustering of networks	ML	5.492245305165548	-37.72642228756594	125645
1e2fe81fadf0f810289e1d8c2087483c1683aacc	a new method for classification of imprecise data using fuzzy rough fuzzification		In this paper, fuzzy rough sets are introduced as a new solution to the problem of handling imprecise input information in classification tasks. The proposed method is shown as an dispensable way to non-singleton fuzzification. Both methods are applied in neuro–fuzzy classifiers and are extended to be applied with logical-type as well as conjunction-type of fuzzy inference. Several theorems describe how to embed non-singleton fuzzification in antecedent fuzzy sets of the logical-type and conjunction-type fuzzy systems. Likewise, fuzzy rough fuzzification embeds in alike the logical-type and conjunction-type fuzzy systems. With reference to classification, the proposed neuro–fuzzy–rough structure may be considered as an extension of the neuro-rough–fuzzy structure by fuzzification of an input space, performed by fuzzy rough sets. The investigations processed for a wide range of fuzzification spread allow to observe the behavior of fuzzification methods under consideration and to verify the common certitude about the meaning of the spread.	fuzzy control system;fuzzy set;neuro-fuzzy;rough set;type theory	Robert Nowicki;Janusz T. Starczewski	2017	Inf. Sci.	10.1016/j.ins.2017.05.049	defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set operations	AI	1.011238805029597	-25.742039759799304	125648
c0a5a4ddd7a4e8f5ffe2b8fe5babef5e4a087cfe	high-performance intrusion detection using optigrid clustering and grid-based labelling	kyoto 2006 data sets;normal traffic distribution;histograms;pattern clustering;optigrid clustering;security of data feature extraction grid computing pattern classification pattern clustering;cluster labelling;kyoto 2006 data sets intrusion detection system grid based labelling anomaly based ids k means clustering normal traffic distribution feature space hyper spherical clusters optigrid clustering feature extraction traffic data kddcup1999 data sets;cluster labelling intrusion detection system anomaly based ids clustering optigrid;intrusion detection;feature space;training data;sensitivity;clustering;feature extraction;anomaly based ids;pattern classification;grid based labelling;clustering algorithms;kddcup1999 data sets;traffic data;hyper spherical clusters;grid computing;proposals;high performance;optigrid;security of data;k means clustering;intrusion detection system;labeling;clustering algorithms labeling training data partitioning algorithms histograms proposals sensitivity;partitioning algorithms	This research aims to construct a high-performance anomaly based intrusion detection system. Most of past studies of anomaly based IDS adopt k-means based clustering, this paper points out that the following reasons cause performance degradation of k-means based clustering when it is deployed in real traffic environment. First, k-means based algorithms have weakness for high dimensional data. Second, in spite of non-hyper spherical distribution of normal traffic in a feature space, these algorithms can only create hyper spherical clusters. Furthermore, unsophisticated algorithms to label clusters cannot achieve high detection performance. In order to solve these issues, this paper proposes a modification of OptiGrid clustering and a cluster labelling algorithm using grids. OptiGrid has robust ability to high dimensional data. Our labelling algorithm divides the feature space into grids and labels clusters using the density of grids. The combination of these two algorithms enables a system to extract the feature of traffic data and classifies the data as attack or normal correctly. We have implemented our system and confirmed efficiency of our system by utilizing both KDDCUP1999 data sets and Kyoto 2006+ data sets.	algorithm;anomaly detection;cluster analysis;computer performance;elegant degradation;experiment;feature vector;intrusion detection system;k-means clustering;the fight: lights out	Moriteru Ishida;Hiroki Takakura;Yasuo Okabe	2011	2011 IEEE/IPSJ International Symposium on Applications and the Internet	10.1109/SAINT.2011.12	intrusion detection system;correlation clustering;computer science;machine learning;pattern recognition;cure data clustering algorithm;data mining;cluster analysis	DB	5.299398348120729	-37.804489934868876	125713
3a467b6439daf8ec4e99de99ef07abaf6b4468b0	automatic generation of knowledge structures	numerical method;weather forecasting;automatic generation;learning system;knowledge structure;knowledge base;expert system	"""The Program for Regional Observing and Forecasting Services is investigating the use of expert systems in weather forecasting, using """"learning systems"""" as models. The first model to be tested was an adaption of Quinlan's ID3 and Friedman's CART. A """"heuristic"""" structure generated by a learning system was compared with that of a human expert. It was tentatively concluded that automatic generation methods (1) did not suffer from loss of dimensionality as do numerical methods, (2) will provide a preliminary knowledge base suitable for modification by domain experts, and (3) may provide some insight into and understanding of data sets having unknown structures."""	decision tree learning;expert system;heuristic;id3 algorithm;knowledge base;numerical method;ross quinlan	Frank H. Merrem	1989	SIGART Newsletter	10.1145/63266.63295	legal expert system;knowledge base;weather forecasting;numerical analysis;computer science;artificial intelligence;data science;machine learning;data mining;expert system	AI	5.041858241163989	-24.907914504168257	125805
0d6a6cafde91ba1b1f57189df7def56b7c66ff29	a bayesian-adaboost model for stock trading rule discovery		Detecting the trading patterns with different technical indicators from the historical financial data is an efficient way to forecast the trading decisions in the financial market. In most cases, the trading patterns which consist of some specific combinations of technical indicators are significant in predicting the efficient trading decisions. However, discovering those combinations is a rather challenge assignment. In this paper, we propose a novel method to detect the trading patterns and later the Naive bayes with Adaboost method was employed to determine the trading decisions. The proposed method has been implemented on two historical stock datasets, the experimental results demonstrate that the proposed algorithm outperforms the other three algorithms and could provide a worthwhile reference for the financial investments.	adaboost;algorithm;algorithmic trading;association rule learning;best, worst and average case;biclustering;naive bayes classifier	Zhoufan Kong;Jie Yang;Qinghua Huang;Xuelong Li	2017	2017 10th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2017.8302138	market research;naive bayes classifier;adaboost;pattern recognition;artificial intelligence;financial market;data mining;computer science;statistical classification;bayesian probability	ML	-0.24349811421003958	-34.42396731127668	125973
97a2549ffe296b267f0a76f4b9e4bbcb91a7f2ae	adaptive modeling and discovery in bioinformatics: the evolving connectionist approach		Most biological processes that are currently being researched in bioinformatics are complex, dynamic processes that are difficult to model and understand. The paper presents evolving connectionist systems (ECOS) as a general approach to adaptive modeling and knowledge discovery in bioinformatics. This approach extends the traditional machine learning approaches with various adaptive learning and rule extraction procedures. ECOS belong to the class of incremental local learning and knowledge-based neural networks. They are applied here to challenging problems in Bioinformatics, such as: microarray gene expression profiling, gene regulatory network (GRN) modeling, computational neurogenetic modeling. The ECOS models have several advantages when compared to the traditional techniques: fast learning, incremental adaptation to new data, facilitating knowledge discovery through fuzzy rules. © 2008 Wiley Periodicals, Inc.	bioinformatics;connectionism	Nikola K. Kasabov	2008	Int. J. Intell. Syst.	10.1002/int.20282	computer science;bioinformatics;artificial intelligence;machine learning;data mining	DB	5.853363813058568	-33.9959571656755	126004
ccd86d957dfe3926fbb14fd01eb02929c663220f	time series analysis: unsupervised anomaly detection beyond outlier detection		Anomaly detection on log data is an important security mechanism that allows the detection of unknown attacks. Self-learning algorithms capture the behavior of a system over time and are able to identify deviations from the learned normal behavior online. The introduction of clustering techniques enabled outlier detection on log lines independent from their syntax, thereby removing the need for parsers. However, clustering methods only produce static collections of clusters. Therefore, such approaches frequently require a reformation of the clusters in dynamic environments due to changes in technical infrastructure. Moreover, clustering alone is not able to detect anomalies that do not manifest themselves as outliers but rather as log lines with spurious frequencies or incorrect periodicity. In order to overcome these deficiencies, in this paper we introduce a dynamic anomaly detection approach that generates multiple consecutive cluster maps and connects them by deploying cluster evolution techniques. For this, we design a novel clustering model that allows tracking clusters and determining their transitions. We detect anomalous system behavior by applying time-series analysis to relevant metrics computed from the evolving clusters. Finally, we evaluate our solution on an illustrative scenario and validate the achieved quality of the retrieved anomalies with respect to the runtime.	algorithm;anomaly detection;autoregressive integrated moving average;cluster analysis;dynamical system;existential quantification;machine learning;malware;map;microsoft windows;norm (social);parsing;quasiperiodicity;sensor;time series;unsupervised learning	Max Landauer;Markus Wurzenberger;Florian Skopik;Giuseppe Settanni;Peter Filzmoser	2018		10.1007/978-3-319-99807-7_2	anomaly detection;time series;outlier;parsing;cluster (physics);spurious relationship;cluster analysis;pattern recognition;computer science;artificial intelligence	ML	0.48700726852687104	-36.431084380633706	126014
d6052b706cfb4b17ed948134f4acf0bf8240d71c	using dynamic time warping to bootstrap hmm-based clustering of time series	hidden markov model;time series;time series data;dynamic time warping	We presented a hybrid time series clustering algorithm that uses Dynamic Time Warping (DTM) and Hidden Markov Model (HMM) induction. The algorithm worked well in experiments with artificial data. The two methods complement each other : DTW produces a rough initial clustering and the HMM removes from these clusters the sequences that do not belong to them. The downside is that the HMM removes some good sequences along with the bad ones. We suggested possible ways of improving the method and are currently working on validating them.	dynamic time warping;hidden markov model;time series	Tim Oates;Laura Firoiu;Paul R. Cohen	2001		10.1007/3-540-44565-X_3	order of integration	ML	-0.9596740629789712	-37.34543159961029	126255
b10954b40e426a0d85945580e94451b39585aca9	an efficient approach in analysis of dna base calling using neural fuzzy model		This paper presented the issues of true representation and a reliable measure for analyzing the DNA base calling is provided. The method implemented dealt with the data set quality in analyzing DNA sequencing, it is investigating solution of the problem of using Neurofuzzy techniques for predicting the confidence value for each base in DNA base calling regarding collecting the data for each base in DNA, and the simulation model of designing the ANFIS contains three subsystems and main system; obtain the three features from the subsystems and in the main system and use the three features to predict the confidence value for each base. This is achieving effective results with high performance in employment.	adaptive neuro fuzzy inference system;artificial neural network;base calling;biological neural networks;fuzzy logic;reason applied by forcast logic to project this vaccine:finding:point in time:^patient:nominal;simulation;interest	Safa A. Hameed;Raed I. Hamed	2017		10.1155/2017/3686025	bioinformatics;machine learning;data mining	Comp.	7.741952191158007	-26.984845775498073	126518
a0067e5a777aba781ecb45135de24982c63fdcd3	fuzzy rough sets based uncertainty measuring for stream based active learning	rough set theory;uncertainty handling approximation theory fuzzy set theory learning artificial intelligence rough set theory;labeled data fuzzy rough set based uncertainty measurement stream based active learning conditional features decision labels lower approximations unlabeled data memberships;uncertainty handling;fuzzy set theory;approximation theory;radio access networks abstracts vectors support vector machines;learning artificial intelligence;uncertainty active learning fuzzy rough sets membership support vector machine	Active learning methods put their efforts on selecting and labeling the most informative examples out of a large amount of unlabeled ones. It is performed in uncertain environments where the learner is required to make some decisions on the observed examples. However, existing algorithms do not have a good formulation to evaluate the example's uncertainty by considering the inconsistency between conditional features and decision labels, while this inconsistency has been taken into account by fuzzy rough sets. Therefore, a fuzzy rough sets based active learning algorithm with stream based settings is proposed in this work. The lower approximations in fuzzy rough sets are used to compute the memberships of the unlabeled example, and the uncertainty is then used for decision. Experimental comparisons with other existing approaches demonstrate the effectiveness of the proposed algorithm.	active learning (machine learning);algorithm;approximation;information;rough set	Xizhao Wang;Sam Kwong;Degang Chen;Qiang He	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6358926	rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;dominance-based rough set approach;approximation theory	AI	2.1303418161158794	-30.492162300224603	126623
09efadeacfb0b6df04381dfdd35483f24bb0bed0	are more features better? a response to attributes reduction using fuzzy rough sets	attributes reduction;dk atira pure researchoutput researchoutputtypes contributiontojournal article;text processing;computational intelligence;rough set theory;curse of dimensionality;indexing terms;feature selection fs;fuzzy set theory;fuzzy sets;attribute reduction;dimensionality reduction;fuzzy rough feature selection;noise reduction;rough sets;feature selection;fuzzy rough feature selector;data reduction;fuzzy rough sets;computer science;rough set;computational efficiency;dimensional reduction;fuzzy systems;fuzzy system;frequency selective surfaces	A recent TRANSACTIONS ON FUZZY SYSTEMS paper proposing a new fuzzy-rough feature selector (FRFS) has claimed that the more attributes remain in datasets, the better the approximations and hence resulting models. [Tsang , IEEE Trans. Fuzzy Syst. , vol. 16, no. 5, pp. 1130-1141]. This claim has been used as a primary criticism of the original FRFS method [Jensen and Shen, IEEE Trans. Fuzzy Syst., vol. 15, no. 1, pp. 73-89, Feb. 2007]. Although, in certain applications, it may be necessary to consider as many features as possible, the claim is contrary to the motivation behind feature selection concerning the curse of dimensionality, the presence of redundant and irrelevant features, and the large amount of literature documenting observed improvements in modeling techniques following data reduction. This letter discusses this issue, as well as two other issues raised by Tsang [IEEE Trans. Fuzzy Syst., vol. 16, no. 5, pp. 1130-1141, Oct. 2008] regarding the original algorithm.	algorithm;approximation;curse of dimensionality;feature selection;jensen's inequality;relevance;rough set;software documentation	Richard Jensen;Qiang Shen	2009	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2009.2026639	rough set;computer science;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;fuzzy control system	Visualization	0.3669205699231079	-29.186008359855734	126664
6c1b9aa86e29577c6bd106c89a3279bc4bb81bd9	rough set based rule evaluations and their applications	thesis or dissertation;personalization;data mining;doctoral thesis;rule evaluations;rough set	Knowledge discovery is an important process in data analysis, data mining and machine learning. Typically knowledge is presented in the form of rules. However, knowledge discovery systems often generate a huge amount of rules. One of the challenges we face is how to automatically discover interesting and meaningful knowledge from such discovered rules. It is infeasible for human beings to select important and interesting rules manually. How to provide a measure to evaluate the qualities of rules in order to facilitate the understanding of data mining results becomes our focus. In this thesis, we present a series of rule evaluation techniques for the purpose of facilitating the knowledge understanding process. These evaluation techniques help not only to reduce the number of rules, but also to extract higher quality rules. Empirical studies on both artificial data sets and real world data sets demonstrate how such techniques can contribute to practical systems such as ones for medical diagnosis and web personalization. In the first part of this thesis, we discuss several rule evaluation techniques that are proposed towards rule postprocessing. We show how properly defined rule templates can be used as a rule evaluation approach. We propose two rough set based measures, a Rule Importance Measure, and a Rules-As-Attributes Measure, to rank the important and interesting rules. In the second part of this thesis, we show how data preprocessing can help with rule evaluation. Because well preprocessed data is essential for important rule generation, we propose a new approach for processing missing attribute values for enhancing the generated rules. In the third part of this thesis, a rough set based rule evaluation system is demonstrated to show the effectiveness of the measures proposed in this thesis. Furthermore, a new user-centric web personalization system is used as a case study to demonstrate how the proposed evaluation measures can be used in an actual application.	data mining;data pre-processing;machine learning;personalization;preprocessor;rough set	Jiye Li	2007			rule-based system;association rule learning;computer science;artificial intelligence;data science;data mining	ML	-3.591896392828571	-30.45454013023408	127101
41ba8da8522dc1d10631bdcab5d9f75e417e6f25	pager: parameterless, accurate, generic, efficient knn-based regression	regression equation;k nearest neighbours;parameterless;regression;time series data;k nearest neighbour;accurate;prediction;hypothesis test	The problem of regression is to estimate the value of a dependent numeric variable based on the values of one or more independent numeric variables. Regression algorithms can be used for prediction (including forecasting of time-series data), inference, hypothesis testing, and modeling of causal relationships. Although this problem has been studied extensively in statistics, it has not received much attention from the data mining community [5, 11]. Statistical approaches try to model the relationship between the dependent and independent variables as a closed form function. It is the user’s responsibility to make an intelligent guess about the form of this function. This is done by studying the application domain, and may involve trial-and-error methods. Once the function’s form is fixed, its parameters (or coefficients) are estimated so as to give a “best fit” to the available data. Typically, the function also has an error term that is used to compensate for unexplained variation in the dependent variable. The above nature of statistical approaches requires that regression problems in each specific application domain are separately studied and solved optimally for that domain. For example, [6] applies non-linear regression models for longitudinal data with errors that follow a skew-elliptical distribution. The main objective of the study was to predict normal versus abnormal pregnancy outcomes from beta human chorionic gonadotropin data. Another problem with the statistical approaches is outlier sensitivity. Outliers (extreme cases) can seriously bias the results by pulling or pushing the regression curve in a particular direction, leading to biased regression coefficients. Often, excluding just a single extreme case can yield a completely different set of results. In this paper we present a new regression algorithm PAGER – Parameterless, Accurate, Generic, Efficient kNN-based Regression. PAGER has the following desirable features:	application domain;comefrom;causality;coefficient;curve fitting;data mining;display resolution;nonlinear system;out of the box (feature);real life;selection algorithm;time series	Himanshu Singh;Aditya Desai;Vikram Pudi	2010		10.1007/978-3-642-15251-1_13	statistical hypothesis testing;regression;prediction;machine learning;time series;data mining;regression analysis;statistics	ML	4.80745551923799	-33.606879030381826	127110
bb2409202d1aae5738c45d7f4cec3fc574ece393	factorization techniques for student performance classification and ranking		Historically, student performance prediction has been approached with regression models. For instance, the KDD Cup 2010 used the root mean squared error (RMSE) as an evaluation criterion. This is appropriate when the goal is to predict student marks or how well will they perform in a given exercise. Since in many datasets the target variable is binary, i.e. a student has solved the exercise or failed in it, it would be natural to look at this problem as to a classification task. Another, probably not so usual case could be when we only have a socalled positive feedback, i.e. only the successful solutions are recorded. In this case, neither the regression nor the classification approaches would be useful and one could look on this problem as to a ranking task. We propose to look at solving the student performance prediction as a classification or ranking tasks, respectively, where models are optimized for appropriate error measures which are the Hinge loss and the Area under the ROC curve. Experimental comparison of these techniques are introduced using two, large-scale datasets. Both methods are well known in their respective fields, thus the goal of this paper is to introduce them in the educational data mining community.	educational data mining;failure;hinge loss;mean squared error;performance prediction;positive feedback;receiver operating characteristic	Lucas Drumond;Nguyen Thai-Nghe;Tomás Horváth;Lars Schmidt-Thieme	2012			regression analysis;hinge loss;ranking svm;performance prediction;factorization;ranking;machine learning;pattern recognition;mean squared error;educational data mining;artificial intelligence;computer science	ML	3.9145535669351497	-34.0394604293683	127164
f511adb45d06e436912535ee145763bfd233369b	sensor selection and fusion using subjective logic	uncertainty;subjective logic;dempster shafer theory;sensor fusion	Sensor fusion is the notion of combining the data from two or more sensors in order to enhance performance compared with that of individual sensors. The most common method for fusing sensors is through Bayesian methods. However, these cannot easily take into account unknown uncertainty or imprecision. A relatively new method is subjective logic. Although similar to Dempster-Shafer theory, it is unique in that it allows us to collapse the frame of discernment into a binary frame, thereby reducing the complexity. In this paper, we show two novel methods for employing subjective logic: 1) it can be used for target identification (and we show some examples for surveillance in the airborne environment); 2) given some knowledge about the performance of a suite of sensors, we might be able to select the best sensor for a given task. This is achieved through the use of the expected decision formula.		Edwin El-Mahassni	2014	IJIDSS	10.1504/IJIDSS.2014.059960	uncertainty;dempster–shafer theory;computer science;artificial intelligence;machine learning;data mining;sensor fusion;statistics;subjective logic	Robotics	-0.4767623626945979	-28.426458541666072	127319
0babaaa9d1a511d0956d994d2221d86de9567aad	discretization for naive-bayes learning: managing discretization bias and variance	bayes estimation;discretisation;analisis estadistico;learning;funcion densidad probabilidad;probability density function;naive bayes;error sistematico;discretization;multiplicite;discretizacion;statistical significance;intelligence artificielle;classification;fonction densite probabilite;naive bayes classifier;estimacion bayes;statistical analysis;bias;analyse statistique;multiplicidad;classification error;artificial intelligence;inteligencia artificial;naive bayes learning;multiplicity;clasificacion;erreur systematique;estimation bayes;variance;variancia	Quantitative attributes are usually discretized in Naive-Bayes learning. We establish simple conditions under which discretization is equivalent to use of the true probability density function during naive-Bayes learning. The use of different discretization techniques can be expected to affect the classification bias and variance of generated naive-Bayes classifiers, effects we name discretization bias and variance. We argue that by properly managing discretization bias and variance, we can effectively reduce naive-Bayes classification error. In particular, we supply insights into managing discretization bias and variance by adjusting the number of intervals and the number of training instances contained in each interval. We accordingly propose proportional discretization and fixed frequency discretization, two efficient unsupervised discretization methods that are able to effectively manage discretization bias and variance. We evaluate our new techniques against four key discretization methods for naive-Bayes classifiers. The experimental results support our theoretical analyses by showing that with statistically significant frequency, naive-Bayes classifiers trained on data discretized by our new methods are able to achieve lower classification error than those trained on data discretized by current established discretization methods.	algorithm;decision boundary;decision tree learning;discretization;error-tolerant design;experiment;free-form deformation;heuristic (computer science);interval arithmetic;machine learning;naive bayes classifier;word lists by frequency	Ying Yang;Geoffrey I. Webb	2008	Machine Learning	10.1007/s10994-008-5083-5	discretization error;naive bayes classifier;machine learning;pattern recognition;discretization;mathematics;discretization of continuous features;statistics	ML	9.885721522918555	-33.10626498203751	127834
02e205875992a5e52122200b795d8d99ae174190	importance degree of features and feature selection	unsupervised learning;search space reduction;monotonic property;importance degree of features;search space;data mining;feature filter method;feature selection importance degree of features feature ranking;machine learning;feature extraction;feature importance degree;classification algorithms;pattern recognition;search space reduction feature importance degree feature selection monotonic property feature filter method machine learning;feature selection;feature ranking;iris;extraterrestrial measurements filters space technology fuzzy systems automation electric variables measurement computational modeling educational institutions computational efficiency support vector machines;algorithm design and analysis;unsupervised learning pattern recognition;reactive power	A novel measure, importance degree of features, is proposed to rank the features. And a new filter method is presented to carry out feature selection based on such measure. The monotonic property of this proposed measure can reduce the search space, which results in enhancing learning efficiency. The simulation results indicate the validity of our method.	algorithm;feature selection;heuristic;information;relevance;simulation	Di Xiao;Junfeng Zhang	2009	2009 Sixth International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2009.625	unsupervised learning;algorithm design;feature extraction;computer science;machine learning;pattern recognition;data mining;mathematics;ac power;feature selection;feature	Robotics	2.7993451214235185	-31.826380780170727	127916
73f0d6618e2995d9bc9a23a44370d32453c424ba	financial distress prediction using svm ensemble based on earnings manipulation and fuzzy integral			inventory	Chao Huang;Qingyu Yang;Mingwei Du;Donghui Yang	2017	Intell. Data Anal.	10.3233/IDA-160034	earnings;distress;fuzzy logic;artificial intelligence;machine learning;support vector machine;computer science;pattern recognition	AI	9.791478103269503	-26.12110641315858	128044
dc9dc0ed8823ad82fc9002169de3499658a20522	an algorithm for decision tree and attribute reduction	rough set theory;rough set theory decision tables decision trees knowledge representation;knowledge representation;decision trees;decision tables;random access memory rain abstracts;degree of importance decision table decision tree attribute reduct rough set;knowledge representation system decision tree attribute reduction decision table rough set	A decision tree and an attribute reduct from the same crisp decision table are often obtained respectively with different algorithms. Developing an algorithm for both of them is theoretically important and practically useful. This paper proposes an algorithm generating both decision tree and attribute reduction from a crisp decision table.	algorithm;decision table;decision tree	Qun-Feng Zhang;Yu-Fen Zhang;Ming-Zhu Hao	2012	2012 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2012.6359029	decision table;knowledge representation and reasoning;rough set;influence diagram;decision tree learning;computer science;information gain ratio;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;decision rule;mathematics;id3 algorithm;weighted sum model;dominance-based rough set approach;decision stump	DB	-1.7268629506104611	-27.73122995308463	128182
601784765d26271bf4c7be92fe6b47ea879fd75e	statistical comparison of modelling methods for software maintainability prediction	significance tests;software maintainability prediction;machine learning	The objective of this paper is statistical comparison of modelling methods for software maintainability prediction. The statistical comparison is performed by building software maintainability prediction models using 27 di®erent regression and machine learning based algorithms. For this purpose, software metrics datasets of two di®erent commercial object-oriented systems are used. These systems were developed using an object oriented programming language Ada. These systems are User Interface Management System (UIMS) and Quality Evaluation System (QUES). It is shown that di®erent measures like MMRE, RMSE, Pred(0.25) and Pred(0.30) calculated on predicted values obtained from leave one out (LOO) cross validation produce very divergent results regarding accuracy of modelling methods. Therefore the 27 modelling methods are evaluated on the basis of statistical signi ̄cance tests. The Friedman test is used to rank various modelling methods in terms of absolute residual error. Six out of the ten top ranked modelling methods are common to both UIMS and QUES. This indicates that modelling methods for software maintainability predicton are solid and scalable. After obtaining ranks, pair wise Wilcoxon Signed rank test is performed. Wilcoxon Sign rank test indicates that the top ranking method in UIMS data set is signi ̄cantly superior to only four other modelling methods whereas the top tanking method in QUES data set is signi ̄cantly superior to 11 other modelling methods. The performance of instance based learning algorithms IBk and Kstar is comparable to modelling methods used in earlier studies.	ada;algorithm;machine learning;management system;programming language;scalability;software metric;user interface management systems	Arvinder Kaur;Kamaldeep Kaur	2013	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194013500198	reliability engineering;computer science;machine learning;data mining	SE	4.012107343456736	-33.925069414272286	128555
371dc23b0e62046dcabcb2984211fd087354e6df	determination of event patterns for complex event processing using fuzzy unordered rule induction algorithm with multi-objective evolutionary feature subset selection	machine learning algorithms;fuzzy unordered rule induction;multi objective evolutionary feature subset selection complex event processing fuzzy unordered rule induction;uncertainty;machine learning approach event pattern complex event processing fuzzy unordered rule induction algorithm multiobjective evolutionary feature subset selection streaming data response action cep system sensor data primitive event furia relevant feature subset elitist pareto based multiobjective evolutionary algorithm for diversity reinforcement enora;filtering algorithms;multi objective evolutionary feature subset selection;pattern matching;classification algorithms;complex event processing;optimization;classification algorithms filtering algorithms uncertainty machine learning algorithms pattern matching optimization real time systems;pareto optimisation data analysis evolutionary computation fuzzy set theory;real time systems	Complex Event Processing (CEP) is an emerging technology to process streaming data and to generate response actions in real time. CEP systems treat all sensor data as primitive events and attempt to detect semantically high level events and related actions by matching them using event patterns. These event patterns are the rules which combine primitive events according to temporal, logical, or spatial correlations among them. Although event patterns (decision rules) can be provided by experts in simplistic scenarios, the huge amount of sensor data makes this unfeasible. The main purpose of the underlying paper is replacing manual identification of event patterns. Considering the uncertainty related to the sensor data, Fuzzy Unordered Rule Induction Algorithm (FURIA) was implemented to identify event patterns after selecting the relevant feature subset using Elitist Pareto-based Multi-Objective Evolutionary Algorithm for Diversity Reinforcement (ENORA). The results were compared to the alternative machine learning approaches.	complex event processing;evolutionary algorithm;high-level programming language;machine learning;pareto efficiency;rule induction;streaming media	Nijat Mehdiyev;Julian Krumeich;Dirk Werth;Peter Loos	2016	2016 49th Hawaii International Conference on System Sciences (HICSS)	10.1109/HICSS.2016.216	statistical classification;uncertainty;computer science;complex event processing;machine learning;pattern matching;pattern recognition;data mining;database;programming language;statistics	DB	-0.6706144208963506	-29.61505382564527	128949
7480002cb8730c73d1b88793445d32790fe274a1	on paradox of fuzzy modeling: supervised learning for rectifying fuzzy membership function	fuzzy membership function;supervised learning;supervised machine learning;artificial intelligent;machine learning;membership function;artificial intelligence;fuzzy mathematics;probability model;fuzzy model	The paradox of fuzzy modeling is recognized due to the co-existence of its effectiveness of solving uncertain problems in the real world and the skepticism of its reasonability in membership function. In this paper, a revised membership function by means of supervised machine learning is introduced, in which the membership function curve is revised from the learning data of existing samples. It points that the information from supervised machine learning by samples is in the same argument to the statistic data from observation in the probability model. The formulations of supervised fuzzy machine learning by samples for revising the membership function are presented, and satisfactory results by the revised membership function compared with the experimental data are shown. It steps forward in promoting the pragmatic application of fuzzy methods in real world problems.	machine learning;rectifier;supervised learning	Shaopei Lin	2004	Artificial Intelligence Review	10.1007/s10462-004-7189-x	semi-supervised learning;unsupervised learning;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;online machine learning;machine learning;pattern recognition;information fuzzy networks;fuzzy set;supervised learning;fuzzy set operations;generalization error	AI	5.655243365765794	-29.92259555944384	129150
c31834b6c2f5dd90740dfb2d7ed3d6d4b3e96309	missing data analyses: a hybrid multiple imputation algorithm using gray system theory and entropy based on clustering	gray system theory;clustering;entropy;missing data;multiple imputation	Researchers and practitioners who use databases usually feel that it is cumbersome in knowledge discovery or application development due to the issue of missing data. Though some approaches can work with a certain rate of incomplete data, a large portion of them demands high data quality with completeness. Therefore, a great number of strategies have been designed to process missingness particularly in the way of imputation. Single imputation methods initially succeeded in predicting the missing values for specific types of distributions. Yet, the multiple imputation algorithms have maintained prevalent because of the further promotion of validity by minimizing the bias iteratively and less requirement on prior knowledge to the distributions. This article carefully reviews the state of the art and proposes a hybrid missing data completion method named Multiple Imputation using Gray-system-theory and Entropy based on Clustering (MIGEC). Firstly, the non-missing data instances are separated into several clusters. Then, the imputed value is obtained after multiple calculations by utilizing the information entropy of the proximal category for each incomplete instance in terms of the similarity metric based on Gray System Theory (GST). Experimental results on University of California Irvine (UCI) datasets illustrate the superiority of MIGEC to other current achievements on accuracy for either numeric or categorical attributes under different missing mechanisms. Further discussion on real aerospace datasets states MIGEC is also applicable for the specific area with both more precise inference and faster convergence than other multiple imputation methods in general.	algorithm;categorization;cluster analysis;computer memories inc.;data quality;database;entropy (information theory);experiment;geo-imputation;iteration;iterative method;missing data;software development;systems theory	Jing Tian;Bing Yu;Dan Yu;Shilong Ma	2013	Applied Intelligence	10.1007/s10489-013-0469-x	entropy;missing data;computer science;machine learning;data mining;cluster analysis;imputation;statistics	AI	1.919755075416153	-37.25942053087841	129185
260e0ed13cfe88339fa32a714b955978b49f206b	application of a rough set-based inductive learning system	rough set		inductive reasoning;rough set	Michael Hadjimichael;Anita Wasilewska	1993	Fundam. Inform.		discrete mathematics;combinatorics;mathematics;rough set	AI	2.0671382209457727	-24.78549173248699	129288
181753fb19501222186603859a1e520f01b07a5d	type-2 fuzzy probabilistic system	type 2 fuzzy sets;probability;characteristic moments;book chapter;probability density function;conference;rule based;probabilistic logic fuzzy sets fuzzy logic probability time series analysis correlation probability density function;probability fuzzy logic fuzzy set theory knowledge based systems;fuzzy subset;probabilistic system;real valued random quantities;rule based fuzzy systems type 2 fuzzy probabilistic system type 2 fuzzy logic system fuzzy subevents membership probability density function mpdf j plane representation type 2 fuzzy set type 2 fuzzy arima proof of concept application real valued problem box jenkins model sarima fuzzy statistical evaluation high order fuzzy events;fuzzy set theory;higher order;statistical evaluation;proof of concept;fuzzy logic;probabilistic model;fuzzy probability;type 2 fuzzy set;fuzzy logic system;arima fuzzy logic system type 2 fuzzy sets fuzzy probability fuzzy event fuzzy subset real valued random quantities characteristic moments;arima;knowledge based systems;fuzzy event;fuzzy system	The paper proposes a new type-2 fuzzy logic system, Type-2 Fuzzy Probabilistic System, coupled with type-2 fuzzy probability, to deal with higher order fuzzy events with fuzzy sub-events together. The new concepts of Membership Probability Density Function (Mpdf) and ̃J-plane Representation for type-2 fuzzy set are also presented. A special Type-2 Fuzzy Probabilistic Model, Type-2 Fuzzy ARIMA, is proposed. As a proof-of-concept application, an experiment with a 36 month forecast of a real-valued problem is conducted showing some improvement over Box-Jenkins Model (SARIMA). As series of concepts and results prove, Type-2 Fuzzy Probabilistic System can be determined by fuzzy statistical evaluation of high order fuzzy events and process. This paper realises type-2 fuzzy logic systems evolving from rule-based fuzzy systems to the systems based on type-2 fuzzy probability.	autoregressive integrated moving average;formal system;fuzzy control system;fuzzy logic;fuzzy rule;fuzzy set;jenkins;logic programming;statistical model;stochastic process	Yuying Wang	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6234273	fuzzy logic;rule-based system;statistical model;probability density function;autoregressive integrated moving average;discrete mathematics;higher-order logic;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;knowledge-based systems;machine learning;probability;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;proof of concept;fuzzy set operations;fuzzy control system;statistics	DB	2.7500044344264527	-25.71666462352825	129407
4f174f39fb6a7eb8e2f5725b493fa218bdca8e4b	anomaly detection analysis of intrusion data using supervised & unsupervised approach		Anomaly based network intrusion detection (ANID) is an important problem that has been researched within diverse research areas and various application domains. Several anomaly based network intrusion detection systems (ANIDS) can be found in the literature. Most ANIDSs employ supervised algorithms, whose performances highly depend on attack-free training data. However, this kind of training data is difficult to obtain in real world network environment. Moreover, with changing network environment or services, patterns of normal traffic will be changed. This leads to high false positive rate of supervised ANIDSs. Using unsupervised anomaly detection techniques, however, the system can be trained with unlabeled data and is capable of detecting previously unseen attacks. We have categorized the existing ANIDSs based on its type, class, nature of detection/ processing, level of security, etc. We also enlist some proximity measures for intrusion data analysis and detection. We also report some experimental results for detection of attacks over the KDD’99 dataset.	algorithm;anomaly detection;categorization;intrusion detection system;performance;sensor;supervised learning;unsupervised learning	Prasanta Gogoi;Bhogeswar Borah;Dhruba Kumar Bhattacharyya	2010	JCIT		anomaly-based intrusion detection system;anomaly detection;computer science;machine learning;pattern recognition;data mining	Security	6.0005706564222665	-37.13964508312428	129439
fd40eb906359047d33af2ce12b97eff98a5f4e2b	construction of a neurofuzzy network capable of extrapolating (and interpolating) with respect to the convex hull of a set of input samples in ${{\bb r}}^n$	input output modeling;interpolation;fuzzy neural nets;neural networks;fuzzy rules;multilayer perceptrons;neuro fuzzy network;extrapolation;mathematical functions;testing;rbf;multilayer perceptron;multi layer neural network;input output;artificial neural networks;statistical distributions;mlp neural network;radial basis function;statistical analysis;regression estimator;neuro fuzzy;regression estimation;hypercubes;artificial datasets;fuzzy rule extrapolation;regression analysis;neurofuzzy network;fuzzy neural networks;convex hull;statistical distribution;multilayered perceptron neural networks;fuzzy rule interpretation	The problem of regression estimation is considered with a specific regard for the distinction between interpolation and extrapolation. A neurofuzzy network named NFECH is proposed that is capable of extrapolating (and interpolating) with respect to the convex hull of a finite set of input samples X sub Ropfn. The geometrical construction of the proposed network is explained both mathematically and graphically. The illustrations explain how the particular parts of the construction work, and also show the final surfaces of the obtained models. The method is tested on artificial datasets generated from mathematical functions according to various statistical distributions. Also, comparisons to the commonly used radial basis function (RBF), multilayered perceptron (MLP) neural networks, and to fuzzy rule interpretation (FRI)/fuzzy rule extrapolation (FRE) approach are presented.	artificial neural network;convex hull;extrapolation;fuzzy rule;interpolation;memory-level parallelism;multilayer perceptron;radial (radio);radial basis function;rule 110	P. Klesk	2008	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2008.924337	probability distribution;computer science;artificial intelligence;machine learning;pattern recognition;mathematics;artificial neural network	Vision	6.9709781093331165	-27.615752025516507	129746
0fdf0f775ca7d452bec08ec3f9716a34de189061	a new fast algorithm for fuzzy rule selection	fuzzy neural network;error reduction;fuzzy neural nets;fuzzy rules;network performance;matrix algebra;orthogonal least square;matrix algebra fuzzy neural nets;fast algorithm;residual matrix;fuzzy neural networks fuzzy systems fuzzy sets neural networks least squares methods us department of transportation nonlinear systems proposals numerical simulation associative memory;error reduction ratio;fast forward rule selection algorithm;fuzzy neural networks;orthogonal least squares algorithm;error reduction ratio fuzzy neural networks fast forward rule selection algorithm residual matrix orthogonal least squares algorithm	This paper investigates the selection of fuzzy rules for fuzzy neural networks. The main objective is to effectively and efficiently select the rules and to optimize the associated parameters simultaneously. This is achieved by the proposal of a fast forward rule selection algorithm (FRSA), where the rules are selected one by one and a residual matrix is recursively updated in calculating the contribution of rules. Simulation results show that, the proposed algorithm can achieve faster selection of fuzzy rules in comparison with conventional orthogonal least squares algorithm, and better network performance than the widely used error reduction ratio method (ERR).	artificial neural network;computation;fast forward;fuzzy rule;network performance;ordinary least squares;recursion;selection algorithm;simulation	Barbara Pizzileo;Kang Li	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295633	mathematical optimization;defuzzification;computer science;neuro-fuzzy;machine learning;pattern recognition;mathematics;fuzzy associative matrix;network performance;fuzzy set operations;artificial neural network	Robotics	6.487654763678807	-27.332112041053765	129803
ce44c9858a8199a11ece4cec9371b058c55ab635	attribute reduction in fuzzy decision formal contexts	α β consistent set attribute reduction discernibility matrix fuzzy decision formal context;boolean functions;matrix algebra;fuzzy set theory;matrix algebra boolean functions decision theory formal concept analysis fuzzy logic fuzzy set theory;fuzzy logic;attribute reduction;hierarchy fuzzy knowledge;concept lattice;α β consistent set;decision theory;fuzzy decision formal contexts;attribute consistent sets;discernibility matrix;fuzzy decision formal context;boolean method;formal concept analysis attribute reduction fuzzy decision formal contexts fuzzy concept lattices object sets attribute consistent sets hierarchy fuzzy knowledge boolean method discernibility matrix;object sets;formal concept analysis;fuzzy concept lattices	The classical concept lattices express the precise relation between object sets and attribute sets, but fuzzy concept lattices express the uncertain relation between object sets and attribute sets. Therefore, it is important to study hierarchy fuzzy knowledge from a fuzzy formal context. In this paper, a kind of fuzzy decision formal context is proposed and (α, β) reduct based on this fuzzy decision formal context is defined. Furthermore, we propose a method to judge attribute consistent sets and reducts in fuzzy decision formal contexts. Finally, a Boolean method is also formulated to attribute reduction in fuzzy decision formal context from the view of the discernibility matrix.	fuzzy concept	Duo Pei;Mei-Zheng Li;Ju-Sheng Mi	2011	2011 International Conference on Machine Learning and Cybernetics	10.1109/ICMLC.2011.6016665	fuzzy logic;discrete mathematics;attribute domain;membership function;decision theory;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;formal concept analysis;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;lattice miner;boolean function;fuzzy set operations	DB	-2.930255773355818	-25.580839511664596	129898
2ed4e44dd8757226e00080cd3ec8e5384a48164b	discovery of time-inconsecutive co-movement patterns of foreign currencies using an evolutionary biclustering method	biclustering;geometric interpretation;foreign exchange rate;genetic algorithm;inconsecutive co movement pattern	This paper proposes an evolutionary biclustering algorithm to discover inconsecutive co-movement patterns of different foreign exchange rates. The rows/columns of a bicluster (i.e. a submatrix with a subset of rows and a subset of columns) are not necessarily consecutive. A typical bicluster with constant values on rows and/or columns is represented as a hyperplane in a high-dimensional space and the coefficients of the hyperplane are determined using a genetic algorithm. A detected bicluster demonstrates the co-moving behaviors of a subset of currencies in inconsecutive time periods, indicating that the currencies moved in different manners in some specific time periods. In our experiments, we relate these patterns to the geographically close economic connections and find out the correspondence between the nominal exchange rates and the economic conditions. The findings are useful as a guide for investing foreign currencies.	additive model;algorithm;biclustering;evolutionary computation;foreign exchange service (telecommunications);synthetic data	Qinghua Huang	2011	Applied Mathematics and Computation	10.1016/j.amc.2011.10.011	mathematical optimization;genetic algorithm;machine learning;data mining;mathematics;biclustering	Vision	1.3564853454292234	-36.50159008258972	130010
2a843b5151b74c394a204fe98603cd50f2ad7e32	rough set based uncertain knowledge expressing and processing	fuzzy set;uncertain knowledge processing;cloud model;rough set;uncertain knowledge expressing	Uncertainty exists almost everywhere. In the past decades, many studies about randomness and fuzziness were developed. Many theories and models for expressing and processing uncertain knowledge, such as probability & statistics, fuzzy set, rough set, interval analysis, cloud model, grey system, set pair analysis, extenics, etc., have been proposed. In this paper, these theories are discussed. Their key idea and basic notions are introduced and their difference and relationship are analyzed. Rough set theory, which expresses and processes uncertain knowledge with certain methods, is discussed in detail.	rough set	Guoyin Wang	2011		10.1007/978-3-642-21881-1_3	rough set;computer science;artificial intelligence;machine learning;data mining;mathematics;fuzzy set;dominance-based rough set approach	DB	-2.5775862136878427	-24.412637798840613	130336
ce1b1f8e8e50dd2dc8c20dd0a37623399e3316ab	rule-based modeling: precision and transparency	cluster algorithm;fuzzy systems fuzzy neural networks fuzzy sets fuzzy set theory fuzzy logic approximation algorithms character generation clustering algorithms control systems neural networks;user interface;rule based;user interface rule based modeling fuzzy set theory fuzzy logic fuzzy systems complexity reduction data driven construction function approximation measurements product space clustering local behavior clustering algorithm expert evaluation rule base maintenance operator training control systems design;indexing terms;fuzzy set theory;fuzzy logic;fuzzy clustering;fuzzy rule base;function approximation;complexity reduction;control system design;numerical approximation;product space;knowledge based systems;knowledge based systems modelling fuzzy set theory fuzzy logic function approximation;fuzzy system	This article is a reaction to recent publications on rulebased modeling using fuzzy set theory and fuzzy logic. The interest in fuzzy systems has recently shifted from the seminal ideas about complexity reduction toward data-driven construction of fuzzy systems. Many algorithms have been introduced that aim at numerical approximation of functions by rules, but pay little attention to the interpretability of the resulting rule base. We show that fuzzy rule-based models acquired from measurements can be both accurate and transparent by using a low number of rules. The rules are generated by product-space clustering and describe the system in terms of the characteristic local behavior of the system in regions identified by the clustering algorithm. The fuzzy transition between rules makes it possible to achieve precision along with a good qualitative description in linguistic terms. The latter is useful for expert evaluation, rule-base maintenance, operator training, control systems design, user interfacing, etc. We demonstrate the approach on a modeling problem from a recently published article.	algorithm;cluster analysis;fuzzy control system;fuzzy logic;fuzzy rule;fuzzy set;linear approximation;logic programming;numerical analysis;reduction (complexity);rule-based modeling;rule-based system;set theory;systems design	Magne Setnes;Robert Babuska;Henk B. Verbruggen	1998	IEEE Trans. Systems, Man, and Cybernetics, Part C	10.1109/5326.661100	fuzzy logic;rule-based system;discrete mathematics;index term;defuzzification;fuzzy clustering;product topology;adaptive neuro fuzzy inference system;function approximation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;knowledge-based systems;machine learning;data mining;mathematics;fuzzy set;fuzzy associative matrix;user interface;fuzzy set operations;fuzzy control system;reduction	ML	4.017939519873427	-27.527329435138196	130610
c100249844eb17f36d8b820d9a8675089350fa47	efficient active novel class detection for data stream classification	support vector machines pattern classification query processing;accuracy topology support vector machines labeling heuristic algorithms learning systems manuals;class classification accuracy active novel class detection multiclass data stream classification unseen class appearance identification class emergence covered feature space known classes incoming data point insider data point outsider data point covered space area class members label query class instances novelty detection class detection accuracy	"""One substantial aspect of data stream classification is the possible appearance of novel unseen classes which must be identified in order to avoid confusion with existing classes. Detecting such new classes is omitted by most existing techniques and rarely addressed in the literature. We address this issue and propose an efficient method to identify novel class emergence in a multi-class data stream. The proposed method incrementally maintains a covered feature space of existing (known) classes. An incoming data point is designated as """"insider"""" or """"outsider"""" depending on whether it lies inside or outside the covered space area. An insider represents a possible instance of an existing class, while an outsider may be an instance of a possible novel class. The proposed method is able to iteratively select those insiders (resp. outsiders) that are more likely to be members of a novel (resp. an existing) class, and eventually distinguish the actual novel and existing classes accurately. We show how to actively query the labels of the identified novel class instances that are most uncertain. The method also allows us to balance between the rapidity of the novelty detection and its efficiency. Experiments using real world data prove the effectiveness of our approach for both the novel class detection and classification accuracy."""	algorithm;curse of dimensionality;data point;emergence;feature vector;information;insider threat;novelty detection;open research;relevance;sensor;subject-matter expert	Mohamed-Rafik Bouguelia;Yolande Belaïd;Abdel Belaïd	2014	2014 22nd International Conference on Pattern Recognition	10.1109/ICPR.2014.487	computer science;machine learning;pattern recognition;data mining;one-class classification	DB	5.443259855177821	-37.544568286897494	130684
7d7cb08e751edb0b5519a041925f1f93200d0119	mining time-changing data streams	concept drift;decision tree;time change;efficient algorithm;random sampling;data stream;data streams;very large database;incremental learning;machine learning;stationary distribution;hoeffding bounds;decision trees;subsampling	Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.	algorithm;database;decision tree;experiment;machine learning;stationary process;tree (data structure)	Geoff Hulten;Laurie Spencer;Pedro M. Domingos	2001		10.1145/502512.502529	decision tree learning;computer science;machine learning;decision tree;pattern recognition;data mining;data stream mining;statistics	ML	-1.914531263279802	-36.709759244232174	130867
319abdbb2366e6d8ec3e370535eb332223027ef0	genetic algorithm for optimizing neural network based software cost estimation	software cost estimation;ann;genetic algorithm;cocomo ii;bp learning	Software engineering cost models and estimation techniques are used for number of purposes. These include budgeting, tradeoff and risk analysis, project planning and control, software improvement and investment analysis. The proposed work uses neural network based estimation, which is essentially a machine learning approach, is one of the most popular techniques. In this paper the author has proposed a 2 step process for software effort prediction. In first phase known as training phase neural network selects the matching class (datasets) for the given input, which is improved by optimizing the parameters of each individual dataset by Genetic algorithm. In second step known as testing phase, the prediction process is done by adaptive neural networks. The proposed method uses COCOMO-II as base model. The experimental results show that our method could significantly improve prediction accuracy of conventional Artificial Neural Networks (ANN) and has potential to become an effective method for software cost estimation.	artificial neural network;cocomo;effective method;genetic algorithm;it risk management;machine learning;neural networks;optimizing compiler;software development effort estimation;software engineering	Tirimula Rao Benala;Satchidananda Dehuri;Suresh Chandra Satapathy;Ch. Sudha Raghavi	2011		10.1007/978-3-642-27172-4_29	genetic algorithm;computer science;artificial intelligence;machine learning;cocomo;data mining	SE	8.116784948979774	-25.53350960595162	130947
ce1423b6502185f5bf7855ab424e3149b57737fd	fuzzy system modeling with the genetic and differential evolutionary optimization	fuzzy set theory;fuzzy systems;genetic algorithms;inference mechanisms;learning (artificial intelligence);kosko standard-additive-model;takagi-sugeno local linear model;zadeh center-of gravity model;differential evolutionary optimization;fuzzy system modeling;fuzzy-rule-base optimization;genetic optimization;inference method	This paper compares the performance of two provably successful evolutionary optimization tools in the optimization of a fuzzy-rule-base (FRB) for the three well known fuzzy modeling inference methods: Zadeh's (center-of gravity), Kosko's (standard-additive-model), and Takagi-Sugeno's (local linear) model. In the fuzzy system modeling of an uncertain data, FRB keeps the model information within the fuzzy rules. The initial fuzzy-rule-base for the evolutionary optimization algorithms is extracted using Bezdek's FCM. In the optimization, the normalized root mean square error of the training data is minimized for the fine-tuning of the FRB parameters for each of the inference models. The performance evaluation with the test cases indicates that differential evolutionary optimization achieves better results in terms of convergence speed and yields better parameters than the elitist genetic optimization	additive model;evolutionary algorithm;fuzzy cognitive map;fuzzy control system;fuzzy rule;mathematical optimization;mean squared error;performance evaluation;systems modeling;test case;uncertain data;utility functions on indivisible goods	Mehmet Bodur;Adnan Acan;Talip Akyol	2005	International Conference on Computational Intelligence for Modelling, Control and Automation and International Conference on Intelligent Agents, Web Technologies and Internet Commerce (CIMCA-IAWTIC'06)	10.1109/CIMCA.2005.11	mathematical optimization;test functions for optimization;genetic algorithm;computer science;artificial intelligence;machine learning;linear model;mathematics;fuzzy set;additive model;center of gravity;fuzzy control system	Robotics	6.557915585204874	-26.13980176373049	131062
e957931132154e4ebeef702ee83b620212cb0823	setting attribute weights for nearest neighour learning algorithms using c4.5	learning from example;learning algorithms;learning algorithm;decision tree;learning;algorithme apprentissage;arbol decision;nearest neighbor learning algorithms;aprendizaje;vecino mas cercano;apprentissage;inductive learning from examples;plus proche voisin;nearest neighbour;decision trees;arbre decision;apprentissage a partir d exemple	Nearest Neighbour (NN) learning algorithms utilize a distance function to determine the classification of testing examples. The attribute weights in the distance function should be set appropriately. We study situations where a simple approach of setting attribute weights using decision trees does not work well, and design three improvements. We test these new methods thoroughly using artificially generated datasets and datasets from the machine learning repository.		Charles X. Ling;John J. Parry;Hangdong Wang	1997	IJPRAI	10.1142/S0218001497000184	computer science;artificial intelligence;machine learning;decision tree;pattern recognition	ML	9.261678827689167	-32.74026978708427	131353
5b88a17c34d7d86a050c80782e46b01c29285593	a list-based compact representation for large decision tables management	decision analysis;np hard problem;decision support system;compact representation;decision support systems;heuristics;weed management;combinatorial optimisation;decision table;learning and explanation	Due to the huge size of the tables we manage when dealing with real decision-making problems under uncertainty, we propose turning them into minimum storage space multidimensional matrices. The process involves searching for the best order of the matrix dimensions, which is a NP-hard problem. Moreover, during the search, the computation of the new storage space that each order requires and copying the table with respect to the new order may be too time consuming or even intractable if we want a process to work in a reasonable time on an ordinary PC. In this paper, we provide efficient heuristics to solve all these problems. The optimal table includes the same knowledge as the original table, but it is compacted, which is very valuable for knowledge retrieval, learning and expert reasoning explanation	computation;decision table;expert system;heuristic (computer science);np-hardness;personal computer;the matrix	Juan A. Fernández del Pozo;Concha Bielza;Manuel Gómez	2005	European Journal of Operational Research	10.1016/j.ejor.2003.10.005	decision table;mathematical optimization;optimal decision;decision support system;computer science;artificial intelligence;weed control;heuristics;machine learning;np-hard;mathematics;algorithm	AI	-1.9011054805929883	-31.085808492770674	131648
cfeae161093cd75568843d12fd37377e55deff8a	logical entropy and logical mutual information of experiments in the intuitionistic fuzzy case		In this contribution, we introduce the concepts of logical entropy and logical mutual information of experiments in the intuitionistic fuzzy case, and study the basic properties of the suggested measures. Subsequently, by means of the suggested notion of logical entropy of an IF-partition, we define the logical entropy of an IF-dynamical system. It is shown that the logical entropy of IF-dynamical systems is invariant under isomorphism. Finally, an analogy of the Kolmogorov–Sinai theorem on generators for IF-dynamical systems is proved.	dynamical system;experiment;intuitionistic logic;mutual information	Dagmar Markechová;Beloslav Riecan	2017	Entropy	10.3390/e19080429	combinatorics;discrete mathematics;mathematics;truth table;conditional entropy;logical framework;information diagram;non-classical logic;joint quantum entropy;analogy;logical conjunction	Logic	-2.9566191978152623	-24.093110735729447	131792
64d8d0a8ec4a8a1eb3d94739a141a783abe76e9c	time-to-event analysis with artificial neural networks: an integrated analytical and rule-based study for breast cancer	modelizacion;partial logistic regression artificial neural networks;cancerology;model selection;npi;sistema experto;base de connaissances;aplicacion medical;survival data;cancer;analisis datos;tumor maligno;neural nets;censored sample;validacion;generacion automatica;technology;rule based;mammary gland;modele lineaire;rule based study;time to event analysis;sobrevivencia;knowledge extraction;pertinencia;linear regression;hombre;cox regression;rule extraction;orthogonal search rule extraction;calcul ensembliste;index;analyse temporelle;modelo lineal;mortalite;automatic rule generation time to event analysis artificial neural networks integrated analytical study rule based study breast cancer disease free survival risk stratification model selection benchmark linear model cox regression partial logistic regression artificial neural networks automatic relevance determination orthogonal search rule extraction;logistic regression;analisis temporal;automatic generation;risk score;time analysis;medical computing;benchmark linear model;glandula mamaria;modelisation;echantillon censure;data analysis;artificial neural networks;calculo conjunto;mortalidad;generation automatique;analisis regresion;science technology;mortality;disease free survival;feature extraction;pertinence;cancerologie;plannard;linear model;human;cox model;regresion lineal;artificial neural networks breast cancer predictive models risk analysis hazards diseases data mining logistics testing neural networks;modele cox;extraction connaissances;neurosciences;survie;analyse regression;artificial intelligence;extraccion conocimiento;base conocimiento;automatic relevance determination;analyse donnee;validation;regression analysis;medical application;tumeur maligne;glande mammaire;set computation;regression analysis cancer feature extraction knowledge based systems medical computing neural nets;cancerologia;relevance;computer science;risk stratification;systeme expert;neural network model;survival;reseau neuronal	This paper presents an analysis of censored survival data for breast cancer specific mortality and disease-free survival. There are three stages to the process, namely time-to-event modelling, risk stratification by predicted outcome and model interpretation using rule extraction. Model selection was carried out using the benchmark linear model, Cox regression but risk staging was derived with Cox regression and with Partial Logistic Regression Artificial Neural Networks regularised with Automatic Relevance Determination (PLANN-ARD). This analysis compares the two approaches showing the benefit of using the neural network framework especially for patients at high risk. The neural network model also has results in a smooth model of the hazard without the need for limiting assumptions of proportionality. The model predictions were verified using out-of-sample testing with the mortality model also compared with two other prognostic models called TNG and the NPI rule model. Further verification was carried out by comparing marginal estimates of the predicted and actual cumulative hazards. It was also observed that doctors seem to treat mortality and disease-free models as equivalent, so a further analysis was performed to observe if this was the case. The analysis was extended with automatic rule generation using Orthogonal Search Rule Extraction (OSRE). This methodology translates analytical risk scores into the language of the clinical domain, enabling direct validation of the operation of the Cox or neural network model. This paper extends the existing OSRE methodology to data sets that include continuous-valued variables.		Paulo J. G. Lisboa;Terence A. Etchells;Ian H. Jarman;M. S. Hane Aung;Sylvie Chabaud;Thomas Bachelot;David Pérol;Thérèse Gargi;Valérie Bourdès;Stéphane Bonnevay;Sylvie Négrier	2008	Neural networks : the official journal of the International Neural Network Society	10.1016/j.neunet.2007.12.034	computer science;artificial intelligence;machine learning;data mining;proportional hazards model;artificial neural network;statistics;cancer	NLP	9.305890653687571	-25.42740378070808	131971
e8f45ae4138f53ac0d1566fdd9b9350df3ced129	a statistical technique for comparing the accuracies of several classifiers	statistique;analyse multivariable;multivariate analysis;analisis datos;estudio comparativo;clasificador;etude comparative;data analysis;classifier;comparative study;statistics;pattern recognition;classificateur;analisis multivariable;analyse donnee;reconnaissance forme;statistical techniques;classification automatique;reconocimiento patron;automatic classification;clasificacion automatica;estadistica	Abstract   The problem of comparing the accuracies of several classifiers that have been applied to the same set of observations is considered. A strategy based on the application of repeated measures analysis procedures to dichotomous data is proposed for making the comparison. The strategy is illustrated using a set of data taken from the literature.		Stephen W. Looney	1988	Pattern Recognition Letters	10.1016/0167-8655(88)90016-5	econometrics;classifier;computer science;comparative research;data mining;mathematics;multivariate analysis;data analysis;statistics	Vision	9.112273034574763	-34.20499607328604	132026
ef3879f221b8e341ffa82dbdfc11b8915370efea	concise representations for approximate association rules	approximate association rules;nonredundant association rules concise representations approximate association rules association rule mining reliable basis;itemsets;generators;association rule mining redundant association ruled closed itemsets generator certainty factor;certainty factor;probability density function;redundancy elimination;concise representations;association rules;data mining;reliability theory;association rule mining;080109 pattern recognition and data mining;generator;association rules redundancy data mining itemsets information technology australia data analysis information retrieval;certainty factor association rule mining redundant association rules closed itemsets generator;redundancy;association rule;reliable basis;nonredundant association rules;closed itemsets;knowledge representation;knowledge representation data mining;redundant association rules	The quality of association rule mining has drawn more and more attention recently. One problem with the quality of the discovered association rules is the huge size of the extracted rule set. Often for a dataset, a huge number of rules can be extracted, but many of them can be redundant to other rules and thus useless in practice. Mining non-redundant rules is a promising approach to solve this problem. In this paper, we firstly propose a definition for redundancy; then we propose a concise representation called reliable basis for representing non-redundant association rules for both exact rules and approximate rules. We prove that the redundancy elimination based on the reliable basis does not reduce the belief to the extracted rules. We also prove that all association rules can be deduced from the reliable basis. Therefore the reliable basis is a lossless representation of association rules. Experimental results show that the reliable basis significantly reduces the number of extracted rules.	approximation algorithm;association rule learning;experiment;expert system;lossless compression;redundancy (engineering)	Yue Xu;Yuefeng Li;Gavin Shaw	2008	2008 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2008.4811257	knowledge representation and reasoning;association rule learning;computer science;machine learning;pattern recognition;data mining;mathematics	DB	-4.2573894337111575	-35.60813620492071	132153
7fc601b338f9a8eecf6c0d77a815350ea4d897ad	a self-organizing computing network for decision-making in data sets with a diversity of data types	tratamiento datos;belief networks;convertisseur;reseau croyance;decision support;fuzzy neural network;belief;systeme intelligent;theorie type;fuzzy neural nets;fuzzy set;information technology and systems;information systems self organizing computing network decision making intelligent system hidden computing cell data converters symbolic data numeric data fuzzy rules sense functions fuzzy neural networks belief distribution;neural nets;transfer functions;systeme aide decision;fuzzy rules;sistema inteligente;information technology;logique floue;distributed computing;tipo dato;data processing;conjunto difuso;logica difusa;traitement donnee;ensemble flou;reseau neuronal flou;technologie information;intelligence artificielle;data type;sistema ayuda decision;prise decision;probabilistic approach;indexing terms;fuzzy set theory;fuzzy sets;computer network;fuzzy logic;decision support system;toda ganancia al vencedor;croyance;machine learning;red celular;transfer function;funcion traspaso;enfoque probabilista;approche probabiliste;cell network;reseau cellulaire;type theory;decision support systems;intelligent system;autoorganizacion;calculo repartido;artificial intelligence;self organization;fonction transfert;inteligencia artificial;information system;learning artificial intelligence;reseau neuronal;creencia;type donnee;toma decision;tecnologia informacion;fuzzy sets information technology and systems decision support machine learning;winner take all;calcul reparti;red neuronal;systeme information;autoorganisation;convertidor;computer networks decision making fuzzy neural networks fuzzy sets fuzzy systems neural networks intelligent systems intelligent networks transfer functions information analysis;converter;neural network;tout le gain au vainqueur;sistema informacion;belief networks learning artificial intelligence fuzzy set theory transfer functions decision making decision support systems neural nets	A self-organizing computing network based on concepts of fuzzy conditions, beliefs, probabilities, and neural networks is proposed for decision-making in intelligent systems which are required to handle data sets with a diversity of data types. A sense-function with a sense-range and fuzzy edges is defined as a transfer function for connections from the input layer to the hidden layer in the network. By generating hidden cells and adjusting the parameters of the sense-functions, the network self-organizes and adapts to a training set. Computing cells in the input layer are designed as data converters so that the network can deal with both symbolic data and numeric data. Hidden computing cells in the network can be explained via fuzzy rules in a similar manner to those in fuzzy neural networks. The values in the output layer can be explained as a belief distribution over a decision space. The final decision is made by means of the winner-take-all rule. The approach was applied to a series of the benchmark data sets with a diversity of data types and comparative results obtained. Based on these results, the suitability of a range of data types for processing by different intelligent techniques was analyzed, and the results show that the proposed approach is better than other approaches for decision-making in information systems with mixed data types.	algorithm;artificial neural network;benchmark (computing);decision support system;degree distribution;frequency response;fuzzy rule;information system;level of measurement;neuron;organizing (structure);parameter (computer programming);self-organization;semantic heterogeneity;synapse;symbolic artificial intelligence;systems biology;test set;transfer function	Qingxiang Wu;T. Martin McGinnity;David A. Bell;Girijesh Prasad	2006	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2006.103	decision support system;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;fuzzy set;artificial neural network	DB	8.016055532739793	-29.435264368732984	132203
766e2e145f1d64c0c46998686a457536128d91d9	cold start approach for data-driven fault detection	unsupervised learning;data driven fault detection unsupervised models tennessee eastman process data cold start learning problem model learning abundant historical data supervised fault detection;fault detection data models predictive models support vector machines semisupervised learning principal component analysis monitoring;semisupervised learning cold start learning fault detection process monitoring;data handling;unsupervised learning data handling fault diagnosis;fault diagnosis	A typical assumption in supervised fault detection is that abundant historical data are available prior to model learning, where all types of faults have already been observed at least once. This assumption is likely to be violated in practical settings as new fault types can emerge over time. In this paper we study this often overlooked cold start learning problem in data-driven fault detection, where in the beginning only normal operation data are available and faulty operation data become available as the faults occur. We explored how to leverage strengths of unsupervised and supervised approaches to build a model capable of detecting faults even if none are still observed, and of improving over time, as new fault types are observed. The proposed framework was evaluated on the benchmark Tennessee Eastman Process data. The proposed fusion model performed better on both unseen and seen faults than the stand-alone unsupervised and supervised models.	benchmark (computing);cold start;fault detection and isolation;in the beginning... was the command line;semi-supervised learning;sensor;supervised learning;unsupervised learning	Mihajlo Grbovic;Weichang Li;Niranjan A. Subrahmanya;Adam K. Usadi;Slobodan Vucetic	2013	IEEE Transactions on Industrial Informatics	10.1109/TII.2012.2231870	semi-supervised learning;unsupervised learning;computer science;engineering;machine learning;group method of data handling;pattern recognition;data mining	SE	6.079378853745345	-35.48283562784592	132361
2959614635392f2a0b71ea0f697e0604f9c7bee0	evolutive identification of fuzzy systems for time-series prediction	multiobjective programming;programmation multiobjectif;optimisation;genetic operator;prediction error;fuzzy set;algoritmo busqueda;optimizacion;series system;algorithme recherche;algoritmo borroso;search algorithm;base connaissance;conjunto difuso;ensemble flou;time series;algoritmo genetico;identificacion sistema;system identification;systeme serie;complex i;sistema serie;analytical method;fuzzy algorithm;serie temporelle;serie temporal;algorithme genetique;algorithme flou;base conocimiento;multiobjective optimization;genetic algorithm;optimization;sistema difuso;systeme flou;identification systeme;fuzzy systems;fuzzy system;time series prediction;evolution;knowledge base;programacion multiobjetivo	This paper presents a new algorithm for designing fuzzy systems. It automatically identifies the optimum number of rules in the fuzzy knowledge base and adjusts the parameters defining them.This algorithm hybridizes the robustness and capability of evolutive algorithms with multiobjective optimization techniques which are able to minimize both the prediction error of the fuzzy system and its complexity, i.e. the number of parameters. In order to guide the search and accelerate the algorithm's convergence, new specific genetic operators have been designed, which combine several heuristic and analytical methods. The results obtained show the validity of the proposed algorithm for the identification of fuzzy systems when applied to time-series prediction.		Jesús González;Ignacio Rojas;Héctor Pomares	2002		10.1007/3-540-45712-7_50	mathematical optimization;defuzzification;adaptive neuro fuzzy inference system;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;time series;mathematics;fuzzy set operations;algorithm;fuzzy control system	NLP	8.26760021917016	-28.592215884549145	132425
b58f696e8c914a4efb152e142040f19b974b8f71	a knowledge acquisition model based on formal concept analysis in complex information systems	会议论文	Normally, in some complex information systems, the binary relation on domain of any attribute is just a kind of ordinary binary, which does not meet some common properties such as reflexivity, transitivity or symmetry. In view of the above-mentioned facts this paper attempts to employ FCA(Formal Concept Analysis), proposes a rough set model based on FCA, in which equivalence relations, dominance relations, similarity relations(or tolerance relations) and neighborhood relations on universe are expanded to general binary relations and problems in rough set theory are discussed based on FCA. Particularly, from the above description of complex information systems, we can see that the relation in domain of any attribute may be extremely complex, which often leads to high time complexity and space complexity in the process of knowledge acquisition. For above reason this paper introduces granular computing(GrC), which can effectively reduce the complexity to a certain extent.	formal concept analysis;information system;knowledge acquisition	Xiangping Kang;Duoqian Miao;Na Jiao	2015		10.1007/978-3-319-25783-9_26	knowledge base;knowledge management;knowledge engineering;knowledge extraction	NLP	-3.009034326837814	-25.63640652269476	132528
e2be23e4f6836af6ee1d80bbf6f812ee45622e61	an introduction to the heuristic programming system	representation;heuristic;hierarchical structure;learning;programming language;information retrieval;artificial intelligent;search;pattern recognition;artificial intelligence;game playing;description;data structure;language design;problem solving	"""ion is a process of changing a (possibly edited) description into an """"abstract"""" object, in which all indexes are undefined All pointers to external objects are replaced by pointers to local blocks, which represent the objects and all their properties (including subclass indicators) Essentially, this is just the process of changing a class member into a ( template) which contains only ( undefined member )'s. The improvement process will be illustrated by a somewhat detailed example, the generation of a subgoal description in a tictac-toe program Although the application is trivial, the idea behind this example is quite powerful Assume that the data structures of the program are """"squares"""", """"lines"""", """"subgoals"""", and """"descriptions"""" Each square has an integer-valued property called """"occupant"""" with values """"X """" , """" 0 """" or """"unoccupied"""". Each line has an integer-valued property called """"occupant"""" with values """" X """" , """" 0 """" , """"unoccupied"""", or """"blocked"""", and an integer-valued property """"number"""" which may have any value from zero to three (zero if the line is blocked, otherwise the number of occupants) Each line consists of three squares. A description of a subgoal consists of an unoccupied square, a list of lines and the squares which they contain, and the properties of the lines and squares. Each subgoal has two properties, its """"side"""" (""""X"""" or """"O"""") and Its """"subgoelvalue"""", with a value of for some n. The Interpretation is that If the present configuration should contain the specified lines and squares and if the player with the proper side should occupy the square, then the resulting configuration would lead to three in a line in n moves or less regardless of the moves made by the opponent (although if he has a more valuable subgoal, he might be able to achieve three In a line first, and therefore win). Assume now that the opponent, X, has just moved to square.I and that his move has created two subgoals, subgoal.J and subgoal.K, with values 1/2%nd 1/2, which cannot be simultaneously blocked Evidently his previous move occupied the square of a subgoal with subgoelvalue equal to the minimum of 1/2and 1/2. The problem is to create a description of this subgoal. The first step has already been done: the relevant objects, square.I, subgoal.J, and"""	data structure;heuristic;line level;undefined behavior	David K. Jefferson	1969			heuristic;data structure;computer science;artificial intelligence;theoretical computer science;machine learning;programming language;representation	AI	5.0409310845427315	-32.95518396805668	132678
4cfd5e13fbf6b2955fd6622ad83f5bafe4ae1bed	detect wi-fi network attacks using parallel genetic programming		Wi-Fi network have been widely used nowadays. However, Intrusion Detection System (IDS) researches on Wi-Fi network were few and difficult since there was no common dataset between researchers on this area. Recently, Kolias et al. [2] published a comprehensive Wi-Fi network dataset extracting from real Wi-Fi traces, which is called the AWID dataset. Gene programming has proven effective in detecting network attacks, but the processing time is quite slow. Today, the development of GPU technology for high-speed parallel processing, the study of parallel programming solutions is essential. In this paper, we examined the Parallel Genetic Programming (Karoo GP) [13] in wireless attack detection to improve detection rates and processing time. The experiments showed that the processing time of Karoo GP was significantly improved compared to standard GP.		Van Canh Vu;Tuan-Hao Hoang	2018	2018 10th International Conference on Knowledge and Systems Engineering (KSE)	10.1109/KSE.2018.8573378	machine learning;artificial intelligence;computer science;genetic programming;intrusion detection system;parallel processing	Metrics	7.492020290228329	-36.42874428325718	132775
28dfb62aaefa92bac845ba1e7c6a1743961fdce8	a scalable bottum-up data mining algorithm for relational databases	memory bound;retail data processing;virtual memory;bottom up;induction algorithms;pentium processor;query processing;perforation;performance;software performance evaluation;retail grocery database;testing;relational database;data mining;law;very large database;bottom up data mining algorithm;classification rule induction algorithm;bottom up rule generation;learning by example;machine learning;indexing;classification rules;knowledge acquisition;data mining algorithm;spatial databases;classification algorithms;bottom up data mining algorithm scalable algorithm relational databases machine learning induction algorithms very large databases memory bound virtual memory performance classification rule induction algorithm bottom up rule generation retail grocery database pentium processor;relational databases;very large databases;induction generators;data mining relational databases induction generators indexing machine learning operating systems classification algorithms spatial databases testing law;scalable algorithm;operating systems;deductive databases	Machine learning induction algorithms are difficult to scal e to very large databases because of their memory-bound nature. Using virtual memory results to a significant performance degradation. To overcome such shortcomings, we developed a classification rule induction algorithm for rel ational databases. Our algorithm uses a bottom-up rule generation strategy that is more effective for mining database s having large cardinality of nominal variables. We have successfully used our algorithm to mine a retail grocery database containing more than 1.6 million records in about 5 hours on a dual Pentium processor PC.	algorithm;computation;data mining;elegant degradation;level of measurement;machine learning;memory bound function;mined;partial template specialization;relational database;rule induction;scalability;time complexity	Giovanni Giuffrida;Lee G. Cooper;Wesley W. Chu	1998		10.1109/SSDM.1998.688125	statistical classification;relational database;computer science;data science;data mining;database;fsa-red algorithm	ML	9.664986295471131	-35.7902047076858	132889
5b112a7ce7c4950c5682521dd712378f9572a355	human-oriented information acquisition in sequential pattern classification: part i — single membership classification	decision theory and analysis;pattern recognition decision theory and analysis;classical bayesian strategies human oriented information acquisition single membership classification information acquisition sequential pattern classification human decision makers;pattern recognition;humans testing bayesian methods pattern classification classification algorithms entropy cybernetics	Information acquisition strategies which incorporate human heuristics are formulated for pattern classification tasks, and their effectiveness is evaluated. The heuristics are based on two observations: (1) human decision-makers tend to limit themselves to a subset of classes and to select features oriented toward this subject only; (2) human decision-makers typically use considerations related to the history of process, that is, to class probabilities in earlier stages, while classical Bayesian strategies consider only the current class probabilities. These heuristics are incorporated in four different strategies with which the authors experimented. The findings are useful for the development of decision aids whose information selection strategies may be tuned to the operator's information selection behaviour by offering the operator an aid which reflects his own information priorities.	bayesian network;heuristic (computer science);statistical classification	Moshe Ben-Bassat;Dov Te'eni	1984	IEEE Transactions on Systems, Man, and Cybernetics	10.1109/TSMC.1984.6313275	computer science;artificial intelligence;machine learning;pattern recognition;data mining;statistics	ML	-0.7300625038769367	-29.28495382278336	132901
247341e098d1091288eea00c6108816f64fbe4a1	"""mining incomplete data with many attribute-concept values and """"do not care"""" conditions"""	set theory;data mining;error analysis;humidity;approximation methods;probabilistic logic;temperature distribution	"""In this paper we present novel experimental results comparing two interpretations of missing attribute values: attribute-concept values and """"do not care"""" conditions. Experiments were conducted on 12 data sets with many missing attribute values using the MLEM2 rule induction system. In the experiments, three kinds of probabilistic approximations were used: singleton, subset and concept; with the error rate of the induced rules evaluated by ten-fold cross validation. The results of the experiments compared two interpretations of missing values, attribute-concept values and """"do not care"""" conditions, finding the best result among the three probabilistic approximations. The outcomes show that for two cases the better performance was accomplished using attribute-concept values, for one case the better performance was accomplished using """"do not care"""" conditions. For remaining three cases the difference in performance was not statistically significant (5% significance level)."""	approximation;cross-validation (statistics);experiment;missing data;rule 90;rule induction	Patrick G. Clark;Jerzy W. Grzymala-Busse	2015	2015 IEEE International Conference on Big Data (Big Data)	10.1109/BigData.2015.7363926	econometrics;data mining;mathematics;statistics	DB	0.5005404305593412	-31.50710267830134	132955
1d563671f2809be4b053287ec0ed269a0d88a764	applying machine learning to solve an estimation problem in software inspections	recoleccion dato;data gathering;ingenieria logiciel;software engineering;reseau bayes;machine learning;red bayes;bayes network;genie logiciel;learning artificial intelligence;reseau neuronal;collecte donnee;software inspection;red neuronal;neural network;apprentissage intelligence artificielle	We use Bayesian neural network techniques to estimate the number of defects in a software document based on the outcome of an inspection of the document. Our neural networks clearly outperform standard methods from software engineering for estimating the defect content. We also show that selecting the right subset of features largely improves the predictive performance of the networks.	artificial neural network;bayesian network;estimation theory;machine learning;software bug;software engineering	Thomas Ragg;Frank Padberg;Ralf Schoknecht	2002		10.1007/3-540-46084-5_84	computer science;artificial intelligence;machine learning;bayesian network;data mining;software inspection;artificial neural network;data collection	SE	7.530989377986636	-30.691288098895292	133045
d9c194ed4e3fc123ef62ddc0e0d8b2056e9ac93a	spam behavior recognition based on session layer data mining	bayes estimation;extraction information;data transmission;largeur bande;decision tree learning;bayesian classification;electronic mail;decision tree;analisis datos;information extraction;securite informatique;logique floue;logica difusa;correo electronico;arbol decision;data mining;classification;fuzzy logic;computer security;data analysis;estimacion bayes;fouille donnee;seguridad informatica;transmission donnee;decouverte connaissance;anchura banda;bandwidth;descubrimiento conocimiento;analyse donnee;busca dato;arbre decision;clasificacion;extraccion informacion;transmision datos;courriel;estimation bayes;knowledge discovery	Various approaches are presented to solve the growing spam problem. However, most of these approaches are inflexible to adapt to spam dynamically. This paper proposes a novel approach to counter spam based on spam behavior recognition using Decision Tree learned from data maintained during transfer sessions. A classification is set up according to email transfer patterns enabling normal servers to detect malicious connections before mail body delivered, which contributes much to save network bandwidth wasted by spams. An integrated Anti-Spam framework is founded combining the Behavior Classification with a Bayesian classification. Experiments show that the Behavior Classification has high precision rate with acceptable recall rate considering its bandwidth saving feature. The integrated filter has a higher recall rate than either of the sub-modules, and the precision rate remains quite close to the Bayesian Classification.	data mining	Xuan Zhang;Jianyi Liu;Yaolong Zhang;Cong Wang	2006		10.1007/11881599_160	fuzzy logic;naive bayes classifier;decision tree learning;biological classification;computer science;artificial intelligence;machine learning;decision tree;data mining;database;data analysis;computer security;information extraction;bandwidth;algorithm;data transmission	ML	6.384746850977327	-33.88024587542824	133192
fc0056a2485eebfe11c5f3b99820ed8b80c07761	enabling anomaly-based intrusion detection through model generalization		Anomaly-based intrusion detection by the means of machine learning techniques is extensively studied in the literature mainly due to its promise to detect new attacks. However, despite the promising reported results, it is hardly deployed to real world environments. The main challenge in its adoption is the discrepancy between the accuracy rates obtained during the classifier development process and the rates obtained during its use in production environments. Such a discrepancy is mainly caused by non-representative training databases and nongeneralizable (scenario-specific) classifier’s model. This paper presents a method to create intrusion databases, which aims at mimicking the production environments characteristics by using well-known tools. Moreover, we present and evaluate a new validation technique, which aims at ensuring the generalization capacity of the obtained models, reached using cross-validating with different intrusion databases. The evaluation tests showed the feasibility of the proposed method. The feature selection technique ensured the model generalization capacity, improving its accuracy rate by 13%, while testing in different intrusion databases. Finally, the proposed anomaly-based approach was compared with Snort, reaching an accuracy rate of 99% against 27% of Snort for detecting DoS attacks.	anomaly detection;database;discrepancy function;feature selection;intrusion detection system;machine learning;sensor;snort	Eduardo Viegas;Altair Olivo Santin;Vilmar Abreu;Luiz Eduardo Soares de Oliveira	2018	2018 IEEE Symposium on Computers and Communications (ISCC)	10.1109/ISCC.2018.8538524	feature selection;distributed computing;feature extraction;machine learning;computer science;intrusion;intrusion detection system;denial-of-service attack;artificial intelligence	Security	7.057979642041097	-37.60414825991349	133306
111735a34c095eba2d8ea20473f686d99222db5f	mining the future: predicting itemsets' support of association rules mining	itemsets;decision support;time series data mining;data mining itemsets association rules time series analysis history databases neural networks computer science expert systems predictive models;time series;data mining;association rule mining;mftp;time series analysis approach;time series analysis;association rule;word prediction;prediction accuracy;mftp data mining itemsets time series analysis approach	This paper proposes a novel research dimension in the field of data mining, which is mining the future data before its arrival, or in other words: predicting association rules ahead before the arrival of the data. To achieve that, we need only predict the itemsets' support, upon which association rules could be easily produced. A time series analysis approach (MFTP) is proposed to perform itemsets' support prediction task. The proposed technique outperforms other prediction techniques for short history. The conducted performance study showed good prediction accuracy and response time. Thus, we provide a new tool to provide more information in the decision support field	association rule learning;data mining;decision support system;response time (technology);time series	Shenoda Guirguis;Khalil M. Ahmed;Nagwa M. El-Makky;Alaaeldin M. Hafez	2006	Sixth IEEE International Conference on Data Mining - Workshops (ICDMW'06)	10.1109/ICDMW.2006.116	association rule learning;decision support system;computer science;data science;time series;pattern recognition;data mining;statistics	DB	-3.227488465719958	-35.45837839214383	133329
89d294b8b6c62e65ab50e12ca3055c50e0ef705e	applying fuzzy neural network to intrusion detection based on sequences of system calls	anomaly;intruder detector;fuzzy neural network;fuzzy neural nets;analisis datos;anomaly detection;securite informatique;reseau neuronal flou;intrusion detection;anomalie;data mining;anomalia;computer security;data analysis;fouille donnee;seguridad informatica;intrusion detection systems;analyse donnee;reseau neuronal;detecteur intrus;detector intruso;busca dato;red neuronal;systeme detection intrusion;neural network	Short sequences of system calls have been proven to be a good signature description for anomalous intrusion detection. The signature provides clear separation between different kinds of programs. This paper extends these works by applying fuzzy neural network (FNN) to solve the sharp boundary problem and decide whether a sequence is “normal” or “abnormal”. By using threat level of system calls to label the sequences the proposed FNN improves the accuracy of anomaly detection.	artificial neural network;intrusion detection system	Guiling Zhang;Jizhou Sun	2005		10.1007/11527503_58	anomaly-based intrusion detection system;intrusion detection system;telecommunications;computer science;artificial intelligence;machine learning;data mining;computer security;artificial neural network	ML	6.55579791660739	-33.612761296926834	133367
c3072e0409ef6eed93f0b9f74e947690a9cca68f	application of rough genetic algorithms	information retrieval;rough computing;highway classification;genetic algorithm;genetic algorithms;interval computing	This paper describes rough genetic algorithms based on the notion of interval values. A gene in a rough genetic algorithm can be represented using an interval. The paper explains how this generalization facilitates development of new genetic operators and evaluation measures. Various operations of rough genetic algorithms are illustrated using a simple document retrieval example. An experiment involving classification of interval-valued traffic patterns demonstrates the usefulness of rough genetic algorithms for real-world applications. A discussion on further extensions of rough genetic algorithms to rough sets is also included.	genetic algorithm	Pawan Lingras;Cedric Davies	2001	Computational Intelligence	10.1111/0824-7935.00156	quality control and genetic algorithms;genetic algorithm;computer science;artificial intelligence;theoretical computer science;machine learning;genetic representation;data mining	AI	0.04008966357354152	-26.609641203480468	133695
f6d052908934b0671b56777efca3ff772bb183f9	analyzing the impact of feature drifts in streaming learning		Learning from data streams requires efficient algorithms capable of deriving a model accordingly to the arrival of new instances. Data streams are by definition unbounded sequences of data that are possibly non stationary, i.e. they may undergo changes in data distribution, phenomenon named concept drift. Concept drifts force streaming learning algorithms to detect and adapt to such changes in order to present feasible accuracy throughout time. Nonetheless, most of works presented in the literature do not account for a specific kind of drifts: feature drifts. Feature drifts occur whenever the relevance of an arbitrary attribute changes through time, also impacting the concept to be learned. In this paper we (i) verify the occurrence of feature drift in a publicly available dataset, (ii) present a synthetic data stream generator capable of performing feature drifts and (iii) analyze the impact of this type of drift in stream learning algorithms, enlightening that there is room and the need for dynamic feature selection strategies for data streams.		Jean Paul Barddal;Heitor Murilo Gomes;Fabrício Enembreck	2015		10.1007/978-3-319-26532-2_3	real-time computing;machine learning;pattern recognition	NLP	-0.13569857224689352	-35.808581874275916	133931
f1e0aa5736510b73e03bf9e55b2f0e20b5d91a32	an efficient approach for intrusion detection using data mining methods	feature clustering intrusion detection approach data mining methods intrusion detection system ids internet interconnected network false alarm rate data mining techniques clustering analysis k means method hybrid data mining approach feature selection feature filtering;pattern clustering;computer network security;intrusion detection clustering algorithms classification algorithms data mining conferences accuracy partitioning algorithms;data mining;internet;feature extraction;pattern clustering computer network security data mining feature extraction internet;false alarm rate intrusion detection system data mining clustering k means ensemble detection rate	Intrusion Detection System (IDS) is becoming a vital component of any network in today's world of Internet. IDS are an effective way to detect different kinds of attacks in an interconnected network thereby securing the network. An effective Intrusion Detection System requires high accuracy and detection rate as well as low false alarm rate. This paper focuses on a hybrid approach for intrusion detection system (IDS) based on data mining techniques. The main research method is clustering analysis with the aim to improve the detection rate and decrease the false alarm rate. Most of the previously proposed methods suffer from the drawback of k-means method with low detection rate and high false alarm rate. This paper presents a hybrid data mining approach encompassing feature selection, filtering, clustering, divide and merge and clustering ensemble. A method for calculating the number of the cluster centroid and choosing the appropriate initial cluster centroid is proposed in this paper. The IDS with clustering ensemble is introduced for the effective identification of attacks to achieve high accuracy and detection rate as well as low false alarm rate.	cluster analysis;computer cluster;data mining;feature selection;filter (signal processing);internet;intrusion detection system;k-means clustering	Kapil Wankhade;Sadia Patka;Ravindra Thool	2013	2013 International Conference on Advances in Computing, Communications and Informatics (ICACCI)	10.1109/ICACCI.2013.6637422	anomaly-based intrusion detection system;anomaly detection;the internet;feature extraction;computer science;network security;machine learning;pattern recognition;data mining;cluster analysis	AI	5.156853323253208	-37.67610667656455	133980
f6a842eada0dc88565178ab7516e38620f0616d7	multiple labels associative classification	extraction information;association statistique;base donnee;analisis datos;information extraction;multi label classification;base donnee tres grande;data collection;database;base dato;statistical association;q science general;data mining;classification;association rule mining;hyperheuristic;qa75 electronic computers computer science;data analysis;frequent itemset;large scale;asociacion estadistica;regle association;regla asociacion;association rule;fouille donnee;scheduling;analyse donnee;very large databases;busca dato;clasificacion;extraccion informacion;ordonnancement;reglamento	Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence that integrating classification and association rule mining can produce more efficient and accurate classifiers than traditional techniques. In this paper, the problem of producing rules with multiple labels is investigated, and we propose a multi-class, multi-label associative classification approach (MMAC). In addition, four measures are presented in this paper for evaluating the accuracy of classification approaches to a wide range of traditional and multi-label classification problems. Results for 19 different data sets from the UCI data collection and nine hyperheuristic scheduling runs show that the proposed approach is an accurate and effective classification technique, highly competitive and scalable if compared with other traditional and associative classification approaches.	algorithm;association rule learning;data mining;database;heuristic;high- and low-level;hyper-heuristic;multi-label classification;plasma cleaning;scalability;scheduling (computing)	Fadi A. Thabtah;Peter I. Cowling;Yonghong Peng	2005	Knowledge and Information Systems	10.1007/s10115-005-0213-x	association rule learning;computer science;data science;pattern recognition;data mining;database;information extraction;one-class classification;statistics	ML	8.72286078696717	-33.70348127084862	134485
b3e9c6d996fb52dd83dee068353c54c6c992fd4c	a differential geometry perspective about multiple data streams preprocessing		In the Multiple Data Streams (MDS) environment, data sources generate data with no end in sight. Because of the difference of data sources, transaction numbers of MDS are not always equal to each other during a same period. Preprocessing MDS to obtain same number of samples for each stream is an essential step for lots of mining tasks. All existing preprocessing methods assume that data arrive simultaneously. However, this assumption may not be true in many real environments due to multiple data sources and different ways of data generating. This asynchronous issue is explored in this paper, by introducing the differential geometry as a trick. First, we establish a novel stream model called POLAR. The POLAR is an intrinsic surface spanned by time, probability and value. And then, we propose a preprocessing approach, called COPOLAR, to obtain same number of samples for each stream of MDS. COPOLAR first projects original observations onto POLAR; and then merges points with shortest geodesic distances along a geodesic on surface into mid-point on the same geodesic iteratively and incrementally until the number of points which we hope to obtain is met. Experimental results on synthetic and real data show that COPOLAR is effective in terms of maintaining characteristics of both statistics and vector.	data stream mining;experiment;http 404;preprocessor;sampling (signal processing);simulation;synthetic intelligence	Wen-Ping Li;Jing Yang;Jianpei Zhang	2015	Int. Arab J. Inf. Technol.		artificial intelligence;theoretical computer science;machine learning;data mining;algorithm;statistics	AI	-1.7068190028238812	-37.903372396504366	134547
70c5be5c02254393f23ef67e8cdbc035da517497	user modelling for interactive optimization using neural network	user modelling;optimisation;interactive optimization environmental resource optimization problem fuzzy logic technique nonlinear user modelling technique hpc based framework training data user rating criteria human fatigue algorithmic search human intuition automated search human participation np hard problems environmental planning systems multiobjective optimization problem user search criteria user preferences information retrieval systems neural network;neural nets;user modelling environmental science computing fuzzy logic human factors information retrieval neural nets optimisation parallel processing;information retrieval;collaborative search;environmental science computing;fuzzy logic;human factors;interactive algorithm;genetic algorithm;environmental planning user modelling genetic algorithm neural network collaborative search interactive algorithm;artificial neural networks computational modeling optimization computers search problems adaptation models;parallel processing;neural network;environmental planning	User modelling is one of the prominent research fields in information retrieval systems. In this paper, we model user's preferences and search criteria using an NN (Neural Network) to solve a multiobjective optimization problem specific to environmental planning systems. We argue that some NP hard problems cannot be solved alone either by a human or by a computer. Human participation in automated search is one way of combining human intuition with algorithmic search to solve such problems. However, even humans have some limitations for participation in that they cannot participate in search completely because of human fatigue. To overcome this, in our approach, an NN tries to model the user's rating criteria and preferences to help the user in rating large set of designs. Although training an NN with limited data is not always feasible, there are many situations where a simple modelling technique (e.g., linear/quadratic mapping) works better if the learning data set is small. In this paper we attempt to get more accuracy of the NN by generating data using other linear/non-linear techniques that fills the gap created by lack of sufficient training data. Also, we provided the architectural design of an HPC based framework we have proposed and compared the performance of the NN with fuzzy logic and other linear/non-linear user modelling techniques for the environmental resources optimization problem.	artificial neural network;batch processing;bayesian network;debora hammond;decision support system;experiment;fits;fuzzy logic;genetic algorithm;information retrieval;jane (software);machine learning;mathematical optimization;multi-objective optimization;multi-user;np-hardness;nonlinear system;optimization problem;particle swarm optimization;program optimization;randomness;reinforcement learning;supercomputer;support vector machine;vii;watershed (image processing);web search engine	Vidya Bhushan Singh;Snehasis Mukhopadhyay;Meghna Babbar-Sebens	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.560	fuzzy logic;genetic algorithm;computer science;artificial intelligence;machine learning;data mining;artificial neural network	Robotics	5.101227858875094	-29.432450155181606	134692
9c56cc2434b9729dbb2f6b7a0ac4083f8eccbae9	a note on bipolar fuzzy graph representation of concept lattice	fca;bipolar fuzzy graph;fuzzy concept lattice;formal fuzzy concept;complete lattice;formal concept analysis	Formal concept analysis (FCA) is a mathematical model based on lattice theory. FCA is successfully extended from crisp setting to fuzzy setting as well as interval-valued fuzzy setting. These extensions of fuzzy set define the data in the unipolar space and lack in representing bipolarity. However, bipolar fuzzy set represents the positive and negative information simultaneously. This extension has been applied recently in lattice theory, graph theory and in FCA as well. In this paper, we focus on introducing the link between bipolar fuzzy graph and concept lattice.	algorithm;formal concept analysis;fuzzy set;graph (abstract data type);graph theory;mathematical model;quantum information	Prem Kumar Singh;Cherukuri Aswani Kumar	2014	IJCSM	10.1504/IJCSM.2014.066426	fuzzy logic;combinatorics;discrete mathematics;membership function;complete lattice;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;formal concept analysis;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;lattice miner;fuzzy set operations	AI	-1.6797851528123087	-24.048528388283913	134844
f627cd8a059e88ff70ac98b399411c7ad2ea7b41	a new formulation of multi-category decision-theoretic rough sets	three way decision;bayesian decision theory;probabilistic approximation;rough sets;multi category classification	As a natural extension to the rough set approximations with two decision classes, this paper provides a new formulation of multicategory decision-theoretic rough sets. A three-way decision is added to each class which gives the user the flexibility of making a deferred decision. Different misclassification errors are treated separately with the notion of loss functions from Bayesian decision theory. The losses incurred for making deferred and rejective decisions to each class are also considered. The main objective of this paper is to tackle the limitations of the previous related work, and therefore provide a more complete solution to multi-category decision making.	rough set;theory	Bing Zhou	2011		10.1007/978-3-642-24425-4_66	rough set;optimal decision;influence diagram;bayes estimator;weighted product model;computer science;machine learning;decision tree;pattern recognition;data mining;decision rule;mathematics;admissible decision rule;evidential reasoning approach;evidential decision theory;weighted sum model;dominance-based rough set approach	Vision	-0.5588332038555831	-28.42840092972279	134900
398e4690423843b73f24655c9461dd8b70454d25	analysis of feature selection and ensemble classifier methods for intrusion detection			ensemble learning;feature selection;intrusion detection system	H. P. Vinutha;Poornima Basavaraju	2018	IJNCR	10.4018/IJNCR.2018010104	artificial intelligence;machine learning;feature selection;computer science;intrusion detection system;classifier (linguistics)	ML	8.399481497500286	-37.176816069802115	135073
0ef17c54862c55755f27782bea24e2329d33edb2	wun-miner: a new method for mining frequent weighted utility itemsets	cybernetics;itemsets;data mining;urban areas;informatics;conferences	In this paper, we propose the WUN-set (Weighted Utility Nodeset) structure, an extension of the Nodeset structure, to solve the problem of mining frequent weighted utility itemsets from a quantitative database. Firstly, some theorems are developed to compute quickly the weighted utility support of an itemset. An algorithm is then proposed for the fast mining frequent weighted utility itemsets. The experimental results on both sparse and dense databases show that the proposed method outperforms existing methods.	algorithm;database;sparse matrix	Huong Bui;Bay Vo;Ham Nguyen	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844428	cybernetics;computer science;artificial intelligence;data science;data mining;database;informatics	Robotics	-4.397320968967408	-37.402512935326534	135141
0ee5800d1f7ac0c9077d43d7daa1fc05f8517167	a new fca algorithm enabling analyzing of complex and dynamic data sets	fuzzy data;tree data structure;finite automata;formal concept analysis	Analyzing data with the use of Formal Concept Analysis (FCA) enables complex insights into hidden relationships between objects and features in a studied system. Several improvements in this research area, such as Fuzzy FCA or L-Fuzzy Concepts, bring the possibility to analyze data with a certain rate of indeterminacy. However, the usage of FCA on larger complex data brings several problems relating to the time-complexities of FCA algorithms and the size of generated concept lattices. The fuzzyfication of FCA emphasizes the mentioned problems. This article describes significant improvements of a selected FCA algorithm. The primary focus was given on the system of an effective data storage. The binary data was stored with the use of finite automata that leads to the lower memory consumption. Moreover, the better querying performance was achieved. Next, we focused on the inner process of the computation of all formal concepts. All improvements were integrated into a new FCA algorithm that can be used to analyze more complex data sets.	algorithm;dynamic data;formal concept analysis	Petr Gajdos;Václav Snásel	2014	Soft Comput.	10.1007/s00500-013-1176-6	computer science;formal concept analysis;theoretical computer science;machine learning;data mining;tree;finite-state machine;algorithm	ML	-4.0876697719592245	-27.192679510250453	135198
b0e86afea574dd4455ed7a8ac966679b92f2d928	a knowledge-light approach to regression using case-based reasoning	raisonnement base sur cas;razonamiento fundado sobre caso;ph d trinity college dublin;maintenance;interrogation base donnee;interrogacion base datos;intelligence artificielle;domain knowledge;analisis regresion;thesis;machine learning;computer science ph d;mantenimiento;analyse regression;artificial intelligence;regression analysis;inteligencia artificial;case based reasoning;use case;database query	Most CBR systems in operation today are ‘retrieval-onlyu0027 in that they do not adapt the solutions of retrieved cases. Adaptation is, in general, a difficult problem that often requires the acquisition and maintenance of a large body of explicit domain knowledge. For certain machine-learning tasks, however, adaptation can be performed successfully using only knowledge contained within the case base itself. One such task is regression (i.e. predicting the value of a numeric variable). This paper presents a knowledge-light regression algorithm in which the knowledge required to solve a query is generated from the differences between pairs of stored cases. Experiments show that this technique performs well relative to standard algorithms on a range of datasets.	case-based reasoning	Neil McDonnell;Padraig Cunningham	2006		10.1007/11805816_9	use case;case-based reasoning;computer science;artificial intelligence;machine learning;data mining;database;computer security;domain knowledge;algorithm;regression analysis	AI	7.7496040089668385	-32.40713405313916	135507
d56585c817ecb7b325b68a82df200d1f3b237531	bayesian inference approach for probabilistic analogy based software maintenance effort estimation	k nearest neighbors;belief networks;point estimation technique software maintenance effort estimation probabilistic analogy bayesian inference analogy based estimation;nearest neighbor searches;estimation theory;point estimation;software cost estimation;probability;software maintenance effort estimation;estimation method;software maintenance;analogy based estimation;bayesian inference;bayesian methods;inference mechanisms;maintenance engineering;point estimation technique;bayesian inference software maintenance software maintenance effort estimation probabilistic analogy based model k nearest neighbors;effort estimation;estimation;nearest neighbor;k nearest neighbor;predictive models;bayesian methods software maintenance predictive models nearest neighbor searches parametric statistics computer industry systems engineering and theory uncertainty software quality performance evaluation;probabilistic logic;software maintenance belief networks estimation theory inference mechanisms probability software cost estimation;probabilistic analogy;probabilistic analogy based model	Software maintenance effort estimation is essential for the success of software maintenance process. In the past decades, many methods have been proposed for maintenance effort estimation. However, most existing estimation methods only produce point predictions. Due to the inherent uncertainties and complexities in the maintenance process, the accurate point estimates are often obtained with great difficulties. Therefore some prior studies have been focusing on probabilistic predictions. Analogy Based Estimation (ABE) is one popular point estimation technique. This method is widely accepted due to its conceptual simplicity and empirical competitiveness. However, there is still a lack of probabilistic framework for ABE model. In this study, we first propose a probabilistic framework of ABE (PABE). The predictive PABE is obtained by integrating over its parameter k number of nearest neighbors via Bayesian inference. In addition, PABE is validated on four maintenance datasets with comparisons against other established effort estimation techniques. The promising results show that PABE could largely improve the point estimations of ABE and achieve quality probabilistic predictions.	bayesian approaches to brain function;cost estimation in software engineering;experiment;external validity;futures studies;gene prediction;interaction;oddworld: new 'n' tasty!;predictive modelling;similarity measure;software development effort estimation;software maintenance;statistical model;stepwise regression	Y. F. Li;Min Xie;Thong Ngee Goh	2008	2008 14th IEEE Pacific Rim International Symposium on Dependable Computing	10.1109/PRDC.2008.21	maintenance engineering;computer science;machine learning;pattern recognition;k-nearest neighbors algorithm;statistics	SE	3.822887050974645	-33.20772186987576	135569
7b0c07c67f38cea6edd2428d0e8e014df908d02c	directed acyclic concept graph based attribute oriented induction	directed graphs;relational databases data mining machine learning database systems sun bismuth computer industry mining industry data analysis computational efficiency;information loss;rule based;relational database;data mining;directed graphs data mining relational databases;knowledge discovery and data mining;rule derivation directed acyclic concept graph based attribute oriented induction knowledge discovery relational database;relational databases;knowledge discovery	This paper introduces an attribute oriented induction method on a directed acyclic concept graph to perform the task of knowledge discovery on a given relational database (table) for rule derivation. Our proposed method is different from previous approaches such as basic attribute oriented induction, rule-based attribute oriented induction, and path-id based attribute oriented induction in avoiding backtracking and information loss.		Junping Sun;Wenyi Bi	2001		10.1109/ICSMC.2001.972949	rule-based system;wait-for graph;relational model;relational calculus;relational database;computer science;machine learning;data mining;database;knowledge extraction	HCI	-1.9191421239920108	-28.821795359031505	135720
6e755b7caa7ddcd177fab34ee0e62841ee3ce63d	revisiting the central and peripheral immune system	artificial immune system;qa75 electronic computers computer science;discrete model;unsupervised machine learning;network model;immune system;context dependent;immune response	The idiotypic network has a long and chequered history in both theoretical immunology and Artificial Immune Systems. In terms of the latter, the drive for engineering applications has led to a diluted interpretation of the immunological models. Research inspired by theoretical immunology has produced compelling models of self-organised tolerance and immunity, but currently fail to have any practical engineering applicability. In this paper, we briefly discuss the engineering applicability of “self-affirming” idiotypic networks, leading to a suggestion that the “Third Generation” network models represent a way forward in this respect. Results obtained by implementing and extending a discrete model of this type of network suggest that the extended prototype is capable of two context-dependent modes of immune response, readily applicable to unsupervised machine-learning.	artificial immune system;context-sensitive language;machine learning;peripheral;prototype;self-organization;unsupervised learning	Chris McEwan;Emma Hart;Ben Paechter	2007		10.1007/978-3-540-73922-7_21	unsupervised learning;simulation;immune system;computer science;artificial intelligence;machine learning;immunology;artificial immune system	AI	7.370175224730802	-24.885811246493528	135968
554a2a7b471257ccb0ba1f2b21e97e355d50d176	a choice of relevant association rules based on multi-criteria analysis approach	itemsets;sorting;electre tri data mining knowledge discovery in database frequent itemsets association rules apriori algorithm quality measurements multiple criteria analysis;multicriteria analysis approach association rules;algorithm design and analysis data mining itemsets knowledge discovery classification algorithms sorting;data mining;classification algorithms;algorithm design and analysis;knowledge discovery	The usefulness and relevance of association rules extracted by the generation algorithms are a critical problem. In fact, in most cases, the real datasets lead to a very large number of association rules, which does not allow users to make their own selection of the most relevant. The searching of the best from the vast array of extracted rules require the identification and use of good measures or techniques of choice. Partial panoramas of these are presented in numerous publications. In this context, we propose a new approach to selecting relevant categories of association rules based on multi criteria analysis using association rules as actions and measures as criteria.	algorithm;association rule learning;common criteria;data mining;relevance;sorting	Addi Ait-Mlouk;Tarik Agouti;Fatima Gharnati;Badi Derbali	2015	2015 5th International Conference on Information & Communication Technology and Accessibility (ICTA)	10.1109/ICTA.2015.7426886	statistical classification;algorithm design;association rule learning;computer science;sorting;data science;pattern recognition;data mining;knowledge extraction	DB	-4.195179446888993	-36.425631157508825	136186
3bb8c658acbceadd66c64ffb479732457dd29740	knowledge discovery with clustering based on rules. interpreting results	cluster algorithm;systeme intelligent;knowledge based system;procesamiento informacion;adquisicion del conocimiento;sistema inteligente;apprentissage conceptuel;statistical test;acquisition connaissance;systeme base connaissances;automatic generation;artificial intelligent;aprendizaje conceptual;complex system;knowledge acquisition;graphical representation;information processing;data visualization;inductive learning;intelligent system;concept learning;missing data;information system;data analysis techniques;traitement information;systeme information;knowledge based systems;decision rule;large data;sistema informacion;knowledge discovery	It is clear that nowadays analysis of complex systems is an important handicap in Statistics, Artificial Intelligence, Information Systems, Data visualization, and other fields. Describing the structure or obtaining knowledge of complex systems is known as a difficult task. The combination of Data Analysis techniques (including clustering), Inductive Learning (knowledge-based systems), Management of Data Bases and Multidimensional Graphical Representation must produce benefits on this field. Clustering based on rules (CBR) is a methodology developed with the aim of finding the structure of complex domains, which performs better than traditional clustering algorithms or knowledge based systems approaches. In our proposal, a combination of clustering and inductive learning is focussed to the problem of finding and interpreting special patterns (or concepts) from large data bases, in order to extract useful knowledge to represent real-world domains. This methodology and its behaviour as a Knowledge Discovery has been, in fact, presented in previous papers ([3], [5], [2]...). The aim of this paper is to emphasize the reporting phase. Some tools oriented to the interpretation of the clusters are presented; automatic rules generation is presented and applied to a real research. Actually, in a KD system, data preparation and interpretation of the results is as important as the analysis itself. In this paper, missing data treatment is analysed; a statistical test, based on non parametric techniques, for comparing several classifications is presented. Also, a method for finding characteristic values of the classes is presented; this is based on the prototype of each class. Finally, these characterizations allow automatic generation of decision rules, as a predictive tool for future items.		Karina Gibert;Tomàs Aluja-Banet;Ulises Cortés	1998		10.1007/BFb0094808	statistical hypothesis testing;fuzzy clustering;missing data;computer science;artificial intelligence;knowledge-based systems;machine learning;data mining;decision rule;database;cluster analysis;data analysis;information system;algorithm;statistics	ML	-0.7706671504544185	-31.114059587795154	136231
e130c074094ac0a51d71a84b457f23b76ccd6007	it infrastructure downtime preemption using hybrid machine learning and nlp	neuro linguistic programming nlp;algorytmy;machine learning;uczenie maszynowe;algorithms;programowanie neurolingwistyczne nlp	IT Infrastructure Management and server downtime have been an area of exploration by researchers and industry experts, for over a decade. Despite the research on web server downtime, system failure and fault prediction, etc., there is a void in the field of IT Infrastructure Downtime Management. Downtime in an IT Infrastructure can cause enormous financial, reputational and relationship losses for customer and vendor. Our attempt is to address this gap by developing an innovative architecture which predicts IT Infrastructure failure. We have used a hybrid approach of human-machine interaction through Big Data, Machine Learning, NLP and IR. We sourced real-time machine, operating system, application logs and unstructured case notes into an algorithm for multi-dimensional symptoms mining, using iterative deepening depth-first search, traversal to create transactions for Sequential Pattern Mining of symptoms to events. It went through multiple statistical tests and review from technology experts, to create and update a dynamic Pattern Dictionary. This dictionary is used for training unsupervised and supervised classification models of machine learning, namely SVM and Random Forrest to score and predict new logs in a real time mode. The approach is also dynamic to use unsupervised clustering methods to give directions to the technicians on future or unknown pattern of errors or fault, to constantly update the Pattern Dictionary and improve classification for new IT products. General Terms—Experimentation, Algorithms, Service Support, Technology, Research.	apache forrest;artificial neural network;big data;blog;cluster analysis;dictionary;downtime;genetic algorithm;human–computer interaction;iterative deepening depth-first search;iterative method;machine learning;natural language processing;operating system;optimizing compiler;preemption (computing);real-time locating system;ripple effect;scalability;sequential pattern mining;server (computing);social media;supervised learning;tree traversal;web server	Chiranjiv Roy;Sourov Moitra;Mainak Das;Subramaniyan Srinivasan;Rashika Malhotra	2015		10.15439/2015F400	computer science;artificial intelligence;data science;operating system;machine learning;data mining;database;management;algorithm	ML	1.5654181500649615	-35.65295898018457	136318
ee58b43eda26c280c7deb3b9fc5aee4030420177	using genetic algorithms to evolve type-2 fuzzy logic systems for predicting bankruptcy		In this paper, we use GAs to design an interval type-2 fuzzy logic system (IT2FLS) for the purpose of predicting bankruptcy. The shape of type-2 membership functions, the parameters giving their spread and location in the fuzzy partitions and the set of fuzzy rules are evolved at the same time, by encoding all together into the chromosome representation. Type-2 FLSs have the potential of outperforming their type-1 FLSs counterparts, because a type-2 fuzzy set has a footprint of uncertainty that gives it more degrees of freedom. The enhanced Karnik-Mendel algorithms are employed for the centroid type-reduction and defuzzification stage. The performance in predicting bankruptcy is evaluated by multiple simulations, in terms of both in-sample learning and out-of sample generalization capability, using a type-1 FLS as a benchmark.	formal system;fuzzy logic;genetic algorithm	Vasile Georgescu	2017	Kybernetes	10.1108/K-06-2016-0152	mathematical optimization;defuzzification;type-2 fuzzy sets and systems;artificial intelligence;fuzzy number;machine learning;mathematics;fuzzy set operations	NLP	5.582752929139759	-26.419560121039723	136357
c17a4f6d910033c780f6be57f94b8af3d6504e0f	a hybrid approach for image recognition combining type-2 fuzzy logic, modular neural networks and the sugeno integral	image recognition;edge detection;fuzzy logic;hybrid approach;neural system;comparative study;fuzzy inference system;decision process;human perception;modular neural network;fuzzy system;neural network;sugeno integral	In this paper, a hybrid approach for image recognition combining type-2 fuzzy logic, modular neural networks and the Sugeno integral is described. Interval type-2 fuzzy inference systems are used to perform edge detection and to calculate fuzzy densities for the decision process. A type-2 fuzzy system is used for edge detection, which is a pre-processing applied to the training data for better use in the neural networks. Another type-2 fuzzy system calculates the fuzzy densities necessary for the Sugeno integral, which is used to integrate results of the neural network modules. In this case, fuzzy logic is shown to be a good methodology to improve the results of a neural system facilitating the representation of the human perception. A comparative study is also made to verify that the proposed approach is better than existing approaches and improves the performance over type-1 fuzzy logic. 2008 Elsevier Inc. All rights reserved.	artificial neural network;computer vision;edge detection;fuzzy control system;fuzzy logic;modular neural network;preprocessor;sugeno integral	Olivia Mendoza;Patricia Melin;Guillermo Licea Sandoval	2009	Inf. Sci.	10.1016/j.ins.2008.11.018	fuzzy logic;fuzzy electronics;edge detection;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;comparative research;mathematics;fuzzy associative matrix;perception;fuzzy set operations;artificial neural network;fuzzy control system;hybrid system;intelligent control	AI	5.047104685813174	-27.03594407168551	137067
7793754ec02c32a40aa9d572f1ae977d17af8bb3	constructing x-of-n attributes for decision tree learning	new attributes;decision tree learning;decision tree;constructive induction;classification;induction;prediction accuracy	While many constructive induction algorithms focus on generating new binary attributes, this paper explores novel methods of constructing nominal and numeric attributes. We propose a new constructive operator, X-of-N. An X-of-N representation is a set containing one or more attribute-value pairs. For a given instance, the value of an X-of-N representation corresponds to the number of its attribute-value pairs that are true of the instance. A single X-of-N representation can directly and simply represent any concept that can be represented by a single conjunctive, a single disjunctive, or a single M-of-N representation commonly used for constructive induction, and the reverse is not true. In this paper, we describe a constructive decision tree learning algorithm, called XofN. When building decision trees, this algorithm creates one X-of-N representation, either as a nominal attribute or as a numeric attribute, at each decision node. The construction of X-of-N representations is carried out by greedily searching the space defined by all the attribute-value pairs of a domain. Experimental results reveal that constructing X-of-N attributes can significantly improve the performance of decision tree learning in both artificial and natural domains in terms of higher prediction accuracy and lower theory complexity. The results also show the performance advantages of constructing X-of-N attributes over constructing conjunctive, disjunctive, or M-of-N representations for decision tree learning.	attribute–value pair;decision tree learning;discretization;disjunctive normal form;experiment;feature vector;fragmentation (computing);goto;greedy algorithm;hill climbing;inductive reasoning;influence diagram;machine learning;mathematical induction;sorting;test set;tree (data structure)	Zijian Zheng	2000	Machine Learning	10.1023/A:1007626017208	decision tree learning;biological classification;computer science;artificial intelligence;information gain ratio;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;mathematics;decision list;id3 algorithm;algorithm;decision stump	AI	5.395830245990603	-32.38265500291847	137180
16af2f0dc9a22fe9f03bf0732d9ce7e1df00ffc3	a near pattern-matching scheme based upon principal component analysis	near pattern matching;analyse multivariable;analisis componente principal;arbre recherche binaire;algoritmo busqueda;algorithm complexity;multivariate analysis;algorithme recherche;complejidad algoritmo;search algorithm;recherche documentaire;complexite algorithme;principal components analysis;arbol investigacion binaria;binary search tree;pattern matching;principal component analysis;recuperacion documental;analyse composante principale;analisis multivariable;binary search;document retrieval;concordance forme	In this paper, we present an efficient heuristic near pattern-matching scheme. Based upon an important multivariate analysis technique in statistics, called the principal components analysis, we develop algorithms to generate a set of new identifying keys for a given set of patterns to reduce the number of comparisons during the near-matching process. After some preprocessing work, the near-matching operation takes O(n log m) time in the worst case, where m is the number of identifying segments extracted from the patterns to be searched in a text file of length n.	algorithm;best, worst and average case;heuristic;pattern matching;preprocessor;principal component analysis	C. Y. Chen;Chin-Chen Chang;Richard C. T. Lee	1995	Pattern Recognition Letters	10.1016/0167-8655(94)00109-G	document retrieval;econometrics;computer science;machine learning;mathematics;algorithm;statistics;principal component analysis	Theory	-2.944413751652531	-34.63243326189491	137381
41aee23c98a4f941ba5948625351d337d4332e46	fuzzy clustering analysis for optimizing fuzzy membership functions	cluster algorithm;fuzzy c mean;learning algorithm;fuzzy membership function;theorie ensemble flou;heuristic method;algorithme apprentissage;fuzzy set theory;parameter identification;identificacion sistema;fuzzy clustering;system identification;clustering method;fonction appartenance;hybrid learning;membership function;fuzzy inference system;funcion pertenencia;algoritmo aprendizaje;identification systeme;fuzzy system;fuzzy model	Fuzzy model identification is an application of fuzzy inference system for identifying unknown functions, for a given set of sampled data. The most important thing for fuzzy identification task is to decide the parameters of membership functions (MFs) used in fuzzy systems. A lot of efforts (Chung and Lee, 1994; Jang, 1993; Sun and Jang, 1993) have been given to initialize the parameters of fuzzy membership functions. However, the problems of parameter identification were not solved formally. Assessments of these algorithms are discussed in the paper. Based on the fuzzy c-means (FCM) Bezdek (1987) clustering algorithm, we propose a heuristic method to calibrate the fuzzy exponent iteratively. A hybrid learning algorithm for refining the system parameters is then presented. Examples are demonstrated to show the effectiveness of the proposed method, comparing with the equalized universe method (EUM) and subtractive clustering method (SCM) Chiu (1994). The simulation results indicate the general applicability of our methods to a wide range of applications.	cluster analysis;membership function (mathematics)	Mu-Song Chen;Shinn-Wen Wang	1999	Fuzzy Sets and Systems	10.1016/S0165-0114(98)00224-3	fuzzy logic;membership function;defuzzification;system identification;fuzzy clustering;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	Logic	7.483970173593571	-28.889593143972156	137702
db97da74785e239c31e570bbfe931743cf865fea	derivation of the multilayer perceptron weight constraints for direct network interpretation and knowledge discovery	verification;biological patents;biomedical journals;adquisicion del conocimiento;etude theorique;fonction poids;text mining;validacion;europe pubmed central;detector;coaccion;citation search;contrainte;detecteur;acquisition connaissances;multilayer perceptron;satisfiability;citation networks;teoria decision;weight constraints;constraint;research articles;decision regions;theorie decision;abstracts;knowledge acquisition;open access;networked learning;estructura datos;decision theory;life sciences;estudio teorico;data relationships;funcion peso;clinical guidelines;significant inputs;validation;structure donnee;weight function;verificacion;theoretical study;full text;data structure;feature detector;rest apis;rules;orcids;validation and verification;europe pmc;biomedical research;bioinformatics;literature search;knowledge discovery	This paper examines the multilayer perceptron (MLP) network from a hidden layer decision region perspective and derives the output layer and hidden layer weight constraints that the network must satisfy in performing a general classification task. This provides a foundation for direct knowledge discovery from the MLP, using a new method published by the author, which finds the key inputs that the MLP uses to classify an input case. The knowledge that the MLP network learns from the training examples is represented as ranked data relationships and induced rules, which can be used to validate the MLP network. The bounds of the network knowledge are established in the n-dimensional input space and a measure of the limit of the MLP network knowledge is proposed. An algorithm is presented for the calculation of the maximum number of hidden layer decision regions in the MLP input space.		Marilyn Lougher Vaughn	1999	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(99)00062-3	detector;text mining;verification and validation;verification;weight function;data structure;decision theory;computer science;artificial intelligence;machine learning;feature detection;data mining;mathematics;constraint;multilayer perceptron;statistics;satisfiability	ML	-0.43180414211076906	-30.67387508851351	137844
05073fb306c677f099a0b3fd0a2be6c90106de0d	on an optimization representation of decision-theoretic rough set model	journal;parameters learning;attribute reduction;optimization representation;decision theoretic rough set model	Decision-theoretic rough set model can derive several probabilistic rough set models by providing proper cost functions. Learning cost functions from data automatically is the key to improving the applicability of decision-theoretic rough set model. Many region-related attribute reductions are not appropriate for probabilistic rough set models as the monotonic property of regions does not always hold. In this paper, we propose an optimization representation of decision-theoretic rough set model. An optimization problem is proposed by considering the minimization of the decision cost. Two significant inferences can be drawn from the solution of the optimization problem. Firstly, cost functions and thresholds used in decision-theoretic rough set model can be learned from the given data automatically. An adaptive learning algorithm and a genetic algorithm are designed. Secondly, a minimum cost attribute reduction can be defined. The attribute reduction is interpreted as finding the minimal attribute set to make the decision cost minimum. A heuristic approach and a particle swarm optimization approach are also proposed. The optimization representation can bring some new insights into the research on decision-theoretic rough set model.	mathematical optimization;rough set;theory	Xiuyi Jia;Zhenmin Tang;Wenhe Liao;Lin Shang	2014	Int. J. Approx. Reasoning	10.1016/j.ijar.2013.02.010	attribute domain;machine learning;pattern recognition;data mining;mathematics;dominance-based rough set approach	AI	-2.4788613297622266	-27.194264153865458	137955
ed7404707f93d9e81e79c964e9fdd84c108acde6	rough set model in incomplete and fuzzy decision information system based on improved-tolerance relation	fuzzy sets fuzzy systems information systems set theory management information systems fuzzy set theory uncertainty pattern recognition data mining data analysis;precision reduction rough set improved tolerance relation incomplete and fuzzy decision information system;rough set theory;fuzzy set theory decision support systems rough set theory;fuzzy set theory;decision support systems;incomplete information system rough set model fuzzy decision information system tolerance relation	The topical rough set theory is a forceful tool to handle the complete information system, and its effect to process incomplete information system is poor, particularly, its function to combine the incomplete information system with fuzzy decision information system is poorer. On the base of improving on the tolerance relation made by M. Kryszkiewez, this paper proposes the improved-tolerance relation and improved-tolerance classes; we simultaneously present the definition of the incomplete and fuzzy decision information system based on the improved-tolerance relation, and give its rough set model, i.e. the rough set model in incomplete and fuzzy decision information system. Finally, the conception of precision reduction is defined, and its algorithm is also supplied.	algorithm;information system;rough set;set theory	Da-kuan Wei;Xianzhong Zhou	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547285	fuzzy logic;rough set;decision support system;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;dominance-based rough set approach	DB	-2.0035493072404145	-26.113443915369192	138080
3f56b4e313441735f394b37ac74bc9b9acda6b9f	induction of fuzzy rules and membership functions from training examples	fuzzy controller;linguistic variable;fuzzy reasoning;expert systems;fuzzy rules;fuzzy clustering;learning methods;fuzzy expert system;machine learning;knowledge acquisition;membership functions;fuzzy inference;membership function;fuzzy if then rules;fuzzy decision rules;fuzzy machine learning;decision rule;expert system	Most fuzzy controllers and fuzzy expert systems must predefine membership functions and fuzzy inference rules to map numeric data into linguistic variable terms and to make fuzzy reasoning work. In this paper, we propose a general learning method as a framework for automatically deriving membership functions and fuzzy if-then rules from a set of given training examples to rapidly build a prototype fuzzy expert system. Based on the membership functions and the fuzzy rules derived, a corresponding fuzzy inference procedure to process inputs is also developed.	expert system;fuzzy logic;fuzzy rule;level of measurement;membership function (mathematics);prototype	Tzung-Pei Hong;Chai-Ying Lee	1996	Fuzzy Sets and Systems	10.1016/0165-0114(95)00305-3	fuzzy logic;combs method;fuzzy cognitive map;membership function;defuzzification;fuzzy clustering;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;decision rule;mathematics;fuzzy set;fuzzy associative matrix;expert system;fuzzy set operations;fuzzy control system	AI	3.816124174372468	-27.824715072655874	138192
9e507be749ad3f61a553093893e50366d9eed8b8	learning-based modelized combination of evidence		Evidence combination is typical uncertainty reasoning or information fusion in the theory of belief functions, which combines bodies of evidence stemming from different information sources. In traditional applications of evidence combination (e.g., pattern classification), given a sample, the basic belief assignments (BBAs) of different information sources are generated first, and then they are combined by a rule, e.g., Dempster's rule. In this paper, we propose a new modelized method for evidence combination. By just inputting the sample into the learned model of combination, a “combined” BBA is obtained. That is, it does not need to generate multiple BBAs for each sample for the combination. In our proposed modelized combination, we can generate different combination models with different combination rules. Experimental results and related analyses validate the rationality and efficiency of our proposed method.	experiment;nonlinear system;rationality;simulation;stemming	Deqiang Han;X. Rong Li	2018	2018 21st International Conference on Information Fusion (FUSION)	10.23919/ICIF.2018.8455635	machine learning;computer science;artificial intelligence;rationality;basic belief	Robotics	-0.3940602788070094	-28.819108317597692	138226
f54770be6bff5be977c272a2842a78da0411601c	piecewise hypersphere modeling by particle swarm optimization in qsar studies of bioactivities of chemical compounds		As the structural diversity in a quantitative structure-activity relationship (QSAR) model increases, constructing a good model becomes increasingly difficult, and simply performing variable selection might not be sufficient to improve the model quality to make it practically usable. To combat this difficulty, an approach based on piecewise hypersphere modeling by particle swarm optimization (PHMPSO) is developed in this paper. It treats the linear models describing the sought-for subsets as hyperspheres which have different radii in the data space. According to the attribute of each hypersphere, all compounds in the training set are allocated to hyperspheres to construct submodels, and particle swarm optimization (PSO) is applied to search the optimal hyperspheres for finding satisfactory piecewise linear models. A new objective function is formulated to determine the appropriate piecewise models. The performance is assessed using three QSAR data sets. Experimental results have shown the good performance of this technique in improving the QSAR modeling.	chemicals;dataspaces;feature selection;linear model;loss function;mathematical optimization;optimization problem;particle swarm optimization;piecewise linear continuation;quantitative structure-activity relationship;test set	Wei-Qi Lin;Jian-Hui Jiang;Qi Shen;Hai-Long Wu;Guo-Li Shen;Ruqin Yu	2005	Journal of chemical information and modeling	10.1021/ci049642m	piecewise;linear model;multi-swarm optimization;feature selection;piecewise linear function;mathematical optimization;hypersphere;machine learning;quantitative structure–activity relationship;artificial intelligence;mathematics;particle swarm optimization	AI	6.992813994353651	-26.41245217499224	138267
127e1fd6fc2444d166355f560ec681e42e3c8a4d	conflict negotiation process with stress parameters control for new classifier decision fusion scheme				Tarek M. Hamdani;Mohamed A. Khabou;Adel M. Alimi	2010			fusion;machine learning;negotiation;classifier (linguistics);pattern recognition;computer science;artificial intelligence	Vision	9.571850219936525	-26.407220445236987	138313
0ec54257c179478ee74adda3342442851699ad41	temporal-based fuzzy utility mining	data mining;fuzzy mining;temporal mining;upper bound;utility mining	In recent years, fuzzy utility mining has become an area of interest due to advancement of human reasoning. With regards to real applications, transactions in a database often involve things, such as transaction time, stamp, and much more. It is also noted that not all products in a store are displayed on the shelf, especially the seasonal ones. This paper, therefore, addresses these issues by presenting an effective framework called temporal-based fuzzy utility mining to give more attention to the transaction period of given items according to the concept of fuzzy utility mining. The temporal-based fuzzy utility mining proposed here is, however, a more complex approach when compared with the traditional fuzzy utility mining. A more complicated model for non-lost upper-bound fuzzy utility is thus proposed for effective mining. Furthermore, based on this model, a two-phase algorithm is developed for temporal-based fuzzy utility mining. Finally, the difference of fuzzy utility item sets with and without consideration of the lifetime of the items is shown by the experimental results under various experimental conditions.	algorithm;basic stamp;transaction time;two-phase locking	Wei-Ming Huang;Tzung-Pei Hong;Guo-Cheng Lan;Ming-Chao Chiang;Jerry Chun-Wei Lin	2017	IEEE Access	10.1109/ACCESS.2017.2774510	fuzzy logic;distributed computing;data mining;computer science;transaction time;database transaction	ML	-3.557860321393913	-29.525697519079735	138851
18ae4aa9d14b726b0ddc0bc6bafdf7283bac972a	optimal fusion rules in team classification under three decision structures	data structures;bayesian inference;maximum likelihood estimation;fuses;automation	In this paper, we study the performance of a team of dichotomous classifiers, where the classifiers' decisions are combined by logical fusion rules. Three decision structures are derived using the confusion matrix of a single classifier and a priori information, and the performances of the different decision structures are compared. First, we consider the performance of a team of three classifiers with a total of 256 fusion rules. Then, we propose a decision structure that utilizes a moderator, i.e., an entity that exploits Bayesian inference from individual classifiers' decisions and makes final decisions based on maximum likelihood classification. We show the benefits of using a moderator (compared to a decision structure without a moderator). Finally, we propose a decision structure that exploits pairing, i.e., fusing the classifiers' decisions sequentially two-by-two. Two pairing schemes, i.e., incremental and tournament-like, are proposed and we show that incremental pairing is the most effective decision structure among the proposed ones.	bayesian approaches to brain function;computation;computer-aided design;confusion matrix;fused filament fabrication;google moderator;naive bayes classifier;numerical analysis;oracle fusion architecture;performance	Songya Pan;Baro Hyun;Pierre T. Kabamba;Anouck R. Girard	2013	2013 American Control Conference		data structure;computer science;machine learning;pattern recognition;data mining;mathematics;maximum likelihood;statistics	ML	4.505042669183705	-32.31955399183368	138875
f7fd396ed27b7049e165e4a34d7a2fce0c5ff492	segmenting the e-commerce market using the generative topographic mapping	commerce electronique;systeme intelligent;comercio electronico;procesamiento informacion;e commerce;sistema inteligente;bayes theorem;consumer electronics;posterior probability;systeme base connaissances;fuzzy clustering;level of detail;backpropagation algorithm;information processing;generative topographic mapping;data visualization;intelligent system;algorithme retropropagation;self organized map;reseau neuronal;traitement information;market segmentation;red neuronal;knowledge based systems;electronic trade;neural network;algoritmo retropropagacion	The neural network-based Generative Topographic Mapping (GTM) (Bishop et al. 1998a, 1998b) is a statistically sound alternative to the well-known Self Organizing Map (Kohonen 1982, 1995). In this paper we propose the GTM as a principled model for cluster-based market segmentation and data visualization. It has the capability to define, using Bayes' theorem, a posterior probability of cluster/segment membership for each individual in the data sample. This, in turn, enables the GTM to be used to perform segmentation to different levels of detail or granularity, encompassing aggregate segmentation and one-to-one micro-segmentation. The definition of that posterior probability also makes the GTM a tool for fuzzy clustering/segmentation. The capabilities of the model are illustrated by a segmentation case study using real-world data of Internet users opinions on business-to-consumer electronic commerce.	e-commerce;generative topographic map;topography	Alfredo Vellido;Paulo J. G. Lisboa;Karon Meehan	2000		10.1007/10720076_43	computer vision;generative topographic map;information processing;fuzzy clustering;computer science;artificial intelligence;backpropagation;machine learning;level of detail;data mining;posterior probability;bayes' theorem;scale-space segmentation;data visualization;artificial neural network;algorithm;market segmentation;statistics	AI	8.519898303351997	-29.841214110992848	138995
56d5da0ca47323ece6bca55b7caea32431b0907f	fuzzy sliq decision tree algorithm	learning artificial intelligence decision trees fuzzy systems;machine learning algorithms;fuzzy membership function;decision tree;supervised learning;gini index;fuzzy supervised learning in quest;fuzzy binary decision tree;classification;gini index classification fuzzy decision tree fuzzy membership function;fuzzy sliq decision tree algorithm;machine learning;clouds;pattern classification;decision trees machine learning algorithms classification tree analysis clouds supervised learning machine learning pattern classification partitioning algorithms;classification tree analysis;fuzzy decision tree;learning artificial intelligence;uci machine learning repository;classification accuracy;decision trees;fuzzy systems;partitioning algorithms;uci machine learning repository fuzzy sliq decision tree algorithm fuzzy supervised learning in quest fuzzy binary decision tree;algorithms computer simulation decision support techniques fuzzy logic models theoretical pattern recognition automated	Traditional decision tree algorithms face the problem of having sharp decision boundaries which are hardly found in any real-life classification problems. A fuzzy supervised learning in Quest (SLIQ) decision tree (FS-DT) algorithm is proposed in this paper. It is aimed at constructing a fuzzy decision boundary instead of a crisp decision boundary. Size of the decision tree constructed is another very important parameter in decision tree algorithms. Large and deeper decision tree results in incomprehensible induction rules. The proposed FS-DT algorithm modifies the SLIQ decision tree algorithm to construct a fuzzy binary decision tree of significantly reduced size. The performance of the FS-DT algorithm is compared with SLIQ using several real-life datasets taken from the UCI Machine Learning Repository. The FS-DT algorithm outperforms its crisp counterpart in terms of classification accuracy. FS-DT also results in more than 70% reduction in size of the decision tree compared to SLIQ.	algorithm;decision boundary;decision tree learning;decision tree model;list of algorithms;machine learning;population parameter;real life;rule (guideline);supervised learning;digital tomosynthesis	Bala Chandra;P. Paul Varghese	2008	IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)	10.1109/TSMCB.2008.923529	influence diagram;decision tree model;decision tree learning;computer science;machine learning;decision tree;pattern recognition;alternating decision tree;incremental decision tree;data mining;information fuzzy networks;supervised learning;id3 algorithm;decision stump	ML	3.361435182264004	-30.245542256070692	139110
f8e72697648c0d21e5d83a9ebbe76b60ab0c8a91	oicrm: an ontology-based interesting co-location rule miner		Spatial co-location rule mining plays an important role in spatial data mining. The usefulness of co-location rules is strongly limited by the huge amount of discovered rules. To overcome this drawback, several methods were proposed in the literatures. However, being generally based on statistical information, most of these methods do not guarantee that the extracted co-location rules are interesting for the user. Thus, it is crucial to help the decision-maker with an efficient processing step to reduce the number of co-location rules. This demonstration presents OICRM, an interactive system to discover interesting co-location rules based on the ontology. First, the ontology is used to improve the integration of user knowledge; next, a powerful formula sub-system is designed to easily represent domain’s background and constraint knowledge; finally, OICRM has an interactive post-processing step (the secondary mining) to reduce the number of rules furthermore.		Xuguang Bao;Lizhen Wang;Meijiao Wang	2016		10.1007/978-3-319-45817-5_67	data mining;drawback;spatial analysis;computer science;ontology	ML	-3.693204718840807	-30.365705756182624	139158
650cbd8cfbef2cdf9675b6fbf8b4040ba99b1134	lvq neural network based classification decision approach to mechanism type in conceptual design	mechanical design;mechanism type selection;lvq neural network;knowledge acquisition	A decision approach to mechanism type selection is presented, which employs LVQ neural network as classifier and decision-maker to recognize a satisfactory mechanism from a range of mechanisms achieving a required kinematic function. Through learning from correct samples extracted from different mechanisms, expert knowledge is acquired and expressed in the form of weight matrix by LVQ network. When selecting mechanism type, through digitizing the design requirements, converting into a characteristic factor set, and fed into the trained LVQ network, a satisfactory mechanism can be automatically recognized from a range of mechanisms with the same kinematic function. Under this approach, the problem of knowledge acquisition and expression can be effectively solved, and the rationality of the decision can be improved at some extent. It is verified this approach is feasible to perform mechanism type selection and possesses a better characteristic of pattern classification compared with BP neural network.	artificial neural network;cluster analysis;knowledge acquisition;learning vector quantization;nonlinear system;rationality;requirement;tree accumulation	Jiande Wu	2011		10.1007/978-3-642-23896-3_46	computer science;artificial intelligence;machine learning;pattern recognition	AI	5.100326893986899	-28.131705411760635	139391
11f5c7c53710e322a832a1d6fb3ed5570f9adeb1	generalized categorization axioms		Categorization axioms have been proposed to axiomatizing clustering results, which offers a hint of bridging the difference between human recognition system and machine learning through an intuitive observation: an object should be assigned to its most similar category. However, categorization axioms cannot be generalized into a general machine learning system as categorization axioms become trivial when the number of categories becomes one. In order to generalize cate-gorization axioms into general cases, cate-gorization input and categorization output are reinterpreted by inner and outer category representation. According to the categoriza-tion reinterpretation, two category representation axioms are presented. Category representation axioms and categorization axioms can be combined into a generalized catego-rization axiomatic framework, which accurately delimit the theoretical categorization constraints and overcome the shortcoming of categorization axioms. The proposed ax-iomatic framework not only discuses catego-rization test issue but also reinterprets many results in machine learning in a unified way, such as dimensionality reduction, density estimation , regression, clustering and classification .	axiomatic system;bridging (networking);categorization;cluster analysis;dimensionality reduction;machine learning;peano axioms;statistical classification	Jian Yu	2015	CoRR		discrete mathematics;armstrong's axioms;machine learning;pattern recognition;mathematics;categorization	ML	5.833571132625234	-31.733519799575237	139424
94814fbb3a4c4f3a75141a33ae0b28b1201d44d3	methodology for handling uncertainty by using interval type-2 fuzzy logic systems	forecasting;condition initiale;prevision;theorie type;dato numerico;incertidumbre;chaos;uncertainty;logique floue;caos;additive noise;ruido aditivo;tipo dato;traitement incertitude;chaotic time series;logica difusa;bruit additif;uncertainty handling;donnee numerique;logical programming;data type;time series;fuzzy logic;condicion inicial;programmation logique;numerical data;type theory;serie temporelle;initial condition;serie temporal;fuzzy logic system;incertitude;systeme chaotique;type donnee;programacion logica;non linear system;chaotic systems	This paper proposes a methodology that is useful for han- dling uncertainty in non-linear systems by using type-2 Fuzzy Logic (FL). This methodology works under a training scheme from numerical data, using type-2 Fuzzy Logic Systems (FLS). Different training methods can be applied while working with it, as well as different training approaches. One of the training methods used here is also a proposal —the One-Pass method for interval type-2 FLS. We accomplished several experiments forecasting a chaotic time-series with an additive noise and obtained better performance with interval type-2 FLSs than with conventional ones. In addition, we used the designed FLSs to forecast the time-series with different initial conditions, and it did not affect their performance.	formal system;fuzzy logic	Germán Montalvo;Rogelio Soto	2004		10.1007/978-3-540-24694-7_55	fuzzy logic;uncertainty;data type;forecasting;computer science;artificial intelligence;fuzzy number;time series;control theory;mathematics;type theory;initial value problem;algorithm;statistics	Robotics	8.421793089549942	-28.6246154863466	139688
45c1002cfe42784cf1e9b82270265093cca098eb	a quick value reduction algorithm of rough set	value reduction;trees mathematics computational complexity rough set theory;attribute value tree;rough set theory;reduction computational complexity quick value reduction algorithm rough set attribute value tree model attribute order attribute reduction discrete table;trees mathematics;attribute reduction;attribute value tree rough sets decision table value reduction;computational complexity;complexity theory educational institutions rough sets computer science machine learning algorithms fuzzy systems decision making;rough sets;rough set;decision table;large data	In order to get a value reduction quickly, this paper puts forwards a new algorithm of value reduction based on attribute-value-tree model in attribute order and proves it's correctness. A attribute reduction and value reduction can be got quickly at the same time from a discrete table in this algorithm. The computational complexity of reduction is changed to O(|U|2|C|) where |U| and |C| are the number of objects and attributes. It is fit to process large data and validated to improve the efficiency by tests.	algorithm;computational complexity theory;correctness (computer science);rough set	Xiao Fan Wang;Baoshu Wang	2012	2012 9th International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2012.6233931	discrete mathematics;rough set;attribute domain;computer science;machine learning;data mining;mathematics;dominance-based rough set approach	EDA	-4.132500143335553	-36.356636105041574	139836
7663453dc033e70ef40c0662cf5c480ece42071f	discovery of decision rules by matching new objects against data tables	systeme intelligent;learning algorithm;adquisicion del conocimiento;systeme aide decision;sistema inteligente;algorithme apprentissage;sistema ayuda decision;acquisition connaissance;classification;decision support system;knowledge acquisition;intelligent system;information system;rough set;algoritmo aprendizaje;ensemble approximatif;breast cancer;clasificacion;systeme information;decision rule;sistema informacion	In this paper we present an exemplary algorithm classifying new objects by matching them directly against data table to generate relevant decision instead of matching it against all rules generated from data table (see 1]). We report results of experiments on three medical data sets, concerning lymphography, breast cancer and primary tumor (see 8]). We compare standard methods for extracting laws from decision tables (see e.g. 17], 1]), based on rough set (see 13]) and boolean reasoning (see 2]), with the method based on algorithms calculating relevant decision rules for new objects. We also compare the results of computer experiments on those data sets obtained by applying our system based on rough set methods with the results on the same data sets obtained with help of several data analysis systems known from literature.	algorithm;computer experiment;decision table;rough set;table (information)	Jan G. Bazan	1998		10.1007/3-540-69115-4_72	rough set;decision support system;biological classification;computer science;artificial intelligence;breast cancer;machine learning;data mining;decision rule;information system;algorithm	DB	-0.2839211631056716	-30.86053540490977	140065
6978803a2e0cf04c1b682f0697d2d6088cd365ba	knowledge modeling methods in the framework of evidence theory: an experimental comparison for melanoma detection	belief function;cancer;skin;evidence theory;inference mechanisms;uncertainty handling;data fusion;medical expert systems;data extraction;dempster shafer theory;dermatological lesion images knowledge modeling methods evidence theory melanoma detection dempster shafer theory data fusion imprecise data belief functions data fusion operators decision rule modeling methods real data;belief maintenance cancer skin medical expert systems inference mechanisms uncertainty handling;belief maintenance;data mining lesions attenuation malignant tumors data analysis upper bound probability distribution equations reliability theory;knowledge modeling;decision rule	The Dempster-Shafer theory, or evidence theory, is used in different fields such as data fusion, regression or classification. Within the framework of this theory, uncertain and imprecise data are represented using belief functions. Data fusion operators as well as the decision rule of this theory were largely developed and formalized. The aim of the paper is to present modeling methods of knowledge for the initialization of belief functions. Moreover, an experimental comparison of these different modeling methods on real data extracted from images of dermatological lesions is presented.	knowledge modeling	Eric Lefevre;Olivier Colot;Patrick Vannoorenberghe;Denis de Brucq	2000		10.1109/ICSMC.2000.884422	dempster–shafer theory;computer science;machine learning;pattern recognition;data mining;decision rule;sensor fusion;skin;statistics;cancer	Logic	-0.5338778237988165	-28.085222353942648	140105
fdd51b2bf642138c03950696ed8764dfa11fecda	inference in a fuzzy system using the consistence level of the rules			fuzzy control system	Armando Blanco;Miguel Delgado;Waldo Fajardo Contreras;Ignacio Requena	1994			adaptive neuro fuzzy inference system;machine learning;fuzzy control system;inference;artificial intelligence;computer science	Robotics	2.7707576613593656	-25.779414265220897	140115
ee2a4017aad747a94ffd89ce6b47d2573b918753	distribution discovery: local analysis of temporal rules	tratamiento datos;extraction information;regle temporelle;analisis datos;information extraction;data processing;traitement donnee;data mining;data analysis;fouille donnee;analyse locale;analyse donnee;incremental algorithm;synthetic data;busca dato;extraccion informacion	In recent years, there bas been increased interest in using data mining techniques to extract temporal rules from temporal sequences. Local temporal rules, which only a subsequence exhibits, are actually very common in practice. Efficient discovery of the time duration in which temporal rules are valid could benefit KDD of many real applications. In this paper, we present a novel problem class that is the discovery of the distribution of temporal rules. We simplify the mining problem and depict a model that could represent this knowledge clearly, uniquely and efficiently. Our methods include four online dividing strategies for different mining interest, an incremental algorithm for measuring rule-sets, and an algorithm for mining this knowledge. We have analyzed the behavior of the problem and our algorithms with both synthetic data and real data. The results correspond with the definition of our problem and reveal a kind of novel knowledge.		Xiaoming Jin;Yuchang Lu;Chunyi Shi	2002		10.1007/3-540-47887-6_47	data processing;computer science;artificial intelligence;data mining;data analysis;information extraction;algorithm;synthetic data	ML	-3.713492526815525	-32.53352748719842	140262
0be1305cdfefd69203b94bee04fbbcd86d927d61	characteristic relations in generalized incomplete information system	lost data;information systems;equivalent known values;rough set theory;similarity relation;classification;incomplete information;generalized incomplete information system;do not care data;information systems sun set theory artificial intelligence learning systems data mining computer science computer applications uncertainty rough sets;classification characteristic relations generalized incomplete information system compatibility relation similarity relation do not care data lost data rough set equivalent known values;data handling;rough set;rough set theory data handling information systems;characteristic class;compatibility relation;characteristic relations	In spite the fact that compatibility relation and similarity relation consider 'do not care' data as lost data, they were introduced to rough set to deal with incomplete information system. Incomplete information system in which 'do not care' data coexists with lost data, this article studies characteristic relation and discusses the unreasonable state which may be brought forth by characteristic class. On one hand, the two objects which do not have the same known value on a certain character seemed to be indiscernible. On the other hand, the two objects which have a lot of equivalent known values are very likely to be distinguished. In order to solve the problems of the classification in generalized incomplete information system, new kinds of characteristic relations named as I and II are brought forth and characteristic relation III is also constructed on advantages of characteristic relation I and II, and at last the validity of the newly constructed characteristic relations is also demonstrated with examples.	data mining;information system;intel matrix raid;internet information services;mined;rough set;set theory	Yunsong Qi;Lihua Wei;Huaijiang Sun;Yuqing Song;Quan-Sen Sun	2008	First International Workshop on Knowledge Discovery and Data Mining (WKDD 2008)	10.1109/WKDD.2008.59	discrete mathematics;rough set;computer science;machine learning;data mining;mathematics	DB	-2.399388088420314	-26.03164589993928	140311
752fd793f1769cb366737230a156fef09fd2038d	one double-reduct approach to get key rules and the experiment in prison computer information security	computers;key rules one double reduct approach prison computer information security data set;information security;association rules;set theory;information security double reduct key rules;association rules computers information security rough sets algorithm design and analysis decision making;double reduct;set theory knowledge engineering security of data;rough sets;security of data;algorithm design and analysis;key rules;knowledge engineering	We introduce one Double-Reduct approach to get key rules of data set in this paper. Adopting rule usefulness as a measure of key rules, our approach greatly reduces the rule numbers comparing to the traditional method. Through an experimental study of computer information security in prison, this approach is proved to be feasible and effective.	information security	Hanfei Lv	2012		10.1109/IPDPSW.2012.262	computer security model;algorithm design;rough set;association rule learning;computer science;information security;theoretical computer science;knowledge engineering;data mining;computer security;set theory	Crypto	-3.9327561414385843	-36.33906077601785	140315
b74e2f724943aa7b23667d949b38af5c6ad26b19	dealing with uncertainty in fuzzy inductive reasoning methodology	fuzzy inductive reasoning	The aim of this research is to develop a strat­ egy of reasoning under uncertainty in the context of the Fuzzy Inductive Reasoning methodology. This methodology allows the prediction of systems behavior by means of two different schemes. The first one corre­ sponds to a pattern prediction scheme, based exclusively on pattern rules. The second one corresponds to a purely Sugeno infer­ ence system, i.e. Sugeno prediction scheme. The Sugeno fuzzy rules are automatically ex­ tracted from the pattern rules producing a compact representation of the system mod­ elled. In this paper a mixed pattern/fuzzy rules scheme is studied to deal with uncer­ tainty in such a way that the best of both perspectives is used. The proposed scheme is applied to a real biomedical system, i.e. the central nervous system control of the cardio­ vascular system.	fuzzy rule;inductive reasoning;object process methodology;rule-based system;scheme	Francisco Mugica;Àngela Nebot;Pilar Gómez	2003			adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics	AI	2.3311433364337932	-26.458206502647176	140359
8f3d662778c27a140103ffe07ec1c0c801ad6824	contrast classification rules for mining local differences in medical data		In this paper, we formally prove that the classification rules formed on the basis of contrast patterns are guaranteed to be of a high quality. We propose to use the new ‘Sets of Contrasting Rules’ pattern for the identification of local differences between the classes of the dataset. Being essentially a contrast pattern formed of several classification rules, ‘Sets of Contrasting Rules’ pattern is guaranteed to have the values of lift and conviction superior to 1. This makes it being valuable for knowledge discovery in such domains as healthcare.	display resolution;experiment;medial graph	Marharyta Aleksandrova;Oleg Chertov;Armelle Brun;Anne Boyer	2017	2017 9th IEEE International Conference on Intelligent Data Acquisition and Advanced Computing Systems: Technology and Applications (IDAACS)	10.1109/IDAACS.2017.8095213	machine learning;knowledge extraction;conviction;artificial intelligence;data mining;computer science	DB	-1.6368252993907384	-34.75762911624083	140548
76475f27406dd89618690822934f0bc23dd21fa3	a statistical-heuristic feature selection criterion for decision tree induction	prepruning;bayesian classifier;decision trees working environment noise testing classification tree analysis learning systems bayesian methods knowledge acquisition pattern recognition noise measurement noise robustness;decision tree;bayes methods;statistical test;cost of complexity heuristic;multibranching decision trees;trees mathematics;middle cut preference;trees mathematics bayes methods decision theory pattern recognition statistics;tau criterion;proportional reduction in error;decision theory;statistics;pattern recognition;dynamic error estimation;robustness;feature selection;dynamic error estimation pattern recognition statistical heuristic feature selection criterion decision tree induction multibranching decision trees bayesian classifier built in statistical test proportional reduction in error cost of complexity heuristic robustness middle cut preference tau criterion prepruning;built in statistical test;decision tree induction;error estimate;statistical heuristic feature selection criterion	The authors present a statistical-heuristic feature selection criterion for constructing multibranching decision trees in noisy real-world domains. Real world problems often have multivalued features. To these problems, multibranching decision trees provide a more efficient and more comprehensible solution that binary decision trees. The authors propose a statistical-heuristic criterion, the symmetrical tau and then discuss its consistency with a Bayesian classifier and its built-in statistical test. The combination of a measure of proportional-reduction-in-error and cost-of-complexity heuristic enables the symmetrical tau to be a powerful criterion with many merits, including robustness to noise, fairness to multivalued features, and ability to handle a Boolean combination of logical features, and middle-cut preference. The tau criterion also provides a natural basis for prepruning and dynamic error estimation. Illustrative examples are also presented. >	decision tree;feature selection;heuristic	Xiao-Jia M. Zhou;Tharam S. Dillon	1991	IEEE Trans. Pattern Anal. Mach. Intell.	10.1109/34.85676	statistical hypothesis testing;naive bayes classifier;decision theory;computer science;machine learning;decision tree;pattern recognition;mathematics;feature selection;statistics;robustness	Vision	2.8791961814049003	-31.581911477850866	140549
7eef129c4d6d8cc6aa6b851ddd9ac5418d387880	a coal mine safety evaluation method based on concept drifting data stream classification	light emitting diodes;error analysis;monitoring;safety;classification algorithms;decision trees;coal mining	Monitoring data in coal mine is essentially data stream. With the change of environment, coal mine monitoring data stream implied concept drifts. Coal mine safety evaluation can be seen as concept drifting data stream classification. The method proposed in this paper is based on random decision tree model, and it uses Hoeffding Bounds inequality and information entropy instead of random selection to determine the split point, and it uses the threshold determined by Hoeffding Bounds inequality detect concept drift. Experimental results show the method can better detect concept drifts in data stream, and it has better classification accuracy for data stream, and it provides a new practical approach for coal mine safety evaluation.	concept drift;decision tree model;entropy (information theory);missing data;social inequality	Gang Sun;Zhongxin Wang;Jia Zhao;Hao Wang;Huaping Zhou;Kelei Sun	2016	2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2016.7603336	statistical classification;computer science;machine learning;decision tree;data mining;coal mining;light-emitting diode	Robotics	0.31966399786200683	-34.33924924347557	140609
d98a7759af1d08def17eb283c5d3967e24814065	choquet fuzzy integral based modeling of nonlinear system	fuzzy set;q measure;fuzzy measure;fuzzy rules;signature verification;benchmark problem;fuzzy integral;fuzzy rule base;fuzzy rule based identification;choquet integral;λ measure;membership function;fuzzy density;nonlinear system;non linear system;fuzzy model	For dealing with the adjacent input fuzzy sets having overlapping information, non-additive fuzzy rules are formulated by defining their consequent as the product of weighted input and a fuzzy measure. With the weighted input, need arises for the corresponding fuzzy measure. This is a new concept that facilitates the evolution of new fuzzy modeling. The fuzzy measures aggregate the information from the weighted inputs using the @l-measure. The output of these rules is in the form of the Choquet fuzzy integral. The underlying non-additive fuzzy model is investigated for identification of non-linear systems. The weighted input which is the additive S-norm of the inputs and their membership functions provides the strength of the rules and fuzzy densities required to compute fuzzy measures subject to q-measure are the unknown functions to be estimated. The use of q-measure is a powerful way of simplifying the computation of @l-measure that takes account of the interaction between the weighted inputs. Two applications; one real life application on signature verification and forgery detection, and another benchmark problem of a chemical plant illustrate the utility of the proposed approach. The results are compared with those existing in the literature.	nonlinear system	Smriti Srivastava;Madhusudan Singh;Vamsi Krishna Madasu;Madasu Hanmandlu	2008	Appl. Soft Comput.	10.1016/j.asoc.2007.07.001	fuzzy logic;mathematical optimization;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;nonlinear system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;choquet integral;fuzzy set operations;fuzzy control system	Robotics	1.1276960106067337	-25.157682330671665	140666
a379f30963b65bb837e38253276d688a36960c8c	attribute transformations on numerical databases. applications to stock market and economic data	attribute transformations;theoretical example;attribution transformation;attribute transformation;classical mathematics;simplest kind;linear transformation;logical pattern;stock market;numerical data mining;numerical databases;economic data	The effects of attribute transformations on numerical data mining are investigated. Theoretical examples from classical mathematics  are used to illustrate its critical-ness. The simplest kind of attribution transformations, linear transformations, is applied  to stock market and economic data. Some useful “predictive” rules are generated. Here “predictive” is used in the sense that  the logical patterns involve time elements.  	database	Tsau Young Lin;Joseph Tremba	2000		10.1007/3-540-45571-X_23	database	ML	-4.007677001286805	-26.77498966699084	140792
1c66356e3333f02557e3d1edc306d471d4f9efd6	artificial immune system for fraud detection	electronic commerce;artificial immune system;case base reasoning;online learning;genetics;genetic algorithms fraud credit transactions electronic commerce learning by example;learning by example;fraud;credit transactions;adaptive system;online learning case based genetic artificial immune system fraud detection credit card transactions electronic commerce stolen account numbers self adapted system case based learning model genetic algorithm;artificial immune systems immune system credit cards event detection electronic commerce biology computing fault detection artificial neural networks computer integrated manufacturing genetics;genetic algorithm;genetic algorithms;case based learning;fraud detection;credit cards;payment system	Credit card transactions continue to grow in number with the rapid growth of the electronic commerce, leading to a higher rate of stolen account numbers and subsequent losses by banks. Improved fraud detection thus has become essential to maintain the viability of the payment system. In this paper, we propose a case-based genetic artificial immune system for fraud detection (AISFD). It is a self-adapted system designed for credit card fraud detection. With the case-based learning model and genetic algorithm, the system can perform online learning with limited time and cost, and update the capability of fraud detection in the rapid growth of transactions and commerce activities.	artificial immune system;credit card fraud;e-commerce;genetic algorithm	Jianyong Tuo;Shouju Ren;Wenhuang Liu;Xiu Li;Bing Li;Lei Lin	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1399827	genetic algorithm;computer science;artificial intelligence;adaptive system;computer security	Robotics	9.001132481013	-26.73780669732275	141105
2fd7180293fca49a5f6fe46bfda5aca562cb4941	data mining approach using machine-oriented modeling: finding association rules using canonical names	databases;list;tratamiento datos;algorithme rapide;extraction information;modelizacion;base donnee;dato;lista;discurso;analisis datos;information extraction;data;database;universe of discourse;base dato;data processing;traitement donnee;data mining;data model;data modeling;modelisation;regle decision;data analysis;association rule;donnee;excavacion;discours;fast algorithm;relational model;liste;analyse donnee;association;regla decision;discourse;modeling;algoritmo rapido;decision rule;excavation;asociacion;extraction informacion;fouille	An attribute value, in a relational model, is a meaningful label of a collection of objects; the collection is referred to as a granule of the universe of discourse. The granule itself can be regarded a label of the collection (granule); it will be referred to as the canonical name of the granule. A relational model using these canonical names themselves as attribute values (their bit patterns or lists of members) is called a machine oriented data model. For moderate size databases, finding association rules, decision rules, and etc., are reduced to easy computation of set theoretical operations of these collections. In this paper, a very fast computing algorithm is presented.	algorithm;association rule learning;computation;data mining;data model;database;domain of discourse;granule (oracle dbms);relational model	Eric Louie;Tsau Y. Lin	2000		10.1117/12.381727	computer science;data mining;database;algorithm	DB	-3.222550414749335	-32.89417828688344	141221
6a578da5652a462d53e435aafad08f15d9722413	accuracy boosting induction of fuzzy rules with artificial immune systems	fuzzy reasoning;artificial immune system;pattern classification artificial immune systems data mining fuzzy reasoning fuzzy set theory learning artificial intelligence;boosting fuzzy systems artificial immune systems fuzzy sets data mining partitioning algorithms genetic algorithms computer science information technology control engineering computing;fuzzy rules;fuzzy inference accuracy boosting extension fuzzy rule induction artificial immune system method fuzzy partition learning fuzzy classification rule mining ifrais algorithm sequential set covering clonal selection algorithm;training;data mining;fuzzy set theory;cloning;accuracy;boosting;pattern classification;learning artificial intelligence;artificial immune systems;partitioning algorithms	The paper introduces accuracy boosting extension to a novel induction of fuzzy rules from raw data using artificial immune system methods. Accuracy boosting relies on fuzzy partition learning. The modified algorithm was experimentally proved to be more accurate for all learning sets containing non-crisp attributes.	algorithm;artificial immune system;experiment	Adam Kalina;Edward Mezyk;Olgierd Unold	2008	2008 International Multiconference on Computer Science and Information Technology	10.1109/IMCSIT.2008.4747233	brownboost;computer science;artificial intelligence;neuro-fuzzy;machine learning;pattern recognition;data mining;artificial immune system	AI	3.7055692494225028	-29.40844583224173	141807
6da87a2b8f426eeb59243828d94d50fc2e4396fc	the need for low bias algorithms in classification learning from large data sets	learning algorithm;analisis estadistico;analisis datos;error sistematico;large data sets;intelligence artificielle;classification;data analysis;statistical analysis;machine learning;bias;analyse statistique;classification error;artificial intelligence;analyse donnee;inteligencia artificial;variance decomposition;clasificacion;080110 simulation and modelling;erreur systematique;variance;variancia	This paper reviews the appropriateness for application to large data sets of standard machine learning algorithms, which were mainly developed in the context of small data sets. Sampling and parallelisation have proved useful means for reducing computation time when learning from large data sets. However, such methods assume that algorithms that were designed for use with what are now considered small data sets are also fundamentally suitable for large data sets. It is plausible that optimal learning from large data sets requires a different type of algorithm to optimal learning from small data sets. This paper investigates one respect in which data set size may affect the requirements of a learning algorithm  the bias plus variance decomposition of classification error. Experiments show that learning from large data sets may be more effective when using an algorithm that places greater emphasis on bias management, rather than variance management.	algorithm;computation;machine learning;norm (social);parallel computing;requirement;statistical classification;time complexity	Damien Brain;Geoffrey I. Webb	2002		10.1007/3-540-45681-3_6	semi-supervised learning;biological classification;computer science;artificial intelligence;machine learning;bias;data mining;variance;stability;data analysis;variance decomposition of forecast errors;active learning;algorithm;statistics;generalization error	ML	9.46455528174779	-33.48155196530257	142081
5eae8b781d570e9b7394b1344144a4463ddf44f8	multiple instance fuzzy inference neural networks		Fuzzy logic is a powerful tool to model knowledge uncertainty, measurements imprecision, and vagueness. However, there is another type of vagueness that arises when data have multiple forms of expression that fuzzy logic does not address quite well. This is the case for multiple instance learning problems (MIL). In MIL, an object is represented by a collection of instances, called a bag. A bag is labeled negative if all of its instances are negative, and positive if at least one of its instances is positive. Positive bags encode ambiguity since the instances themselves are not labeled. In this paper, we introduce fuzzy inference systems and neural networks designed to handle bags of instances as input and capable of learning from ambiguously labeled data. First, we introduce the Multiple Instance Sugeno style fuzzy inference (MI-Sugeno) that extends the standard Sugeno style inference to handle reasoning with multiple instances. Second, we use MI-Sugeno to define and develop Multiple Instance Adaptive Neuro Fuzzy Inference System (MI-ANFIS). We expand the architecture of the standard ANFIS to allow reasoning with bags and derive a learning algorithm using backpropagation to identify the premise and consequent parameters of the network. The proposed inference system is tested and validated using synthetic and benchmark datasets suitable for MIL problems. We also apply the proposed MI-ANFIS to fuse the output of multiple discrimination algorithms for the purpose of landmine detection using Ground Penetrating Radar.	adaptive neuro fuzzy inference system;algorithm;artificial neural network;backpropagation;benchmark (computing);dhrystone;encode;fuzzy logic;inference engine;multiple instance learning;radar;vagueness	Amine Ben Khalifa;Hichem Frigui	2016	CoRR		adaptive neuro fuzzy inference system;artificial intelligence;machine learning;data mining	ML	3.123815611502172	-27.806854943643323	142266
304b8a6ab97518a91f93cb27ca1dd7e75fe9bf8c	rule acquisition and attribute reduction in real decision formal contexts	rules acquisition;attribute reduction;concept lattice;real relation;formal concept analysis	Formal Concept Analysis of real set formal contexts is a generalization of classical formal contexts. By dividing the attributes into condition attributes and decision attributes, the notion of real decision formal contexts is introduced. Based on an implication mapping, problems of rule acquisition and attribute reduction of real decision formal contexts are examined. The extraction of ‘‘if–then’’ rules from the real decision formal contexts, and the approach to attribute reduction of the real decision formal contexts are discussed. By the proposed approach, attributes which are non-essential to the maximal s rules or l rules (to be defined later in the text) can be removed. Furthermore, discernibility matrices and discernibility functions for computing the attribute reducts of the real decision formal contexts are constructed to determine all attribute reducts of the real set formal contexts without affecting the results of the acquired maximal s rules or l rules.	formal concept analysis;fuzzy set;information system;maximal set;rough set;set theory	Hong-Zhi Yang;Yee Leung;Ming-Wen Shao	2011	Soft Comput.	10.1007/s00500-010-0578-y	formal system;discrete mathematics;computer science;formal concept analysis;machine learning;data mining;mathematics;lattice miner;algorithm	DB	-3.005821295918028	-25.789012367335904	142398
218b509529fb205b93fa5da3e1069f383915cbbe	approximate reduction in inconsistent formal decision contexts	lattices;approximate reduction;inconsistent formal decision context;data mining;approximation theory;concept lattice;congruence relation;congruence relation inconsistent formal decision context concept lattice approximate reduction;formal concept analysis formal decision contexts approximate reduction object power set discernibility matrices discernibility functions knowledge representation knowledge discovery rough set theory;knowledge representation;knowledge representation approximation theory data mining formal concept analysis;formal concept analysis	This paper deals with approaches to approximate reduction in inconsistent formal decision contexts. Congruence relations on the object power set are first introduced in a formal context. Then relationships between congruence relations and the corresponding concept lattices are discussed. Based on the congruence relations, notions of lower approximate and upper approximate attribute reduct are then developed in inconsistent formal decision contexts. Finally, discernibility matrices and discernibility functions associated with the proposed attribute reduction are defined, from which we can calculate all attribute reducts.	approximation algorithm;congruence of squares;decision problem;formal concept analysis;knowledge representation and reasoning;rough set;set theory;zeller's congruence	Xia Wang;Wei-Zhi Wu	2012	2012 IEEE International Conference on Granular Computing	10.1109/GrC.2012.6469272	knowledge representation and reasoning;combinatorics;discrete mathematics;computer science;formal concept analysis;congruence relation;lattice;mathematics;lattice miner;approximation theory	EDA	-2.932815525312226	-25.810677295956246	142761
00cfe4c8f0fafb99112cae64650c7c7de8b7c464	an efficient compression technique for frequent itemset generation in association rule mining	extraction information;association statistique;evaluation performance;algorithm performance;performance evaluation;analisis datos;information extraction;efficient algorithm;evaluacion prestacion;statistical association;data mining;association rule mining;data analysis;frequent itemset;asociacion estadistica;regle association;regla asociacion;association rule;fouille donnee;resultado algoritmo;relacion compresion;decouverte connaissance;performance algorithme;compression ratio;descubrimiento conocimiento;analyse donnee;taux compression;busca dato;extraccion informacion;knowledge discovery	Association Rule mining is one of the widely used data mining techniques. To achieve a better performance, many efficient algorithms have been proposed. Despite these efforts, we are often unable to complete a mining task because these algorithms require a large amount of main memory to enumerate all frequent itemsets, especially when dataset is large or the user-specified support is low. Thus, it becomes apparent that we need to have an efficient main memory handling technique, which allows association rule mining algorithms to handle larger datasets in main memory. To achieve this goal, in this paper we propose an algorithm for vertical association rule mining that compresses a vertical dataset in an efficient manner, using bit vectors. Our performance evaluations show that the compression ratio attained by our proposed technique is better than those of the other well known techniques.	algorithm;association rule learning;bendix g-15;bit array;computer data storage;data mining;enumerated type;performance evaluation	Mafruz Zaman Ashrafi;David Taniar;Kate Smith-Miles	2005		10.1007/11430919_16	association rule learning;computer science;artificial intelligence;data mining;knowledge extraction;information extraction;algorithm	ML	-3.508731713200408	-33.78953286954667	142889
842438c4308af094fbb057fb631aba777cb83c95	designing ensembles of fuzzy classification systems: an immune-inspired approach	fuzzy classification;sistema experto;modele agrege;dato numerico;clonal selection;immune algorithm;sistema inmunitario;ensemble of classifiers;teoria sistema;rule based;multiple solution;logique floue;clone;modelo agregado;base connaissance;logica difusa;donnee numerique;algoritmo inmunitario;classification;clona;fuzzy logic;systems theory;computer experiment;numerical data;biomimetique;solution multiple;theorie systeme;fonction appartenance;immune system;membership function;aggregate model;base conocimiento;funcion pertenencia;systeme expert;solucion multiple;clasificacion;fuzzy system;systeme immunitaire;biomimetics;knowledge base;expert system;algorithme immunitaire	In this work we propose an immune-based approach for designing of fuzzy systems. From numerical data and with membership function previously defined, the immune algorithm evolves a population of fuzzy classification rules based on the clonal selection, hypermutation and immune network principles. Once AIS are able to find multiple good solutions of the problem, accurate and diverse fuzzy systems are built in a single run. Hence, we construct an ensemble of these classifier in order to achieve better results. An ensemble of classifiers consists of a set of individual classifiers whose outputs are combined when classifying novel patterns. The good performance of an ensemble is strongly dependent of individual accuracy and diversity of its components. We evaluate the proposed methodology through computational experiments on some datasets. The results demonstrate that the performance of the obtained fuzzy systems in isolation is very good. However when we combine these systems, a significant improvement is obtained in the correct classification rate, outperforming the single best classifier.	algorithm;computation;experiment;fuzzy classification;fuzzy control system;level of measurement;numerical analysis	Pablo Alberto Dalbem de Castro;Guilherme Palermo Coelho;Marcelo F. Caetano;Fernando José Von Zuben	2005		10.1007/11536444_36	fuzzy logic;biomimetics;knowledge base;immune system;computer experiment;membership function;defuzzification;biological classification;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;clone;expert system;systems theory;algorithm	ML	9.6645680964761	-31.380704234169958	142957
af784f032e2351c1694f1ac7a9589b22549b64f0	auto-adaptative robot-aided therapy based in 3d virtual tasks controlled by a supervised and dynamic neuro-fuzzy system	tecnologias generalidades;tecnologias	— This paper presents an application formed by a classification method based on the architecture of ART neural network (Adaptive Resonance Theory) and the Fuzzy Set Theory to classify physiological reactions in order to automatically and dynamically adapt a robot-assisted rehabilitation therapy to the patient needs, using a three-dimensional task in a virtual reality system. Firstly, the mathematical and structural model of the neuro-fuzzy classification method is described together with the signal and training data acquisition. Then, the virtual designed task with physics behavior and its development procedure are explained. Finally, the general architecture of the experimentation for the auto-adaptive therapy is presented using the classification method with the virtual reality exercise.	adaptive resonance theory;artificial neural network;data acquisition;fuzzy classification;fuzzy control system;fuzzy set;neuro-fuzzy;robot;set theory;virtual reality	Luis Daniel Lledó;Arturo Bertomeu;Jorge A. Díez;Francisco J. Badesa;Ricardo Morales;José María Sabater;Nicolás García Aracil	2015	IJIMAI	10.9781/ijimai.2015.328	simulation;computer science;artificial intelligence;machine learning	Visualization	6.276254632676048	-28.953329193698885	143067
dd660d4c57542341f0577b4a0f42d00c9b64f0cd	r-functions based classification for abnormal software process detection	irregularity;calcul tolerant les pannes;boundary representation;regularisation;image processing;fault tolerant;securite;analytic representation;representation limite;procesamiento imagen;clasificador;intelligence artificielle;test bed;classification;traitement image;regularization;irregularite;representation analytique;fault tolerant computing;classifier;detection defaut;fault detection;safety;irregularidad;pattern recognition;classificateur;representacion limite;artificial intelligence;regularizacion;representacion analitica;inteligencia artificial;reconnaissance forme;reconocimiento patron;seguridad;deteccion imperfeccion;clasificacion;software process;defect detection	An R-functions based classification approach along with a regularization framework is proposed. The abnormal software process detection problem was used as the test bed. The R-functions based classification method is termed as the R-cloud method. The approach was validated both on synthetic and real-world data. Regularization allows to achieve good generalization and classification performance. In addition, the R-cloud approach gives the benefit of the analytical representation of the decision boundary. The introductory study on practical use of the R-cloud classifiers yielded promising results. The prototyping has shown that application of the R-functions based pattern recognition technique is a significant and practical tool for fault detection in providing fault tolerant computing.		Anton Bougaev;Aleksey M. Urmanov	2005		10.1007/11596448_147	regularization;fault tolerance;classifier;image processing;biological classification;computer science;artificial intelligence;machine learning;mathematics;boundary representation;software development process;fault detection and isolation;algorithm;testbed	SE	9.742295581562392	-30.813930339115856	143095
3f4e6561fbde37c8fcfbfca2bf4176cffa92a4fe	automating personal categorization using artificial neural networks	document clustering;metodo vectorial;learning algorithm;supervised learning;classification supervisee;system with n degrees of freedom;supervised classification;algorithme apprentissage;higher order;cuantificacion vectorial;vector quantization;modelo 2 dimensiones;systeme n degres liberte;vector method;high dimensional data;modele 2 dimensions;clasificacion supervisada;autoorganizacion;self organization;methode vectorielle;self organized map;document classification;classification automatique;reseau neuronal;sistema n grados libertad;automatic classification;algoritmo aprendizaje;data classification;clasificacion automatica;red neuronal;autoorganisation;two dimensional model;artificial neural network;learning vector quantization;neural network;quantification vectorielle	Organizations as well as personal users invest a great deal of time in assigning documents they read or write to categories. Automatic document classification that matches user subjective classification is widely used, but much challenging research still remain to be done. The self-organizing map (SOM) is an artificial neural network (ANN) that is mathematically characterized by transforming high-dimensional data into two-dimensional representation. This enables automatic clustering of the input, while preserving higher order topology. A closely related method is the Learning Vector Quantization (LVQ) algorithm, which uses supervised learning to maximize correct data classification. This study evaluates and compares the application of SOM and LVQ to automatic document classification, based on a subjectively predefined set of clusters in a specific domain. A set of documents from an organization, manually clustered by a domain expert, was used in the experiment. Results show that in spite of the subjective nature of human categorization, automatic document clustering methods match with considerable success subjective, personal clustering, the LVQ method being more advantageous.	algorithm;artificial neural network;categorization;cluster analysis;document classification;learning vector quantization;neural networks;organizing (structure);self-organization;self-organizing map;subject-matter expert;supervised learning	Dina Goren-Bar;Tsvi Kuflik;Dror Lev;Peretz Shoval	2001		10.1007/3-540-44566-8_19	self-organization;higher-order logic;learning vector quantization;computer science;artificial intelligence;machine learning;data mining;vector quantization;artificial neural network;algorithm;clustering high-dimensional data	Web+IR	9.82904715878782	-32.623230662644914	143139
8b197d7c7b83575e72c937385780dc5ae3c9b533	a practical approach to nonlinear fuzzy regression	metodo cuadrado menor;methode moindre carre;fuzzy regression;modele mathematique;least squares method;fuzzy data;normalisation;conical membership function;62j02;modele lineaire;fuzzy nonlinear models;regression model;non linear model;modelo matematico;modele non lineaire;modelo lineal;65c20;algorithme;function space;algorithm;least squares;modelo regresion;modelo no lineal;programacion lineal;espace fonctions;sistema borroso;modele regression;linear model;normalizacion;linear programming;mathematical model;programmation lineaire;systeme flou;62f99;standardization;fuzzy system;espacio funciones;algoritmo	This paper presents a new method of mathematical modeling in an uncertain environment. The uncertainties of data and model are treated using concepts of fuzzy set theory. The model fitting principle is the minimization of a least squares objective function. A practical modeling procedure is obtained by restricting the type of data and parameter fuzziness to conical membership functions. Under this restriction, the model fitting problem can be solved numerically with the aid of any least squares software for regression with implicit constraint equations. The paper contains a short discussion of the geometry of fuzzy point and function spaces with conical membership functions, and illustrates the application of fuzzy regression with an example from terminal ballistics. Key words, fuzzy nonlinear models, fuzzy data, fuzzy regression, least squares, conical membership function AMS(MOS) subject classifications. 62J02, 65C20, 62F99		Aivars K. Celmins	1991	SIAM J. Scientific Computing	10.1137/0912029	membership function;defuzzification;fuzzy classification;linear programming;fuzzy number;calculus;mathematics;fuzzy set;least squares;fuzzy set operations;algorithm;statistics	ML	9.461690583883149	-28.76761590754392	143449
e8bfa7121a36489f54b8d331889618cd1b389e9d	goal programming approach for multi-criteria decision-making for an energy efficient event recognition scheme	goal programming approach;active sensor nodes;pattern recognition approaches;pattern recognition energy consumption programming decision making wireless sensor networks optimization conferences;wireless sensor network applications;energy efficient event recognition scheme;energy consumption goal programming approach multicriteria decision making energy efficient event recognition scheme pattern recognition approaches wireless sensor network applications event detection pattern recognition approach active sensor nodes;multicriteria decision making;event detection;operations research;pattern recognition approach;wireless sensor networks decision making energy consumption mathematical programming operations research pattern recognition;mathematical programming;energy consumption;pattern recognition;optimization;programming;wireless sensor networks;conferences	Pattern recognition approaches are commonly adopted by many wireless sensor network applications due to their traditional effectiveness in detecting and recognizing events. Performance of a pattern recognition approach is directly proportional to the number of active sensor nodes. However, a high number of active nodes results in high energy consumption. Thus, the above situation can be considered as a multi-criteria decision-making problem, since the two factors are conflicting in nature. This paper proposes the application of goal programming approach to solve a multi-criteria decision-making problem considering pattern recognition success and energy consumption per node as the two decision-making criteria. Results suggest that goal programming is an efficient technique to address the above problem.	goal programming;pattern recognition;sensor web	Salman A. Khan;Mohamed Baqer	2013	2013 IEEE Symposium on Computational Intelligence in Multi-Criteria Decision-Making (MCDM)	10.1109/MCDM.2013.6595444	programming;mathematical optimization;wireless sensor network;computer science;artificial intelligence;machine learning;data mining	Vision	-0.7344935598688629	-27.7721264573837	143602
907ff38beb7cd41238e2e3cd5d539234d3f972ed	the cluster-indexing method for case-based reasoning using self-organizing maps and learning vector quantization for bond rating cases	cluster algorithm;self organizing maps;case base reasoning;corporate bonds;data mining;financial data;hybrid model;cluster indexing method;indexing method;indexation;bond rating;inductive learning;self organized map;case based reasoning;classification accuracy;artificial neural network;learning vector quantization	This paper presents a hybrid data mining model for the prediction of corporate bond rating. This model uses a new case-indexing method of case-based reasoning (CBR), which utilizes the cluster information of financial data in order to improve classification accuracy. This method uses not only case-specific knowledge of past problems like conventional CBR, but also uses additional knowledge derived from the clusters of cases. The cluster-indexing method assumes that there are some distinct subgroups (clusters) in each rated group. Competitive artificial neural networks are used to generate the centroid values of clusters because these techniques produce better adaptive clusters than statistical clustering algorithms. The experiments using corporate bond rating cases show that the cluster-indexing CBR is superior to conventional CBR and inductive learning-indexing CBR—a rival case indexing method.	case-based reasoning;learning vector quantization;organizing (structure);self-organization;self-organizing map	Kyung-Sup Kim;Ingoo Han	2001	Expert Syst. Appl.	10.1016/S0957-4174(01)00036-7	case-based reasoning;learning vector quantization;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	7.015533773331004	-34.035594264706596	143703
e0679297018a32ad77ab5c6e9911125bdfe99105	feature selection in mixed data: a method using a novel fuzzy rough set-based information entropy	fuzzy rough set theory;feature selection;information entropy;mixed data	Feature selection in the data with different types of feature values, i.e., the heterogeneous or mixed data, is especially of practical importance because such types of data sets widely exist in real world. The key issue for feature selection in mixed data is how to properly deal with different types of the features or attributes in the data set. Motivated by the fuzzy rough set theory which allows different fuzzy relations to be defined for different types of attributes to measure the similarity between objects and in view of the effectiveness of entropy to measure information uncertainty, we propose in this paper a fuzzy rough set-based information entropy for feature selection in a mixed data set. It is proved that the newly-defined entropy meets the common requirement of monotonicity and can equivalently characterize the existing attribute reductions in the fuzzy rough set theory. Then, a feature selection algorithm is formulated based on the proposed entropy and a filter-wrapper method is suggested to select the best feature subset in terms of classification accuracy. An extensive numerical experiment is further conducted to assess the performance of the feature selection method and the results are satisfactory. HighlightsA novel fuzzy rough set-based information entropy is constructed for mixed data.The proposed entropy can equivalently characterize the existing attribute reductions in the fuzzy rough set theory.A feature selection algorithm is formulated based on the proposed entropy.A filter-wrapper method is suggested to select a best feature subset.	entropy (information theory);feature selection;rough set	Xiao Zhang;Changlin Mei;Degang Chen;Jinhai Li	2016	Pattern Recognition	10.1016/j.patcog.2016.02.013	rough set;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;fuzzy set operations;feature;entropy	Vision	-2.6258568025976947	-27.32593017621307	144083
0517fa8309d6b6ce1460094de3c9b6c363535ad5	explainable ai for understanding decisions and data-driven optimization of the choquet integral		To date, numerous ways have been created to learn a fusion solution from data. However, a gap exists in terms of understanding the quality of what was learned and how trustworthy the fusion is for future—i.e., new—data. In part, the current paper is driven by the demand for so-called explainable AI (XAI). Herein, we discuss methods for XAI of the Choquet integral (ChI), a parametric nonlinear aggregation function. Specifically, we review existing indices, and we introduce new data-centric XAI tools. These various XAI-ChI methods are explored in the context of fusing a set of heterogeneous deep convolutional neural networks for remote sensing.	artificial neural network;convolutional neural network;fuzzy control system;information;nonlinear system;optimizing compiler;overfitting;trust (emotion)	Bryce Murray;Muhammad Aminul Islam;Anthony J. Pinar;Timothy C. Havens;Derek Anderson;Grant J. Scott	2018	2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2018.8491501	convolutional neural network;trustworthiness;machine learning;artificial intelligence;data-driven;computer science;nonlinear system;parametric statistics;choquet integral	Robotics	5.869135446754974	-25.2752771329435	144206
7777d65b424a25a067256ee41dae13c2119a5f74	homogeneity and stability in conceptual analysis		This work comes within the field of data analysis using Galois lattices. We consider ordinal, numerical single or interval data as well as data that consist on frequency/probability distributions on a finite set of categories. Data are represented and dealt with on a common framework, by defining a generalization operator that determines intents by intervals. In the case of distribution data, the obtained concepts are more homogeneous and more easily interpretable than those obtained by using the maximum and minimum operators previously proposed. The number of obtained concepts being often rather large, and to limit the influence of atypical elements, we propose to identify stable concepts using interval distances in a cross validation-like approach.	cross-validation (statistics);feature selection;interval arithmetic;maxima and minima;modal logic;modal operator;numerical analysis;ordinal data;recommender system;supervised learning	Paula Brito;Géraldine Polaillon	2011			calculus;mathematics;statistics	ML	1.7973111485074749	-25.135667140182278	144245
5022708381090f10899967bc24a5060ddd945bf5	mining classification rules without support: an anti-monotone property of jaccard measure	jaccard measure;anti monotony property;classification rules	We propose a general definition of anti-monotony, and study the anti-monotone property of the Jaccard measure for classification rules. The discovered property can be inserted in an Apriori-like algorithm and can prune the search space without any support constraint. Moreover, the algorithm is complete since, it outputs all interesting rules with respect to the measure of Jaccard. The proposed pruning strategy can then be used to efficiently find nuggets of knowledge.	apriori algorithm;computation;database;experiment;genetic algorithm;hereditary property;jaccard index;monotone	Yannick Le Bras;Philippe Lenca;Stéphane Lallich	2011		10.1007/978-3-642-24477-3_16	machine learning;pattern recognition;data mining;mathematics;jaccard index	DB	-1.8306176783319688	-34.59512069144165	144463
bf8267be7a70b1f9df982155f12c5786451c7756	an improved symbolic aggregate approximation distance measure based on its statistical features	dimension reduction;time series;classification;classi cation;statistical features;symbolic representation	The challenges in efficient data representation and similarity measures on massive amounts of time series have enormous impact on many applications. This paper addresses an improvement on Symbolic Aggregate approXimation (SAX), is one of the efficient representations for time series mining. Because SAX represents its symbols by the average (mean) value of a segment with the assumption of Gaussian distribution, it is insufficient to serve the entire deterministic information and causes sometimes incorrect results in time series classification. In this work, SAX representation and distance measure is improved with the addition of another moment of the prior distribution, standard deviation; SAX_SD is proposed. We provide comprehensive analysis for the proposed SAX_SD and confirm both the highest classification accuracy and the highest dimensionality reduction ratio on University of California, Riverside (UCR) datasets in comparison to state of the art methods such as SAX, Extended SAX (ESAX) and SAX Trend Distance (SAX_TD).	aggregate data;aggregate function;approximation;data (computing);dimensionality reduction;time series	Chaw Thet Zan;Hayato Yamana	2016		10.1145/3011141.3011146	machine learning;data mining;mathematics;statistics	ML	-2.6249180807235355	-35.18854138371023	144553
fc7161c4387a122e98f126598adeaa0ab7e143a4	using genetic algorithms to evolve a rule hierarchy	genetic engineering;genetic operator;systeme intelligent;learning algorithm;procesamiento informacion;adquisicion del conocimiento;sistema inteligente;knowledge extraction;algorithme apprentissage;acquisition connaissances;algoritmo genetico;data mining;association rule;knowledge acquisition;information processing;intelligent system;algorithme genetique;genetic algorithm;information system;traitement information;algoritmo aprendizaje;systeme information;sistema informacion	This paper describes the implementation and the functioning of RAGA (Rule Acquisition with a Genetic Algorithm), a genetic-algorithm-based data mining system suitable for both supervised and certain types of unsupervised knowledge extraction from large and possibly noisy databases. The genetic engine is modified through the addition of several methods tuned specifically for the task of association rule discovery. A set of genetic operators and techniques are employed to efficiently search the space of potential rules. During this process, RAGA evolves a default hierarchy of rules, where the emphasis is placed on the group rather than each individual rule. Rule sets of this type are kept simple in both individual rule complexity and the total number of rules that are required. In addition, the default hierarchy deals with the problem of over-fitting, particularly in classification tasks. Several data mining experiments using RAGA are described.	genetic algorithm	Robert Cattral;Franz Oppacher;Dwight Deugo	1999		10.1007/978-3-540-48247-5_32	genetic engineering;genetic algorithm;association rule learning;information processing;computer science;artificial intelligence;genetic operator;machine learning;data mining;mathematics;knowledge extraction;information system;algorithm	Theory	7.670427492724543	-31.908474614581458	144676
8bb6ab15c2ec18de3b9496699418c9f483f5349a	interval data classification under partial information: a chance-constraint approach	partial information;interval data;second order cone program;chance constraint	This paper presents a novel methodology for constructing maximum-margin classifiers which are robust to interval-valued uncertainty in examples. The idea is to employ chance-constraints which ensure that the uncertain examples are classified correctly with high probability. The key novelty is in employing Bernstein bounding schemes to relax the resulting chance-constrained program as a convex second order cone program. The Bernstein based relaxations presented in the paper require the knowledge of support and mean of the uncertain examples alone and make no assumptions on distributions regarding the underlying uncertainty. Classifiers built using the proposed methodology model interval-valued uncertainty in a less conservative fashion and hence are expected to generalize better than existing methods. Experimental results on synthetic and real-world datasets show that the proposed classifiers are better equipped to handle interval-valued uncertainty than state-of-the-art.	bounding volume hierarchy;margin classifier;phil bernstein;second-order cone programming;synthetic intelligence;with high probability	Sahely Bhadra;J. Saketha Nath;Aharon Ben-Tal;Chiranjib Bhattacharyya	2009		10.1007/978-3-642-01307-2_21	mathematical optimization;machine learning;data mining;mathematics;statistics	ML	1.8662532812927082	-30.445887548226338	144677
90be7186969472391d6f3ab27c38a37bf2755747	multideme evolution algorithm based approach to the generation of fuzzy systems	gray code;evolutionary computation;genetic algorithm multideme evolutionary algorithm optimization coding system multidimensional matrices error approximation membership functions fuzzy set theory tuning time series fuzzy system;neural networks;input variables;time series;fuzzy set theory;multidimensional matrices;coding system;computer architecture;tuning;error approximation;membership functions;membership function;genetic algorithm;genetic algorithms;evolutionary computation fuzzy systems input variables genetic algorithms transmission line matrix methods computer architecture microelectronics multidimensional systems neural networks algorithm design and analysis;optimization;multideme evolutionary algorithm;transmission line matrix methods;microelectronics;evolutionary algorithm;algorithm design and analysis;fuzzy systems;multidimensional systems;fuzzy system;time series fuzzy systems fuzzy set theory genetic algorithms	"""In this paper we propose a GA that is capable of simultaneously optimizing the structure of the system and tuning the parameters that define the fuzzy system. For this purpose, we use the concept of multiple-deme GAS, in which several populations with different structures (number of input variables) evolve and compete with other. In each of these populations, the element also has different numbers of membership functions in the input spaces and different numbers of rules. Instead of the normal coding system used to represent a fuzzy system, in which all the parameters are represented in vector form, we have performed coding by means of multidimensional matrices, in which the elements are real-valued numbers, rather than the traditional binary or Gray coding. I. Two primary tasks of fizzy system conshuction are s t r u m identification and paramadjusbnent. The former determines the number of input variables, its number of partition or membership hctions, and the number of fizzy rules. The latter identifies the set of parameters to optima appmximate the set of YO vectors a la given fizzy slrum. The design of a f h y system involves the strum of the rules of the system, and the membership firnction parameters. Typical examples using the technique of leaming h m examples can be found by applying either neural networks or genetic algorithms in the design of filzzv systems. The fonner approach has some drawback 1) it can only use numerical data pairs; 2) it does not always guarantee the optimal system performance due to easy trapping to local minimum solutioq and 3) it is not easy to interpret the created fuzzy rules due to its intemal representation of weights. Designing a fuzzy systems based on the GA's has been widely attempted since it can provide more possibilities of finding an optimal (or near-optimal) solution due to the implicit parallelism of GA's. GAS have the potential to be used to evolve both the f u q des and the corresponding fizzy set parameters. Some of the wok of fuzzy systems and GAS concentrates exclusively on tuning of membership functions [q or on the selecting an optimal set of fuzzy rules [8], while others attempt to derive rules and membership functions together [3]. To obtain optimal rule sets and optimal sets of membership hctions, it is preferable that both are acquired s~ultaneously. To 0ptimi.e the whole f k y system simultaneously, two structures will be used: one to encode the membership hctions and the other for the f k z y rules. Fuzzy MODELING BY GENETIC ALGORITHMS A. Membershipfinction coding The membership hctions are encoded within an """"inmmplete"""" matrix in which each mw represents one of the variables of the system, and where the columns encode the para mete^^ of the membership functions. Because each of the input variables of the system has a different number of membership functions, the chromosome structure used to store the membership functions is not a '%Omplete"""" matnix, as each of the m mws has a different number of columns n, , . As we have selected a triangular partition (TF'), the only parameter that needs to be stored is the centre of the triangular function [9]. A limy set X l is delined by a linguistic"""	artificial neural network;column (database);encode;fuzzy control system;genetic algorithm;implicit parallelism;level of measurement;linear algebra;maxima and minima;membership function (mathematics);numerical analysis;parallel computing;population;software release life cycle;triangular function	Ignacio Rojas;Héctor Pomares;Jesús González;Peter Gloesekoetter;J. Diestuhl;Karl Goser	2001		10.1109/FUZZ.2001.1008923	mathematical optimization;genetic algorithm;computer science;artificial intelligence;fuzzy number;theoretical computer science;machine learning;mathematics;fuzzy control system	AI	6.092061656560993	-26.556917390757825	144756
5933d1e010623a6b5ca92093a99f7968256fc07f	adaptive rule weights in neuro-fuzzy systems	linguistic model;learning algorithm;fuzzy neural nets;fuzzy set;theorie ensemble flou;fuzzy rules;fuzzy relation;reseau neuronal flou;algorithme apprentissage;systeme adaptatif;fuzzy set theory;modele linguistique;fuzzy rule base;neuro fuzzy;neuro fuzzy system;modelo linguistico;adaptive system;sistema adaptativo;reseau neuronal;relation floue;algoritmo aprendizaje;red neuronal;relacion difusa;fuzzy system;neural network	Neuro-fuzzy systems have recently gained a lot of interest in research and application. They are approaches that use learning techniques derived from neural networks to learn fuzzy systems from data. A very simple ad hoc approach to apply a learning algorithm to a fuzzy system is to use adaptive rule weights. In this paper, we argue that rule weights have a negative effect on the linguistic interpretation of a fuzzy system, and thus remove one of the key advantages for applying fuzzy systems. We show how rule weights can be equivalently replaced by modifying the fuzzy sets of a fuzzy system. If this is done, the actual effects that rule weights have on a fuzzy rule base become visible. We demonstrate at a simple example the problems of using rule weights. We suggest that neuro-fuzzy learning should be better implemented by algorithms that modify the fuzzy sets directly without using rule weights.	algorithm;approximation;artificial neural network;fuzzy classification;fuzzy control system;fuzzy logic;fuzzy rule;fuzzy set;hoc (programming language);machine learning;neuro-fuzzy;rule 90;rule-based system	Detlef D. Nauck	2000	Neural Computing & Applications	10.1007/s005210070036	defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;adaptive system;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy set operations;artificial neural network;algorithm;fuzzy control system	ML	8.759276932106932	-29.75841822411286	144865
07bd8ff0231cadb1fd666b2e9eeacba8f63ea313	classification rule-based models for malicious activity detection			logic programming	Vitali Herrera-Semenets;Osvaldo Andrés Pérez-García;Andrés Gago Alonso;Raudel Hernández-León	2017	Intell. Data Anal.	10.3233/IDA-163137	artificial intelligence;machine learning;classification rule;pattern recognition;computer science	Vision	8.444692009026413	-36.887645628754406	144916
6b5eeff392177fe6a77efa7204ca185a3d791af4	study of intelligent diagnosis system for mechanism wear fault based on fuzzy-neural networks	fuzzy neural network;spectrum;learning methods;fault diagnosis	According to fuzziness and complexity of mechanism wear fault,a kind of intelligent diagnosis system for mechanism wear fault based on fuzzyneural network is put forward. The structure and learning method of fuzzyneural network are introduced. This paper analysises how to create the characteristic vector of wear particles and standard wear particles spectrum by combine the characteristics of mechanism wear fault,and the fuzzy-neural network of mechanism wear fault intelligent diagnosis is built. Mechanism wear faults can be diagnosed to apply the fuzzy-neural network and fault causes are determined. This system is developed by applying MATLAB software and the GUI functione of MATLAB for the simplicity and intuition of diagnosis.	artificial neural network;graphical user interface;matlab	Sanmao Xie	2010		10.1007/978-3-642-18369-0_36	control engineering;artificial intelligence;machine learning	AI	8.020450837359249	-26.30381222799858	145038
4e8777e4c6c2b0fc0d2e08305308b5eb036e37b0	training classifiers for unbalanced distribution and cost-sensitive domains with roc analysis	analyse par courbe caracteristique d operation du recepteur;distribution donnee;ingenierie connaissances;receiver operator characteristic;competitive algorithms;intelligence artificielle;acquisition connaissances;classification;data distribution;error correcting output coding;roc;algorithme competitif;receiver operator characteristic curve;knowledge acquisition;roc analysis;artificial intelligence;analisis roc;inteligencia artificial;cost sensitive learning;adquisicion de conocimientos;distribucion dato;clasificacion;error correcting output code;knowledge engineering	ROC (Receiver Operating Characteristic) has been used as a tool for the analysis and evaluation of two-class classifiers, even the training data embraces unbalanced class distribution and cost-sensitiveness. However, ROC has not been effectively extended to evaluate multi-class classifiers. In this paper, we proposed an effective way to deal with multi-class learning with ROC analysis. An EMAUC algorithm is implemented to transform a multi-class training set into several two-class training sets. Classification is carried out with these two-class training sets. Empirical results demonstrate that the classifiers trained with the proposed algorithm have competitive performance for unbalanced distribution and cost-sensitive domains.	receiver operating characteristic	Xiaolong Zhang;Chuan Jiang;Mingjian Luo	2006		10.1007/11961239_8	random subspace method;computer science;artificial intelligence;machine learning;knowledge engineering;pattern recognition;receiver operating characteristic	ML	9.724328275227192	-32.99240385596369	145171
6c742b75e540b820bde33a8e6f79a626d28d84ed	propagation of knowledge from crisp and soft clustering through a granular hierarchy	granular computing;graph theory;cluster quality crisp rough fuzzy clustering granular graphs knowledge propagation mobile call mining;pattern clustering;rough clustering knowledge propagation crisp clustering soft clustering granular hierarchy information granules clustering schemes hierarchical granular graphs phone calls phone numbers fuzzy clustering;indexes mobile handsets clustering algorithms transforms upper bound data mining hybrid intelligent systems;pattern clustering fuzzy set theory granular computing graph theory;fuzzy set theory	Information granules allow us to abstract real world objects using their relevant attributes. Level of abstraction allows us to create a network of information granules that are connected to each other through a real-world relationship. For example, the phone calls are connected to the origin and destination phone numbers. This paper describes the relationship between clustering schemes by propagating the information through the connection between granules. We focus on the hierarchical granular graphs, where phone numbers represent the higher level (coarser) granules that are connected to phone calls which are lower level (finer) granules. The paper studies the effect of transferring crisp and fuzzy clustering schemes through the granular hierarchy. The fuzzy clustering is shown to provide a better technique for such a transfer. Rough clustering derived from fuzzy clustering is shown to be a good instrument for comparing crisp and fuzzy clustering schemes.	arithmetical hierarchy;chomsky hierarchy;cluster analysis;fuzzy clustering;mobile phone;rough set;software propagation;telephone number	Pawan Lingras;Parag Bhalchandra;Santosh Khamitkar;Satish Mekewad;Ravindra Rathod	2012	2012 12th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2012.6421301	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;flame clustering;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;mathematics;hierarchical clustering;cluster analysis;brown clustering;biclustering;affinity propagation;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	AI	1.1883506817007654	-37.969536846908575	145216
08807165b9e86963559772ec9874b47d6eee3626	a rough set based pcm for authorizing grid resources	phase change materials;hierarchical clustering;authorizing grid resources;pattern clustering;security rule checking;decision tree;grid authorization systems;authorisation;rough set theory;primitive clustering mechanism;hierarchical clustering mechanism;grid authorization;force;dynamic environments;dynamic environments rough set based pcm authorizing grid resources grid authorization systems security policy storage security rule checking hierarchical clustering mechanism brute force approach primitive clustering mechanism;dynamic environment;brute force approach;redundancy;security policy storage;authoring system;clustering algorithms;authorization;access control;phase change materials authorization decision trees redundancy clustering algorithms force;rough set;pcm;decision trees;security policy;grid computing;access control grid authorization hierarchical clustering mechanism hcm primitive clustering mechanism pcm rough set;rough set based pcm;rough set theory authorisation grid computing pattern clustering;hcm	Many existing grid authorization systems adopt an inefficient structure of storing security policies for the available resources. That leads to huge repetitions in checking security rules. One of the efficient mechanisms that handle these repetitions is the Hierarchical Clustering Mechanism (HCM) [1]. HCM reduces the redundancy in checking security rules compared to the Brute Force Approach as well as the Primitive Clustering Mechanism (PCM). Further enhancement of HCM is done to make it suitable for dynamic environments [2]. However, HCM is not totally free from repetitions. Moreover, HCM is an expensive process in terms of decision tree size and memory consuming. In this paper, a new Rough Set based PCM is proposed which increases the efficiency of the authorization process and further reduces the redundancy.	authorization;brute force;decision tree;hierarchical clustering;rough set	Mustafa Kaiiali;Rajeev Wankar;C. Raghavendra Rao;Arun Agarwal	2010	2010 10th International Conference on Intelligent Systems Design and Applications	10.1109/ISDA.2010.5687232	rough set;computer science;theoretical computer science;machine learning;decision tree;data mining;database;authorization	HPC	-3.9535203536016374	-36.95879490947306	145521
ac236d0e50f0bfd8099c6dc90a5fc9aca9b6426f	a framework for generating attribute extractors for web data sources	extraction information;complex objects;web pages;information extraction;information retrieval;xml language;semistructured data;internet;dato semi estructurado;recherche information;recuperacion informacion;extraccion informacion;langage xml;lenguaje xml;donnee semistructuree	To cope with the irregularities of typical semistructured Web data, extraction tools usually break the extraction task in two phases: an extraction phase, in which atomic attribute values are extracted from Web pages, and an assembling phase, in which these atomic values are grouped to form complex objects. As a consequence, the whole process is highly dependent on the attribute values collected in the first phase. All attribute values of interest should be properly recognized and spurious values should be discarded. Thus, attribute values extraction is an important problem. In this paper, we propose a new framework for generating attribute value extractors. The main appeal of this framework is that it can be adapted for dealing with specific types of data sources and to incorporate distinct types of heuristics for achieving good extraction performance. To demonstrate the feasibility of this proposal, we present an implementation of this framework for data-rich Web pages and show how a number of simple heuristics, some of them presented in the recent literature, can be incorporated into this framework. We also show experimental results and, in most cases, our results are at least as good as results previously presented in the literature.		Davi de Castro Reis;Robson Braga Araújo;Altigran Soares da Silva;Berthier A. Ribeiro-Neto	2002		10.1007/3-540-45735-6_19	the internet;xml;computer science;web page;data mining;database;world wide web;information extraction;information retrieval;algorithm	DB	-4.083035663833277	-33.07118070595966	145767
b53d49ddf233c17694a9c4d21d8843fc1f049397	neural network recognition of otoneurological vertigo diseases with comparison of some other classification methods	aide diagnostic;systeme nerveux pathologie;informatica biomedical;biomedical data processing;decision tree;computer aided diagnosis;estudio comparativo;informatique biomedicale;hombre;arbol decision;algoritmo genetico;classification;discriminant analysis;analyse discriminante;etude comparative;analisis discriminante;nervous system diseases;trouble equilibre;orl pathologie;sistema nervioso patologia;human;comparative study;algorithme genetique;vertige;orl patologia;genetic algorithm;vertigo;reseau neuronal;ent disease;arbre decision;clasificacion;red neuronal;diagnostic aid;homme;neural network;trastorno equilibrio;ayuda diagnostica;equilibrium disorder	We have studied computer-aided diagnosis of otoneurological diseases which are difficult, even for experienced specialists, to determine and separate from each other. Since neural networks require plenty of training data, we restricted our research to the commonest otoneurological diseases in our database and to the very most essential parameters used in their diagnostics. According to our results, neural networks can be efficient in the recognition of these diseases provided that we shall be able to add our available cases concerning those diseases which are rare in our database. We compared the results yielded by neural networks to those given by discriminant analysis, genetic algorithms and decision trees.	artificial neural network	Martti Juhola;Jorma Laurikkala;Kati Viikki;Yrjö Auramo;Erna Kentala;Ilmari Pyykkö	1999		10.1007/3-540-48720-4_23	genetic algorithm;biological classification;computer science;artificial intelligence;machine learning;decision tree;comparative research;linear discriminant analysis;operations research;artificial neural network	Vision	8.384057817269264	-30.805638311928057	145915
32366a0ef3533893361b0051b06577d0b244edee	statistical interval-valued fuzzy systems via linear regression	fuzzy membership function;fuzzy reasoning;probability;type 2 fuzzy logic system;probability type reduce reasoning method;uncertainty;fuzzy control;rule based;statistical interval valued fuzzy reasoning;linear regression;uncertainty handling;fuzzy set theory;fuzzy sets;statistical interval valued fuzzy logic systems;fuzzy logic;fuzzy reasoning statistical interval valued fuzzy logic systems linear regression type 2 fuzzy sets theory rule base fuzzy logic system uncertainty handling type 2 fuzzy logic system probability type reduce reasoning method fuzzy control;type theory;type 2 fuzzy set;fuzzy logic system;regression analysis;computer science;type 2 fuzzy logic;rule base fuzzy logic system;interval valued fuzzy logic;fuzzy systems;knowledge based systems;fuzzy system;fuzzy control interval valued fuzzy logic type 2 fuzzy logic statistical interval valued fuzzy reasoning;type 2 fuzzy sets theory;knowledge based systems fuzzy control fuzzy systems uncertainty handling fuzzy set theory regression analysis probability type theory fuzzy reasoning;fuzzy systems linear regression fuzzy logic fuzzy sets fuzzy reasoning probability uncertainty fuzzy control computer science fuzzy set theory	In recent years, the type-2 fuzzy sets theory has been used to model and minimize the effects of uncertainties in rule-base fuzzy logic system. In order to make the type-2 fuzzy logic system reasonable and reliable, a new simple statistical linear method to decide interval-valued fuzzy membership functions and a new probability type reduce reasoning method for the interval-valued fuzzy logic system are proposed in this paper. An example of statistical interval-valued FLS is performed and results show that the developed method is more accurate to design a fuzzy logic system than type-1 method and computation is efficient.	computation;formal system;free library of springfield township;fuzzy control system;fuzzy logic;fuzzy set;linear model;nonlinear system;test set;type-2 fuzzy sets and systems	Yu Qiu;Yanqing Zhang;Yichuan Zhao	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547273	fuzzy logic;discrete mathematics;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	Robotics	2.6628995796244572	-25.646948462740223	146039
ea50499524307405293d87e396fd34f20ecfcdfd	"""corrigendum to """"a class-oriented feature selection approach for multi-class imbalanced network traffic datasets based on local and global metrics fusion"""" [neurocomputing 168 (2015) 365-381]"""			feature selection;network traffic control;neurocomputing	Zhen Liu;Ruoyu Wang;Ming Tao;Xianfa Cai	2016	Neurocomputing	10.1016/j.neucom.2015.07.047	machine learning;pattern recognition;data mining	Vision	7.901412724477198	-37.25825811919614	146232
a8f1a8bb45d5a76c18facfe1325063bd548c05ef	knowledge discovery in the prediction of bankruptcy	fuzzy classification;noisy data;fuzzy clustering;real world application;extreme value;data mining algorithm;feature selection;knowledge discovery in database;missing data;missing values	Knowledge discovery in databases (KDD) is the process of discovering interesting knowledge from large amounts of data. However, real-world datasets have problems such as incompleteness, redundancy, inconsistency, noise, etc. All these problems affect the performance of data mining algorithms. Thus, preprocessing techniques are essential in allowing knowledge to be extracted from data. This work presents a real world application of knowledge discovery in databases, with the objective of prediction of bankruptcy. For this task fuzzy classification models based on fuzzy clustering are used, which are developed solely from numerical data. This data set has missing values, extreme values and also presents a much smaller bankruptcy class than the not bankruptcy class, which makes it a challenging problem in the scope of KDD. Keywords— Knowledge discovery in databases, feature selection, missing data, noisy data, prediction of bankruptcy, fuzzy classifica-	algorithm;cluster analysis;data mining;database;feature selection;fuzzy classification;fuzzy clustering;fuzzy concept;international standard book number;level of measurement;missing data;numerical analysis;preprocessor;redundancy (engineering);signal-to-noise ratio	Rui Jorge Almeida;Susana M. Vieira;Viorel Milea;Uzay Kaymak;João Miguel da Costa Sousa	2009			computer science;data science;pattern recognition;data mining	ML	-0.24388058979316402	-32.863087681448135	146244
cb694fc1a4c8ef1e7c286e34ce34d36f34830b6a	a fuzzy optimization method for support vector classification	quadratic programming;fuzzy classification;optimisation;fuzzy support vector classification;quadratic program;support vector machines constraint handling fuzzy set theory optimisation pattern classification;support vector machines;fuzzy number;training;triangle fuzzy number;possibility measure;testing;fuzzy optimization method;training support vector machine classification classification algorithms programming testing quadratic programming;fuzzy set theory;constrained programming;triangle fuzzy number machine learning fuzzy support vector classification possibility measure;quadratic programming fuzzy optimization method support vector classification fuzzy classification function triangle fuzzy number constrained programming;machine learning;support vector classification;classification algorithms;pattern classification;constraint handling;support vector machine classification;programming;fuzzy optimization;fuzzy classification function	This paper is concerned with the fuzzy support vector classification, in which both of the type of the output training point and the value of the final fuzzy classification function are triangle fuzzy number. First, the fuzzy classification problem is formulated as a fuzzy chance constrained programming. Then, we transform this programming into its equivalence quadratic programming. Final, a fuzzy support vector classification algorithm is proposed to deal with the problem. An example is presented to illustrate rationality of the algorithm.	algorithm;fuzzy classification;fuzzy number;mathematical optimization;quadratic programming;rationality;support vector machine;turing completeness	Zhimin Yang;Xiao Yang	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569677	fuzzy logic;support vector machine;programming;mathematical optimization;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;mathematics;software testing;fuzzy set;fuzzy associative matrix;quadratic programming;fuzzy set operations;fuzzy control system	ML	3.359875125726712	-29.123010283950457	146368
3c6833889ceb884a6d24d65a4e70980232c54588	a hybrid higher order neural classifier for handling classification problems	model selection;structural model;high order unit;higher order neural network honn;higher order;classification problems;feature subset selection;breast cancer;neural network	This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final form. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.	portable document format	Mehdi Fallahnezhad;Mohammad Hassan Moradi;Salman Zaferanlouei	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.06.077	higher-order logic;computer science;artificial intelligence;breast cancer;machine learning;pattern recognition;data mining;artificial neural network;model selection;statistics	ML	2.333610924228382	-34.316889456274296	146626
d4cb864ccacd7feb7ccb5db437ddb3847e071184	evolving output-context fuzzy system for effective rule base	dynamic constraint;evolving system;information granule;output context fuzzy system	Prominent distinction points are realized on the output domain.Adaptation of the fuzzy levels is based on dynamic uncertainty function.The concurrent execution of the evolving and self-organizing process is considered.Interpretability and reasonable accuracy are observed with respect to the existing methods. This study develops an evolving output-context fuzzy system (EOCFS) that initiates the evolving process with a single fuzzy rule in which consequent and antecedent parts cover the whole output and input domains respectively. The EOCFS further evolves the output domain and adds more fuzzy rules to achieve an effective rule base. The proposed model is a self-organizing method that can automatically identify prominent distinct data in the output domain for a new fuzzy rule. Thus, the EOCFS constantly evolves by reducing the model error. In addition, the evolving process is realized on the output domain while the self-adaptive process is achieved on the output domain and its associated input domain. The evolving termination index and uncertainty controller of the self-adaptive process are dynamically attained from past and current knowledge. Therefore, effective rule base is the balanced fuzzy model of the approximated system. To illustrate the effectiveness of the proposed algorithm, synthetic and real-world data are considered with low to high dimension inputs. Results show that the proposed EOCFS achieves better performance than the existing models with regard to accuracy and number of rules.	fuzzy control system;rule-based system	Md. Manjur Ahmed;Nor Ashidi Mat Isa	2015	Expert Syst. Appl.	10.1016/j.eswa.2014.09.056	artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;control theory;mathematics;fuzzy set operations	DB	4.528698785801207	-29.223065979479745	146673
fbc3d835808aafac07888a1e5492b16f42d0c869	intelligent tool condition monitoring system for turning operations	modelizacion;reseau capteur;systeme intelligent;coeficiente correlacion;surveillance;sistema inteligente;logique floue;logica difusa;non linear model;intelligence artificielle;modele non lineaire;data fusion;fuzzy logic;modelisation;herramienta corte;outil coupe;monitoring system;modelo no lineal;vigilancia;red sensores;condition monitoring;monitoring;fusion donnee;tool wear;intelligent system;sensor array;artificial intelligence;b spline;inteligencia artificial;monitorage;cutting tool;reseau neuronal;fusion datos;correlation coefficient;monitoreo;modeling;coefficient correlation;red neuronal;b splin;fuzzy model;neural network	In order to predict tool wear accurately and reliably under different cutting conditions, a new monitoring methodology for turning operations is proposed. Firstly, the correlation coefficients approaches were used to indicate the dependencies between the different sensed information features and tool wear amount, and the most appropriate features were selected. Secondly, B-spline neural networks were introduced to model the non-linear relationship between extracted features and tool wear amount, and multi-sensor information were fused by an integrated neural network. Lastly, the final result of tool wear was given through fuzzy modeling. Experimental results have proved that the monitoring system based on the methodology is reliable and practical.		Hongli Gao;Mingheng Xu	2005		10.1007/11427469_140	fuzzy logic;b-spline;simulation;systems modeling;computer science;artificial intelligence;machine learning;sensor fusion;sensor array;artificial neural network	Logic	8.922750464422236	-27.76388202451618	146829
38ce373eeb4d351ce28d5edc3d454a3dc9006f0e	wise mining method through ant colony optimization	optimisation;general semantic miner;ant colony optimization;evolutionary algorithm wise mining method ant colony optimization data mining pheromone miner ant colony based data miner knowledge extraction general semantic miner;ant colony based data miner;ant colony;training;knowledge extraction;data mining;optimisation data mining;pheromone;classification algorithms;knowledge discovery ant colony optimization algorithm pheromone data mining;clustering algorithms;wise mining method;robustness;evolutionary algorithm;insects;ant colony optimization data mining clustering algorithms insects databases robustness cybernetics usa councils production systems electronic mail;algorithm design and analysis;ant colony optimization algorithm;pheromone miner;knowledge discovery	This paper proposes an algorithm for data mining named Pheromone-Miner (ant-colony-based data miner). The algorithm is inspired by both researches on the behavior of real ant colonies and data mining concepts as well as principles. The goal of Pheromone-Miner is to extract more exact knowledge from a database. Pheromone-based mining breaks through limitations of other mining approaches. We compare the performance of pheromone-miner with a general semantic miner. The accident causes discovered by ant-miner are considerably more accurate than those discovered by a general semantic miner. In a word, this evolutionary algorithm is suitable for improving the accuracy of data miners.	ant colony optimization algorithms;data mining;database;evolutionary algorithm;mathematical optimization	Jianxiong Yang;Junzo Watada	2009	2009 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2009.5346807	algorithm design;ant colony optimization algorithms;computer science;artificial intelligence;data science;ant colony;machine learning;data mining;knowledge extraction;cluster analysis;robustness	DB	-3.591889351948592	-36.97966362908569	147003
24a225c756352ac5ab3bde169827c91c054db4b3	multirelational classification: a multiple view approach	extraction information;modelizacion;base relacional dato;empirical study;multi view learning;methode empirique;valorisation;multirelational data mining;analisis datos;information extraction;metodo empirico;empirical method;relational database;data mining;classification;multiple views;multiple view;modelisation;ensemble;data analysis;learning methods;flattening;fouille donnee;aplanamiento;upgrading;decouverte connaissance;base de donnees relationnelle;vue multiple;descubrimiento conocimiento;analyse donnee;valorizacion;modeling;busca dato;clasificacion;extraccion informacion;classification relational database;vista multiple;aplatissement;knowledge discovery	Multirelational classification aims at discovering useful patterns across multiple inter-connected tables (relations) in a relational database. Many traditional learning techniques, however, assume a single table or a flat file as input (the so-called propositional algorithms). Existing multirelational classification approaches either “upgrade” mature propositional learning methods to deal with relational presentation or extensively “flatten” multiple tables into a single flat file, which is then solved by propositional algorithms. This article reports a multiple view strategy—where neither “upgrading” nor “flattening” is required—for mining in relational databases. Our approach learns from multiple views (feature set) of a relational databases, and then integrates the information acquired by individual view learners to construct a final model. Our empirical studies show that the method compares well in comparison with the classifiers induced by the majority of multirelational mining systems, in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for multirelational research and applications. In addition, the method has practical significance: it is appropriate for directly mining many real-world databases.	algorithm;data mining;diagram;entity–relationship model;feature selection;feature vector;flat file database;google bigtable;image scaling;informatics;machine learning;preprocessor;problem domain;relational database;scalability;time complexity;view (sql)	Hongyu Guo;Herna L. Viktor	2008	Knowledge and Information Systems	10.1007/s10115-008-0127-5	computer science;artificial intelligence;machine learning;data mining;database;empirical research;information extraction	ML	8.112305441947239	-33.08132147121673	147198
a89950317e6b55fa3f7812489f2ee6dd118f4573	updating generalized association rules with evolving fuzzy taxonomies	generalized association rule;fuzzy classification;itemsets;fdiff_et2 generalized association rule mining fuzzy taxonomic structures fuzzy classification fdiffet;association mining;printers;generalized association rule mining;association rules;data mining;fuzzy set theory;scaling up;frequent itemset;facsimile;taxonomy;fuzzy taxonomic structures;pattern classification;fdiffet;pattern classification data mining fuzzy set theory;fdiff_et2;empirical evaluation;taxonomy itemsets association rules printers facsimile	Mining generalized association rules with fuzzy taxonomic structures has been recognized as a important extension of generalized associations mining problem. To date most work on this problem, however, required the taxonomies to be static, ignoring the fact that the taxonomies of items cannot necessarily be kept unchanged. For instance, some items may be reclassified from one hierarchy tree to another for more suitable classification, abandoned from the taxonomies if they will no longer be produced, or added into the taxonomies as new items. Additionally, the membership degrees expressing the fuzzy classification may also need to be adjusted. Under these circumstances, effectively updating the discovered generalized association rules is a crucial task. In this paper, we examine this problem and propose two novel algorithms, called FDiffET and FDiff_ET2, to update the discovered frequent generalized itemsets.	algorithm;association rule learning;data mining;fuzzy classification;fuzzy markup language;prospective search;scalability;taxonomy (general)	Wen-Yang Lin;Ming-Cheng Tseng;Ja-Hwung Su	2010	International Conference on Fuzzy Systems	10.1109/FUZZY.2010.5584845	association rule learning;fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics;fuzzy set;taxonomy	DB	-3.42252290079336	-28.718210043115434	147280
18de2897615ffc838ff4c0f9274b7d5bc0e105b5	cl-gbi: a novel approach for extracting typical patterns from graph-structured data	induction machine;subgrafo;analisis datos;memorizacion por bloque;search strategy;intelligence artificielle;data mining;data analysis;machine learning;fouille donnee;sous graphe;decouverte connaissance;estructura datos;strategie recherche;artificial intelligence;descubrimiento conocimiento;analyse donnee;structure donnee;maquina induccion;inteligencia artificial;subgraph;machine induction;memorisation par bloc;data structure;busca dato;chunking;structured data;estrategia investigacion;knowledge discovery	Graph-Based Induction (GBI) is a machine learning technique developed for the purpose of extracting typical patterns from graph-structured data by stepwise pair expansion (pair-wise chunking). GBI is very efficient because of its greedy search strategy, however, it suffers from the problem of overlapping subgraphs. As a result, some of typical patterns cannot be discovered by GBI though a beam search has been incorporated in an improved version of GBI called Beam-wise GBI (B-GBI). In this paper, improvement is made on the search capability by using a new search strategy, where frequent pairs are never chunked but used as pseudo nodes in the subsequent steps, thus allowing extraction of overlapping subgraphs. This new algorithm, called Cl-GBI (Chunkingless GBI), was tested against two datasets, the promoter dataset from UCI repository and the hepatitis dataset provided by Chiba University, and shown successful in extracting more typical patterns than B-GBI.	graph (abstract data type)	Phu Chien Nguyen;Kouzou Ohara;Hiroshi Motoda;Takashi Washio	2005		10.1007/11430919_74	data structure;data model;computer science;artificial intelligence;machine learning;data mining;knowledge extraction;data analysis;chunking	NLP	-2.7732754537434436	-33.58497906712621	147286
18b8c35b33b5e193a8ce6b36b4092c16fc4da9cf	a methodology for analysis of concept lattice reduction		Independent methodology for analysis of concept lattice reduction.We use sets of proper implications holding in the original and reduced structure.Highlight the kinds of changes propitiated by the different classes of techniques.Four reduction techniques were used to illustrate the proposed methodology. Formal concept analysis (FCA) is a mathematical theory of data analysis with applications in many areas. The problem of obtaining a concept lattice of an appropriate size was identified in several applications as one of the most important problems of FCA. In order to deal with this problem several techniques with different characteristics were proposed for concept lattice reduction. However, there are currently no adequate methods to assess what types of knowledge transformations can result from a reduction. A methodology for analysis of concept lattice reduction is presented here. It is based on the use of sets of proper implications holding in the original and reduced formal contexts or concept lattices. Working with both sets of implications, the methodology is able to show what is preserved, eliminated, inserted or transformed by a reduction technique. Three classes of reduction techniques are analyzed from the standpoint of the methodology in order to highlight techniques of each class have in common with respect to the transformations performed. Such analysis is followed by specific examples in each class.	formal concept analysis;lattice reduction	Sérgio M. Dias;Newton José Vieira	2017	Inf. Sci.	10.1016/j.ins.2017.02.037	discrete mathematics;lattice miner;mathematics;descriptive knowledge;lattice (order);formal concept analysis;mathematical theory;lattice reduction	Logic	-3.4157986607623854	-26.154933043335433	147376
a7b03285eb0d32a5ddc514df6d223ce7c8927f6a	preventing interval-based inference by random data perturbation	important type;accurate enough interval;new rdp method;rdp method;new type;sensitive information;random data perturbation;gaussian distribution;inference interval;interval-based inference	Random data perturbation (RDP) method is often used in statistical databases to prevent inference of sensitive information about individuals from legitimate sum queries. In this paper, we study the RDP method for preventing an important type of inference: interval-based inference. In terms of interval-based inference, the sensitive information about individuals is said to be compromised if an accurate enough interval, called inference interval, is obtained into which the value of the sensitive information must fall. We show that the RDP methods proposed in the literature are not effective for preventing such interval-based inference. Based on a new type of random distribution, called Ɛ-Gaussian distribution, we propose a new RDP method to guarantee no interval-based inference.		Yingjiu Li;Lingyu Wang;Sushil Jajodia	2002		10.1007/3-540-36467-6_12	statistical inference;fiducial inference;interval estimation;frequentist inference;pattern recognition;data mining;algorithmic inference	Security	0.4328419961761316	-32.01734365647884	147479
67d9457faea2c0623a559d7f24f823df032e8eb9	dynamics and topographic organization of recursive self-organizing maps	tratamiento datos;dimensionalidad;model design;memoire;calcul neuronal;neural computation;procesamiento informacion;champ recepteur;modelo markov;dynamique;carte topographique;systeme non autonome;structure arborescente;weighting;campo receptor;dimensionality;data processing;conception;adaptive dynamics;traitement donnee;metodo secuencial;sequential method;ponderacion;topographic map;dinamica;dynamical system;systeme dynamique;markov model;sistema no autonomo;memoria;dynamics;estructura arborescente;dimensionnalite;tree structure;application auto organisante;information processing;non autonomous dynamical system;diseno;methode sequentielle;autoorganizacion;self organization;design;self organized map;receptive field;som;ponderation;modele markov;sistema dinamico;reseau neuronal;traitement information;data structure;nonautonomous dynamical system;red neuronal;computacion neuronal;autoorganisation;memory;plano topografico;neural network;non autonomous system	Recently there has been an outburst of interest in extending topographic maps of vectorial data to more general data structures, such as sequences or trees. However, there is no general consensus as to how best to process sequences using topographic maps, and this topic remains an active focus of neurocomputational research. The representational capabilities and internal representations of the models are not well understood. Here, we rigorously analyze a generalization of the self-organizing map (SOM) for processing sequential data, recursive SOM(RecSOM) (Voegtlin, 2002), as a nonautonomous dynamical system consisting of a set of fixed input maps. We argue that contractive fixed-input maps are likely to produce Markovian organizations of receptive fields on the RecSOM map. We derive bounds on parameter (weighting the importance of importing past information when processing sequences) under which contractiveness of the fixed-input maps is guaranteed. Some generalizations of SOM contain a dynamic module responsible for processing temporal contexts as an integral part of the model. We show that Markovian topographic maps of sequential data can be produced using a simple fixed (nonadaptable) dynamic module externally feeding a standard topographic model designed to process static vectorial data of fixed dimensionality (e.g., SOM). However, by allowing trainable feedback connections, one can obtain Markovian maps with superior memory depth and topography preservation. We elaborate on the importance of non-Markovian organizations in topographic maps of sequential data.	biologic preservation;data structure;dynamical system;experiment;floor and ceiling functions;fuzzy control system;generalization (psychology);organizing (structure);population parameter;recursion (computer science);representation (action);self-organization;self-organizing map;topography;trees (plant)	Peter Tiño;Igor Farkas;Jort van Mourik	2006	Neural Computation	10.1162/neco.2006.18.10.2529	design;topographic map;dynamics;neuroscience;data structure;data processing;information processing;computer science;artificial intelligence;dynamical system;machine learning;quasi-open map;weighting;mathematics;tree structure;markov model;memory;receptive field;artificial neural network;algorithm	ML	9.697078522384023	-29.936851265056905	147490
a7a40c40f445a3fd4d41644e9961d5dd29add6d0	prediction of financial information manipulation by using support vector machine and probabilistic neural network	sensitivity and specificity;logistic regression;discriminant analysis;financial ratios;support system;financial information manipulation;support vector machine;classification accuracy;probabilistic neural network	Different methods have been used to predict financial information manipulation that can be defined as the distortion of the information in the financial statements. The purpose of this paper is to predict financial information manipulation by using support vector machine (SVM) and probabilistic neural network (PNN). A number of financial ratios are used as explanatory variables. Test performance of classification accuracy, sensitivity and specificity statistics for PNN and SVM are compared with the results of discriminant analysis, logistics regression (logit), and probit classifiers, which have been used in other studies. We have found that the performance of SVM and PNN are higher than that of the other classifiers analyzed before. Thus, both classifiers can be used as automated decision support system for the detection of financial information manipulation. 2008 Elsevier Ltd. All rights reserved.	artificial neural network;consistency model;decision support system;distortion;linear discriminant analysis;logistics;naive bayes classifier;probabilistic neural network;sensitivity and specificity;support vector machine	Hulisi Ögüt;Ramazan Aktas;Ali Alp;M. Mete Doganay	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.06.055	support vector machine;probabilistic neural network;computer science;machine learning;pattern recognition;data mining;logistic regression;linear discriminant analysis;financial ratio	AI	9.635284624381207	-26.005125598785764	147742
f33adb6eec4adccbf6db21e6ae9e97ede1bccf73	gawdn-nfis: neural-fuzzy inference system with a genetic algorithm based on weighted data normalization and its application in medicine	fuzzy neural nets;fuzzy reasoning;medical decision support system;patient treatment decision support systems diseases fuzzy neural nets fuzzy reasoning genetic algorithms learning artificial intelligence medical computing;descent algorithm;genetic algorithms input variables fuzzy neural networks fuzzy systems neural networks knowledge engineering decision support systems optimization methods inference algorithms prototypes;haemodialysis patient neural fuzzy inference system genetic algorithm weighted data normalization medical decision support system descent algorithm nfis learning feature selection;nfis learning;medical computing;neural fuzzy inference system;haemodialysis patient;decision support systems;fuzzy inference;weighted data normalization;fuzzy inference system;diseases;patient treatment;genetic algorithm;genetic algorithms;feature selection;learning artificial intelligence;haemodialysis;medical decision support;steepest descent;neural network	This paper introduces an approach of neural-fuzzy inference system (NFIS) with a genetic algorithm (GA) based on weighted data normalization (WDN) and its application in a medical decision support system. The WDN method optimizes the data normalization ranges for the input variables of the neural-fuzzy inference system and a genetic algorithm is used as part of the WDN method. A steepest descent algorithm (BP) is used for NFIS learning on the normalized data set. The derived weights have the meaning of feature importance and can be used for feature selection to decrease the number of input variables. The GAWDN-NFIS is illustrated on the case study: a real medical decision support problem of estimating the survival of haemodialysis patients. This approach can also be applied to other distance-based, prototype learning neural network or fuzzy inference models.	artificial neural network;decision support system;feature selection;fuzzy logic;generic programming;genetic algorithm;gradient descent;inference engine;personalization;prototype;software release life cycle	Qun Song;Tianmin Ma	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.341	genetic algorithm;computer science;machine learning;pattern recognition;data mining;feature selection	ML	3.9506040578596644	-28.828422986964227	147765
3723fb8548359d5e1c8938fb69d0012dfe70b1cf	voltime: unsupervised anomaly detection on users' online activity volume			anomaly detection	Daniel Y. T. Chino;Alceu Ferraz Costa;Agma J. M. Traina;Christos Faloutsos	2017		10.1137/1.9781611974973.13	machine learning;computer science;artificial intelligence;pattern recognition;anomaly detection	HCI	6.45931921663802	-36.16278724773089	147882
1ead50d865f6095d3f8e9ca7589eaf85a6590b92	improving fuzzy pattern matching techniques to deal with non discrimination ability features	soft computing;real time;fuzzy set theory pattern matching;fuzzy integral;fuzzy set theory;wisconsin breast cancer data set fuzzy pattern matching techniques nondiscrimination ability features fuzzy methods supervised fuzzy pattern recognition feature selection simulated data set xor problem real data set;fuzzy pattern matching;pattern matching;pattern recognition;feature selection;pattern matching pattern recognition fuzzy set theory fuzzy systems fuzzy sets prototypes computational modeling breast cancer bayesian methods background noise;breast cancer;fuzzy system	Fuzzy pattern matching technique represents a group of fuzzy methods for supervised fuzzy pattern recognition. It has a number of advantages over other pattern recognition methods, including simpler methods of feature selection or ability to learn in real time environments, but its main drawback is it is not able to model the correlation between features, since fuzzy pattern matching assumes non interactivity between them. This paper presents an attempt to extend this technique to deal with this kind of features. To show the accuracy of the proposed solution, we present the results obtained in a simulated data set (an extension of the xor problem) and a real data set (the Wisconsin breast cancer data set).	exclusive or;feature selection;interactivity;pattern matching;pattern recognition;perceptrons: an introduction to computational geometry	José Manuel Cadenas;M. Carmen Garrido;J. J. Hernandez	2004	2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)	10.1109/ICSMC.2004.1401104	membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;breast cancer;machine learning;pattern matching;pattern recognition;data mining;mathematics;soft computing;fuzzy set;fuzzy associative matrix;feature selection;fuzzy set operations	Vision	3.7118332761909887	-29.30698375194329	147921
7b0149d5dea9d91f52d39fce00d77026b790fcb8	computational intelligence for data mining	information systems;expert systems;hybrid intelligent systems;application software;computational intelligence;logic;data mining;computer networks;artificial neural networks;computational intelligence data mining computer networks hybrid intelligent systems expert systems application software information systems artificial neural networks genetic algorithms logic;genetic algorithms		computation;computational intelligence;data mining	Mark J. Embrechts	2001		10.1109/ICSMC.2001.973492	application software;marketing and artificial intelligence;intelligent decision support system;computer science;artificial intelligence;neuro-fuzzy;machine learning;computational intelligence;data mining;artificial intelligence system;logic;artificial neural network;intelligent control	AI	5.47621404548522	-28.332804080386612	147947
0d2d169612451a97c4e9a862e982e90cef1300cb	saturatedness and a hierarchy of approximate identities	saturatedness;fuzzy set;higgs category;leibniz s principle;rough set;approximate identity	In this paper, 'identity' is eventually considered to be determined by a specified collection of 'properties' represented extensionally by subsets. Generally, the collection may contain fuzzy subsets representing vague properties, thus rendering vagueness to the concept of identity. Saturatedness condition, which in essence is a portion of Leibniz's principle of identity, has been dealt with in some detail. A hierarchy of identities is obtained. This hierarchy also indicates some relationship between the concepts 'identity' and 'property'. Some applications of the results to the modelling of indistinguishability in information systems have been indicated.		Mihir K. Chakraborty;Mohua Banerjee	1994	International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems	10.1142/S0218488594000341	discrete mathematics;rough set;computer science;identity function;machine learning;pure mathematics;mathematics;fuzzy set	Arch	-0.7192212580886488	-24.048811262233986	148404
809008449c84b85307ed3320d5e92a5beca9ddce	adaptive neuro-fuzzy inference systems based approach to nonlinear noise cancellation for images	engineering;gaussian noise;image degradation;filtering;filtrage;anfis;fuzzy set;fuzzy reasoning;procesamiento informacion;ondelette;optimal method;filtrado;65kxx;ajustement;conjunto difuso;optimization method;ensemble flou;65t60;image restoration;raisonnement;systeme adaptatif;metodo optimizacion;wavelet packet;ingenierie;fitting;49xx;estimation erreur;42c40;triangle;error estimation;mean square error;noise cancellation;information processing;estimacion error;systeme non lineaire;adaptive system;razonamiento;membership function;methode optimisation;pattern recognition;spatial filtering;sistema adaptativo;ingenieria;sistema difuso;reconnaissance forme;wiener filter;systeme flou;reasoning;reconocimiento patron;ajuste;frequency domain;traitement information;sistema no lineal;wavelets;adaptive filter;non linear system;adaptive neuro fuzzy inference system;fuzzy system	The adaptive neuro-fuzzy inference system (ANFIS) is an effective tool that can be applied to induct rules from observations, e.g. pattern recognition. In this paper, we extend the nonlinear noise cancellation method using ANFIS from 1-D signals to 2-D counterpart images. First, the image restoration contaminated with Gaussian noise is investigated in nonlinear passage dynamics of order 2. We inspect eight types of membership functions (MF): bell MF, triangle MF, Gaussian MF, two sided Gaussian MF, pi-shaped MF, product of two sigmoidal MFs, difference of two sigmoidal MFs, and trapezoidal MF. In addition, the other parameters, such as the training epochs, the number of MFs for each input, the optimization method, the type of output MFs, and the over-fitting problem, are investigated. For comparison with the noise cancellation using ANFIS, we simulate 22 conventional filtering techniques: spatial filters, optimal Wiener filter, frequency domain filters, wavelet, wavelet packet, 2-D adaptive filters, etc. The quality in terms of mean square error (MSE) of image restoration using the proposed noise cancellation using ANFIS (Pi-shaped MF) is at least 75 times better for Gaussian noise than that derived using any of these conventional filtering techniques.	fuzzy logic;neuro-fuzzy;nonlinear system	Hao Qin;Simon X. Yang	2007	Fuzzy Sets and Systems	10.1016/j.fss.2006.10.028	information processing;adaptive neuro fuzzy inference system;computer science;artificial intelligence;adaptive system;machine learning;control theory;mathematics;fuzzy control system	EDA	8.613721337893807	-28.758136934127336	148519
0a2d27c9c44d64e3bc7cc0b1369382049d085fc2	a complete pattern recognition approach under atanassov's intuitionistic fuzzy sets	turbine generators;pattern recognition;atanassov s intuitionistic fuzzy sets aifss;similarity measure;fault diagnosis	This research is aimed at developing a method for solving pattern recognition problems under the Atanassov’s intuitionistic fuzzy sets based on similarity measures. First we proposed two similarity measures and then developed a method based on our similarity measures. We also proved that our method is able to solve the pattern recognition problems. Finally, a fault diagnosis example of the turbine vibration has been examined by our method. The example demonstrates that the proposed method cannot only diagnose the main faults of the turbine generator but also it can detect useful information for future trends and multi-fault analysis. In addition, for the convenience of computing and ranking processes, a computer interface decision support system is also developed to help decision maker make diagnoses more efficiently.	fuzzy logic;fuzzy set;pattern recognition	Chun-Hsiao Chu;Kuo-Chen Hung;Peterson Julian	2014	Knowl.-Based Syst.	10.1016/j.knosys.2014.04.014	computer science;machine learning;data mining	Vision	1.9361328797673272	-26.015159604968197	148568
af5b047c781470d1309e84cf3f7d20036ad8abba	nonlinear context adaptation in the calibration of fuzzy sets	fuzzy set;information granularity;neurocomputing;frame of cognition;universe of discourse;nonlinear transformation;context adaptation;knowledge representation;neural network	"""In this note we elaborate on the concept and use of context adaptation. The underlying idea hinges upon a nonlinear transformation of an actual reference unit universe of discourse into a subset of reals, say [a, b], that is implied by actually available data (current context). Assuming a collection of fuzzy sets ,~ -{A1, A2 . . . . . A,,} defined over [0, 1], the context adaptation gives rise to a new frame of cognition ~_/' ' ' ' {AL, A 2 . . . . ,An} expressed over [a,b]. Owing to the inherent nonlinearity of the developed mapping, different elements (fuzzy sets) of ,4 can be """"stretched"""" or """"expanded"""" according to the given experimental data. Proposed is a neural network as a relevant optimization tool. @1997 Elsevier Science B.V."""	artificial neural network;cognition;domain of discourse;fuzzy set;mathematical optimization;nonlinear system;sentient computing	Witold Pedrycz;Ricardo R. Gudwin;Fernando A. C. Gomide	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00057-7	knowledge representation and reasoning;type-2 fuzzy sets and systems;fuzzy mathematics;computer science;artificial intelligence;machine learning;domain of discourse;mathematics;fuzzy set;artificial neural network;algorithm	NLP	0.8018635933358547	-25.910685473883355	148606
4ad767001f868ad28d858c994ece2abfbf5d771e	the identification of low-paying workplaces: an analysis using the variable precision rough sets model	wage;enquete socioeconomique;hd industries land use labor;encuesta socioeconomica;decision tree;analisis datos;socioeconomic inquiry;qa mathematics;arbol decision;standard error;data analysis;salaire;minimum wage;prediction accuracy;analyse donnee;salario;h social sciences general;rough set;ensemble approximatif;arbre decision;ha statistics	The identification of workplaces (establishments) most likely to pay low wages is an essential component of effectively monitoring a minimum wage. The main method utilised in this paper is the Variable Precision Rough Sets (VPRS) model, which constructs a set of decision 'if ... then ...' rules. These rules are easily readable by non-specialists and predict the proportion of low paid employees in an establishment. Through a 'leave n out' approach a standard error on the predictive accuracy of the sets of rules is calculated, also the importance of the descriptive characteristics is exposited based on their use. To gauge the effectiveness of the VPRS analysis, comparisons are made to a series of decision tree analyses.	rough set	Malcolm J. Beynon	2002		10.1007/3-540-45813-1_70	econometrics;rough set;computer science;artificial intelligence;machine learning;decision tree;data mining;mathematics;data analysis;standard error;computer security;algorithm;statistics	SE	8.128889480452257	-33.836528187060026	148754
9e717c0881fc2b5b1335075bc5e5b3d989d5fe35	a neuro-fuzzy decision support system for selection of small scale business	decision support;rule extraction;interface design;fuzzy logic;support system;neuro fuzzy;neuro fuzzy system;intelligent system;rural development;artificial neural network;neural network	Artificial Neural Network (ANN) and Fuzzy Logic (FL) are two important and useful technologies having their strengths and weaknesses. The combination of fuzzy logic and neural networks constitutes a powerful means for intelligent system development and offers dual advantages of the technologies. This article describes four approaches of neuro-fuzzy systems with their broad design and also presents general structure of a business advisory system using hybrid neuro-fuzzy approach. The system utilizes ANN that considers basic parameters and data from the environment for selection of a small-scale business in the given area and generates rules accordingly. Finally, the article presents sample rules extracted from the neuro-fuzzy system, screens for the interface design and parameters for implementation.		Rajendra Akerkar;Priti Srinivas Sajja	2010		10.1007/978-3-642-14058-7_31	fuzzy logic;fuzzy electronics;adaptive neuro fuzzy inference system;computer science;artificial intelligence;interface design;neuro-fuzzy;machine learning;data mining;artificial neural network	HCI	4.89587913477722	-24.872949844907836	148937
1fa8f5e7d1e5375ce867d84f420404b9fdc58bd5	an improved kernel clustering algorithm used in computer network intrusion detection		In the computer network intrusion detection system, data objects, mapped from original space, are analyzed based on kernel clustering algorithm. During the process of kernel clustering, some representative points are introduced to represent a cluster. In just one iteration, the distance between a data object and representative points of a cluster is computed to partition the data objects. The clustering results contain normal data and abnormal data, which achieve the goal of intrusion detection. At the same time, KDD CUP 1999 dataset is used to make simulations. The results show that the proposed algorithm has higher detecting probability under the condition of low constant false alarm rate compared with K-Means clustering algorithm and SVM.	algorithm;cups;cluster analysis;constant false alarm rate;data mining;data point;intrusion detection system;iteration;k-means clustering;kernel (operating system);sensor;simulation	Di He;Xin Chen;Danping Zou;Ling Pei;Ling-ge Jiang	2018	2018 IEEE International Symposium on Circuits and Systems (ISCAS)	10.1109/ISCAS.2018.8350994	kernel (linear algebra);support vector machine;cluster analysis;feature extraction;constant false alarm rate;computer network;intrusion detection system;computer science	ML	5.279210106225789	-37.82944646838693	149072
124a243669b827b13bbcca2e0d0fe0b204e05569	human-centric analysis and interpretation of time series: a perspective of granular computing	linguistic description;higher order granular models;granular time series;principle of justifiable granularity;information granularity	In spite of the truly remarkable diversity of models of time series, there is still an evident need to develop constructs whose accuracy and interpretability are carefully identified and reconciled subsequently leading to highly interpretable (human-centric) constructs. While a great deal of research has been devoted to the design of nonlinear numeric models of time series (with an evident objective to achieve high accuracy of prediction), an issue of interpretability (transparency) of models of time series becomes an evident and ongoing challenge. The user-friendliness of models of time series comes with an ability of humans to perceive and process abstract constructs rather than dealing with plain numeric entities. In perception of time series, information granules (which are regarded as realizations of interpretable entities) play a pivotal role. This gives rise to a concept of granular models of time series or granular time series, in brief. This study revisits generic concepts of information granules and elaborates on a fundamental way of forming information granules (both sets—intervals as well as fuzzy sets) through applying a principle of justifiable granularity encountered in granular computing. Information granules are discussed with regard to the granulation of time series in a certain predefined representation space (viz. a feature space) and granulation carried out in time. The granular representation and description of time series is then presented. We elaborate on the fundamental hierarchically organized layers of processing supporting the development and interpretation of granular time series, namely (a) formation of granular descriptors used in their visualization, (b) construction of linguistic descriptors used afterwards in the generation of (c) linguistic description of time series. The layer of the linguistic prediction models of time series exploiting the linguistic descriptors is outlined as well. A number of examples are offered throughout the entire paper with intent to illustrate the main functionalities of the essential layers of the granular models of time series.	granular computing;time series	Witold Pedrycz;Wei Lu;Xiaodong Liu;Wei David Wang;Lizhong Wang	2014	Soft Comput.	10.1007/s00500-013-1213-5	computer science;artificial intelligence;machine learning;data mining;mathematics;algorithm	Logic	6.570767226134003	-24.524672560666122	149259
3f4096f2c58925c292850bfcc7e000c903d6a296	intelligent data and knowledge analysis and verification: towards a taxonomy of some specific problems	specific problems;intelligent data;knowledge analysis	This paper addresses the problem of intelligent analysis of data and knowledge. A common model for data and knowledge representation is proposed. The model takes the form of extended relational database table and it is provided logical interpretation. The main interest is in analysis of certain properties of such bodies of data and knowledge. In order to assure satisfactory performance of a system based on such data and knowledge several theoretical properties should be satisfied. The principal issue is then analysis and verification of these properties which are believed to constitute decisive factors for efficiency, reliability, quality and safety of operation. Several theoretical problems concerning data and knowledge are identified, defined and discussed. These problems cover adequate data and knowledge representation, completeness, consistency, correctness, equivalency, generalization and manipulation, similarity and inductive generalization. Influence of selected issues on system reliability, safety and quality is analyzed and a uniform logical framework for discussion is put forward.	numerical taxonomy	Antoni Ligeza	1999			computer science;knowledge management;data mining;database	NLP	-3.3683011016710385	-24.88877340528905	149362
c7d3d5cb9cffc87313b6789c58c83af92c802c4f	a survey on feature drift adaptation: definition, benchmark, challenges and future directions		Data stream mining is a fast growing research topic due to the ubiquity of data in several real-world problems. Given their ephemeral nature, data stream sources are expected to undergo changes in data distribution, a phenomenon called concept drift. This paper focuses on one specific type of drift that has not yet been thoroughly studied, namely feature drift. Feature drift occurs whenever a subset of features becomes, or ceases to be, relevant to the learning task, thus, learners must detect and adapt to these changes accordingly. We survey existing work on feature drift adaptation in both explicit and implicit approaches. Additionally, we benchmark several algorithms and a naive proposal in synthetic and real-world datasets. The results from our experiments indicate the need for future research in this area as even naive approaches produced gains in accuracy while reducing resources usage. Finally, we state current research topics, challenges and future directions for feature drift adaptation.		Jean Paul Barddal;Heitor Murilo Gomes;Fabrício Enembreck;Bernhard Pfahringer	2017	Journal of Systems and Software	10.1016/j.jss.2016.07.005	data science;machine learning;data mining	ML	-0.19600383832700374	-35.94109526852622	149725
418ea59d1383dbe28d6d9bb5f9fdd9e86409f632	medical diagnostic system basing fuzzy rough neural-computing for breast cancer		Medical diagnostic system is a branch in bioinformatics that is concerned with classifying medical records. Breast cancer is the most common deployed cancer in females worldwide. The main obstacle is the vagueness and ambiguity involving the breast cancer data. Human nature handles the vagueness and ambiguity easily. Therefore, doctors diagnose the patient condition using their expertise. Fuzziness and rough boundary theories simulate the human thinking. The fuzzy rough hybrids address the uncertainty in terms of membership degree of truth and lower and upper boundaries of fuzzy rough set theory. This research solves the diagnostic breast cancer problems via a proposed hybrid model of fuzzy rough feature selection and rough neural networks. The medical data is preprocessed by the fuzzy rough feature selection algorithm to remove unnecessary attributes. The reduced data set is applied to the rough neural network to learn the connection weights iteratively. The test data set are used to measure the proposed model accuracy and time complexities. Lower and upper approximations of the input features are weighted by input synapses learnt through training phase. The fuzzy rough proposed model design and implementation are declared. The experiments used WDBC and WPBC data sets from the UCI machine learning repository. The experimental results proved the fuzzy rough model ability to classify new instances compared with the conventional neural network.		Mona Gamal Gafar	2016		10.1007/978-3-319-48308-5_45	fuzzy logic;degree of truth;ambiguity;artificial neural network;feature selection;data set;rough set;test data;artificial intelligence;computer science;pattern recognition	AI	3.3804663188342143	-29.02620450637751	149736
ccf8bc780c7dc732081c61f52f30a8b498d8b90e	extended hausdorff distance for spatial objects in gis	relative position;median hausdorff distance;computer model;extended hausdorff distance median hausdorff distance;journal;cluster analysis;minimum distance;hausdorff distance;hausdorff metric;extended hausdorff distance;structural similarity;spatial distance	Distance is a fundamental concept in spatial sciences. Spatial distance is a very important parameter to measure the relative positions between spatial objects and to indicate the degree of similarity between neighbouring objects. Indeed, spatial distance plays an important role in many areas such as neighbourhood analysis, structural similarity measure, image (or object) matching, clustering analysis, and so on. In this paper, existing computational models for the distance between spatial objects are evaluated and their problems pointed out; then, the concept of the Hausdorff distance is introduced as a metric indicator for different types of spatial objects. This distance is extended to a uniform representation by the introduction of the quantile, leading to the extended Hausdorff distance. Indeed, the so‐called extended Hausdorff distance is, in fact, a kind of metric characterized by the minimum distance, the Hausdorff distance, and the median Hausdorff distance. The first two can be used for measurin...	geographic information system;hausdorff dimension	Min Deng;Zhilin Li;Xiaoyong Chen	2007	International Journal of Geographical Information Science	10.1080/13658810601073315	computer simulation;earth mover's distance;hausdorff distance;mathematical analysis;minkowski distance;topology;distance matrix;effective dimension;computer science;hausdorff dimension;total variation distance of probability measures;gromov–hausdorff convergence;mathematics;geometry;hausdorff measure;outer measure;distance transform;distance;jaro–winkler distance	DB	-0.6685524964009756	-25.28603944709952	150439
40aff450eabbc2005c8bedaeef2729862058e4a2	software effort estimation by analogy using attribute selection based on rough set analysis	learning;attribute weighting;attribute selection;software effort estimation;rough sets;feature selection;heuristics;rough set;effort estimation by analogy	Estimation by analogy (EBA) predicts effort for a new project by learning from the performance of former projects. This is done by aggregating effort information of similar projects from a given historical data set that contains projects, or objects in general, and attributes describing the objects. While this has been successful in general, existing research results have shown that a carefully selected subset, as well as weighting, of the attributes may improve the performance of the estimation methods. In order to improve the estimation accuracy of our former proposed EBA method AQUA, which supports data sets that have non-quantitative and missing values, an attribute weighting method using rough set analysis is proposed in this paper. AQUA is thus extended to AQUA by incorporating the proposed attribute weighting and selection method. Better prediction accuracy was obtained by AQUA compared to AQUA for five data sets. The proposed method for attribute weighting and selection is effective in that (1) it supports data sets that have non-quantitative and missing values; (2) it supports attribute selection as well as weighting, which are not supported simultaneously by other attribute selection methods; and (3) it helps AQUA to produce better performance.	aqua;attribute grammar;benchmark (computing);cost estimation in software engineering;cybernetics;decision support system;discretization;experimental software engineering;expert system;feature selection;geo-imputation;heuristic (computer science);informatics;information system;jim christy;machine learning;missing data;nl (complexity);numerical aperture;overhead (computing);preprocessor;rough set;selection (genetic algorithm);selection (relational algebra);soft computing;software development effort estimation;software metric;software project management	Jingzhou Li;Günther Ruhe	2008	International Journal of Software Engineering and Knowledge Engineering	10.1142/S0218194008003532	rough set;attribute domain;computer science;machine learning;pattern recognition;data mining;feature selection	SE	3.3331117104717514	-33.29069069040238	150857
15effdc6fa2c3feb23d2ff0c4a9ab38b9fab437f	using neural networks to predict software faults during testing	software reliability software faults prediction software testing principal components analysis neural network modeling neural networks training observed data predictive quality hidden layers software engineering model predictive quality;modelizacion;prediccion;fiabilidad;reliability;neural nets;qualite;data collection;software systems;software fault tolerance;ingenieria logiciel;development process;software engineering;analisis programa;learning artificial intelligence software fault tolerance program testing neural nets;modelisation;program testing;quality;principal component analysis;fiabilite;genie logiciel;program analysis;neural network model;learning artificial intelligence;reseau neuronal;analyse programme;point of view;modeling;prediction;red neuronal;training algorithm;calidad;neural network;neural networks software testing predictive models application software principal component analysis software measurement software systems data mining neurons computer architecture;principal component	1. Ten software product measures were gathered from a large commercial software system. Principal components were then extracted from these measures. 2. We trained two neural networks, one with the observed (raw) data, and one with principal components. 3. We compare the predictive quality of the two competing models using data collected from two similar systems. These systems were developed by the same organization, and used the same development process.	artificial neural network;commercial software;software quality assurance;software system	Taghi M. Khoshgoftaar;Robert M. Szabo	1996	IEEE Trans. Reliability	10.1109/24.537016	reliability engineering;computer science;machine learning;data mining;mathematics;artificial neural network;statistics;principal component analysis	SE	7.440242998868997	-30.511619994869843	150894
0736554cc0bddb854642f959a3bf9ee4e22c24d1	radial basis functions versus geostatistics in spatial interpolations	neural nets;objeto de conferencia;soft computing;radial basis functions rbf;radial basis function;spatial interpolation;ciencias informaticas;computing methodologies;artificial neural network;environmental monitoring	A key problem in environmental monitoring is the spatial interpolation. The main current approach in spatial interpolation is geostatistical. Geostatistics is neither the only nor the best spatial interpolation method. Actually there is no “best” method, universally valid. Choosing a particular method implies to make assumptions. The understanding of initial assumption, of the methods used, and the correct interpretation of the interpolation results are key elements of the spatial interpolation process. A powerful alternative to geostatistics in spatial interpolation is the use of the soft computing methods. They offer the potential for a more flexible, less assumption dependent approach. Artificial Neural Networks are well suited for this kind of problems, due to their ability to handle non-linear, noisy, and inconsistent data. The present paper intends to prove the advantage of using Radial Basis Functions (RBF) instead of geostatistics in spatial interpolations, based on a detailed analyze and modeling of the SIC2004 (Spatial Interpolation Comparison) dataset.	artificial neural network;multivariate interpolation;neural networks;nonlinear system;radial (radio);radial basis function;soft computing	Cristian Rusu;Virginica Rusu	2006		10.1007/978-0-387-34747-9_13	computer science;artificial intelligence;machine learning;radial basis function network;algorithm	Vision	6.595670501295855	-27.42536021117861	150895
3984882b10082298c93245fc09f45d400917bf4e	pop-traffic: a novel fuzzy neural approach to road traffic analysis and prediction	modelizacion;ecoulement trafic;feedforward neural network;forecasting;traffic forecasting;feedforward neural networks;road traffic analysis;fuzzy neural network;fuzzy neural nets;neural networks;fuzzy knowledge extraction road traffic analysis road traffic prediction statistical regression models fuzzy logic pseudo outer product fuzzy neural network truth value restriction method;design engineering;roads telecommunication traffic traffic control fuzzy neural networks neural networks design engineering fuzzy logic data mining feedforward neural networks backpropagation;truth value restriction fuzzy inference;road traffic;gestion trafic;traffic prediction;logique floue;knowledge extraction;traffic control;logica difusa;road traffic prediction;regression model;reseau neuronal flou;traffic flow;circuito logico;traffic management;backpropagation;data mining;statistical regression;noise tolerance;statistical regression models;truth value restriction fuzzy inference density and volume prediction fuzzy neural network noise tolerance pseudo outer product based fuzzy neural network popfnn traffic analysis traffic prediction and modeling;fuzzy knowledge extraction;fuzzy logic;modelisation;truth value restriction method;modelo regresion;telecommunication traffic;density and volume prediction;trafic routier;circuit logique;design and implementation;roads;regresion estadistica;knowledge acquisition;backpropagation algorithm;modele regression;decouverte connaissance;fuzzy inference;inferencia;court terme;extraction connaissances;gestion trafico;traffic analysis;algorithme retropropagation;extraccion conocimiento;descubrimiento conocimiento;trafico carretera;regression analysis;traffic prediction and modeling;traffic engineered;knowledge acquisition road traffic regression analysis fuzzy logic fuzzy neural nets;mathematical prediction;reseau neuronal;fuzzy neural networks;regression statistique;pseudo outer product based fuzzy neural network popfnn;modeling;logic circuit	Although much research has been done over the decades on the formulation of statistical regression models for road traffic relationships, they have been largely unsuitable due to the complexity of traffic characteristics. Traffic engineers have resorted to alternative methods such as neural networks, but despite some promising results, the difficulties in their design and implementation remain unresolved. In addition, the opaqueness of trained networks prevents understanding the underlying models. Fuzzy neural networks, which combine the complementary capabilities of both neural networks and fuzzy logic, thus constitute a more promising technique for modeling traffic flow. This paper describes the application of a specific class of fuzzy neural network known as the pseudo outer-product fuzzy neural network using the truth-value-restriction method (POPFNN-TVR) for short-term traffic flow prediction. The obtained results highlight the capability of POPFNN-TVR in fuzzy knowledge extraction and generalization from input data as well its high degree of prediction capability as compared to traditional feedforward neural networks using backpropagation learning.	artificial neural network;authentication;autonomous car;backpropagation;computation;computational intelligence;data-flow analysis;dynamic problem (algorithms);feedforward neural network;fingerprint;fuzzy control system;fuzzy logic;neuro-fuzzy;nonlinear system;norm (social);outer product;problem domain;statistical model;traffic analysis;value restriction	Hiok Chai Quek;Michel Pasquier;Bernard Boon Seng Lim	2006	IEEE Transactions on Intelligent Transportation Systems	10.1109/TITS.2006.874712	adaptive neuro fuzzy inference system;computer science;engineering;artificial intelligence;neuro-fuzzy;machine learning;neural modeling fields;data mining;time delay neural network;artificial neural network;regression analysis;intelligent control	ML	8.576874796282851	-28.340003137122334	151024
2cae9095b74e7b2a80b2b9528a974fe99c7eab54	a survey of credit card fraud detection techniques: data and technique oriented perspective		Credit card plays a very important rule in today's economy. It becomes an unavoidable part of household, business and global activities. Although using credit cards provides enormous benefits when used carefully and responsibly,significant credit and financial damagesmay be causedby fraudulent activities. Many techniques have been proposed to confront thegrowthin credit card fraud. However, all of these techniques have the same goal of avoiding the credit card fraud; each one has its own drawbacks, advantages and characteristics. In this paper, after investigating difficultiesof credit card fraud detection, we seek to review the state of the art in credit card fraud detection techniques, datasets and evaluation criteria.The advantages and disadvantages of fraud detection methods are enumerated and compared.Furthermore, a classification of mentioned techniques into two main fraud detection approaches, namely, misuses (supervised) and anomaly detection (unsupervised) is presented. Again, a classification of techniques is proposed based on capability to process the numerical and categorical datasets. Different datasets used in literatureare then described and grouped into real and synthesized data and the effective and common attributesare extracted for further usage.Moreover, evaluation employed criterions in literature are collected and discussed.Consequently, open issues for credit card fraud detection are explained as guidelinesfor	anomaly detection;credit card fraud;numerical analysis	Samaneh Sorournejad;Zahra Zojaji;Reza Ebrahimi Atani;Amir Hassan Monadjemi	2016	CoRR		data mining;computer security	Web+IR	1.3686492469933762	-33.55819948830196	151225
0c0df9c3298f7b92cfdc813492e2cd5d9f8e4785	clustering categorical data streams	cluster algorithm;empirical analysis;data stream;data mining;artificial intelligent;cluster analysis;clustering;categorical data	The data stream model is relevant to new classes of applications involving massive datasets, such as web click stream analysis and detection of network intrusions. The cluster analysis on evolving data stream becomes more difficult, because the data objects in the stream must be accessed in order and can read only once or a small number of times with limited resources. In more recently years, a few clustering algorithms have be developed for data stream problem. However, to our knowledge, there is nothing to date in the literature describing clustering algorithms for categorical data streams. This paper presents an effective categorical data stream clustering algorithm. The proposed algorithm has provably small memory footprints. We also provide empirical evidence of the algorithm’s performance on real datasets and synthetic data streams.	categorical variable;digimon world;exponential hierarchy;letter-quality printer;rs-232	Zengyou He;Xiaofei Xu;Shengchun Deng;Joshua Zhexue Huang	2011	J. Comput. Meth. in Science and Engineering	10.3233/JCM-2011-0363	correlation clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;data science;canopy clustering algorithm;machine learning;consensus clustering;pattern recognition;cure data clustering algorithm;data mining;cluster analysis;single-linkage clustering;affinity propagation;clustering high-dimensional data	DB	-1.7715209952328232	-36.93157764671781	151475
3257fb0cc2ff20dd5c66bc87377cf18efa886a04	group-wise similarity and classification of aggregate scanpaths	data gathering;scanpath comparison;classification;eye tracking	"""We present a novel method for the measurement of the similarity between aggregates of scanpaths. This may be thought of as a solution to the """"average scanpath"""" problem. As a by-product of this method, we derive a classifier for groups of scanpaths drawn from various classes. This capability is empirically demonstrated using data gathered from an experiment in an attempt to automatically determine expert/novice classification for a set of visual tasks."""	aggregate data;aggregate function;statistical classification	Thomas Grindinger;Andrew T. Duchowski;Michael W. Sawyer	2010		10.1145/1743666.1743691	computer science;machine learning;pattern recognition;data mining	ML	2.850780146433594	-36.38100658005196	151690
11c5ead16efb7fe85314b051be75b5c8156009ae	a genetic based fuzzy-neural networks design for system identification	fuzzy neural network;fuzzy neural nets;fuzzy set;system identification fuzzy neural networks function approximation fuzzy control artificial neural networks fuzzy logic genetic algorithms biological cells genetic mutations nonlinear systems;fuzzy set theory;nonlinear functions;fuzzy sets genetic fuzzy neural network system identification modified genetic algorithm nonlinear function approximation chromosome structure mutation operation fitness function;fuzzy neural network genetic algorithms;function approximation;system identification;nonlinear functions fuzzy set theory genetic algorithms fuzzy neural nets;genetic algorithm;genetic algorithms;fitness function;neural network	In this paper, we use a modified genetic algorithm (MGA) to construct a fuzzy neural network (FNN), spontaneously, which can approximate a nonlinear function as well as possible. With the specific structure of the chromosome, the special mutation operation and the adequate fitness function, the proposed method with MGA produces a FNN with minimum structure of neural network, smaller number of rules, suitable placement of the premise's fuzzy sets and proper location of the consequent singletons. Finally, an example is illustrated to show the effectiveness of the proposed method on the nonlinear function approximation.	approximation algorithm;artificial neural network;convex function;fitness function;fuzzy set;genetic algorithm;hercules graphics card;neuro-fuzzy;nonlinear system;system identification	T. G. Yen;Chi-Hsu Kang;Wen June Wang	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571224	mathematical optimization;genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;artificial neural network;intelligent control	Robotics	5.9075476696019	-26.68064327284474	151830
908b7c9961ed978dc2be1ac13d2c9eb62d9db03a	programming risk assessment models for online security evaluation systems	risk management security genetic programming biological cells risk analysis mathematical model humans frequency costs fuzzy logic;security evaluation;programming risk assessment models;genetic program;human reasoning;risk analysis;risk management;genetic programming;security of data genetic algorithms risk management;data mining;genetics;fuzzy logic;genetic programming methods;computational modeling;biological cells;machine intelligence;machine intelligence risk assessment fuzzy logic genetic programming;mathematical model;perception process;risk assessment;genetic algorithms;humans;frequency;genetic programming methods online security evaluation systems programming risk assessment models human reasoning perception process;online security evaluation systems;security;programming;security of data	Risk assessment is often done by human experts, becausethere is no exact and mathematical solution to the problem.Usually the human reasoning and perception process cannotbe expressed precisely. This paper propose a geneticprogramming approach for risk assessment. Preliminaryresults indicate that genetic programming methods are robustand suitable for this problem when compared to otherrisk assessment models.	code;equation solving;genetic programming;intrusion detection system;risk assessment	Ajith Abraham;Crina Grosan;Václav Snásel	2009	2009 11th International Conference on Computer Modelling and Simulation	10.1109/UKSIM.2009.75	reliability engineering;computer science;machine learning;data mining	Vision	3.329772677632009	-27.28074272011423	151933
43e3052bbfc3c20d2941ec26021cbaaf19cff54c	towards association rules with hidden variables	busqueda informacion;variable cachee;hidden variable theory;association statistique;information systems;information retrieval;technology;comercializacion;database;teoria variable escondida;base dato;statistical association;data mining;commercialisation;asociacion estadistica;science technology;association rule;fouille donnee;recherche information;marketing;hidden variables;theorie variable cachee;decouverte connaissance;base de donnees;artificial intelligence;descubrimiento conocimiento;computer science;busca dato;knowledge discovery	The mining of association rules can provide relevant and novel information to the data analyst. However, current techniques do not take into account that the observed associations may arise from variables that are unrecorded in the database. For instance, the pattern of answers in a large marketing survey might be better explained by a few latent traits of the population than by direct association among measured items. Techniques for mining association rules with hidden variables are still largely unexplored. This paper provides a sound methodology for finding association rules of the type H ⇒ A1, . . . , Ak, where H is a hidden variable inferred to exist by making suitable assumptions and A1, . . . , Ak are discrete binary or ordinal variables in the database.	algorithm;association rule learning;dynamic programming;hidden variable theory;ordinal data;population;preprocessor;scalability;self-replicating machine	Ricardo Bezerra de Andrade e Silva;Richard Scheines	2006		10.1007/11871637_63	computer science;artificial intelligence;data science;data mining;knowledge extraction;hidden variable theory;statistics	DB	-0.7690795522135311	-31.55716277335318	151972
8c527b27109da03c9d5b521a562a655d4f0472b7	a rough set approach for approximating differential dependencies		Abstract Data dependencies in databases and attribute dependencies in decision systems are important when addressing problems concerning data quality and attribute reduction, in which measures play a significant role in approximating these dependencies to achieve better adaptation to uncertain data. This paper proposes a differential-relation-based rough set model from the perspective of relational databases to express the dependency degree, error measures, confidence, information granulation and differential class distance for differential dependencies (DDs) and the relationships among them in a unified framework. Moreover, the error measure g 3 has been widely studied and applied for data dependencies. However, the computation of g 3 for DDs is NP-complete. Therefore, based on the proposed rough set, we introduce a new method that can compute the approximate error measure g 3 ˜ of g 3 in polynomial time. This study demonstrates that our approach can provide a substantially better approximation, that is, an approximation closer to the optimal solution g 3 , compared to the existing greedy method. We also introduce the differential-relation-based rough set from the perspective of information systems and make a connection to the rough sets induced by non-equivalence relations. The two views of the differential-relation-based rough sets form an essential bridge between the DDs in databases and attribute dependencies in differential decision systems (DDSs) that allows sharing measures for approximating the dependencies. These results are meaningful for approximate computations, the development of algorithms for attribute reduction in decision systems and the discovery of approximate differential dependencies (ADDs) in databases.	rough set	Anh Duy Tran;Somjit Arch-int;Ngamnij Arch-int	2018	Expert Syst. Appl.	10.1016/j.eswa.2018.06.025	time complexity;data mining;uncertain data;computation;information system;relational database;computer science;rough set;data quality;greedy algorithm	DB	-2.9999335101024696	-28.578541637239017	152145
197254b75475a3b21eaba2eed8c6711c25cad795	systematic mapping study of dealing with error in software development effort estimation	uncertainty;systematic mapping study;software development effort estimation;error	Over the last decades, the software engineering community has investigated new techniques for software development effort estimation. Unfortunately, the estimates were not always accurate. Error approaches are then, aninteresting track for improving the projects running performances and their financial profitability. The aim of this systematic mapping study is to summarize and synthesize theexisting studies dealing with effort estimation error and uncertainty and to classify them based on research approaches, contribution types, accuracy criteria, datasets, error approaches and effort estimation techniques used. In total 19papers published between 1990 and 2015 were selected. We observed a balance between the managerial approaches and the technical ones. Furthermore, the proposed errortechniques and frameworks improve in general the accuracy ofeffort estimation techniques. Fuzzy logic, bootstrapping andrisk analysis are promising avenues that could be combined with various estimation techniques.	artificial neural network;cocomo;categorization;ensemble learning;error message;experiment;fuzzy logic;genetic algorithm;it risk management;machine learning;neural network software;performance;security device event exchange;software development effort estimation;systematic review	Salma El Koutbi;Ali Idri;Alain Abran	2016	2016 42th Euromicro Conference on Software Engineering and Advanced Applications (SEAA)	10.1109/SEAA.2016.39	uncertainty;computer science;data mining;management science	SE	3.472576618639708	-33.46931524094434	152153
72a32366e757f79c6f8f4d11fcabfc1fb01991e4	efficiency evaluation based on data envelopment analysis in the big data context		Abstract Data envelopment analysis (DEA) is a self-evaluation method which assesses the relative efficiency of a particular decision making unit (DMU) within a group of DMUs. It has been widely applied in real-world scenarios, and traditional DEA models with a limited number of variables and linear constraints can be computed easily. However, DEA using big data involves huge numbers of DMUs, which may increase the computational load to beyond what is practical with traditional DEA methods. In this paper, we propose novel algorithms to accelerate the computation process in the big data environment. Specifically, we firstly use an algorithm to divide the large scale DMUs into small scale and identify all strongly efficient DMUs. If the strongly efficient DMU set is not too large, we can use the efficient DMUs as a sample set to evaluate the efficiency of inefficient DMUs. Otherwise, we can identify two reference points as the sample in the situation of just one input and one output. Furthermore, a variant of the algorithm is presented to handle cases with multiple inputs or multiple outputs, in which some of the strongly efficient DMUs are reselected as a reduced-size sample set to precisely measure the efficiency of inefficient DMUs. Last, we test the proposed methods on simulated data in various scenarios.	big data;data envelopment analysis	Qingyuan Zhu;Jie Wu;Malin Song	2018	Computers & OR	10.1016/j.cor.2017.06.017	efficiency;mathematical optimization;mathematics;big data;data envelopment analysis	ML	1.6335816114403274	-31.215254919400167	152157
8782e618fe556d7a97de60d8ffca8a59f4c82085	clustering data streams	computational complexity;data analysis;deterministic algorithms;pattern clustering;very large databases;web click stream analysis;constant-factor approximation algorithms;data stream clustering;data stream model;deterministic algorithms;k-median problem;massive data sets;multimedia data analysis;point sequence	We study clustering under the data stream model of computation where: given a sequence of points, the objective is to maintain a consistently good clustering of the sequence observed so far, using a small amount of memory and time. The data stream model is relevant to new classes of applications involving massive data sets, such as web click stream analysis and multimedia data analysis. We give constant-factor approximation algorithms for the k{Median problem in the data stream model of computation in a single pass. We also show negative results implying that our algorithms cannot be improved in a certain sense.	apx;approximation algorithm;clickstream;cluster analysis;model of computation;streaming algorithm	Sudipto Guha;Nina Mishra	2016		10.1007/978-3-540-28608-0_8	correlation clustering;constrained clustering;data stream clustering;fuzzy clustering;computer science;data science;theoretical computer science;data mining;data stream mining;cluster analysis;data analysis;computational complexity theory;algorithm;clustering high-dimensional data	Theory	-2.847122312143668	-37.26204690858089	152261
ab6b823b7bd371da51cfb39bd7c6d1bc4531ce21	quaternion neuro-fuzzy learning algorithm for fuzzy rule generation	neural networks;training;quaternions noise measurement fuzzy logic neural networks training fuzzy systems educational institutions;noise measurement;fuzzy logic;neural networks neuro fuzzy quaternion neural networks fuzzy;fuzzy;neuro fuzzy;quaternion neural networks;fuzzy systems;quaternions	In order to generate or tune fuzzy rules, Neuro-Fuzzy learning algorithms with Gaussian type membership functions based on gradient-descent method are well known. In this paper, we propose a new learning approach, the Quaternion Neuro-Fuzzy learning algorithm. This method is an extension of the conventional method to four-dimensional space by using a quaternion neural network that maps quaternion to real values. Input, antecedent membership functions and consequent singletons are quaternion, and output is real. Four-dimensional input can be better represented by quaternion than by real values. We compared it with the conventional method by several function identification problems, and revealed that the proposed method outperformed the counterpart: The number of rules was reduced to 5 from 625, the number of epochs by one fortieth, and error by one tenth in the best cases.	algorithm;artificial neural network;fuzzy control system;fuzzy rule;gradient descent;machine learning;map;membership function (mathematics);neuro-fuzzy;time series	Ryusuke Hata;Md. Monirul Islam;Kazuyuki Murase	2013	2013 Second International Conference on Robot, Vision and Signal Processing	10.1109/RVSP.2013.22	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;fuzzy mathematics;fuzzy classification;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;control theory;mathematics;fuzzy associative matrix;fuzzy set operations	Robotics	5.7478125605979	-27.446228219571275	152885
f046ca3355275e0554683d405418c7744b98ba7e	hybrid connectionist fuzzy production system: towards building comprehensive ai	neural networks;production systems;fuzzy systems;hybrid systems	The article presents major principles of building complex hybrid systems for knowledge engineering. The systems make use of neural networks (for low level processing), fuzzy systems (for intermediate level processing), and symbolic AI systems (for higher level processing). Knowledge acquisition and rules extraction is considered an important part of the whole system. An experimental environment, FuzzyCOPE, which facilitates building comprehensive AI systems, is described. It consists of data a analysis module, a neural network module, a fuzzy inference module, a production rules module, and a fuzzy rules extraction module. Such an environment makes possible using all of the three paradigms, i.e. fuzzy rules, neural networks and symbolic production rules, as well as other paradigms of soft computing, in one system. A methodology for building comprehensive AI systems is described. The use of FuzzyCOPE for building hybrid systems is illustrated on a test problem for decision making in stock trading. A comprehensive AI system for spoken language recognition is also given for illustration.	artificial neural network;connectionism;fuzzy control system;fuzzy rule;hybrid system;information extraction;knowledge acquisition;knowledge engineering;soft computing;symbolic artificial intelligence	Nikola K. Kasabov	1995	Intelligent Automation & Soft Computing	10.1080/10798587.1995.10750641	fuzzy electronics;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;production system;fuzzy associative matrix;fuzzy set operations;artificial neural network;fuzzy control system;hybrid system	AI	4.376948232855089	-26.370409200708245	152948
92361b12b79689a3b384d056de91c7f8c63cb02f	computational intelligence in data mining	computational intelligence;soft computing;rule based;model performance;data mining;model reduction;fuzzy rule base;feature extraction;feature selection;data classification;knowledge discovery;expert system;fuzzy classifier	This paper describes links between computational intelligence (CI), data mining and knowledge discovery. The generating elements of soft computing based data mining algorithms are defined where the extracted knowledge is represented by fuzzy rule-based expert systems. It is recognized that both model performance and interpretability are of major importance, and effort has to make to keep the resulting rule bases small and comprehensible. Therefore, the model is derived from data by CI techniques developed for feature selection, feature extraction, model optimization and model reduction (rule base simplification). Applications of these generating elements to the Wine data classification problem are also shown.	cluster analysis;computation;computational intelligence;data mining and knowledge discovery;decision tree model;expert system;feature extraction;feature selection;fuzzy rule;genetic algorithm;knowledge representation and reasoning;level of detail;linear separability;logic programming;mathematical optimization;rule-based system;synergy	János Abonyi;Balazs Feil;Ajith Abraham	2005	Informatica (Slovenia)		feature extraction;computer science;artificial intelligence;machine learning;computational intelligence;pattern recognition;data mining;soft computing;feature selection;expert system	AI	4.253056714786646	-27.98164279965082	153246
91bb28eca455e75277a617d52d64dbf12eefb90e	what can we do with graph-structured data? – a data mining perspective	tratamiento datos;extraction information;graph theory;teoria grafo;analisis datos;information extraction;complex structure;data processing;traitement donnee;intelligence artificielle;data mining;theorie graphe;graph mining;data analysis;fouille donnee;estructura datos;artificial intelligence;analyse donnee;structure donnee;inteligencia artificial;web browsing;data structure;busca dato;extraccion informacion;structured data	Feedback in multimodal self-organizing networks enhances perception of corrupted stimuli p. 19 Identification of fuzzy relation model using HFC-based parallel genetic algorithms and information data granulation p. 29 Evaluation of incremental knowledge acquisition with simulated experts p. 39 Finite domain bounds consistency revisited p. 49 Speeding up weighted constraint satisfaction using redundant modeling p. 59 Verification of multi-agent systems via bounded model checking p. 69 Logical properties of belief-revision-based bargaining solution p. 79 Knowledge compilation for belief change p. 90 Design methodologies of fuzzy set-based fuzzy model based on GAs and information granulation p. 100 ALE defeasible description logic p. 110 Representation and reasoning for recursive probability models p. 120 Forgetting and knowledge update p. 131 Enhanced temporal difference learning using compiled eligibility traces p. 141 A simple artificial immune system (SAIS) for generating classifier systems p. 151 Selection for feature gene subset in microarray expression profiles based on an improved genetic algorithm p. 161 An efficient alternative to SVM based recursive feature elimination with applications in natural language processing and bioinformatics p. 170 Efficient AUC learning curve calculation p. 181 Learning hybrid Bayesian networks by MML p. 192 A novel nearest neighbor classifier based on adaptive nonparametric separability p. 204 Virtual attribute subsetting p. 214 Parallel chaos immune evolutionary programming p. 224 Classification of lung disease pattern using seeded region growing p. 233 Voting massive collections of Bayesian network classifiers for data streams p. 243 Feature weighted minimum distance classifier with multi-class confidence estimation p. 253 z-SVM : an SVM for improved classification of imbalanced data p. 264 GP for object classification : brood size in brood recombination crossover p. 274 IPSOM : a self-organizing map spatial model of how humans complete interlocking puzzles p. 285 Mining generalised emerging patterns p. 295 Using attack-specific feature subsets for network intrusion detection p. 305 Time series analysis using fractal theory and online ensemble classifiers p. 312 MML mixture models of heterogeneous poisson processes with uniform outliers for bridge deterioration p. 322 Extracting structural features among words from document data streams p. 332 Clustering similarity comparison using density profiles p. 342 Efficient mining of frequent itemsets in distorted databases p. 352 Improved support vector machine generalization using normalized input space p. 362 An efficient similarity measure for clustering of categorical sequences p. 372 SDI shape distribution indicator and its application to find interrelationships between physical activity tests and other medical measures p. …	artificial immune system;bayesian network;belief revision;bioinformatics;cluster analysis;compiler;constraint satisfaction;data breach;data mining;database;defeasible reasoning;description logic;evolutionary programming;fractal;fuzzy set;genetic algorithm;graph (abstract data type);hybrid fibre-coaxial;intrusion detection system;knowledge acquisition;knowledge compilation;linear separability;microarray;mixture model;model checking;multi-agent system;multimodal interaction;natural language processing;nearest neighbour algorithm;organizing (structure);recursion;region growing;self-organization;self-organizing map;similarity measure;support vector machine;temporal difference learning;time series;tracing (software)	Hiroshi Motoda	2006		10.1007/11941439_1	data structure;data processing;data model;computer science;graph theory;data science;machine learning;data mining;generalized complex structure;data stream mining;data analysis;molecule mining;information extraction	AI	-0.6678242436375588	-32.85876466598411	153381
127620dc39a46a1e48e8d80525e8a0191866c6f6	evolutionary multiobjective optimization and multiobjective fuzzy system design	fuzzy rule based system;accuracy complexity tradeoff;genetic program;evolutionary multiobjective optimization emo;fuzzy rule based systems;evolutionary multiobjective optimization;data mining;many objective optimization;computer experiment;multiobjective optimization;feature selection;fuzzy system;neural network;evolutionary computing;multiobjective design	Evolutionary multiobjective optimization (EMO) is one of the most active research areas in evolutionary computation. EMO algorithms have been successfully used in various application areas. Among them are multiobjective design of neural networks and fuzzy systems. Especially, fuzzy system design has often been discussed as multiobjective problems. This is because we have two conflicting objectives in the design of fuzzy systems: accuracy maximization and complexity minimization. In this paper, we first explain some basic concepts in multiobjective optimization, a basic framework of EMO algorithms and some hot research issues in the EMO community. Next we explain EMO-based approaches to the design of fuzzy systems. We demonstrate through computational experiments that a large number of non-dominated fuzzy systems with different accuracy-complexity tradeoffs can be obtained by a single run of an EMO algorithm. Then we describe the use of EMO algorithms in other areas such as neural networks, genetic programming, clustering, feature selection, and data mining.	approximation algorithm;artificial neural network;cluster analysis;data mining;evolutionary computation;expectation–maximization algorithm;experiment;feature selection;fuzzy control system;genetic programming;mathematical optimization;multi-objective optimization;systems design	Hisao Ishibuchi	2008		10.1145/1456223.1456226	mathematical optimization;computer experiment;computer science;artificial intelligence;multi-objective optimization;machine learning;feature selection;artificial neural network	AI	4.987148131171149	-29.416827189434784	153619
fd73436666cff76c9c576dfd47d361caa523cc22	detection of discriminating rules		Assume a population partitioned in two subpopulations, e.g. a set of normal individuals and a set of abnormal individuals, is given. Assume, moreover, that we look for a characterization of the reasons discriminating one subpopulation from the other. In this paper, we provide a technique by which such an evidence can be mined, by introducing the notion of discriminating rule, that is a kind of logical implication which is much more valid in one of the two subpopulations than in the other one. In order to avoid mining a potentially huge number of (not necessarily interesting) rule, we define a preference relationship among rules and exploit a suitable graph encoding in order to single out the most interesting ones, which we call outstanding rules . We provide an algorithm for detecting the outstanding discriminating rules and present experimental results obtained by applying the technique in several scenarios.	algorithm;association rule learning;database;de bruijn graph;druid;kramer graph;mined;population;rule 90;sensor;version space learning	Fabrizio Angiulli;Fabio Fassetti;Luigi Palopoli;Domenico Trimboli	2010			machine learning;data mining;artificial intelligence;computer science	DB	-4.192634433863685	-29.639851659289754	153725
97424425708f9e5d69aa1d99bf89f2377213f3a9	using artificial neural networks to aid decision making processes	fuzzy number;backpropagation;decision making process;backpropagation algorithm;artificial neural network	Ranking fuzzy numbers is very necessary when we have to make a decision with imprecise information.The comparison depends on decision-maker's subjectivity and then capturing it into algorithms is difficult. Several methods has been developed in order to ranking fuzzy numbers, each of them being subjective, but the lack of real fitness is always present. Artificial Neural Networks (ANN) are able to model systems with unknown performance (learning their behavior) and thus ANN may be used in Decision Making Problems to disclose decision maker's unknown behavior. In this paper, we propose ranking fuzzy numbers using ANNs. We present several experiences: We have simulated an ANN that use the Backpropagation algorithm for learning. Also we show that it is possible to take a decision using ANNs, when we have fuzzy information.	algorithm;artificial neural network;backpropagation;fuzzy logic;fuzzy number;game-maker;neural networks;neural network software	José E. Cano;Miguel Delgado;Ignacio Requena	1991		10.1007/BFb0035928	rprop;types of artificial neural networks;computer science;artificial intelligence;backpropagation;neuro-fuzzy;machine learning;time delay neural network;deep learning;artificial neural network	AI	4.054886676565964	-25.974612262856706	153874
9e5f2606f33a4c427ac2ab857a22ecda2405f5ad	novel distance and similarity measures on hesitant fuzzy sets with applications to clustering analysis	期刊论文	Distance and similarity measures are fundamentally important in a variety of scientific fields such as clustering analysis, pattern recognition and decision making, etc. In this paper, by analyzing the existing distance and similarity measures between hesitant fuzzy sets, we show that they are not reasonable in some situations. To this end, we propose a novel concept of hesitancy index of hesitant fuzzy set to measure the hesitancy degree among the possible values in each hesitant fuzzy element of the hesitant fuzzy set. By taking their hesitancy indices into account, new methods for measuring the distances between hesitant fuzzy sets are proposed and their properties are discussed. According to the relationship between the distance measure and the similarity measure, two novel similarity measures for hesitant fuzzy sets are further developed. Afterwards, we propound a novel hesitant fuzzy clustering algorithm on the basis of the novel similarity measures for classifying objects with hesitant fuzzy sets. At length, a real-life example is given to illustrate the detailed implementation process of the proposed clustering approach, and a comparative study on the same example is conducted.	algorithm;cluster analysis;fuzzy clustering;fuzzy set;pattern recognition;real life;similarity measure	Xiaolu Zhang;Zeshui Xu	2015	Journal of Intelligent and Fuzzy Systems	10.3233/IFS-141511	fuzzy clustering;computer science;data mining	ML	-1.1985227995364296	-26.743557120622445	153903
28a23e786f806c5ab619cd42b7912473b10345a4	basis of fuzzy knowledge discovery system	extraction information;raisonnement probabiliste;systeme intelligent;procesamiento informacion;adquisicion del conocimiento;information extraction;systeme apprentissage;fuzzy rules;sistema inteligente;acquisition connaissances;learning systems;knowledge acquisition;indexation;information processing;intelligent system;sistema difuso;systeme flou;traitement information;requirement specification;modus ponens;fuzzy system;probabilistic reasoning;knowledge discovery;extraction informacion	Considering a fuzzy knowledge discovery system we have realiz ed we describe here the main features of such systems. First, we consider possible methods to define fuzzy partitions on numerical attributes in order to replace continuous or sy mbolic attributes by fu zzy ones. We explain th en how to generalize statistical indexes to evaluate fuzzy rules, detailing a special index, the intensity of implication and its generalization to fuzzy rules. We describe then one algorithm use to extract fuzzy rules. Since many fuzzy operators are available, we propose a method to choose one fuzzy conjunction, one fuzzy implication and one fu zzy aggregation, and we explain how th is choice may be validated by comparing the results of the Generalized Modus Ponens applied on the premises of the examples to the effective conclusions in the database. To reduce the important number of fuzzy rules extracted, we consider also some methods to aggregate fuzzy rules, showing that usage of classical reduction schemes requires specific choices of fuzzy operators.	aggregate data;algorithm;discovery system;execution unit;fuzzy rule;numerical analysis;statistical model	Maurice Bernadet	2000		10.1007/3-540-45372-5_3	fuzzy logic;fuzzy cognitive map;membership function;defuzzification;information processing;modus ponens;adaptive neuro fuzzy inference system;fuzzy transportation;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;fuzzy measure theory;data mining;mathematics;fuzzy set;knowledge extraction;fuzzy associative matrix;probabilistic logic;fuzzy set operations;information extraction;algorithm;fuzzy control system	ML	-0.20349573146756542	-26.365139677169765	153985
535891c03c914fcbc183a6661b25102d45785892	fuzzy associative memories: a design through fuzzy clustering	reconstruction;fuzzy correlation matrix;fuzzy clustering;optimization of recall;fuzzy associative memories	In this study, we discuss a design of fuzzy associative structures (memories) realized within the framework of fuzzy clustering. Associative memories are inherently direction-free structures (and the recall of objects can be realized for any variable or a subset of variables). Fuzzy clustering being direction-free comes here as a sound design alternative. Two recall proposals are studied: one involves prototypes available data to be recalled whereas the second proposal involves fuzzy correlation matrices and in principle exhibits some resemblance with a standard correlation associative memories. In the setting of associative memories, Fuzzy C-Means (FCM) is studied. The recall error is discussed with regard to the essential parameters of the FCM (the number of clusters and the fuzzification coefficient). Furthermore we discuss an optimization of the distance function used in the clustering algorithm realized with regard to the recall error. Experimental results are provided along with a comparative study involving correlation-based associative memories. & 2015 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;coefficient;fuzzy clustering;fuzzy cognitive map;fuzzy set;jaccard index;mathematical optimization	Chunfu Zhong;Witold Pedrycz;Zhiwu Li;Dan Wang;Lina Li	2016	Neurocomputing	10.1016/j.neucom.2015.08.072	fuzzy clustering;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;fuzzy associative matrix;algorithm	AI	-0.5018502527659253	-26.396336867919413	154083
0e100ea0c72fcfcdd76a8cced24bf9cf63531b9b	evaluation of ecological conditions using bioindicators: application of fuzzy modeling	fuzzy set;environmental evaluation;biological indicators;biological indicator;adaptive neuro fuzzy inference system;fuzzy model;fuzzy set methodology	Exploring biological indicators as tool for evaluation of ecological conditions is one of prime interest for planning process. The focus of this paper is biological indicator based on seed characteristics and defined with the use of fuzzy sets methodology. It is considered application of fuzzy biological indicators in combination with the minimum average weighted deviation method. Finally, Adaptive Neuro-Fuzzy Inference System is utilized for categorization of biological indicators.		Michael Arkhipov;Elena Krueger;Dmitry Kurtener	2008		10.1007/978-3-540-69839-5_36	adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;neuro-fuzzy;machine learning;data mining;mathematics;bioindicator	NLP	2.4023086610874156	-25.868191139131095	154084
af2a5125328e762e76c260eae2e95a9348ef1c5d	improved fuzzy and neural network algorithms for word frequency prediction in document filtering	word frequency;information retrieval;fuzzy logic;parameter set reduction;neural network			Péter Baranyi;László T. Kóczy;Tamás D. Gedeon	1998	JACIII	10.20965/jaciii.1998.p0088	fuzzy logic;adaptive neuro fuzzy inference system;fuzzy classification;computer science;neuro-fuzzy;machine learning;pattern recognition;data mining;time delay neural network;word lists by frequency;artificial neural network	ML	4.031004189028934	-25.933929960942713	154092
3b1220d2f837c1717aeb60da4791a6ce34a71b85	a generalized framework for multi-criteria classifiers with automated learning: application on flir ship imagery		Supervised classification often consists in assigning a set of entities (e.g. alternatives, images, projects, subjects) into pre-defined and homogeneous categories. Categories are known a priori either by defining profiles limit between them or by a set of typical profiles (reference prototypes or elements) for each category. Ordinal Classification (or Sorting) usually refers to an order relationship between the categories, and nominal classification otherwise. Recently, a variety of classification methods–based on Artificial Intelligence (AI) and Operations Research (OR) techniques–have been proposed to solve classification problems [41]. Neural Networks (NN), Machine Learning (ML), Rough Sets (RS), Fuzzy Sets (FS) and Multi-Criteria Decision Analysis (MCDA) were used for the development and the validation of these methods. This paper focuses on classification methods based on MCDA methodology. In this paper we use Multi-Criteria Classifiers (MCCs) to designate supervised classification methods based on MCDA methodology. The most MCCs are based on either outranking or multi-attribute utility approaches. Roy and Moscarola [35], Masaglia and Ostanello [24], Yu [42], Perny [31], Belacel [3] and Henriet [15] have proposed MCCs based on the outranking approach, while M.H.DIS (Multi-group Hierarchical DIScrimination) method [40] and UTADIS (UTilités Additives DIScriminantes) method and its variants ([21], [39], [10]) are typical methods based on multi-attribute utility theory. This paper focuses essentially on outranking-based nominal MCCs where there is no order relationship between the categories. These MCCs are based on concordance/discordance concepts. Limitation of outranking-based methods is due to the large number of parameters (e.g. discrimination thresholds, weights, reference alternatives, etc.) required. In MCDA context, these parameters are generally elicited using interactive approaches from the decision-maker to articulate his relational preference system: it’s the Direct Elicitation Approach (DEA). However, it is difficult for the decision-maker to provide such information in a coherent way when the number of these parameters is considerable. Indirect Elicitation Approach (IEA) or Automatic Learning Methods (ALMs) might be the solution to elicit automatically the values of these parameters based on a training set of pre-assigned examples. These two elicitation approaches will be discussed in Section 3. This paper makes three main contributions. First, we propose a generalized framework for Nominal Concordance/Discordance-based MCC (NCD-based MCCs). Second, we develop an ALM based on RealCoded Genetic Algorithm (RCGA) to estimate the parameters of NCD-based MCCs. Then we illustrate and assess the performance of the proposed approach on selected NCD-based MCCs. Even if the purpose of the comparison might be seen limited, we present exper-	application lifecycle management;artificial intelligence;artificial neural network;coherence (physics);concordance (publishing);decision analysis;entity;fuzzy set;genetic algorithm;international ergonomics association;level of measurement;machine learning;network computing devices;neural network software;operations research;reed–solomon error correction;rough set;sorting;test set;utility	Khaled Jabeur;Adel Guitouni	2009	J. Adv. Inf. Fusion		mathematics;machine learning;computer vision;artificial intelligence	AI	0.1108027763553986	-29.140911293487566	154182
289fc0323b609803bdfa5826d66232dd861a04c0	strongly typed evolutionary programming	evolutionary programming;steps computer software artificial intelligence;escher language;genetic	As the potential of applying machine learning techniques to perplexing problems is realised, increasingly complex problems are being tackled, requiring intricate explanations to be induced. Escher is a functional logic language whose higher-order constructs allow arbitrarily complex observations to be captured and highly expressive generalisations to be conveyed. The work presented in this thesis alleviates the challenging problem of identifying an underlying structure normally required to search the resulting hypothesis space e ciently. This is achieved through STEPS, an evolutionary based system that allows the vast space of highly expressive Escher programs to be explored. STEPS provides a natural upgrade of the evolution of concept descriptions to the higher-order level. In particular STEPS uses the individual-as-terms approach to knowledge representation where all the information provided by an example is localised as a single closed term so that examples of arbitrary complexity can be treated in a uniform manner. STEPS also supports -abstractions as arguments to higher-order functions thus enabling the invention of new functions not contained in the original alphabet. Finally, STEPS provides a number of specialised genetic operators for the design of speci c concept learning strategies. STEPS has been successfully applied to a number of complex real world problems, including the international PTE2 challenge. This problem involves the prediction of the Carcinogenic activity of a test set of 30 chemical compounds. The results produced by STEPS rank joint second if the hypothesis must be interpretable and joint rst if interpretability is sacri ced for increased accuracy.	challenge–response authentication;concept learning;escher;evolutionary programming;genetic operator;higher-order function;knowledge representation and reasoning;machine learning;strong and weak typing;test set	Claire Julia Kennedy	2000			evolutionary programming;genetic programming;evolutionary music;programming domain;computer science;artificial intelligence;extensible programming;machine learning;functional logic programming;genetic representation;programming paradigm;symbolic programming;inductive programming;fifth-generation programming language;programming language theory;programming language	AI	8.553813758229815	-26.630107844980994	154404
f6c44a44f5ee4fa3faefbc5120f0a113c5a5dfc0	use of fuzzy sets in semantic nets for providing on-line assistance to user of technological systems		"""The main objective of this paper is to develop a new semantic Network structure, based on the fuzzy sets theory, used in Artificial Intelligent system in order to provide effective on-line assistance to users of new technological systems. This Semantic Networks is used to describe the knowledge of an """"ideal"""" expert while fuzzy sets are used both to describe the approximate and uncertain knowledge of novice users who intervene to match fuzzy labels of a query with categories from an """"ideal"""" expert. The technical system we consider is a word processor software, with Objects such as “Word” and Goals such as “Cut” or “Copy”. We suggest to consider the set of the system's Goals as a set of linguistic variables to which corresponds a set of possible linguistic values based on the fuzzy set. We consider, therefore, a set of interpretation’s levels for these possible values to which corresponds a set of membership functions. We also propose a method to measure the similarity degree between different fuzzy linguistic variables for the partition of the semantic network in class of similar objects to make easy the diagnosis of the user’s fuzzy queries."""	approximation algorithm;artificial intelligence;database;fuzzy set;interpretation (logic);membership function (mathematics);online and offline;semantic network	Mohamed Nazih Omri;Mohamed Ali Mahjoub	2007	CoRR		fuzzy logic;semantic similarity;fuzzy cognitive map;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;artificial intelligence;fuzzy number;theoretical computer science;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy set;fuzzy set operations;fuzzy control language	AI	0.7020106529380054	-24.711389870757444	154473
eff7b3c4d90abc177e21da9792c3a0f963012e06	refined time stamps for concept drift detection during mining for classification rules	concept drift;decision tree;classification rules	In many application areas where databases are mined for classification rules, the latter may be subject to concept drift, that is change over time. Mining without taking this into account can result in severe degradation of the acquired classifier's performance. This is especially the case when mining is conducted incrementally to maintain knowledge used by an on-line system. The TSAR methodology detects and copes with drift in such situations through the use of a time stamp attribute, applied to incoming batches of data, as an integral part of the mining process. Here we extend the use of TSAR by employing more refined time stamps: first to individual batches, then to individual examples within a batch. We develop two new decision tree based TSAR algorithms, CD4 and CD5 and compare these to our original TSAR algorithm CD3.	concept drift	Ray J. Hickey;Michaela M. Black	2000		10.1007/3-540-45244-3_3	engineering;artificial intelligence;data mining;operations research	ML	0.13812773552130264	-35.03745411926828	154539
09f383cf846e78769f28ecbfd80d6a280ceb6ac0	using fuzzy dependency-guided attribute grouping in feature selection	high dimensionality;logique floue;logica difusa;intelligence artificielle;classification;fuzzy logic;attribute reduction;machine learning;artificial intelligence;feature selection;inteligencia artificial;rough set;ensemble approximatif;clasificacion	Feature selection has become a vital step in many machine learning techniques due to their inability to handle high dimensional descriptions of input features. This paper demonstrates the applicability of fuzzy-rough attribute reduction and fuzzy dependencies to the problem of learning classifiers, resulting in simpler rules with little loss in classification accuracy.	feature selection;machine learning;xslt/muenchian grouping	Richard Jensen;Qiang Shen	2003		10.1007/3-540-39205-X_32	fuzzy logic;rough set;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;data mining;feature selection	ML	8.505317939225097	-32.78768169777435	154625
2d86679597106476537336bd6d4817caef2d21b4	predicting going concern opinion with data mining	tratamiento datos;extraction information;sistema experto;analisis estadistico;base de connaissances;analisis datos;information extraction;echantillonnage;rule based;hd28 management industrial management;accounting;data processing;traitement donnee;clasificador;comptabilite;data mining;logistic regression;classification;statistical regression;consultation;consulta;sampling;inference rule;classification a vaste marge;data analysis;regresion logistica;classifier;statistical analysis;fouille donnee;table decision;regresion estadistica;regression logistique;analyse statistique;tabla decision;classificateur;base conocimiento;analyse donnee;contabilidad;audicion;statistical techniques;support vector machine;systeme expert;maquina ejemplo soporte;vector support machine;muestreo;regression statistique;busca dato;audit;extraccion informacion;decision table;going concern opinion;knowledge base;expert system	The auditor is required to evaluate whether substantial doubt exists about the client entity's ability to continue as a going concern. Accounting debacles in recent years have shown the importance of proper and thorough audit analysis. Since the 80s, many studies have applied statistical techniques, mainly logistic regression, as an automated tool to guide the going concern opinion formulation. In this paper, we introduce more advanced data mining techniques, such as support vector machines and rulebased classifiers, and empirically investigate the ongoing discussion concerning the sampling methodology. To provide specific audit guidelines, we infer rules with the state-of-the-art classification technique AntMiner+, which are subsequently converted into a decision table allowing for truly easy and user-friendly consultation in every day audit business practices. © 2008 Elsevier B.V. All rights reserved.	data mining;decision table;logistic regression;naive bayes classifier;sampling (signal processing);support vector machine;usability	David Martens;Liesbeth Bruynseels;Bart Baesens;Marleen Willekens;Jan Vanthienen	2008	Decision Support Systems	10.1016/j.dss.2008.01.003	decision table;support vector machine;sampling;knowledge base;classifier;data processing;biological classification;computer science;artificial intelligence;data science;data mining;logistic regression;data analysis;audit;expert system;information extraction;regression analysis;statistics;rule of inference	AI	8.370857533313714	-33.80524621010168	154697
489441e423ddd203a8b3fa0e61c94809f8429541	adaptive sampling methods for scaling up knowledge discovery algorithms	sample size;tecnologia electronica telecomunicaciones;random sampling;concentration bounds;data mining;scaling up;data mining application;data mining algorithm;adaptive sampling;scalability;tecnologias;grupo a;sequential sampling;knowledge discovery	Scalability is a key requirement for any KDD and data mining algorithm, and one of the biggest research challenges is to develop methods that allow to use large amounts of data. One possible approach for dealing with huge amounts of data is to take a random sample and do data mining on it, since for many data mining applications approximate answers are acceptable. However, as argued by several researchers, random sampling is difficult to use due to the difficulty of determining an appropriate sample size. In this paper, we take a sequential sampling approach for solving this difficulty, and propose an adaptive sampling method that solves a general problem covering many actual problems arising in applications of discovery science. An algorithm following this method obtains examples sequentially in an on-line fashion, and it determines from the obtained examples whether it has already seen a large enough number of examples. Thus, sample size is not fixed a priori; instead, it adaptively depends on the situation. Due to this adaptiveness, if we are not in a worst case situation as fortunately happens in many practical applications, then we can solve the problem with a number of examples much smaller than required in the worst case. We prove the correctness of our method and estimates its efficiency theoretically. For illustrating its usefulness, we consider one concrete task requiring sampling, provide an algorithm based on our method, and show its efficiency experimentally.		Carlos Domingo;Ricard Gavaldà;Osamu Watanabe	2002	Data Mining and Knowledge Discovery	10.1023/A:1014091514039	sample size determination;sampling;scalability;computer science;sequential analysis;machine learning;data mining;mathematics;knowledge extraction;data stream mining;statistics	ML	-1.935731273008248	-36.36365467947745	154946
0eea56e1feac480ff0856a21473bfa3af46347f5	optimal fuzzy model construction with statistical information using genetic algorithm		Fuzzy rule based models have a capability to approximate any continuous function to any degree of accuracy on a compact domain. The majority of FLC design process relies on heuristic knowledge of experience operators. In order to make the design process automatic we present a genetic approach to learn fuzzy rules as well as membership function parameters. Moreover, several statistical information criteria such as the Akaike information criterion (AIC), the Bhansali-Downham information criterion (BDIC), and the Schwarz-Rissanen information criterion (SRIC) are used to construct optimal fuzzy models by reducing fuzzy rules. A genetic scheme is used to design Takagi-Sugeno-Kang (TSK) model for identification of the antecedent rule parameters and the identification of the consequent parameters. Computer simulations are presented confirming the performance of the constructed fuzzy logic controller.	akaike information criterion;approximation algorithm;computer simulation;fitness function;fuzzy control system;fuzzy logic;fuzzy rule;genetic algorithm;heuristic;mathematical optimization;mean squared error;optimality criterion;system identification	Md. Amjad Hossain;Pintu Chandra Shill;Bishnu Sarker;Kazuyuki Murase	2011	CoRR	10.5121/ijcsit.2011.3619	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;fuzzy associative matrix;fuzzy set operations	AI	5.738837681312415	-26.709954561671093	155034
3fd630fa756da6a6540126e90f645a440aa7b6d8	predicting rare classes: comparing two-phase rule induction to cost-sensitive boosting	regle inference;rule induction;model generation;intelligence artificielle;aprendizaje probabilidades;data mining;classification;inference rule;learning scenario;rare event;fouille donnee;apprentissage probabilites;artificial intelligence;inteligencia artificial;false positive;busca dato;clasificacion;probability learning;regla inferencia	Learning good classifier models of rare events is a challenging task. On such problems, the recently proposed two-phase rule induction algorithm, PNrule, outperforms other non-meta methods of rule induction. Boosting is a strong meta-classifier approach, and has been shown to be adaptable to skewed class distributions. PNrule’s key feature is to identify the relevant false positives and to collectively remove them. In this paper, we qualitatively argue that this ability is not guaranteed by the boosting methodology. We simulate learning scenarios of varying difficulty to demonstrate that this fundamental qualitative difference in the two mechanisms results in existence of many scenarios in which PNrule achieves comparable or significantly better performance than AdaCost, a strong cost-sensitive boosting algorithm. Even a comparable performance by PNrule is desirable because it yields a more easily interpretable model over an ensemble of models generated by boosting. We also show similar supporting results on real-world and benchmark datasets.	algorithm;benchmark (computing);boosting (machine learning);rare events;rule induction;simulation;two-phase commit protocol	Mahesh V. Joshi;Ramesh C. Agarwal;Vipin Kumar	2002		10.1007/3-540-45681-3_20	type i and type ii errors;biological classification;computer science;artificial intelligence;machine learning;data mining;gradient boosting;boosting;statistics;rule of inference	ML	8.85330040722786	-32.679475286018096	155335
0a29a126bb1ea7c3640338ef23fe3b0917d2081d	introducing a classification model based on svm for network intrusion detection		Intrusion Detection Systems are designed to provide security into computer networks. In this article, we used rough sets theory for feature selection to enhance support vector machine in intrusion detection. Testing and evaluation of the proposed method has been performed mainly on NSL-KDD data sets as a corrected version of KDD-CUP99. Experimental results indicate that the proposed method shows a good performance in providing high precision, intrusion detection readout, less error notification rate and more detailed detection compared to its basic and simpler methods.		Ghodratolah Dastfal;Samad Nejatian;Hamid Parvin;Vahideh Rezaie	2017		10.1007/978-3-030-02837-4_5	support vector machine;artificial intelligence;feature selection;computer science;intrusion detection system;pattern recognition;data set;rough set	ML	6.896951026799927	-37.41228291937422	155363
90abe13a4aa81575aec1e5e452c6179c4a2d4c2d	fuzzy systems modeling in practice	modelizacion;learning algorithm;fuzzy system models;learning;modelisation floue;logique floue;logica difusa;ensemble apprentissage;algorithme apprentissage;generalization ability;linear functionals;algorithme;aprendizaje;fuzzy logic;fuzzy modeling;modelisation;algorithm;training set;apprentissage;fuzzy algorithm;capacite generalisation;algorithme flou;sistema difuso;systeme flou;algoritmo aprendizaje;modeling;fuzzy systems;fuzzy system;fuzzy model;algoritmo	Instead of describing a fuzzy modeling algorithm that is new, powerful, robust, and with outstanding learning abilities, the objective of this paper is to point out four important topics usually “ignored” in fuzzy model’s design. These are: 1. The generalization ability of the fuzzy model. 2. The appearance of empty rules at a fuzzy model whose conclusions could not be extracted. 3. The presence of noise as source of ambiguity to the fuzzy model. 4. The in4uence of training set size on learning performance. These topics are analyzed and discussed by modeling a linear functional relation using a basic learning algorithm. These conditions allow a better understanding, visualization, and separation of the causes a5ecting the fuzzy models performance when using this learning algorithm. Results show that it is important to understand what information can be obtained from a previous analysis of the training data which can help to design reasonable and e7cient fuzzy models to work in practical environments. c © 2001 Elsevier Science B.V. All rights reserved.	algorithm;cluster analysis;data pre-processing;fuzzy concept;fuzzy control system;fuzzy set;mask (computing);preprocessor;sparse language;sparse matrix;systems modeling;test set	Paulo J. Costa Branco;J. A. Dente	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(99)00173-6	fuzzy logic;training set;systems modeling;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;information fuzzy networks;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	AI	9.230024039862336	-30.655172026422107	155383
5f953523e4f541bf3c25f22fe9cf0cad4ed8cf15	fading histograms in detecting distribution and concept changes		The remarkable number of real applications under dynamic scenarios is driving a novel ability to generate and gather information. Nowadays, a massive amount of information is generated at a high-speed rate, known as data streams. Moreover, data are collected under evolving environments. Due to memory restrictions, data must be promptly processed and discarded immediately. Therefore, dealing with evolving data streams raises two main questions: (i) how to remember discarded data? and (ii) how to forget outdated data? To maintain an updated representation of the time-evolving data, this paper proposes fading histograms. Regarding the dynamics of nature, changes in data are detected through a windowing scheme that compares data distributions computed by the fading histograms: the adaptive cumulative windows model (ACWM). The online monitoring of the distance between data distributions is evaluated using a dissimilarity measure based on the asymmetry of the Kullback–Leibler divergence. The experimental results support the ability of fading histograms in providing an updated representation of data. Such property works in favor of detecting distribution changes with smaller detection delay time when compared with standard histograms. With respect to the detection of concept changes, the ACWM is compared with 3 known algorithms taken from the literature, using artificial data and using public data sets, presenting better results. Furthermore, we the proposed method was extended for multidimensional and the experiments performed show the ability of the ACWM for detecting distribution changes in these settings.	algorithm;experiment;information privacy;kullback–leibler divergence;microsoft windows;sensor	Raquel Sebastião;João Gama;Teresa Mendonça	2017	International Journal of Data Science and Analytics	10.1007/s41060-017-0043-4	computer science;theoretical computer science;data mining;statistics	DB	-1.6651006314830479	-36.49787611077263	155495
80fe1abae2594a2cb5466d3646abbc57fb13d144	finding hierarchical heavy hitters in streaming data	online algorithm;approximate algorithm;approximation algorithms;data stream;exact solution;data mining;network data analysis;traffic monitoring;data structure	Data items that arrive online as streams typically have attributes which take values from one or more hierarchies (time and geographic location, source and destination IP addresses, etc.). Providing an aggregate view of such data is important for summarization, visualization, and analysis. We develop an aggregate view based on certain organized sets of large-valued regions (“heavy hitters”) corresponding to hierarchically discounted frequency counts. We formally define the notion of hierarchical heavy hitters (HHHs). We first consider computing (approximate) HHHs over a data stream drawn from a single hierarchical attribute. We formalize the problem and give deterministic algorithms to find them in a single pass over the input.  In order to analyze a wider range of realistic data streams (e.g., from IP traffic-monitoring applications), we generalize this problem to multiple dimensions. Here, the semantics of HHHs are more complex, since a “child” node can have multiple “parent” nodes. We present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees. The product of hierarchical dimensions forms a mathematical lattice structure. Our algorithms exploit this structure, and so are able to track approximate HHHs using only a small, fixed number of statistics per stored item, regardless of the number of dimensions.  We show experimentally, using real data, that our proposed algorithms yields outputs which are very similar (virtually identical, in many cases) to offline computations of the exact solutions, whereas straightforward heavy-hitters-based approaches give significantly inferior answer quality. Furthermore, the proposed algorithms result in an order of magnitude savings in data structure size while performing competitively.	aggregate data;approximation algorithm;computation;crystal structure;data structure;experiment;geographic coordinate system;online algorithm;online and offline;provable security;stream (computing)	Graham Cormode;Flip Korn;S. Muthukrishnan;Divesh Srivastava	2008	TKDD	10.1145/1324172.1324174	online algorithm;data structure;computer science;theoretical computer science;machine learning;data mining;statistics	DB	-2.1162707495765867	-36.14790629888687	155717
83c9a99fcbe5fa5b1266ddfb1741f617a50c3c09	a general measure of similarity for categorical sequences	lenguaje natural;reconocimiento lenguaje;tratamiento transaccion;raisonnement base sur cas;text;razonamiento fundado sobre caso;reconnaissance langage;analisis datos;competitividad;langage naturel;significant patterns;tipo dato;metric;texte;data type;data mining;similitude;chronological order;language recognition;search;data analysis;reconnaissance caractere;reconocimiento voz;data mining application;fouille donnee;pattern matching;natural language;matching;similarity;competitiveness;statistics;pattern recognition;speech recognition;analyse donnee;metrico;concordance forme;reconnaissance forme;reconnaissance parole;similitud;case based reasoning;reconocimiento patron;transaction processing;type donnee;texto;competitivite;similarity measure;article;character recognition;busca dato;algorithm design;domain specificity;metrique;categorical sequences;reconocimiento caracter;traitement transaction	Measuring the similarity between categorical sequences is a fundamental process in many data mining applications. A key issue is extracting and making use of significant features hidden behind the chronological and structural dependencies found in these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological order, but such sequences often have similar structural features in chronologically different order. In this paper we propose SCS, a novel, effective and domain-independent method for measuring the similarity between categorical sequences, based on an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS captures significant patterns that represent the natural structure of sequences, and reduces the influence of those which are merely noise. It constitutes an effective approach to measuring the similarity between data in the form of categorical sequences, such as biological sequences, natural language texts, speech recognition data, certain types of network transactions, and retail transactions. To show its effectiveness, we have tested SCS extensively on a range of data sets from different application fields, and compared the results with those obtained by various mainstream algorithms. The results obtained show that SCS produces results that are often competitive with domain-specific similarity approaches.	algorithm;data mining;domain-specific language;natural language;pattern matching;speech recognition	Abdellali Kelil;Shengrui Wang;Qingshan Jiang;Ryszard Brzezinski	2009	Knowledge and Information Systems	10.1007/s10115-009-0237-8	matching;algorithm design;case-based reasoning;chronology;speech recognition;similarity;transaction processing;metric;data type;computer science;artificial intelligence;similitude;pattern matching;data mining;database;mathematics;natural language;data analysis;programming language	ML	-3.51633000321863	-33.09137915514784	156007
d0e6de1a44d35e90b9b50187faaa71c5ecc0f5bb	improving spatial intersect joins using symbolic intersect detection	spatial information system;evaluation performance;base donnee;systeme information geographique;performance evaluation;geographic information system;spatial join;etude experimentale;information retrieval;evaluacion prestacion;information technology;polygone;database;base dato;technologie information;refinement;spatial join processing;interseccion;question processing;polygon;afinamiento;recherche information;traitement question;symbolic intersect detection;systeme information spatiale;poligono;affinement;recuperacion informacion;experimental evaluation;intersection;tecnologia informacion;estudio experimental;sistema informacion geografica	We introduce a novel technique to drastically reduce the computation required by the refinement step during spatial intersect join processing. This technique, called Symbolic Intersect Detection (SID), detects most of the true hits during a spatial intersect join by scrutinizing symbolic topological relationships between candidate polygon pairs. SID boosts performance by detecting true hits early during the refinement step, thus avoiding expensive polygon intersect computations that would otherwise be required to detect the true hits. Our experimental evaluation with real GIS map data demonstrates that SID can identify more than 80% of the true hits with only minimal overhead. Consequently, SID outperforms known techniques for resolving polygon intersection during the refinement step by more than 50%. Most state-of-the-art methods in spatial join processing can benefit from SID's performance gains because the SID approach integrates easily into the established two-phase spatial join process.		Yun-Wu Huang;Matthew C. Jones;Elke A. Rundensteiner	1997		10.1007/3-540-63238-7_29	computer science;polygon;data mining;database;geographic information system;programming language;information technology;algorithm	DB	-3.552231629693446	-33.90906046132796	156306
c8c5ac5af7271dc9b9ce7dd9742296f0243ee469	finding the capacity of fuzzy neural networks (fnns) via its equivalent fully connected neural networks (ffnns)	fuzzy neural networks fuzzy control training upper bound systematics input variables pragmatics;pragmatics;fuzzy neural network;fuzzy neural nets;neural networks;systematics;lower bounds;input variables;fuzzy neural nets backpropagation fuzzy logic;fuzzy control;upper bounds;training;backpropagation;capacity of neural networks;capacity of neural networks neural networks fuzzy logic fuzzy neural networks back propagations universal approximation theorem;fuzzy logic;ffnn;upper bound;engineering applications;engineering applications fuzzy neural networks fully connected neural networks ffnn lower bounds upper bounds theoretical bounds capacity matching;universal approximation theorem;proceedings paper;capacity matching;theoretical bounds;back propagations;fuzzy neural networks;back propagation;fully connected neural networks;neural network	The capacity of Fuzzy Neural Network (FNN) is explored in this paper. The FNN is first transformed into an equivalent fully connected three layer neural network, or FFNN, via a new approach proposed in this paper. Then the lower and upper bounds of FNN can be found. To check the validity of the theoretical bounds, an example is illustrated with the trainings to yield excellent capacity matching with the theoretical bounds. This new finding has its emerging values in all engineering applications using FNN, such as intelligent adaptive control, pattern recognition, and signal processing, …, etc.	multitier architecture;neural networks;neuro-fuzzy;pattern recognition;signal processing;vii	Jing Wang;Chi-Hsu Wang;C. L. Philip Chen	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007473	computer science;artificial intelligence;backpropagation;machine learning;artificial neural network;algorithm	Robotics	5.929846673353106	-27.60089139901616	156654
38b92a439d417713a262e5cffa738a7a43d75c1c	comparing approaches to prepare data in classification problems	directed graphs;computers;data preparation;data mining;artificial neural networks;pattern classification data mining data preparation directed graphs numerical analysis;numerical analysis;data mining application;machine learning;directed graph;data mining algorithm;classification problem dmpml weka rapidminer kn ime directed graph approach data preparation task execution data mining algorithm data set numerical test categorical test mixed test;xml;pattern classification;ieee xplore;data mining computers xml machine learning artificial neural networks conferences;conferences;artificial neural network	This paper presents a comparison between DMPML and three data mining applications (Weka, RapidMiner, and KN-IME) that implement the directed graph approach, concerning the time spent to create and execute the data preparation tasks for two data mining algorithms. The tests were executed using different types of data sets: numerical, categorical, and mixed. We observed that the scheme used by the DMPML framework can simplify the usage of different data mining algorithms and reduce the time spent creating the data preparation tasks.	algorithm;categorical variable;data mining;directed graph;input method;numerical analysis;rapidminer;weka	Paulo Mauricio Gonçalves;Roberto Souto Maior de Barros	2011	2011 9th IEEE/ACS International Conference on Computer Systems and Applications (AICCSA)	10.1109/AICCSA.2011.6126613	directed graph;computer science;machine learning;data mining;database;data stream mining;artificial neural network	HPC	1.7191990656296898	-35.30179856743995	156722
8234c4e731918c607462b62034bef0e7b7090551	incremental approaches to knowledge reduction of covering decision information systems with variations of coverings		In practical situations, calculating approximations of co n epts is the central step for knowledge reduction of dynamic covering decision infor mation system, which has received growing interests of researchers in recent years. In this paper, the second and sixth lower and upper approximations of sets in dynamic covering i nformation systems with variations of coverings are computed from the perspective of matr ix using incremental approaches. Especially, effective algorithms are designed for calculating the second a nd sixth lower and upper approximations of sets in dynamic covering informati on systems with the immigration of coverings. Experimental results demonstrate that t e designed algorithms provide an efficient and effective method for constructing the second and sixth lower an d upper approximations of sets in dynamic covering information syste ms. Two examples are explored to illustrate the process of knowledge reduction of dynamic covering decision information systems with the covering immigration.	algorithm;approximation;effective method;information system	Guangming Lang	2015	CoRR		combinatorics;discrete mathematics;mathematics	EDA	-2.8039208899786923	-25.221323558661943	156946
68f93319b71e44ecc2715f1efc45957ded7f5e85	practical viability of multiple imputation as a tool for disclosure protection for large scale recurring surveys			geo-imputation	Pat Doyle	2004			imputation (statistics);econometrics;data mining;computer science	EDA	1.983791778777953	-34.82853952284779	157643
2c0005665472b687cca2d1b527ad3b0254e905c0	predicting defect-prone software modules using support vector machines	software metrics;quality assurance;support vector machines;defect prone modules;comparative modeling;machine learning;software development;software metric;predictive models;prediction model;support vector machine	Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models. 2007 Elsevier Inc. All rights reserved.	machine learning;modular programming;software bug;software developer;support vector machine	Karim O. Elish;Mahmoud O. Elish	2008	Journal of Systems and Software	10.1016/j.jss.2007.07.040	quality assurance;support vector machine;verification and validation;software sizing;computer science;data science;machine learning;software construction;data mining;predictive modelling;relevance vector machine;structured support vector machine;software metric	AI	3.735265224698774	-33.66349026159282	157803
0607e4d4157b243d11a785e90f16897de66a4245	pattern recognition method based on the attribute computing network	computers;learning algorithm;pattern recognition character recognition computers handwriting recognition training conferences learning systems;attribute computing network;handwriting recognition;common sense reasoning;training;pattern recognition common sense reasoning learning artificial intelligence;learning algorithm pattern recognition attribute computing network qualitative mapping feedback adjustment qualitative criterion;computer network;learning systems;pattern recognition;qualitative mapping;qualitative criterion;learning artificial intelligence;character recognition;feedback adjustment;conferences	An attribute computing network induced by qualitative mapping is presented in this paper and the feedback adjustment mechanism for qualitative criterion and the learning algorithm are given. After that, the pattern recognition method based on the attribute computing network is brought forward. An actual application using such method is given in the end.	pattern recognition	Jiali Feng;Guanglin Xu;Xiaofeng Wang	2008		10.1109/GRC.2008.4664628	commonsense reasoning;attribute domain;computer science;artificial intelligence;machine learning;pattern recognition;handwriting recognition	Vision	6.744775578220719	-28.354520501029345	157970
6bfd2802a712294a2548b756534f6fd0aa9bd217	extracting failure time data from industrial maintenance records using text mining	text mining;naive bayes;work orders analysis;support vector machine	Reliability modelling requires accurate failure time of an asset. In real industrial cases, such data are often buried in different historical databases which were set up for purposes other than reliability modelling. In particular, two data sets are commonly available: work orders (WOs), which detail maintenance activities on the asset, and downtime data (DD), which details when the asset was taken offline. Each is incomplete from a failure perspective, where one wishes to know whether each downtime event was due to failure or scheduled activities.#R##N##R##N#In this paper, a text mining approach is proposed to extract accurate failure time data from WOs and DD. A keyword dictionary is constructed using WO text descriptions and classifiers are constructed and applied to attribute each of the DD events to one of two classes: failure or nonfailure. The proposed method thus identifies downtime events whose descriptions are consistent with urgent unplanned WOs. The applicability of the methodology is demonstrated on maintenance data sets from an Australian electricity and sugar processing companies. Analysis of the text of the identified failure events seems to confirm the accurate identification of failures in DD. The results are expected to be immediately useful in improving the estimation of failure times (and thus the reliability models) for real-world assets.	text mining	Kazi Arif-Uz-Zaman;Michael E. Cholette;Lin Ma;Azharul Karim	2017	Advanced Engineering Informatics	10.1016/j.aei.2016.11.004	support vector machine;text mining;naive bayes classifier;computer science;engineering;artificial intelligence;data science;machine learning;data mining;database;statistics	DB	0.18402417254194	-34.92316596721766	158317
ed9e3575eb8804cb3559e1b5b9da446f32fa818b	on the sensitivity of weighted general mean based type-2 fuzzy signatures	fuzzy signatures;weighted general mean;sensitivity;aggregation operators;fuzzy model	Fuzzy signatures offer a possible way of describing, modeling and analysing of complex systems, when the exact mathematical model is not known or too difficult to handle. In these cases the input values have uncertainties, due to lack of knowledge or human activities. These uncertainties have influence on the final decision about the system. The uncertainties are taken into consideration as fuzzy sets, for example representing the uncertainty of a linguistic variable. In this paper we discuss the input sensitivity of type-2 weighted general mean aggregation operator and fuzzy signatures which are equipped with general means as aggregation operators.	electronic signature;fuzzy logic	István Á. Harmati;László T. Kóczy	2016		10.1007/978-3-319-39378-0_19	ordered weighted averaging aggregation operator;mathematical optimization;discrete mathematics;sensitivity;machine learning	Crypto	0.9135953285944772	-25.112601547534375	158494
9edd69ecfa4f3cff20af9c584918cc186f35c543	similarity model based on cbr and fca	libraries;analytical models;lattices;computational modeling;concept lattice;cognition;case based reasoning;context;formal concept analysis;case library	Case-based reasoning (CBR) is one of the research highlights in the artificial intelligence field. In the process of case retrieval of CBR, Similarity is an important index in evaluation. This paper proposed a new model calculating similarity between source case and target case. The model is suitable for using Formal Concept Analysis (FCA) in the case of CBR. The model considered the case attributes weights between two formal concepts and feature attributes weights in concept lattices. Comparing to the similarity model put forward by Jirapond Tadrat, this model cut down the comparison to the objects, weight more in the attributes and shorten the time of solving the similarity. Theoretical deduction proves that the proposed similarity model satisfy the basic conditions which all these models need to meet. This article chose the UCI data sets and the method of cross validation, carried on an experiment from both similarity model aspect and classifier aspect respectively. The former experimental results show that the similarity model has higher accuracy than others. The latter experimental results show that the similarity model of CBR classifier has higher accuracy in the attribute set density compared with other small data set classifier.	artificial intelligence;case-based reasoning;cross-validation (statistics);formal concept analysis;natural deduction	Chongyang Shi;Linjing Lai;Jing Fan;Yu Bai	2016	2016 17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD)	10.1109/SNPD.2016.7515965	case-based reasoning;cognition;computer science;formal concept analysis;artificial intelligence;machine learning;lattice;data mining;database;computational model;algorithm;similarity heuristic	SE	-4.066790306563982	-25.42447129180515	158527
ae0267a002a4c61cbf6c56a69a4002d5f03fca71	sirms connected fuzzy inference model with compatibility functions	silicon;bismuth;fuzzy control;gravity;fuzzy sets;fuzzy logic;approximate reasoning;computational modeling;compatibility function;fuzzy function;sirms connected fuzzy inference model	The single input rule modules connected fuzzy inference model (SIRMs model) can decrease the number of fuzzy rules drastically in comparison with the conventional fuzzy inference models. However, the inference results obtained by the SIRMs model is generally simple comapred with the conventional fuzzy inference models. For example, the SIRMs model can not transform to the product-sum-gravity model, if the fuzzy sets of the antecedent parts are limited to normal fuzzy sets. In this paper, we propose a SIRMs model with compatibility functions, which weights the rules of the SIRMs model. Moreover, this paper shows that the inference results of the proposed model can be easily obtained even as the proposed model uses involved compatibility functions.	fuzzy logic;fuzzy set;gravity model of trade	Hirosato Seki	2014	2014 IEEE International Conference on Granular Computing (GrC)	10.1109/GRC.2014.6982843	fuzzy logic;discrete mathematics;gravity;defuzzification;adaptive neuro fuzzy inference system;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;bismuth;data mining;mathematics;fuzzy set;silicon;computational model;fuzzy set operations;fuzzy control system	Vision	4.8579652411658785	-27.23759815007259	158757
2d118c518308bec8496c07055735d5d7dac82d5d	phishing emails detection using cs-svm		Phishing attacks are common online, which have resulted in financial losses through using either malware or social engineering. Thus, phishing email detection with high accuracy has been an issue of great interest. Machine learning-based detection methods, particularly Support Vector Machine (SVM), have been proved to be effective. However, the parameters of kernel method, whose default is that class numbers reciprocals in general, affect the classification accuracy of SVM. In order to improve the classification accuracy, this paper proposes a model, called Cuckoo Search SVM(CS-SVM). The CS-SVM extracts 23 features, which are used to construct the hybrid classifier. In the hybrid classifier, Cuckoo Search (CS) is integrated with SVM to optimize parameter selection of Radial Basis Function(RBF). Experiments are performed on a dataset consisting of 1,384 phishing emails and 20,071 non-phishing emails. Experimental results show that the proposed method has higher phishing email detection accuracy than SVM classifier with default parameter value. The CS-SVM classifier can obtain the highest accuracy of 99.52 percent.	archive;cs games;cs-blast;cs-cipher;computer security;cuckoo search;email;experiment;fitness function;kernel method;machine learning;malware;mapreduce;mathematical optimization;phishing;radial (radio);radial basis function;radial basis function kernel;search algorithm;social engineering (security);support vector machine;test set	Wei-na Niu;Xiaosong Zhang;Guowu Yang;Zhiyuan Ma;Zhongliu Zhuo	2017	2017 IEEE International Symposium on Parallel and Distributed Processing with Applications and 2017 IEEE International Conference on Ubiquitous Computing and Communications (ISPA/IUCC)	10.1109/ISPA/IUCC.2017.00160	human–computer interaction;support vector machine;kernel (linear algebra);kernel method;phishing;malware;feature extraction;cuckoo search;computer science;artificial intelligence;pattern recognition	Robotics	6.72217380840969	-37.911275940546936	159221
91d31f5b6f617659b548232f0d5700aae1af749e	kernel logistic pls: a tool for supervised nonlinear dimensionality reduction and binary classification	kernels;kernel;learning;60j20;analisis datos;latent variable;methode noyau;implementation;pls regression;segmentation;logistic regression;classification;statistical regression;62jxx;algorithme;aprendizaje;algorithm;data analysis;regresion logistica;apprentissage;dimensionality reduction;62h30;nonlinear dimensionality reduction;noyau mathematiques;visual inspection;regresion estadistica;regression logistique;metodo nucleo;high dimensional data;statistical computation;calculo estadistico;donnee binaire;analyse donnee;kernel method;calcul statistique;binary classification;dato binario;binary data;support vector machine;implementacion;regression statistique;dimensional reduction;clasificacion;segmentacion;algoritmo	Kernel logistic PLS” (KL-PLS) is a new tool for supervised nonlinear dimensionality reduction and binary classification. The principles of KL-PLS are based on both PLS latent variables construction and learning with kernels. The KL-PLS algorithm can be seen as a supervised dimensionality reduction (complexity control step) followed by a classification based on logistic regression. The algorithm is applied to 11 benchmark data sets for binary classification and to three medical problems. In all cases, KL-PLS proved its competitiveness with other state-of-the-art classification methods such as support vector machines. Moreover, due to successions of regressions and logistic regressions carried out on only a small number of uncorrelated variables, KL-PLS allows handling high-dimensional data. The proposed approach is simple and easy to implement. It provides an efficient complexity control by dimensionality reduction and allows the visual inspection of data segmentation. © 2007 Elsevier B.V. All rights reserved.	algorithm;benchmark (computing);binary classification;kl-one;kernel (operating system);latent variable;logistic regression;nonlinear dimensionality reduction;nonlinear system;support vector machine;visual inspection	Arthur Tenenhaus;Alain Giron;Emmanuel Viennet;Michel Béra;Gilbert Saporta;Bernard Fertil	2007	Computational Statistics & Data Analysis	10.1016/j.csda.2007.01.004	latent variable;binary classification;support vector machine;kernel method;kernel;biological classification;multifactor dimensionality reduction;machine learning;pattern recognition;mathematics;nonlinear dimensionality reduction;logistic regression;logistic model tree;data analysis;implementation;segmentation;regression analysis;statistics;dimensionality reduction;clustering high-dimensional data;visual inspection	ML	9.273589447470924	-33.71681310500769	159598
ce8bc35ce62c245c662ad2683f901e43b40b79b2	randomized k-d tree relieff algorithm for feature selection in handling high dimensional process parameter data	approximation algorithms;industries;vegetation;monitoring;information filters;algorithm design and analysis	In complex manufacturing processes, large amounts of process parameters are monitored and recorded, creating a high-dimensional and heterogonous data warehouse. In order to improve process yield and ensure product quality, comprehensive knowledge of the process should be acquired and critical features should be identified. However, in modern industry production, big data has become quite common; online monitoring and prediction is also usually required. Therefore, an effective feature selection algorithm for industry applications should be fast and robust. Traditional feature selection methods often fails to deal with such demands very well. In this paper, a modified approach for feature selection based on ReliefF is proposed for modelling and analysis of complex manufacturing processes, with improved speed and stability. Randomized k-d tree search is introduced to speed up the feature selection algorithm. The proposed method is also tested with two datasets from real industry process.	big data;computation;feature selection;nearest neighbor search;offline learning;online and offline;performance;randomized algorithm;selection algorithm;stream (computing);streaming media;time complexity	Sitong Xu;Xiang Li;Wen Feng Lu	2016	2016 IEEE 21st International Conference on Emerging Technologies and Factory Automation (ETFA)	10.1109/ETFA.2016.7733508	algorithm design;computer science;data science;machine learning;data mining;approximation algorithm;vegetation;statistics	Robotics	-2.7465922267785334	-36.32896935985757	159874
88a0af14752fd8307869b4cf05b6931787f0d71d	mining simplified fuzzy if-then rules for pattern classification	fuzzy set;fuzzy data;fuzzy rules;data mining;discriminant function;fuzzy sets;fuzzy rule base;function approximation;simplified fuzzy rules;classification system;pattern classification;fuzzy if then rules;genetic algorithm;computer simulation	A fuzzy if-then rule whose consequent part is a real number is referred to as a simplified fuzzy rule. Simplified fuzzy if-then rules have been widely used in function approximation problems due to no complicated defuzzification is required. The proposed simplified fuzzy rule-based classification system, whose number of output is equal to the number of different classes, approximates an unknown mapping from input to desired output for each discriminant function. Not only a fuzzy data mining method is proposed to find simplified fuzzy if-then rules from training data, but also the genetic algorithm is employed to determine some user-specified parameters. To evaluate the classification performance of the proposed method, computer simulations are performed on some well-known datasets, showing that the generalization ability of the proposed method is comparable to the other fuzzy or nonfuzzy methods.		Yi-Chung Hu;Fang-Mei Tseng	2009	International Journal of Information Technology and Decision Making	10.1142/S021962200900348X	computer simulation;fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	DB	4.536229986028762	-27.90909031849711	160165
9ff810d0ca232e28eb4f5134db4a76f8a1ea1ccf	a fuzzy evidential reasoning data fusion approach with uncertainty evaluation for robust pattern classification	structural model;shannon entropy;fuzzy reasoning;information sources;entropy pattern classification fuzzy reasoning sensor fusion case based reasoning uncertainty handling;image classification;evidence theory;uncertainty handling;data fusion;evidential reasoning;mr imaging;fuzzy reasoning uncertainty robustness pattern classification fuzzy sets entropy humans fuzzy logic image classification remote sensing;pattern classification;dempster shafer;image classification data fusion dempster shafer evidence theory fuzzy evidential reasoning;entropy;sensor fusion;case based reasoning;classification accuracy;human brain;decision rule;dempster shafer evidence theory fuzzy evidential reasoning data fusion uncertainty evaluation robust pattern classification fuzzy evidence structure model probabilistic evidence generalized dempster rule shannon entropy fuzzy entropy multimodality human brain mr images image classification	This paper presents a data fusion approach for pattern classification, based on the fuzzy evidential reasoning technique. First, a new fuzzy evidence structure model is introduced to formulate probabilistic evidence and fuzzy evidence in a unified manner. A generalized Dempster's rule is then used to combine fuzzy evidence structures associated with multiple information sources. Finally, an effective decision rule is developed to take into account uncertainty, quantified by Shannon entropy and fuzzy entropy, of probabilistic evidence and fuzzy evidence, to deal with conflict and to achieve robust decisions. To demonstrate the effectiveness of this approach, we apply it to classify multimodality human brain MR images in a supervised manner. The proposed approach outperforms both the traditional evidential reasoning technique and the fuzzy reasoning technique, in terms of robustness and classification accuracy.	entropy (information theory);shannon (unit);statistical classification;supervised learning	Hongwei Zhu;Otman A. Basir	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571240	defuzzification;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;sensor fusion;fuzzy associative matrix;evidential reasoning approach;fuzzy set operations;statistics	Robotics	-0.4481579029340551	-28.261392114381323	160275
829e33df05598c31b73ede3596ef2b64ba6061e2	structure learning of bayesian networks using dual genetic algorithm	structure learning;simulation ordinateur;evaluation performance;bayesian network;condition dependence;tecnologia electronica telecomunicaciones;dual chromosomes;genetic operator;operator;performance evaluation;learning;operador;relacion orden;evaluacion prestacion;ordering;algoritmo genetico;genetics;aprendizaje;reseau bayes;relation ordre;codificacion;apprentissage;entire solution;red bayes;coding;bayes network;algorithme genetique;operateur;algorithme evolutionniste;genetic algorithm;genetic algorithms;algoritmo evolucionista;simulacion computadora;evolutionary algorithm;tecnologias;grupo a;computer simulation;codage	A new structure learning approach for Bayesian networks (BNs) based on dual genetic algorithm (DGA) is proposed in this paper. An individual of the population is represented as a dual chromosome composed of two chromosomes. The first chromosome represents the ordering among the BN nodes and the second represents the conditional dependencies among the ordered BN nodes. It is rigorously shown that there is no BN structure that cannot be encoded by the proposed dual genetic encoding and the proposed encoding explores the entire solution space of the BN structures. In contrast with existing GA-based structure learning methods, the proposed method learns not only the topology of the BN nodes, but also the ordering among the BN nodes, thereby, exploring the wider solution space of a given problem than the existing method. The dual genetic operators are closed in the set of the admissible individuals. The proposed method is applied to real-world and benchmark applications, while its effectiveness is demonstrated through computer simulation. key words: Bayesian network, genetic algorithms, structure learning, dual chromosomes	bayesian network;benchmark (computing);computer simulation;conditional (computer programming);domain generation algorithm;feasible region;genetic algorithm;genetic operator;software release life cycle	Jaehun Lee;Wooyong Chung;Euntai Kim	2008	IEICE Transactions	10.1093/ietisy/e91-d.1.32	computer simulation;genetic algorithm;computer science;artificial intelligence;machine learning;evolutionary algorithm;bayesian network;mathematics;algorithm	AI	10.005549128101787	-30.991630844212096	160467
977427366cfa0dcb236ebab72eb04a69ee742303	an evolutionary algorithm to discover numeric association rules	evaluation function;association rules;data mining;frequent itemset;association rule;evolutionary algorithms;evolutionary algorithm	Association rules are one of the most used tools to discover relationships among attributes in a database. Nowadays, there are many efficient techniques to obtain these rules, although most of them require that the values of the attributes be discrete. To solve this problem, these techniques discretize the numeric attributes, but this implies a loss of information. In a general way, these techniques work in two phases: in the first one they try to find the sets of attributes that are, with a determined frequency, within the database (frequent itemsets), and in the second one, they extract the association rules departing from these sets. In this paper we present a technique to find the frequent itemsets in numeric databases without needing to discretize the attributes. We use an evolutionary algorithm to find the intervals of each attribute that conforms a frequent itemset. The evaluation function itself will be the one that decide the amplitude of these intervals. Finally, we evaluate the tool with synthetic and real databases to check the efficiency of our algorithm.	association rule learning;database;dhrystone;discrete mathematics;discretization;evaluation function;evolutionary algorithm	Jacinto Mata Vázquez;José Luis Álvarez Macías;José Cristóbal Riquelme Santos	2002		10.1145/508791.508905	association rule learning;computer science;artificial intelligence;machine learning;evolutionary algorithm;pattern recognition;data mining	DB	-2.0519010007775385	-34.76122008648715	160555
6285e230cbc3b5ace7cbbb7c1e9f1a919fc6ca37	pac analyses of a 'similarity learning' ibl algorithm	instance based learning;raisonnement base sur cas;razonamiento fundado sobre caso;learning algorithm;pac learning;intelligence artificielle;algorithme apprentissage;artificial intelligence;inteligencia artificial;case based reasoning;algoritmo aprendizaje;similarity measure	Abs t r ac t . VS-CBR [14] is a simple instance-based learning algorithm that adjusts a weighted similarity measure as well as collecting cases. This paper presents a 'PAC' analysis of VS-CBR, motivated by the PAC learning framework, which demonstrates two main ideas relevant to the study of instance-based learners. Firstly, the hypothesis spaces of a learner on different target concepts can be compared to predict the difficulty of the target concepts for the learner. Secondly, it is helpful to consider the 'constituent parts' of an instance-based learner: to explore separately how many examples are needed to infer a good similarity measure and how many examples are needed for the case base. Applying these approaches, we show that VS-CBR learns quickly if most of the variables in the representation are irrelevant to the target concept and more slowly if there are more relevant variables. The paper relates this overall behaviour to the behaviour of the constituent parts of VS-CBR.	algorithm;case-based reasoning;instance-based learning;probably approximately correct learning;relevance;similarity learning;similarity measure	Anthony D. Griffiths;Derek G. Bridge	1997		10.1007/3-540-63233-6_514	case-based reasoning;instance-based learning;computer science;artificial intelligence;machine learning;mathematics;probably approximately correct learning;algorithm	AI	8.289675348181557	-31.844749661986302	160639
40b9add8c1034f756e5c8e6a93a5f4325eaeb5f3	a tentative approach to minimal reducts by combining several algorithms	np hard problem;binary search;depth first search;information system;rough set;data structure	Finding minimal reducts is a NP-hard problem. For obtain a feasible solution, depth-first-searching is mainly used and a feasible reduct always can be gotten. Whether the feasible reduct is a minimal reduct or not and how far it is to minimal reduct, both are not known. It only gives the information that how many attributes it has and it is a reduct. Based on rough sets reduction theory and the data structure of information system, the least condition attributes to describe the system’s classified characteristics can be known. So an area of searching minimal reducts is decided. By binary search in the area, the minimal reducts can be gotten quickly and doubtlessly.	algorithm	Ning Xu;Yunxiang Liu;Ruqi Zhou	2008		10.1007/978-3-540-85930-7_16	discrete mathematics;rough set;data structure;breadth-first search;computer science;machine learning;pattern recognition;np-hard;mathematics;information system;algorithm;binary search algorithm	ML	-4.133445235520188	-36.46159380882297	160714
f18cd594fbf8c69e65a16ed3b7456936a7e15acd	f-parallel reducts in the information view	complexity theory;attribute significance;rough set theory;f attribute significance;f attribute significance rough sets parallel reducts dynamic reducts attribute significance;time efficiency f parallel reducts information view f attribute significance dynamic reducts;heuristic algorithms;parallel reducts;rough sets;entropy;information entropy;dynamic reducts;heuristic algorithms complexity theory entropy rough sets information entropy educational institutions	Parallel reducts in the information view and an algorithm with the matrix of attribute significance in the information view are introduced in this paper. Furthermore, F-attribute significance in the information view is defined, and its properties are investigated. We apply it to obtain parallel reducts, and an algorithm with F-attribute significance in the information view is proposed. Experiments show that our methods have advantages over dynamic reducts, and the new method not only improves the time efficiency but also decreases the length of parallel reducts.	algorithm;attribute-value system;heuristic;the matrix	Lin Chen;Dayong Deng;Chunping Wang	2012	2012 Fifth International Joint Conference on Computational Sciences and Optimization	10.1109/CSO.2012.134	rough set;computer science;machine learning;pattern recognition;data mining;mathematics	HPC	-3.953240077362447	-36.48699292299206	160729
7ec69cb07ac8944284dfbbd43c294940c5889d08	a linguistic fuzzy model with a monotone rule base is not always monotone	rule based;input output;agriculture and food sciences;center of gravity;membership function;fuzzy model	In this paper experiments are described with three linguistic fuzzy models sharing the same monotone rule base and the same membership functions for the two input variables, but applying different membership functions in the output domain. We investigated which inference methods result in a monotonic input-output behaviour. Apart from the conventional Mamdani–Assilian inference method with Center of Gravity (COG) defuzzification, three implicator-based inference methods combined with COG-like defuzzification are discussed.	defuzzification;experiment;rule-based system;monotone	Ester Van Broekhoven;Bernard De Baets	2005			mathematical optimization;discrete mathematics;defuzzification;machine learning;mathematics	AI	2.95368935250849	-25.64878204384586	160819
56c6e923661433781e0e23700cf6625b78dfbb17	an information-theoretic approach to quantitative association rule mining	quantitative databases;association statistique;logica booleana;confiance;information theoretic approach;database;informacion mutual;base dato;statistical association;association rules;data mining;association rule mining;normalized mutual information;frequent itemset;confidence;asociacion estadistica;information mutuelle;confianza;association rule;fouille donnee;edge graph;base de donnees;arete graphe;mutual information;logique booleenne;theorie information;boolean logic;information theoretic;busca dato;information theory;arista grafico;teoria informacion	Quantitative association rule (QAR) mining has been recognized an influential research problem over the last decade due to the popularity of quantitative databases and the usefulness of association rules in real life. Unlike boolean association rules (BARs), which only consider boolean attributes, QARs consist of quantitative attributes which contain much richer information than the boolean attributes. However, the combination of these quantitative attributes and their value intervals always gives rise to the generation of an explosively large number of itemsets, thereby severely degrading the mining efficiency. In this paper, we propose an information-theoretic approach to avoid unrewarding combinations of both the attributes and their value intervals being generated in the mining process. We study the mutual information between the attributes in a quantitative database and devise a normalization on the mutual information to make it applicable in the context of QAR mining. To indicate the strong informative relationships among the attributes, we construct a mutual information graph (MI graph), whose edges are attribute pairs that have normalized mutual information no less than a predefined information threshold. We find that the cliques in the MI graph represent a majority of the frequent itemsets. We also show that frequent itemsets that do not form a clique in the MI graph are those whose attributes are not informatively correlated to each other. By utilizing the cliques in the MI graph, we devise an efficient algorithm that significantly reduces the number of value intervals of the attribute sets to be joined during the mining process. Extensive experiments show that our algorithm speeds up the mining process by up to two orders of magnitude. Most importantly, we are able to obtain most of the high-confidence QARs, whereas the QARs that are not returned by MIC are shown to be less interesting.	algorithm;association rule learning;clique (graph theory);computation;database;experiment;information theory;mutual information;ozoneweb;real life	Yiping Ke;James Cheng;Wilfred Ng	2007	Knowledge and Information Systems	10.1007/s10115-007-0104-4	association rule learning;information theory;computer science;machine learning;pattern recognition;data mining;database;mathematics;statistics	DB	-3.3536323470742904	-33.60029192447804	160851
03f8ea2be9debf87eb661bc4c634e771d8dd0936	neural network ensembles design with self-configuring genetic programming algorithm for solving computer security problems		Artificial neural networks based ensembles are used for solving the computer security problems. Ensemble members and the ensembling method are generated automatically with the self-configuring genetic programming algorithm that does not need preliminary adjusting. Performance of the approach is demonstrated with test problems and then applied to two real world problems from the field of computer security – intrusion and spam detection. The proposed approach demonstrates results competitive to known techniques.	algorithm;artificial neural network;computer security;genetic programming	Eugene Semenkin;Maria Semenkina;Ilia Panfilov	2012		10.1007/978-3-642-33018-6_3	algorithm design;computer science;artificial intelligence;theoretical computer science;machine learning	ML	7.159679211973964	-37.07912408886896	160930
762215847f0542710674fba9fc4d58a53b270c9d	non-commutative product logic and probability of fuzzy events		In this paper we develop the non-commutative product logic psΠL as the non-commutative analogue of the product logic ΠL introduced by Hajek, Godo and Esteva [10]. The investigation of this logical system is an open problem in Hajek [9]. We also introduce a probabilistic logic based on the non-commutative product logic capable to reason about the probability of fuzzy events.		Denisa Diaconescu	2012		10.1007/978-3-642-31715-6_22	fuzzy logic;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;fuzzy associative matrix;fuzzy set operations;fuzzy control system	NLP	0.8158447478451858	-24.339204805246034	160936
888ca950a5429f038a6d3a0cce2aedb22fa102f1	sncstream+: extending a high quality true anytime data stream clustering algorithm	unsupervised learning;social networks theory;data stream clustering	Data Stream Clustering is an active area of research which requires efficient algorithms capable of finding and updating clusters incrementally as data arrives. On top of that, due to the inherent evolving nature of data streams, it is expected that algorithms undergo both concept drifts and evolutions, which must be taken into account by the clustering Network Clusterer Stream (SNCStream ). SNCStream tackles the data stream clustering problem as a network formation and evolution problem, where instances and microclusters form clusters based on homophily. Our proposal has its parameters analyzed and it is evaluated in a broad set of problems against literature baselines. Results show that SNCStreamþ achieves superior clustering quality (CMM), and feasible processing time and memory space usage when compared to the original SNCStream and other proposals of the literature. & 2016 Elsevier Ltd. All rights reserved.	anytime algorithm;baseline (configuration management);capability maturity model;cluster analysis;computation;curse of dimensionality;dspace;data mining;data stream clustering;display resolution;html;intrusion detection system;network formation;space–time tradeoff;stream cipher;stream processing;streaming media;synthetic intelligence;xml	Jean Paul Barddal;Heitor Murilo Gomes;Fabrício Enembreck;Jean-Paul A. Barthès	2016	Inf. Syst.	10.1016/j.is.2016.06.007	unsupervised learning;correlation clustering;constrained clustering;determining the number of clusters in a data set;data stream clustering;fuzzy clustering;flame clustering;computer science;artificial intelligence;data science;canopy clustering algorithm;machine learning;consensus clustering;cure data clustering algorithm;data mining;database;data stream mining;cluster analysis;brown clustering;dbscan;biclustering;affinity propagation;statistics;clustering high-dimensional data;conceptual clustering	DB	-1.580407773143444	-37.82116452178093	160960
366a75f411a302cf440dff3eea141d253b108a4e	athena: mining-based interactive management of text database	cluster algorithm;classification algorithm;procesamiento informacion;systeme apprentissage;k means;user feedback;satisfiability;information organization;systeme conversationnel;learning systems;naive bayes classifier;organizacion informacion;interactive system;text database;clustering method;linear time;information processing;sistema conversacional;organisation information;full text databases;information system;traitement information;base donnee texte integral;systeme information;sistema informacion	We describe Athena: a system for creating, exploiting, and maintaining a hierarchy of textual documents through interactive miningbased operations. Requirements of any such system include speed and minimal end-user e ort. Athena satis es these requirements through linear-time classi cation and clustering engines which are applied interactively to speed the development of accurate models. Naive Bayes classi ers are recognized to be among the best for classifying text. We show that our specialization of the Naive Bayes classi er is considerably more accurate (7 to 29% absolute increase in accuracy) than a standard implementation. Our enhancements include using Lidstone's law of succession instead of Laplace's law, under-weighting long documents, and over-weighting author and subject. We also present a new interactive clustering algorithm, C-Evolve, for topic discovery. C-Evolve rst nds highly accurate cluster digests (partial clusters), gets user feedback to merge and correct these digests, and then uses the classi cation algorithm to complete the partitioning of the data. By allowing this interactivity in the clustering process, C-Evolve achieves considerably higher clustering accuracy (10 to 20% absolute increase in our experiments) than the popular K-Means and agglomerative clustering methods.	algorithm;cluster analysis;experiment;interactivity;k-means clustering;naive bayes classifier;open road tolling;partial template specialization;requirement;succession;time complexity	Rakesh Agrawal;Roberto J. Bayardo;Ramakrishnan Srikant	2000		10.1007/3-540-46439-5_25	time complexity;correlation clustering;naive bayes classifier;information processing;computer science;artificial intelligence;machine learning;data mining;database;cluster analysis;information system;algorithm;k-means clustering;satisfiability	DB	-4.375859224276322	-32.92437391009716	160997
eeec54d6dea9015140c6830102234b7a62766551	dispensability of bias for three-layer max-min fuzzy neural networks	feedforward neural network;fuzzy neural network;fuzzy neural nets;fuzzy neural nets feedforward neural nets;fuzzy neural networks fuzzy control neural networks fuzzy logic fuzzy systems fuzzy sets equations multilayer perceptrons educational institutions information science;three layers max min fnns three layer max min fuzzy neural networks feedforward neural network classification problems approximation problems;three layer max min fuzzy neural networks;classification problems;feedforward neural nets;approximation problems;three layers max min fnns	It is well-known that a conventional feedforward neural network always has bias (threshold) terms, which is necessary for it to solve classification or approximation problems. But for fuzzy neural networks (FNNs), it seems not quite clear yet whether the bias is dispensable or not. Some authors introduce the bias, while the others do not. We consider a three layers max-min FNNs, and prove that the bias indeed can be useless in certain cases specified in this paper.	approximation;artificial neural network;exponent bias;feedforward neural network;maxima and minima;multitier architecture	Jie Yang;Long Li;Yan Liu	2007	Third International Conference on Natural Computation (ICNC 2007)	10.1109/ICNC.2007.332	feedforward neural network;probabilistic neural network;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;time delay neural network;intelligent control	Robotics	6.059526689799915	-27.785777335423113	161219
f59a7df447b9fe59e03cc39bbf8cc2d6183a1736	hierarchical fuzzy systems for function approximation on discrete input spaces with application	kolmogorov s theorem;hierarchical structure;theorem;discrete input spaces;decision support;separable hierarchical structure;variables;neural networks;fuzzy system identification method;helium;input variables;hierarchical systems;identification approximation theory discrete systems fuzzy set theory hierarchical systems;site selection;discrete spaces;neural network approaches;sufficient conditions;kolmogorov theorem;fuzzy set theory;approximation;approximation theory;hierarchical fuzzy systems;regression;capabilities;biomedical engineering;function approximation;site selection decision support;identification;performance model;universal;councils;discrete systems;universal approximation discrete spaces function approximation hierarchical fuzzy systems kolmogorov s theorem site selection decision support;humans;modeling;neural network approaches hierarchical fuzzy systems function approximation discrete input spaces kolmogorov theorem separable hierarchical structure fuzzy system identification method site selection decision support;fuzzy systems function approximation neural networks biomedical engineering modeling input variables humans knowledge engineering councils;fuzzy systems;fuzzy system;network;neural network;universal approximation;knowledge engineering	This paper investigates the capabilities of hierarchical fuzzy systems to approximate functions on discrete input spaces. First, it is shown that any function on a discrete space has an arbitrary separable hierarchical structure and can be naturally approximated by hierarchical fuzzy systems. As a by-product of this result, a discrete version of Kolmogorov's theorem is obtained; second, it is proven that any function on a discrete space can be approximated to any degree of accuracy by hierarchical fuzzy systems with any desired separable hierarchical structure. That is, functions on discrete spaces can be approximated more simply and flexibly than those on continuous spaces; third, a hierarchical fuzzy system identification method is proposed in which human knowledge and numerical data are combined for system construction and identification. Finally, the proposed method is applied to the market condition performance modeling problem in site selection decision support and shows the better performance in both accuracy and interpretability than the regression and neural network approaches. In additions, the reason and mechanism why hierarchical fuzzy systems outperform regression and neural networks in this type of application are analyzed.	approximation algorithm;artificial neural network;decision support system;fuzzy control system;fuzzy logic;level of measurement;numerical analysis;performance prediction;spaces;system identification	Xiao-Jun Zeng;John Yannis Goulermas;Panos Liatsis;Di Wang;John A. Keane	2008	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2008.924343	variables;identification;mathematical optimization;discrete mathematics;theorem;systems modeling;regression;function approximation;computer science;artificial intelligence;machine learning;approximation;control theory;mathematics;fuzzy set;helium;artificial neural network;fuzzy control system;approximation theory	Robotics	4.595527784411251	-27.335416135679658	161406
7272f34aa821eb5c645cc4930200841e4b243080	approximate bayesian network classifiers	bayes estimation;directed acyclic graph;grafo aciclico;bayesian network;bayesian network classifier;entropia;probabilidad condicional;naive bayes;probabilite conditionnelle;graphe acyclique;intelligence artificielle;probabilistic approach;classification;acyclic graph;reseau bayes;estimacion bayes;red bayes;table decision;enfoque probabilista;approche probabiliste;directed graph;entropie;graphe oriente;tabla decision;bayes network;artificial intelligence;grafo orientado;entropy;inteligencia artificial;conditional probability;clasificacion;decision table;estimation bayes	Bayesian network (BN) is a directed acyclic graph encoding probabilistic independence statements between variables. BN with decision attribute as a root can be applied to classification of new cases, by synthesis of conditional probabilities propagated along the edges. We consider approximate BNs, which almost keep entropy of a decision table. They have usually less edges than classical BNs. They enable to model and extend the well-known Naive Bayes approach. Experiments show that classifiers based on approximate BNs can be very efficient.	approximation algorithm;bayesian network;decision table;directed acyclic graph;experiment;naive bayes classifier;simulation;statement (computer science);whole earth 'lectronic link	Dominik Slezak;Jakub Wroblewski	2002		10.1007/3-540-45813-1_48	entropy;computer science;machine learning;pattern recognition;bayesian network;mathematics;directed acyclic graph;statistics	AI	7.63621250162092	-32.49836451320983	161559
4f23ca67e421f214ef70fc9d64295817efa0b09c	sensory quality assessment of food using active learning		The correctness of sensory assessment of food quality based on machine learning approach is significantly growing in the food industry. It contributes to an improvement of the food composition and develops the new food products. However, this process requires human intervention. And thus, it is costly, time consuming and easily be biased. In this paper, we propose the Active learning method based on Sequential minimizing optimization in order to evaluate sensory of red wine quality. The general idea is letting the algorithm choose the most uncertain products and asking experts for their opinions. This scheme greatly reduces the number of labels needed for the training process, and, consequently leads to the reduction on the cost of the sensory evaluation process. Experimental results show that the prospect of this method can be widely applied in the optimization of food ingredient and consumer tastes from food consumption markets.		Nhat-Vinh Lu;Van-Nam Huynh;Takaya Yuizono;Trung-Ky Nguyen	2018		10.1007/978-3-319-75429-1_17	active learning;ingredient;correctness;food quality;food composition data;food industry;machine learning;sensory system;artificial intelligence;computer science	NLP	0.7843018572196309	-28.71357095945851	161578
b9575e60a92098d2528e89fd400a8c8ec8011802	geometric type-1 and type-2 fuzzy logic systems	fuzzy set;fuzzy set theory computational geometry fuzzy logic;uoa 23 computer science and informatics;computational geometry;fuzzy logic fuzzy sets computational geometry computational complexity uncertainty consumer products control systems application software fuzzy systems councils;fuzzy set theory;fuzzy logic;geometric approach;computational geometry geometric type 1 fuzzy logic system geometric type 2 fuzzy logic systems fuzzy sets;rae 2008;and type 2;type 2 fuzzy logic computational geometry geometric approach;fuzzy logic system;type 2 fuzzy logic;article	This paper presents a novel approach to the representation of type-1 and type-2 fuzzy sets utilising computational geometry. To achieve this our approach borrows ideas from the field of computational geometry and applies these techniques in the novel setting of fuzzy logic. We provide new algorithms for various operations on type-1 and type-2 fuzzy sets and for defuzzification. Results of experiments indicate that this approach reduces the execution speed of these operations	algorithm;computational geometry;defuzzification;experiment;formal system;fuzzy control system;fuzzy logic;fuzzy set;interval arithmetic;linear function;real-time clock;type-2 fuzzy sets and systems	Simon Coupland;Robert Ivor John	2007	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.889764	fuzzy logic;t-norm fuzzy logics;combs method;fuzzy electronics;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;computational geometry;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;mathematics;t-norm;fuzzy set;fuzzy associative matrix;fuzzy set operations;algorithm;fuzzy control system	Visualization	0.2132413210170579	-24.753370067864942	161877
f71818707353fa6a19bfb2f604445805cfe222cb	"""erratum to """"specifying fuzzy data from grey-tone pictures for pattern recognition"""" [pattern recognition letters 17 (1996) 585-592]"""	fuzzy data;pattern recognition		fuzzy logic;pattern recognition letters	Hans Bandemer	1996	Pattern Recognition Letters	10.1016/S0167-8655(96)00111-0	speech recognition;feature;computer science;machine learning;pattern recognition	Vision	2.4376463650115388	-25.14440465148944	162029
1d71e22cf436665f0fee5ecc85c7095f35d51ae7	a safety evaluation method of mine pressure based on data stream classification		Mine pressure monitoring data is essentially a data stream, the safety evaluation of mine pressure can be seen as data stream classification, and classification labels are safety and unsafety. The safety evaluation method of mine pressure based on random decision tree is proposed in the paper, and the method determines the split point by Hoeffding Bounds inequality and information entropy instead of random selection. Experimental results show that the method has better accuracy for data stream classification; and it can be applied to the safety evaluation of mine pressure.	decision tree;decision tree model;entropy (information theory);social inequality	Gang Sun;Zhongxin Wang;Jia Zhao;Hao Wang;Chuanyun Ni;Jie Bai;Nannan Wu	2017	2017 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)	10.1109/FSKD.2017.8393347	data stream;computer science;data mining	Robotics	0.3005345825777457	-34.290696401475714	162062
6581de1d64bb490cf20e7bd50fbeb4a97e133ec1	the matrix representation of fuzzy knowledge and its application to the expert systems design	matrix representation;fuzzy data;conjunctive normal form;equivalent transformation;knowledge base;expert system	An approach to the diagnostic type expert systems design based on the special matrix representation of fuzzy predicates in the attribute model of the problem domain is presented. Intensive representation of predicates by means of sectional matrices is an analogue of the conjunctive normal form. Rules, positive examples and negative examples (in general, all fuzzy) can be used to form knowledge base. Diagnostics problem is thought of as finding some attribute values provided that the information about other attribute values is available. Logical inference is based on an equivalent transformation of the matrix to that containing all prime disjuncts by using the operation < xk > of fuzzy resolution. Two strategies to carry out such transformation are described. On the basis of formalism presented the expert system shell EDIP is developed, the first version of that is non-fuzzy and the second one allows working with fuzzy data and conclusions.	conjunctive normal form;expert system;formal system;fuzzy logic;knowledge base;matrix representation;problem domain;systems design;the matrix	V. Levchenko;Alexandr Savinov	1993	The Computer Science Journal of Moldova		conjunctive normal form;knowledge base;matrix representation;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;data mining;mathematics;fuzzy associative matrix;expert system;fuzzy set operations;algorithm;fuzzy control system	AI	1.560549371256825	-26.261156207939045	162082
01c283d254c325e3aef29a792d2eb97acd9bfef2	a data dictionary for learning data analysis	base donnee;learning;database;functional dependency;aprendizaje;modelisation;data analysis;apprentissage;dictionnaire;dependance fonctionnelle;dictionaries;analyse donnee;base datos;modeling;diccionario;modelaje		data dictionary	R. N. Maddison;A. J. Gawronski	1985	Comput. J.	10.1093/comjnl/28.3.270	speech recognition;systems modeling;machine-readable dictionary;computer science;machine learning;pattern recognition;database;functional dependency;data analysis	Theory	7.241111549924876	-31.389475579562838	162294
ceab18b6bf69179a67977e7bffcddf94c2ea1558	edge characterization using a model-based neural network	electronic mail;neural networks;neural nets;edge detection;network weights;training;noisy images;noise robustness;noisy images edge characterization model based neural network modular architecture image edges significant features training edge detection decision parameters network weights noise contaminations;current measurement;image edge detection;image edges;pixel;model based neural network;mathematical model;significant features;learning artificial intelligence edge detection neural nets;humans;noise contaminations;contamination;learning artificial intelligence;modular architecture;edge characterization;decision parameters;australia;neural networks image edge detection mathematical model humans australia electronic mail noise robustness contamination pixel current measurement;neural network	In this paper, we investigate the feasibility of characterizing signi cant image edges using a model-based neural network with modular architecture. Instead of employing traditional mathematical models for characterization, we ask human users to select what they regard as signi cant features on an image, and then incorporate these selected edges directly as training examples for the network. Unlike conventional edge detection schemes where decision thresholds have to be speci ed, the current NN-based edge characterization scheme implicitly represents these decision parameters in the form of network weights which are updated during the training process. Experiments have con rmed that the resulting network is capable of generalizing this previously acquired knowledge to identify important edges in images not included in the training set. Most importantly, the current approach is very robust against noise contaminations, such that no re-training of the network is required when it is applied to noisy images.		Hau-San Wong;Terry Caelli;Ling Guan	1999		10.1109/ICASSP.1999.759938	computer vision;edge detection;computer science;artificial intelligence;machine learning;mathematical model;network simulation;contamination;artificial neural network;pixel	Vision	7.022006887005763	-28.43051547196238	162718
22e1cf4853324768d0caff46e697b7d28f76f38c	combined association rules for dealing with missing values	etude utilisation;regle;estudio utilizacion;knowledge extraction;methode;relational database;data mining;experimental result;recovery rate;frequent itemset;association rule;fouille donnee;missing value;resultado experimental;extraction connaissances;extraccion conocimiento;association;missing values;resultat experimental;metodo;busca dato;method;use study;asociacion	With the rapid increase in the use of databases, the problem of missing values inevitably arises. The techniques developed to effectively recover these missing values should be highly precise in order to estimate the missing values completely. The mining of association rules can effectively establish the relationship among items in databases. Therefore, discovered association rules are usually applied to recover the missing values in databases. This study presents a Fast Recycle Combined Association Rules (FRCAR) method to fill in the missing values. FRCAR applies a technique to recycle sub-frequent itemsets and bit-arrays to discover more association rules than the Missing Value Completion (MVC) approach.	asp.net mvc;association rule learning;bitwise operation;data mining;data pre-processing;database;missing data;model–view–controller	Jau-Ji Shen;Chin-Chen Chang;Yu-Chiang Li	2007	J. Information Science	10.1177/0165551506075329	association;method;association rule learning;missing data;relational database;computer science;artificial intelligence;data mining;knowledge extraction;imputation	DB	-3.3766868876760374	-33.150926810957124	162933
0e52e1e6d438ce47f5cb0c462916beced5a837e6	a proposal of fuzzy inference model composed of small-number-of-input rule modules	learning algorithm;fuzzy reasoning;time complexity;single input rule module;a large number of input variables;weighted sums;fuzzy reasoning model;fuzzy inference;small number of input rule module;non linear system;fuzzy system;numerical simulation	The automatic construction of fuzzy system with a large number of input variables involves many difficulties such as large time complexities and getting stuck in a shallow local minimum. In order to overcome them, an SIRMs (Single-Input Rule Modules) model has been proposed. However, such a simple model does not always achieve good performance in complex non-linear systems. This paper proposes a fuzzy reasoning model as a generalized SIRMs model, in which each module deals with a small number of input variables. The reasoning output of the model is determined as the weighted sum of all modules, where each weight is the importance degree of a module. Further, in order to construct a simpler model, we introduce a module deletion function according to the importance degree into the proposed system. With the deletion function, we propose a learning algorithm to construct a fuzzy reasoning system consisting of smallnumber-of-input rule modules (SNIRMs). The conducted numerical simulation shows that the proposed method is superior in terms of accuracy compared to the conventional SIRMs model.	algorithm;computer simulation;fuzzy control system;linear system;machine learning;maxima and minima;nonlinear system;numerical analysis;reasoning system;weight function	Noritaka Shigei;Hiromi Miyajima;Shinya Nagamine	2009		10.1007/978-3-642-01510-6_14	computer simulation;time complexity;discrete mathematics;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set operations;algorithm;fuzzy control system	AI	5.412410960233758	-27.31537011761645	162985
c6584a57e3d519794843641b2418f190b7a590ac	ga-based fuzzy neural controller design for municipal incinerators	multiobjective programming;incinerador;programmation multiobjectif;fuzzy set;control difusa;incinerator;ga;control inteligente;soft computing;fuzzy control;rule based;fuzzy logic control;simulation;solid waste management;conjunto difuso;simulacion;estrategia;ensemble flou;algoritmo genetico;operational risk;intelligent control;fuzzy sets;automatic combustion control;strategy;fluidized bed;incinerateur;chemical properties;icc;combustion efficiency;identification;intelligent combustion control;indice combustion;controller design;algorithme genetique;taux combustion;identificacion;genetic algorithm;control;scientific communication;commande intelligente;reseau neuronal;strategie;red neuronal;control strategy;acc;commande floue;neural network;programacion multiobjetivo	The successful operation of mass burn municipal incinerators for solid waste management involves many uncertain factors. Not only the physical composition and chemical property of waste streams but also the complexity of combustion mechanism would signi4cantly in5uence the performance of waste incineration. Due to the rising concerns of dioxin=furan emissions from those incineration facilities in the metropolitan region, the applications of fuzzy control technologies for reducing the operational risks have gradually received wide attention in the scienti4c community. Recent advances of intelligent combustion control (ICC) technologies indicate that hybrid fuzzy control schemes, integrating some ideas and paradigms existing in di9erent soft computing approaches, may provide more reliable control performance of combustion process. The proposed integrated methodology, using genetic algorithms and neural networks as tools to aid in fuzzy logic control, therefore employs a three-stage analysis. It applies three soft computing approaches simultaneously for generating a representative state function, searching for a set of multi-objective control strategies, and auto-tuning the fuzzy control rule base for use in the case study. The 4ndings from this research clearly indicate that the control performance at least in three types of municipal incinerators (i.e., patented stokers are Takuma, Volund, and Martin) is greatly enhanced via the use of ICC technologies. By using this integrated methodology, the case study not only veri4es the applicability and suitability for controlling municipal incinerators but also presents application potentials for controlling many other types of industrial incinerators, such as modular, rotary kiln, and 5uidized bed incinerators. c © 2002 Elsevier Science B.V. All rights reserved.	approximation algorithm;artificial neural network;control theory;flue-gas stack;fuzzy control system;fuzzy logic;genetic algorithm;inference engine;knowledge acquisition;knowledge representation and reasoning;logic control;mathematical model;nonlinear system;requirement;rotary woofer;rule-based system;self-tuning;simulation;soft computing;software release life cycle	W. C. Chen;Ni-Bin Chang;Jeng-Chung Chen	2002	Fuzzy Sets and Systems	10.1016/S0165-0114(01)00205-6	computer science;artificial intelligence;soft computing;fuzzy set;fuzzy control system;intelligent control	AI	8.425466402135024	-25.117341859549608	163036
09e6a101cb2e02f9f533320b51455a46d5b7d7f5	immune genetic algorithm-based adaptive evidential model for estimating unmeasured parameter: estimating levels of coal powder filling in ball mill	evidence theoretic k nn classifier;unmeasured parameter;euclidean distance;power plant;level of coal powder;classification rules;distance metric;estimating;genetic algorithm;global optimization;evolutionary algorithm;parameter estimation;immune genetic algorithm;ball mill	To estimate the unmeasured parameter from experts and running data, in this paper, a novel method named ''immune genetic algorithm-based adaptive evidential classification rule (IGA-EC)'' was proposed. The IGA-EC model was realized by two strategies: (1) a new parametric distance metric was applied instead of Euclidean distance to enhance the robust adaptive ability of the traditional evidence-theoretic classification rule; and (2) the powerful evolutionary algorithm immune genetic algorithm was used to parallel search the global optimal solutions of the parameters involved in the proposed model. To validate IGA-EC model, some experiments were conducted based on some popular data sets, and the experimental results show that the proposed method was powerful with respect to the accuracy. Finally, the IGA-EC model was used to estimate the unmeasured parameter level of coal powder filling in the ball mill in power plant. From the analysis of the estimating results, it suggests that the proposed method was applicable for estimating the level of coal powder, and the proposed method can also be applied for estimating other unmeasured parameters in industry.	genetic algorithm	Zhi-gang Su;Pei-hong Wang;Xiang-jun Yu	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.12.077	power station;econometrics;estimation;genetic algorithm;metric;computer science;machine learning;evolutionary algorithm;euclidean distance;mathematics;ball mill;estimation theory;statistics;global optimization	Web+IR	6.590634341114494	-25.73679296756076	163037
717d1d6c9b40fec6297f0f365d711983b9e4abf9	feature selection, ranking of each feature and classification for the diagnosis of community acquired legionella pneumonia	selection problem;metodo analitico;problema seleccion;aplicacion medical;legionellaceae;bacterie;receiver operator characteristic;legionella pneumophila;false negative;laboratory tests;ensayo laboratorio;classification;validacion prueba;laboratory test;neural net;test validation;laboratory techniques;analytical method;community acquired pneumonia;pattern classification;pattern recognition;true positive;methode analytique;feature selection;medical application;bacteria;reconnaissance forme;reseau neuronal;reconocimiento patron;essai laboratoire;clasificacion;red neuronal;neural network;validation test;application medicale;prospective study;probleme selection;classification forme	Diagnosis of community acquired legionella pneumonia (CALP) is currently performed by means of laboratory techniques which may delay diagnosis several hours. To determine whether ANN can categorize CALP and non-legionella community-acquired pneumonia (NLCAP) and be standard for use by clinicians, we prospectively studied 203 patients with community-acquired pneumonia (CAP) diagnosed by laboratory tests. Twenty one clinical and analytical variables were recorded to train a neural net with two classes (LCAP or NLCAP class). In this paper we deal with the problem of diagnosis, feature selection, and ranking of the features as a function of their classification importance, and the design of a classifier the criteria of maximizing the ROC (Receiving operating characteristics) area, which gives a good trade-off between true positives and false negatives. In order to guarantee the validity of the statistics; the train-validation-test databases were rotated by the jackknife technique, and a multistarting procedure was done in order to make the system insensitive to local maxima.	artificial neural network;categorization;database;experiment;feature selection;jackknife resampling;maxima and minima	Enric Monte-Moreno;Jordi Solé i Casals;José Antonio Fiz;Nieves Sopena	2001		10.1007/3-540-45723-2_43	prospective cohort study;simulation;bacteria;biological classification;computer science;artificial intelligence;machine learning;test validity;receiver operating characteristic;artificial neural network;statistics	AI	8.424955380861803	-30.907110716288287	163134
bc9546e5103672a42108ffdfcee23343466096ec	fuzzy rule based networks	fuzzy set theory boolean functions;complex networks;formal model;rule based systems;boolean functions;rule based system;complex network;rule based;block schemes fuzzy rule based networks fuzzy system multiple rule bases single rule bases formal models boolean matrices binary relations topological expressions;fuzzy set theory;rule based networks;fuzzy rule base;complex system;binary relation;complex systems;fuzzy networks;rule based networks complex systems complex networks fuzzy systems fuzzy networks rule based systems;fuzzy systems;fuzzy system	This tutorial introduces the novel concept of a fuzzy rule based network whose nodes are fuzzy rule bases and the connections between the nodes are the interactions between these rule bases. A fuzzy network is viewed as a fuzzy system with networked rule bases as opposed to fuzzy systems with single or multiple rule bases. A comparison between different types of fuzzy systems is presented as well as formal models for fuzzy networks such as Boolean matrices, binary relations, block schemes and topological expressions.	artificial intelligence;data structure;fuzzy control system;fuzzy rule;interaction;interconnection;knowledge engineering;rule 184;rule-based system;system of systems	Alexander E. Gegov	2011	2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI)	10.1109/FOCI.2011.5949481	fuzzy logic;rule-based system;discrete mathematics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;complex network;algorithm;fuzzy control system	Theory	1.6488212079998594	-25.441729921910376	163150
3e6ed1a640168e4a08a3f9ab407a8b3501c8cc20	statistical inference of unknown attribute values in databases	regression model;statistical method;relational database;statistical inference	In this paper, we propose to use statistical methods to estimate unknown attribute values in databases, as compared to assigning possible values at users’ discretion in common practice. Regression models and classification analysis are introduced for estimating continuous and categorical unknown attribute values, respectively. Procedures for selecting relevant attributes in a relation and preliminary experimental results of the proposed models on a real life database are also presented.	database;real life	Wen-Chi Hou;Zhongyang Zhang;Nong Zhou	1993		10.1145/170088.170097	statistical model;statistical inference;predictive inference;fiducial inference;statistical theory;frequentist inference;relational database;computer science;pattern recognition;data mining;regression analysis;statistics	DB	-0.17833948004944775	-31.750095266660168	163248
d4a89222fc88e5bdf0869d8680e84548ad45d8e6	first-order interval type-1 non-singleton type-2 tsk fuzzy logic systems	modelizacion;laminoir feuillard;entrada salida;surface temperature;theorie type;beam mechanics;continuous casting;logique floue;laminador fleje;logica difusa;intelligence artificielle;logical programming;temperatura superficial;temperature superficielle;viga;input output;fuzzy logic;modelisation;systeme incertain;surface treatment;first order;strip mill;colada continua;programmation logique;error propagation;backpropagation algorithm;traitement surface;type theory;fuzzy logic system;algorithme retropropagation;artificial intelligence;sistema difuso;inteligencia artificial;systeme flou;poutre;sistema incierto;coulee continue;programacion logica;modeling;tratamiento superficie;antecedent;uncertain system;back propagation;fuzzy system;fuzzy model;entree sortie;algoritmo retropropagacion;antecedente	This article presents the implementation of first-order interval type-1 non-singleton type-2 TSK fuzzy logic system (FLS). Using input-output data pairs during the forward pass of the training process, the interval type-1 non-singleton type-2 TSK FLS output is calculated and the consequent parameters are estimated by back-propagation (BP) method. In the backward pass, the error propagates backward, and the antecedent parameters are estimated also by back-propagation. The proposed interval type-1 non-singleton type-2 TSK FLS system was used to construct a fuzzy model capable of approximating the behaviour of the steel strip temperature as it is being rolled in an industrial Hot Strip Mill (HSM) and used to predict the transfer bar surface temperature at finishing Scale Breaker (SB) entry zone, being able to compensate for uncertain measurements that first-order interval singleton type-2 TSK FLS can not do.	first-order predicate;formal system;fuzzy logic;type-2 fuzzy sets and systems	Gerardo M. Mendez;Luis Adolfo Leduc	2006		10.1007/11925231_8	computer science;artificial intelligence;backpropagation;machine learning;control theory;algorithm;fuzzy control system	Logic	8.587637363520697	-28.305220970441937	163527
0dfa7d018d6213922a3f3a8660f993f80eb80b93	hierarchical bayesian networks: an approach to classification and learning for structured data	bayes estimation;bayesian network;bayesian classifier;incertidumbre;uncertainty;error sistematico;mutagenese;intelligence artificielle;classification;reseau bayes;estimacion bayes;complex data;learning methods;bias;red bayes;estructura datos;inferencia;bayes network;artificial intelligence;structure donnee;modele donnee;incertitude;inteligencia artificial;mutagenesis;data structure;clasificacion;inference;erreur systematique;data models;estimation bayes;structured data	BayesianNetworks areoneof the mostpopularformalismsfor reasoningunderuncertainty. HierarchicalBayesianNetworks(HBNs)areanextensionof BayesianNetworks that areableto dealwith structureddomains,using knowledgeaboutthestructureof thedatato introduceabiasthatcancontributeto improving inferenceandlearningmethods.In effect,nodesin anHBN are(possibly nested)aggregationsof simplernodes.Everyaggregatenodeis itself anHBN modelingindependences insideasubsetof thewholeworld underconsideration. In thispaperwediscusshow HBNscanbeusedasBayesianclassifiersfor structured domains.We also discusshow HBNs can be further extendedto model morecomplex datastructures,suchaslists or sets,andwe presenttheresultsof preliminaryexperimentson themutagenesisdataset.	bayesian network;type constructor;xml	Elias Gyftodimos;Peter A. Flach	2004		10.1007/978-3-540-24674-9_31	data structure;computer science;machine learning;bayesian network;data mining;programming language;statistics	ML	7.537523111852745	-31.348564114237607	163542
21d71989340549968a8f105bdb227369a35a026a	the soft computing approach to program development time estimation	soft computing approach;software effort modeling;program development time estimation;ann training;ann training soft computing approach program development time estimation software effort modeling neural network model three layer feed forward network;three layer feed forward network;feedforward neural nets;mathematical model neural networks predictive models object oriented modeling neurons programming profession testing software engineering feeds transfer functions;neural network model;learning artificial intelligence;software development management;software development management feedforward neural nets learning artificial intelligence	Software effort modeling is one of the fields in which machine learning techniques have proved effective. In this paper, attempt has been made to establish a relation between program development time with respect to its dependence on various program and personnel attributes. A neural network model has been developed to predict the development time of various software programs. The model is a three layer feed forward network with 17 neurons in the hidden layer. The model has 5 inputs and 1 output.	artificial neural network;feed forward (control);machine learning;multitier architecture;network model;soft computing	Vandana Bhattacherjee	2006	9th International Conference on Information Technology (ICIT'06)	10.1109/ICIT.2006.84	nervous system network models;simulation;computer science;artificial intelligence;machine learning;time delay neural network;artificial neural network	SE	8.924709294479781	-25.722925401527263	163575
356cf742ec96f08e28f5d4dba30ee3b25b3c85dc	zoomed ranking: selection of classification algorithms based on relevant performance information	classification algorithm;learning algorithm;procesamiento informacion;adquisicion del conocimiento;algorithm performance;integration information;algorithme apprentissage;acquisition connaissances;information integration;resultado algoritmo;knowledge acquisition;information processing;integracion informacion;performance algorithme;classification automatique;traitement information;automatic classification;algoritmo aprendizaje;clasificacion automatica	The need for methods which would assist the user in selecting classiication algorithms for a new problem has frequently been recognized as an important issue. Previous meta-learning approaches to algorithm selection consist of suggesting one algorithm or a small group of algorithms that are expected to perform well on the given problem 1, 3]. We believe that a more informative and exible solution is to provide rankings of the candidate algorithms 5, 2]. A ranking can be used to select just one algorithm, or more, if enough resources are available. The problem of constructing rankings can be seen as an alternative to other Machine Learning methods, such as classiication and regression. One reason why selecting appropriate classiication algorithms is diicult is related to the issue of how to assess their performance. The evaluation measure that is commonly used in classiication problems is error rate. However, it is often important to consider a combination of several criteria 5]. We may consider, for instance, time to learn and time to apply a learned model. With the increasing computational power available today, it may be argued that time is not so important. Still some classiication algorithms may take weeks or months to run on the volumes of data that organizations commonly store nowadays, which may not be acceptable. We may also consider interpretability, since in many applications the decision model must be understood. Not much work has been done to incorporate these criteria into the KDD process. Recently, several methods that generate rankings of algorithms based on their past performance have been developed with promising results. Some are based only on accuracy 2], others on accuracy and time 6]. These methods were tested using all available information, i.e. concerning all datasets used in A complete version of this paper can be found in	algorithm selection;bit error rate;computation;data mining;information;machine learning	Carlos Soares;Pavel Brazdil	2000		10.1007/3-540-45372-5_13	ranking;information processing;computer science;artificial intelligence;information integration;machine learning;data mining;database;ranking svm	ML	7.939364809531171	-32.87735428668148	164853
df324afeb4e7e62b7fe30bf90256d5453911df55	on combining unsupervised classification and ontology knowledge	image interpretation unsupervised classification ontology knowledge clustering algorithm knowledge discovery remote sensing;knowledge based system;image segmentation;clustering algorithm;remote sensing data mining geophysics computing image classification ontologies artificial intelligence;image classification;ontologies clustering algorithms remote sensing image classification clustering methods knowledge based systems information resources pattern recognition organizing spatial resolution;data mining;ontologies artificial intelligence;image interpretation;accuracy;geophysics computing;roads;clustering method;remote sensing;pattern recognition;unsupervised classification;clustering algorithms;ontologies;ontology knowledge;clustering methods;knowledge based systems image classification pattern recognition clustering methods;knowledge based systems;knowledge discovery	This paper presents a way to combine knowledge obtained from a clustering algorithm and from an ontology. Using the both sources of information allows to improve the results of the knowledge discovery process. The basic property of clustering algorithms, which is to group similar objects, is the key of this approach. We use it to extend the knowledge given by an ontology. Indeed, this knowledge can be partial or not enough accurate, and clustering can then be used to fill this lack of information. We also present results and validation in the field of remote sensing image interpretation.	algorithm;cluster analysis;ontology (information science);unsupervised learning	Germain Forestier;Cédric Wemmert;Pierre Gançarski	2008	IGARSS 2008 - 2008 IEEE International Geoscience and Remote Sensing Symposium	10.1109/IGARSS.2008.4779741	computer science;data science;knowledge-based systems;pattern recognition;data mining;knowledge extraction;cluster analysis;ontology-based data integration;process ontology	Robotics	0.503529402474045	-37.14504610697699	164891
741f573b8e210ad95e3d1d9599f86d20aa248560	a study of data classification and selection techniques for medical decision support systems		Artificial Intelligence techniques have been increasingly used in medical decision support systems to aid physicians in their diagnosis procedures; making decisions more accurate and effective, minimizing medical errors, improving patient safety and reducing costs. Our research study indicates that it is difficult to compare different artificial intelligence techniques which are utilised to solve various medical decision-making problems using different data models. This makes it difficult to find out the most useful artificial intelligence technique among them. This paper proposes a classification approach that would facilitate the selection of an appropriate artificial intelligence technique to solve a particular medical decision making problem. This classification is based on observations of previous research studies.	clinical decision support system	Ahmed J. Aljaaf;Dhiya Al-Jumeily;Abir Jaafar Hussain;David J. Lamb;Mohammed Al-Jumaily;Khaled Abdel-Aziz	2014		10.1007/978-3-319-09339-0_14	intelligent decision support system;machine learning;pattern recognition;data mining;medical algorithm	ECom	-0.19996450396746457	-30.568274252452618	165279
69155a4ce3c977158713140fcfca7b94d93a2a4d	learning recursive relations with randomly selected small training sets		We evaluate CRUSTACEAN, an inductive logic programming algorithm that uses inverse implication to induce recursive clauses from examples. This approach is well suited for learning a class of self-recursive clauses, which commonly appear in logic programs , because it searches for common sub-structures among the examples. However, little evidence exists that inverse implication approaches perform well when given only randomly selected positive and negative examples. We show that CRUSTACEAN learns recursive relations with higher accuracies than GOLEM, yet with reasonable ef-ciency. We also demonstrate that increasing the number of randomly selected positive and negative examples increases its accuracy on randomly selected test examples, increases the frequency in which it outputs the target relation, and reduces its number of outputs. We also prove a theorem that deenes the class of logic programs for which our approach is complete.	algorithm;golem (ilp);inductive logic programming;randomness;recursion (computer science)	David W. Aha;Stephane Lapointe;Charles X. Ling;Stan Matwin	1994			discrete mathematics;computer science;machine learning;mathematics;algorithm	ML	5.605370927726763	-32.38491377932031	165350
661bbc14db28ba927e0d670281cb7cfa96779c24	a method for training finite mixture models under a fuzzy clustering principle	metodo estadistico;fuzzy c mean;fuzzy set;procesamiento informacion;learning;analisis datos;maximum likelihood;maximum vraisemblance;ajustement;conjunto difuso;ensemble flou;statistical method;fuzzy statistics and data analysis;classification;algorithme;aprendizaje;fitting;probabilistic model;algorithm;fuzzy clustering;data analysis;apprentissage;methode statistique;information processing;modele probabiliste;analyse donnee;sistema difuso;systeme flou;ajuste;traitement information;finite mixture model;clasificacion;maxima verosimilitud;fuzzy system;fuzzy model;modelo probabilista;algoritmo	In this paper, we establish a novel regard towards fuzzy clustering, showing it provides a sound framework for fitting finite mixture models. We propose a novel fuzzy clustering-type methodology for finite mixture model fitting, effected by utilizing a regularized form of the fuzzy c-means (FCM) algorithm, and introducing a proper dissimilarity functional for the algorithm with respect to the probabilistic properties of the model being treated. We apply the proposed methodology in a number of popular finite mixture models, and the corresponding expressions of the fuzzy model fitting algorithm are derived. We examine the efficacy of our novel approach in both clustering and classification applications of benchmark data sets, and we demonstrate the advantages of the proposed approach over maximum-likelihood. © 2010 Elsevier B.V. All rights reserved.	algorithm;benchmark (computing);best, worst and average case;cluster analysis;computation;computational complexity theory;curve fitting;expectation–maximization algorithm;fuzzy clustering;fuzzy cognitive map;fuzzy concept;loss function;mixture model;optimization problem;simulated annealing	Sotirios P. Chatzis	2010	Fuzzy Sets and Systems	10.1016/j.fss.2010.03.015	statistical model;econometrics;defuzzification;information processing;fuzzy clustering;biological classification;fuzzy classification;computer science;fuzzy number;machine learning;mixture model;mathematics;maximum likelihood;fuzzy set;data analysis;fuzzy control system;statistics	AI	8.508361769203827	-34.39470877121213	165493
5ce201204a0e1e1ce0fd61cc15394bc7f4b0f775	a fuzzy bayesian classifier with learned mahalanobis distance		Recent developments show that naive Bayesian classifier (NBC) performs significantly better in applications, although it is based on the assumption that all attributes are independent of each other. However, in the NBC each variable has a finite number of values, which means that in large data sets NBC may not be so effective in classifications. For example, variables may take continuous values. To overcome this issue, many researchers used fuzzy naive Bayesian classification for partitioning the continuous values. On the other hand, the choice of the distance function is an important subject that should be taken into consideration in fuzzy partitioning or clustering. In this study, a new fuzzy Bayes classifier is proposed for numerical attributes without the independency assumption. To get high accuracy in classification, membership functions are constructed by using the fuzzy C-means clustering (FCM). The main objective of using FCM is to obtain membership functions directly from the data set instead of consulting to an expert. The proposed method is demonstrated on the basis of two well-known data sets from the literature, which consist of numerical attributes only. The results show that the proposed the fuzzy Bayes classification is at least comparable to other methods. C © 2014 Wiley Periodicals, Inc.	algorithm;bayesian network;cluster analysis;euclidean distance;freebasic;fuzzy cognitive map;fuzzy set;john d. wiley;membership function (mathematics);naive bayes classifier;numerical analysis;seed;simulation	Necla Kayaalp;Guvenc Arslan	2014	Int. J. Intell. Syst.	10.1002/int.21659	membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;fuzzy number;machine learning;pattern recognition;data mining;mathematics;fuzzy set operations;statistics	ML	1.5007206065524124	-30.006446618980924	165706
0091396bd64bfc42559468f09aba7933c37dcbf8	on fuzzy associative memory with multiple-rule storage capacity	scale model;modele kosko;fuzzy neural nets;memoire associative;fuzzy set;rule modification fuzzy associative memory multiple rule storage capacity neural network model fuzzy systems fuzzy rule base fam matrix;matrix circuit;modele reduit;fuzzy rules;rule based;conjunto difuso;inference mechanisms;ensemble flou;fuzzy set theory;capacite stockage;circuito matricial;associative memory fuzzy systems fuzzy sets hardware neural networks fuzzy neural networks fuzzy control automatic control decision making;modelo reducido;associative processing;minimax techniques;fuzzy rule base;composition max min;capacidad almacenaje;storage capacity;circuit matriciel;inferencia;associative memory;memoria asociativa;sistema difuso;systeme flou;neural network model;reseau neuronal;content addressable storage;red neuronal;fuzzy system;inference;neural network;inference mechanisms fuzzy neural nets content addressable storage associative processing fuzzy set theory minimax techniques	Kosko's fuzzy associative memory (FAM) is the very first neural network model for implementing fuzzy systems. Despite its success in various applications, the model suffers from very low storage capacity, i.e., one rule per FAM matrix. A lot of hardware and computations are usually required to implement the model and, hence, it is limited to applications with small fuzzy rule-base. In this paper, the inherent property for storing multiple rules in a FAM matrix is identified. A theorem for perfect recalls of all the stored rules is established and based upon which the hardware and computation requirements of the FAM model can be reduced significantly. Furthermore, we have shown that when the FAM model is generalized to the one with max-bounded-product composition, single matrix implementation is possible if the rule-base is a set of semi-overlapped fuzzy rules. Rule modification schemes are also developed and the inference performance of the established high capacity models is reported through a numerical example.	content-addressable memory	Korris Fu-Lai Chung;Tong Lee	1996	IEEE Trans. Fuzzy Systems	10.1109/91.531778	computer science;artificial intelligence;machine learning;mathematics;fuzzy set;fuzzy associative matrix;artificial neural network;algorithm	Embedded	5.4588028694763695	-27.55381686418517	165917
596c53f6b855580c5cc1a4f698695ef2edbbfd4a	a hybrid algorithm for structure identification of neuro-fuzzy modeling	pattern clustering;neuro fuzzy modeling;fuzzy neural nets;data mining prototypes clustering algorithms fuzzy reasoning fuzzy sets fuzzy systems partitioning algorithms knowledge acquisition neural networks convergence;fuzzy if then rule;convergence;fuzzy reasoning;neural networks;numerical data point similarities;fuzzy rules;prototypes;fuzzy rule extraction;structure identification;rule extraction;movable fuzzy prototypes;data mining;fuzzy set theory;fuzzy sets;data partitioning;input output data;input output;fuzzy logic;data set clustering;mean square error;knowledge acquisition;membership functions;neuro fuzzy;membership function;fuzzy if then rules;mean square error hybrid algorithm structure identification neuro fuzzy modeling fuzzy rule extraction input output data membership functions numerical data point similarities data partitioning data set clustering movable fuzzy prototypes fuzzy if then rule statistical techniques convergence;clustering algorithms;statistical techniques;fuzzy systems;hybrid algorithm;partitioning algorithms;knowledge acquisition fuzzy neural nets fuzzy logic fuzzy set theory pattern clustering convergence	The problems with which we are often confronted in neuro-fuzzy modeling are how to adequately decide the number of fuzzy rules extracted from a set of input-output data and how to precisely define the membership functions of each fuzzy rule. In this paper, we propose a hybrid algorithm that can automatically extract fuzzy rules from a set of numerical data points. Our algorithm is mainly composed of two phases, viz. data partitioning and rule extraction. In the first phase, the data set is partitioned into several clusters according to the similarities between the data points. In other words, the nearby data points are grouped into the same cluster. This is completed by a sequence of combinations of movable fuzzy prototypes. In the second phase, a fuzzy IF-THEN rule is extracted from each cluster and the membership functions of the corresponding rule are determined by statistical techniques. Experimental results show that the proposed algorithm converges quickly and can generate fewer rules with a lower mean-square error.	hybrid algorithm;neuro-fuzzy	Chen-Sen Ouyang;Shie-Jue Lee	2000		10.1109/ICSMC.2000.886570	membership function;defuzzification;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;fuzzy set;fuzzy set operations;artificial neural network;fuzzy control system	NLP	3.42006368940629	-28.49745808037742	166004
4a8c4caa7be036950c6dd826f1519bc433df6143	time series shapelets: a new primitive for data mining	resource limitation;time series;data mining;classification;empirical evidence;nearest neighbor;space complexity;classification accuracy;empirical evaluation;pattern extraction	Classification of time series has been attracting great interest over the past decade. Recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems. While this may be considered good news, given the simplicity of implementing the nearest neighbor algorithm, there are some negative consequences of this. First, the nearest neighbor algorithm requires storing and searching the entire dataset, resulting in a time and space complexity that limits its applicability, especially on resource-limited sensors. Second, beyond mere classification accuracy, we often wish to gain some insight into the data.  In this work we introduce a new time series primitive, time series shapelets, which addresses these limitations. Informally, shapelets are time series subsequences which are in some sense maximally representative of a class. As we shall show with extensive empirical evaluations in diverse domains, algorithms based on the time series shapelet primitives can be interpretable, more accurate and significantly faster than state-of-the-art classifiers.	dspace;data mining;experiment;ibm notes;k-nearest neighbors algorithm;motion capture;ralf brown's interrupt list;sensor;time series	Lexiang Ye;Eamonn J. Keogh	2009		10.1145/1557019.1557122	empirical evidence;biological classification;computer science;machine learning;time series;pattern recognition;data mining;mathematics;dspace;k-nearest neighbors algorithm;statistics	ML	-1.9392008856630816	-37.00564446486527	166156
491e62d3ea2f3971c9f2487bb7e9718af4204329	prediction of chenille yarn and fabric abrasion resistance using radial basis function neural network models	modelizacion;prediccion;calcul neuronal;neural computation;activation function;fonction base radiale;statistical model;modelisation;radial basis function;prediction theory;rbfn;fonction activation;radial basis function neural network;analyse performance;performance analysis;modele statistique;modelo estadistico;modeling methodology;reseau neuronal;theorie prediction;funcion radial base;modeling;prediction;red neuronal;computacion neuronal;reseau neuronal artificiel;artificial neural network;neural network;analisis eficacia	The abrasion resistance of chenille yarn is crucially important in particular because the effect sought is always that of the velvety feel of the pile. Thus, various methods have been developed to predict chenille yarn and fabric abrasion properties. Statistical models yielded reasonably good abrasion resistance predictions. However, there is a lack of study that encompasses the scope for predicting the chenille yarn abrasion resistance with artificial neural network (ANN) models. This paper presents an intelligent modeling methodology based on ANNs for predicting the abrasion resistance of chenille yarns and fabrics. Constituent chenille yarn parameters like yarn count, pile length, twist level and pile yarn material type are used as inputs to the model. The intelligent method is based on a special kind of ANN, which uses radial basis functions as activation functions. The predictive power of the ANN model is compared with different statistical models. It is shown that the intelligent model improves prediction performance with respect to statistical models.	activation function;apache hadoop;approximation;artificial neural network;ising model;radial (radio);radial basis function;simulation;statistical model	Erhan Kenan Çeven;Sezai Tokat;Özcan Özdemir	2006	Neural Computing and Applications	10.1007/s00521-006-0048-8	statistical model;radial basis function;systems modeling;prediction;computer science;artificial intelligence;machine learning;activation function;artificial neural network;models of neural computation	AI	9.569537294835467	-24.92791408244517	166227
b41abfaef1072fae8152c7408159e54ba86c95be	fuzzy data window memory and its application to system identification by ga	fuzzy identification;fuzzy data window memory;genetic algorithms	In this paper, an approach for storing and retrieving the past data by means of a novel Fuzzy Data Window Memory (FDWM) is reported. The data, which is selected for memorization, is based on the highest firing strength of the fuzzy rule. The size of the proposed FDWM is much smaller than traditional window memory with no degradation in performance. A computer simulation study of one of the applications of the proposed FDWM is reported which uses the FDWM to reduce the training data set that will be used in the evaluation module in system identification by Genetic Algorithms (GA).	computer simulation;elegant degradation;fuzzy logic;fuzzy rule;genetic algorithm;software release life cycle;system identification;test set	K. M. Chow;Ahmad B. Rad;Kai Ming Tsang	2003	Intelligent Automation & Soft Computing	10.1080/10798587.2000.10642838	genetic algorithm;defuzzification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining	AI	7.793823630664193	-25.882893072831685	166268
6eb5e1b070325f60b378d22c7bb61fcc25ac4bad	online shopping customer data analysis by using association rules and cluster analysis	association rules analysis;k means;a priori algorithm;data mining;cluster analysis	Data Mining is the process of exploration and analysis of large quantities of data in order to discover meaningful patterns and rules. Data mining is considered as the only solution towards efficient use of increasing amounts of data worldwide. The process of converting data into information is achieved by means of data mining. In this study, first the concept of data mining is presented, then CRISP-DM process are described. In this paper Cluster Analysis and Association Rules are used to analyze the data. k-means Algorithm, Confidence and Support Ratios are theoretically explained and these techniques applied to a data set obtained from 314 customers from 7 regions of Turkey to identify their profile.	cluster analysis;online shopping	Serhat Güden;Umman Tugba Gursoy	2013		10.1007/978-3-642-39736-3_10	text mining;association rule learning;computer science;data science;machine learning;data warehouse;data mining;fsa-red algorithm;data pre-processing;data stream mining;cluster analysis;apriori algorithm;data analysis;k-means clustering	ML	-4.012598072397916	-34.91954172176246	166642
042967969e4d5f9ea2493ed57f8e36d3c10b8c55	improve analogy-based software effort estimation using principal components analysis and correlation weighting	software;software development cost estimation;software cost estimation;correlation weighting software effort estimation analogy principal components analysis;software quality principal component analysis software cost estimation;project manager;correlation weighting;software effort estimation;principal component analysis costs euclidean distance feature extraction programming quality management project management software development management software quality nasa;euclidean distance;data mining;effort estimation;estimation;principal components analysis;analogy based software effort estimation;feature extraction;principal component analysis;software development;analogy;prediction accuracy;independent feature extraction;correlation;benchmark datasets;correlation coefficient;correlation weighting analogy based software effort estimation principal components analysis software development cost estimation software quality euclidean distance analogy based effort estimation methods independent feature extraction benchmark datasets;analogy based effort estimation methods;similarity measure;software quality;covariance matrix	Software development cost overruns often induce project managers to cut down manpower cost at the expense of software quality. Accurate effort estimation is beneficial to the prevention of cost overruns. Analogy-based effort estimation predicts the effort of a new project by using the information of its similar historical projects, where the similarity is measured via Euclidean distance. To calculate the Euclidean distance, traditional analogy-based effort estimation methods usually adopt the original project features and assign uniform weights to them. However, it would lead to inappropriate similarity measure and result in inaccurate effort estimate if the original features are interdependent or have unequal impacts on the project effort. In this paper, we propose to use principal components analysis (PCA) to extract independent features, and then use Pearson correlation coefficients between the extracted features and the project effort as the weights for Euclidean distance calculation in similarity measure. Extensive experiments were further conducted on three benchmark datasets: COCOMO, Desharnais, and NASA. The experimental results show that our approach significantly improves prediction accuracy and reliability over the traditional method, either by using correlation weighting alone or by using PCA combined with correlation weighting. The comparison of our approach with other approaches reported in literature also suggests that our approach is competitive.	benchmark (computing);cocomo;coefficient;cost estimation in software engineering;euclidean distance;experiment;interdependence;principal component analysis;similarity measure;software development effort estimation;software quality	Jianfeng Wen;Shixian Li;Linyan Tang	2009	2009 16th Asia-Pacific Software Engineering Conference	10.1109/APSEC.2009.40	computer science;data mining;principal component analysis	SE	3.471065770760462	-33.6734112296351	166648
5c101299914d5f0109d19a750a6fd270b6913923	data streams and applications in computer science		This is a short survey of my work in data streams and related applications, such as communication complexity, numerical linear algebra, and sparse recovery. The goal is give a non-technical overview of results in data streams, and highlight connections between these different areas. It is based on my Presburger lecture award given at ICALP, 2014. 1 The Data Stream Model Informally speaking, a data stream is a sequence of data that is too large to be stored in available memory. The data may be a sequence of numbers, points, edges in a graph, and so on. There are many examples of data streams, such as internet search logs, network traffic, sensor networks, and scientific data streams (such as in astronomics, genomics, physical simulations, etc.). The abundance of data streams has led to new algorithmic paradigms for processing them, which often impose very stringent requirements on the algorithm’s resources. Formally, in the streaming model, there is a sequence of elements a1, . . . , am presented to an algorithm, where each element is drawn from a universe [n] = {1, . . . , n}. The algorithm is allowed a single or a small number of passes over the stream. In network applications, the algorithm is typically only given a single pass, since if data on a network is not physically stored somewhere, it may be impossible to make a second pass over it. In other applications, such as when data resides on external memory, it may be streamed through main memory a small number of times, each time constituting a pass of the algorithm. The algorithm would like to compute a function or relation of the data stream. Because of the sheer size, for many interesting problems the algorithm is necessarily randomized and approximate in order to be efficient. One should note that the randomness is in the random coin tosses of the algorithm rather than in the stream. That is, with high probability over the coin tosses of the algorithm, the algorithm should correctly compute the function or relation for any stream that is presented to it. This is more robust than say, if the algorithm assumed particular orderings of the stream that could make the problem easier to solve. In this survey we focus on computing or approximating order-independent functions f (a1, . . . , am). A function is order-independent if applying any permutation to its inputs results in the same function value. As we will see, this is often the case in numerical applications, such as if one is interested in the number of distinct values in the sequence a1, . . . , am. One of the main goals of a streaming algorithm is to use as little memory as possible in order to compute or approximate the function of interest. The amount of memory used (in bits) is referred to as the space complexity of the algorithm. While it is always possible for the algorithm to store the entire sequence a1, . . . , am, this is usually extremely prohibitive in applications. For example, internet routers often have limited resources; asking them to store a massive sequence of network traffic is infeasible. Another goal of streaming algorithms is their processing time, i.e., how often it takes to update their memory contents when presented with a new item in the stream. Often items in streams are presented at very high speeds and the algorithm needs to quickly update the data structures in its memory in order to be ready to process future updates. For order-independent functions, we can think of the stream as an evolution of an underlying vector x ∈ Rn. That is, x is initialized to the all zero vector, and when the item i appears in the stream, x undergoes the update	approximation algorithm;big data;communication complexity;compressed sensing;computer data storage;computer science;dspace;data structure;graph (discrete mathematics);graph theory;icalp;internet;machine learning;network traffic control;numerical analysis;numerical linear algebra;randomized algorithm;randomness;requirement;sampling (signal processing);simulation;sparse matrix;streaming algorithm;streaming media;the matrix;web search engine;with high probability	David P. Woodruff	2014	Bulletin of the EATCS		data science	DB	-2.449916784863679	-37.14048789204715	166690
628efca275504447715ff8d89718aa17c51f815f	on facts versus misconceptions about rough sets	rough set		rough set	Igor Kononenko	1996	Informatica (Slovenia)		machine learning;artificial intelligence;computer science;rough set	AI	1.0674574043236655	-26.649521647371568	166702
81ba2d5b73fcb98ff0ae72212ed100f4257a1ed4	a hybrid approach for efficient ensembles	extraction information;analisis envolvimiento datos;apilamiento;modele agrege;analisis datos;information extraction;ensemble of classifiers;estimation non parametrique;modelo agregado;clasificador;data mining;classification;non parametric estimation;multiple classifiers;data analysis;hybrid approach;classifier;ensembles;stacking;fouille donnee;data envelopment analysis;classificateur;aggregate model;analyse donnee;modele donnee;estimacion no parametrica;data envelope analysis;busca dato;clasificacion;extraccion informacion;empilement;data models;analyse enveloppement donnee	An ensemble of classifiers, or a systematic combination of individual classifiers, often results in better classifications in comparison to a single classifier. However, the question regarding what classifiers should be chosen for a given situation to construct an optimal ensemble has often been debated. In addition, ensembles are often computationally expensive since they require the execution of multiple classifiers for a single classification task. To address these problems, we propose a hybrid approach for selecting and combining data mining models to construct ensembles by integrating Data Envelopment Analysis and stacking. Experimental results show the efficiency and effectiveness of the proposed approach.		Dan Zhu	2010	Decision Support Systems	10.1016/j.dss.2009.06.007	random subspace method;cascading classifiers;computer science;machine learning;pattern recognition;data mining;data envelopment analysis;information extraction	EDA	9.016720408848467	-33.62579921411884	167266
224bcbb6b6e29a0bd034cbd5e3dcb4e3ace5d6cd	a fast framework for abrupt change detection based on binary search trees and kolmogorov statistic		Change-Point (CP) detection has attracted considerable attention in the fields of data mining and statistics; it is very meaningful to discuss how to quickly and efficiently detect abrupt change from large-scale bioelectric signals. Currently, most of the existing methods, like Kolmogorov-Smirnov (KS) statistic and so forth, are time-consuming, especially for large-scale datasets. In this paper, we propose a fast framework for abrupt change detection based on binary search trees (BSTs) and a modified KS statistic, named BSTKS (binary search trees and Kolmogorov statistic). In this method, first, two binary search trees, termed as BSTcA and BSTcD, are constructed by multilevel Haar Wavelet Transform (HWT); second, three search criteria are introduced in terms of the statistic and variance fluctuations in the diagnosed time series; last, an optimal search path is detected from the root to leaf nodes of two BSTs. The studies on both the synthetic time series samples and the real electroencephalograph (EEG) recordings indicate that the proposed BSTKS can detect abrupt change more quickly and efficiently than KS, t-statistic (t), and Singular-Spectrum Analyses (SSA) methods, with the shortest computation time, the highest hit rate, the smallest error, and the highest accuracy out of four methods. This study suggests that the proposed BSTKS is very helpful for useful information inspection on all kinds of bioelectric time series signals.	binary search algorithm;computation;data mining;electroencephalograph;genealogical tree;h-wave therapy;haar wavelet;kolmogorov complexity;kolmogorov-smirnov test;name;path (variable);sample variance;short;singular;statistic (data);synthetic intelligence;thrombocytopenia;time complexity;time series;tree (data structure);trees (plant);wavelet transform;web search engine;windows legacy audio components	Jin-Peng Qi;Jie Qi;Qing Zhang	2016		10.1155/2016/8343187	econometrics;machine learning;data mining;mathematics;algorithm;statistics	Web+IR	-2.1428210125495015	-37.37467916825566	167407
329bdc6cc5c927f5d46be60d269b23600da27573	logic-based fuzzy neurocomputing with unineurons	fuzzy neural nets;unineurons;fuzzy set;rule based system;logic connectives operators;uninorms;fuzzy set theory;architectures of logic processing;fuzzy logic;fuzzy clustering;logic neurons;parametric optimization;or ness;granular fuzzy modeling;and ness of uninorms;calibration;connected operator;logical process;knowledge based systems;fuzzy model	In this paper, we introduce a new category of logic neurons- unineurons that are based on the concept of uninorms. As uninorms form a certain generalization of the generic categories of fuzzy set operators such as t-norms and t-conorms, the proposed unineurons inherit their logic processing capabilities which make them flexible and logically appealing. We discuss several fundamental categories of uninorms (such as UNI_or, UNI_and, and alike). In particular, we focus on the interpretability of networks composed of unineurons leading to several categories of rules to be exploited in rule-based systems. The learning aspects of the unineurons are presented along with detailed optimization schemes. Experimental results tackle two categories of problems such as: (a) a logic approximation of fuzzy sets, and (b) a design of associations between information granules where the ensuing development schemes directly relate to the fundamentals of granular (fuzzy) modeling	approximation;emoticon;fuzzy set;mathematical optimization;neurocomputing;rule-based system;t-norm	Witold Pedrycz	2006	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2006.879977	rule-based system;discrete mathematics;computer science;artificial intelligence;fuzzy number;knowledge-based systems;mathematics;fuzzy set;algorithm	EDA	1.133068938267416	-25.803717386528586	167644
017c193a9e05f705307bbaeef676221768cc9b24	a fuzzy time series forecasting method induced by intuitionistic fuzzy sets	fuzzy time series;intuitionistic fuzzy sets;nondeterminacy;induced fuzzy set;data models	Intuitionistic fuzzy sets (IFSs) are well established as a tool to handle the hesitation in the decision system. In this research paper, fuzzy sets induced by IFS are used to develop a fuzzy time series forecasting model to incorporate degree of hesitation (nondeterminacy). To improve the forecasting accuracy, induced fuzzy sets are used to establish fuzzy logical relations. To verify the performance of the proposed model, it is implemented on one of the benchmarking time series data. Further, developed forecasting method is also tested and validated by applying it on a financial time series data. In order to show the accuracy in forecasting, the method is compared with other forecasting methods using different error measures.	fuzzy set;time series	Sanjay Kumar;Sukhdev Singh Gangwar	2015	IJMSSC	10.1142/S1793962315500415	fuzzy logic;data modeling;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations	ML	2.810503325193419	-25.94951581589229	168289
ea6116af52daf6b71e0b4f4f46d32706865c908c	confidence bounds of petrophysical predictions from conventional neural networks	tratamiento datos;reservoirs;prevision;learning algorithm;linguistic variable;metodologia;porosite;neural networks;neural nets;permeabilite;logique floue;neural networks backpropagation algorithms petroleum reservoirs artificial neural networks geology predictive models australia fuzzy logic training data;hydrocarbure;diagraphie;petrophysics;data processing;traitement donnee;geophysics computing neural nets geophysical prospecting;well logging;indexing terms;backpropagation;petroleo;classification;methodologie;puits forage;regresion;diagrafia;algorithme;fuzzy logic;petroleum;regression;geophysics computing;petrole;hydrocarbons;algorithms;permeabilidad;boreholes;permeability;petrofisica;reseau neuronal;methodology;porosity;porosidad;prediction;petrophysique;prediction confidence confidence bounds petrophysical predictions petroleum reservoirs neural networks backpropagation networks data pre processing algorithms linguistic variables classification network output activations averaging algorithm spreadsheets data post processing algorithms fuzzy logic geosciences complex regression problems;clasificacion;red neuronal;reservoir;hidrocarburo;geophysical prospecting;neural network;algoritmo	Neural networks are powerful tools for solving the complex regression problems which abound in geosciences. Estimation of prediction confidence from neural networks is an important area. Many procedures are available to date, but it is often tedious for practitioners to implement such procedures without significant modification of the existing learning algorithms. In many cases, the procedures are also computationally intensive. This paper presents a practical solution using conventional backpropagation networks with simple data pre-processing and post-processing algorithms. The methodology involves conversion of the target outputs into linguistic variables (classes) prior to learning. When the classification network converges, minimum and maximum predictions are derived from the output activations using a simple averaging algorithm. Two examples from petroleum reservoirs are used to demonstrate the proposed methodology. The results show that the confidence bounds of the petrophysical predictions are realistic in both cases. The proposed methodology is generally useful, and can be implemented in simple spreadsheets without altering any existing neural network code.	algorithm;artificial neural network;backpropagation;data pre-processing;machine learning;neural network software;preprocessor;spreadsheet;subroutine;video post-processing	Patrick M. Wong;Alexander G. Bruce;Tamás D. Gedeon	2002	IEEE Trans. Geoscience and Remote Sensing	10.1109/TGRS.2002.800278	computer science;artificial intelligence;machine learning;data mining;artificial neural network;statistics;reservoir	ML	9.468689256150014	-29.628782108551796	168504
e7af32a43389102d3a35c8fe2308ae91b4eec7bd	modeling unreliable data and sensors: using f-measure attribute performance with test samples from low-cost sensors	fwi;sensor phenomena and characterization meteorology mathematical model accuracy indexes barium;forestry;supervised learning;sensors;sensor networks sampling sensors dataming machine learning ranking functions ir event modeling forest fires fwi temporal patterns;temporal patterns;data collection;weather forecasting;dataming;data mining;sensor network;ranking function;forest fires;empirical evidence;knowledge discovery high performance classifier training data labeled data unlabeled data machine learning algorithm weight ranking hidden pattern future event prediction weather pattern automated weather prediction algorithm supervised learning noisy weather data low cost sensors forest fire event manual data logs geographical area online sensors fire weather index fwi forest fire classifier noisy sensor measurements parameter estimation peak fire seasons domain specific knowledge;machine learning;sensor networks;fire weather index;ir;seasonality;forest fire;temporal pattern;ranking functions;event modeling;pattern classification;cross validation;computerised instrumentation;learning artificial intelligence;sampling methods;fires;sampling sensors;high performance;domain specificity;weather forecasting computerised instrumentation data mining fires forestry learning artificial intelligence pattern classification sampling methods sensors;knowledge discovery	Building a high performance classifier requires training with labeled data, which is supervised and allows generalizing the classifier's decision boundary and in practice most of the data is unlabeled, newer algorithms needs to be learn by knowledge discovery. Sufficient training data are collected in the form of empirical evidence, which have labeled positive and negative samples to build the hypothesis. The hypothesis is constructed by the conjunction of the attributes, which can be learnt by machine learning algorithm. In this paper, we work with two forms of ranking weights, precision and relevance, which help in finding hidden patterns and prediction future events. Empirical evidence for a weather patterns and tracking of a phenomenon needs to accurately extract the attributes and label the training samples, which is a very laborious and time-consuming effort. Automating weather prediction algorithms, which are trained by supervised learning, needs to be generalized so that it can be tested with unreliable and noisy weather data from low cost sensors. We use a training data from previous forest fires events, the datasets containing all the attributes are labeled using manual data logs for a given geographical area. The labeled original dataset is mapped to the data collected from on-line sensors, which further improves the accuracy of the training set. As some of classes have very few samples, which are related to the peak fire seasons, domain specific knowledge are added by sensor measurements and Fire Weather Index (FWI) to help accurately model the events. We show that training accuracy of the small forest fire classifier using attributes from manual logs is enhanced by 30% by using sensor data. The rare and hard to classify large forest fires are 95% accurately classified by using the new Fire Weather Index (FWI). We also show that our framework is more robust to outliers from noisy sensor measurements by accounting for in the model parameters. The model allows further generalization for linearly and non-linearly separable datasets by estimating the parameters d and minimum allowable error ? for hypothesis, sampling accuracy and cross validation.	algorithm;baton;comment (computer programming);cross-validation (statistics);data mining;decision boundary;f1 score;geographic coordinate system;linear separability;loss function;machine learning;national lidar dataset;nonlinear system;online and offline;rouge (metric);relevance;sampling (signal processing);sensor;software deployment;statistical classification;supervised learning;test set;weka	Vasanth Iyer;S. Sitharama Iyengar	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.124	wireless sensor network;computer science;data science;machine learning;data mining;supervised learning;statistics	ML	0.8032748294531455	-35.17711091629139	168561
6bdcbe99754fe50876cdc2b1742724932713c412	experience-consistent modeling for radial basis function neural networks	experience consistent neural network;consistency model;radial basis function neural network;knowledge transfer;data quality	"""We develop a new approach to the design of neural networks, which utilizes a collaborative framework of knowledge-driven experience. In contrast to the """"standard"""" way of developing neural networks, which explicitly exploits experimental data, this approach incorporates a mechanism of knowledge-driven experience. The essence of the proposed scheme of learning is to take advantage of the parameters (connections) of neural networks built in the past for the same phenomenon (which might also exhibit some variability over time or space) for which are interested to construct the network on a basis of currently available data. We establish a conceptual and algorithmic framework to reconcile these two essential sources of information (data and knowledge) in the process of the development of the network. To make a presentation more focused and come up with a detailed quantification of the resulting architecture, we concentrate on the experience-based design of radial basis function neural networks (RBFNNs). We introduce several performance indexes to quantify an effect of utilization of the knowledge residing within the connections of the networks and establish an optimal level of their use. Experimental results are presented for low-dimensional synthetic data and selected datasets available at the Machine Learning Repository."""		Witold Pedrycz;Partab Rai;Jacek M. Zurada	2008	International journal of neural systems	10.1142/S0129065708001592	stochastic neural network;nervous system network models;data quality;computer science;artificial intelligence;consistency model;machine learning;data mining;deep learning	ML	5.6361215530042035	-33.54232411846592	168767
86034060b1c95e347c40e60bec15256eb0472f39	a supervised energy monitoring-based machine learning approach for anomaly detection in a clean water supply system		Industrial Control Systems are part of our daily life in industries such as transportation, water, gas, oil, smart cities, and telecommunications. Technological development over time have improved their components including operating system platforms, hardware capabilities, and connectivity with networks inside and outside the organization. Consequently, the Industrial Control Systems components are exposed to sophisticated threats with weak security mechanism in place. This paper proposes a supervised energy monitoring-based machine learning approach for anomaly detection in a clean water supply system. A testbed of such a system is implemented using the Festo MPA Control Process Rig. The machine-learning algorithms, which include SVN, KNN, and Random Forest, perform classification tasks process in three different datasets obtained from the testbed. The algorithms are compared in terms of accuracy and F-measure. The results show that Random Forest achieves 5% better performance over KNN and SVM with small datasets and 4% regarding large datasets. For the time taken to build the model, KNN presents the best performance. However, its difference with Random Forest is smaller than with SVM.		Andres Robles-Durazno;Naghmeh Moradpoor;James McWhinnie;G. Russell	2018	2018 International Conference on Cyber Security and Protection of Digital Services (Cyber Security)	10.1109/CyberSecPODS.2018.8560683	support vector machine;anomaly detection;industrial control system;machine learning;water supply;testbed;computer science;scada;artificial intelligence;random forest		6.820357213894237	-37.08294274678404	168856
577c984434ba6a883f2d03d3a0dee7e024dbc2ee	k- local maximum margin feature extraction algorithm for churn prediction in telecom		Telecom customer churn data is not publicly available because involving users’ personal privacy. In 2009, the French telecommunications company Orange for knowledge discovery and data mining (KDD) competition provides a telecom customer churn data set KDD Cup 09. In order to solve the high dimensional problem of KDD Cup 09, a new feature reduction method is used to explore the influence of different features on the prediction of classification model. In this paper, a new K- local maximum margin feature extraction algorithm (KLMM) is proposed. Through researching on the diversification subspace partition rules, the corresponding potential field structure is constructed. According to the data source in the dimension of scalability, the intrinsic link between data attributes and classification results is revealed. The extracted features can reduce the dimension of the churn prediction in telecom data. The KLMM method adapts auto selection sigma factor to reflect the anisotropy of features. The potential function is used to assess the weights of attributes and find the potential important weight. Experiments and analysis show that the extracted features by KLMM are more likely to find a classification hyperplane which can separate data points of the different classes.	algorithm;data mining;data point;dimensionality reduction;diversification (finance);equivalence partitioning;euclidean distance;experiment;feature extraction;maxima and minima;potential method;privacy;sigkdd;scalability	Long Zhao;Qian Gao;Xiangjun Dong;Aimei Dong;Xue Dong	2017	Cluster Computing	10.1007/s10586-017-0843-2	data science;machine learning;data mining	ML	3.8557556265919866	-36.51564487800561	169050
f1c379ff4ab13c5445229178fbc1f041b0743b5c	linguistically oriented fuzzy logic control and its design	fuzzy controller;fuzzy set;fuzzy logic control;fuzzy logic;inference rule;approximate reasoning;membership function	Abstract   The main advantage of the classical fuzzy controller (FLC) should be the ease of its design, which is close to the human way of thinking. However, tuning its performance requires modification of membership functions, and thus the result may be very far from the original linguistic description. In this paper, we analyze the standard Max-t-norm interpolation and compare it with logical inference. Several logical inference rules suitable for approximate reasoning are presented. We deal with the idea of a fuzzy controller that is both linguistic and logical in the highest possible degree (LFLC). It interprets  if-then  rules as linguistically expressed logical implications which are treated as special axioms in fuzzy logic. Therefore, it is more closely related to the human operator's language, which is “understood” by LFLC without the need to specify and modify the membership functions of fuzzy sets of inputs and outputs.	fuzzy logic;logic control	Vilém Novák	1995	Int. J. Approx. Reasoning	10.1016/0888-613X(94)00037-4	fuzzy logic;t-norm fuzzy logics;combs method;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control language;algorithm;fuzzy control system;rule of inference	AI	0.63218247850354	-25.801620402317006	169205
e8dd02b0b835c17ea320c0648d45ca18d63ef49b	fuzzy rough incremental attribute reduction applying dependency measures		Since data increases with time and space, many incremental rough based reduction techniques have been proposed. In these techniques, some focus on knowledge representation on the increasing data, some focus on inducing rules from the increasing data. Whereas there is less work on incremental feature selection (i.e., attribute reduction) from the increasing data, especially the increasing real valued data. And fuzzy rough sets is then applied in this incremental method because fuzzy rough set can effectively reduce attributes from the real valued data. By analyzing the basic concepts, such as lower approximation and positive region, of fuzzy rough sets on incremental datasets, the incremental mechanisms of these concepts are then proposed. An incremental algorithm is then designed. Finally, some numerical experiments demonstrate that the incremental algorithm is effective and efficient compared to nonincremental attribute reduction algorithms, especially on the datasets with large number of attributes.	algorithm;approximation;experiment;feature selection;genetic algorithm;knowledge representation and reasoning;numerical analysis;rough set	Yangming Liu;Suyun Zhao;Hong Chen;Cuiping Li;Yanmin Lu	2017		10.1007/978-3-319-63579-8_37	data mining;computer science;incremental learning;machine learning;artificial intelligence;fuzzy logic;knowledge representation and reasoning;feature selection;rough set	DB	-2.2339836015413495	-28.5271202039903	169385
e44f626344301957053d3efb8cf416383d142306	fuzzy geometric spaces associated to fuzzy hypersemigroups				R. Ameri;M. Asghari-Larimi;N. Firouzkouhi	2018	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-162444	discrete mathematics;machine learning;mathematics;fuzzy logic;artificial intelligence	Robotics	1.688616191139054	-24.11537822674106	169691
3d9284462ee5f216a212cb0995c197db320878c6	a recycle technique of association rule for missing value completion	distributed database;data mining distributed databases;recycling association rules distributed databases internetworking accuracy information management chaos database systems management information systems cleaning;data mining;distributed database system;global information systems;association rule;missing value;distributed databases;internetworking;missing values;recycle technique;validity association rule missing value completion distributed database rule recycle technique attribute value association rules	The technologies of internetworking and database have expedited the distributed database systems to be implemented in the global information systems. But it cannot avoid to occur random missing values in the process of gathering data. The research on association rules can effectively establish the relationship of attribute in databases containing missing values. While this technique is used to resolve the issues on replacing the missing values, the integrity of the methodology and the measurability of the quality can already be verified in related essays. However, the accuracy of replaced missing values in the database can only be satisfactory with specific criteria and a higher completion rate cannot yet be achieved. In other words, a given accuracy cannot be reached at the same time as resolving the database validity issues. Therefore, we propose a Rule Recycle Technique, which reuses and composes the rules in order to retrieve more complete attribute value association rules. This will not only enable the database recovered to improve the accuracy and completion rate but also increase the validity of missing value completion.	association rule learning;distributed database;global information system;internetworking;missing data	Jau-Ji Shen;Ming-Tsung Chen	2003		10.1109/AINA.2003.1192936	global information system;association rule learning;computer science;data mining;database;distributed database;information retrieval	DB	-3.782946391763278	-28.918117474654373	169695
811cc4a64c3cdb253cc91e5ea30a52059b531839	an intrusion detection system based on hierarchical self-organization		An intrusion detection system (IDS) monitors the IP packets flowing over the network to capture intrusions or anomalies. One of the techniques used for anomaly detection is building statistical models using metrics derived from observation of the user's actions. A neural network model based on self organization is proposed for detecting intrusions. The selforganizing map (SOM) has shown to be successful for the analysis of high-dimensional input data as in data mining applications such as network security. The proposed growing hierarchical SOM (GHSOM) addresses the limitations of the SOM related to the static architecture of this model. The GHSOM is an artificial neural network model with hierarchical architecture composed of independent growing SOMs. Randomly selected subsets that contain both attacks and normal records from the KDD Cup 1999 benchmark are used for training the proposed	anomaly detection;artificial neural network;benchmark (computing);data mining;data structure;disk mirroring;euclidean distance;intrusion detection system;level of measurement;map;network model;network security;numerical analysis;organizing (structure);randomness;self-organization;sensor;simulation;statistical model	Esteban J. Palomo;Enrique Domínguez;Rafael Marcos Luque Baena;José Muñoz	2008		10.1007/978-3-540-88181-0_18	engineering;artificial intelligence;machine learning;data mining	ML	6.6004815172822	-36.44653165495134	169726
c0704f11e099a4e04eaebff75ea9da584d3fd5b4	complexity-entropy causality plane: a useful approach for distinguishing songs	complexity measure;time series analysis;permutation entropy;music	Nowadays we are often faced with huge databases resulting from the rapid growth of data storage technologies. This is particularly true when dealing with music databases. In this context, it is essential to have techniques and tools able to discriminate properties from these massive sets. In this work, we report on a statistical analysis of more than ten thousand songs aiming to obtain a complexity hierarchy. Our approach is based on the estimation of the permutation entropy combined with an intensive complexity measure, building up the complexity-entropy causality plane. The results obtained indicate that this representation space is very promising to discriminate songs as well as to allow a relative quantitative comparison among songs. Additionally, we believe that the here-reported method may be applied in practical situations since it is simple, robust and has a fast numerical implementation.	blum axioms;causality;computational complexity theory;computer data storage;database;information theory;lz77 and lz78;list of online music databases;numerical analysis;response surface methodology	Haroldo V. Ribeiro;Luciano Zunino;Renio S. Mendes;Ervin K. Lenzi	2011	CoRR	10.1016/j.physa.2011.12.009	music;mathematics;algorithm;statistics	ML	-2.0272749032870903	-37.23386344230762	170185
e34a5fac59f5c14334f8395d22a31c14a651b885	classifying high-speed data streams using statistical decision trees	vfdt;starminer tree;classification;data stream mining;incremental decision trees;automatic starminer tree	Every day a large amount of data is collected by applications such as credit card transactions, monitoring networks and sensors. This type of data, called data streams, are generated in an automatic way, and its storage and knowledge extraction techniques differ from those used on traditional data. The classification task builds a model to describe and distinguish classes of data. In the context of data stream classification, many incremental techniques have been proposed. The existent methods tent to improve the classification accuracy as the number of processed examples increases. However, this characteristic makes the techniques conservative when the dataset is not too big, since they are dependent on the amount of data available. In this work we propose two algorithms that are not dependent on the number of examples read and that present a high accuracy and low execution time. We describe an incremental decision tree algorithm called StARMiner Tree (ST), which is based on Very Fast Decision Tree (VFDT) system, deals with numerical data and uses a method based on statistics as the heuristic to decide when to split a node, and also to choose the best attribute to be used in the test node. We also present a non-parametric version of ST called AST. We applied ST and AST in four datasets, one synthetic and three real-world, comparing their performance to the VFDT and VFDTcNB, which is an extension of VFDT and uses Naïve Bayes in the leaves. In all experiments ST and AST achieved better accuracy results, dealing well with noise data, describing the data from the earliest examples and maintaining a good execution time. The obtained results indicate that ST and AST are well-suited for data streams classification.		Mirela Teixeira Cazzolato;Marcela Xavier Ribeiro	2014	JIDM		computer science;machine learning;incremental decision tree;data mining;database	ML	-1.6732472018758515	-36.85549702565017	170337
574a7a0ab8b877469f37b2438a5676a1b5b7b59d	a rule based approach to group recommender systems	rule based systems;machine learning;recommender systems	The problem of building Recommender Systems has attracted considerable attention in recent years, but most recommender systems are designed for recommending items for individuals. In this paper we develop a content based group recommender system that can recommend TV shows to a group of users. We propose a method that uses decision list rule learner (DLRL) based on Ripper to learn the rule base from user viewing history and a method called RTL strategy based on social choice theory strategies to generate group ratings. We compare our learning algorithm with the existing C4.5 rule learner and the experimental results show that the performance of our rule learner is better in terms of literals learned (size of the rule set) and our rule learner takes time that is linear to the number of training examples.	c4.5 algorithm;decision list;recommender system;ripper;rule-based system	Vineet Padmanabhan;Siva Krishna Seemala;Wilson Naik Bhukya	2011		10.1007/978-3-642-25725-4_3	rule-based system;computer science;artificial intelligence;machine learning;data mining	AI	-2.033385647153544	-29.380841178446506	170436
6b7fcbbe3e8fa205731999d97c2f74973a45d3fb	echidna: efficient clustering of hierarchical data for network traffic analysis	ecoulement trafic;distributed system;hierarchical clustering;analyse amas;systeme reparti;classification non supervisee;gestion red;gestion trafic;hierarchical data;traffic flow;traffic management;data mining;network analysis;hierarchical classification;flujo red;sistema repartido;cluster analysis;network traffic analysis;fouille donnee;network traffic;clasificacion no supervisada;gestion reseau;classification hierarchique;gestion trafico;unsupervised classification;analisis cluster;network management;network flow;analyse circuit;clasificacion jerarquizada;flujo trafico;busca dato;flot reseau;analisis circuito	There is significant interest in the network management community about the need to improve existing techniques for clustering multi-variate network traffic flow records so that we can quickly infer underlying traffic patterns. In this paper we investigate the use of clustering techniques to identify interesting traffic patterns in an efficient manner. We develop a framework to deal with mixed type attributes including numerical, categorical and hierarchical attributes for a one-pass hierarchical clustering algorithm. We demonstrate the improved accuracy and efficiency of our approach in comparison to previous work on clustering network traffic.	algorithm;cluster analysis;hierarchical clustering;network packet;network traffic control;numerical analysis;traffic analysis	Abdun Naser Mahmood;Christopher Leckie;Parampalli Udaya	2006		10.1007/11753810_92	network management;correlation clustering;constrained clustering;active traffic management;data stream clustering;flow network;fuzzy clustering;network analysis;telecommunications;computer science;artificial intelligence;canopy clustering algorithm;machine learning;hierarchical network model;traffic flow;consensus clustering;cure data clustering algorithm;data mining;hierarchical clustering;cluster analysis;brown clustering;hierarchical database model;hierarchical clustering of networks;clustering high-dimensional data;conceptual clustering	ML	-3.257302696922379	-33.07740701721423	170592
f0fb3ba503a6d7ea03c8c9e8affea4d6c63ee007	intrusion detection based on organizational coevolutionary fuzzy classifiers	fuzzy classification;anomaly detection;intrusion detection;organization coevolutionary;fuzzy logic;fuzzy classifier	To solve the intrusion detection question, we introduce the fuzzy logic into Organization CoEvolutionary algorithm[1] and present the algorithm of Organization CoEvolutionary Fuzzy Classification (OCEFC). In this paper, we give an intrusion detection models based on OCEFC. After illustrating our model and applying it to the real-world network datasets KDD Cup 1999, we obtain the better performance than other traditional methods.	intrusion detection system	Liu Fang;Chen Zhen-guo	2004		10.1007/0-387-23152-8_22	fuzzy classification;computer science;artificial intelligence;machine learning;data mining	NLP	7.620354114703551	-37.52061122925705	171383
7569372c521199953247e862e081eba4b7d0fdf3	rough neural network based on bottom-up fuzzy rough data analysis	top down method;methode descendante;modelizacion;metodo adaptativo;cluster algorithm;bottom up method;rough logic neural network;analyse amas;bottom up;fuzzy set;metodo ascendente;partition donnee;analisis datos;defecto;top down;algoritmo borroso;rough set theory;logique floue;data partition;conjunto difuso;logica difusa;prise de decision;ensemble flou;methode adaptative;circuito logico;logical programming;decision quantifiee;methode ascendante;data model;fuzzy logic;modelisation;fuzzy clustering;data analysis;structure reseau;cluster analysis;programmation logique;circuit logique;theorie ensemble approximatif;metodo descendente;fuzzy algorithm;defect;adaptive method;adaptive learning;fuzzy rough data model;defaut;algorithme flou;decision programada;rough sets;analyse donnee;analisis cluster;modele donnee;information system;network structure;reseau neuronal;rough set;toma decision;programacion logica;modeling;logic circuit;soft decision;red neuronal;systeme information;gaustafason kessel g k clustering;particion dato;data models;neural network;sistema informacion	Based on bottom-up fuzzy rough data analysis, a new rough neural network decision-making model is proposed. Through supervised Gaustafason–Kessel (G–K) clustering algorithm, proper fuzzy clusters are found to partition the input data space. At the same time cluster number is searched by monotone increasing process. If the cluster number matches with that exactly exist in data sets then excellent fuzzy rough data modeling (FRDM) model can be built. And by integrating it with neural network technique, corresponding rough neural network is constructed. Our method overcomes the defects of conventional top-down based rough logic neural network (RLNN) method, and it also achieves adaptive learning ability and comprehensive soft decision-making ability compared with FRDM model. The experiment results indicate that our method has stronger generalization ability and more compact network structure than conventional RLNN.	algorithm;algorithmic efficiency;artificial neural network;bottom-up proteomics;cluster analysis;computation;data modeling;dataspaces;discretization;fuzzy clustering;iterative method;online and offline;software bug;top-down and bottom-up design;monotone	Dongbo Zhang;Yaonan Wang	2009	Neural Processing Letters	10.1007/s11063-009-9118-0	rough set;computer science;artificial intelligence;neuro-fuzzy;machine learning;mathematics;artificial neural network;algorithm	ML	9.479647179687893	-30.116015069010025	171624
f4fbca46a26087a2cfa99b6438613169bce59c2a	reliefmss: a variation on a feature ranking relieff algorithm	ruido aleatorio;relief algorithms;bruit aleatoire;attribute estimators;data mining;vecino mas cercano;random noise;hierarchical classification;fouille donnee;classification hierarchique;plus proche voisin;number of nearest neighbours;nearest neighbour;feature selection;clasificacion jerarquizada;busca dato	Relief algorithms are successful attribute estimators. They are able to detect conditional dependencies between attributes and provide a unified view on the attribute estimation. In this paper, we propose a variant of ReliefF algorithm: ReliefMSS. We analyse the ReliefMSS parameters and compare ReliefF and ReliefMSS performances as regards the number of iterations, the number of random attributes, the noise effect, the number of nearest neighbours and the number of examples presented. We find that for the most of these parameters, ReliefMSS is better than ReliefF.	dijkstra's algorithm;iteration;performance;real life	Salim Chikhi;Sadek Benhammada	2009	IJBIDM	10.1504/IJBIDM.2009.029085	computer science;machine learning;pattern recognition;data mining;feature selection	AI	9.609479861257997	-35.03763860334738	172016
ec306f40126605652223a004d2c29c7900d27bc0	evaluation and comparison of classification techniques for network intrusion detection	false alarm;training;prediction algorithms;intrusion detection;data mining;security of data data mining pattern classification;noise measurement;classifier;kdd 99 dataset classification technique evaluation classification technique comparison network intrusion detection system logs data mining network ids domain noisy network environment threat detection threat classification nsl kdd dataset;ignored attack intrusion detection classifier prediction anomaly identification false alarm;noise classification algorithms noise measurement intrusion detection training algorithm design and analysis prediction algorithms;classification algorithms;pattern classification;ignored attack;anomaly identification;prediction;security of data;algorithm design and analysis;noise	Data mining provides a useful environment and set of tools for processing large datasets such as Intrusion Detection Systems (IDS) logs. Researchers improve existing IDS models by comparing the performance of various algorithms on these datasets. It is very important to keep in mind that an IDS often has to work in a noisy network environment. Network noise is one of the most challenging issues for efficient threat detection and classification. In this study, normal and noisy datasets for network IDS domain are used and various classification algorithms are evaluated. The results show that an evaluation of algorithms without noise is misleading for IDSs since algorithms that perform best without noise do not necessarily achieve the same in a realistic noisy environment. Moreover refined NSL KDD dataset allows a more realistic evaluation of various algorithms than the original KDD 99 dataset.	algorithm;data mining;intrusion detection system;threat (computer)	Sait Murat Giray;Aydin Goze Polat	2013	2013 IEEE 13th International Conference on Data Mining Workshops	10.1109/ICDMW.2013.83	statistical classification;prediction;computer science;machine learning;pattern recognition;data mining;statistics	DB	6.232663705025102	-37.476715494505065	172025
d58455b242a2b45bf6fa5c3e5717762ed8d4f80b	ameva: an autonomous discretization algorithm	supervised learning;machine learning;computational complexity;supervised discretization;genetic algorithm;knowledge discovery	This paper describes a new discretization algorithm, called Ameva, which is designed to work with supervised learning algorithms. Ameva maximizes a contingency coefficient based on Chi-square statistics and generates a potentially minimal number of discrete intervals. Its most important advantage, in contrast with several existing discretization algorithms, is that it does not need the user to indicate the number of intervals. We have compared Ameva with one of the most relevant discretization algorithms, CAIM. Tests performed comparing these two algorithms show that discrete attributes generated by the Ameva algorithm always have the lowest number of intervals, and even if the number of classes is high, the same computational complexity is maintained. A comparison between the Ameva and the genetic algorithm approaches has been also realized and there are very small differences between these iterative and combinatorial approaches, except when considering the execution time. 2008 Elsevier Ltd. All rights reserved.	chi;coefficient;computational complexity theory;contingency table;decision tree;discretization;genetic algorithm;goto;greedy algorithm;html attribute;interdependence;iterative method;machine learning;maxima and minima;run time (program lifecycle phase);sorting;supervised learning;terminate (software)	Luis González Abril;Francisco Javier Cuberos;Francisco Velasco Morente;Juan Antonio Ortega	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.06.063	discretization error;mathematical optimization;genetic algorithm;computer science;machine learning;pattern recognition;discretization;mathematics;supervised learning;computational complexity theory;discretization of continuous features	AI	1.9666937677532983	-35.748455642206665	172109
ff140190dd49d57f224aa3f336a978433c379692	a proposal on analysis support system based on association rule analysis for non-dominated solutions	association rule grouping;trees mathematics aerospace computing data mining evolutionary computation propellants rockets;market research;evolutionary computation;ndss;rockets;concept design problem;nondominated solutions;association rules;trees mathematics;data mining;hierarchical tree;analysis support system;multigranularity analysis;engines;emo;aerospace computing;evolutionary multicriterion optimization;feature extraction;variable parameters;data visualization;hybrid rocket engine;hre;optimization;association rule analysis;inclusion relations;jaxa;jaxa analysis support system association rule analysis nondominated solutions ndss evolutionary multicriterion optimization emo multigranularity analysis hierarchical tree association rule grouping inclusion relations hybrid rocket engine hre concept design problem variable parameters thrust propellant;thrust propellant;propellants;association rules feature extraction rockets data visualization engines market research optimization	This paper presents a new analysis support system for analyzing non-dominated solutions (NDSs) derived by evolutionary multi-criterion optimization (EMO). The main features of the proposed system are to use association rule analysis and to perform a multi-granularity analysis based on a hierarchical tree of NDSs. The proposed system applies association rule analysis to the whole NDSs and derives association rules related to NDSs. And a hierarchical tree is created through our original association rule grouping that guarantees to keep at least one common features. Each node of a hierarchical tree corresponds to one group consisting of association rules and is fixed in position according to inclusion relations between nodes. Since each node has some kinds of common features, the designer can analyze each node with previous knowledge of these common features. To investigate the characteristics and effectiveness of the proposed system, the proposed system is applied to the concept design problem of hybrid rocket engine (HRE) which has two objectives and six variable parameters. HRE separately stores two different types of thrust propellant unlike in the case of usual other rockets and the concept design problem of HRE has been provided by JAXA. The results of this application provided possible to analyze the trends and specifics contained in NDSs in an organized way unlike analysis approaches targeted at the whole NDSs.	association rule learning;discretization;formal concept analysis;mathematical optimization;non-repudiation;recursion;rough set;thrust;tree structure	Shinya Watanabe;Yuta Chiba;Masahiro Kanazaki	2014	2014 IEEE Congress on Evolutionary Computation (CEC)	10.1109/CEC.2014.6900650	market research;mathematical optimization;simulation;association rule learning;feature extraction;computer science;machine learning;propellant;operations research;data visualization;evolutionary computation	Robotics	-1.767687562033847	-29.865562838666126	172308
3d107c33441ed301b3163477b7a447c04503a8d6	intrusion detection based on back-propagation neural network and feature selection mechanism	intrusion detection;independent component analysis;back propagation neural network;system performance;feature selection;information system;network intrusion detection system;intrusion detection system;neural network	Intrusion detection is a critical component of secure information systems. Current intrusion detection systems (IDS) especially NIDS (Network Intrusion Detection System) examine all data features to detect intrusions. However, some of the features may be redundant or contribute little to the detection process and therefore they have an unnecessary negative impact on the system performance. This paper proposes a lightweight intrusion detection model that is computationally efficient and effective based on feature selection and back-propagation neural network (BPNN). Firstly, the issue of identifying important input features based on independent component analysis (ICA) is addressed, because elimination of the insignificant and/or useless inputs leads to a simplification of the problem, therefore results in faster and more accurate detection. Secondly, classic BPNN is used to learn and detect intrusions using the selected important features. Experimental results on the well-known KDD Cup 1999 dataset demonstrate the proposed model is effective and can further improve the performance by reducing the computational cost without obvious deterioration of detection performances.	backpropagation;feature selection;intrusion detection system;software propagation	Ning-Qing Sun;Yang Li	2009		10.1007/978-3-642-10509-8_18	anomaly-based intrusion detection system;engineering;machine learning;pattern recognition;data mining	ML	7.495710330640002	-37.98394282706184	172343
cbedf63749ca369792bfffd980face4fb0faff66	extracting rules from artificial neural networks with kernel-based representation	representacion conocimientos;learning algorithm;simulation systeme;procesamiento informacion;algorithm performance;neural model;fuzzy rules;knowledge extraction;rule extraction;algorithme apprentissage;radial basis function;fonction radiale base;machine learning;resultado algoritmo;information processing;performance algorithme;membership function;support vector machine;neural network model;reseau neuronal;knowledge representation;coulomb potential;traitement information;representation connaissances;algoritmo aprendizaje;probabilistic neural network;system simulation;simulacion sistema;red neuronal;problem solving;artificial neural network;neural network	In Neural Networks models the knowledge synthesized from the training process is represented in a subsymbolic fashion (weights, kernels, combination of numerical descriptions) that makes difficult its interpretation. The interpretation of the internal representation of a successful Neural Network can be useful to understand the nature of the problem and its solution, to use the Neural model as a tool that gives insights about the problem solved and not just as a solving mechanism treated as a black box. The internal representation used by the family of kernel-based Neural Networks (including Radial Basis Functions, Support Vector machines, Coulomb potential methods, and some probabilistic Neural Networks) can be seen as a set of positive instances of classification and, thereafter, used to derive fuzzy rules suitable for explanation or inference processes. The probabilistic nature of the kernel-based Neural Networks is captured by the membership functions associated to the components of the rules extracted. In this work we propose a method to extract fuzzy rules from trained Neural Networks of the family mentioned; comparing the quality of the knowledge extracted by different methods using known machine learning benchmarks.	artificial neural network;kernel (operating system)	José M. Ramírez	1999		10.1007/BFb0100473	stochastic neural network;support vector machine;radial basis function;probabilistic neural network;types of artificial neural networks;membership function;adaptive neuro fuzzy inference system;computer science;artificial intelligence;neuro-fuzzy;machine learning;time delay neural network;mathematics;deep learning;artificial neural network;algorithm;electric potential	ML	9.70443223683113	-29.38506913387891	172364
6df5ac0e40b000e436d90e8b75d2a3b9c7e895c2	a behavior-based anti-spam technology based on immune-inspired clustering algorithm	cluster algorithm;spam filtering;false positive;spam detection	The paper describes a novel behavior-based anti-Spam technology at email service based on an immune-inspired clustering algorithm.  Compared with popular client anti-Spam filtering system based on content classification technology, our approach is capable  of continuously delivering the most relevant Spam from the collection of all Spam that is reported by members of the network.,  then mail servers shall implement anti-Spam technology by using the “Black lists” that have been recognized. Experiment are  discussed with real-world datasets, the conclusion have shown the technology is reliable, efficient and scalable, because  no single technology can achieve one hundred percent Spam detection with zero false positives, however, it can be used in  conjunction with other filtering systems to minimize errors.  	algorithm;anti-spam techniques;cluster analysis	Xun Yue;Zhong-Xian Chi;Zu-bo Yu	2005		10.1007/3-540-32391-0_29	machine learning;filter (signal processing);scalability;false positive paradox;cluster analysis;artificial intelligence;server;computer science	EDA	5.854983508986601	-37.846606468778155	172464
2be665672adee03b281b9cf97f695e34b064e89f	a neuro-computational intelligence analysis of the ecological footprint of nations	classification automatique statistiques;prediccion;metodo estadistico;theorie filtrage;analyse multivariable;medio ambiente;estimator robustness;stochastic process;analisis estadistico;multivariate analysis;analisis datos;62m20;computational intelligence;computer model;hombre;62m45;statistical method;statistical regression;62jxx;statistical model;algorithme;discriminant analysis;analyse discriminante;algorithm;data analysis;analisis discriminante;robustez estimador;environmental sustainability;red multinivel;analisis regresion;statistical analysis;prediction theory;general regression neural network;62h30;methode statistique;regresion estadistica;environment;statistical computation;analyse statistique;calculo estadistico;human;processus stochastique;modele statistique;discriminant;analyse regression;60g25;analisis multivariable;analyse donnee;environnement;multi layer perceptron;modelo estadistico;regression analysis;calcul statistique;statistical techniques;multilayer network;reseau multicouche;estimation statistique;reseau neuronal;proceso estocastico;theorie prediction;classification accuracy;regression statistique;62p12;estimacion estadistica;probabilistic neural network;ecological footprint;statistical estimation;prediction;human activity;red neuronal;filtering theory;homme;neural network;algoritmo;robustesse estimateur	The per capita ecological footprint (EF) is one of the most-widely recognized measures of environmental sustainability. It seeks to quantify the Earth's biological capacity required to support human activity. This study uses three neuro-computational methodologies: multi-layer perceptron neural network (MLP), probabilistic neural network (PNN) and generalized regression neural network (GRNN) to predict and classify the EF of 140 nations. Accuracy indices are used to assess the prediction and classification accuracy of the three methodologies. The study shows that neuro-computational models outperform traditional statistical techniques such as regression analysis and discriminant analysis in predicting and classifying per capita EF due to their robustness and flexibility of modeling algorithms.	computational intelligence	Mohamed M. Mostafa;Rajan Nataraajan	2009	Computational Statistics & Data Analysis	10.1016/j.csda.2009.03.003	stochastic process;econometrics;artificial intelligence;computational intelligence;mathematics;linear discriminant analysis;regression analysis;statistics	AI	9.758037982123874	-24.70845563773778	172678
4324548d6fecb32d0c8a715dc6f25c97c9f403b9	using a hybrid meta-evolutionary rule mining approach as a classification response model	statistical approach;decision models;meta evolutionary;knowledge discovery from databases;statistical method;data mining;classification;machine learning;model building;classification rules;decision rules;prediction accuracy;support vector machine;numerical experiment;classification accuracy;decision rule;neural network;expert system	Data mining usually means the approaches and appliances for the valid new knowledge discovery from databases. A response model can be built as a decision model for prediction or classification of a domain problem potential like expert systems. In this paper, a hybrid meta-evolutionary rule mining based approach to assess numerical data pattern in the classification problems is proposed for extracting the decision rules including the predictors, the corresponding inequalities and parameters simultaneously so as to building a decision-making model with maximum classification accuracy. In real world, problems are highly nonlinear in nature so that it's hard to develop a comprehensive model taking into account all the independent variables through the conventional statistical methods. Recently, nonlinear and complex machine learning approaches such as neural networks and support vector machines have been demonstrated to be with more reliable than the conventional statistical approaches. Although the usefulness of using neural networks and support machines has been reported in literatures, the most obstacles are in model building and use of model in which the classification rules are hard to be realized. Through two numerical experiments, we compared our results against the commercial data mining software and other methods in literature, and then we show experimentally that the proposed approach is promising for improving prediction accuracy and enhancing the modeling simplicity.		Ta-Cheng Chen;Huei-Ling Tsao	2009	Expert Syst. Appl.	10.1016/j.eswa.2007.12.050	computer science;artificial intelligence;machine learning;pattern recognition;data mining;decision rule;expert system;artificial neural network	ML	4.066803350567121	-27.541722089104272	172706
4cf1fc4258250978161f90f1f3a1372ddc503dcc	object order based on concept analysis and moebius inversion function	specific geographical area;important contribution;important key indicator;area description;moebius function;conjugate moebius inversion function;object order;certain important feature;new method;description method;concept analysis;pilot area	Ordering objects of interest according to a given criterion often provides a useful piece of information to solve a problem of information processing. A new method of linear order based on Formal Concept Analysis (FCA) and Moebius function is described. The description method uses an example of selecting important key indicators in a specific geographical area. An indicator is defined as a characteristic number (weight) representing in a unique way certain important features of the area in question. The weights are assigned to each indicator in different categories. Using FCA, interdependencies among the indicators are analysed and the indicators are sorted according to their importance and uniqueness in the area description. A new method based on Conjugate Moebius Inversion Function (CMI) was used to evaluate the set of indicators selected as representatives for a pilot area. The output of the method is a sequence of objects / indicators ordered according to their calculated value of importance of every indicator. The method described in the article appears to be an important contribution to the evaluation of regional competitiveness in the 5th Framework Programme RTD project “Iron Curtain”.	formal concept analysis;moebius: empire rising	Petr Gajdos;Daniela Duráková	2005			calculus;mathematics;algorithm	Logic	-3.688676384095082	-24.506243478130557	172797
6c381cabc2eb8d3543c7126d323d1793fca55308	kernel-based fuzzy clustering and fuzzy clustering: a comparative experimental study	kernels;fuzzy c mean;comparative analysis;fuzzy set;procesamiento informacion;learning;fuzzy kernel based clustering;etude experimentale;f space;classification rate;estudio comparativo;conjunto difuso;ensemble flou;gustafson kessel;polynomial;feature space;classification;algorithme;aprendizaje;etude comparative;algorithm;fuzzy clustering;apprentissage;estimation erreur;machine learning;parameter selection;error estimation;clustering method;evaluation criteria;polinomio;fuzzy c means;information processing;estimacion error;comparative study;gaussian kernel;reconstruction error;gustafson kessel fcm;sistema difuso;systeme flou;traitement information;polynome;estudio experimental;clasificacion;fuzzy system;espace f;algoritmo	In this study, we present a comprehensive comparative analysis of kernel-based fuzzy clustering and fuzzy clustering. Kernel based clustering has emerged as an interesting and quite visible alternative in fuzzy clustering, however, the effectiveness of this extension vis-à-vis some generic methods of fuzzy clustering has neither been discussed in a complete manner nor the performance of clustering quantified through a convincing comparative analysis. Our focal objective is to understand the performance gains and the importance of parameter selection for kernelized fuzzy clustering. Generic Fuzzy C-Means (FCM) and Gustafson–Kessel (GK) FCM are compared with two typical generalizations of kernel-based fuzzy clustering: one with prototypes located in the feature space (KFCM-F) and the other where the prototypes are distributed in the kernel space (KFCM-K). Both generalizations are studied when dealing with the Gaussian kernel while KFCM-K is also studied with the polynomial kernel. Two criteria are used in evaluating the performance of the clustering method and the resulting clusters, namely classification rate and reconstruction error. Through carefully selected experiments involving synthetic and Machine Learning repository (http://archive.ics.uci.edu/beta/) data sets, we demonstrate that the kernel-based FCM algorithms produce a marginal improvement over standard FCM and GK for most of the analyzed data sets. It has been observed that the kernel-based FCM algorithms are in a number of cases highly sensitive to the selection of specific values of the kernel parameters. © 2009 Elsevier B.V. All rights reserved.	algorithm;cluster analysis;experiment;focal (programming language);feature vector;fuzzy clustering;fuzzy cognitive map;gustafson's law;kernel method;machine learning;marginal model;mathematical optimization;polynomial kernel;qualitative comparative analysis;synthetic intelligence;test data;user space	Daniel Graves;Witold Pedrycz	2010	Fuzzy Sets and Systems	10.1016/j.fss.2009.10.021	qualitative comparative analysis;correlation clustering;constrained clustering;kernel method;data stream clustering;kernel embedding of distributions;feature vector;radial basis function kernel;k-medians clustering;information processing;fuzzy clustering;biological classification;flame clustering;computer science;canopy clustering algorithm;machine learning;comparative research;pattern recognition;cure data clustering algorithm;mathematics;fuzzy set;cluster analysis;single-linkage clustering;gaussian function;variable kernel density estimation;polynomial kernel;fuzzy control system;statistics;polynomial;clustering high-dimensional data	AI	9.698634112833652	-34.30129932273756	172821
32ae05a2876ab50643fe75a9eae5475af10bee05	summarization of spacecraft telemetry data by extracting significant temporal patterns	operateur humain;analyse amas;artificial satellite;satellite artificiel;operador humano;numerical method;base donnee temporelle;resumen;time series;classification;telemedida;cluster analysis;vehiculo espacial;metodo numerico;resume;temporal pattern;serie temporelle;human operator;serie temporal;point changement;telemesure;time series data;analisis cluster;temporal databases;telemetry;abstract;punto cambio;clasificacion;spacecraft;methode numerique;change point;spationef;satelite artificial	This paper presents a method to summarize massive spacecraft telemetry data by extracting significant event and change patterns in the lowlevel time-series data. This method first transforms the numerical timeseries into a symbol sequence by a clustering technique using DTW distance measure, then detects event patterns and change points in the sequence. We demonstrate that our method can successfully summarize the large telemetry data of an actual artificial satellite, and help human operators to understand the overall system behavior.	cluster analysis;numerical analysis;time series	Takehisa Yairi;Shiro Ogasawara;Koichi Hori;Shinichi Nakasuka;Naoki Ishihama	2004		10.1007/978-3-540-24775-3_31	computer science;artificial intelligence;time series;statistics	ML	-2.8076033471036967	-32.284833750451234	172839
d3e4bc3319166c42312357f28fc8cdc6fe45fbd7	relative reducts in consistent and inconsistent decision tables of the pawlak rough set model	general decision;classification quality;pawlak regions;relative relationship;consistent and inconsistent decision tables;attribute reduction;certainty of decision making;difference set;rough set;decision table;pawlak rough set model	A relative reduct can be considered as a minimum set of attributes that preserves a certain classification property. This paper investigates three different classification properties, and suggests three distinct definitions accordingly. In the Pawlak rough set model, while the three definitions yield the same set of relative reducts in consistent decision tables, they may result in different sets in inconsistent tables. Relative reduct construction can be carried out based on a discernibility matrix. The study explicitly stresses a fact, that the definition of a discernibility matrix should be tied to a certain property. Regarding the three classification properties, we can define three distinct definitions accordingly. Based on the common structure of the specific definitions of relative reducts and discernibility matrices, general definitions of relative reducts and discernibility matrices are suggested. 2009 Elsevier Inc. All rights reserved.	decision table;rough set	Duoqian Miao;Yan Zhao;Yiyu Yao;H. X. Li;Feifei Xu	2009	Inf. Sci.	10.1016/j.ins.2009.08.020	decision table;discrete mathematics;rough set;computer science;machine learning;pattern recognition;data mining;mathematics;difference set	Web+IR	-2.7619944144395	-25.538438130713114	172926
7b8bb80aa50b6eaae74df429a7be4c9d14e2ea9a	local spatial biclustering and prediction of urban juvenile delinquency and recidivism	neural networks;nonstationarity;two way clustering;plaid models;data mining;prodes database;biclustering;urban crime;getis ord statistic;neural network model;spatial clustering;spatial statistics;juvenile delinquency;neural network	Using a novel database, ProDES, developed by the Crime and Justice Research Center at Temple University, this article investigates the relationship between spatial characteristics and juvenile delinquency and recidivism—the proportion of delinquents who commit crimes following completion of a court-ordered program—in Philadelphia, PA. ProDES was originally a case-based sample, where the cases were adjudicated in family court, 1994–2004. For our analysis, we focused attention on studying 6768 juvenile males from the data set. To address the difficult issue of nonstationarity in the data, we considered various two-way clustering algorithms to group the juveniles into ‘types’ by way of the many variables that described the juveniles. Following different modeling scenarios, we applied the plaid biclustering algorithm in which a sequence of subsets (‘layers’) of both juveniles and variables are extracted from the data one layer at a time, but where overlapping layers are allowed. This type of ‘biclustering’ is a new way of studying juvenile-offense data. We show that the juveniles within each layer can be viewed as spatially clustered. The layers were determined as descriptive tools to aid in identifying subsets of the data that could be useful in policy making. Statistical relationships of the variables and juveniles within each layer are then studied using neural network models. Results indicate that the methods of this paper are more successful in predicting juvenile recidivism in urban environments when different crimes are modeled as separate data sets rather than being pooled together as a single data set.  2011 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 4: 259–275, 2011	algorithm;biclustering;cluster analysis;data mining;john d. wiley	Alan J. Izenman;Philip W. Harris;Jeremy Mennis;Joseph Jupin;Zoran Obradovic	2011	Statistical Analysis and Data Mining	10.1002/sam.10123	juvenile delinquency;computer science;artificial intelligence;machine learning;data mining;spatial analysis;operations research;artificial neural network;statistics	ML	2.6296401938075995	-31.994516478393372	172991
82db86ef770a63538e7905dcba9969546362fa7b	market basket analysis in a multiple store environment	operations strategy;etude marche;tratamiento datos;extraction information;analisis datos;information extraction;comercializacion;data processing;traitement donnee;market basket analysis;association rules;data mining;association rule mining;commercialisation;algorithm;data analysis;regle association;association rule;fouille donnee;marketing;market survey;estudio mercado;analyse donnee;analyse information;empirical evaluation;information analysis;busca dato;extraccion informacion;store chain	Market basket analysis (also known as association-rule mining) is a useful method of discovering customer purchasing patterns by extracting associations or co-occurrences from stores’ transactional databases. Because the information obtained from the analysis can be used in forming marketing, sales, service, and operation strategies, it has drawn increased research interest. The existing methods, however, may fail to discover important purchasing patterns in a multi-store environment, because of an implicit assumption that products under consideration are on shelf all the time across all stores. In this paper, we propose a new method to overcome this weakness. Our empirical evaluation shows that the proposed method is computationally efficient, and that it has advantage over the traditional method when stores are diverse in size, product mix changes rapidly over time, and larger numbers of stores and periods are considered. D 2004 Elsevier B.V. All rights reserved.	adjusted winner procedure;affinity analysis;algorithmic efficiency;apriori algorithm;association rule learning;data mining;database;moe;online and offline;procurement;purchasing;simulation;taxonomy (general);time complexity	Yen-Liang Chen;Kwei Tang;Ren-Jie Shen;Ya-Han Hu	2005	Decision Support Systems	10.1016/j.dss.2004.04.009	association rule learning;data processing;computer science;marketing;operations management;data mining;database;data analysis;information extraction	AI	-4.532803611716988	-33.985702862449	173183
893b19d8fdba0bbc6cc0337535936ff2c593ed73	the dsm approach as a special case of the dempster-shafer theory	belief function;dsm theory;belief functions;non separable elements;computational complexity;dempster shafer theory;overlapping elements;dempster shafer;exclusive elements;constraints	This contribution deals with a belief processing which enables managing of multiple and overlapping elements of a frame of discernment. An outline of the Dempster-Shafer theory for such cases is presented, including several types of constraints for simplification of its large computational complexity. DSmT — a new theory rapidly developing the last five years — is briefly introduced. Finally, it is shown that the DSmT is a special case of the general Dempster-Shafer approach.	computational complexity theory;level of detail;state space	Milan Daniel	2007		10.1007/978-3-540-75256-1_35	dempster–shafer theory;artificial intelligence;mathematics;algorithm;statistics	Graphics	-2.5182358269156366	-24.327215879726417	173254
e3fab004a9c66c474f9ea113513e6946db13e328	pattern classification method by integrating interval feature values	erbium;probability;bayesian statistics;bayes methods pattern classification decision theory probability;interval feature values;bayes methods;probability density function;probability law knowledge;pattern classification bayesian methods decision theory noise robustness artificial intelligence probability density function erbium;bayesian methods;noise robustness;integration algorithm pattern classification bayesian statistical decision theory interval feature values probability law knowledge constant feature values noise dempster shafer measure;decision theory;pattern classification;dempster shafer;artificial intelligence;dempster shafer measure;constant feature values;bayesian statistical decision theory;noise;integration algorithm	Pattern classification based on Bayesian statistical decision theory needs a complete knowledge of the probability laws to perform the classification. In the actual pattern classification, however, it is generally impossible to get the complete knowledge as constant feature values by the influence of noise. In this paper, a pattern classification theory using feature values defined on closed interval is formalized in the framework of Dempster-Shafer measure. Then, in order to make up lacked information, a new integration algorithm is	algorithm;decision theory;statistical classification	Takahiko Horiuchi	1997		10.1109/ICDAR.1997.620631	probability density function;erbium;dempster–shafer theory;feature;decision theory;bayesian probability;noise;machine learning;linear classifier;pattern recognition;probability;bayesian statistics;feature;statistics	AI	-0.8096941761243691	-27.67220197641704	173451
94cb653c534f86f5daac0796f6162447a163db2c	conformal sets in neural network regression		This paper is concerned with predictive regions in regression models, especially neural networks. We use the concept of conformal prediction (CP) to construct regions which satisfy given confidence level. Conformal prediction outputs regions, which are automatically valid, but their width and therefore usefulness depends on the used nonconformity measure. A nonconformity measure should tell us how different a given example is with respect to other examples. We define nonconformity measures based on some reliability estimates such as variance of a bagged model or local modeling of prediction error. We also present results of testing CP based on different nonconformity measures showing their usefulness and comparing them to traditional confidence intervals.	artificial neural network;computation;curve fitting;nonlinear system;radial (radio)	Radim Demut;Martin Holena	2012			statistics;regression analysis;probabilistic neural network;mathematics;nonconformity;artificial neural network;conformal map;mean squared prediction error;confidence interval	ML	7.160475800190369	-27.534295531166745	173647
e9317513e21e6372590354f02047976db3b71bfe	dependence space-based model for constructing dual concept lattice in sub-formal context		This paper mainly studies a dependence space-based model for constructing dual concept lattice in sub-formal context. Based on the operators of a dual concept lattice , a ∩−congruence on the power set of objects is defined, and ∩−dependence space is then obtained. By the ∩−congruence, dual concept granules and an inner operator decided by any subset of attributes are introduced. It is proved that each open element of the inner operator is just the minimal element in an dual concept granule. Furthermore, the open element is also the extension of a dual concept lattice.	formal concept analysis	Jianmin Ma	2012		10.1007/978-3-642-32115-3_41	combinatorics;discrete mathematics;dual pair;dual representation;mathematics	NLP	-1.8197337026759435	-24.550145564803632	173694
f30826bdd268399a1dc80c42dda82d8ddda407fb	a new rough set approach to multicriteria and multiattribute classification	business and management;multicriteria analysis;theorie approximation;computability;classification;computing;approximation theory;calculabilite;analisis multicriterio;analyse multicritere;point of view;rough set;business information systems;clasificacion;calculabilidad	As pointed out by Greco, Matarazzo and Slowinski [1] the original rough set approach does not consider criteria, i.e. attributes with ordered domains. However, in many real problems the ordering properties of the considered attributes may play an important role. E.g. in a bankruptcy evaluation problem, if firm A has a low value of the debt ratio (Total debt/Total assets) and firm B has a large value of the same ratio, within the original rough set approach the two firms are just discernible, but no preference is established between them two with respect to the attribute “debt ratio”. Instead, from a decisional point of view, it would be better to consider firm A as preferred to firm B, and not simply “discernible”, with respect to the attribute in question. Motivated by the previous considerations, Greco, Matarazzo and Slowinski [2] proposed a new rough set approach to take into account the ordering properties of criteria. Similarly to the original rough set analysis, the proposed approach is based on approximations of a partition of objects in some pre-defined categories. However, differently from the original approach, the categories are ordered from the best to the worst and the approximations are built using dominance relations, being specific order binary relations, instead of indiscernibility relations, being equivalence relation. The considered dominance relations are built on the basis of the information supplied by condition attributes which are all criteria. In this paper we generalize this approach considering a set of condition attributes which are not all criteria. The paper is organized in the following way. In the second section, the main concepts of the rough approximation based on criteria and attributes are introduced. In section 3 we apply the proposed approach to a didactic example to compare the results with the original rough set approach. Final section groups conclusions.	approximation;dominance drawing;ordered pair;relation (database);rough set;turing completeness	Salvatore Greco;Benedetto Matarazzo;Roman Slowinski	1998		10.1007/3-540-69115-4_9	computing;rough set;computer science;artificial intelligence;operating system;machine learning;management information systems;data mining;mathematics;computability;algorithm;dominance-based rough set approach;approximation theory	AI	-2.593448623550169	-25.35700692966612	173705
32da93e92b91337619d47ac804289506109bf201	nlc: a measure based on projections	critere selection;selection criterion;pertinencia;intelligence artificielle;criterio seleccion;classification;pertinence;artificial intelligence;feature selection;inteligencia artificial;relevance;clasificacion	In this paper, we propose a new feature selection criterion. It is based on the projections of data set elements onto each attribute. The main advantages are its speed and simplicity in the evaluation of the attributes. The measure allows features to be sorted in ascending order of importance in the definition of the class. In order to test the relevance of the new feature selection measure, we compare the results induced by several classifiers before and after applying the feature selection algorithms.	algorithm;algorithmic efficiency;computation;feature selection;natural language processing;relevance;sorting	Roberto Ruiz Sánchez;José Cristóbal Riquelme Santos;Jesús S. Aguilar-Ruiz	2003		10.1007/978-3-540-45227-0_88	relevance;biological classification;computer science;artificial intelligence;machine learning;pattern recognition;feature selection;law	Vision	8.473402924800755	-32.992185359448634	173732
6deba04ebe72740767bf6c19c28d4f95add87b48	neural network based effort estimation using class points for oo systems	software cost estimation;project management;neural network based effort estimation;object oriented products;neural networks;neural networks object oriented systems class points effort estimation regression analysis;design engineering;software measurement;neural nets;size measurement;regression model;object oriented programming;oo systems;object oriented systems;effort estimation;object oriented;phase estimation;software cost estimation neural nets object oriented programming project management;software projects;class points;lab on a chip;regression analysis;programming;object oriented modeling;software projects neural network based effort estimation class points oo systems object oriented products;neural network;neural networks object oriented modeling costs size measurement programming phase estimation design engineering regression analysis lab on a chip software measurement	Class points have been accepted to estimate the size of object oriented (OO) products and to directly predict the effort, cost and duration of the software projects. Most estimation models in use or proposed in the literature are based on regression techniques. In this paper, we attempt on using neural networks to estimate the development effort of OO systems using class points. The estimation model uses class points as the independent variable and development effort as the dependent variable. The results show that the estimation accuracy is higher in neural networks compared to the regression model. This experiment is carried out using the data set used in the literature	artificial neural network;cost estimation in software engineering;radial (radio);radial basis function	S. Kanmani;Jayabalan Kathiravan;S. Senthil Kumar;Mourougane Shanmugam	2007	2007 International Conference on Computing: Theory and Applications (ICCTA'07)	10.1109/ICCTA.2007.89	computer science;machine learning;data mining;object-oriented programming;artificial neural network;regression analysis	Robotics	8.733061229704308	-25.684846076323524	173762
73e2598f26c4c7ff05ba9ef26cdafa885cb204be	obtaining complex linguistic rules from decision information system based genetic algorithms	linguistic decision rules;information systems;probability density function;complex linguistic rules;data mining;information systems genetic algorithms fuzzy sets data mining iris mathematics genetic engineering optimization methods natural languages databases;decision information system;data summary;natural language;information systems computational linguistics decision making genetic algorithms;parameter optimization decision information system data summary genetic algorithm;genetic algorithm;genetic algorithms;optimization;computational linguistics;information system;linguistic decision rules complex linguistic rules decision information system genetic algorithms natural language;iris;parameter optimization;decision rule;fitness function;gallium	As a rule, in real Decision Information System (DIS), attributes' values of objects are continuous numerical. And it is difficult to “read” good information from the data. Numerical values are too fine to directly obtain decision rules which are expressed by natural language. In this paper, based on genetic algorithms (GA), extracting linguistic decision rules from DIS is discussed. In order to obtain complex linguistic decision rules, a fitness function for linguistic terms is proposed, the function expresses the degree of all the thresholds used in the sentences for each linguistic term, obtain complex linguistic decision rules.	fitness function;genetic algorithm;information system;natural language;numerical analysis	Jiafeng Ji;Zheng Pei	2009	2009 IEEE International Conference on Granular Computing	10.1109/GRC.2009.5255114	natural language processing;genetic algorithm;optimal decision;computer science;computational linguistics;machine learning;data mining;decision rule;information system;statistics	DB	2.438200281597339	-28.020078144925694	174077
69d7e2981334510d07b54704a902ac4492ed30d5	on the variability of the concept of variance for fuzzy random variables	second order;fuzzy interval;fuzzy random variable;fuzzy reasoning;probability;uncertainty;helium;variance concept;variance fuzzy random variable imprecise probabilities random set second order possibility measure;fuzzy relation;random variables;possibility measure;random variables fuzzy sets statistics uncertainty informatics probability distribution distribution functions random processes;fuzzy set theory;second order possibility measure;fuzzy sets;random processes fuzzy reasoning fuzzy set theory probability;imprecise probability;ill known conditional probability;probability distribution;imprecise probabilities;random processes;random variable;statistics;informatics;distribution functions;crisp number;fuzzy relation fuzzy random variable variance concept crisp number fuzzy interval ill known conditional probability;conditional probability;random set;variance	Fuzzy random variables possess several interpretations. Historically, they were proposed either as a tool for handling linguistic label information in statistics or to represent uncertainty about classical random variables. Accordingly, there are two different approaches to the definition of the variance of a fuzzy random variable. In the first one, the variance of the fuzzy random variable is defined as a crisp number, which makes it easier to handle in further processing. In the second case, the variance is defined as a fuzzy interval, thus offering a gradual description of our incomplete knowledge about the variance of an underlying, imprecisely observed, classical random variable. In this paper, we also discuss another view of fuzzy random variables, which comes down to a set of random variables induced by a fuzzy relation describing an ill-known conditional probability. This view leads to yet another definition of the variance of a fuzzy random variable in the context of the theory of imprecise probabilities. The new variance is a real interval, which achieves a compromise between both previous definitions in terms of representation simplicity. Our main objective is to demonstrate, with the help of simple examples, the practical significance of these definitions of variance induced by various existing views of fuzzy random variables.	fuzzy concept;heart rate variability;yet another	Inés Couso;Didier Dubois	2009	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2009.2021617	independent and identically distributed random variables;random variable;stochastic process;discrete mathematics;random field;covariance and correlation;multivariate random variable;random element;defuzzification;convergence of random variables;type-2 fuzzy sets and systems;fuzzy classification;conditional variance;fuzzy number;sum of normally distributed random variables;machine learning;random function;mathematics;fuzzy set;algebra of random variables;algebraic formula for the variance;exchangeable random variables;statistics	AI	-1.0345621661160165	-27.438592251984822	174082
7ff7104fc0da0f46c4e91610d2184323ff21b902	disease classification from capillary electrophoresis: mass spectrometry	capillary electrophoresis;electrophorese;record format;multiagent system;diagnostic tool;format enregistrement;electrophoresis;mass spectrometry;tipo dato;hombre;spectrometrie;intelligence artificielle;data type;data mining;data format;classification;qa75 electronic computers computer science;fouille donnee;electroforesis;human;pattern recognition;spectrometry;artificial intelligence;inteligencia artificial;reconnaissance forme;formato grabacion;reconocimiento patron;high throughput;type donnee;sistema multiagente;busca dato;clasificacion;systeme multiagent;homme;espectrometria	We investigate the possibility of using pattern recognition techniques to classify various disease types using data produced by a new form of rapid Mass Spectrometry. The data format has several advantages over other high-throughput technologies and as such could become a useful diagnostic tool. We investigate the binary and multi-class performances obtained using standard classifiers as the number of features is varied and conclude that there is potential in this technique and suggest research directions that would improve performance.		Simon Rogers;Mark A. Girolami;Ronald Krebs;Harald Mischak	2005		10.1007/11551188_20	electrophoresis;mass spectrometry;artificial intelligence	Vision	7.014724530779523	-33.1153283954573	174095
b19b37a291bffdb9038f06e2bfadf26209c53280	an active learning based tcm-knn algorithm for supervised network intrusion detection	network security;active learning;intrusion detection;network intrusion detection;machine learning;detection rate;k nearest neighbor;tcm knn transductive confidence machines for k nearest neighbors algorithm;information system;false positive;data acquisition	As network attacks have increased in number and severity over the past few years, intrusion detection is increasingly becoming a critical component of secure information systems and supervised network intrusion detection has been an active and difficult research topic in the field of intrusion detection for many years. However, it hasn’t been widely applied in practice due to some inherent issues. The most important reason is the difficulties in obtaining adequate attack data for the supervised classifiers to model the attack patterns, and the data acquisition task is always time-consuming and greatly relies on the domain experts. In this paper, we propose a novel supervised network intrusion detection method based on TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) machine learning algorithm and active learning based training data selection method. It can effectively detect anomalies with high detection rate, low false positives under the circumstance of using much fewer selected data as well as selected features for training in comparison with the traditional supervised intrusion detection methods. A series of experimental results on the well-known KDD Cup 1999 data set demonstrate that the proposed method is more robust and effective than the state-of-the-art intrusion detection methods, as well as can be further optimized as discussed in this paper for real applications. a 2007 Elsevier Ltd. All rights reserved.	attack patterns;data acquisition;data mining;data security;information system;intrusion detection system;k-nearest neighbors algorithm;machine learning;supervised learning;toolkit for conceptual modeling;whole earth 'lectronic link	Yang Li;Li Guo	2007	Computers & Security	10.1016/j.cose.2007.10.002	anomaly-based intrusion detection system;intrusion detection system;anomaly detection;type i and type ii errors;computer science;network security;machine learning;pattern recognition;data mining;active learning;data acquisition;k-nearest neighbors algorithm;computer security;information system	AI	7.170519406785563	-37.7447610762046	174274
515ea014bb8bc902f4c4c19a51b2995d5e6d1253	wavelet based interference reduction with fuzzy rule for mc-ds-cdma system			fuzzy rule;statistical interference;wavelet	Takafumi Shibuya;Shigeo Wada	2003			adaptive neuro fuzzy inference system;neuro-fuzzy;fuzzy set operations;fuzzy associative matrix;defuzzification;fuzzy rule;fuzzy number;fuzzy classification;artificial intelligence;computer science;pattern recognition	EDA	3.7190548838386213	-25.563368815120374	174386
762ee5928f4634efd4f093606994802ea910fe4d	distributed association rules mining based on pruned concept lattices	closed frequent itemsets;umpcl distributed association rules mining pruned concept lattices frequent itemset representation closed frequent itemsets;association rules data mining lattices itemsets distributed databases transaction databases data engineering cities and towns data analysis application software;data mining;frequent itemset representation;distributed association rules mining;association rule mining;distributed databases data mining;frequent itemset;umpcl;concept lattice;theoretical analysis;distributed databases;pruned concept lattices	The common association rules mining methods in multiple databases were inefficient due either to larger amount of candidate itemsets for communication overhead or higher times of database scan. Based on discussing the relation between the concept of pruned concept lattice and the representation of frequent itemsets, the closed frequent itemsets of pruned concept lattice was defined. UMPCL, an approximately association rules mining method in horizontally partitioned databases based on multiple PCLs, was proposed. The main ideas of this method are using a frequent concept to represent some few of frequent itemsets to decrease the number of frequent itemsets and rules, and using a slightly lower support for pruning concept lattices before been merged to decrease the size of exchanged messages. The theoretic analysis and experiment show that such method is efficient.	association rule learning;database;formal concept analysis;overhead (computing);theory	Yong Xu;Sen-xin Zhou	2007	Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)	10.1109/FSKD.2007.258	association rule learning;computer science;pattern recognition;data mining;database;mathematics;distributed database	DB	-4.536059431908353	-36.795806158826736	174534
13acb92522ff00f732cfb1e915e98186b1b24864	a hybrid case-based reasoning approach to business failure prediction	case base reasoning;failure prediction	Case-based reasoning (CBR) is a problem-solving and reasoning paradigm that can overcome limitations of the rule-based expert systems. Instead of rules, a CBR system stores and maintains past cases as a case base. When a new problem arises, the system searches through the case base for similar cases to construct a solution for addressing the new problem. Nearest neighbor is a common CBR algorithm for retrieving similar cases, whose similarity function is sensitive to irrelevant attributes. To ensure the effective retrieval of similar cases, a hybrid case-based reasoning approach which employs statistical evaluation for automatically assigning attribute weights and nearest-neighbor algorithm for case retrieval is proposed. This approach is applied to business failure prediction in Australia. The results indicate that in this case it outperforms discriminant analysis in terms of classification accuracy and is an effective and competitive alternative in providing early warnings of those companies at risk of failing in a comprehensible manner.	case-based reasoning	Angela Y. N. Yip	2003			computer science;artificial intelligence;machine learning;data mining	AI	4.805532892271634	-31.23959410888759	174916
84ad143dfa70bf56aff1fb1487ab3c8b92ab9524	integration of neural networks and expert systems for microscopic wear particle analysis	knowledge based system;neural networks;computer vision;image analysis;neural network;knowledge base;expert system	A loosely coupled integration of neural networks and expert systems, using simple message interaction, has been developed for the analysis of microscopic wear particles from machinery. The neural networks are used to classify particle features, whilst a knowledge based system is used for overall wear assessment (using the feature classifications). This type of integration maintains the individual strengths of neural networks and expert systems. The particle features are obtained from a computer vision system linked to the neural networks, and the wear assessment relies on a knowledge base derived from human experts. The system is adaptable, extendable, modular and fast.	artificial neural network;expert system	Ken Xu;A. R. Luxmoore;L. M. Jones;Farzin Deravi	1998	Knowl.-Based Syst.	10.1016/S0950-7051(98)00052-5	cellular neural network;computer science;artificial intelligence;machine learning;data mining;time delay neural network;artificial neural network	ML	6.243293895403698	-28.910981328933502	175031
76d0ace474af3b59b0a0bcd69e310281ef6cf4f7	genetic program based data mining of fuzzy decision trees and methods of improving convergence and reducing bloat	extraction information;0705k;topology;genetic program;convergence;information extraction;information retrieval;tree growth;structure arborescente;metodo arborescente;rule based;logic;decision borrosa;logique floue;resource management;data processing;traitement donnee;decision floue;data mining;fuzzy logic;computer programming;symposia;data analysis;fouille donnee;estructura arborescente;data mining algorithm;tree structure;algorithme genetique;computing systems;algorithms;tree structured method;analyse donnee;genetic algorithm;genetic algorithms;methode arborescente;fuzzy decision tree;decision trees;computer algebra;reduction method;arbre decision;extraccion informacion;fuzzy decision	A data mining procedure for automatic determination of fuzzy decision tree structure using a genetic program (GP) is discussed. A GP is an algorithm that evolves other algorithms or mathematical expressions. Innovative methods for accelerating convergence of the data mining procedure and reducing bloat are given. In genetic programming, bloat refers to excessive tree growth. It has been observed that the trees in the evolving GP population will grow by a factor of three every 50 generations. When evolving mathematical expressions much of the bloat is due to the expressions not being in algebraically simplest form. So a bloat reduction method based on automated computer algebra has been introduced. The effectiveness of this procedure is discussed. Also, rules based on fuzzy logic have been introduced into the GP to accelerate convergence, reduce bloat and produce a solution more readily understood by the human user. These rules are discussed as well as other techniques for convergence improvement and bloat control. Comparisons between trees created using a genetic program and those constructed solely by interviewing experts are made. A new co-evolutionary method that improves the control logic evolved by the GP by having a genetic algorithm evolve pathological scenarios is discussed. The effect on the control logic is considered. Finally, additional methods that have been used to validate the data mining algorithm are referenced.	aerial photography;data mining;database;decision tree;embedded system;francis;fitness function;fuzzy logic;fuzzy rule;genetic algorithm;genetic programming;iterative and incremental development;mined;symbolic computation;tree (data structure);tree structure;unmanned aerial vehicle	James F. Smith;ThanhVu Nguyen	2007		10.1117/12.716973	genetic algorithm;data processing;computer science;resource management;machine learning;data mining;information extraction;algorithm	ML	0.6360053971475097	-30.68749318764442	175150
75dd618ca58b54ae0b103ac91ffc0872aaaffeef	reliability estimation measure: generic discounting approach	belief function theory;classification;conflict management;source confusion;discounting	In the belief function theory, several measures of uncertainty have been introduced. One of their possible use is unreliable source discounting before the fusion stage. Two different measures of uncertainty exist which are the intrinsic and extrinsic ones. The intrinsic measure makes it possible to assess the source’s confusion whereas the extrinsic one measures the contradiction between sources. In this paper, we associate both measures in order to estimate the global reliability of a source. This method, named Generic Discounting Approach (GDA), is proposed in two different versions: Weighted GDA and Exponent GDA. Those reliability measures are integrated into a classifier. The method was tested, against to some pioneer approaches, on several UCI datasets as well as on an urban image classification problem and showed very encouraging results.	computer vision;gnome-db;information source;pixel;statistical classification	Ahmed Samet;Eric Lefevre;Imen Hammami;Sadok Ben Yahia	2015	IJPRAI	10.1142/S0218001415590119	biological classification;misattribution of memory;discounting;data mining;mathematics;statistics	Vision	-0.4528966038058346	-28.463556123557566	175308
af9455532d2f6584e5e3a346bbae6118422b85ff	resolution-based outlier factor: detecting the top-n most outlying data points in engineering data	anomaly;extraction information;analisis datos;information extraction;systeme aide decision;base donnee tres grande;outlier;almacen dato;sistema ayuda decision;anomalie;data mining;outlier detection;anomalia;observacion aberrante;data analysis;decision support system;hierarchical classification;engineering information systems;fouille donnee;classification hierarchique;observation aberrante;analyse donnee;entrepot donnee;very large databases;data warehouse;clasificacion jerarquizada;busca dato;extraccion informacion;systeme information ingenierie	One of the common endeavours in engineering applications is outlier detection, which aims to identify inconsistent records from large amounts of data. Although outlier detection schemes in data mining discipline are acknowledged as a more viable solution to efficient identification of anomalies from these data repository, current outlier mining algorithms require the input of domain parameters. These parameters are often unknown, difficult to determine and vary across different datasets containing different cluster features. This paper presents a novel resolution-based outlier notion and a nonparametric outlier-mining algorithm, which can efficiently identify and rank top listed outliers from a wide variety of datasets. The algorithm generates reasonable outlier results by taking both local and global features of a dataset into account. Experiments are conducted using both synthetic datasets and a real life construction equipment dataset from a large road building contractor. Comparison with the current outlier mining algorithms indicates that the proposed algorithm is more effective and can be integrated into a decision support system to serve as a universal detector of potentially inconsistent records.	approximation;data mining;data point;data structure;database;decision support system;elliptic curve cryptography;experiment;k-nearest neighbors algorithm;local outlier factor;radio over fiber;real life;research data archiving;sensor;synthetic data;synthetic intelligence	Hongqin Fan;Osmar R. Zaïane;Andrew Foss;Junfeng Wu	2008	Knowledge and Information Systems	10.1007/s10115-008-0145-3	outlier;anomaly;decision support system;computer science;data science;data warehouse;data mining;data analysis;information extraction;statistics	ML	-0.3014625789334479	-37.38862711824006	175506
61c9cb9c275e534eff7f05560d55641e7939a0c2	applying fuzzy data mining for soaring area selection	fuzzy data;data mining;number of factors;data warehouse	Soaring is a recreational activity and competitive sport where individuals fly un-powered aircrafts known as gliders. Soaring place selection process depends on a number of factors, resulting in a complex decision-making task. In this paper, we propose the use of the dmFSQL language for fuzzy queries as one of the techniques of Data Mining, which can be used to solve the problem of offering the better place for soaring given the environment conditions and customer characteristics. After doing a process of clustering and characterization of a Customers Database in a Data Warehouse we are able of classify next customer in a cluster and offer an answer according it.	cluster analysis;data mining;fuzzy classification;fuzzy set;prototype;real-time computing;requirement	Alberto Salguero;Francisco Araque;Ramón Alberto Carrasco;M. Amparo Vila;Luis Martínez-López	2007		10.1007/978-3-540-73007-1_72	computer science;data science;data warehouse;data mining;database	ML	-0.6572243451623083	-32.18575396404496	175639
71e41534e9edc99dd29d1cb40289bab6aa60df38	from soft sets to information systems	fuzzy soft set soft set information system approximation space partition type soft set;information systems;information systems fuzzy sets fuzzy systems information analysis set theory uncertainty fuzzy set theory stability analysis game theory operations research;fuzzy set theory information systems;fuzzy set theory;fuzzy information system partition type soft set formal structure fuzzy soft set;information system	This paper discusses the relationship between soft sets and information systems. It is shown that soft sets are a class of special information systems. After soft sets are extended to several classes of general cases, the more general results also show that partition-type soft sets and information systems have the same formal structures, and that fuzzy soft sets and fuzzy information systems are equivalent.	approximation;fuzzy logic;information system;partition type;rough set	Daowu Pei;Duoqian Miao	2005	2005 IEEE International Conference on Granular Computing	10.1109/GRC.2005.1547365	fuzzy logic;combinatorics;discrete mathematics;rough set;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy subalgebra;fuzzy number;neuro-fuzzy;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy set operations;information system	Robotics	-1.319567232859601	-24.353765666512132	175766
b4e547b533923ecba4aa4642b917b830909a55c9	svm and classification ensembles based high-voltage transmission line fault diagnosis	classifier ensemble;support vector machines support vector machine classification transmission lines fault diagnosis neural networks power transmission lines transmission line theory power system faults power system restoration power system protection;support vector machines;neural nets;high voltage transmission line fault diagnosis;power transmission faults;rough set theory;neural network ensemble;neural network ensembles model;support vector machines fault diagnosis neural nets pattern classification power engineering computing power transmission faults power transmission lines rough set theory;power engineering computing;high voltage;pattern classification;svm based htl diagnosis models;classifier ensembles;rough set;power transmission lines;rough set rules classifier;fault diagnosis;rough set rules classifier high voltage transmission line fault diagnosis svm based htl diagnosis models neural network ensembles model classifier ensembles;transmission line	This paper analyzes the inner mechanism of basic methods for high-voltage transmission line (HTL) fault diagnosis, and proposes the new SVM based HTL diagnosis models, which has the characteristic of good generalization. We also put forward the neural network ensembles model and multiple kinds of classifiers ensembles model based on the technology of classifier ensembles. These models can further promote the performance of single classifiers, such as traditional NN, rough set rules classifier, SVM etc. The simulation and experiments results completely show that our new models are more efficient than traditional ones	artificial neural network;experiment;high threshold logic;rough set;simulation;transmission line	Bin Shen;Min Yao;Yuan Bo	2005	The Fifth International Conference on Computer and Information Technology (CIT'05)	10.1109/CIT.2005.180	rough set;computer science;artificial intelligence;machine learning;pattern recognition;artificial neural network	Vision	7.585728557322906	-27.064703062299817	175852
d4119bd768f9e7cc6cbc36e4fa5b68f3083b5f76	a new rough sets model based on database systems	database system;base donnee;relation algebra;rough set theory;database;base dato;large data sets;database systems;rough sets;feature selection;rough set;reduct;ensemble approximatif;relational algebra	Rough sets theory was proposed by Pawlak in the 1980s and has been applied successfully in a lot of domains. One of the major limitations of the traditional rough sets model in the real applications is the inefficiency in the computation of core and reduct, because all the intensive computational operations are performed in flat files. In order to improve the efficiency of computing core attributes and reducts, many novel approaches have been developed, some of which attempt to integrate database technologies. In this paper, we propose a new rough sets model and redefine the core attributes and reducts based on relational algebra to take advantages of the very efficient set-oriented database operations. With this new model and our new definitions, we present two new algorithms to calculate core attributes and reducts for feature selections. Since relational algebra operations have been efficiently implemented in most widely-used database systems, the algorithms presented in this paper can be extensively applied to these database systems and adapted to a wide range of real-life applications with very large data sets. Compared with the traditional rough set models, our model is very efficient and scalable.	algorithmic efficiency;computation;data mining;definition;experiment;feature selection;preprocessor;real life;relational algebra;relational database management system;rough set;rule induction;sql;scalability;select (sql);selection algorithm;sorting;table (information)	Xiaohua Hu;Tsau Young Lin;Jianchao Han	2003	Fundam. Inform.	10.1007/3-540-39205-X_15	rough set;relational database;computer science;theoretical computer science;machine learning;data mining;database;mathematics;feature selection;database design;dominance-based rough set approach	DB	-3.9365189543905075	-37.838356409264534	175962
8a1c226500e75aa724b5e829dd3552506a6b7030	solving multiple-instance and multiple-part learning problems with decision trees and rule sets. application to the mutagenesis problem	multiple instance;learning algorithm;decision tree;supervised learning;multiple instance learning;efficient algorithm;inductive logic programming;mutagenese;intelligence artificielle;algorithme apprentissage;arbol decision;resolucion problema;regle decision;learning problems;artificial intelligence;inteligencia artificial;regla decision;apprentissage supervise;mutagenesis;algoritmo aprendizaje;arbre decision;decision rule;structural properties;problem solving;resolution probleme	"""In recent work, Dietterich et al. (1997) have presented the problem of supervised multiple-instance learning and how to solve it by building axis-parallel rectangles. This problem is encountered in contexts where an object may have di erent possible alternative con gurations, each of which is described by a vector. This paper introduces the multiple-part problem, which is related to the multiple-instance problem, and shows how it can be solved using the multiple-instance algorithms. These two so-called \multiple"""" problems could play a key role both in the development of eÆcient algorithms for learning the relations between the activity of a structured object and its structural properties and in relational learning. This paper analyzes and tries to clarify multiple-problem solving. It goes on to propose multiple-instance extensions of classical learning algorithms to solve multiple-problems by learning multiple-decision trees (Id3-Mi) and multiple-decision rules (RipperMi). In particular, it suggests a new multiple-instance entropy function and a multiple-instance coverage function. Finally, it successfully applies the multiple-part framework on the well-known mutagenesis prediction problem."""	algorithm;apache axis;decision tree;experiment;inductive logic programming;john the ripper;machine learning;multiple-instance learning;naruto shippuden: clash of ninja revolution 3;problem solving;rule 184;whole earth 'lectronic link	Yann Chevaleyre;Jean-Daniel Zucker	2001		10.1007/3-540-45153-6_20	semi-supervised learning;mutagenesis;double loop learning;computer science;artificial intelligence;machine learning;decision tree;decision rule;mathematics;supervised learning;algorithm	ML	8.40171573724297	-31.957125012572856	176079
cd80d52cb0320b0ee03b2d453b7638a8d8ee42e2	flow-based tolerance rough sets for pattern classification	pairwise comparison;preference information;classification;mcda;tolerance rough set	Rough set theory is a useful mathematical tool for pattern classification to deal with vagueness in available information. The main disadvantage of rough set theory is that it cannot handle continuous attributes. Although various discretization methods have been proposed to deal with this problem, discretization can result in information loss. It has been found that tolerance rough sets with a tolerance relation can operate effectively on continuous attributes. A tolerance relation is related to a similarity measure which is commonly defined by a simple distance function to measure the proximity of any two patterns distributed in feature space. However, for a simple distance measure, it oversimplifies the criteria aggregation resulting from not considering attribute weights, and it is not a unique way of expressing the preference information on each attribute for any two patterns. This paper proposes a flow-based tolerairwise comparison CDA ance rough set using flow, which represents the intensity of preference for one pattern over another, to measure similarity between two patterns. To yield high classification performance, a genetic-algorithmbased learning algorithm has been designed to determine parameter specifications and generate the tolerance class of a pattern. The proposed method has been tested on several real-world data sets. Its classification performance is comparable to that of other rough-set-based methods.	.cda file;algorithm;discretization;feature vector;rough set;set theory;similarity measure;statistical classification;vagueness	Yi-Chung Hu	2015	Appl. Soft Comput.	10.1016/j.asoc.2014.11.021	pairwise comparison;biological classification;artificial intelligence;machine learning;pattern recognition;data mining;mathematics;multiple-criteria decision analysis;statistics;dominance-based rough set approach	ML	1.2572701896729068	-29.755223253453142	176245
6ff91b0865fb799269b067dac90cf9b15977cef0	fault diagnosis using dynamic trend analysis: a review and recent developments	qualitative trend analysis;trend analysis;trend matching;intelligent control;hierarchical representation;simulation study;qualitative simulation;similarity search;fault detection and diagnosis;interval halving;fault diagnosis	Dynamic trend analysis is an important technique for fault detection and diagnosis. Trend analysis involves hierarchical representation of signal trends, extraction of the trends, and their comparison (estimation of similarity) to infer the state of the process. In this paper, an overview of some of the existing methods for trend extraction and similarity estimation is presented. A novel interval-halving method for trend extraction and a fuzzy-matching-based method for similarity estimation and inferencing are also presented. The effectiveness of the interval halving and trend matching is shown through simulation studies on the fault diagnosis of the Tennessee Eastman process. Industrial experiences on the application of trend analysis technique for fault detection and diagnosis is also presented followed by a discussion on outstanding issues and solution approaches. r 2006 Elsevier Ltd. All rights reserved.	bisection method;division by two;fault detection and isolation;information extraction;simulation	Mano Ram Maurya;Raghunathan Rengaswamy;Venkat Venkatasubramanian	2007	Eng. Appl. of AI	10.1016/j.engappai.2006.06.020	trend analysis;computer science;machine learning;data mining;statistics;intelligent control	AI	1.2229477359174585	-32.116640244160045	176298
9e5842f884bdc5ef41af7d5b0fce75f48549ab6c	data mining methods in hot steel rolling for scale defect prediction	data mining;hot steel rolling;sc ale defects;neural networks;neural network;high dimensional data	Scale defects are common surface defects in hot ste el rolling. The reasons for such defects are not straightforward. With data mining methods, the multidimensional dependencies between process varia bles and product composition can be discovered. For this research, a high-dimensional data set from Rautaruukki Oyj, Raahe, Finland was gathered. The d ata contained both averaged values and process values measured with different frequencies. The synchronis ation of the variables as well as the allocation of the measurements on the steel strip were solved before the modelling phase. The research enabled the visualisa tion of the rolling process and scale defect modelling. Selforganizing maps (SOM) were used for these tasks.	data mining;hot spare;map;software bug	Jarno Haapamäki;Satu Tamminen;Juha Röning	2005			clustering high-dimensional data;data mining;artificial neural network;machine learning;artificial intelligence;engineering	ML	2.7518761442043096	-33.194089128936696	176304
cd726c2945e8963c0981833a886078982b50e15b	internet anomaly detection based on statistical covariance matrix	anomaly detection;second order statistics;covariance matrix	Intrusion detection is an important part of assuring the reliability of computer systems. Different intrusion detection approaches vary with different patterns used and different intrusions addressed. However, what patterns are effective in constructing a detection system is still a challenge. This paper attempts to apply the traditional covariance matrix concept to the detection of multiple known and unknown network anomalies. With respect to the initiation of typical flood-based network intrusions, the proposed approach takes the measure of covariance matrix to reflect the changes of sequential correlativity of the network traffic when flood-based attacks happen. The differences among covariance matrices of network samples collected in temporal sequences of fixed and equal length are directly evaluated to detect multiple network anomalies. Extensive experiments on the subset of KDDCUP 1999 dataset show that the covariance matrix, as a new pattern, can be directly utilized to construct an effective detection system for flood-based attacks. It also points out that utilizing the covariance matrix in the detection of flood-based attacks can achieve higher performance over traditional approaches.	anomaly detection	Shuyuan Jin;Daniel S. Yeung;Xizhao Wang	2007	IJPRAI	10.1142/S0218001407005557	covariance matrix;anomaly detection;computer science;machine learning;data mining;statistics	AI	5.487221047695696	-37.10015571009587	176629
9c6b3040d167952ccd1f0aa1407391b1f70984ba	notice of violation of ieee publication principles agents and multi-agent systems and application to condition monitoring	multiagent system;mechanical engineering computing;registry agent;multi agent system;vibrations;interpretation agent;support vector machines;extension neural network;multilayer perceptrons;engineering assistant agent;multilayer perceptron;vibration condition monitoring;artificial intelligent;artificial intelligence multiagent system vibration condition monitoring feature extraction agent interpretation agent diagnosis agent engineering assistant agent registry agent support vector machine multilayer perceptron extension neural network;feature extraction agent;multiagent systems condition monitoring support vector machines intelligent systems machine intelligence feature extraction multilayer perceptrons artificial neural networks neural networks multi layer neural network;multi agent systems;monitoring system;condition monitoring;feature extraction;intelligent system;diagnosis agent;artificial intelligence;vibrations condition monitoring mechanical engineering computing multi agent systems multilayer perceptrons support vector machines;multi layer perceptron;support vector machine;neural network	In recent years the need for improved monitoring systems has necessitated the application of intelligent systems. In this study, a multi-agent system (MAS) is used for vibration condition monitoring. The system consists of functional agents such as feature extraction agent, interpretation agent, diagnosis agent, engineering assistant agent and registry agent. Support vector machine (SVM), multi-layer perceptron (MLP) and extension neural network (ENN) are used as interpretation agent. The rationale behind using multi-agent system for condition monitoring is explained. Experimental results show that multi-agent system is an effective artificial intelligence techniques for condition monitoring.	artificial intelligence;artificial neural network;design rationale;extension neural network;feature extraction;layer (electronics);memory-level parallelism;multi-agent system;multilayer perceptron;support vector machine	Christina B. Vilakazi;Tshilidzi Marwala	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4413641	support vector machine;computer science;artificial intelligence;machine learning;multi-agent system;data mining;multilayer perceptron	AI	6.017746484895189	-28.71369557050047	176870
f86a94dfcb286dd36dec1b24fbc5a9002dc8cdbb	a novel approach to design classifiers using genetic programming	nondestructive directed point mutation;trees mathematics genetic algorithms pattern classification;genetic program;genetic operator;chromosomes multitree representation;multitree representation;image classification;multicategory pattern classification design classifier genetic programming chromosomes multitree representation;genetic programming;trees mathematics;indexing terms;binary trees;classifier;or ing operation;biological cells;genetic programming gp;dynamic range;pattern classification;point mutation;design classifier;arithmetic;genetic algorithms;genetic mutations;classification tree analysis;multi category pattern classification;multicategory pattern classification;conflict resolution;genetic programming binary trees arithmetic biological cells genetic mutations pattern classification algorithm design and analysis image classification classification tree analysis dynamic range;algorithm design and analysis	"""We propose a new approach for designing classifiers for a c-class (c/spl ges/2) problem using genetic programming (GP). The proposed approach takes an integrated view of all classes when the GP evolves. A multitree representation of chromosomes is used. In this context, we propose a modified crossover operation and a new mutation operation that reduces the destructive nature of conventional genetic operations. We use a new concept of unfitness of a tree to select trees for genetic operations. This gives more opportunity to unfit trees to become fit. A new concept of OR-ing chromosomes in the terminal population is introduced, which enables us to get a classifier with better performance. Finally, a weight-based scheme and some heuristic rules characterizing typical ambiguous situations are used for conflict resolution. The classifier is capable of saying """"don't know"""" when faced with unfamiliar examples. The effectiveness of our scheme is demonstrated on several real data sets."""	computer performance;denial-of-service attack;genetic programming;heuristic;iterative and incremental development;logic programming;multiclass classification;mutation (genetic algorithm);scheme;statistical classification;tree (data structure)	Durga Prasad Muni;Nikhil R. Pal;Jyotirmay Das	2004	IEEE Transactions on Evolutionary Computation	10.1109/TEVC.2004.825567	point mutation;genetic programming;algorithm design;contextual image classification;dynamic range;genetic algorithm;index term;classifier;binary tree;computer science;artificial intelligence;genetic operator;machine learning;conflict resolution;pattern recognition;mathematics;algorithm	Visualization	3.957553147585614	-31.182140277237206	176959
e3c38263c483298b9be0a2fabef458a4371ee199	immune inspired fault detection and diagnosis: a fuzzy-based approach of the negative selection algorithm and participatory clustering	anomaly detection systems;artificial immune systems;fault detection and diagnosis	This paper describes an immune-inspired system based on an alternate theory about the self–nonself distinction theory, which defines the negative selection process as a mechanism of a fuzzy system based on the affinity between antigen and T-cells. This theory may provide a decision making tool which improves the generation of detectors or even define new data monitoring in order to detect an extreme variation of the system behavior, which means anomalies occurrences. Through these algorithms, tests are performed to detect faults of a DC motor. Upon detection of faults, a participatory clustering algorithm is used to classify these faults and tested to obtain the best set of parameters to achieve the most accurate clustering for these tests in the application being discussed in the article. 2012 Elsevier Ltd. All rights reserved.	armature (computer animation);cluster analysis;computer simulation;fuzzy control system;linear algebra;processor affinity;selection algorithm;sensor;short circuit;software bug;weatherstar	Guilherme Costa Silva;Reinaldo Martinez Palhares;Walmir M. Caminhas	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.04.066	computer science;artificial intelligence;machine learning;data mining;artificial immune system	AI	7.6329632081815415	-27.296052984396027	176984
06e144bf080af96cc55106539dea59907ded2e2f	a real-time analysis of granular information: some initial thoughts on a convex hull-based fuzzy regression approach	real time granular fuzzy regression analysis;granular computing;cluster algorithm;pattern clustering;fuzzy c mean;fuzzy regression;clustering algorithms numerical models linear regression real time systems prototypes algorithm design and analysis;beneath beyond algorithm;prototypes;real time;regression analysis computational complexity data analysis fuzzy set theory genetic algorithms granular computing pattern clustering;ga fcm;linear regression;fuzzy set theory;data analysis;granular information;genetic algorithm fuzzy c means;numerical model;computational complexity;fuzzy c means;granular computing convex hull fuzzy regression fuzzy c means genetic algorithm;linear programming;clustering algorithms;genetic algorithm;genetic algorithms;regression analysis;genetically guided clustering algorithm;convex hull based fuzzy regression approach;numerical models;convex hull;linear programming granular information convex hull based fuzzy regression approach data analysis genetically guided clustering algorithm genetic algorithm fuzzy c means ga fcm granular computing computational complexity real time granular fuzzy regression analysis beneath beyond algorithm;algorithm design;algorithm design and analysis;real time systems	Regression models are well known and widely used as one of the important categories of models in system modeling. In this paper, we extend the concept of fuzzy regression in order to handle real-time implementation of data analysis of information granules. An ultimate objective of this study is to develop a hybrid of a genetically-guided clustering algorithm called genetic algorithm-Fuzzy C-Means (GA-FCM) and a convex hull-based fuzzy regression approach being regarded as a potential solution to the formation of information granules. It is anticipated that the setting of Granular Computing will help us reduce the computing time, especially in case of real-time data analysis, as well as an overall computational complexity. We propose an efficient real-time granular fuzzy regression analysis based on the convex hull approach in which a Beneath-Beyond algorithm is employed to design a convex hull. In the proposed design setting, we emphasize a pivotal role of the convex hull approach, which becomes crucial in alleviating limitations of linear programming manifesting in system modeling.	cluster analysis;computational complexity theory;convex hull;fuzzy clustering;fuzzy cognitive map;fuzzy logic;genetic algorithm;granular computing;linear programming;real-time clock;real-time data;real-time transcription;systems modeling	Azizul Azhar Ramli;Witold Pedrycz;Junzo Watada;Nureize Arbaiy	2011	2011 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2011)	10.1109/FUZZY.2011.6007429	algorithm design;mathematical optimization;genetic algorithm;granular computing;computer science;linear programming;machine learning;data mining;mathematics;output-sensitive algorithm	Robotics	4.964603323516543	-28.964636408081915	177182
1134a0e4d079ea1a6d32a75022956491a561e601	credit card fraud detection: a realistic modeling and a novel learning strategy		Detecting frauds in credit card transactions is perhaps one of the best testbeds for computational intelligence algorithms. In fact, this problem involves a number of relevant challenges, namely: concept drift (customers’ habits evolve and fraudsters change their strategies over time), class imbalance (genuine transactions far outnumber frauds), and verification latency (only a small set of transactions are timely checked by investigators). However, the vast majority of learning algorithms that have been proposed for fraud detection rely on assumptions that hardly hold in a real-world fraud-detection system (FDS). This lack of realism concerns two main aspects: 1) the way and timing with which supervised information is provided and 2) the measures used to assess fraud-detection performance. This paper has three major contributions. First, we propose, with the help of our industrial partner, a formalization of the fraud-detection problem that realistically describes the operating conditions of FDSs that everyday analyze massive streams of credit card transactions. We also illustrate the most appropriate performance measures to be used for fraud-detection purposes. Second, we design and assess a novel learning strategy that effectively addresses class imbalance, concept drift, and verification latency. Third, in our experiments, we demonstrate the impact of class unbalance and concept drift in a real-world data stream containing more than 75 million transactions, authorized over a time window of three years.	algorithm;authorization documentation;bcl10 protein, human;checking (action);computational intelligence;concept drift;credit card fraud;experiment;family computer disk system;feedback;greater than;health smart cards;how true feel alert right now;large;learning to rank;linear logic;machine learning;mandatory - hl7definedroseproperty;nonlinear system;probability;semi-supervised learning;state printed on filter paper card:id:pt:nbs card:nom;statistical classification;supervised learning;verification and validation;verification of theories	Andrea Dal Pozzolo;Giacomo Boracchi;Olivier Caelen;Cesare Alippi;Gianluca Bontempi	2018	IEEE Transactions on Neural Networks and Learning Systems	10.1109/TNNLS.2017.2736643	artificial intelligence;credit card fraud;latency (engineering);business intelligence;data stream;machine learning;concept drift;computer science;small set;computer security;computational intelligence	DB	0.7787740632569976	-34.83913767978935	177244
21e86ccda0672da6b14a9f0c16889dcc95c7e146	comenvprs: a novel approach for inducing decision tree classifiers	decision tree classifier;modelizacion;entropia;decision tree;analisis datos;rough set theory;intelligence artificielle;arbol decision;data mining;classification;modelisation;data analysis;machine learning;fouille donnee;theorie ensemble approximatif;entropie;artificial intelligence;analyse donnee;entropy;inteligencia artificial;information entropy;rough set;modeling;busca dato;arbre decision;clasificacion	This paper presents a new approach for inducing decision trees by combining information entropy criteria with VPRS based methods. From the angle of rough set theory, when inducing decision trees, entropy based methods emphasize the effect of class distribution. Whereas the rough set based approaches emphasize the effect of certainty. The presented approach takes the advantages of both criteria for inducing decision trees. Comparisons between the presented approach and the fundamental information entropy based method on some data sets from the UCI Machine Learning Repository are also reported.	decision tree;entropy (information theory);machine learning;rough set;set theory	Shuqin Wang;Jinmao Wei;Junping You;Dayou Liu	2006		10.1007/11811305_13	entropy;rough set;decision tree learning;computer science;machine learning;pattern recognition;data mining;mathematics;dominance-based rough set approach	AI	9.079527227443517	-32.668510842007485	177470
8f359a192fac810ca07d55d4caed06ab08238443	nonparametric neural network model based on rough-fuzzy membership function for classification of remotely sensed images	remote sensing image;modelizacion;vision ordenador;linguistique;distance minimale;fuzzy membership function;analisis estadistico;image processing;logique floue;procesamiento imagen;logica difusa;image classification;multilayer perceptron;probabilistic approach;traitement image;computer vision;minimal distance;fuzzy logic;modelisation;linguistica;red multinivel;statistical analysis;minimum distance;teledeteccion multiespectral;enfoque probabilista;approche probabiliste;fonction appartenance;backpropagation algorithm;remote sensing;analyse statistique;classification image;membership function;multispectral remote sensing;back propagation algorithm;algorithme retropropagation;vision ordinateur;multilayer network;funcion pertenencia;multi spectral;reseau multicouche;neural network model;reseau neuronal;teledetection multispectrale;modeling;back propagation;red neuronal;distancia minima;neural network;algoritmo retropropagacion;linguistics	A nonparametric neural network model based on Rough-Fuzzy Membership function, multilayer perceptron, and back-propagation algorithm is described. The described model is capable to deal with rough uncertainty as well as fuzzy uncertainty associated with classification of remotely sensed multi-spectral images. The input vector consists of membership values to linguistic properties while the output vector is defined in terms of rough fuzzy class membership values. This allows efficient modeling of indiscernibility and fuzziness between patterns by appropriate weights being assigned to the backpropagated errors depending upon the Rough-Fuzzy Membership values at the corresponding outputs. The effectiveness of the model is demonstrated on classification problem of IRS-P6 LISS IV images of Allahabad area. The results are compared with statistical (Minimum Distance), conventional MLP, and FMLP models.	algorithm;artificial neural network;backpropagation;computer vision;convergence insufficiency;experiment;fuzzy logic;memory-level parallelism;multilayer perceptron;multispectral image;network model;neuro-fuzzy;rough set;software propagation;turing completeness;vagueness	Niraj Kumar;Anupam Agrawal	2006		10.1007/11949619_10	membership function;fuzzy classification;artificial intelligence;machine learning;mathematics;algorithm	ML	9.396128181838177	-30.29203575274659	177621
09516eba11a22fe52e3c1e751039c2e919072a9a	incremental updating rough approximations in interval-valued information systems		Interval-valued Information System (IvIS) is a generalized model of single-valued information system, in which the attribute values of objects are all interval values instead of single values. The attribute set in IvIS is not static but rather dynamically changing over time with the collection of new information. The rough approximations may evolve accordingly, which should be updated continuously for data analysis based on rough set theory. In this paper, on the basis of the similarity-based rough set theory in IvIS, we first analyze the relationships between the original approximation sets and the updated ones. And then we propose the incremental methods for updating rough approximations when adding and removing attributes, respectively. Finally, a comparative example is used to validate the effectiveness of the proposed incremental methods.	approximation;information system;rough set	Yingying Zhang;Tianrui Li;Chuan Luo;Hongmei Chen	2015		10.1007/978-3-319-25754-9_22	machine learning;pattern recognition;information system;artificial intelligence;rough set;computer science	DB	-2.7497888323386506	-28.598750265735102	177797
6db94a71926be2606b12b133759949b10610df95	analysis of k-nearest neighbor branch and bound rules	simulation ordinateur;arbre recherche;methode branch and bound;research tree;cluster;amas;classification;interpretacion probabilista;algorithme;probabilistic interpretation;algorithm;vecino mas cercano;algorritmo;branch and bound method;interpretation probabiliste;branch and bound algorithms;arbol investigacion;metodo branch and bound;clustering;nearest neighbors;plus proche voisin;nearest neighbour;k nearest neighbor;monton;simulacion por computadora;branch and bound;computer simulation;clasificacion	This paper analyzes the applicability and relative efficiency of four pruning rules for finding k-nearest neighbors by a branch and bound algorithm. Results of experimental simulations with various clustering schemes and data distributions are included.	branch and bound;k-nearest neighbors algorithm	Soren Larsen;Laveen N. Kanal	1986	Pattern Recognition Letters	10.1016/0167-8655(86)90026-7	computer simulation;combinatorics;biological classification;computer science;machine learning;mathematics;cluster analysis;k-nearest neighbors algorithm;branch and bound;algorithm;cluster	Vision	8.617510858331114	-34.39821763789269	178109
0afd5901b7a2f6ff043e4aa11f7ecd6db09e3567	a comparative study of divisive hierarchical clustering algorithms		A general scheme for divisive hierarchica l lustering algorithms is proposed. It is made of three main steps : first a splitting proced ur for the subdivision of clusters into two subclusters, second a local evaluation of the bipar titions resulting from the tentative splits and, third, a formula for determining the nodes levels o f the resulting dendrogram. A handfull of such algorithms is given. These algor ithms are compared using the GoodmanKruskal correlation coefficient. As a global criter on it is an internal goodness-of-fit measure based on the set order induced by the hierarchy compared to the order associated to the given dissimilarities. Applied to a hundred of random data tables, these c omparisons are in favor of two methods based on non-usual ratio-type formulas for the spli tting procedures, namely the Silhouette criterion and the Dunn's criterion. These two criteria take i nto account both the within cluster and the between cluster mean dissimilarity. In general the results of these two algorithms are better than the classical Agglomerative Average Link method.	akaike information criterion;algorithm;cluster analysis;coefficient;computation;dendrogram;differential inclusion;hierarchical clustering;human-readable medium;kruskal's algorithm;randomness;subdivision surface	Maurice Roux	2015	CoRR		complete-linkage clustering;mathematical optimization;combinatorics;machine learning;data mining;mathematics;statistics;hierarchical clustering of networks	ML	2.46565907153219	-37.970574731905074	178251
44492de6c544a2e71a3ff5fc68ed21a26e96b2b4	sensitivity analysis of fuzzy rule-based classification systems by means of the lipschitz condition	multi polar aggregation;fuzzy reasoning;supervised learning;classification;accuracy	The fuzzy rule-based classifier can be taken as a function that assigns to a point from the feature space a class, or a class with an association degree. Under this assumption, the robustness of fuzzy rule-based classifiers is investigated by means of the Lipschitz condition. The Lipschitz continuity of fuzzy sets, fuzzy rules and whole fuzzy rule-based classifiers is examined for multi-polar outputs, extended multi-polar outputs and outputs in the form of a class. Related performance of a fuzzy rule-based classifier is also discussed. All studied concepts are shown on an exemplar fuzzy rule-based classifier.	fuzzy rule;logic programming	Andrea Mesiarová-Zemánková	2016	Soft Comput.	10.1007/s00500-015-1744-z	fuzzy logic;membership function;defuzzification;adaptive neuro fuzzy inference system;biological classification;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;fuzzy subalgebra;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;accuracy and precision;fuzzy set;fuzzy associative matrix;supervised learning;fuzzy set operations;fuzzy control system	Logic	2.0075145501897778	-28.459635540030128	178339
65db3519b00b2284e427ec1e8beedb30fd2ff71c	flexible-hybrid sequential floating search in statistical feature selection	dimensionalidad;british telecom;selection problem;problema seleccion;time trade off;tiempo busqueda;algoritmo busqueda;analisis estadistico;empaqueteur;forme onde;analisis estructural;algorithme recherche;modelo hibrido;search algorithm;dimensionality;almacen dato;sequential search;temps recherche;probabilistic approach;classification;modele hybride;hybrid model;envolvero;analyse syntaxique;forma onda;statistical analysis;analisis sintaxico;dimensionnalite;enfoque probabilista;approche probabiliste;syntactic analysis;analyse statistique;pattern recognition;feature selection;waveform;reconnaissance forme;entrepot donnee;analyse structurale;reconocimiento patron;data warehouse;classification accuracy;structural analysis;search time;clasificacion;hybrid algorithm;wrapper;probleme selection	Among recent topics studied in context of feature selection the hybrid algorithms seem to receive particular attention. In this paper we propose a new hybrid algorithm, the flexible hybrid floating sequential search algorithm, that combines both the filter and wrapper search principles. The main benefit of the proposed algorithm is its ability to deal flexibly with the quality-of-result versus computational time tradeoff and to enable wrapper based feature selection in problems of higher dimensionality than before. We show that it is possible to trade significant reduction of search time for negligible decrease of the classification accuracy. Experimental results are reported on two data sets, WAVEFORM data from the UCI repository and SPEECH data from British Telecom.	academy;computation;feature selection;hybrid algorithm;linear search;search algorithm;time complexity;waveform	Petr Somol;Jana Novovicová;Pavel Pudil	2006		10.1007/11815921_69	linear search;curse of dimensionality;waveform;hybrid algorithm;biological classification;computer science;artificial intelligence;machine learning;data warehouse;parsing;structural analysis;programming language;feature selection;algorithm;search algorithm	AI	8.839801198220282	-33.709915152063495	178719
c86bfbbf4943a9551a0d78377c24b38ec6afb9cb	an overview of regression techniques for knowledge discovery	recursive partitioning;machine learning;regression tree;knowledge discovery;projection pursuit regression;rule based;multivariate adaptive regression splines	Predicting or learning numeric features is called regression in the statistical literature, and it is the subject of research in both machine learning and statistics. This paper reviews the important techniques and algorithms for regression developed by both communities. Regression is important for many applications, since lots of real life problems can be modeled as regression problems. The review includes Locally Weighted Regression (LWR), rule-based regression, Projection Pursuit Regression (PPR), instance-based regression, Multivariate Adaptive Regression Splines (MARS) and recursive partitioning regression methods that induce regression trees (CART, RETIS and M5).		Ilhan Uysal;H. Altay Güvenir	1999	Knowledge Eng. Review		rule-based system;principal component regression;ordinal regression;projection pursuit regression;proper linear model;local regression;multivariate adaptive regression splines;computer science;machine learning;decision tree;bayesian multivariate linear regression;stepwise regression;polynomial regression;pattern recognition;regression diagnostic;logistic regression;knowledge extraction;nonparametric regression;robust regression;regression analysis;recursive partitioning	ML	5.992609482232487	-24.634574721723837	178753
62a4c1a792f456439c94ec80aded9961e4bd8af0	type-2 ga-tsk fuzzy neural network	fuzzy neural network;fuzzy neural nets;fuzzy logic;neuro fuzzy;type 2 fuzzy set;fuzzy logic system;takagi sugeno kang;neuro fuzzy classifiers type 2 ga tsk fuzzy neural network type 2 fuzzy logic system genetic algorithm takagi sugeno kang fuzzy neural network rule uncertainties;genetic algorithms fuzzy logic fuzzy neural nets;genetic algorithm;genetic algorithms;process simulation;fuzzy neural networks evolutionary computation	A novel fuzzy-neural network, the type-2 GA- TSKfnn (T2GA-TSKfnn), combining a type-2 fuzzy logic system (FLS) and a genetic algorithm (GA) based Takagi-Sugeno- Kang fuzzy neural network (GA-TSKfnn), is presented. The rational for this combination is that type-2 fuzzy sets are better able to deal with rule uncertainties, while the optimal GA-based tuning of the T2GA-TSKfnn parameters achieves better classification results. However, a general T2GA-TSKfnn is computationally very intensive due to the complexity of the type-2 to type-1 reduction. Therefore, we adopt an interval T2GA-TSKfnn implementation to simplify the computational process. Simulation results are provided to compare the T2GA-TSKfnn against other fuzzy neural networks. These results show that the proposed system is able to achieve a higher classification rate when compared against a number of other traditional neuro-fuzzy classifiers.	computation;free library of springfield township;fuzzy logic;fuzzy set;genetic algorithm;neuro-fuzzy;simulation;type-2 fuzzy sets and systems	Alvin Cai;Hiok Chai Quek;Douglas L. Maskell	2007	2007 IEEE Congress on Evolutionary Computation	10.1109/CEC.2007.4424661	genetic algorithm;process simulation;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;mathematics;fuzzy associative matrix;fuzzy set operations;fuzzy control system;intelligent control	AI	5.567800004889038	-26.728784179309205	178808
4a87d7049cd48e21ae632a719a968e78168782c4	attribute reduction in utility-based decision-theoretic rough set models		Decision-theoretic rough set (DTRS) model, proposed by Yao in the early 1990’s, introduces Bayesian decision procedure and loss function in rough set theory. Considering utility function in decision processing, utility-based decision-theoretic rough set model (UDTRS) is given in this paper. The utility of the positive region, the boundary region and the negative region are obtained respectively. We provide a reduction definition which can obtain the maximal utility in decisions. A heuristic reduction algorithm with respect to the definition is proposed. Finally, experimental results show the proposed algorithm is effective.	algorithm;decision problem;heuristic;loss function;maximal set;rough set;set theory;utility;yao graph	Nan Zhang;Lili Jiang;Caihui Liu	2017		10.1007/978-3-319-60840-2_27	mathematical optimization;dominance-based rough set approach;decision-theoretic rough sets;mathematics;heuristic;rough set;bayesian probability;attribute domain	AI	-2.48978876008468	-27.04805761830123	179119
ef4298796628fa16b9ab09239c3fdd6365c6f02d	effective spatial characterization system using density-based clustering	concept hierarchy	Previous spatial characterization methods does not analyze well spatial regions for a given query since it only focus on characterization for user's pre-selected area and without consideration of spatial density. Consequently, the effectiveness of characterization knowledge is decreased in these methods. In this paper, we propose a new hybrid spatial characterization system combining the density-based clustering module which consists of the attribute removal generalization and the concept hierarchy generalization. The proposed method can generate characteristic rule and apply density-based clustering to enhance the effectiveness of generated rules.	computer cluster	Chan-Min Ahn;Jae-Hyun You;Ju-Hong Lee;Deok-Hwan Kim	2007		10.1007/978-3-540-72586-2_76	discrete mathematics;computer science;machine learning;data mining;mathematics	Vision	-2.829904492898205	-28.850942849989135	179192
3a3a8dc75a53c80c8afa6eff91e03ede11836b94	rough and fuzzy geographical data integration	fuzzy classification;fuzzy set;data integrity	We show how fuzzy and rough data are transformed into a unified representation using the concept of rough fuzzy sets. We introduced a representation for rough fuzzy classifications that might be used when the classification process introduces uncertainty in the data due to vagueness and indiscernibility. We demonstrate the viability of our method by performing classification experiments using real data. The experiment uses data that have been classified using different classification schemes. To make them compatible we reclassify some data and use some additional knowledge that requires the use of rough fuzzy classifications.	experiment;fuzzy concept;fuzzy set;rough set;statistical classification;vagueness	Ola Ahlqvist;Johannes Keukelaar;Karim Oukbir	2003	International Journal of Geographical Information Science	10.1080/13658810210157750	fuzzy logic;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data integrity;data mining;database;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;dominance-based rough set approach	ML	-1.1976475017148078	-27.149376721844735	179514
f726ce51b22950fafb659d2ab3d6d16d4e172e71	an imbalanced data rule learner	base donnee;distribution donnee;learning algorithm;analisis datos;database;base dato;intelligence artificielle;algorithme apprentissage;rule learning;data distribution;data analysis;decouverte connaissance;artificial intelligence;descubrimiento conocimiento;analyse donnee;inteligencia artificial;algoritmo aprendizaje;empirical evaluation;categorical data;distribucion dato;large classes;imbalanced data sets;donnee categorielle;dato categorico;knowledge discovery	Imbalanced data learning has recently begun to receive much attention from research and industrial communities as traditional machine learners no longer give satisfactory results. Solutions to the problem generally attempt to adapt standard learners to the imbalanced data setting. Basically, higher weights are assigned to small class examples to avoid their being overshadowed by the large class ones. The difficulty determining a reasonable weight for each example remains. In this work, we propose a scheme to weight examples of the small class based solely on local data distributions. The approach is for categorical data, and a rule learning algorithm is constructed taking the weighting scheme into account. Empirical evaluations prove the advantages of this approach.	adaboost;algorithm;categorical variable;computation;computational complexity theory	Canh Hao Nguyen;Tu Bao Ho	2005		10.1007/11564126_65	categorical variable;computer science;artificial intelligence;machine learning;data mining;mathematics;knowledge extraction;data analysis;algorithm;statistics	ML	8.574546567076514	-32.86699630025373	179530
3cf16bef3b76d3a48f95e4368aeda51510b27950	cognition-based deep learning: progresses and perspectives		The human brain is composed of multiple modular subsystems, with a unique way interacting among each other. These subsystems have their own unique characteristics and interact to support cognitive functions such as memory, attention and cognitive control. Nowadays, deep learning methods based on the above-mentioned functions accompanied with knowledge are widely used to design more dynamic, robust and powerful systems. We first review and summarize the progresses of cognition-based deep neural networks, and how cognitive mechanisms can be applied to more brain-like neural networks. Then we propose a general framework for the design of cognition-based deep learning system. Although great efforts have been made in this field, cognition-based deep learning is still in its early age. We put forward the potential directions towards this field, such as associative memory in deep learning, interpretable network with cognitive mechanisms, and deep reinforcement learning based on cognitive science.	cognition;deep learning	Kai Yi;Shi-tao Chen;Yu Chen;Chao Xia;Nanning Zheng	2018		10.1007/978-3-319-92007-8_11	machine learning;computer science;content-addressable memory;artificial neural network;reinforcement learning;deep learning;cognition;modular design;artificial intelligence	ML	8.512043200121889	-26.48761988426681	179649
f8e7b4452a8278ba586a49d9bb2bfcec2098d1f2	a new multivariate decision tree construction algorithm based on variable precision rough set	tolerancia falta;decision tree;relation equivalence;fault tolerant;noisy data;rough set theory;arbol decision;equivalence relation;theorie ensemble approximatif;fault tolerance;rapport signal bruit;relacion senal ruido;signal to noise ratio;rough set;relacion equivalencia;arbre decision;tolerance faute	In this paper we extend previous research and present a novel approach to construct multivariate decision tree, which has to some extent the ability of fault tolerance, by employing a development of RST, namely the variable precision rough sets (VPRS) model. Based on variable precision rough set theory, a new concept of generalization of one equivalence relation with respect to another one with precision β is introduced and used for construction of multivariate decision tree. The experimentation result shows its fitness to create multivariate decision tree retrieved from noisy data.	algorithm;decision tree;rough set	Liang Zhang;Yunming Ye;Shui Yu;Fanyuan Ma	2003		10.1007/978-3-540-45160-0_23	fault tolerance;rough set;decision tree learning;computer science;machine learning;incremental decision tree;mathematics;id3 algorithm;algorithm;statistics;dominance-based rough set approach	AI	8.115862783150268	-33.87507667061519	179992
aa74a25c109a41fb0439892114a1d01b5158a77e	an improved intrusion detection system based on a two stage alarm correlation to identify outliers and false alerts	false positives;outliers;clustering;intrusion detection systems;binary classification	To ensure the protection of computer networks from attacks, an intrusion detection system IDS should be included in the security architecture. Despite the detection of intrusions is the ultimate goal, IDSs generate a huge amount of false alerts which cannot be properly managed by the administrator, along with some noisy alerts or outliers. Many research works were conducted to improve IDS accuracy by reducing the rate of false alerts and eliminating outliers. In this paper, we propose a two-stage process to detect false alerts and outliers. In the first stage, we remove outliers from the set of meta-alerts using the best outliers detection method after evaluating the most cited ones in the literature. In the last stage, we propose a binary classification algorithm to classify meta-alerts whether as false alerts or real attacks. Experimental results show that our proposed process outperforms concurrent methods by considerably reducing the rate of false alerts and outliers.	intrusion detection system	Fatma Hachmi;Mohamed Limam	2015		10.1007/978-3-319-26832-3_13	computer science;pattern recognition;data mining;computer security	ML	6.15800693163441	-37.66472899699291	180072
d4264a42df76d1cac1262e79185e7d07fd32f485	obtaining low-arity discretizations from online data streams	evaluation function;data stream;statistical test;left to right;linear time;growth rate;prediction accuracy;empirical evaluation	Cut point analysis for discretization of numerical attributes has shown, for many commonly-used attribute evaluation functions, that adjacent value range intervals with an equal relative class distribution may be merged together without risking to find the optimal partition of the range. A natural idea is to relax this requirement and rely on a statistical test to decide whether the intervals are probably generated from the same distribution. ChiMerge is a classical algorithm for numerical interval processing operating just in this manner. ChiMerge handles the interval mergings in the order of their statistical probability. However, in online processing of the data the required n log n time is too much. In this paper we propose to do the mergings during a left-to-right scan of the intervals. Thus, we reduce the time requirement of merging down to more reasonable linear time. Such linear time operations are not necessary in connection of every example. Our empirical evaluation shows that intervals get effectively combined, their growth rate remains very moderate even when the number of examples grows excessive, and that the substantial reduction of interval numbers can even benefit prediction accuracy.	algorithm;discretization;interval arithmetic;numerical analysis;overfitting;recursion;regular language description for xml;time complexity;vertical blanking interval	Tapio Elomaa;Petri Lehtinen;Matti Saarela	2008		10.1007/978-3-540-68123-6_10	time complexity;statistical hypothesis testing;computer science;artificial intelligence;machine learning;evaluation function;data mining;algorithm	AI	-2.2781960068526663	-36.25723353836318	180084
b8189d85ac4464e8b220c9a3e6a93b59611ed0da	a new data pre-processing approach for the dendritic cellalgorithm based on fuzzy rough set theory	dendritic cells;feature selection;fuzzy rough sets;artificial immune systems	The aim of this paper is to develop a new data pre-processing method for the dendritic cell algorithm (DCA) based on Fuzzy Rough Set Theory (FRST). In this new fuzzy-rough model, the data pre-processing phase is based on the fuzzy positive region and the fuzzy dependency degree concepts. Results show that applying FRST is more convenient for the DCA data pre-processing phase yielding much better performance in terms of accuracy.	algorithm;data pre-processing;dendritic spine;preprocessor;rough set;set theory	Zeineb Chelly;Zied Elouedi	2013		10.1145/2464576.2464657	defuzzification;fuzzy classification;computer science;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;mathematics;feature selection;artificial immune system	AI	3.6830741211776874	-29.619711285030064	180746
54fff51fe102c862620b46a63f6ae8de26e08aa5	iris recognition algorithm based on point covering of high-dimensional space and neural network	high dimensionality;biometrie;biometrics;biometria;iris recognition;intelligence artificielle;data mining;fouille donnee;cognition;人工智能;pattern recognition;error rate;cognicion;artificial intelligence;inteligencia artificial;reconnaissance forme;reseau neuronal;reconocimiento patron;busca dato;red neuronal;neural network	In this paper, we constructed a Iris recognition algorithm based on point covering of high-dimensional space and Multi-weighted neuron of point covering of high-dimensional space, and proposed a new method for iris recognition based on point covering theory of high-dimensional space. In this method, irises are trained as “cognition” one class by one class, and it doesn’t influence the original recognition knowledge for samples of the new added class. The results of experiments show the rejection rate is 98.9%, the correct cognition rate and the error rate are 95.71% and 3.5% respectively. The experimental results demonstrate that the rejection rate of test samples excluded in the training samples class is very high. It proves the proposed method for iris recognition is effective.	algorithm;artificial neural network;cognition;covering space;experiment;iris recognition;neuron;nyquist rate;rejection sampling	Wenming Cao;Jianhui Hu;Gang Xiao;Shoujue Wang	2005		10.1007/11510888_30	computer vision;cognition;word error rate;computer science;artificial intelligence;machine learning;data mining;iris recognition;artificial neural network;algorithm;biometrics	ML	9.764033694408939	-32.575377592290636	180936
cc4d0ef3005c7472be37f2ba80923bb935ee2e01	incremental learning from stream data	support vector machines;mapping function;data mining;algorithms artificial intelligence computer simulation data mining information storage and retrieval models theoretical pattern recognition automated;learning systems;learning system;computer architecture;distribution function;mapping function adaptive classification concept shifting data mining incremental learning machine learning;incremental learning;estimation;machine learning;decision making process;adaptive classification;distribution functions data mining learning systems machine learning support vector machines;support vector machine;data handling;distribution functions;learning artificial intelligence;knowledge representation;data flow;learning artificial intelligence data handling decision making knowledge representation;concept shifting;biological neural networks;adain stream data incremental learning machine learning data flow representative training data knowledge representation decision making process;neural network	Recent years have witnessed an incredibly increasing interest in the topic of incremental learning. Unlike conventional machine learning situations, data flow targeted by incremental learning becomes available continuously over time. Accordingly, it is desirable to be able to abandon the traditional assumption of the availability of representative training data during the training period to develop decision boundaries. Under scenarios of continuous data flow, the challenge is how to transform the vast amount of stream raw data into information and knowledge representation, and accumulate experience over time to support future decision-making process. In this paper, we propose a general adaptive incremental learning framework named ADAIN that is capable of learning from continuous raw data, accumulating experience over time, and using such knowledge to improve future learning and prediction performance. Detailed system level architecture and design strategies are presented in this paper. Simulation results over several real-world data sets are used to validate the effectiveness of this method.	application domain;buffers;data acquisition;data buffer;data-intensive computing;dataflow;decision making;decision boundary;download;ensemble learning;increment;knowledge representation and reasoning;machine learning;name;reinforcement learning;sampling (signal processing);simulation;solutions;statistical test	Haibo He;Sheng Chen;Kang Li;Xin Xu	2011	IEEE Transactions on Neural Networks	10.1109/TNN.2011.2171713	knowledge representation and reasoning;support vector machine;instance-based learning;computer science;online machine learning;distribution function;machine learning;pattern recognition;data mining;data stream mining;stability;active learning;artificial neural network	ML	4.96983771678967	-34.83648935999225	181360
571b9bc139c90d8e1f71e01a0f74e9500c2a119d	extending a simple genetic cooperative-competitive learning fuzzy classifier to low quality datasets	fuzzy data;genetics;competitive learning;vague data;genetic fuzzy systems;fuzzy rule base;possibilistic data;fuzzy knowledge based rules;cooperative competitive learning;genetic fuzzy system;knowledge base;fuzzy classifier	Exploiting the information in low quality datasets has been recently acknowledged as a new challenge in Genetic Fuzzy Systems. Owing to this, in this paper we discuss the basic principles that govern the extension of a fuzzy rule based classifier to interval and fuzzy data. We have also applied these principles to the genetic learning of a simple cooperative-competitive algorithm, that becomes the first example of a Genetic Fuzzy Classifier able to use low quality data. Additionally, we introduce a benchmark, comprising some synthetic samples and two real-world problems that involve interval and fuzzy-valued data, that can be used to assess future algorithms of the same kind.	benchmark (computing);competitive learning;fuzzy control system;fuzzy logic;fuzzy rule;genetic algorithm;genetic fuzzy systems;synthetic intelligence	Ana M. Palacios;Luciano Sánchez;Inés Couso	2009	Evolutionary Intelligence	10.1007/s12065-009-0024-1	fuzzy logic;knowledge base;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;data mining;information fuzzy networks;fuzzy associative matrix;competitive learning;fuzzy set operations;fuzzy control system	AI	4.251527381052748	-29.922308267304164	181377
6b997c23b88f91113f73159991bae55652367a36	feature region-merging based fuzzy rules extraction for pattern classification	hyperboxes;supervised learning;design engineering;membership functions feature region merging fuzzy rules extraction pattern classification supervised learning method hyperboxes multidimensional feature space singleton class compound class data existence region iris data set human brain mri images;fuzzy rules;pattern classification data mining merging fuzzy systems design engineering systems engineering and theory supervised learning iris humans magnetic resonance;data existence region;knowledge based systems pattern classification learning artificial intelligence merging feature extraction fuzzy set theory;rule learning;data mining;feature space;fuzzy set theory;magnetic resonance image;systems engineering and theory;multi dimensional;magnetic resonance;feature extraction;membership functions;singleton class;feature region merging;pattern classification;merging;iris data set;supervised learning method;humans;compound class;fuzzy rules extraction;learning artificial intelligence;multidimensional feature space;iris;human brain;region merging;fuzzy systems;knowledge based systems;human brain mri images	A supervised learning method is proposed to automatically extract fuzzy rules for numerical pattern classification problems. fuzzy rules are constructed corresponding to hyperboxes in a multi-dimensional feature space, where a hyperbox indicates an existence region of data belonging to a singleton class or a compound class. Hyperboxes are effectively realized by means of a linked list based region-merging technique. The method supports the representation of the union of multiple classes in the region merging process and hence it can deal with compound classes in the cases where highly mixed classes exist. Also, the method is capable of automatically deleting trivial features during the rule learning process. To demonstrate the effectiveness of the proposed method, experiments are carried out for classifying Iris data set and human brain magnetic resonance images (MRI). It is concluded that the proposed method performs well and is quite competitive to other fuzzy rule extraction techniques.		Hongwei Zhu;Otman A. Basir	2003		10.1109/FUZZ.2003.1209448	feature vector;feature extraction;singleton pattern;iris flower data set;fuzzy classification;computer science;artificial intelligence;magnetic resonance imaging;machine learning;pattern recognition;data mining;mathematics;fuzzy set	NLP	3.3762950602483683	-29.978085995284474	181413
be020cee0453b0e35247b4735048761e3094d860	relational type-2 interval fuzzy systems	fuzzy logic;performance improvement;type 2 fuzzy set;fuzzy system;numerical simulation	In this paper we combine type-2 fuzzy logic with a relational fuzzy system paradigm. Incorporating type-2 fuzzy sets brings new possibilities for performance improvement. The relational fuzzy system scheme reduces significantly the number of system parameters to be trained. Numerical simulations of the proposed combination of systems are presented.	fuzzy control system;fuzzy logic	Rafal Scherer;Janusz T. Starczewski	2009		10.1007/978-3-642-14390-8_37	computer simulation;fuzzy logic;fuzzy electronics;fuzzy cognitive map;membership function;defuzzification;adaptive neuro fuzzy inference system;type-2 fuzzy sets and systems;fuzzy mathematics;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;fuzzy measure theory;data mining;mathematics;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system	Logic	4.716790262837177	-26.407998067952775	181515
b5ae8bb37167e06febf4df293e210fce498173a7	an interval lattice model for grid resource searching	interval concept lattice;grid resource searching;concept lattice;interval fca;lattice model;formal concept analysis	In practice the information mostly expressed in interval form. Formal Concept Analysis can not deal with the interval information. So the research on interval lattice is an important task. In the paper, an interval lattice model is proposed. Then a Grid resource matching algorithm through the interval lattice is presented. In the end of the paper, experimental results show that the construction algorithm has reasonable performance on the complexity.	lattice model (physics)	Wen Zhou;Zongtian Liu;Yan Zhao	2007		10.1007/978-3-540-72586-2_99	combinatorics;discrete mathematics;lattice model;computer science;formal concept analysis;theoretical computer science;machine learning;mathematics;lattice miner	HPC	-3.2637120982003895	-25.6360815368426	181662
a0b0ec8307ec035696a89393ff783b6e1a93edfa	run-time malware detection based on positive selection	novel classification algorithm;program behavior;positive selection;program execution behavior;bayesian networks;important algorithm;run-time malware detection;general purpose classification problem;proposed algorithm;decision tree;artificial immune systems	This paper presents a supervised methodology that detects malware based on positive selection. Malware detection is a challenging problem due to the rapid growth of the number of malware and increasing complexity. Run-time monitoring of program execution behavior is widely used to discriminate between benign and malicious executables due to its effectiveness and robustness. This paper proposes a novel classification algorithm based on the idea of positive selection, which is one of the important algorithms in Artificial Immune Systems (AIS), inspired by positive selection of T-cells. The proposed algorithm is applied to learn and classify program behavior based on I/O Request Packets (IRP). In our experiments, the proposed algorithm outperforms ANSC, Naï ve Bayes, Bayesian Networks, Support Vector Machine, and C4.5 Decision Tree. This algorithm can also be used in general purpose classification problems not just two-class but multi-class problems.	application programming interface;approximation algorithm;artificial immune system;bayesian network;c4.5 algorithm;callback (computer programming);clonal selection algorithm;decision tree;executable;experiment;i/o request packet;input/output;malware;multiclass classification;naive bayes classifier;protection ring;sensor;supervised learning;support vector machine	FuYong Zhang;DeYu Qi	2011	Journal in Computer Virology	10.1007/s11416-011-0154-8	computer science;artificial intelligence;machine learning;data mining;computer security	ML	6.990383068884386	-37.272803094742045	182001
6b4eb31aef567175748ad17e1f55a031b20c2e6c	extended tolerance relation to define a new rough set model in incomplete information systems	mathematical property;attribute value;extended tolerance relation;information system;incomplete information system;tolerance relation;rough set model;rough set extension;new rough set model;paper discusses	mathematical property;attribute value;extended tolerance relation;information system;incomplete information system;tolerance relation;rough set model;rough set extension;new rough set model;paper discusses	information system;rough set	Do Van Nguyen;Koichi Yamada;Muneyuki Unehara	2013	Adv. Fuzzy Systems	10.1155/2013/372091	data mining;algorithm;dominance-based rough set approach	DB	-2.424099888637728	-24.438999106927643	182049
9ca74e2d9bf42f0716b2c2b6a61dfb2478cd3214	outliers detection and confidence interval modification in fuzzy regression	valeur limite;limite tolerance;organigramme;linear estimation;fuzzy regression;fuzzy set;flowchart;regression lineaire floue;confidence limit;limite tolerancia;programmation lineaire floue;estimation non parametrique;estimacion lineal;logique floue;linear regression;logica difusa;outlier;data type;result;classification;statistical regression;outlier detection;fuzzy logic;observacion aberrante;non parametric estimation;estimation lineaire;confidence interval;programacion lineal;analisis regresion;tolerance limit;limiting value;mathematical programming;fuzzy linear regression;regresion estadistica;fonction appartenance;regresion lineal;limite confianza;estimacion parametro;membership function;linear programming;programmation lineaire;resultado;analyse regression;observation aberrante;fuzzy linear programming;regression analysis;resultat;estimacion no parametrica;funcion pertenencia;parameter estimation;estimation parametre;estimation statistique;k limiting value;regression statistique;estimacion estadistica;programmation mathematique;limite confiance;statistical estimation;organigrama;programacion matematica;clasificacion;regression lineaire	The existence of outliers in a set of experimental data can cause incorrect interpretation of the fuzzy linear regression results. Peters (Fuzzy Sets and Systems 63 (1994) 45–55) considered this problem for the nonfuzzy input and nonfuzzy output data type. The present investigation focuses on nonfuzzy input and fuzzy output data type and proposes approaches to handle the outlier problem. The main idea is to introduce a pre-assigned k-limiting value whose value must be determined based on the conditions of the current problem. The advantages and problems of the proposed approaches are compared and discussed. c © 2001 Elsevier Science B.V. All rights reserved.	fuzzy sets and systems;realization (probability);sensor	Yun-Shiow Chen	2001	Fuzzy Sets and Systems	10.1016/S0165-0114(99)00049-4	econometrics;confidence interval;type-2 fuzzy sets and systems;computer science;linear programming;mathematics;fuzzy set operations;algorithm;regression analysis;statistics	AI	9.401923860768223	-28.829475450949097	182087
d7cff56b79798ebaa104d83db3e1695c7a1a7c07	imprecise imputation as a tool for solving classification problems with mean values of unobserved features	unobserved feature;mean value;selected distribution;available information;partial information;imprecise imputation;classification problem;numerical example;monte carlo technique;minimin strategy;probability distribution	A method for solving a classification problem when there is only partial information about some features is proposed. This partial information comprises the mean values of features for every class and the bounds of the features. In order to maximally exploit the available information a set of probability distributions is constructed such that two distributions are selected from the set which define the minimax and minimin strategies. Random values of features are generated in accordance with the selected distributions by using Monte Carlo technique. As a result, the classification problem is reduced to the standard model which is solved by means of the support vector machine. Numerical examples illustrate the proposed method.		Lev V. Utkin;Yulia A. Zhuk	2013	Adv. Artificial Intellegence	10.1155/2013/176890	machine learning;pattern recognition;data mining	ML	1.5580212294328626	-30.09601085065284	182092
a8877d1ef150287ed11ef990d6d26c6791f69597	application of a genetic algorithm to nearest neighbour classification	census;hombre;intelligence artificielle;algoritmo genetico;classification;vecino mas cercano;human;algorithme genetique;plus proche voisin;artificial intelligence;nearest neighbour;genetic algorithm;censo;peritaje;recensement;inteligencia artificial;expertise;clasificacion;variance;homme;variancia	This paper describes the application of a genetic algorithm to nearest-neighbour based imputation of sample data into a census data dataset. The genetic algorithm optimises the selection and weights of variables used for measuring distance. The results show that the measure of fit can be improved by selecting imputation variables using a genetic algorithm. The percentage of variance explained in the goal variables increases compared to a simple selection of imputation variables. This quantitative approach to the selection of imputation variables does not deny the importance of expertise. Human expertise is still essential in defining the optional set of imputation variables.		Semen Simkin;Tim Verwaart;Hans Vrolijk	2005		10.1007/11504894_73	genetic algorithm;census;biological classification;computer science;artificial intelligence;machine learning;data mining;variance;imputation;statistics	AI	9.552907547309747	-33.845521665079005	182339
9ba8da2c8585b7cd856a73688128071504c6cc88	an inductive learning algorithm in fuzzy systems	learning environment;fuzzy logic;automatic learning;environment;truth maintenance system;inductive learning;fuzzy system;fuzzy model	The aim of this paper is to present a method for identifying the structure of a rule in a fuzzy model. For this purpose, an ATMS shall be used. An algorithm obtaining the identiication of the structure will be suggested. The minimal structure of the rule (with respect to the number of variables that must appear in the rule) will be found by this algorithm. Furthermore, the identiication parameters shall be obtained simultaneously. The proposed method shall be applied for classiication to an example. The Iris Plant Database shall be learnt for all three kinds of plants.	algorithm;fuzzy control system;inductive reasoning	Juan Luis Castro;Jose Manuel Zurita	1997	Fuzzy Sets and Systems	10.1016/S0165-0114(96)00106-6	fuzzy logic;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;natural environment;fuzzy set operations;fuzzy control system	AI	2.849482768290784	-26.377404871930157	182374
a861548de4f8224330081e0057610f749c2d6aaa	a class-selective rejection scheme based on blockwise similarity of typicality degrees	kernel;integral equations;prototypes;triangular norm;glass;mathematical operators;distance measurement;discrete sugeno integral class selective rejection scheme classifier performance pattern classification;laboratories fuzzy sets pattern classification aggregates measurement uncertainty decision making pattern recognition;pattern classification;pattern recognition;iris;labeling;pattern classification integral equations mathematical operators;sugeno integral	Overlapping classes and outliers can significantly decrease a classifier performance. We address here the problem of giving a classifier the ability to reject some patterns either for ambiguity or for distance in order to improve its performance. Given a set of typicality degrees for a pattern to be classified, we use an operator based on triangular norms and a discrete Sugeno integral to quantify their blockwise similarities. We propose a new class-selective rejection scheme which uses this operator outputs. We present the resulting algorithm which allows to assign a pattern to zero, one or several classes, and show its efficiency on real data sets.	algorithm;pattern recognition;rejection sampling;scheme;similarity learning;statistical classification;sugeno integral	Hoel Le Capitaine;Carl Frélicot	2008	2008 19th International Conference on Pattern Recognition	10.1109/ICPR.2008.4761813	labeling theory;discrete mathematics;kernel;machine learning;pattern recognition;mathematics;prototype;glass;integral equation	Vision	1.4406532735648014	-29.04913190675278	182380
162aaf35a7c2259768fbcbcaceadcbb4aca1338a	a relative tolerance relation of rough set for incomplete information systems		Rough set theory is an effective approach to imprecision, vagueness, and uncertainty. This theory overlaps with many other theories such that fuzzy sets, evidence theory, and statistics. From a practical point of view, it is a good tool for data analysis. However, classical rough set theory cannot cope with the incomplete information systems where some attribute values are missing. There have been efforts in studying incomplete information systems for data classification which are based on the extensions of rough set theory. Moreover, the existing approaches have their weaknesses in terms of inflexible and imprecise in data classifications. To overcome these issues, we propose a relative tolerance relation of rough set (RTRS) to handling incomplete information systems, which it has flexibility and precisely for data classification. We compared RTRS with the existing approaches, the results show that our proposed method relatively achieves higher flexibility and precisely in data classification in incomplete information systems.	information system;rough set	Rd Rohmat Saedudin;Hairulnizam Mahdin;Shahreen Kasim;Edi Sutoyo;Iwan Tri Riyadi Yanto;Rohayanti Hassan	2018		10.1007/978-3-319-72550-5_8	artificial intelligence;complete information;machine learning;data classification;fuzzy set;tolerance relation;computer science;rough set;vagueness	DB	-2.3583420441465024	-26.477574894873463	182487
526d96b835b07cc2e4c711426b0feea3b61e7f2b	extending cc4 neural networks to classify real life documents	cc4 neural network;dimensionality reduction;document classification		neural networks;real life	Enhong Chen;Zhenya Zhang;Hans-Dieter Burkhard;Gabriela Lindemann von Trzebiatowski	2004	Informatica (Slovenia)		machine learning;artificial neural network;data mining;artificial intelligence;computer science	ML	8.470705764947331	-36.99329923965295	182610
80905c224b9c4996914bf1297e96e196794e7c6b	growing self-organizing maps for data analysis.	data analysis;self organized map	Currently, there exist many research areas that produce large multivariable datasets that are difficult to visualize in order to extract useful information. Kohonen selforganizing maps have been used successfully in the visualization and analysis of multidimensional data. In this work, a projection technique that compresses multidimensional datasets into two dimensional space using growing self-organizing maps is described. With this embedding scheme, traditional Kohonen visualization methods have been implemented using growing cell structures networks. New graphical map displays have been compared with Kohonen graphs using two groups of simulated data and one group of real multidimensional data selected from a satellite scene.	existential quantification;organizing (structure);self-organization;self-organizing map	Soledad Delgado;Consuelo Gonzalo;Estibaliz Martinez;Agueda Arquero	2009		10.4018/978-1-59904-849-9.ch116	multivariable calculus;visualization;exploratory data analysis;artificial neural network;embedding;machine learning;artificial intelligence;unsupervised learning;self-organizing map;computer science;graph	Visualization	1.293690358035832	-37.45955045999605	182654
46968ae7805c90a89908c5d4b5516362f89cd04b	semi-supervised learning in knowledge discovery	unlabeled data;engineering;analisis imagen;fuzzy classification;fuzzy set;procesamiento informacion;learning;conjunto difuso;ensemble flou;data mining;classification;semi supervised learning;ingenierie;algorithme;aprendizaje;algorithm;apprentissage;classification rules;fuzzy classification rules;information processing;ingenieria;image analysis;sistema difuso;systeme flou;evolutionary algorithm;traitement information;analyse image;clasificacion;fuzzy system;algoritmo;knowledge discovery	Recently, semi-supervised learning has received quite a lot of attention. The idea of semi-supervised learning is to learn not only from the labeled training data, but to exploit also the structural information in additionally available unlabeled data. In this paper we review existing semi-supervised approaches, and propose an evolutionary algorithm suited to learn interpretable fuzzy if-then classification rules from partially labeled data. Feasibility of our approach is shown on artificial datasets, as well as on a real-world image analysis application.		Aljoscha Klose;Rudolf Kruse	2005	Fuzzy Sets and Systems	10.1016/j.fss.2004.07.016	semi-supervised learning;image analysis;information processing;biological classification;fuzzy classification;computer science;artificial intelligence;machine learning;evolutionary algorithm;data mining;fuzzy set;fuzzy control system	ML	8.931852833986303	-31.695265585300746	182747
7337c442d71304a722ab28f240a5c5b267ea71bb	an application of fuzzy sets to the classification of geometric figures and chromosome images	fuzzy set	The concept of a fuzzy set is applied to the classification of geometric figures and chromosome images through the use of shape-oriented angular and dimensional proximity measures. Various properties of approximate isosceles, approximate equilateral, approximate right, and approximate isosceles right triangles are investigated. A method for classifying a triangle as an ''approximate right triangle,'' ''approximate isosceles triangle,'' ''approximate isosceles right triangle,'' ''approximate equilateral triangle,'' or ''ordinary triangle'' is presented. A method used to classify a quadrangle as ''approximate square,'' ''approximate rectangle,'' ''approximate rhombus,'' ''approximate parallelogram,'' ''approximate trapezoid,'' or ''ordinary quadrangle'' is also presented. The measures of proximity employed for this purpose have an intuitive interpretation. The stochastic syntactic analysis technique and the ''rubber-mask'' technique have, in the past, been applied to the classification of chromosome images. In this paper, chromosome images are classified through the use of angular and dimensional proximity measures. The uttermost dimensional proximity and the least dimensional dissimilarity of chromosome images are defined and investigated. The results obtained in this paper may contribute to processing a picture from the polygonal approximation stage to the final classification stage in order to recognize a picture.	fuzzy set	Edward T. Lee	1976	Inf. Sci.	10.1016/S0020-0255(76)90650-2	mathematical optimization;combinatorics;topology;computer science;artificial intelligence;machine learning;mathematics;geometry;fuzzy set	Vision	-0.7188055112527726	-25.13496958269487	182785
cd3fb06d7d5651fca2e21dfd4a7d40df9f3fd3f8	some experiments in the use of clustering for data validation	detection erreur;metodo estadistico;deteccion error;base donnee;dato;data;database;base dato;statistical method;donnee;methode statistique;validation;information system;error detection;classification automatique;automatic classification;clasificacion automatica;data validation;systeme information;sistema informacion	Previous work has demonstrated the feasibility of using clustering to detect errors in small databases. Singleton clusters, containing only one record, often represent errors. Use of a new clustering algorithm oriented to this application provided improvements in time complexity without degradation of the error detection performance in a larger collection of data. Key wor& Clustering. data errors, data validation	algorithm;cluster analysis;computer cluster;data validation;database;elegant degradation;error detection and correction;experiment;time complexity	William F. Storer;Caroline M. Eastman	1990	Inf. Syst.	10.1016/0306-4379(90)90026-L	correlation clustering;data stream clustering;error detection and correction;computer science;canopy clustering algorithm;data validation;cure data clustering algorithm;data mining;database;cluster analysis;information system;algorithm;statistics;data;clustering high-dimensional data	DB	-2.645132256145479	-34.3099519482362	182942
759fd4b3b147098d30cf02ec8cead15eeb84acb9	classification of symbolic objects: a lazy learning approach	classification algorithm;relational database;data mining;lazy learning;symbolic data analysis;nearest neighbor;k nearest neighbor	Symbolic data analysis aims at generalizing some standard statistical data mining methods, such as those developed for classification tasks, to the case of symbolic objects (SOs). These objects synthesize information concerning a group of individuals of a population, eventually stored in a relational database, and ensure confidentiality of original data. Classifying SOs is an important task in symbolic data analysis. In this paper a lazy-learning approach that extends a traditional distance weighted k-Nearest Neighbor classification algorithm to SOs, is presented. The proposed method has been implemented in the system SO-NN (Symbolic Objects Nearest Neighbor) and evaluated on symbolic datasets.	adaptive behavior;algorithm;approximation algorithm;artificial neural network;benchmark (computing);confidentiality;cryptography;data mining;decision tree;k-nearest neighbors algorithm;lazy evaluation;lazy learning;missing data;relational data mining;relational database;relevance;symbolic computation;symbolic data analysis;table (information);workbench;psos	Annalisa Appice;Claudia d'Amato;Floriana Esposito;Donato Malerba	2006	Intell. Data Anal.		computer science;machine learning;pattern recognition;data mining;symbolic data analysis;k-nearest neighbors algorithm	ML	-0.4045237164505684	-32.219251895812455	183135
151402061b00aa64c6c5cf0352e3c19479de931a	the application of emerging patterns for improving the quality of rare-class classification	base donnee;nouveaute;novelty;database;base dato;novedad;classification;defaillance;experimental evaluation;failures;fallo;clasificacion	The classification of rare cases is a challenging problem in many real life applications. The scarcity of the rare cases makes it difficult for traditional classifiers to classify them correctly. In this paper, we propose a new approach to use emerging patterns (EPs) [3] in rare-class classification (EPRC). Traditional EP-based classifiers [2] fail to achieve accepted results when dealing with rare cases. EPRC overcomes this problem by applying three improving stages: generating new undiscovered EPs for the rare class, pruning low-support EPs, and increasing the supports of the rare-class EPs. An experimental evaluation carried out on a number of rare-class databases shows that EPRC outperforms EP-based classifiers as well as other classification methods such as PNrule [1], Metacost [6], and C4.5 [7].	c4.5 algorithm;database;expectation propagation;real life	Hamad Alhammady;Kotagiri Ramamohanarao	2004		10.1007/978-3-540-24775-3_27	biological classification;computer science;artificial intelligence;machine learning;data mining;database	ML	8.96198896629093	-35.53464228434477	183202
850d2b72372c8d2265d0ac2a22b94be628615d88	medical diagnosis based on distance measures between picture fuzzy sets			fuzzy set	Palash Dutta	2018	IJFSA	10.4018/IJFSA.2018100102		Vision	1.9646088975119171	-25.504883630458895	183285
c82026862593dd22e8d4fb6a619396d946773d24	a genetic algorithm based searching of maximal frequent itemsets	frequent itemset;genetic algorithm		genetic algorithm;maximal set	Jen-peng Huang;Che-Tsung Yang;Chih-Hsiung Fu	2004			genetic algorithm;computer science;pattern recognition;artificial intelligence	DB	-0.5634802864582312	-33.22913912139185	183375
6e253bc53f9b9493b01e5041f10b57fdd889353f	iot sensing parameters adaptive matching algorithm		As the ‘Industry 4.0’ and ‘Made in China 2025’ has been put forward, the need of the large-scale system integration for Internet of Things (IoT) has been more and more urgent. At present, different IoT systems have different database types, table structures and denominating rules for sensing parameters. So for the existing IoT system integration, there are such as sensing parameter’s conversion difficulty, complex matching process, low integrating efficiency issues. To solve these problems, we propose a novel model for IoT sensing parameter automatically matching which can achieve the IoT system integration on a large-scale. Meanwhile combining KNN thought, using a weighted method to improve the KNN algorithm, we put forward the automatic IoT sensing parameters matching algorithm. By the multiple practical IoT system integration cases, we validate the rationality and efficiency of the model and the algorithm. The result shows that the model and the algorithm are feasible and efficient. They realize the rapid automatic matching for the heterogeneous IoT sensing parameters, improving the IoT system’s integration efficiency. It is conducive to the large-scale heterogeneous IoT system quick integration and has great significance to promote the IoT’s application in large scale.	algorithm	Zhijin Qiu;Naijun Hu;Zhongwen Guo;Like Qiu;Shuai Guo;Xi Wang	2016		10.1007/978-3-319-42553-5_17	machine learning;computer science;artificial intelligence;system integration;blossom algorithm;k-nearest neighbors algorithm;internet of things	Robotics	-2.125532279664058	-28.88326574584943	183536
c05b8f79a2dad172a64f7be8fdb7c3858bd36c89	an automatic identifier of confinement regimes at jet combining fuzzy logic and classification trees	classification and regression tree;classification tree;fuzzy logic	In modern thermonuclear fusion devices it is possible to distinguish distinct types of plasma confinement regimes which have different performance in terms of confinement time. Discriminating among them could represent a useful feature for an efficient control of a plasma experiment. An automatic identifier based on fuzzy logic is here proposed together with an unsupervised technique, using classification and regression trees, for selecting, among several diagnostic signals available, the inputs to be provided to the identifier.	decision tree;feature selection;fuzzy logic;identifier;jet;plasma active;unsupervised learning	Guido Vagliasindi;Paolo Arena;Luigi Fortuna;Andrea Murari;Giuseppe Mazzitelli;Antonio Gallo;Umberto Vagliasindi	2008			fuzzy logic;decision tree learning;fuzzy classification;computer science;machine learning;pattern recognition;data mining;mathematics	SE	9.020816462987593	-37.13557567014395	183816
bbdbb44d481a70601394b124d407fda78c4ae634	neural networks	connectionism and neural nets;design;experimentation;neural nets;theory	This paper presents a comprehensive overview of modelling, simulation and implementation of neural networks, taking into account that two aims have emerged in this area: the improvement of our understanding of the behaviour of the nervous system and the need to find inspiration from it to build systems with the advantages provided by nature to perform certain relevant tasks. The development and evolution of different topics related to neural networks is described (simulators, implementations, and real-world applications) showing that the field has acquired maturity and consolidation, proven by its competitiveness in solving real-world problems. The paper also shows how, over time, artificial neural networks have contributed to fundamental concepts at the birth and development of other disciplines such as Computational Neuroscience, Neuro-engineering, Computational Intelligence and Machine Learning. A better understanding of the human brain is considered one of the challenges of this century, and to achieve it, as this paper goes on to describe, several important national and multinational projects and initiatives are marking the way to follow in neural-network research.	neural networks	Markus Spitzer	1994	Medizinische Monatsschrift fur Pharmazeuten	10.1007/978-1-4899-7687-1_586	machine learning;physical neural network;artificial neural network;types of artificial neural networks;artificial intelligence;computer science	ECom	7.173583871140652	-24.677329308992206	183893
d6dd9b78e80c33fcc65b953a499bf70d0d4a7e45	a learning-based algorithm selection meta-reasoner for the real-time mpe problem	selection model;modelizacion;evaluation performance;selection problem;problema seleccion;learning algorithm;algorithm performance;performance evaluation;apprentissage inductif;learning model;real time;evaluacion prestacion;inductive logic programming;intelligence artificielle;algorithme apprentissage;modelisation;aprendizaje por induccion;resultado algoritmo;temps reel;inductive learning;most probable explanation;performance algorithme;tiempo real;artificial intelligence;inteligencia artificial;algoritmo aprendizaje;modeling;programmation logique inductive;probleme selection	The algorithm selection problem aims to select the best algorithm for an input problem instance according to some characteristics of the instance. This paper presents a learning-based inductive approach to build a predictive algorithm selection system from empirical algorithm performance data of the Most Probable Explanation(MPE) problem. The learned model can serve as an algorithm selection meta-reasoner for the real-time MPE problem. Experimental results show that the learned algorithm selection models can help integrate multiple MPE algorithms to gain a better overall performance of reasoning.	algorithm selection;approximation algorithm;cpt (file format);computable function;computation;computational resource;exact algorithm;machine learning;np-hardness;polynomial;predictive modelling;problem solving;real-time clock;real-time transcription;sampling (signal processing);selection algorithm;semantic reasoner;time complexity	Haipeng Guo;William H. Hsu	2004		10.1007/978-3-540-30549-1_28	systems modeling;computer science;artificial intelligence;machine learning;fsa-red algorithm;algorithm;population-based incremental learning	AI	9.388848293128637	-31.575125784420955	184359
1e767a489e21a39ba8ffd7954c34484d90c6a216	use of algorithms for a user specific reduction of amounts of interesting association rules	association rule		algorithm	Birgit Wenke	2010			association rule learning;machine learning;artificial intelligence;pattern recognition;computer science	DB	-0.7526786780373901	-33.144835119086494	184575
1f48557cd3054e585f9971cc17c458bb16a8926b	ordinal association rules for error identification in data sets	association rules;data mining;association rule;data cleansing;ordinal rules	A new extension of the Boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. One use for ordinal rules is to identify possible errors in data. A method that finds these rules and identifies potential errors in data is proposed.	association rule learning;ordinal data	Andrian Marcus;Jonathan I. Maletic;King-Ip Lin	2001		10.1145/502585.502700	ordinal regression;association rule learning;computer science;pattern recognition;data mining;ordinal data;statistics	DB	-0.32866223301002506	-31.831990580717346	184696
4c9659925f653952e2c335784b249d168878648a	enhancing the generality of fuzzy relational models for control	systeme commande;model based reasoning;sistema control;raisonnement base sur modele;fuzzy controller;control difusa;algoritmo borroso;real time;fuzzy control;fuzzy relation;fuzzy modelling control;chemical engineering;input output;control proceso;control system;fuzzy algorithm;relational model;model based control;process control;algorithme flou;process model;algebre relationnelle;ph control;on line control;relational algebra;commande processus;relational modelling;commande floue	A promising area of research in fuzzy control is the model-based fuzzy controller. At the heart of this approach is a fuzzy relational model of the process to be controlled. Since this model is identified directly from process input-output data it is likely that 'holes' will be present in the identified relational model. These holes pose real problems when the model is incorporated into a model-based controller since the model will be unable to make any predictions whatsoever if the system drifts into an unknown region. The present work deals with the completeness of the fuzzy relational model which forms the core of the controller. This work proposes a scheme of post-processing to 'fill in' the fuzzy relational model once it has been built and thereby improve its applicability for on-line control. A comparative study of the post-processed model and conventional relational model is presented for Box-Jenkins data identification system and a real-time, highly non-linear application of pH control identification.		B. Kelkar;Bruce E. Postlethwaite	1998	Fuzzy Sets and Systems	10.1016/S0165-0114(97)00045-6	input/output;relational model;statistical relational learning;defuzzification;type-2 fuzzy sets and systems;relational algebra;fuzzy classification;computer science;control system;artificial intelligence;fuzzy number;model-based reasoning;process control;process modeling;control theory;mathematics;fuzzy set operations;logical data model;fuzzy control system	AI	8.7641361594264	-28.163242610053814	185248
38b0877fa6ac3ebfbb29d74f761fea394ee190f3	lazy decision trees	decision tree;learning;lazy learning;artificial intelligence;algorithms;decision tree analysis;mathematics computers information science management law miscellaneous	Lazy learning algorithms, exemplified by nearestneighbor algorithms, do not induce a concise hypothesis from a given training set; the inductive process is delayed until a test instance is given. Algorithms for constructing decision trees, such as C4.5, ID3, and CART create a single “best” decision tree during the training phase, and this tree is then used to classify test instances. The tests at the nodes of the constructed tree are good on average, but there may be better tests for classifying a specific instance. We propose a lazy decision tree algorithm-LAzuDT-that conceptually constructs the “best” decision tree for each test instance. In practice, only a path needs to be constructed, and a caching scheme makes the algorithm fast. The algorithm is robust with respect to missing values without resorting to the complicated methods usually seen in induction of decision trees. Experiments on real and artificial problems are presented.	c4.5 algorithm;decision tree learning;experiment;id3 algorithm;lazy evaluation;lazy learning;list of algorithms;machine learning;mathematical induction;missing data;test set	Jerome H. Friedman;Ron Kohavi;Yeogirl Yun	1996			instance-based learning;influence diagram;decision tree model;decision tree learning;computer science;artificial intelligence;machine learning;decision tree;alternating decision tree;incremental decision tree;data mining;search tree;id3 algorithm;tree traversal;weighted sum model;decision stump	ML	5.528566498489193	-32.38301709297531	185359
bb8cbb897d4c55e3680646b1c7f61b9ccf6f2484	rough sets and functional dependencies in data: foundations of association reducts	reducts;approximate functional dependencies;functional dependency;rough sets;rough set	We investigate the notion of an association reduct. Association reducts represent data-based functional dependencies between the sets of attributes, where it is preferred that possibly smallest sets determine possibly largest sets. We compare the notions of an association reduct to other types of reducts previously studied within the theory of rough sets. We focus particularly on modeling inexactness of dependencies, which is crucial for many real-life data applications. We also study the optimization problems and algorithms that aim at searching for the most interesting approximate association reducts in data.	approximation algorithm;association rule learning;functional dependency;mathematical optimization;optimization problem;real life;rough set	Dominik Slezak	2009	Trans. Computational Science	10.1007/978-3-642-02097-1_10	discrete mathematics;pattern recognition;data mining;mathematics	DB	-3.290846101429356	-28.143864460022858	185407
2ee0743424ce45acbed215025b243a50682b930a	from data to knowledge: probabilist objects for a symbolic data analysis		The main aim of the symbolic approach in data analysis is to extend problems, methods and algorithms used on classical data to more complex data called “symbolic objects” which are well adapted to representing knowledge and which can “unify” unlike usual observations which characterize “individual things”. We introduce two kinds of symbolic objects: boolean and probabilist. We briefly present some of their qualities and properties; a theorem shows how Probability, theory may be extended on these objects. Finally four kinds of data analysis problems including the symbolic extension are illustrated by several algorithms which induce knowledge from classical data or from a set of symbolic objects.	symbolic data analysis	Edwin Diday	1993			data mining;database;symbolic data analysis	ML	-3.9748311876993014	-26.865302049038426	185450
b0774b91758b4fc02c5aa26fd991aeb43105580a	a fuzzy multistage evolutionary (fume) clustering technique	analyse amas;base donnee;multistage;technology;vus classification;database;theorie ensemble;speech;multietage;set theory;classification;fuzzy clustering;classification vus;cluster analysis;multistage clustering;technologie;n purlieus relation;parole;min max clustering;clasificacion;tecnologia	In this paper, a multistage evolutionary scheme is proposed for clustering in a large data base, like speech data. This is achieved by clustering a small subset of the entire sample set in each stage and treating the cluster centroids so obtained as samples, together with another subset of samples not considered previously, as input data to the next stage. This is continued till the whole sample set is exhausted. The clustering is accomplished by constructing a fuzzy similarity matrix and using the fuzzy techniques proposed here. The technique is illustrated by an efficient scheme for voiced-unvoiced-silence classification of speech.	cluster analysis;multistage amplifier	B. Bharathi Devi;V. V. S. Sarma	1984	Pattern Recognition Letters	10.1016/0167-8655(84)90037-0	correlation clustering;constrained clustering;data stream clustering;k-medians clustering;fuzzy clustering;flame clustering;computer science;speech;canopy clustering algorithm;machine learning;pattern recognition;cure data clustering algorithm;data mining;mathematics;cluster analysis;single-linkage clustering;set theory;clustering high-dimensional data;technology	Vision	2.5576686047857824	-37.0250694932736	185594
a96ce646b58ae3c9828d3993afbf0b8ad5e99780	on the choice of the pair conjunction–implication into the fuzzy morphological edge detector	detectors;hysteresis;fuzzy gradient fuzzy mathematical morphology edge detection;morphology;statistical analysis;image edge detection;ip networks;statistical analysis edge detection fuzzy set theory gradient methods;idempotent uninorm pair conjunction implication fuzzy morphological edge detector fuzzy morphological gradients fuzzy mathematical morphologies t norms conjunctive uninorms edge detection applications three step algorithm pratt figure of merit statistical analysis kleene dienes implication;algorithm design and analysis;image edge detection detectors morphology algorithm design and analysis ip networks hysteresis statistical analysis	In this paper, the fuzzy morphological gradients from the fuzzy mathematical morphologies based on t-norms and conjunctive uninorms are deeply analyzed in order to establish which pair of conjunction and fuzzy implications are optimal, in accordance with their performance in edge detection applications. A novel three-step algorithm based on the fuzzy morphology is proposed. The comparison is performed by means of the so-called Pratt's figure of merit. In addition, a statistical analysis is carried out to study the relationship between the different configurations and to establish a classification of the conjunctions and implications considered. Both the objective measure and the statistical analysis conclude that the pairs nilpotent minimum t-norm and the Kleene-Dienes implication, and the idempotent uninorm obtained with the classical negation as a generator and its residual implication, are the best configurations in this approach, because they also obtain competitive results with respect to other approaches.	edge detection;idempotence;knuth–morris–pratt algorithm;mathematical morphology;morphological gradient;t-norm	Manuel González Hidalgo;Sebastià Massanet;Arnau Mir;Daniel Ruiz-Aguilera	2015	IEEE Transactions on Fuzzy Systems	10.1109/TFUZZ.2014.2333060	algorithm design;detector;combinatorics;discrete mathematics;morphology;defuzzification;hysteresis;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;algorithm	Visualization	0.22993763542142262	-24.275367290776714	185826
be1b9015e9a13d529096f1e64a9cfa2ea3d6f083	a new model for linguistic modifiers	linguistic variable;fuzzy set;fuzzy logic;natural language;single positive	Abstract   This paper presents a new model for the representation of linguistic modifiers. Each modifiers is characterized by three parameters and is identified by a single positive real number n. The number n fully characterizes the generic term of a boolean linguistic variable. The representation of the terms maintains the order relation with which they are used in natural language. Lastly, it is shown how this new model allows the fuzzy set of each boolean expression to be evaluated through suitably simple fuzzy functions.		Luigi Di Lascio;Antonio Gisolfi;Vincenzo Loia	1996	Int. J. Approx. Reasoning	10.1016/0888-613X(95)00133-2	fuzzy logic;natural language processing;fuzzy classification;computer science;artificial intelligence;fuzzy number;machine learning;mathematics;fuzzy set;natural language;fuzzy set operations;algorithm	NLP	0.23823113124807926	-24.04631370503573	185922
4013b2b31d4a52cce422b86c3553d3e95f8e63b3	path bitmap indexing for retrieval of xml documents	document structure;evaluation performance;analyse amas;performance evaluation;estructura documental;systeme aide decision;structure document;fundamental unit;xml language;evaluacion prestacion;intelligence artificielle;sistema ayuda decision;classification;three dimensional;similitude;indexing method;decision support system;cluster analysis;recherche documentaire;indexing;busqueda documental;indexation;estructura datos;similarity;indizacion;xml document;artificial intelligence;analisis cluster;structure donnee;document retrieval;inteligencia artificial;similitud;data structure;clasificacion;langage xml;lenguaje xml	The path-based indexing methods such as the three-dimensional bitmap indexing have been used for collecting and retrieving the similar XML documents. To do this, the paths become the fundamental unit for constructing index. In case the document structure changes, the path extracted before the change and the one after the change are regarded as totally different ones regardless of the degree of the change. Due to this, the performance of the path-based indexing methods is usually bad in retrieving and clustering the documents which are similar. A novel method which can detect the similar paths is needed for the effective collecting and retrieval of XML documents. In this paper, a new path construction similarity which calculates the similarity between the paths is defined and a path bitmap indexing method is proposed to effectively load and extract the similar paths. The proposed method extracts the representative path from the paths which are similar. The paths are clustered using this, and the XML documents are also clustered using the clustered paths. This solves the problem of existing three-dimensional bitmap indexing. Through the performance evaluation, the proposed method showed better clustering accuracy over existing methods.	bitmap;xml	Jae-Min Lee;Byung-Yeon Hwang	2006		10.1007/11681960_32	document retrieval;xml;decision support system;data structure;computer science;artificial intelligence;data mining;database;information retrieval	DB	-4.438068796564755	-32.84726068194538	186241
dcb0d892f0b9587d4799d75175db73d61ecd7b60	a novel z-soft rough fuzzy bci-algebras (ideals) of bci-algebras			brain–computer interface	Kuanyun Zhu;Bao Qing Hu	2018	Soft Comput.	10.1007/s00500-017-2816-z	fuzzy logic;brain–computer interface;machine learning;theoretical computer science;artificial intelligence;computer science	ECom	2.1617995132161503	-24.564045960045963	186250
523d2d496b87a8b26383a7c6d3abf8d271ec5130	statistical analysis for human preference to colors	freedman s test;thurstone s method of paired comparisons;kendal s coefficient of concordance;method of rank order		color	Taki Kanda	2011	JACIII	10.20965/jaciii.2011.p0433	artificial intelligence;computer science;pattern recognition;machine learning	Robotics	2.2558505999920584	-24.195894482109125	186415
6a5b83a79e5a1d0e01e84c704b7168a61797d801	a uniform framework for rough approximations based on generalized quantifiers	fuzzy set;proceedings paper;variable precision rough set;rough set;fuzzy cardinality	The rough set theory provides an effective tool for decision analysis in the way of extracting decision rules from information systems. The rule induction process is based on the definitions of lower and upper approximations of the decision class. The condition attributes of the information system constitute an indiscernibility relation on the universe of objects. An object is in the lower approximation of the decision class if all objects indiscernible with it are in the decision class and it is in the upper approximation of the decision class if some objects indiscernible with it are in the decision class. Various generalizations of rough set theory have been proposed to enhance the capability of the theory. For example, variable precision rough set theory is used to improve the robustness of rough set analysis and fuzzy rough set approach is proposed to deal with vague information. In this paper, we present a uniform framework for different variants of rough set theory by using generalized quantifiers. In the framework, the lower and upper approximations of classical rough set theory are defined with universal and existential quantifiers respectively, whereas variable precision rough approximations correspond to probability quantifiers. Moreover, fuzzy rough set approximations can be defined by using different fuzzy quantifiers. We show that the framework can enhance the expressive power of the decision rules induced by rough set-based decision analysis.	approximation	Tuan-Fang Fan;Churn-Jung Liau;Duen-Ren Liu	2015	Trans. Rough Sets	10.1007/978-3-662-47815-8_1	combinatorics;discrete mathematics;rough set;fuzzy classification;data mining;mathematics;fuzzy set operations;dominance-based rough set approach	EDA	-2.040407148225074	-25.148161141778008	186509
073ac0377845e8259e72ad912e9f63a5db6e5799	batch som algorithms for interval-valued data with automatic weighting of the variables	self organizing maps;interval valued data;adaptive distances;symbolic data analysis;batch training algorithms	Interval-valued data is most utilized to represent either the uncertainty related to a single measurement, or the variability of the information inherent to a group rather than an individual. In this paper, we focus on Kohonen self-organizing maps (SOMs) for interval-valued data, and design a new Batch SOM algorithm that optimizes an explicit objective function. This algorithm can handle, respectively, suitable City-Block, Euclidean and Hausdorff distances with the purpose to compare interval-valued data during the training of the SOM. Moreover, most often conventional batch SOM algorithms consider that all variables are equally important in the training of the SOM. However, in real situations, some variables may be more or less important or even irrelevant for this task. Thanks to a parameterized definition of the above-mentioned distances, we propose also an adaptive version of the new algorithm that tackles this problem with an additional step where a relevance weight is automatically learned for each interval-valued variable. Several examples with synthetic and real interval-valued data sets illustrate the usefulness of the two novel batch SOM algorithms.	algorithm	Francisco de A. T. de Carvalho;Patrice Bertrand;Eduardo C. Simões	2016	Neurocomputing	10.1016/j.neucom.2015.11.084	self-organizing map;computer science;artificial intelligence;machine learning;data mining;symbolic data analysis	Logic	1.8787577681645158	-30.828440620622438	186541
cff9d80336fc0136d077242fa2ce8338ba8457b0	semantic artificial immune model for fault diagnosis	artificial immune system;semantics;lymphocyte;fault diagnosis	Applying artificial immune system to fault diagnosis is a new development direction in artificial intelligence, but the traditional artificial immune mode could not reasonably reflect the semantic similarity in the complexity problem space. For issues of semantic description of fault diagnosis, this paper introduces group cooperative mechanism of lymphocyte with a semantic tag to artificial immune system, thus solves the problem of semantic logical reasoning of fault knowledge. This paper presents a semantic-based artificial immune diagnosis model; designs an immune negative selection diagnostic in semantic environment; utilizes new coevolutionary algorithm diagnostic; and diagnoses fault in large electromechanical devices. The experimental results show that the method used in this paper has higher classification accuracy compared with the traditional artificial immune diagnostics, at the same time, verified expression capacity of semanticbased lymphocytes, which can provide more valuable diagnostic information. Index Terms —Fault diagnosis; Artificial immune system; Lymphocyte; Semantics	algorithm;artificial immune system;artificial intelligence;problem domain;semantic similarity	Chu-jiao Wang;Shixiong Xia;Yong Zhou	2013	JCP	10.4304/jcp.8.8.2059-2068	computer science;artificial intelligence;machine learning;semantics;artificial immune system	AI	3.66970278856062	-31.231634325443526	186843
87b2c9484da781be528c946b7fd2abfa584d7d19	fuzzy sets and expert systems	fuzzy set;expert system	Abstract   Several ways of using fuzzy set theory and fuzzy logic in expert systems to take into account different forms of uncertainty are discussed. This includes a treatment of a theory of dispositions, Baysian decision theory applied to expert systems, incomplete knowledge representation, and evidence theory. The work is discussed in relation to two programming languages—FRIL and FProlog—which have the capability of reasoning under uncertainty.	expert system;fuzzy set	James F. Baldwin	1985	Inf. Sci.	10.1016/0020-0255(85)90028-3	fuzzy logic;legal expert system;membership function;defuzzification;type-2 fuzzy sets and systems;fuzzy classification;computer science;knowledge management;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;mathematics;fuzzy set;fuzzy associative matrix;expert system;fuzzy set operations;fuzzy control system	AI	1.4099689096224797	-25.51962003739339	187124
9035c8964c9875e8f17854627c628ef3e85a29a9	robustness for evaluating rule?s generalization capability in data mining	extraction information;algorithm performance;analisis estadistico;analisis datos;information extraction;protein sequence;regle production;probabilistic approach;algoritmo genetico;data mining;classification;data analysis;statistical analysis;fouille donnee;resultado algoritmo;enfoque probabilista;approche probabiliste;robustesse;data mining algorithm;analyse statistique;performance algorithme;algorithme genetique;analyse donnee;genetic algorithm;robustness;busca dato;clasificacion;extraccion informacion;production rule;regla produccion;robustez;generalization capability	The evaluation of production rules generated by different data mining algorithms currently depends upon the data set used, thus their generalization capability cannot be estimated. Our method consists of three steps. Firstly, we take a set of rules, copy these rules into a population of rules, and then perturb the parameters of individuals in this population. Secondly, the maximum robustness bounds for the rules is then found using genetic algorithms, where the performance of each individual is measured with respect to the training data. Finally, the relationship between maximum robustness bounds and generalization capability is constructed using statistical analysis for a large number of rules. The significance of this relationship is that it allows the algorithms that mine rules to be compared in terms of robustness bounds, independent of the test data. This technique is applied in a case study to a protein sequence classification problem.	data mining;genetic algorithm;perturbation theory;test data	Dianhui Wang;Tharam S. Dillon;Xiaohang Ma	2003		10.1007/978-3-540-24581-0_60	genetic algorithm;biological classification;computer science;protein sequencing;data mining;mathematics;data analysis;information extraction;algorithm;statistics;robustness	ML	9.32213378355574	-34.63620528622022	187249
3c7b44bf7dac710cfa758f04d10253bf9ba23573	reducing communication cost in a privacy preserving distributed association rule mining	extraction information;association statistique;evaluation performance;association mining;analisis estadistico;performance evaluation;analisis datos;information extraction;model generation;evaluacion prestacion;statistical association;privacy preservation;probabilistic approach;high precision;data mining;association rule mining;statistical properties;vida privada;data analysis;asociacion estadistica;regle association;regla asociacion;statistical analysis;private life;association rule;data privacy;fouille donnee;enfoque probabilista;approche probabiliste;precision elevee;decouverte connaissance;algorithme reparti;analyse statistique;precision elevada;communication cost;vie privee;descubrimiento conocimiento;analyse donnee;algoritmo repartido;distributed algorithm;busca dato;confidentialite donnee;extraccion informacion;knowledge discovery	Data mining is a process that analyzes voluminous digital data in order to discover hidden but useful patterns from digital data. However, discovery of such hidden patterns has statistical meaning and may often disclose some sensitive information. As a result privacy becomes one of the prime concerns in data mining research community. Since distributed association mining discovers global association rules by combining local models from various distributed sites, breaching data privacy happens more often than it does in centralized environments. In this work we present a methodology that generates global association rules without revealing confidential inputs such as statistical properties of individual sites and yet retains high level of accuracy in resultant rules. One of the important outcomes of the proposed technique is that it reduces the overall communication costs. Performance evaluation of our proposed method shows that it reduces the communication cost significantly when we compare with some well-known distributed association rule mining algorithms. Furthermore, the global rule model generated by the proposed method is based on the exact global support of each itemsets, and hence diminished inconsistency, which indeed occurs when global models are generated from partial support count of an itemset.	association rule learning;centralized computing;computation;confidentiality;data mining;digital data;direct memory access;distortion;high-level programming language;information privacy;information sensitivity;parallel algorithm;performance evaluation;resultant;whole earth 'lectronic link	Mafruz Zaman Ashrafi;David Taniar;Kate Smith-Miles	2004		10.1007/978-3-540-24571-1_35	distributed algorithm;association rule learning;computer science;artificial intelligence;data science;data mining;information extraction	ML	-3.506383137316881	-33.65612833491283	187383
5685f603865631661222d4404602c8d5aa1d115c	outlier detection techniques for data mining	outlier detection;data mining		anomaly detection;data mining	Fabrizio Angiulli	2009			anomaly detection;local outlier factor;outlier;data mining;mathematics	ML	-0.2744541648768799	-37.790155852285295	187416
c6a398c47315a794bfedc339fbddde817b849b37	bidirectional comparison of multi-attribute qualitative objects		Abstract In the paper, the multi-attribute objects with repeating qualitative values of attributes are considered. Each object is represented by a collection of multisets drawn from sets of values of the attributes. Formalism of the theory of multisets allows taking into account simultaneously all the combinations of attribute values and various versions of the objects. The effective procedure for comparing such objects as well as groups of such objects is developed. The proposed concept of the perturbation of one object by another is considered as the difference of the multisets representing the objects. The measure of perturbation describes remoteness between the considered objects, and, in general, is asymmetrical. Next, we consider the measure of the perturbation of one group of objects by another group of objects. Then, we generate the description of each group in the form of the classification rules. A practical illustration of the proposed approach is carried out for the task of classification of text documents.		Maciej Krawczak;Grazyna Szkatula	2018	Inf. Sci.	10.1016/j.ins.2017.12.039	artificial intelligence;discrete mathematics;mathematics;machine learning;formalism (philosophy);perturbation (astronomy)	AI	-3.7122369964748465	-24.07582268697985	187447
a391d421136f8a3c5bfd6338736673a64d60f80d	mining significant change patterns in multidimensional spaces	tratamiento datos;extraction information;modelizacion;periodicity awareness;base dato multidimensional;cube gradients;multidimensional spaces;base de donnees multidimensionnelle;analisis datos;information extraction;cube;on line;en linea;analyse tendance;cubo;heuristic method;data cubing;partitioning;trend analysis;data processing;online analytical processing;e olap mining;metodo heuristico;traitement donnee;periodicite;data mining;multidimensional database;modelisation;periodicity;data analysis;gradient search;hierarchical classification;periodicidad;fouille donnee;change patterns;classification hierarchique;ranking cubes;analyse donnee;en ligne;methode heuristique;olap mining;modeling;clasificacion jerarquizada;busca dato;extraccion informacion;change analysis;analisis tendencia;multidimensional data mining	In this paper, we present a new OLAP Mining method for exploring interesting trend patterns. Our main goal is to mine the most (TOP-K) significant changes in Multidimensional Spaces (MDS) applying a gradientbased cubing strategy. The challenge is then finding maximum gradient regions, which maximises the task of detecting TOP-K gradient cells. Several heuristics are also introduced to prune MDS efficiently. In this paper, we motivate the importance of the proposed model, and present an efficient and effective method to compute it by • evaluating significant changes by means of pushing gradient search into the partitioning process • measuring Gradient Regions (GR) spreadness for data cubing • measuring Periodicity Awareness (PA) of a change, assuring that it is a change pattern and not only an isolated event • devising a Rank Gradient-based Cubing to mine significant change patterns in MDS.	algorithmic efficiency;computation;database;display resolution;effective method;futures studies;gradient;heuristic (computer science);online analytical processing;prune and search;quasiperiodicity;sensor;smoothing;spaces;sparse matrix;word lists by frequency	Ronnie Alves;Joel Ribeiro;Orlando Belo	2009	IJBIDM	10.1504/IJBIDM.2009.029073	data processing;online analytical processing;computer science;artificial intelligence;data mining;information extraction;algorithm	ML	-3.1168694211848247	-32.95545122081745	187453
0056f3a6c651a0e0a8827d129847884580a6fe65	evaluation of the serial association rule mining algorithms	association rule mining		algorithm;association rule learning	Ferenc Kovács;Renata Iváncsy;István Vajk	2004			association rule learning;data mining;artificial intelligence;computer science;pattern recognition	ML	-0.7757080446874153	-32.886323782301275	187607
6af4a076a6340dec3412a334ac49ceaf68ec29b0	fuzzy logic and neural network handbook	fuzzy logic;neural network	Will reading habit influence your life? Many say yes. Reading fuzzy logic and neural network handbook is a good habit; you can develop this habit to be such interesting way. Yeah, reading habit will not only make you have any favourite activity. It will be one of guidance of your life. When reading has become a habit, you will not make it as disturbing activities or as boring activity. You can gain many benefits and importances of reading.	artificial neural network;fuzzy logic	Chi Hau Chen	1996			fuzzy electronics;probabilistic neural network;adaptive neuro fuzzy inference system;fuzzy classification;neuro-fuzzy;time delay neural network;fuzzy associative matrix;fuzzy set operations;fuzzy control system;intelligent control	AI	4.166011077355425	-25.47260291010699	188026
80775fb2a81c7cc1425e178c286a1e88e671cf0a	medical reasoning with rough-set influence diagrams	medical decision;approximate reasoning;rough sets;influence diagrams;information system	There are several advantages to evaluating a problem using influence diagram operations. The analyst can use a representation that is natural to the decision maker, since the algorithm executes all of the inference and analysis automatically. The influence diagram solution procedure can also result in significant gains in efficiency. Conditional independence is clearly exhibited in the diagram, so the size of intermediate calculations can be reduced, resulting in considerable reductions in both processing time and memory requirements. However, when imprecise knowledge from data sets is involved in the systems, how to reason from approximate information becomes a main issue in evaluating influence diagrams effectively. This study develops an alternative knowledge model, rough-set influence diagrams (RSID), which combine rough-set decision rules and graphical structures of influence diagrams in medical settings. The proposed RSID provides a comprehensive schema for knowledge representation and decision support.		Chia-Hui Huang	2015	Journal of computational biology : a journal of computational molecular cell biology	10.1089/cmb.2014.0293	rough set;influence diagram;computer science;artificial intelligence;machine learning;data mining;information system;algorithm	ML	-3.1058524052192356	-29.808574359403696	188097
5a35d65ce19c6a038f41373eeac15efc513161b9	concept lattice reduction using fuzzy k-means clustering	fuzzy k means;singular value decomposition;fuzzy k means clustering;concept lattice;formal concept analysis	0957-4174/$ see front matter 2009 Elsevier Ltd. A doi:10.1016/j.eswa.2009.09.026 * Corresponding author. Tel.: +91 416 2202023; fax E-mail addresses: aswanis@gmail.com (Ch. Aswa hotmail.com (S. Srinivas). During the design of concept lattices, complexity plays a major role in computing all the concepts from the huge incidence matrix. Hence for reducing the size of the lattice, methods based on matrix decompositions like SVD are available in the literature. However, SVD computation is known to have large time and memory requirements. In this paper, we propose a new method based on Fuzzy K-Means clustering for reducing the size of the concept lattices. We demonstrate the implementation of proposed method on two application areas: information retrieval and information visualization. 2009 Elsevier Ltd. All rights reserved.	cluster analysis;computation;fax;formal concept analysis;incidence matrix;information retrieval;information visualization;k-means clustering;lattice reduction;outlook.com;requirement;singular value decomposition;the matrix;turing completeness	Ch. Aswanikumar;S. Srinivas	2010	Expert Syst. Appl.	10.1016/j.eswa.2009.09.026	combinatorics;discrete mathematics;fuzzy clustering;computer science;formal concept analysis;theoretical computer science;machine learning;mathematics;lattice miner;singular value decomposition	AI	-3.4974867824802414	-25.897138249637344	188150
efa41d2e371a0e1af6a559c5dbcff8e99c640f24	locating faults in tree-structured networks	tree structure;minimum message length	An important problem in fault diagnosis is how to locate faulty components by analysing performance measurements from those components. In this paper, we present an algorithm that uses the informationtheoretic Minimum Message Length (MML) principle to locate faults in tree-structured networks. Treestructured networks are an important application for fault diagnosis due to their use as distribution networks in telecommunication and power industries. The main advantage of using a MML approach is that we can model both the complexity of an explanation for a set of measurements, as well as the accuracy of the explanation. We demonstrate the effectiveness of this approach through an empirical evaluation on a range of simulated data.	abductive reasoning;algorithm;minimum message length;network topology;snapshot (computer storage);time series	Christopher Leckie;Michael Dale	1997			minimum message length;theoretical computer science;tree structure;computer science;distributed computing	AI	2.342747250720856	-32.17657657010803	188221
c64a92372577aae9b86d94fdc892bb19438f75db	automatic document clustering of concept hypergraph decompositions	document clustering;algorithme rapide;association statistique;cluster algorithm;hipergrafico;web documents;analyse amas;red www;reseau web;statistical association;association rules;data mining;classification;graph connectivity;asociacion estadistica;cluster analysis;regle association;hypergraph partition;internet;regla asociacion;association rule;fouille donnee;clustering method;fast algorithm;decouverte connaissance;conectividad grafo;world wide web;descubrimiento conocimiento;analisis cluster;hypergraph;connected component;connectivite graphe;busca dato;algoritmo rapido;clasificacion;hypergraphe;knowledge discovery	This paper presents an approach to classify/cluster the web documents by decompositions of hypergraphs. The various levels of co-occurring frequent terms, called association rules (undirected rules), of documents form a hypergraph. Clustering methods is then applied to analyze such hypergraphs; a simple and fast clustering algorithm is used to decomposing hypergraph into connected components. Each connected component represents a primitive concept within the given documents. The documents will then be classified/clustered by such primitive concepts.	cluster analysis	Tsau Young Lin;I-Jen Chiang	2004		10.1117/12.543817	machine learning;data mining;mathematics;algorithm	NLP	-3.3734207629933217	-32.793072794648666	188368
74655f5e8be0a9b7b2a94cb239f02946ca9ab39c	evolutionary multi-objective optimization of fuzzy rule-based classifiers in the roc space	medical image processing evolutionary computation fuzzy reasoning fuzzy systems lung;sensitivity and specificity;evolutionary computation;fuzzy reasoning;computed tomography;lung ct image evolutionary multiobjective optimization fuzzy rule based classifier roc space computer aided diagnosis system;computer aided diagnosis;roc space;application software;approximation algorithms;evolutionary multi objective optimization;input variables;pareto front;lungs;evolutionary multiobjective optimization;data mining;pareto optimization;lung;computer aided diagnosis system;fuzzy rule base;medical image processing;lungs sensitivity and specificity fuzzy systems data mining computed tomography costs pareto optimization approximation algorithms input variables application software;lung ct image;convex hull;optimal algorithm;fuzzy rule based classifier;fuzzy systems;fuzzy system	An approach to select the most suitable fuzzy rule-based binary classifier to a specific application is proposed. First, an evolutionary three-objective optimization algorithm is applied to generate an approximation of a Pareto front composed of fuzzy rule-based binary classifiers with different trade-offs between accuracy and complexity. Accuracy is measured in terms of sensitivity and specificity, whereas complexity is computed as sum of the conditions which compose the antecedents of the rules included in the classifiers. Thus, low values of complexity correspond to fuzzy systems characterized by a low number of rules and a low number of input variables actually used in each rule. This ensures a high comprehensibility of the classifiers. Then, the most suitable classifier is selected by using the ROC convex hull method. We discuss the application of the proposed approach to generate a classifier for discriminating lung nodules from non-nodules in a computer aided diagnosis (CAD) system. Results obtained on a real data set extracted from lung CT images are also discussed	algorithm;approximation;binary classification;ct scan;complexity;computer-aided design;convex hull;fuzzy control system;fuzzy rule;logic programming;mathematical optimization;multi-objective optimization;pareto efficiency;sensitivity and specificity	Marco Cococcioni;Pietro Ducange;Beatrice Lazzerini;Francesco Marcelloni	2007	2007 IEEE International Fuzzy Systems Conference	10.1109/FUZZY.2007.4295465	random subspace method;mathematical optimization;application software;computer science;artificial intelligence;fuzzy number;convex hull;multi-objective optimization;machine learning;pattern recognition;mathematics;fuzzy control system	Robotics	4.0505661611128945	-29.803581980157567	188722
111e2fa15601d0e432096bae0e1a07cf608f0c78	keynote speaker 2: real time data mining	machine learning algorithms;open research lines keynote speaker real time data mining algorithms transient data streams machine learning dynamic environments learning decision models change detection learning algorithms computational resources cpu power;learning artificial intelligence data mining;change detection algorithms algorithm design and analysis data mining heuristic algorithms machine learning algorithms conferences adaptive systems;data mining;adaptive systems;heuristic algorithms;algorithm design and analysis;conferences;change detection algorithms	Nowadays, there are applications in which the data are modelled best not as persistent tables, but rather as transient data streams. In this keynote, we discuss the limitations of current machine learning and data mining algorithms. We discuss the fundamental issues in learning in dynamic environments like learning decision models that evolve over time, learning and forgetting, concept drift and change detection. Data streams are characterized by huge amounts of data that introduce new constraints in the design of learning algorithms: limited computational resources in terms of memory, processing time and CPU power. In this talk, we present some illustrative algorithms designed to taking these constrains into account. We identify the main issues and current challenges that emerge in learning from data streams, and present open research lines for further developments.	algorithm;central processing unit;computational resource;concept drift;data mining;machine learning;open research	João Gama	2015	2015 IEEE International Conference on Evolving and Adaptive Intelligent Systems (EAIS)	10.1109/EAIS.2015.7368772	unsupervised learning;instance-based learning;algorithmic learning theory;computer science;data science;online machine learning;machine learning;data mining;data stream mining;stability;computational learning theory;active learning;generalization error	ML	-1.2378975611883003	-35.90759629642423	188847
b54d957a46a93538c48207bf3dddb22f9807f7e9	an efficient attribute reduction in decision information systems	databases;machine learning algorithms;database system;information systems;database management systems;rough set theory;barium;positive region rough set decision information system attribute reduction;set theory;data mining;decision information system;attribute reduction;computer experiment;positive region;knowledge discovery and data mining;positive region calculation algorithm;rough set theory data mining data reduction database management systems;approximation methods;data reduction;information system;rough set;algorithm design and analysis;information systems data mining data analysis computer science software engineering information technology educational institutions entropy set theory databases;knowledge discovery;database system attribute reduction decision information system knowledge discovery data mining rough set theory positive region calculation algorithm	An important issue of knowledge discovery and data mining is the attribute reduction. In this paper, a novel method based on positive region in rough set is proposed for attribute reduction in decision information systems. Compared with the classical positive region calculating algorithm, the new algorithm deletes the compared objects timely and cuts down the combinations of object pairs for next computing. Experiments show that the new algorithm is more efficient on attribute reduction in decision information system.	algorithm;data mining;information system;rough set	Feixiang Zhu;Yingjun Zhang;Zhao Li	2008	2008 International Conference on Computer Science and Software Engineering	10.1109/CSSE.2008.1090	rough set;attribute domain;computer science;machine learning;pattern recognition;data mining;database;knowledge extraction;information retrieval;information system	DB	-4.460525061579391	-36.66122627190924	189043
693a9b92eeffd18f5cefbf954739da32eab23149	energy information analysis using data algorithms based on big data platform	machine learning algorithms;support vector machines;prediction algorithms;data analysis;big data;algorithm design and analysis;conferences	With the development of the IoT market, collectable data is increasing exponentially. Recently, various methods for big data analysis are being suggested. Existing general research on data analysis has some problem that if the size of data is getting bigger, the processing speed is rapidly slow. In this paper, we find out the optimal algorithm that efficiently manage the energy data based on Big data by comparing data which is analyzed using three algorithms.	algorithm;big data	Dongjun Kang;Seunghwan Kim;Tacklim Lee;Junyeon Hwang;Sanghoon Lee;Seongman Jang;Sehyun Park	2016	2016 IEEE 18th International Conference on High Performance Computing and Communications; IEEE 14th International Conference on Smart City; IEEE 2nd International Conference on Data Science and Systems (HPCC/SmartCity/DSS)	10.1109/HPCC-SmartCity-DSS.2016.0217	support vector machine;algorithm design;big data;prediction;computer science;data science;machine learning;data warehouse;data mining;data analysis	DB	-2.7306215920432493	-36.29847995875471	189093
d6892b44eb3c480041d06515221750bafbba6047	fuzzy logic synthesis with genetic algorithms	genetic algorithm;fuzzy logic		fuzzy logic;genetic algorithm;linear temporal logic to büchi automaton;logic synthesis	Philip R. Thrift	1991			fuzzy logic;fuzzy set operations;adaptive neuro fuzzy inference system;neuro-fuzzy;genetic fuzzy systems;fuzzy electronics;fuzzy associative matrix;artificial intelligence;computer science;fuzzy classification	EDA	3.348027759045694	-24.99747166887508	189132
5e92bb852d0fb8ee32d3cbde737835d5397690eb	neighborhood systems-based rough sets in incomplete information system	incomplete information system;neighborhood system;informing science;operations research;incomplete information;knowledge acquisition;maximal consistent block;rough set;descriptor;is research;knowledge engineering	Neighborhood system formalized the ancient intuition, infinitesimals, which led to the invention of calculus, topology and non-standard analysis. In this paper, the neighborhood system is researched from the view point of knowledge engineering and then each neighborhood is considered as a basic unit with knowledge. By using these knowledge in neighborhood system, the rough approximations and the corresponding properties are discussed. It is shown that in the incomplete information system, the smaller upper approximations can be obtained by neighborhood system based rough sets than by the methods in [Y. Leung, D.Y. Li, Maximal consistent block technique for rule acquisition in incomplete information systems, Information Sciences 115 (2003) 85-106] and [Y. Leung, W.Z. Wu, W.X. Zhang, Knowledge acquisition in incomplete information systems: a rough set approach, European Journal of Operational Research 168 (2006) 164-180]. Furthermore, a new knowledge operation is discussed in the neighborhood system, from which more knowledge can be derived from the initial neighborhood system. By such operations, the regions of lower and upper approximations are further expanded and narrowed, respectively. Some numerical examples are employed to substantiate the conceptual arguments.	information system;rough set	Xibei Yang;Ming Zhang;Huili Dou;Jingyu Yang	2011	Knowl.-Based Syst.	10.1016/j.knosys.2011.03.007	rough set;computer science;artificial intelligence;machine learning;knowledge engineering;data mining;complete information;algorithm	DB	-2.551787864158487	-24.844965950560905	189133
fd10db8bbd3a8e55d3d446a259d6e2c40a6ee980	a comparative study in data mining: clustering and classification capabilities		The ICT evolution has driven on the creation of a capable society, in providing new kinds and type of information. The gathered information is stored continuously, meaning that a great amount of databases has to be created. The problem that arises is whether there is a global manner of managing and gaining knowledge out of the rising variety and volumes of data. Many efforts have been developed for addressing the emerging challenges of data mining based on statistics and machine learning techniques that can significantly boost the ability to analyze data. In this paper, a detailed study on the data mining field takes place, followed by a comparative study between clustering and classification techniques, resulting that the integration of clustering and classification techniques can provide more accurate results than a simple classification technique that classifies datasets with priorly known attributes and classes.	cluster analysis;data mining	Argyro Mavrogiorgou;Athanasios Kiourtis;Dimosthenis Kyriazis;Marinos Themistocleous	2017		10.1007/978-3-319-65930-5_7	cluster analysis;data mining;computer science;information and communications technology	ML	-0.946590994289409	-34.10982816964191	189487
8da420f336dd1a7f4644d2b0981237e56ce07926	fuzzy rule-based system identification using novel concepts of certainty factors and utility factors of rules			expert system;fuzzy rule;rule-based system;system identification	Tandra Pal	2002			neuro-fuzzy;pattern recognition;machine learning;computer science;artificial intelligence;adaptive neuro fuzzy inference system;fuzzy set operations;fuzzy associative matrix;defuzzification;fuzzy control system;fuzzy rule;fuzzy classification	ML	3.516467840649632	-25.880019215402434	189508
9a8e3883078e560cc608674388a58a843e161b6f	a cascaded genetic algorithm for improving fuzzy-system design	fuzzy controller;fuzzy set;design optimization;automatic generation;complex system;genetic algorithm;high performance;fuzzy system	Abstract   We present a cascaded genetic algorithm which automatically generates high-performance fuzzy systems with a minimal number of fuzzy sets and rules. Such a tool is especially useful for complex systems which can no longer be designed and optimized manually. The cascade technique is tested on a fuzzy controller design task. Experimental results show that the proposed algorithm yields considerably better results than a conventional genetic algorithm.	genetic algorithm;systems design	Henning Heider;Thorsten Drabe	1997	Int. J. Approx. Reasoning	10.1016/S0888-613X(97)00003-0	complex systems;fuzzy electronics;multidisciplinary design optimization;genetic algorithm;defuzzification;adaptive neuro fuzzy inference system;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;control theory;fuzzy set;fuzzy associative matrix;fuzzy set operations;fuzzy control system;population-based incremental learning	AI	5.576894312329287	-26.975748776224112	189513
38334366ad5c10e3f8e366969e48d73443d0b4da	type-2 fuzzy logic as a method of response integration in modular neural networks	pattern recognition;fuzzy system;genetic algorithm;neural network;speaker recognition;fuzzy logic	We describe in this paper the use of neural networks and type-2 fuzzy logic for pattern recognition. In particular, we consider the case of speaker recognition by analyzing the sound signals with the help of intelligent techniques, such as the neural networks and fuzzy systems. We use the neural networks for analyzing the sound signal of an unknown speaker, and after this first step, a set of type-2 fuzzy rules is used for decision making. We need to use fuzzy logic due to the uncertainty of the decision process. We also use genetic algorithms to optimize the architecture of the neural networks. We illustrate our approach with a sample of sound signals from real speakers in our institution.	artificial neural network;convolutional neural network;fuzzy control system;fuzzy logic;fuzzy set;genetic algorithm;modular neural network;neural networks;pattern recognition;speaker recognition	Jérica Urías;Daniel Solano;Miguel Soto;Miguel Lopez;Patricia Melin	2006			fuzzy set operations;neuro-fuzzy;fuzzy logic;adaptive neuro fuzzy inference system;fuzzy electronics;neural modeling fields;fuzzy associative matrix;intelligent control;machine learning;artificial intelligence;computer science	AI	4.5401802382993415	-26.226432754932826	189676
48cb9456ed1be8bfc1c8d2d501da53e37ed59a28	a fuzzy reasoning design for fault detection and diagnosis of a computer-controlled system	biological patents;tmu;biomedical journals;fuzzy reasoning;decision tree;fault tolerant;text mining;europe pubmed central;frvpns;pns;fuzzy rules;citation search;wrfswinning rule firing strength;promf;citation networks;dldm;mf;research articles;abstracts;open access;eddm;ehl;rt;life sciences;error diagnosis;clinical guidelines;frdt;error detection;hierarchical design;petri nets;full text;cpu;ht;rfs;md;petri net;fuzzy rule;tcpuu;rest apis;fault detection and diagnosis;orcids;europe pmc;biomedical research;bioinformatics;literature search;pcpuu	A Fuzzy Reasoning and Verification Petri Nets (FRVPNs) model is established for an error detection and diagnosis mechanism (EDDM) applied to a complex fault-tolerant PC-controlled system. The inference accuracy can be improved through the hierarchical design of a two-level fuzzy rule decision tree (FRDT) and a Petri nets (PNs) technique to transform the fuzzy rule into the FRVPNs model. Several simulation examples of the assumed failure events were carried out by using the FRVPNs and the Mamdani fuzzy method with MATLAB tools. The reasoning performance of the developed FRVPNs was verified by comparing the inference outcome to that of the Mamdani method. Both methods result in the same conclusions. Thus, the present study demonstratrates that the proposed FRVPNs model is able to achieve the purpose of reasoning, and furthermore, determining of the failure event of the monitored application program.		Yung Ting;Wen-Bin Lu;Chih-Ho Chen;Gi-Kai Wang	2008	Engineering applications of artificial intelligence	10.1016/j.engappai.2007.04.007	text mining;defuzzification;adaptive neuro fuzzy inference system;fuzzy classification;computer science;artificial intelligence;fuzzy number;neuro-fuzzy;machine learning;data mining;petri net;fuzzy set operations	AI	7.614692803706535	-27.217124237246697	189701
06a3d1a87a443de8d4a6571aaf54d277be78c2e9	using weighted hybrid discretization method to analyze climate changes		Data mining is the process of posing queries to large quantities of data and extracting information, often previously unknown, using mathematical, statistical and machine learning techniques. However some of the data mining techniques like classification and clustering cannot deal with numeric attributes though most real dataset contains some numeric attributes. Continuous attributes should be divided into a small distinct range of nominal attributes in order to apply data mining techniques. Correct discretization makes the dataset succinct and contributes to the high performance of classification algorithms. Meanwhile, several methods are presented and applied, but it is often dependent on the area. In this paper, we propose a weighted hybrid discretization technique based on entropy and contingency coefficient. Also we analyze performance evaluation with well-known techniques of discretization such as Equal-width binning, 1R, MDLP and ChiMerge.	algorithm;cluster analysis;coefficient;contingency table;data mining;discretization;machine learning;performance evaluation;product binning	Yong-Gyu Jung;Kyoung Min Kim;Young Man Kwon	2012		10.1007/978-3-642-35600-1_28	contingency table;cluster analysis;artificial intelligence;discretization;statistical classification;computer science;pattern recognition	ML	-0.301780568704968	-32.99206952250478	189804
ba9f559daa7ec57bdfdec8ea0a17e681c3c13a05	decision making in medical diagnosis via distance measures on interval valued fuzzy sets			fuzzy set	Palash Dutta	2017	IJSDA	10.4018/IJSDA.2017100104	machine learning;medical diagnosis;distance measures;fuzzy set;mathematical optimization;artificial intelligence;mathematics	Vision	2.019829254567908	-25.621609439414605	189835
36c7c4e1b79c8370aa5275f3f7f3dd050d69cd9e	nondeterministic decision rules in rule-based classifier		In the paper is discussed the truncated nondeterministic rules and their role in an evaluation of classification model. The nondeterministic rules are created as the result of shorting deterministic rules in accordance with the principle of minimum description length (MDL). As deterministic rules in database we treat the full objects description in a meaning of descriptors conjunction. The nondeterministic rules are calculated in polynomial time by using greedy strategy. The classification model is composed in two steps process. In the first step deterministic and nondeterministic rules are constructed. Next these rules are used for classifier evaluation. The evaluation results are compared with classifiers only based on deterministic rules creating by different algorithms. The experiments shows that such nondeterministic rules could be treat as an extra knowledge about data. This knowledge is able to improve the classification quality. It should be pointed out that classification process requires tuning some of their parameters relative to analyzed data.		Piotr Paszek;Barbara Marszal-Paszek	2014		10.1007/978-3-319-06932-6_18	machine learning;data mining;mathematics;nondeterministic algorithm;algorithm	AI	4.146861034331625	-31.97554023676626	190343
2831f4c732ae55b87685a6acb7ebd1188f82b4c2	a detailed analysis of the kdd cup 99 data set	support vector machines;testing intrusion detection data security statistical analysis computer security computer aided manufacturing learning systems computational intelligence computer networks application software;application software;computational intelligence;anomaly detection;training;kdd cup 99 data set analysis;signature based intrusion detection system;intrusion detection;testing;data mining;computer networks;attack detection;computer security;learning systems;accuracy;statistical analysis;machine learning;feature extraction;computer aided manufacturing;statistical analysis kdd cup 99 data set analysis anomaly detection signature based intrusion detection system attack detection;security;security of data;statistical analysis security of data;data security	During the last decade, anomaly detection has attracted the attention of many researchers to overcome the weakness of signature-based IDSs in detecting novel attacks, and KDDCUP'99 is the mostly widely used data set for the evaluation of these systems. Having conducted a statistical analysis on this data set, we found two important issues which highly affects the performance of evaluated systems, and results in a very poor evaluation of anomaly detection approaches. To solve these issues, we have proposed a new data set, NSL-KDD, which consists of selected records of the complete KDD data set and does not suffer from any of mentioned shortcomings.	anomaly detection;cups;data mining;sigkdd;sensor	Mahbod Tavallaee;Ebrahim Bagheri;Wei Lu;Ali A. Ghorbani	2009	2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications	10.1109/CISDA.2009.5356528	intrusion detection system;support vector machine;anomaly detection;application software;feature extraction;computer science;information security;machine learning;computational intelligence;data mining;accuracy and precision;software testing;data security;computer security	Security	6.392897865443158	-37.602678246358764	190384
889325b3e66433dd219c527f44d6a59be35b44ed	knowledge granulation, knowledge entropy and knowledge uncertainty measure in ordered information systems	knowledge granulation;information system;knowledge uncertainty measure;rough set;ordered information systems;knowledge entropy	In this paper, concepts of knowledge granulation, knowledge entropy and knowledge uncertainty measure are given in ordered information systems, and some important properties of them are investigated. From these properties, it can be shown that these measures provides important approaches to measuring the discernibility ability of different knowledge in ordered information systems. And relationship between knowledge granulation, knowledge entropy and knowledge uncertainty measure are considered. As an application of knowledge granulation, we introduce definition of rough entropy of rough sets in ordered information systems. By an example, it is shown that the rough entropy of rough sets is more accurate than classical rough degree to measure the roughness of rough sets in ordered information systems. 2009 Elsevier B.V. All rights reserved. * Corresponding author. E-mail addresses: datongxuweihua@126.com (X. Wei-hua), zxy19790915@163.com (Z. Xiao-yan), wxzhang@mail.xjtu.edu.cn (Z. Wen-xiu).	computation;information system;rough set;set theory;vagueness	Wei-Hua Xu;Xiao-yan Zhang;Wen-Xiu Zhang	2009	Appl. Soft Comput.	10.1016/j.asoc.2009.03.007	rough set;computer science;knowledge management;artificial intelligence;machine learning;data mining;mathematics;information system	AI	-2.558614206086662	-24.927237189688377	190720
e2cb5802cb9c130dd08593a585f698ccd7d20553	a new cognitive model: cloud model	gauss membership function;cloud model;important kind;new cognitive model;important uncertainty;human cognition;normal distribution;normal cloud model;linguistic concept;generalized normal distribution;cognitive model;systems;intelligent	Randomness and fuzziness are the two most important uncertainties inherent in human cognition, which have attracted great attention in artificial intelligence research. In this paper, regarding linguistic terms or concepts as the basic units of human cognition, we propose a new cognitive model—cloud model, which can synthetically describe the randomness and fuzziness of concepts and implement the uncertain transformation between a qualitative concept and its quantitative instantiations. Furthermore, by analyzing in detail the statistical properties of normal cloud model, that is, an important kind of cloud models based on normal distribution and Gauss membership function, we show that normal cloud model can not only be viewed as a generalized normal distribution with weak constraints but also avoid the flaw of fuzzy sets to quantify the membership degree of an element as an accurate value between 0 and 1 and, therefore, may be more adaptive for the uncertainty description of linguistic concepts. Finally, two demonstration examples about the fractal evolution of plants and network topologies based on cloud models are given to illustrate the promising applications of cloud models in some more complex knowledge representation tasks. © 2009 Wiley Periodicals, Inc.	cognitive model	Deyi Li;Changyu Liu;Wenyan Gan	2009	Int. J. Intell. Syst.	10.1002/int.20340	cognitive model;simulation;computer science;artificial intelligence;machine learning;mathematics;statistics	Web+IR	-3.2839470614503323	-24.095327091649125	191127
f0acfea12f4bb9b14b17da056e519b0bda46b89c	inducing decision trees from medical decision processes	inducing decision tree;medical decision problem;medical decision process;induced tree;medical correctness;final decision;decision process;decision tree;medical sense;right final decision;medical decision tree;classical decision tree structure	inducing decision tree;medical decision problem;medical decision process;induced tree;medical correctness;final decision;decision process;decision tree;medical sense;right final decision;medical decision tree;classical decision tree structure	decision tree	Pere Torres;David Riaño;Joan Albert López-Vallverdú	2010		10.1007/978-3-642-18050-7_4	decision model;optimal decision;influence diagram;decision tree learning;decision field theory;computer science;artificial intelligence;machine learning;decision tree;incremental decision tree;data mining;decision rule;evidential decision theory;id3 algorithm;weighted sum model;business decision mapping;grafting;decision stump	ML	-0.6952386875100656	-28.418585747208073	191197
9b3bc709889e64fc0495f05a8ad0f295729c3852	a goal-dependent abstraction for legal reasoning by analogy	legislation;order sorted logid;legal reasoning;goal dependent abstraction;taxonomic hierarchy;similarity;analogy	This paper presents a new algorithm to find an appropriate similarityunder which we apply legal rules analogically. Since there may exist a lotof similarities between the premises of rule and a case in inquiry, we haveto select an appropriate similarity that is relevant to both thelegal rule and a top goal of our legal reasoning. For this purpose, a newcriterion to distinguish the appropriate similarities from the others isproposed and tested. The criterion is based on Goal-DependentAbstraction (GDA) to select a similarity such that an abstraction basedon the similarity never loses the necessary information to prove the ground (purpose of legislation) of the legal rule. In order to cope withour huge space of similarities, our GDA algorithm uses some constraintsto prune useless similarities.	algorithm;gnome-db	Tokuyasu Kakuta;Makoto Haraguchi;Yoshiaki Okubo	1997	Artificial Intelligence and Law	10.1023/A:1008272013974	taxonomic rank;similarity;analogy;computer science;artificial intelligence;data mining;algorithm	AI	-3.7054355602776035	-28.551228580550774	191321
f4459709954735017724a769a31987ade3cb2826	adaptive fuzzy pattern recognition in the anaerobic digestion process	systeme commande;sistema control;fuzzy c mean;classification algorithm;fuzzy set;supervisory control;aplicacion;epuration eau usee;algoritmo adaptativo;conjunto difuso;ensemble flou;classification;fuzzy sets;adaptive algorithm;control proceso;control system;algorithme adaptatif;adaptation;depuracion aguas servidas;process control;normal operator;pattern recognition;anaerobic digestion;reconnaissance forme;waste water purification;reconocimiento patron;application;supervision;clasificacion;commande processus;wastewater treatment;biological wastewater treatment	This paper addresses the problem of coordinating the operation of several units of a biological wastewater treatment process in order to avoid process failure. The state of the process is monitored through a number of chemical and biological variables and this information is used to infer the plant loading state. After a brief introduction of the plant supervision scheme, the paper describes how the Fuzzy C-Means (FCM) classification algorithm can be used in this application and enhanced with an adaptive capability, enabling the supervisory control system to detect process departure from normal operating conditions.	pattern recognition	Stefano Marsili-Libelli;Andreas Müller	1996	Pattern Recognition Letters	10.1016/0167-8655(96)00030-X	computer science;control system;artificial intelligence;process control;control theory;fuzzy set	Vision	8.315523439890514	-27.807723163599043	191519
ab41fedf1793d65c3aca92d9499edc55c1fbf324	classification model based on topological approximation space		In this paper we present an application of a topological approximation space and a rough fuzzy membership function in aim to get classification models. We propose a model of obtaining coverings based on statistical methods applied to attributes in decision systems (where missing values are also considered). We include in this paper experimental results on classification of Horse Colic, Diabetes and Austra data sets, and compare the results with classifiers built in RSES2.	approximation	Bozena Staruch	2017		10.1007/978-3-319-60840-2_41	function space;fuzzy logic;topological vector space;zero-dimensional space;topological space;topology;mathematics;horse colic;membership function;locally finite collection	Theory	1.7206391745241194	-25.92172122229123	191609
76f7319128836a9e5dbd58a1a72426daa2f18ace	a fast approach to attribute reduction in incomplete decision systems with tolerance relation-based rough sets	time complexity;rough set theory;tolerance relation;attribute reduction;incomplete information;positive region;incomplete decision system;numerical experiment;rough set	Efficient attribute reduction in large, incomplete decision systems is a challenging problem; existing approaches have time complexities no less than O(|C|^2|U|^2). This paper derives some important properties of incomplete information systems, then constructs a positive region-based algorithm to solve the attribute reduction problem with a time complexity no more than O(|C|^2|U|log|U|). Furthermore, our approach does not change the size of the original incomplete system. Numerical experiments show that the proposed approach is indeed efficient, and therefore of practical value to many real-world problems. The proposed algorithm can be applied to both consistent and inconsistent incomplete decision systems.	rough set	Zuqiang Meng;Zhongzhi Shi	2009	Inf. Sci.	10.1016/j.ins.2009.04.002	discrete mathematics;rough set;computer science;machine learning;data mining;mathematics;algorithm	DB	-2.7651709535899904	-28.49311084932955	191889
4df5c2293a8c8395265908f12413381410787d94	multiple attributes decision fusion for wireless sensor networks based on intuitionistic fuzzy set		Decision fusion is an important issue in wireless sensor networks (WSN), and intuitionistic fuzzy set (IFS) is a novel method for dealing with uncertain data. We propose a multi-attribute decision fusion model based on IFS, which includes two aspects: data distribution-based IFS construction algorithm (DDBIFCA) and the category similarity weight-based TOPSIS intuitionistic fuzzy decision algorithm (CSWBT-IFS). The DDBIFCA is an IFS construction algorithm that transforms the original attribute values into intuitionistic fuzzy measures, and theCSWBT-IFS is an intuitionistic fuzzy aggregation algorithm improved by the traditional TOPSIS algorithm, which combines intuitionistic fuzzy values of different attributes and obtains a final decision for the monitoring target. Both algorithms have benefits, such as low energy consumption and low computational complexity, which make them suitable for implementation in energy-constrained WSNs. Simulation results show the efficiency of intuitionistic fuzzification for the DDBIFCA and a high classification accuracy, compared with traditional fuzzy fusion and other intuitionistic fuzzy aggregation algorithms, for the CSWBT-IFS.	algorithm;computational complexity theory;fuzzy measure theory;fuzzy set;intuitionistic logic;simulation;uncertain data	Zhenjiang Zhang;Ziqi Hao;Sherali Zeadally;Jing Zhang;Bowen Han;Han-Chieh Chao	2017	IEEE Access	10.1109/ACCESS.2017.2722483	fuzzy set operations;fuzzy logic;uncertain data;computer science;algorithm design;fuzzy set;topsis;machine learning;fuzzy number;artificial intelligence;fuzzy classification	Embedded	-0.14148598744868757	-27.02434900802046	192179
1d60c7122028100191bf483e2a6d923175d7adcd	scalable, distributed and dynamic mining of association rules	algoritmo paralelo;parallel algorithm;procesamiento informacion;base donnee tres grande;integration information;information organization;association rule mining;algorithme parallele;organizacion informacion;information integration;regle association;compact representation;association rule;tree structure;information processing;integracion informacion;organisation information;information system;very large databases;traitement information;systeme information;distributed architecture;sistema informacion	We propose a novel pattern tree called Pattern Count tree (PC-tree) which is a complete and compact representation of the database. We show that construction of this tree and then generation of all large itemsets requires a single database scan where as the current algorithms need at least two database scans. The completeness property of the PCtree with respect to the database makes it amenable for mining association rules in the context of changing data and knowledge, which we call dynamic mining. Algorithms based on PC-tree are scalable because PC-tree is compact. We propose a partitioned distributed architecture and an efficient distributed association rule mining algorithm based on the PC-tree structure.		V. S. Ananthanarayana;D. K. Subramanian;M. Narasimha Murty	2000		10.1007/3-540-44467-X_51	parallel computing;association rule learning;information processing;computer science;artificial intelligence;machine learning;data mining;database	DB	-3.302131849384506	-32.859569024984076	192197
