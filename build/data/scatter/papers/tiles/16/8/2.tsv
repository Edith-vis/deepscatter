id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
25afb620cc20712a20a2e480f9d57032c2d834f3	bayesian network-based model for the diagnosis of deterioration of semantic content compatible with alzheimer's disease	bayesian network;naive bayes;cognitive deterioration;alzheimer s disease;corpus of oral definitions;influence diagram	Alzheimer's Disease (AD) has become a serious public health problem that affects both the patient and his family and social environment, not to mention the high economic cost for families and public administrations. The early detection of AD has become one of the principal focuses of research, and its diagnosis is fundamental when the disease is incipient or even prodromic, because it is at these stages when treatments are more effective. There are numerous research studies to characterise the disease in these stages, and we have used the specific research carried out by Drs. Herminia Peraita and Lina Grasso. The application of Artificial Intelligence techniques, such as Bayesian Networks and Influence Diagrams, may provide a very valuable contribution both to the very research and the application of results. This article justifies using Bayesian Networks and Influence Diagrams to solve this type of problems and because of their great contribution to this application field. The modelling techniques used for constructing the Bayesian Network are mentioned in this article, and a mechanism for automatic learning of the model parameters is established.	bayesian network	José María Guerrero Triviño;Rafael Martínez-Tomás;Herminia Peraita Adrados	2011		10.1007/978-3-642-21344-1_44	naive bayes classifier;influence diagram;computer science;artificial intelligence;data science;machine learning;bayesian network;data mining	NLP	5.4424501070795905	-76.96001012284194	49452
e36158ad39bc94b07793845feb0aa3d9c96ded7b	the sarc eyesweb catalog: a pattern recognition toolbox for musician-computer interaction	pattern recognition;gesture recognition	This paper presents the SARC EyesWeb Catalog (SEC), a group of blocks designed for real-time gesture recognition that have been developed for the open source program EyesWeb. We describe how the recognition of real-time body movements can be used for musician-computer-interaction.	gesture recognition;open-source software;pattern recognition;real-time locating system;real-time web	Nicholas Edward Gillian;R. Benjamin Knapp;M. Sile O'Modhrain	2009			computer vision;speech recognition;computer science;gesture recognition	Vision	-3.248068961403652	-74.24666767503844	49647
a1b6cd15fba55756b109d82bef7eeca46b1a358b	treatment interruptions to decrease risk of resistance emerging during therapy switching in hiv treatment	treatment failure;drugs;multi drug resistant;aids treatment interruptions therapy switching hiv treatment multi drug resistant strains treatment failure treatment progression;medical treatment human immunodeficiency virus immune system drugs switches capacitive sensors genetic mutations evolution biology usa councils electric resistance;patient treatment;patient treatment drugs	The development of multi-drug resistant strains of HIV remains the primary reason for treatment failure and progression to AIDS in the United States. The failure of a particular multi-drug regimen necessitates a switch to a new multi-drug regimen. We use a simple model of the interaction of resistant strains to show that the transition to the new regimen involves a significant risk of strains resistant to the new regimen emerging, and that treatment interruptions using the failing regimen can be used to decrease this risk.	color gradient;failure	Ryan Zurakowski;Dominik Wodarz	2007	2007 46th IEEE Conference on Decision and Control	10.1109/CDC.2007.4434887	biological engineering	Vision	8.274596265760758	-68.95294091092839	49954
cdbca498639bf5b1db7b34576467137f0984dd12	leveraging hypoxia-activated prodrugs to prevent drug resistance in solid tumors	oxygen;cancer treatment;drug therapy;drug administration;toxicity;blood plasma;cell death;blood vessels	Experimental studies have shown that one key factor in driving the emergence of drug resistance in solid tumors is tumor hypoxia, which leads to the formation of localized environmental niches where drug-resistant cell populations can evolve and survive. Hypoxia-activated prodrugs (HAPs) are compounds designed to penetrate to hypoxic regions of a tumor and release cytotoxic or cytostatic agents; several of these HAPs are currently in clinical trial. However, preliminary results have not shown a survival benefit in several of these trials. We hypothesize that the efficacy of treatments involving these prodrugs depends heavily on identifying the correct treatment schedule, and that mathematical modeling can be used to help design potential therapeutic strategies combining HAPs with standard therapies to achieve long-term tumor control or eradication. We develop this framework in the specific context of EGFR-driven non-small cell lung cancer, which is commonly treated with the tyrosine kinase inhibitor erlotinib. We develop a stochastic mathematical model, parametrized using clinical and experimental data, to explore a spectrum of treatment regimens combining a HAP, evofosfamide, with erlotinib. We design combination toxicity constraint models and optimize treatment strategies over the space of tolerated schedules to identify specific combination schedules that lead to optimal tumor control. We find that (i) combining these therapies delays resistance longer than any monotherapy schedule with either evofosfamide or erlotinib alone, (ii) sequentially alternating single doses of each drug leads to minimal tumor burden and maximal reduction in probability of developing resistance, and (iii) strategies minimizing the length of time after an evofosfamide dose and before erlotinib confer further benefits in reduction of tumor burden. These results provide insights into how hypoxia-activated prodrugs may be used to enhance therapeutic effectiveness in the clinic.	adverse reaction to drug;cytostatic agents;emergence;mathematical model;mathematics;maximal set;neoplasms;non-small cell lung carcinoma;population;prodrugs;protein tyrosine kinase;protein-tyrosine kinase inhibitor;schedule (computer science);schedule (document type);small cell carcinoma of lung;therapeutic procedure;tumor burden;tumor hypoxia;benefit;erlotinib;evofosfamide;solid tumor	Danika Lindsay;Colleen M. Garvey;Shannon M. Mumenthaler;Jasmine Foo	2016		10.1371/journal.pcbi.1005077	pharmacotherapy;pharmacology;programmed cell death;biology;toxicology;toxicity;oxygen	Robotics	8.2981430736166	-68.58044486983654	50015
663daa163a0f64e078cb46e5804d94dba77cf339	interfacing with nanomachines through molecular communication	molecular communication nanobioscience biological systems biological information theory nanoscale devices cells biology biology computing electrons electromagnetic scattering proteins;communication system;molecular communication;synthetic biology;molecular biophysics;nanobiotechnology molecular biophysics;biological systems;biological nanomachine interfacing;biological nanomachine interfacing molecular communication;nanobiotechnology	Molecular communication is a new paradigm for communication between biological nanomachines over a short-range (a nano- and micro-scale range). Biological nanomachines are nano- and micro-scale devices that either exist in the biological world or are artificially created from biological materials and that perform simple functions such as sensing, logic, and actuation. Molecular communication provides a mechanism for biological nanomachines to communicate information by propagating molecules that represent the information. Molecular communication is based on observations of existing biological systems which use molecules as communication carriers. With the advancement of current research in areas such as synthetic biology and bio-nanotechnologies, it may become relatively easy in the near future to develop systems of biological nanomachines communicating through molecules. In this paper, we present a framework for describing molecular communication systems.	biological system;british informatics olympiad;nanorobotics;programming paradigm;synthetic biology	Michael J. Moore;Akihiro Enomoto;Tadashi Nakano;Yutaka Okaie;Tatsuya Suda	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414237	computer science;nanonetwork;nanobiotechnology;synthetic biology;communications system;molecular biophysics	HPC	3.7202857040679818	-68.92179098411701	50043
9c286e9f3a4d427e6dfdf6f497965c2b8f565732	modeling the health care costs of geriatric inpatients	geriatrics;continuous time;bayesian network;patients clinical variables;stochastic process;policy change;bayesian networks bns;costing;bayes methods;medical services costs geriatrics hospitals resource management bayesian methods joining processes costing character generation process planning;survival analysis bayesian networks bns geriatrics hospital costs medical services planning stochastic processes;probabilistic representation;length of stay;survival time;hospital cost;stochastic processes;survival analysis health care geriatric inpatients bayesian network theory graphical representation probabilistic representation patients clinical variables costing hospital costs medical services planning stochastic processes;medical information systems;bayesian network theory;graphical representation;stochastic processes bayes methods costing geriatrics health care medical information systems;survival analysis;health care cost;geriatric inpatients;medical services planning;computer simulation health care costs health services for the aged humans inpatients length of stay models economic models statistical survival analysis united states;hospital costs;health care	This paper extends a method for modeling the survival of patients in hospitals to allow the expected cost to be estimated for the patients' accumulated duration of time in care. An extension of Bayesian network (BN) theory has previously been developed to model patients' survival time in hospitals with respect to the graphical and probabilistic representation of the interrelationships between the patients' clinical variables. Unlike previous BN techniques, this extended model can accommodate continuous times that are skewed in nature. This paper presents the theory behind such an approach and extends it by attaching a cost variable to the survival times, enabling the costing and efficient management of groups of patients in hospitals. An application of the model is illustrated by considering a group of 4260 patients admitted into the geriatric department of a U.K. hospital between 1994-1997. Results are derived for the distribution for their length of stay in the hospital and associated costs. The model's practical use is highlighted by illustrating how hospital managers could benefit using such a method for investigating the influence of future decisions and policy changes on the hospital's expenditure	bayesian network;graphical user interface;health care;hospital admission;patients;inpatient	Barry Shaw;Adele H. Marshall	2006	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2005.863821	stochastic process;actuarial science;medicine;nursing;bayesian network;mathematics;survival analysis;medical emergency;activity-based costing;health care;statistics	Visualization	5.908194184072521	-73.54511843757308	50385
aa82aa827b739f2060a02d4faed3b0ed6101cdeb	fuzzy rule based expert system for diagnosis of lung cancer	expert systems;cancer;uncertainty;cancer lungs expert systems diseases fuzzy logic uncertainty;lungs;user system communication lung cancer diagnosis fuzzy rule based medical expert system working memory knowledge base risk factors lung cancer symptoms domain expert knowledge rule sets type 2 fuzzy inference engine probability disease graphical user interface;fuzzy logic;probability cancer fuzzy reasoning knowledge based systems lung medical expert systems patient diagnosis;diseases;type 2 fuzzy logic fuzzy rule based medical expert system lung cancer diagnosis	Lung cancer is the second most common cancer in both men and women in the world. The focus of this paper is to design a fuzzy rule based medical expert system for diagnosis of lung cancer. The proposed system consists of four modules: working memory, knowledge base, inference engine and user interface. The system takes the risk factors and symptoms of lung cancer in a two-step process and stores them as facts of the problem in working memory. Also domain expert knowledge is gathered to generate rules and stored in the rule base. The rule base consists of two different rule sets related to risk factors and symptoms of lung cancer respectively. Finally, type-2 fuzzy inference engine fires relevant rules under appropriate condition and provides the probability of disease as output of the system. The output of the system could act as a second opinion to assist the physicians. Also graphical user interface is presented to facilitate the communication between user and system.	expert system;fuzzy rule;graphical user interface;inference engine;knowledge base;risk factor (computing);rule-based system;subject-matter expert	Farzad Vasheghani Farahani;Mohammad Hossein Fazel Zarandi;Abbas Ahmadi	2015	2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC)	10.1109/NAFIPS-WConSC.2015.7284206	fuzzy logic;legal expert system;uncertainty;computer science;artificial intelligence;machine learning;data mining;expert system;cancer	AI	4.498437677008401	-78.88743899264166	50457
6b80b25d5b2c26e940f02e5d4aa2f92a0e528e54	identification of disease states for trauma patients using commonly available hospital data		Trauma is one of the main causes of hospitalization. Time is of the essence in diagnosis and treatment of trauma patients with severe injuries. To assist in decision-making, we propose a hidden Markov model for identification of disease states through which patients progress. An important property of our model is that it is based on features which are routinely collected in hospital trauma centers. Using a hidden Markov model based on fifteen features, six different patient states are identified. The resulting Markov model can be useful in identifying patients’ states to assist in diagnosis and treatment.	hidden markov model;markov chain	Mahnaz Koupaee;Yuanyang Zhang;Tie Bo Wu;Mitchell J. Cohen;Linda R. Petzold	2018	2018 IEEE 8th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)	10.1109/ICCABS.2018.8542101		Robotics	5.079099478765531	-78.21683505238575	50773
fe0ac3c7de31c60db22c1b9bee4b3ccd9d646235	acute kidney injury detection: an alarm system to improve early treatment		This work aims to help in the correct and early diagnosis of the acute kidney injury, through the application of data mining techniques. The main goal is to be implemented in Intensive Care Units (ICUs) as an alarm system, to assist health professionals in the diagnosis of this disease. These techniques will predict the future state of the patients, based on his current medical state and the type of ICU.		Ana Rita Nogueira;Carlos Abreu Ferreira;João Gama	2017		10.1007/978-3-319-60438-1_6	data mining;computer science;disease;alarm;medical emergency;acute kidney injury	Robotics	5.163434559090799	-78.59750452932373	51394
338e5b262fb45023a28592f6cbcdd12995e1ba89	efficient attractor analysis based on self-dependent subsets of elements an application to signal transduction studies	discrete dynamical system;signal transduction;signal transduction pathway;interconnection network	External signals are transmitted to the cells through receptors activating signal transduction pathways. These pathways form a complicated interconnected network, which is able to answer to different stimuli. Here we analyze an important pathway for oncogenesis namely RAS/MAPK signal transduction pathway. We show that the interaction of the elements of this pathway induces topological structure in the element set and that the knowledge of the topology simplifies the analysis of the set. With a computer algorithm, we isolate from a large and complex group, smaller, independent, more manageable subsets, and build their hierarchy. Subsets introduction makes easier the search for attractors in discrete dynamical system, it permits the prediction of final states for elements involved in signal transduction pathways.	anatomy, regional;dynamical system;gene regulatory network;license;mapk signaling pathway;signal transduction pathways;small;transduction (machine learning);algorithm;disease transmission	Maura Cárdenas-García;Jaime Lagunez-Otero;Nikolai A. Korneev	2000	Proceedings. International Conference on Intelligent Systems for Molecular Biology		biology;biochemistry;bioinformatics;signal transduction	Comp.	6.114396203005412	-66.56657995471272	51770
b4a0070038337d10fc68f7fa8472482f4d6e9f11	classification method using fuzzy level set subgrouping	ideal matrices;medical diagnostic;level set;fuzzy level set subgrouping;classification;classification system;data classification	We present a new classification system which is based on fuzzy level sets subgrouping. This new classification system allows a fast classification method with quite accurate results. Classification runs were carried out with four different data sets. All four data sets were related to medical diagnostics. Data sets were related to thyroid and diabetes diagnostics, echocardiogram data relates to predicting patients chances of survival after acute heart attack. With lymphography data there are four classes to predict, normal find, metastases, malign lymph and fibrosis, from the existing data. Classification results are compared to some existing results in the literature and the results seem to compare well.		Paavo Kukkurainen;Pasi Luukka	2008	Expert Syst. Appl.	10.1016/j.eswa.2006.10.023	biological classification;level set;machine learning;classification rule;pattern recognition;data mining;mathematics;one-class classification	Vision	6.9514865006018915	-77.42407954621947	52327
f20e69b10b5556c906d5fa7b824895f8368406d0	the use of feed forward neural network for recognizing characters of dactyl alphabet	feed forward neural network;character recognition;neural network	"""This paper deals with recognition in dependency of count of objects in the training data set. For the capture the fingers position is used the sensory glove 5DT Data Glove Ultra.We describe the computer software """"Dactyl Teacher"""" for recognition characters of Dactyl alphabet. This software contains a feed forward neural network, which is used for character recognition. In experimental part we examined count of iterations to train the neural network in influence of count of hidden layers and of count of neurons in these hidden layers. We choose the sufficient combination of these parameters for our feed forward neural network. Next we examined the success of recognition in dependence of count of training objects in training data set."""	artificial neural network	Roman Záluský;Emil Raschman;Mário Krajmer;Daniela Durackova	2010		10.1007/978-3-642-15825-4_14	feedforward neural network;speech recognition;computer science;artificial intelligence;machine learning;time delay neural network;artificial neural network	AI	-3.383592903766319	-75.60677654231465	52708
7479af3d5998c0ecdecb2d8dba917e117933a49e	data mining in the study of the chronic obstructive pulmonary disease	decision models;data mining;chronic obstructive pulmonary disease;business intelligence	Data Mining algorithms have been used to analyse huge amounts of data and extract useful models or patterns from the analysed data. Those models or patterns can be used to support the decision making process in organizations. In the health domain, and besides the support to the decision process, those algorithms are useful in the analysis and characterization of several diseases. This paper presents the particular case of the use of different Data Mining algorithms to support health care specialists in the analysis and characterization of symptoms and risk factors related with the Chronic Obstructive Pulmonary Disease. This is an airflow limitation that is not fully reversible and that affects up to one quarter of the adults with 40 or more years. For this specific study, data from 1.880 individuals were analysed with decision trees and artificial neural networks in order to identify predictive models for this disease. Clustering was used to identify groups of individuals, with the chronic obstructive pulmonary disease, presenting similar risk factors and symptoms. Furthermore, association rules were used to identify correlations among the risk factors and the symptoms. The results obtained so far are promising as several models confirm the difficulties that are normally associated to the diagnosis of this disease and point to characteristics that must be taken into account in its comprehension.	data mining	Maribel Yasmina Santos;Jorge A. de la Cruz;Artur Teles de Araújo	2012		10.1007/978-3-642-32395-9_2	health care;business intelligence;decision tree;cluster analysis;disease;decision model;artificial neural network;data mining;decision-making;computer science	NLP	4.205399604565542	-76.67085331380366	53072
b133c1f4d0a3149f63a0246eecbf68df08a1b591	a statistical model that takes into account patient heterogeneity in decision making.		Statistical evaluation of clinical treatments or preventive medicine has profoundly contributed to decision making in medical fields such as with the acceptance of new treatment methods and health promotion policies. It is crucial in such decision making to find a correct statistical model to treat a surprisingly large variety of patients, or a heterogeneous group of patients, even with the same diagnosis. In diseases such as cancer, cardiovascular disease or diabetes, patients are often followed up to certain endpoints and these data are frequently analyzed by logrank tests or Cox-models to evaluate treatment effects. Although these methods have been widely accepted and extensively studied, we are sometimes faced with problems in applying these methods when the heterogeneity of patients is large and a lot of prognostic factors affecting the endpoints have to be considered. Based on the results of the analyses of survival data from more than 6,000 gastric cancer patients, it is revealed that the stratified logrank test may suffer serious power loss, even though primary prognostic factors are used as stratified factors. A so-called 'piecewise linear Cox regression method' for properly treating the heterogeneity of patients is introduced and extensively studied. This method is shown to be appropriate for patient groups with a high degree of heterogeneity such as the gastric cancer patients. The same method is, in principle, applicable to patients of other diseases, too, using statistical software such as SAS, BMDP and etc.		Kouhei Akazawa;Tsuyoshi Nakamura;Yoshiaki Nose	1998	Studies in health technology and informatics	10.3233/978-1-60750-896-0-525	data mining;statistical model;medicine	AI	5.797405862233109	-75.80135901504063	53196
18e1a54582b4ffb790a8257e341a3c2a0896f33a	interpreting historical icu data using associational and temporal reasoning	medical administrative data processing;intensive care unit;associate system historical icu data intensive care unit associational reasoning temporal reasoning physiological source historical data analysis patient state assessment temporal expert system data filtering interval identification temporal interval filtered data trend template taxonomical knowledge;medical administrative data processing medical computing data analysis temporal reasoning;filtering medical treatment filters biomedical monitoring heart rate signal processing data analysis medical expert systems noise generators character generation;medical computing;data analysis;historical data;temporal reasoning;expert system	Medical staff in the Intensive Care Unit (ICU) are confronted with large volumes of continuous data from several physiological sources which require interpretation. The ASSOCIATE system analyses historical data for summarisation and patient state assessment. It uses a temporal expert system based on associational reasoning and applies three consecutive processes: filtering, which is used to remove noise; interval identification to generate temporal intervals from the filtered data - intervals which are characterised by a common direction of change (i.e. increasing, decreasing or steady); and interpretation which performs summarisation and patient state-assessments. Using the temporal intervals, interpretation involves differentiating between events which are clinically insignificant and events which are clinically significant and determining the outcome of therapy. Inherent in this process is the trend template which is used to represent events. Trend templates support temporal reasoning, knowledge to differentiate between events and taxonomical knowledge. Algorithms which are analogous to the way clinicians identify events use these trend templates.	international components for unicode	Apkar Salatian	2003		10.1109/TAI.2003.1250223	computer science;artificial intelligence;data science;machine learning;data mining;data analysis;expert system	AI	3.02139416649707	-76.76821151308795	53366
266cf6427ec1e534cc349c84f77047681a34833d	a model for quality defects detection in record sequences and its application for obstetric data in electronic medical records		Scenarios, in which real-world state transitions are documented by a sequence of data records, introduce unique data quality (DQ) challenges. Due to time and workload constraints, one might choose to update the values only for a subset of the required attributes, while replicating previous values of others. As a result, the record sequence might reflect the current state incorrectly and fail to capture critical transitions. Our model addresses such scenarios by evaluating record sequences and alerting on high likelihood of erroneous replication. The metrics consider attribute characteristics, the distances between consecutive values, and the likelihood of value transition. The potential contribution of the model is demonstrated with a preliminary evaluation of 200 real-world records collected in an Obstetrics unit in a large hospital. A model trained with 100 records reached accuracy level of 85% with detecting data acquisition flaws in the other 100. The manuscript introduces the model development, describes the preliminary evaluation with real-world data, and highlights directions for future research progress. • Information Systems  Database Management Systems  Data Cleaning • Medical Information Policy  Medical Records	data acquisition;data quality;distance matrix;information system;management system;sensor;tree accumulation	Tal Aboudy;Yoav Brezinov;Adir Even;Edi Vaisbuch	2016			medical record;data mining;computer science	DB	4.048393659321523	-77.67930992149905	53369
3605c45e4f7bc7b20785e081bd632ac70f0cdf8b	mobile cloud-based depression diagnosis using an ontology and a bayesian network	depression diagnosis;bayesian network;ontology application;mobile cloud;mobile and ubiquitous healthcare;article	Recently, depression has becomes a widespread disease throughout the world. However, most people are not aware of the possibility of becoming depressed during their daily lives. Therefore, obtaining an accurate diagnosis of depression is an important issue in healthcare. In this study, we built an inference model based on an ontology and a Bayesian network to infer the possibility of becoming depressed, and we implemented a prototype using a mobile agent platform as a proof-of-concept in the mobile cloud. We developed an ontology model based on the terminology used to describe depression and we utilized a Bayesian network to infer the probability of becoming depressed. We also implemented the system using multi-agents to run on the Android platform, thereby demonstrating the feasibility of this method, and we addressed various implementation issues. The results showed that our method may be useful for inferring a diagnosis of depression. Integrating Ontology with Bayesian Network to predict getting depressed or not.Using mobile agent and cloud environment to implement the diagnosis environment.Evaluation result shown the system is feasibility for doing the prediction.	bayesian network;cloud computing	Yue-Shan Chang;Chih-Tien Fan;Win-Tsung Lo;Wan-Chun Hung;Shyan-Ming Yuan	2015	Future Generation Comp. Syst.	10.1016/j.future.2014.05.004	simulation;computer science;bayesian network;data mining;world wide web	AI	1.996168937580102	-78.9373420944666	53514
42770cab9a2d6a8fe32b4e73464047ac115006ec	application of data mining techniques to a behavioral risk factor data set to predict long-term disability after stroke			data mining	Sunmoo Yoon;Jose Gutierrez;Adam B. Wilcox;Suzanne Bakken	2012			data mining;risk factor;psychology;stroke	ML	3.526861913537036	-76.06893906751054	53716
fdfa8ba836008eb5c8f542a994104f794a89868c	a conceptual and computational framework for modelling and understanding the non-equilibrium gene regulatory networks of mouse embryonic stem cells		"""The capacity of pluripotent embryonic stem cells to differentiate into any cell type in the body makes them invaluable in the field of regenerative medicine. However, because of the complexity of both the core pluripotency network and the process of cell fate computation it is not yet possible to control the fate of stem cells. We present a theoretical model of stem cell fate computation that is based on Halley and Winkler's Branching Process Theory (BPT) and on Greaves et al.'s agent-based computer simulation derived from that theoretical model. BPT abstracts the complex production and action of a Transcription Factor (TF) into a single critical branching process that may dissipate, maintain, or become supercritical. Here we take the single TF model and extend it to multiple interacting TFs, and build an agent-based simulation of multiple TFs to investigate the dynamics of such coupled systems. We have developed the simulation and the theoretical model together, in an iterative manner, with the aim of obtaining a deeper understanding of stem cell fate computation, in order to influence experimental efforts, which may in turn influence the outcome of cellular differentiation. The model used is an example of self-organization and could be more widely applicable to the modelling of other complex systems. The simulation based on this model, though currently limited in scope in terms of the biology it represents, supports the utility of the Halley and Winkler branching process model in describing the behaviour of stem cell gene regulatory networks. Our simulation demonstrates three key features: (i) the existence of a critical value of the branching process parameter, dependent on the details of the cistrome in question; (ii) the ability of an active cistrome to """"ignite"""" an otherwise fully dissipated cistrome, and drive it to criticality; (iii) how coupling cistromes together can reduce their critical branching parameter values needed to drive them to criticality."""	abstract summary;agent-based model;agent-based social simulation;cpu power dissipation;complex systems;computation (action);computer simulation;criticality matrix;embryonic stem cells;equilibrium;gene regulatory network;histopathologic grade differentiation;interaction;iteration;medical transcription;population parameter;process modeling;regenerative medicine;self-organization;self-organized criticality;transcription factor;theory	Richard B. Greaves;Sabine Dietmann;Austin G. Smith;Susan Stepney;Julianne D. Halley	2017		10.1371/journal.pcbi.1005713	complex system;bioinformatics;stem cell;cellular differentiation;branching process;biology;gene regulatory network;cistrome;critical value;cell fate determination	Comp.	7.017254576712525	-67.66965789783457	54331
757f83a690890bdc7038ffa1531847d40a9f74a4	overnight features of transcutaneous carbon dioxide measurement as predictors of metabolic status	sleep disordered breathing;carbon dioxide;predictive mathematical modeling;pattern classification;mathematical model	OBJECTIVE To systematically investigate whether overnight features in transcutaneous carbon dioxide (P(TcCO(2)) measurements can predict metabolic variables in subject with suspected sleep-disordered breathing.   METHODS The features extracted from the P(TcCO(2)) signal included the number of abrupt descents per hour and attributes that characterize the recovery after such an event. For each outcome variable, the subgroup of the 108 study subjects with the particular variable present was divided into two representative classes, and the optimal features that can predict the classes were learned. Overfitting was avoided by evaluating the classification algorithms using 10-fold cross-validation.   RESULTS (P(TcCO(2)) signal has a key role in determining the classes of high-density lipoprotein cholesterol and thyroid-stimulating hormone concentrations, and it improves the classification accuracy of glycosylated hemoglobin A1c and fasting plasma glucose values.   CONCLUSIONS The features learned from the (P(TcCO(2)) signal reflected the state of the selected metabolic variables in a subtle, but systematic, way. These findings provide a step towards understanding how metabolic disturbances are connected to carbon dioxide exchange during sleep.		Arho Virkki;Olli Polo;Tarja Saaresranta;Anne Laapotti-Salo;Mats Gyllenberg;Tero Aittokallio	2008	Artificial intelligence in medicine	10.1016/j.artmed.2007.09.001	simulation;carbon dioxide;mathematical model;statistics	AI	9.248739082698076	-76.68717505530367	54639
b3a0cc4a642000eaa344a61a60400dece099c150	adaptive apriori and weighted association rule mining on visual inspected variables for predicting obstructive sleep apnea		A prediction model for Obstructive Sleep Apnea (OSA) is developed based on a novel formulation approach by using customized Associative Rule (AR) Mining Techniques, i.e. Adaptive Apriori (AA) and Weighted Association Rule Mining (WARM), on visual inspected variables. This prediction model is based on the typical clinical data sets obtained from several hospitals where from derivation of association rule mining (data-driven) techniques and medical knowledge on these variables (knowledge-driven) were applied separately to our training data sets. Application of our prediction framework to our testing data sets showed a significant improvement in terms of prediction accuracy and level of efficiency as compared with the classical approach of using medical Experts’ Rules (ERs).	apriori algorithm;association rule learning	Doreen Ying Ying Sim;Chee Siong Teh;Ahmad Izuanuddin Ismail	2014	Austr. J. Intelligent Information Processing Systems		engineering;data science;pattern recognition;data mining	ML	5.110660328468505	-76.65817510237616	54871
5e358bd7a52564f5d27985148d5e386a1487cf74	investigating the pattern of syndrome based on the difference of symptom network in depression	patient diagnosis;analytical models;support vector machines;bayes methods;support vector machines accuracy computational modeling data models diseases analytical models blood;medical computing;accuracy;computational modeling;blood;pattern of syndrome;pattern classification;diseases;patient treatment;depression symptom network network difference pattern of syndrome;learning artificial intelligence;network difference;support vector machines bayes methods diseases learning artificial intelligence medical computing patient diagnosis patient treatment pattern classification;depression symptom network tcm theory diseases diagnosis patient treatment syndrome identification syndrome pattern symptomatic level naive bayes network svm syndrome classification interaction profile fisher score demonstrate;symptom network;depression;data models	In TCM theory, the syndrome is crucial to diagnose diseases and treat patients. In syndrome identification, the relation of symptoms usually correlates with syndrome and represents the pattern of syndrome at symptomatic level. Hence, we learn models for classifying syndromes in depression using 4 different algorithms, which are naive Bayes, Bayes network, SVM and C4.5. From the results of classification, we find that the dependence of symptoms has something to do with the accuracies of syndrome classification. Then, 8 symptom networks corresponding to depression and 7 syndromes are constructed to explore the interaction profile of symptoms under syndrome. By comparing syndrome-specific symptom network to the base network of depression, we discover the enriched edges and different nodes to represent the pattern of each syndrome. Literature and symptom ranking by Fisher score demonstrate the correctness of the different nodes selected through network comparison. After all, the enriched edges and different nodes associated with a given syndrome reveal the pattern of that syndrome at symptomatic level.	bayesian network;c4.5 algorithm;correctness (computer science);fisher information;interaction design pattern;linear discriminant analysis;naive bayes classifier;toolkit for conceptual modeling	Jianglong Song;Xi Liu;Wen Dai;Yibo Gao;Lin Chen;Yunling Zhang;Hong Zheng;Zhichen Zhang;Miao Yu;Jianxin Chen;Peng Lu;Rongjuan Guo	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732682	data modeling;support vector machine;computer science;machine learning;pattern recognition;data mining;accuracy and precision;computational model;statistics	Vision	10.001197792194505	-75.77492254365028	55186
2ab92785565740f5e72254a618d3486439e5385f	fuzzy neural network-based influenza diagnostic system	patient diagnosis;fuzzy theory;fuzzy neural network;fuzzy neural nets;influenza back propagation neural network diagnostic expert system fuzzy theory fuzzy neural network;fuzzy neural networks equations mathematical model influenza diseases expert systems;backpropagation;influenza;back propagation neural network;fuzzy set theory;medical expert systems;patient diagnosis backpropagation diseases fuzzy neural nets fuzzy set theory medical expert systems;diseases;illnesses fuzzy neural network based influenza diagnostic system described symptoms medical diagnostic aid expert system fuzzy system backpropagation neural network bpnn core engines influenza diagnostic expert system three systems diagnostic reference diagnostic error rates early detections doctor treatment;diagnostic;expert system	As certain diseases are characterized by subjective perceptions of described symptoms, if symptoms are not obvious, physicians can easily mistake them for other illnesses. In order to assist physicians to quickly and accurately diagnose results, a medical diagnostic aid expert system was put forth in this study. The system uses the fuzzy system, back-propagation neural network (BPNN), and fuzzy neural network (FNN) as the core engines of the influenza diagnostic expert system. The three systems were compared whereas the expert system's inferred output served as the data for the prognosis of occurrences of illnesses, thereby providing physicians a diagnostic reference and reducing diagnostic error rates in order to ensure early detections and treatment by doctors and prevent more serious illnesses that may arise due to complications.	artificial neural network;backpropagation;expert system;fuzzy control system;neuro-fuzzy;sensor;software propagation	Chun-Ling Lin;Sheng-Ta Hsieh;You-Jhong Hu	2013	2013 First International Symposium on Computing and Networking	10.1109/CANDAR.2013.115	medicine;artificial intelligence;machine learning;data mining	Arch	4.912982815250514	-78.7826872232972	55578
c681324ce114d76c78820f8f249a35fc0a10f6d4	automated diagnosis of breast cancer on medical images	medical images;esi embedded systems innovation;healthy for life;ts technical sciences;industrial innovation;diagnosis computer assisted;communication information;electronics;automated;breast cancer	The development and use of computerized decision-support systems in the domain of breast cancer has the potential to facilitate the early detection of disease as well as spare healthy women unnecessary interventions. Despite encouraging trends, there is much room for improvement in the capabilities of such systems to further alleviate the burden of breast cancer. One of the main challenges that current systems face is integrating and translating multi-scale variables like patient risk factors and imaging features into complex management recommendations that would supplement and/or generalize similar activities provided by subspecialty-trained clinicians currently. In this chapter, we discuss the main types of knowledge—object-attribute, spatial, temporal and hierarchical—present in the domain of breast image analysis and their formal representation using two popular techniques from artificial intelligence—Bayesian networks and first-order logic. In particular, we demonstrate (i) the explicit representation of uncertain relationships between low-level image features and high-level image findings (e.g., mass, microcalcifications) by probability distributions in Bayesian networks, and (ii) the expressive power of logic to generally represent the dynamic number of objects in the domain. By concrete examples with patient data we show the practical application of both formalisms and their potential for use in decision-support systems.		Marina Velikova;Inês de Castro Dutra;Elizabeth S. Burnside	2015		10.1007/978-3-319-28007-3_4	medicine;pathology;gynecology;biological engineering	Vision	0.9878679029207484	-76.32500150444683	56407
5d311c709cd6a3e97cb02f646cb732261db75db3	uncovering disease associations via integration of biological networks	thesis or dissertation		biological network	Kai Sun	2014			psychology;developmental psychology;bioinformatics;data science	Theory	0.3825185463991549	-66.45704457525176	56510
e63e4233a1b95168d5558dc1ead59c8853823e02	dynamic handwriting analysis for supporting earlier parkinson's disease diagnosis		Machine learning techniques are tailored to build intelligent systems to support clinicians at the point of care. In particular, they can complement standard clinical evaluations for the assessment of early signs and manifestations of Parkinson’s disease (PD). Patients suffering from PD typically exhibit impairments of previously learned motor skills, such as handwriting. Therefore, handwriting can be considered a powerful marker to develop automatized diagnostic tools. In this paper, we investigated if and to which extent dynamic features of the handwriting process can support PD diagnosis at earlier stages. To this end, a subset of the publicly available PaHaW dataset has been used, including those patients showing only early to mild degree of disease severity. We developed a classification framework based on different classifiers and an ensemble scheme. Some encouraging results have been obtained; in particular, good specificity performances have been observed. This indicates that a handwriting-based decision support tool could be used to administer screening tests useful for ruling in disease.		Donato Impedovo;Giuseppe Pirlo;Gennaro Vessio	2018	Information	10.3390/info9100247	artificial intelligence;machine learning;data mining;computer-aided diagnosis;parkinson's disease;computer science;intelligent decision support system;point of care;decision support system;handwriting	AI	6.821086819147223	-76.76180151200333	57122
882eeab38bfaf7abda043e2439f1ee7f540a7386	biomarker clustering of colorectal cancer data to complement clinical classification	pattern clustering;measurement;cancer;tumours;medical computing;indexes;immune system;pattern classification;tumors;tumours cancer medical computing pattern classification pattern clustering;colorectal cancer;and patient histochemistry biomarker clustering colorectal cancer data clinical classification colorectal tumours immunological status tumour removal tumour classification optimal clustering treatment options tumour physiology;tumors cancer immune system indexes educational institutions measurement	In this paper, we describe a dataset relating to cellular and physical conditions of patients who are operated upon to remove colorectal tumours. This data provides a unique insight into immunological status at the point of tumour removal, tumour classification and post-operative survival. Attempts are made to cluster this dataset and important subsets of it in an effort to characterize the data and validate existing standards for tumour classification. It is apparent from optimal clustering that existing tumour classification is largely unrelated to immunological factors within a patient and that there may be scope for re-evaluating treatment options and survival estimates based on a combination of tumour physiology and patient histochemistry.	cluster analysis	Chris M. Roadknight;Uwe Aickelin;Alexandros Ladas;Daniele Soria;John Scholefield;Lindy Durrant	2012	2012 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.2139/ssrn.2828496	database index;immune system;bioinformatics;colorectal cancer;measurement;cancer	Vision	5.065970133643994	-72.55232184614185	57271
387dbd35afcd96b45a54771f455fe87badfd2e05	mixing logic programming and neural networks to support neurological disorders analysis		The incidence of neurological disorders is constantly growing, and the use of Artificial Intelligence techniques in supporting neurologists is steadily increasing. Deductive reasoning and neural networks are two prominent areas in AI that can support discovery processes; unfortunately, they have been considered as separate research areas for long time. In this paper we start from a specific neurological disorder, namely Multiple Sclerosis, to define a generic framework showing the potentially significant impact of mixing rule-based systems and neural networks. The ambitious goal is to boost the interest of the research community in developing a more tight integration of these two approaches.	logic programming;neural networks	Francesco Calimeri;Francesco Cauteruccio;Aldo Marzullo;Claudio Stamile;Giorgio Terracina	2018		10.1007/978-3-319-99906-7_3	data mining;neurological disorder;deductive reasoning;rule-based system;artificial neural network;logic programming;artificial intelligence;computer science	ML	7.256987024957543	-79.43651718920799	57675
eaea6ea44b85f6832d6927e55bb94cfde2d48274	analysis of adaptive response to dosing protocols for biofilm control	dosing strategies;bacterial biofilms;escherichia coli;grupo de excelencia;35l02;gene expression;persister formation;biocide action;antibiotic resistance;ciencias basicas y experimentales;pseudomonas aeruginosa biofilms;matematicas;biofilm;adaptive response;analysis;beta lactamase;antimicrobial agents;in vitro;92b	Biofilms are sessile populations of microbes that live within a self-secreted matrix of extracellular polymers. They exhibit high tolerance to antimicrobial agents, and experimental evidence indicates that in many instances repeated doses of antimicrobials further reduce disinfection efficiency due to an adaptive stress response. In this investigation, a mathematical model of bacterial adaptation is presented consisting of an adapted-unadapted population system embedded within a moving boundary problem coupled to a reaction-diffusion equation. The action of antimicrobials on biofilms under different dosing protocols is studied both analytically and numerically. We find the limiting behavior of solutions under periodic and on-off dosing as the period is made very large or very small. High dosages often carry undesirable side effects so we specially consider low dosing regimes. Our results indicate that on-off dosing for small doses of biocide is more effective than constant dosing. Moreover, in a specific case, on-off dosing for short periods is again more effective regardless of the biocide dose. We also provide sufficient conditions for the eradication of biofilms under a constant dosing regime.		Barbara Szomolay;Isaac Klapper;Martin Dindos	2010	SIAM Journal of Applied Mathematics	10.1137/080739070	antibiotic resistance;gene expression;antimicrobial;adaptive response;analysis;in vitro;escherichia coli;biofilm	Metrics	8.478045981709055	-66.66551062587004	57705
cea3652a3bd1d47e5b1f37661c810fb801ecf8fc	cancer stem cell modeling using a cellular automaton		We used a cellular automaton model for cancer growth simulation at cellular level, based on the presence of different cancer hallmarks acquired by the cells. The rules of the cellular automaton determine cell mitotic and apoptotic behaviors, which are based on the acquisition of the hallmarks in the cells by means of mutations. The simulation tool allows the study of the emergent behavior of tumor growth. This work focuses on the simulation of the behavior of cancer stem cells to inspect their capability of regeneration of tumor growth in different scenarios.	cellular automaton	Ángel Monteagudo;José Santos Reyes	2013		10.1007/978-3-642-38622-0_3	cell biology;cancer;computer science;machine learning;artificial intelligence;cellular automaton;cancer stem cell;apoptosis;mitosis;cell	Robotics	6.80866727539618	-67.5593394561494	58391
ac30e8845375c8c87f12bd675bdfdd20b8c57978	missing data imputation on the 5-year survival prediction of breast cancer patients with unknown discrete values	discrete data;5 year survival prediction;missing data;breast cancer;imputation	Breast cancer is the most frequently diagnosed cancer in women. Using historical patient information stored in clinical datasets, data mining and machine learning approaches can be applied to predict the survival of breast cancer patients. A common drawback is the absence of information, i.e., missing data, in certain clinical trials. However, most standard prediction methods are not able to handle incomplete samples and, then, missing data imputation is a widely applied approach for solving this inconvenience. Therefore, and taking into account the characteristics of each breast cancer dataset, it is required to perform a detailed analysis to determine the most appropriate imputation and prediction methods in each clinical environment. This research work analyzes a real breast cancer dataset from Institute Portuguese of Oncology of Porto with a high percentage of unknown categorical information (most clinical data of the patients are incomplete), which is a challenge in terms of complexity. Four scenarios are evaluated: (I) 5-year survival prediction without imputation and 5-year survival prediction from cleaned dataset with (II) Mode imputation, (III) Expectation-Maximization imputation and (IV) K-Nearest Neighbors imputation. Prediction models for breast cancer survivability are constructed using four different methods: K-Nearest Neighbors, Classification Trees, Logistic Regression and Support Vector Machines. Experiments are performed in a nested ten-fold cross-validation procedure and, according to the obtained results, the best results are provided by the K-Nearest Neighbors algorithm: more than 81% of accuracy and more than 0.78 of area under the Receiver Operator Characteristic curve, which constitutes very good results in this complex scenario.	complexity;cross reactions;cross-validation (statistics);data mining;decision tree;expectation–maximization algorithm;experiment;geo-imputation;greater than;k-nearest neighbors algorithm;logistic regression;machine learning;mammary neoplasms;missing data;partial;patients;receiver operating characteristic;silo (dataset);statistical imputation;support vector machine	Pedro J. García-Laencina;Pedro Abreu;Miguel Henriques Abreu;Noémia Afonoso	2015	Computers in biology and medicine	10.1016/j.compbiomed.2015.02.006	econometrics;medicine;missing data;breast cancer;data mining;mathematics;imputation;statistics	ML	5.550175484122563	-76.12380121382536	58542
67fecd7d53950cddff98efcb19c7947be6aab2a0	explodelayout: comprehending patient subgroups in large networks		Networks have been used to successfully identify and comprehend patient subgroups based on their characteristics (e.g., comorbidities or genes) with the goal of designing targeted interventions, a cornerstone of precision medicine. However, current network layout algorithms often fail to reveal patterns in large and dense networks despite having significant clustering. We therefore developed an algorithm called ExplodeLayout, which exploits the existence of clusters to automatically “explode” a traditional network layout with the goal of separating overlapping clusters to enhance their comprehensibility. We demonstrate the use of this algorithm to visualize a large dataset extracted from Medicare consisting of readmitted hip-fracture patients and their comorbidities, which enabled clinicians to comprehend patient subgroups and their comorbidities, and to infer mechanisms related to hospital readmission.	algorithm;cluster analysis;precision medicine	Bryant Dang;Joseph Mathew;Tianlong Chen;Suresh K. Bhavnani	2016			medicine	ML	1.809103501452745	-70.0565620208783	58646
e2e1f65f286f81ff54841fe4552890c1d091f6c3	bayesian network models for the management of ventilator-associated pneumonia = bayesiaanse netwerkmodellen voor de diagnose en behandeling van beademingsgerelateerde longontsteking	bayesian network;mechanical ventilation;electronic patient record;ventilator associated pneumonia;data collection;temporal data;clinical decision making;statistical model;decision support system;intensive care;dynamic bayesian network;clinical decision support system;expert knowledge			Stefan Visscher	2008			intensive care medicine;medicine;data science;data mining	AI	3.9050100960640797	-76.09548157626404	59014
a3b6c5ef6a86c56885bc98c969f1d4ccfbe03506	detection of correct and incorrect measurements in real-time continuous glucose monitoring systems by applying a postprocessing support vector machine	support vector machines;iso standards;support vector machines sugar accuracy insulin standards monitoring;sensitivity;blood;support vector machines svms balanced performance continuous glucose monitoring critically ill patients fault detection;signal classification;diseases;patient treatment;patient monitoring;time 72 h correct measurement detection incorrect measurement detection real time continuous glucose monitoring systems post processing support vector machine svm learning mechanism postprocessing strategy imbalanced datasets sensitivity critically ill patient monitoring insulin therapy dataset classification international standards organization sepsis septic shock;biomedical measurement;medical signal processing;biochemistry;biomedical equipment;aged algorithms blood glucose computer systems diagnosis computer assisted drug therapy computer assisted female humans hyperglycemia hypoglycemic agents insulin male middle aged reproducibility of results sensitivity and specificity support vector machines;support vector machines biochemistry biomedical equipment biomedical measurement blood diseases iso standards medical signal processing patient monitoring patient treatment sensitivity signal classification	Support vector machines (SVMs) are an attractive option for detecting correct and incorrect measurements in real-time continuous glucose monitoring systems (RTCGMSs), because their learning mechanism can introduce a postprocessing strategy for imbalanced datasets. The proposed SVM considers the geometric mean to obtain a more balanced performance between sensitivity and specificity. To test this approach, 23 critically ill patients receiving insulin therapy were monitored over 72 h using an RTCGMS, and a dataset of 537 samples, classified according to International Standards Organization (ISO) criteria (372 correct and 165 incorrect measurements), was obtained. The results obtained were promising for patients with septic shock or with sepsis, for which the proposed system can be considered as reliable. However, this approach cannot be considered suitable for patients without sepsis.	analyzer, device;blood volume;cgms-a;calibration;capillary permeability;classification;critical illness;diabetes mellitus;double minutes;durability (database systems);experiment;futures studies;glucose;international components for unicode;international normalized ratio;interstitial webpage;job control (unix);laboratory procedures;normal range;patients;plasma active;real-time clock;requirement;schedule ii substance;sensitivity and specificity;sensor;sepsis;septic shock;septic equation;silo (dataset);support vector machine;gas analyser;intensive care unit;standards characteristics	Yenny Leal;Luis González Abril;Carol Lorencio;Jorge Bondia;Josep Vehí	2013	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2013.2244092	support vector machine;intensive care medicine;medicine;sensitivity;computer science;remote patient monitoring;data mining;biological engineering	Visualization	8.51565931497235	-77.28046704110072	59241
596c4b9e0b7817120acb917cff4728fab9f95ca8	sample size estimation in diagnostic test studies of biomedical informatics	sample size;area under the curve;sensitivity;roc analysis;diagnostic studies;specificity	OBJECTIVES This review provided a conceptual framework of sample size calculations in the studies of diagnostic test accuracy in various conditions and test outcomes.   METHODS The formulae of sample size calculations for estimation of adequate sensitivity/specificity, likelihood ratio and AUC as an overall index of accuracy and also for testing in single modality and comparing two diagnostic tasks have been presented for desired confidence interval.   RESULTS The required sample sizes were calculated and tabulated with different levels of accuracies and marginal errors with 95% confidence level for estimating and for various effect sizes with 80% power for purpose of testing as well. The results show how sample size is varied with accuracy index and effect size of interest.   CONCLUSION This would help the clinicians when designing diagnostic test studies that an adequate sample size is chosen based on statistical principles in order to guarantee the reliability of study.	area under curve;confidence intervals;diagnostic tests;estimated;informatics (discipline);international normalized ratio;marginal model;modality (human–computer interaction);sample size;sensitivity and specificity	Karimollah Hajian-Tilaki	2014	Journal of biomedical informatics	10.1016/j.jbi.2014.02.013	sample size determination;sensitivity;area under the curve;computer science;data mining;receiver operating characteristic;diagnostic test;statistics	HCI	7.210281241977652	-74.83270345934905	60360
03b739d77aba75ea6ee5de389f85c1f683a6bf90	a new neural network structure for detection of coronary heart disease	coronary heart disease;appareil circulatoire pathologie;diagnostico;exploracion;statistical method;backpropagation;aparato circulatorio patologia;risk factors;cardiovascular disease;system design;exploration;multi layer perceptron;reseau neuronal;coronary heart dis ease;diagnosis;selection criteria;cardiopatia coronaria;red neuronal;cardiopathie coronaire;neural network;high risk;diagnostic	We investigate the application of neural networks for the detection of Coronary Heart Disease (CHD). We have used a Neural Network (NN) on data from a self- applied questionnaire to implement a decision system designed to seek out high risk individuals in a large population. A Multi- Layered Perceptron (MLP) was trained with risk factors to distinguish CHD. We also describe a modification to the architecture of the neural network in which an extra layer of neurons is added at the input. We present possible interpretations of the weights of these neurons, and show how they can be used as a selection criteria for which questions to use as inputs. The technique is compared against other statistical methods. We go on to demonstrate the system's capability for detecting both the symptomatic and asymptomatic patient.	artificial neural network;decision support system;memory-level parallelism;perceptron;sensor	Z. Shen;Malcolm Clarke;R. W. Jones;T. Alberti	1995	Neural Computing & Applications	10.1007/BF01414079	exploration;computer science;artificial intelligence;backpropagation;machine learning;time delay neural network;multilayer perceptron;risk factor;artificial neural network;systems design	ML	5.265610400360444	-79.04991038240644	60379
b5487b8abc7934c29118a3dc7a18e5f25ff2d96b	living on three time scales: the dynamics of plasma cell and antibody populations illustrated for hepatitis a virus	hepatitis a virus;journal contribution;plasma cells;hepatitis a vaccines;antibodies viral;time factors;models immunological;antigens viral;humans;immunity innate;computer simulation	Understanding the mechanisms involved in long-term persistence of humoral immunity after natural infection or vaccination is challenging and crucial for further research in immunology, vaccine development as well as health policy. Long-lived plasma cells, which have recently been shown to reside in survival niches in the bone marrow, are instrumental in the process of immunity induction and persistence. We developed a mathematical model, assuming two antibody-secreting cell subpopulations (short- and long-lived plasma cells), to analyze the antibody kinetics after HAV-vaccination using data from two long-term follow-up studies. Model parameters were estimated through a hierarchical nonlinear mixed-effects model analysis. Long-term individual predictions were derived from the individual empirical parameters and were used to estimate the mean time to immunity waning. We show that three life spans are essential to explain the observed antibody kinetics: that of the antibodies (around one month), the short-lived plasma cells (several months) and the long-lived plasma cells (decades). Although our model is a simplified representation of the actual mechanisms that govern individual immune responses, the level of agreement between long-term individual predictions and observed kinetics is reassuringly close. The quantitative assessment of the time scales over which plasma cells and antibodies live and interact provides a basis for further quantitative research on immunology, with direct consequences for understanding the epidemiology of infectious diseases, and for timing serum sampling in clinical trials of vaccines.	bone marrow;communicable diseases;follow-up report;hepatitis a;hepatitis c virus;humoral immunity;immune response;immunology;kinetics internet protocol;long-term follow-up;mathematical induction;mathematical model;mathematics;nonlinear system;persistence (computer science);plasma active;plasma cells;population;sampling (signal processing);sampling - surgical action	Mathieu Andraud;Olivier Lejeune;Jammbe Z. Musoro;Benson Ogunjimi;Philippe Beutels;Niel Hens	2012		10.1371/journal.pcbi.1002418	computer simulation;biology;virology;immunology	ML	9.521817578305011	-68.66685759482782	61202
c47f1442baddf54b0ad35a6a2e7262142ec3d5e1	estimating second order probability beliefs from subjective survival data	second order;subjective expectations;survival data;econometric model;subjective probability;health and retirement study;model performance;epistemic probability beliefs;health literacy;ambiguity;survival;survey research methodology;survival probability	Based on subjective survival probability questions in the Health and Retirement Study (HRS), we use an econometric model to estimate the determinants of individual-level uncertainty about personal longevity. This model is built around the modal response hypothesis (MRH), a mathematical expression of the idea that survey responses of 0%, 50%, or 100% to probability questions indicate a high level of uncertainty about the relevant probability. We show that subjective survival expectations in 2002 line up very well with realized mortality of the HRS respondents between 2002 and 2010. We show that the MRH model performs better than typically used models in the literature of subjective probabilities. Our model gives more accurate estimates of low probability events and it is able to predict the unusually high fraction of focal 0%, 50%, and 100% answers observed in many data sets on subjective probabilities. We show that subjects place too much weight on parents' age at death when forming expectations about their own longevity, whereas other covariates such as demographics, cognition, personality, subjective health, and health behavior are under weighted. We also find that less educated people, smokers, and women have less certain beliefs, and recent health shocks increase uncertainty about survival, too.	cessation of life;cognition;continuance of life;demography;diagnostic self evaluation;econometric model;estimated;focal (programming language);high-level programming language;lipoid dermatoarthritis;mathematics;modal logic;probability;shock;structural dynamics	Péter Hudomiet;Robert J. Willis	2013	Decision analysis : a journal of the Institute for Operations Research and the Management Sciences	10.1287/deca.2013.0266	econometrics;actuarial science;mathematics;econometric model;second-order logic;statistics	AI	7.209979247080582	-71.56652635831028	61658
2dac941605f99215178e6601efdca9daf967d365	a systems-theoretic model of a biological circuit for molecular communication in nanonetworks	biological nanomachine;molecular communication;synthetic biology;nanonetworks;biological circuit;diffusion equation	Recent advances in synthetic biology, in particular towards the engineering of DNAbased circuits, are providing tools to program man-designed functions within biological cells, thus paving the way for the realization of biological nanoscale devices, known as nanomachines. By stemming from the way biological cells communicate in the nature, Molecular Communication (MC), i.e., the exchange of information through the emission, propagation, and reception of molecules, has been identified as the key paradigm to interconnect these biological nanomachines into nanoscale networks, or nanonetwork. The design of MC nanonetworks built upon biological circuits is particularly interesting since cells possess many of the elements required to realize this type of communication, thus enabling the design of cooperative functions in the biological environment. In this paper, a systems-theoretic modeling is realized by analyzing a minimal subset of biological circuit elements necessary to be included in an MC nanonetwork design where the messagebearing molecules are propagated via free diffusion between two cells. The obtained system-theoretic models stem from the biochemical processes underlying cell-to-cell MC, and are analytically characterized by their transfer functions, attenuation and delay experienced by an information signal exchanged by the communicating cells. Numerical results are presented to evaluate the obtained analytical expressions as functions of realistic biological parameters. © 2014 Elsevier Ltd. All rights reserved.	cell signaling;embedded system;gene regulatory network;nanonetwork;nanorobotics;numerical analysis;programming paradigm;software propagation;stemming;synthetic biological circuit;synthetic biology;systems design;theory;transfer function	Massimiliano Pierobon	2014	Nano Comm. Netw.	10.1016/j.nancom.2014.04.002	diffusion equation;electronic engineering;telecommunications;computer science;bioinformatics;engineering;nanotechnology;nanonetwork;synthetic biology	EDA	3.777900980793498	-68.88405618798488	61883
be32acf88dfe4b0e33056720e838fa88eee9f723	decision support systems for incurable non-small cell lung cancer: a systematic review	decision support systems;non-small-cell lung cancer;prognosis;survival	BACKGROUND Individually tailored cancer treatment is essential to ensure optimal treatment and resource use. Treatments for incurable metastatic non-small cell lung cancer (NSCLC) are evolving rapidly, and decision support systems (DSS) for this patient population have been developed to balance benefits and harms for decision-making. The aim of this systematic review was to inventory DSS for stage IIIB/IV NSCLC patients.   METHODS A systematic literature search was performed in Pubmed, Embase and the Cochrane Library. DSS were described extensively, including their predictors, model performances (i.e., discriminative ability and calibration), levels of validation and user friendliness.   RESULTS The systematic search yielded 3531 articles. In total, 67 articles were included after additional reference tracking. The 39 identified DSS aim to predict overall survival and/or progression-free survival, but give no information about toxicity or cost-effectiveness. Various predictors were incorporated, such as performance status, serum and inflammatory markers, and patient and tumor characteristics. Some DSS were developed for the entire incurable NSCLC population, whereas others were specifically for patients with brain or spinal metastases. Few DSS had been validated externally using recent clinical data, and the discrimination and calibration were often poor.   CONCLUSIONS Many DSS have been developed for incurable NSCLC patients, but DSS are still lacking that are up-to-date with a good model performance, while covering the entire treatment spectrum. Future DSS should incorporate genetic and biological markers based on state-of-the-art evidence, and compare multiple treatment options to estimate survival, toxicity and cost-effectiveness.	adverse reaction to drug;biological markers;calibration;cochrane library;color gradient;cost effectiveness;decision making;decision support systems, clinical;decision support system;embase;neoplasm metastasis;neoplasms;non-small cell lung carcinoma;overall survival;patients;progression-free survival;pubmed;small cell carcinoma of lung;systematic review;usability;benefit;cancer therapy;decision support systems;performance status;spinal metastases	Dóra Révész;E. G. Engelhardt;J. J. Tamminga;F. M. N. H. Schramel;Bregje D. Onwuteaka-Philipsen;E. M. W. van de Garde;Ewout Willem Steyerberg;E. P. Jansma;H. C. W. De Vet;Veerle M. H. Coupé	2017		10.1186/s12911-017-0542-1	cancer;lung cancer;decision support system;population;health informatics;intensive care medicine;medicine	Comp.	8.561172679430413	-74.13493968641917	62109
3f4e07f2273c0eb215cbe0d27e927b133fe82da2	biopatrec: a modular research platform for the control of artificial limbs based on pattern recognition algorithms	health research;uk clinical guidelines;biological patents;text;regulatory feedback networks;europe pubmed central;citation search;emg;computational biology bioinformatics;uk phd theses thesis;life sciences;pattern recognition;artificial limbs;prosthetic control;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics;myoelectric signals	Processing and pattern recognition of myoelectric signals have been at the core of prosthetic control research in the last decade. Although most studies agree on reporting the accuracy of predicting predefined movements, there is a significant amount of study-dependent variables that make high-resolution inter-study comparison practically impossible. As an effort to provide a common research platform for the development and evaluation of algorithms in prosthetic control, we introduce BioPatRec as open source software. BioPatRec allows a seamless implementation of a variety of algorithms in the fields of (1) Signal processing; (2) Feature selection and extraction; (3) Pattern recognition; and, (4) Real-time control. Furthermore, since the platform is highly modular and customizable, researchers from different fields can seamlessly benchmark their algorithms by applying them in prosthetic control, without necessarily knowing how to obtain and process bioelectric signals, or how to produce and evaluate physically meaningful outputs. BioPatRec is demonstrated in this study by the implementation of a relatively new pattern recognition algorithm, namely Regulatory Feedback Networks (RFN). RFN produced comparable results to those of more sophisticated classifiers such as Linear Discriminant Analysis and Multi-Layer Perceptron. BioPatRec is released with these 3 fundamentally different classifiers, as well as all the necessary routines for the myoelectric control of a virtual hand; from data acquisition to real-time evaluations. All the required instructions for use and development are provided in the online project hosting platform, which includes issue tracking and an extensive “wiki”. This transparent implementation aims to facilitate collaboration and speed up utilization. Moreover, BioPatRec provides a publicly available repository of myoelectric signals that allow algorithms benchmarking on common data sets. This is particularly useful for researchers lacking of data acquisition hardware, or with limited access to patients. BioPatRec has been made openly and freely available with the hope to accelerate, through the community contributions, the development of better algorithms that can potentially improve the patient’s quality of life. It is currently used in 3 different continents and by researchers of different disciplines, thus proving to be a useful tool for development and collaboration.	algorithm;benchmark (computing);data acquisition;evaluation;feature selection;global variable;image resolution;issue tracking system;limb prosthesis;limb structure;linear discriminant analysis;movement;open-source software;patients;pattern recognition;perceptron;real-time transcription;regulatory feedback network;seamless3d;shared web hosting service;signal processing;wiki	Max Ortiz-Catalan;Rickard Branemark;Bo Håkansson	2012		10.1186/1751-0473-8-11	medical research;computer science;bioinformatics;artificial intelligence;data mining	Vision	-1.5853907861375314	-68.91954350212352	62202
055c183c8bf3d8e9e5222e5e0c23d405d1cb14c0	biomedical relation extraction: from binary to complex	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Biomedical relation extraction aims to uncover high-quality relations from life science literature with high accuracy and efficiency. Early biomedical relation extraction tasks focused on capturing binary relations, such as protein-protein interactions, which are crucial for virtually every process in a living cell. Information about these interactions provides the foundations for new therapeutic approaches. In recent years, more interests have been shifted to the extraction of complex relations such as biomolecular events. While complex relations go beyond binary relations and involve more than two arguments, they might also take another relation as an argument. In the paper, we conduct a thorough survey on the research in biomedical relation extraction. We first present a general framework for biomedical relation extraction and then discuss the approaches proposed for binary and complex relation extraction with focus on the latter since it is a much more difficult task compared to binary relation extraction. Finally, we discuss challenges that we are facing with complex relation extraction and outline possible solutions and future directions.	biological science disciplines;bitwise operation;foundations;relationship extraction;protein protein interaction	Deyu Zhou;Dayou Zhong;Yulan He	2014		10.1155/2014/298473	psychology;biology;medical research;medicine;computer science;data science;data mining;operations research;algorithm	NLP	-2.0323434861799243	-66.16227642764807	62363
3b715ddfe40b723fee8bcfbce2e77b118e0dc282	a layered behavioural model of platelets	medical robotics;central feature;communicating sequential processes;artificial platelet;nanotechnology;modelling technique;layered behavioural model;great interest;nanorobot;case study;biology computing;nanomedicine application;safety case;complex network;clotting behaviour;csp	There is great interest in the application of nanotechnology to medicine, but concerns for safety are paramount. We present a modelling technique based on CSP and B as a starting point for simulation of networks of nano-robots. The model and the simulations are central features of our proposed approach to the construction of safety cases for nanomedicine applications, and complex networks of cooperating components in general. Our work is based on a case study: the clotting behaviour of (artificial) platelets. We present a model, and discuss its analysis and uses	complex network;gnu nano;nanorobotics;robot;simulation	Steve A. Schneider;Helen Treharne;Ana Cavalcanti;Jim Woodcock	2006	11th IEEE International Conference on Engineering of Complex Computer Systems (ICECCS'06)	10.1109/ICECCS.2006.80	computer science;communicating sequential processes;biological engineering;programming language	Robotics	4.407262881829074	-68.89547950772815	62524
56445b60db6e00e32fe4f7615431507947ddeef7	machine learning for integrating data in biology and medicine: principles, practice, and opportunities	computational biology;heterogeneous data;machine learning;personalized medicine;systems biology	New technologies have enabled the investigation of biology and human health at an unprecedented scale and in multiple dimensions. These dimensions include myriad properties describing genome, epigenome, transcriptome, microbiome, phenotype, and lifestyle. No single data type, however, can capture the complexity of all the factors relevant to understanding a phenomenon such as a disease. Integrative methods that combine data from multiple technologies have thus emerged as critical statistical and computational approaches. The key challenge in developing such approaches is the identification of effective models to provide a comprehensive and relevant systems view. An ideal method can answer a biological or medical question, identifying important features and predicting outcomes, by harnessing heterogeneous data across several dimensions of biological variation. In this Review, we describe the principles of data integration and discuss current methods and available implementations. We provide examples of successful data integration in biology and medicine. Finally, we discuss current challenges in biomedical integrative methods and our perspective on the future development of the field.		Marinka Zitnik;Francis Nguyen;Bo Wang;Jure Leskovec;Anna Goldenberg;Michael M. Hoffman	2019	An international journal on information fusion	10.1016/j.inffus.2018.09.012	implementation;data integration;data type;machine learning;mathematics;artificial intelligence;multiple time dimensions;emerging technologies;epigenome;biology	Comp.	-1.6057738720125816	-66.41049584398745	62540
512a17e619a24ca439636213831f7fe78f847229	resistance evolution in hiv — modeling when to intervene		The treatment of HIV is complicated by the evolution of antiviral drug resistant virus and the limited availability of antigenically independent antiviral regimens. The consequences to the patient of successive virological failures is such that many strategies to minimize the occurrence of such failures are being investigated. In this paper, a Markov chain-based model of virological failure is introduced. This model considers sequential failure events, and differentiates between several modes of virological failure. This model is then used to evaluate the resistance- targeted interventions by means of testing the impact of a viral load preconditioning strategy on total treatment regimen longevity in HIV patients. It is shown that a proposed intervention targeting pre-existing resistance has the potential to increase the expected time to three sequential virological failures by an average of 3.3 years per patient. When combined with an intervention targeting patient compliance, the total potential increase in the time to three sequential virological failures is as high as 11.2 years. The impact on patient and public health is discussed.	antiviral agents;average-case complexity;compliance behavior;hiv infections;heart failure;limited availability;liver failure, acute;markov chain;patients;preconditioner;virus diseases;cellular targeting	Liliana Mabel Peinado Cortes;Ryan Zurakowski	2012	2012 American Control Conference (ACC)		failure analysis;immune system;strain;markov process;microorganism;health care	EDA	8.222330057044125	-68.92959452242869	62856
757e4cb981e807d83539d9982ad325331cb59b16	demographics versus biometric automatic interoperability		Humans are naturally experts in recognizing faces. Such skills are enforced through a mix of cultural and cognitive processes. This allows human vision system to be especially efficient and effective in processing faces in a familiar environment. Automatic recognition system are currently not able (will ever be?) to achieve similar performance, especially when cross-demographic features are involved (gender, ethnicity, and age). Recent studies suggest a significant decrease of the number of recognition errors by limiting the search space to faces with the same demographics. This can be obtained by preliminarily annotating faces with a demographic profile, or by using demographic features as soft biometrics to be determined as a support to actual recognition. Especially in the second case, a multi-demographics dataset is needed to appropriately train a recognition system, and/or to test its performance. In this paper we use EGA dataset to test how interoperability relationships between biometric and demographics can be exploited for better recognition, though avoiding human intervention to preventively select appropriate demographic parameters.	computer performance;downstream (software development);enhanced graphics adapter;face (geometry);humans;interoperability;norm (social);rapid refresh;round-robin scheduling;soft biometrics;statistical classification	Maria De Marsico;Michele Nappi;Daniel Riccio;Harry Wechsler	2013		10.1007/978-3-642-41181-6_48	database;world wide web;computer security	AI	-3.3671069811151195	-77.29492314311824	63036
08e9f336683bcd69f53e1da347217554b4a09555	retromine, or how to provide in-depth retrospective studies from medline in a glance: the hepcidin use-case	animals;female;retrospective studies;medline;male;data mining;hepcidins;humans	The rapid expansion of biomedical literature has provoked an increased development of advanced text mining tools to rapidly extract relevant events from the continuously increasing amount of knowledge published periodically in PubMed. However, bioinvestigators are still reluctant to use these tools for two reasons: i) a large volume of events is often extracted upon a query, and this volume is hard to manage, and ii) background events dominate search results and overshadow more pertinent published information, especially for domain experts. In this paper, we propose an approach that incorporates the temporal dimension of published events to the process of information extraction to improve data selection and prioritize more pertinent periodically published knowledge for scientists. Indeed, instead of providing the total knowledge associated with a PubMed query, which is usually a mix of trivial background information and non-background information, we propose a method that incorporates time and selects non background and highly relevant biological entities and events published over time for bioinvestigators. Before excluding background events from the total knowledge extracted, a quantification of their amount is also provided. This work is illustrated by a case study regarding Hepcidin gene publications over a decade, a duration that is sufficiently long enough to generate alternative views on the overall data extracted.		Bertrand Ameline de Cadeville;Olivier Loréal;Fouzia Moussouni-Marzolf	2015	Journal of integrative bioinformatics	10.2390/biecoll-jib-2015-275	bioinformatics;data science;retrospective cohort study;data mining;database	NLP	-3.5864285818613646	-66.44800196435102	63273
bf200cb2d160a142ca68af38314f78ba477217f1	editorial message: special track on bioinformatics	microarray data;daml;analytic hierarchy process;decision support;owl;outsourcing;policy;multi criteria decision making;chemical process;mass spectrometry;distilled statecharts;biological system modeling;heterogeneous databases;signal transduction pathway;karmen;agent;microarray analysis;multi agent systems;kaos;monitoring;life sciences;notification;biological systems;fuzzy decision making;computer analysis;metabolic pathway;high throughput;flexfeed;workflow patterns;ontology	Advances in bioinformatics are providing the foundations for the convergence of agribusiness, healthcare, pharmaceuticals and computing into what promises to be the largest industry in the world, the life sciences industry. A large part of the information to support biology research is available on large number of heterogeneous databases in both structured and unstructured formats. The challenge is to obtain information and knowledge from these databases using innovative computational approaches to support and promote biomedical research. One example of such a computational challenge is in identifying biological pathways using data, information, and knowledge scattered over heterogeneous databases. Computational tools using system-theoretic approaches are needed to model metabolic pathways, signal-transduction pathways, genetic regulatory circuits and biological systems modeling. By comparing the genomes and pathways of several species at a high level, we hope to understand how stable biological systems have evolved. Over the last few years, microarray data have provided many insights into the transcriptome and into cellular function. These data are now increasingly complemented by mass spectrometry data of the proteome, whose analysis poses new computational challenges.	bioinformatics;biological system;computation;database;high-level programming language;microarray;systems modeling;theory;transduction (machine learning)	Mathew J. Palakal;Sven Rahmann;Birong Liao	2004		10.1145/1066677.1066705	microarray analysis techniques;mass spectrometry;computer science;bioinformatics;artificial intelligence;data science;machine learning;ontology;data mining;database;world wide web;computer security	Comp.	3.7065499106350512	-67.09940295659777	63435
1479e9ea49b896c1da06bb0e66f297f41558bff5	xkin: an open source framework for hand pose and gesture recognition using kinect	human computer interaction;xkin;hand pose;kinect;gesture recognition;open source	This work targets real-time recognition of both static hand-poses and dynamic hand-gestures in a unified open-source framework. The developed solution enables natural and intuitive hand-pose recognition of American Sign Language (ASL), extending the recognition to ambiguous letters not challenged by previous work. While hand-pose recognition exploits techniques working on depth information using texture-based descriptors, gesture recognition evaluates hand trajectories in the depth stream using angular features and hidden Markov models (HMM). Although classifiers come already trained on ASL alphabet and 16 uni-stroke dynamic gestures, users are able to extend these default sets by adding their personalized poses and gestures. The accuracy and robustness of the recognition system have been evaluated using a publicly available database and across many users. The XKin open project is available online (Pedersoli, XKin libraries. https://github.com/fpeder/XKin , 2013) under FreeBSD License for researchers in human–machine interaction.	angularjs;application programming interface;database;freebsd;gesture recognition;hidden markov model;human–computer interaction;information theory;kinect;library (computing);markov chain;open-source software;personalization;programming paradigm;real-time clock;real-time locating system;robustness (computer science);user interface	Fabrizio Pedersoli;Sergio Benini;Nicola Adami;Riccardo Leonardi	2014	The Visual Computer	10.1007/s00371-014-0921-x	computer vision;speech recognition;computer science;gesture recognition;natural user interface;sketch recognition	Vision	-3.2125194880422487	-74.25566180114551	63829
5880dbe2157fd5ae1429768abdbdc926675adb85	artificial neural network for herbal ingredient discoveries	traditional chinese medicine;neural nets;formalism;information technology;training;real clinical environments;ann;set theory;backpropagation;trusted herbal ingredient discoveries;formalism ann relevance index training herbal ingredient discoveries;medical expert systems;indexes;artificial neural networks;artificial neural networks training neurons indexes backpropagation ontologies context;indexation;relevance index;medicine;ontologies;herbal ingredient discoveries;neurons;tcm;set theory backpropagation medical expert systems medicine neural nets;parallel execution;context;traditional chinese medicine artificial neural network backpropagation trusted herbal ingredient discoveries parallel execution tcm real clinical environments relevance index;artificial neural network	A novel approach, which is based on artificial neural network (ANN) by backpropagation, for fast and trusted herbal ingredient discoveries, is proposed. It is fast, because different ANN modules can be executed in parallel, and the ANN results are trustworthy, because they can be verified by TCM domain experts in real clinical environments. The ANN is able to learn the relationship between herbal ingredients and the set of information given (e.g. symptoms and illnesses). The ANN output is called the relevance index (RI), which conceptually associates two TCM entities (e.g. U and V) in a 2-D or 3-D manner (D for dimension). RI is the quantified P(U∩V) part of P(U ∪ V) = P(U) + P(V) − P(U ∩ V), an IT (information technology) formalism in which P stands for probability. The interpretation of P(U ∩ V) adheres to TCM formalism(s).	artificial neural network;backpropagation;entity;formal system;rs-232;relevance;toolkit for conceptual modeling	Jackei H. K. Wong;Wilfred W. K. Lin;Allan K. Y. Wong;Tharam S. Dillon	2010	2010 IEEE 23rd International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2010.6042643	database index;computer science;ontology;artificial intelligence;backpropagation;machine learning;data mining;formalism;artificial neural network;set theory	Arch	0.9623356591553983	-77.0174972495059	64193
b9ff49b9bcfadf08919d982c3b75c4f8a00a33e0	disease outbreak detection through clique covering on a weighted icpc-coded graph	syndromic surveillance;algorithms;decision support;population surveillance;. epidemiological research;incidence rate;disease outbreak	Even after a decade of increased research into the problem of detecting disease outbreaks, we lack a system that can limit the number of patients affected by a potential epidemic by recognising its existence at an early stage. In this paper we suggest the use of a weighted graph representing symptoms with an exceptionally high prevalence. Cliques with high weighted edges in such a graph will represent groups of symptoms that occur together more often than usual. As a result each clique will represent the main symptoms of a disease with a high incidence rate. This will make it easier to diagnose the nature of an outbreak, to reach the affected patients at an early stage and to distinguish between outbreaks occurring simultaneously.	acm international collegiate programming contest;approximation algorithm;clique (graph theory);graph (discrete mathematics);graph - visual representation;incidence matrix;limited stage (cancer stage);maximal set;paget's disease, mammary;patients;sensor	Klaske van Vuurden;Gunnar Hartvigsen;Johan Gustav Bellika	2008	Studies in health technology and informatics	10.3233/978-1-58603-864-9-271	data mining;microprocessor;clique;sink (computing);alarm;leakage (electronics);surface runoff;graph;medicine;control system	AI	9.757904143781502	-77.31304463761633	64380
01acccc6bbaadba0cea2a2f2a0552184c28ee93b	removing confounding factors associated weights in deep neural networks improves the prediction accuracy for healthcare applications		The proliferation of healthcare data has brought the opportunities of applying data-driven approaches, such as machine learning methods, to assist diagnosis. Recently, many deep learning methods have been shown with impressive successes in predicting disease status with raw input data. However, the “black-box” nature of deep learning and the highreliability requirement of biomedical applications have created new challenges regarding the existence of confounding factors. In this paper, with a brief argument that inappropriate handling of confounding factors will lead to models’ sub-optimal performance in real-world applications, we present an efficient method that can remove the influences of confounding factors such as age or gender to improve the across-cohort prediction accuracy of neural networks. One distinct advantage of our method is that it only requires minimal changes of the baseline model’s architecture so that it can be plugged into most of the existing neural networks. We conduct experiments across CT-scan, MRA, and EEG brain wave with convolutional neural networks and LSTM to verify the efficiency of our method.		Haohan Wang;Zhenglin Wu;Eric P. Xing	2019			convolutional neural network;architecture;deep learning;artificial neural network;machine learning;confounding;biology;artificial intelligence;bioinformatics	ML	6.120010395898526	-78.717194116331	64448
152a839ce7011f9bddcf8c5e2e0f844d6f64a8f0	knowledge discovery using medical data mining	continuous time;reference model;time series;data mining;sequential pattern;knowledge discovery	In this paper we describe the process of discovering underlying knowledge in a set of isokinetic tests (continuous time series) using data mining techniques. The methods used are based on the discovery of sequential patterns in time series and the search for similarities and differences among exercises. They were applied to the processed information in order to characterise injuries and discover reference models specific to populations. The discovered knowledge was evaluated against the expertise of a physician specialised in isokinetic techniques and applied in the I4 project (Intelligent Interpretation of Isokinetic Information).	data mining	Fernando Alonso;África López-Illescas;Loïc Martínez;César Montes;Juan Pedro Caraça-Valente	2002		10.1007/3-540-36104-9_1	computer science;data science;machine learning;data mining	ML	3.2617077448230347	-76.32936505504733	64475
ed68eb78ec13faf5538cf919951994e5488bea19	a digital communication analysis of gene expression of proteins in biological systems: a layered network model view	digital communication;gene expression;protein;biological communication;layered network model;medical applications	Biological communication is a core component of biological systems, mainly presented in the form of evolution, transmitting information from a generation to the next. Unfortunately, biological systems also include other components and functionalities that would cause unwanted information processing and/or communication problems that manifest as diseases. On the other hand, general communication systems, e.g. digital communications, have been well developed and analysed to yield accuracy, high performance, and efficiency. Therefore, we extend the theories of digital communication systems to analyse biological communications. However, in order to accurately model biological communication as digital ones, an analysis of the analogies between both systems is essential. In this work, we propose a novel stacked-layer network model that presents gene expression (i.e. the process by which the information carried by deoxyribonucleic acid or DNA is transformed into the appropriate proteins) and the role of the Golgi apparatus in transmitting these proteins to a target organ. This is analogous to the transmit process in digital communications where a transmitting device in some network would send digital information to a destination/receiver device in another network through a router. The proposed stacked-layer network model exploits key networks’ theories and applies them into the broad field genomic analysis, which in turn can impact our understanding and use of medical methods. For example, it would be useful in detecting a target site (e.g. tumour cells) for drug therapy, improving the targeting accuracy (addressing), and reducing side effects in patients from health and socio-economic perspectives. Besides improving our understanding of biological communication systems, the proposed model unleashes the true duality between digital and biological communication systems. Therefore, it could be deployed into leveraging the advantages and efficiencies of biological systems into digital communication systems as well and to further develop efficient models that would overcome the disadvantages of either system.	biological system;digital data;digital photography;information processing;network model;router (computing);sensor;theory;transmitter	Yesenia Cevallos-Villacrés;Lorena Molina;Alex Santillán;Floriano De Rango;Ahmad Rushdi;Jesús B. Alonso	2016	Cognitive Computation	10.1007/s12559-016-9434-4	computer science;bioinformatics;theoretical computer science;communication	HPC	3.590473131101425	-69.03153248479927	64536
7d262b2119df2ca04469b2933170c0f218735127	non-invasive detection of coronary artery disease in high-risk patients based on the stenosis prediction of separate coronary arteries	coronary artery disease;feature selection;naive bayes and c4.5 classifiers;support vector machine	BACKGROUND AND OBJECTIVE Cardiovascular diseases are an extremely widespread sickness and account for 17 million deaths in the world per annum. Coronary artery disease (CAD) is one of such diseases with an annual mortality rate of about 7 million. Thus, early diagnosis of CAD is of vital importance. Angiography is currently the modality of choice for the detection of CAD. However, its complications and costs have prompted researchers to seek alternative methods via machine learning algorithms.   METHODS The present study proposes a novel machine learning algorithm. The proposed algorithm uses three classifiers for detection of the stenosis of three coronary arteries, i.e., left anterior descending (LAD), left circumflex (LCX) and right coronary artery (RCA) to get higher accuracy for CAD diagnosis.   RESULTS This method was applied on the extension of Z-Alizadeh Sani dataset which contains demographic, examination, ECG, and laboratory and echo data of 500 patients. This method achieves an accuracy, sensitivity and specificity rates of 96.40%, 100% and 88.1%, respectively for the detection of CAD. To our knowledge, such high rates of accuracy and sensitivity have not been attained elsewhere before.   CONCLUSION This new algorithm reliably distinguishes those with normal coronary arteries from those with CAD which may obviate the need for angiography in the normal group.	anterior descending branch of left coronary artery;arterial system;arteries;arteriopathic disease;cardiovascular diseases;cessation of life;computer-aided design;coronary artery disease;illness (finding);least absolute deviations;machine learning;modality (human–computer interaction);patients;right coronary artery structure;sensitivity and specificity;silo (dataset);stenosis;structure of circumflex branch of left coronary artery;algorithm;angiogram	Roohallah Alizadehsani;Mohammad Javad Hosseini;Abbas Khosravi;Fahime Khozeimeh;Mohamad Roshanzamir;Nizal Sarrafzadegan;Saeid Nahavandi	2018	Computer methods and programs in biomedicine	10.1016/j.cmpb.2018.05.009	coronary arteries;stenosis;intravascular ultrasound;coronary artery disease;statistics;angiography;computer science;cad;circumflex;right coronary artery;cardiology;internal medicine	AI	8.669333466595596	-77.89159245266663	64573
b189a12d0690e4fa02d3fed3e33ebf483f3c3e7d	molecular communication and signaling in human cells	molecular networks;human disease;signaling error;systems biology;signal transduction;decision support systems cells biology communication channels error probability biomembranes biological system modeling;cell signaling errors molecular communication human cells signaling networks cell membrane biochemical interactions communication channels transmission error probability;transmission error probability;capacity;communication channels;wireless channels cellular biophysics molecular biophysics molecular communication telecommunication telecommunication signalling;capacity systems biology signal transduction molecular networks human disease signaling error communication channels transmission error probability	Signaling networks in human cells convey signals from the cell membrane to specific target molecules via biochemical interactions, to control a variety of cellular functions. We have modeled signaling networks as communication channels where molecules communicate with each other to transfer signals. We have defined and computed the fundamental parameters of transmission error probability and signaling capacity in signaling networks. This systematic approach can be used to understand how cell signaling errors and malfunctioning molecules may contribute to the development of complex human disorders with unknown molecular bases.	cell signaling;interaction	Iman Habibi;Ali Abdi;Effat S. Emamian	2015	2015 49th Asilomar Conference on Signals, Systems and Computers	10.1109/ACSSC.2015.7421097	biology;cell biology;telecommunications;communication;channel-associated signaling	ML	3.686301527665931	-68.97148995557235	65205
f6c1e303cbdebbe6aab22b3936f0ef75db69d9ae	what we found on our way to building a classifier: a critical analysis of the aha screening questionnaire		The American Heart Association has recommended a 12-element questionnaire for pre-participation screening of athletes (before they take on athletic activities), in order to reduce and hopefully prevent sudden cardiac death in young athletes. This screening procedure is widely used throughout the United States, but its efficacy for discriminating Normal from Nonnormal heart condition is not clear. As part of a larger study on cardiovascular disorders in young athletes, we set out aiming to pursue a classification task: namely, training a machine-learning-based classifier to automatically categorize athletes into risk-levels based on their respective answers to the AHA questionnaire. We also conducted information-based analysis of each question to identify the ones that may best predict the athletes’ heart condition and thus enhance the classification performance. However, surprisingly, rather than obtaining a classifier, the classification results, the information contents of the questions, as well as further probabilistic analysis, all indicate that the AHA-recommended 12 elements screening procedure does not effectively distinguish between Normal and Non-normal heart as identified by cardiologists using Electroand Echo-cardiogram examinations. Our results suggest that ECG and (possibly Echo) rather than the questionnaire should be considered for screening young athletes.	categorization;echo;ground truth;informatics;information;linear classifier;machine learning;probabilistic analysis of algorithms;self-information	Quazi Abidur Rahman;Sivajothi Kanagalingam;Aurelio Pinheiro;Theodore Abraham;Hagit Shatkay	2013		10.1007/978-3-319-02753-1_23	medicine;pathology;physical therapy;pediatrics	Web+IR	8.326025508009245	-78.44632294747764	65372
814274cd8314c3c17c05ed3ecf8cdc51cef17ba6	frequent pathological human mutations: a survey		We analyzed some mutations from dbSNP database which were known to be pathological, but are relatively frequent in humans with the intention of testing PolyPhen-2's efficiency on particularly difficult cases. We proved that the majority of these mutations could benefit from a therapeutic approach with pharmacological chaperones. Exon-sequencing will soon become a common practice in disease diagnoses. All the steps that follow the acquisition of the sequence must be optimized if we wish to find one or more mutations associated to the pathological phenotype. We believe the diagnosis will expand from only working on early onset and severe phenotype diseases to working on late onset and mild phenotype diseases too. Therefore the cases of research on mutations that cause little damage to the mutant protein will get more and more frequent. One of the most common programs used to distinguish pathological mutations is PolyPhen-2[1]. PolyPhen-2 algorithm classifies variations using eight sequence-based and three structure-based predictive features. We analyzed cases that may be found in clinical practice with a relative frequency (minor allele frequency MAF>0). We found that in half of the cases PolyPhen-2 erroneously classifies these mutations as benign or neutral. The percentage of false negatives does not decrease when the 3D structure of the protein is known and PolyPhen-2 can also use three structure-based predictive features. The prediction becomes more precise when	algorithm;disease gene identification;onset (audio);dbsnp	Marco Cammisa;Antonella Correra;Giuseppina Andreotti;Maria Vittoria Cubellis	2013			pathology;biology	Comp.	8.097960478380328	-70.89217372713209	65499
08d090a05649f9d6a976b8e6be3f067140f117b6	risk prediction for chronic kidney disease progression using heterogeneous electronic health record data and time series analysis	electronic health records;risk prediction;survival analysis;topic modeling	BACKGROUND As adoption of electronic health records continues to increase, there is an opportunity to incorporate clinical documentation as well as laboratory values and demographics into risk prediction modeling.   OBJECTIVE The authors develop a risk prediction model for chronic kidney disease (CKD) progression from stage III to stage IV that includes longitudinal data and features drawn from clinical documentation.   METHODS The study cohort consisted of 2908 primary-care clinic patients who had at least three visits prior to January 1, 2013 and developed CKD stage III during their documented history. Development and validation cohorts were randomly selected from this cohort and the study datasets included longitudinal inpatient and outpatient data from these populations. Time series analysis (Kalman filter) and survival analysis (Cox proportional hazards) were combined to produce a range of risk models. These models were evaluated using concordance, a discriminatory statistic.   RESULTS A risk model incorporating longitudinal data on clinical documentation and laboratory test results (concordance 0.849) predicts progression from state III CKD to stage IV CKD more accurately when compared to a similar model without laboratory test results (concordance 0.733, P<.001), a model that only considers the most recent laboratory test results (concordance 0.819, P < .031) and a model based on estimated glomerular filtration rate (concordance 0.779, P < .001).   CONCLUSIONS A risk prediction model that takes longitudinal laboratory test results and clinical documentation into consideration can predict CKD progression from stage III to stage IV more accurately than three models that do not take all of these variables into consideration.	andrey ershov;chronic kidney diseases;color gradient;concordance (publishing);demography;documentation;electronic health records;experiment;financial risk modeling;gene prediction;genetic heterogeneity;glomerular filtration rate;interpretation (logic);kalman filter;laboratory procedures;manuscripts;patients;population;progressive disease;randomness;selection algorithm;survival analysis;time series analysis;treatment - filtration;stage iv childhood hodgkin's lymphoma	Adler J. Perotte;Rajesh Ranganath;Jamie S. Hirsch;David M. Blei;Noémie Elhadad	2015		10.1093/jamia/ocv024	econometrics;medicine;data mining;statistics	ML	6.8237270485590065	-74.62310183162063	65738
779dbefa7c6832c83757610209b7737afbdcf801	aid decision algorithms to estimate the risk in congenital heart surgery	congenital heart disease;info eu repo semantics article;data analysis;artificial neural networks;classifiers;decision trees	BACKGROUND AND OBJECTIVE In this paper, we have tested the suitability of using different artificial intelligence-based algorithms for decision support when classifying the risk of congenital heart surgery. In this sense, classification of those surgical risks provides enormous benefits as the a priori estimation of surgical outcomes depending on either the type of disease or the type of repair, and other elements that influence the final result. This preventive estimation may help to avoid future complications, or even death.   METHODS We have evaluated four machine learning algorithms to achieve our objective: multilayer perceptron, self-organizing map, radial basis function networks and decision trees. The architectures implemented have the aim of classifying among three types of surgical risk: low complexity, medium complexity and high complexity.   RESULTS Accuracy outcomes achieved range between 80% and 99%, being the multilayer perceptron method the one that offered a higher hit ratio.   CONCLUSIONS According to the results, it is feasible to develop a clinical decision support system using the evaluated algorithms. Such system would help cardiology specialists, paediatricians and surgeons to forecast the level of risk related to a congenital heart disease surgery.	architecture as topic;artificial intelligence;cns disorder;cardiac surgery procedures;cardiology discipline;cessation of life;classification;clinical decision support system;congenital heart disease;decision trees;decision tree;heart diseases;heart failure;hit (internet);machine learning;multilayer perceptron;ninety nine;objective-c;organizing (structure);projections and predictions;radial (radio);radial basis function;self-organization;self-organizing map;thrombocytopenia;trees (plant);algorithm;benefit	Daniel Ruiz Fernández;Ana Monsalve-Torra;Antonio Soriano Payá;Oscar Marín-Alonso;Eddy Triana Palencia	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2015.12.021	computer science;artificial intelligence;machine learning;decision tree;data mining;data analysis;artificial neural network	AI	6.682611440199669	-76.968519666546	65981
005d5cab78ad46d5e73623c953453f39534e64f6	drug-drug interactions detection from online heterogeneous healthcare networks	databases;drugs;drug drug interactions;data mining;logistic regression;media;social networking online health care medical computing regression analysis;drugs media feature extraction databases diseases data mining;feature extraction;logistic regression drug drug interactions healthcare social media heterogeneous healthcare network heterogeneous network mining;healthcare social media;diseases;heterogeneous network mining;logistic regression drug drug interaction detection online heterogeneous health care networks ddi detection drug safety problem health consumers data sources online health care social media heterogeneous topological features heterogeneous healthcare network;heterogeneous healthcare network	Drug-drug interactions (DDIs) are a serious drug safety problem for health consumers and how to detect such interactions effectively and efficiently has been of great medical significance. Currently, methods proposed to detect DDIs are mainly based on data sources such as clinical trial data, spontaneous reporting systems, electronic medical records, and chemical/pharmacological databases. However, those data sources are limited either by cohort biases, low reporting ratio, or access issue. In this study, we propose to use online healthcare social media, an informative and publicly available data source, to detect DDI signals. We construct a heterogeneous healthcare network based on consumer contributed contents, develop heterogeneous topological features, and use logistic regression as prediction model for DDI detection. The experiment results show that the proposed heterogeneous topological features substantially outperform the homogenous ones in the training set but only slightly outperform the homogeneous ones in the testing set, and interesting heterogeneous paths with strong predictive power are discovered.	database;information;interaction;logistic regression;social media;spontaneous order;test set	Haodong Yang;Christopher C. Yang	2014	2014 IEEE International Conference on Healthcare Informatics	10.1109/ICHI.2014.9	medicine;data science;data mining;internet privacy	DB	-3.6344269442239523	-69.58960226991198	66558
a7a78971d9562c2db694ece7e0ca468245c1ff34	investigation of diabetic microvascular complications using data mining techniques	sensitivity and specificity;artificial neural networks joints conferences;biology computing;physiological data;c5 0;eye;physiology biology computing data mining diseases eye neural nets;neural networks;data mining techniques;neural nets;nephropathy;diabetic neuropathy;diabetes;blood pressure;joints;neuropathy;diabetes mellitus;data mining;diabetic microvascular complications;artificial neural networks;creatinine diabetic microvascular complications data mining techniques physiological data diabetic retinopathy diabetic nephropathy diabetic neuropathy c5 0 neural network;physiology;family history;neural network diabetes mellitus retinopathy nephropathy neuropathy c5 0;retinopathy;diabetic retinopathy;diabetic foot;diseases;diabetic nephropathy;creatinine;conferences;neural network	This study theoretically analyzes and numerically explores the relationship between the physiological data and three diabetic microvascular complications: diabetic retinopathy, diabetic nephropathy, and diabetic neuropathy (foot problem). Method: The analysis results of 8,736 diabetic patients in northern Taiwan by using two data mining models: C5.0 and neural network were presented and compared. Results: It is found that Creatinine is the most important predictor for diabetic retinopathy. If Creatinine is out of control, diabetic patients will easily suffer from diabetic retinopathy in spite of many other laboratory evaluations are normal. The sensitivity and specificity for diabetic retinopathy prediction using C5.0 are 58.62 and 74.73, and those using neural network are 59.48 and 99.86, respectively. In addition, diabetic nephropathy will happen when several laboratory evaluation values are worse than target values. Female diabetics with diabetic family history are easier to undergo this complication. The sensitivity and specificity for diabetic nephropathy prediction using C5.0 are 69.44 and 81.36, and those using neural network are 74.44 and 98.55, respectively. For diabetic neuropathy, female diabetics feature unqualified BMI, HbAlc and AC sugar, while male diabetics mostly have uncontrolled blood pressure. Besides, smoking diabetics are more difficult to avoid this complication. The sensitivity and specificity for diabetic foot problem prediction using C5.0 are 64.71 and 83.48, and those using neural network are 67.63 and 99.70, respectively.	artificial neural network;brain–computer interface;c4.5 algorithm;data mining;kerrison predictor;numerical analysis;sensitivity and specificity;uncontrolled format string	Chien-Lung Chan;Yu-Chen Liu;Shih-Hui Luo	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4633893	computer science;blood pressure;machine learning;artificial neural network	Visualization	6.879132057741312	-78.0860188915323	66644
33da2cacd5653c2b40a4ec9d345894abdcc9ac66	a recommender system approach for predicting drug side effects		The accurate identification of drug side effects represents a major concern for public health. We propose a collaborative filtering model for large-scale prediction of drug side effects. Our approach provides side effects recommendations for drugs to safety professionals. The proposed latent factor model relies solely on the public drug-side effect relationships from safety data. Applied to 1,525 marketed drugs and 2,050 side effect terms, we achieved an AUPRC (area under the precision- recall curve) of 0.342 in a test set, with a sensitivity of 0.73 given a specificity of 0.95, providing state-of-the-art performance in side effect prediction. We analyze the performance of the method on drug-specific Anatomical Therapeutic and Chemical (ATC) category and side effect- specific medical category of disorders. Our findings suggest that latent factor models can be useful for the early and accurate detection of unknown adverse drug events.	advanced transportation controller;collaborative filtering;recommender system;sensitivity and specificity;side effect (computer science);test set	Diego Galeano;Alberto Paccanaro	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489025	collaborative filtering;drug;recommender system;factor analysis;machine learning;side effect;artificial intelligence;matrix decomposition;recall;test set;computer science	Web+IR	6.760207169138797	-74.3737086302459	66710
fa8275c04bd68868993635c0a44c72519884d5a8	inspecting the role of pi3k/akt signaling pathway in cancer development using an in silico modeling and simulation approach		PI3K/AKT signaling pathway plays a crucial role in the control of functions related to cancer biology, including cellular proliferation, survival, migration, angiogenesis and apoptosis; what makes this signaling pathway one of the main processes involved in cancer development. The analysis and prediction of the anticancer targets acting over the PI3K/AKT signaling pathway requires of a deep understanding of its signaling elements, the complex interactions that take place between them, as well as the global behaviors that arise as a result, that is, a systems biology approach. Following this methodology, in this work, we propose an in silico modeling and simulation approach of the PI3K class I and III signaling pathways, for exploring its effect over AKT and SGK proteins, its relationship with the deregulated growth control in cancer, its role in metastasis, as well as for identifying possible control points. The in silico approach provides symbolic abstractions and accurate algorithms that allow dealing with crucial aspects of the cellular signal transduction such as compartmentalization, topology and timing. Our results show that the activation or inhibition of target signaling elements in the overall signaling pathway can change the outcome of the cell, turning it into apoptosis or proliferation.	gene regulatory network;simulation	Pedro Pablo González Pérez;Maura Cárdenas-García	2018		10.1007/978-3-319-78723-7_7	computer vision;computational biology;artificial intelligence;engineering;cancer;signal transduction;protein kinase b;pi3k/akt/mtor pathway;angiogenesis;in silico;akt/pkb signaling pathway;systems biology	Logic	6.659701234991642	-67.17185581707469	67054
5a6d05f6c1108c6ff04bc160d47663675a5941c2	impact of precision of bayesian network parameters on accuracy of medical diagnostic systems	probability elicitation;medical diagnostic systems;sensitivity analysis;bayesian networks	OBJECTIVE One of the hardest technical tasks in employing Bayesian network models in practice is obtaining their numerical parameters. In the light of this difficulty, a pressing question, one that has immediate implications on the knowledge engineering effort, is whether precision of these parameters is important. In this paper, we address experimentally the question whether medical diagnostic systems based on Bayesian networks are sensitive to precision of their parameters.   METHODS AND MATERIALS The test networks include Hepar II, a sizeable Bayesian network model for diagnosis of liver disorders and six other medical diagnostic networks constructed from medical data sets available through the Irvine Machine Learning Repository. Assuming that the original model parameters are perfectly accurate, we lower systematically their precision by rounding them to progressively courser scales and check the impact of this rounding on the models' accuracy.   RESULTS Our main result, consistent across all tested networks, is that imprecision in numerical parameters has minimal impact on the diagnostic accuracy of models, as long as we avoid zeroes among parameters.   CONCLUSION The experiments' results provide evidence that as long as we avoid zeroes among model parameters, diagnostic accuracy of Bayesian network models does not suffer from decreased precision of their parameters.		Agnieszka Onisko;Marek J. Druzdzel	2013	Artificial intelligence in medicine	10.1016/j.artmed.2013.01.004	variable-order bayesian network;computer science;machine learning;bayesian network;data mining;sensitivity analysis;statistics	AI	5.789858037425539	-74.8283559588392	67076
83b4b89fa8ba41e91870acb7e564c6a348b5b1b9	a modular simulation model for assessing interventions for abdominal aortic aneurysms	patient treatment diseases;sociology statistics diseases analytical models adaptation models computational modeling aneurysm;population characteristics alterations modular simulation model structure intervention assessment patient treatment abdominal aortic aneurysms medical doctors health technology assessment hta experts hta modelers aaa disease treatment patient specific properties;diseases;patient treatment	This paper discusses the development of an individual based simulation model for evaluation of interventions for better treatment of patients with abdominal aortic aneurysms (AAA). The interdisciplinary subject required collaboration of medical doctors, Health Technology Assessment (HTA) experts and modelers. The here presented modular model structure is flexible enough to allow adaptation on screening research questions for similar diseases. Another focus of the work was integration of risk factors and how it determines our model choice, especially because steadily increasing knowledge about or improved treatment of AAA could cause necessity of reevaluation. Through inclusion of several patient specific properties the model does not only provide comparison of current state with screening but also elaboration of alterations of population characteristics and its consequences on AAA cases.	aaa (video game industry);agent-based model;computed tomography of the abdomen and pelvis;html application;risk factor (computing);simulation	Christoph Urach;Günther Zauner;Gottfried Endel;Ingrid Wilbacher;Felix Breitenecker	2013	2013 Winter Simulations Conference (WSC)	10.1109/WSC.2013.6721408	biological engineering	SE	5.288057819554574	-72.91556540441796	67262
f8e9745455c51f27ab2723c26bf13a1cbd7a7aaf	modeling spatial-temporal epidemics using stbl model	patient diagnosis;receiver operating characteristic curve;acute coronary syndrome;emergency room;neural networks medical diagnostic imaging pain myocardium biomedical informatics educational institutions blood arteries heart muscles;learning artificial intelligence;biomedical informatics;medical diagnostic computing;biomedical informatics neural networks acute coronary syndrome forecasting;neural network;patient diagnosis learning artificial intelligence medical diagnostic computing	The Space Time Bilinear (STBL) model is a special form of a multiple bilinear time series which can be used to model time series which exhibit bilinear behavior on a spatial neighborhood structure. The STBL model and its identification have been proposed and discussed by Dai and Billard (1998). In this paper, we compare the STBL model with STARMA and single ARMA model. All problems are addressed by setting up the model in state space form and applying the Kalman filter. An application of the STBL model to epidemic surveillance data is given and the results com pared with those from other models.	bilinear filtering;bilinear transform;kalman filter;state space;time series	Lynne Billard;Duck-Ki Kim;Chan Hee Lee;Sung Duck Lee;Keon-Myung Lee;Sung-Soo Kim	2007	Sixth International Conference on Machine Learning and Applications (ICMLA 2007)	10.1109/ICMLA.2007.112	health informatics;computer science;machine learning;receiver operating characteristic;artificial neural network	Robotics	8.145880934739083	-79.05283628476391	67796
4c188d58dae2d8ed321b559e15a08ddd749ea520	reduction of massive eeg datasets for epilepsy analysis using artificial neural networks		Epileptic seizure source identification involves neurologists combing through a substantial amount of data manually, which sometimes takes weeks per patient. This paper presents a methodology for minimizing the amount of data a neurologist has to analyze to identify the seizure focus. The method keeps the neurologist as the final decision maker and aids in the decision making process. It has to be noted that the primary focus of the work was not improving the accuracy of interictal spike detection but reduction of the volume of data. The presented methodology is based on Artificial Neural Networks (ANN) and is implemented on EEG data collected on 5 patients using a dense array EEG reader. As a baseline, a simple template matching was implemented on the same dataset. Experimental results showed that the ANN based methodology was able to reduce the dataset by 98%, a significant improvement on the template matching method.	artificial neural network;baseline (configuration management);electroencephalography;neural networks;template matching	Howard J. Carey;Kasun Amarasinghe;Milos Manic	2017	2017 10th International Conference on Human System Interactions (HSI)	10.1109/HSI.2017.8005015	epileptic seizure;template matching;ictal;epilepsy;artificial neural network;electroencephalography;machine learning;artificial intelligence;computer science	ML	5.257332504355303	-77.74250140084126	67891
d5877a70d3b42b9e32ab9526982637005c930bee	a decision tree based approach with sampling techniques to predict the survival status of poly-trauma patients	trauma patients;survival prediction;imbalanced classification problems;decision trees;sampling techniques		decision tree;sampling (signal processing)	José Antonio Sanz;Javier Fernández;Humberto Bustince;Carlos Gradin;Mariano Fortún;Tomás Belzunegui	2017	Int. J. Comput. Intell. Syst.	10.2991/ijcis.2017.10.1.30	sampling;computer science;machine learning;decision tree;pattern recognition;data mining	AI	6.720496661933734	-76.89635962884213	67951
4a29f6df03b35f60bba4135854d9842ef66a31a9	metabolite analysis in sepsis through conditional independence maps	statistics algebra biochemistry bioinformatics biological organs biological tissues cellular biophysics drugs genomics organic compounds patient care;electric shock metabolomics discharges electric biological systems mutual information hospitals plasmas;time 48 h icu discharge isoleucine valerylcarnitine time snapshots c5 hydroxybutyrylcarnitine c3 dc c4 oh algebraic statistics septic shock mortality rates patient treatment multiorganic failure intensive care antibiotics vaccines biological tissues biological organs lesions infection host conditional independence maps sepsis metabolite analysis	Sepsis is the response of the host to an infection that produces lesions in its own organs and tissues. Despite the great advances in modern medicine, including vaccines, antibiotics and intensive care, it is still the primary cause of death due to infection. Sepsis may result in shock, multi-organic failure and death unless there is a rapid identification of the infection and timely administration of treatment. Its mortality rates can reach up to 45.7% for septic shock, its most acute manifestation. In this paper we also present these conditional independence maps in the context of algebraic statistics. The results of this analysis over a small cohort of nine patients at three different times (ICU admission, 48h and ICU discharge) showed that there is a significant interaction between C3- DC / C4-OH (Hydroxybutyrylcarnitine) and C5 (Valerylcarnitin) for the three time snapshots. We also found a significant interaction between C3-DC / C4-OH (Hydroxybutyrylcarnitine) and C5 (Valerylcarnitine) and Isoleucine (Ile) at 48h and ICU discharge.	alyref gene;amino acids;body tissue;cessation of life;dependence;discharger;fatty acids;heart failure;international components for unicode;isoleucine;large;map;metabolomics;modulation;multiple organ failure;numerous;patients;plasma active;register machine;septic shock;septic equation;severe sepsis;vii;carnitine shuttle;hydroxybutyrylcarnitine;intensive care unit	Vicent J. Ribas;Eduardo Romay;Laura Brunelli;Roberta Pastorelli;Gemma Gomà;Ana Navas;Antonio Artigas;Ricard Ferrer	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319876	biology;intensive care medicine;pathology;bioinformatics	Visualization	8.134097014146217	-69.74107842118812	68127
1864e02e20350c0864ec53aaa84f43432fed37dc	bayesian classification applied to strain in arrythmogenic left-ventricle cardiomyopathy		Arrhythmogenic cardiomyopathy (AC) is a rare disease associated with ventricular arrhythmias and sudden cardiac death. While AC of the right ventricle has been more extensively studied, exclusive left-ventricle involvement needs to be better characterized. Myocardial strain, obtained by feature tracking, provide insight into its biomechanical behavior. To characterize it, multivariate classifiers can be applied. The sample consisted of 13 AC-LV and 13 non-carriers of the mutation. The feature tracking algorithm of Circle cvi42 was applied to the cardiac magnetic resonance of each patient. A Naïve Bayes classifier with a feature subset selection method was applied to the parameters of peak strain, strain rate, displacement and velocity. We obtained an accuracy of 90% in NB and we arrived to 93% for CFS-NB. The strain parameters selected by the FSS algorithm were three: longitudinal peak strain and peak systolic and diastolic velocities. In all the selected features, AC-LV patients had smaller values as controls. In conclusion, myocardial strain is affected in AC-LV patients. Naïve Bayes classifiers allow obtaining a good discriminating accuracy among groups.	algorithm;climate forecast system;discriminant;displacement mapping;feature selection;flying-spot scanner;linear algebra;logical volume management;machine learning;motion estimation;naive bayes classifier;radial (radio);resonance;velocity (software development)	Yolanda Vives-Gilabert;Begoña Igual;Santiago Jiménez-Serrano;Jorge Sanz;Raquel Cervigón Abad;Antonio Cebrián;Jose Manuel Santabárbara;José Millet-Roig;Esther Zorio;Francisco Castells	2017	2017 Computing in Cardiology (CinC)		naive bayes classifier;diastole;strain rate;cardiomyopathy;sudden cardiac death;ventricle;multivariate statistics;pattern recognition;mathematics;strain (chemistry);artificial intelligence	Vision	9.208132186627862	-78.5895395587722	68455
004b4dfd92110c69255f65ef3a5f7895a382b267	predicting the impact of an electronic health record on practice patterns using computational modeling and simulation	electronic health record;practice pattern;point-of-contact clinical documentation;training model;computational modeling;clinical performance outcome;community hospital;clinical practice guideline;computational model;current practice pattern;clinical outcome	The overall purpose of this research study is to discover and apply new knowledge regarding methods to predict the impact of an electronic health record (EHR) on clinical practice guidelines in complex systems such as hospitals. Specifically, the aims of this study are: 1) to build, simulate and validate the accuracy of a computational model representing the current practice patterns in a sample of patients diagnosed with heart failure (HF) and treated in a community hospital; and 2) using computational modeling and simulation, develop a method to predict the effects of best practice guidelines on practice patterns after implementation of an EHR.	best practice;complex systems;computation;computational model;computer simulation;electronic health records;heart failure;patients;practice guidelines as topic	Thomas R. Clancy;Connie White-Delaney;Alberto Maria Segre;Kathleen M. Carley;Andrew Kuziak;Hwanjo Yu	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium		clinical practice;best practice;medical record;medical emergency;modeling and simulation;population;nursing;medicine	Logic	4.981421398131617	-73.92950258740444	68659
2e2a87c7abc0ed5f5f708fb502a326c3384cab07	a simulation-based approach to modeling the uncertainty of two-substrate clinical enzyme measurement processes	monte carlo methods calibration decision making enzymes liver measurement uncertainty medical computing molecular biophysics;net measurement uncertainty simulation based approach uncertainty modeling two substrate clinical enzyme measurement processes clinical laboratory testing medical decision making process organ system function liver gastrointestinal tract physics based mathematical model alanine aminotransferase assay monte carlo method simulation model calibration process;uncertainty measurement uncertainty biochemistry mathematical model calibration optical variables measurement instruments	Results of clinical laboratory tests inform every stage of the medical decision-making process, and measurement of enzymes such as alanine aminotransferase provide vital information regarding the function of organ systems such as the liver and gastrointestinal tract. Estimates of measurement uncertainty quantify the quality of the measurement process, and therefore, methods to improve the quality of the measurement process require minimizing assay uncertainty. To accomplish this, we develop a physics-based mathematical model of the alanine aminotransferase assay, with uncertainty introduced into its parameters that represent variation in the measurement process, and then use the Monte Carlo method to quantify the uncertainty associated with the model of the measurement process. Furthermore, the simulation model is used to estimate the contribution of individual sources of uncertainty as well as that of uncertainty in the calibration process to the net measurement uncertainty.	biological system;experiment;mathematical model;monte carlo method;simulation;tract (literature)	Varun Ramamohan;James T. Abbott;Yuehwern Yih	2014	Proceedings of the Winter Simulation Conference 2014	10.1109/WSC.2014.7019988	econometrics;simulation;uncertainty analysis;computer science;sensitivity analysis;statistics	Robotics	9.95360624067515	-69.53730766220691	68705
1a31c729eed3957dc033cc23d3435af307484cba	computational simulation for identification, evaluation and relocation of pregnant women in a possible zika infection outbreak in south american cities	computational simulation;zika virus;biological systems;bioinformatics	We use computational simulation to identify, evaluate and relocate pregnant women in a possible scenario of attack of Zika virus (ZIKV) on Peri-urban zones of South American cities. The plausibility of this study is based on the following points: (i) existence of high population of pregnant women in Peri-urban areas of large cities, (ii) appropriate conditions for fast reproduction of Aedes aegypti mosquitoes along hills, and (iii) the potential association of the ZIKV infection to microcephaly in babies. By assuming an attack rate of 15 per 1000 habitants as well as rate of confirmation of 10 cases per day, and rapid intervention based on the displacements of pregnant women to cleaned areas, the simulation yields that at least 2±1 of 100 pregnant women might be infected by ZIKV. The simulations have emphasized the effect of applying rapid intervention expected to be done by the health specialists, that targets to identify as soon as possible all those women in situation of pregnancy few hours after the presence of Aedes is already confirmed.	attack rate;computation;computer simulation;monte carlo method;plausibility structure;relocation (computing)	Huber Nieto-Chaupis	2016	2016 35th International Conference of the Chilean Computer Science Society (SCCC)	10.1109/SCCC.2016.7836015	simulation;medicine;genealogy;surgery	EDA	7.2961725348622	-70.33319430474279	69351
27da5c0ad29d27a021d21d0574cb673527eb6f46	robustness surfaces of complex networks	health research;uk clinical guidelines;control de robustesa;biological patents;complex networks;article publisher version;europe pubmed central;citation search;ordinadors xarxes d avaluacio;robust control;info eu repo semantics article;telecommunication systems;uk phd theses thesis;telecomunicacio sistemes de;life sciences;phase transitions and critical phenomena;computer networks evaluation;computer science;applied mathematics;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	Despite the robustness of complex networks has been extensively studied in the last decade, there still lacks a unifying framework able to embrace all the proposed metrics. In the literature there are two open issues related to this gap: (a) how to dimension several metrics to allow their summation and (b) how to weight each of the metrics. In this work we propose a solution for the two aforementioned problems by defining the R*-value and introducing the concept of robustness surface (Ω). The rationale of our proposal is to make use of Principal Component Analysis (PCA). We firstly adjust to 1 the initial robustness of a network. Secondly, we find the most informative robustness metric under a specific failure scenario. Then, we repeat the process for several percentage of failures and different realizations of the failure process. Lastly, we join these values to form the robustness surface, which allows the visual assessment of network robustness variability. Results show that a network presents different robustness surfaces (i.e., dissimilar shapes) depending on the failure scenario and the set of metrics. In addition, the robustness surface allows the robustness of different networks to be compared.	complex network;design rationale;emoticon;information;principal component analysis;robustness (computer science);robustness of complex networks;spatial variability;summation (document)	Marc Manzano;Faryad Darabi Sahneh;Caterina M. Scoglio;Eusebi Calle;José-Luis Marzo	2014		10.1038/srep06133	robust control;medicine;computer science;bioinformatics;artificial intelligence;data mining;operations research;complex network;robustness	Vision	5.068684569311364	-70.03854168918397	69429
8c1e4d92981a633e018bc2bffef5434fc51c064b	a quantitative measure for discriminating between self and non-self antigens in immune response	cellular automata model;chronicity;lymphocyte proliferation;immune memory;immune system;tolerance;cellular automata;immune response	We present a new theory of how lymphocyte-antigen interaction is governed. We present 'chronicity', a quantitative record of previous lymphocyte-antigen interactions, which is used to regulate lymphocyte behavior. When the chronicity of a lymphocyte increases with the interaction and gets beyond the lower threshold, the lymphocyte can proliferate. Non-self antigens cause lymphocyte proliferation which destroys the antigen. However, self antigens are not destroyed. When the chronicity gets beyond the upper threshold, the lymphocytes get in the tolerance state ensuring non-destruction of self antigens. The discrimination between self and non-self results from the difference in the termination process between self and non-self antigens, caused by the difference in the frequency between interaction of lymphocyte with both antigens.	antigens;autoantigens;cdisc send chronicity terminology;interaction;lymphocyte proliferation	Yoshiki Kashimori;Yoshihiro Ochi;Takeshi Kambara	2010	Bio Systems	10.1016/j.biosystems.2010.03.008	cellular automaton;biology;immune system;immunology	HCI	8.25523621530824	-66.97332818279085	69569
915d7603593a044a72ddbc2f754412986c659ce3	mining classification rules for detecting medication order changes by using characteristic cpoe subsequences	characteristic order entry sub-sequences;order entry change;characteristic cpoe subsequence;medical doctor;mining classification rule;order entry;order change;daily order entry sequence;characteristic order entry subsequence;medication order change;order correction;classification rule;computer physician order entry	characteristic order entry sub-sequences;order entry change;characteristic cpoe subsequence;medical doctor;mining classification rule;order entry;order change;daily order entry sequence;characteristic order entry subsequence;medication order change;order correction;classification rule;computer physician order entry	sensor	Hidenao Abe;Shusaku Tsumoto	2011		10.1007/978-3-642-21916-0_9	bioinformatics;data mining;algorithm	Networks	3.52862315140346	-75.51876353518395	70038
2b7d3c017047253efe05e32df61f0fb54c07c3ae	clustering barotrauma patients in icu-a data mining based approach using ventilator variables	intensive medicine;data mining;clustering;barotrauma;similarity;correlation;plateau pressure	Abstract: Predicting barotrauma occurrence in intensive care patients is a difficult task. Data Mining modelling can contribute significantly to the identification of patients who will suffer barotrauma. This can be achieved by grouping patient data, considering a set of variables collected from ventilators directly related with barotrauma, and identifying similarities among them. For clustering have been considered k-means and k-medoids algortihms (Partitioning Around Medoids). The best model induced presented a Davies-Bouldin Index of 0.64. This model identifies the variables that have more similarity among the variables monitored by the ventilators and the occurrence of barotrauma.	cluster analysis;data mining;davies–bouldin index;international components for unicode;k-means clustering;k-medoids;medoid	Sérgio Oliveira;Filipe Portela;Manuel Filipe Santos;José Machado;António Abelha;Álvaro M. Silva;Fernando Rua	2015		10.1007/978-3-319-23485-4_13	simulation;similarity;computer science;cluster analysis;correlation	ML	4.289995510624484	-76.43583649564172	70260
1269f5862956bfef3932246ea9f622ccd548b182	automatic selection of landmarks for navigation guidance		Although current navigation services provide significant benefits to people’s mobility, the turn-by-turn instructions they provide are sometimes ineffective. These instructions require people to maintain a high level of attention and cognitive workload while performing distance or angle measurements on their own mental map. To overcome this problem, landmarks have been identified as playing a major role in turnby-turn instructions. This requires the availability of landmarks in navigation databases. Landmarks are commonly selected manually, which involves time-consuming and tedious tasks. Automatic selection of landmarks has recently gained the attention of researchers but currently there are only a few techniques that can select appropriate landmarks. In this article, we present a technique based on a neural network model, where both static and dynamic features are used for selecting landmarks automatically. To train and test this model, two labeling approaches, manual labeling and rule-based labeling, are also discussed. Experiments on the developed technique were conducted and the results show that rule-based labeling has a precision of approximately 90%, which makes the technique suitable and reliable for automatic selection of landmarks.	artificial neural network;database;experiment;high-level programming language;landmark point;logic programming;mental mapping;network model;turn-by-turn navigation	Rui Zhu;Hassan A. Karimi	2015	Trans. GIS	10.1111/tgis.12095	computer vision;simulation;computer science;artificial intelligence	HCI	-3.0031981204445	-72.46620979746426	70295
5f584de769ca8f2b12bbff9fa3da00f4a841cb4b	dynamical behavior of an epidemiological model with a demographic allee effect	extinction;threshold;allee effect;saddle node bifurcation	As the Allee effect refers to small density or population size, it cannot be deduced whether or not the Allee mechanisms responsible for an Allee effect at low population density or size will affect the dynamics of a population at high density or size as well. We show using susceptible–exposed–infectious (SEI) model that such mechanisms combined with disease pathogenicity have a detrimental impact on the dynamics of a population at high population level. In fact, the eventual outcome could be an inevitable population crash to extinction. The tipping point marking the unanticipated population collapse at high population level is mathematically associated with a saddle–node bifurcation. The essential mechanism of this scenario is the simultaneous population size depression and the increase of the extinction threshold owing to disease virulence and the Allee effect. Using numerical continuation software MatCont another saddle–node bifurcation is detected, which results in the re-emergence of two non-trivial equilibria since highly pathogenic species cause their own extinction but not that of their host. c ⃝ 2016 International Association for Mathematics and Computers in Simulation (IMACS). Published by Elsevier B.V. All rights reserved.	bifurcation theory;butterfly effect;crash (computing);dynamical system;emergence;item unique identification;numerical analysis;numerical continuation;simulation;software engineering institute	Salisu Usaini;Alun L. Lloyd;Roumen Anguelov;Salisu M. Garba	2017	Mathematics and Computers in Simulation	10.1016/j.matcom.2016.04.010	demography;simulation;extinction;mathematics;saddle-node bifurcation;allee effect	Metrics	8.518070403758433	-68.0439333422788	70354
eb7b43490a667e30aa95b374e9ed3607c1c43a69	machine learning-as-a-service and its application to medical informatics		Machine learning as an advanced computational technology has been around for several years in discovering patterns from diverse biomedical data sources and providing excellent capabilities ranging from gene annotation to predictive phenotyping. However, machine learning strategies remain underused in small and medium-scale biomedical research labs where they have been collaboratively providing a reasonable amount of scientific knowledge. While most machine learning algorithms are complicated in code, theses labs and individual researchers could accomplish iterative data analysis using different machine learning techniques if they had access to highly available machine learning components and powerful computational infrastructures. In this contribution, we provide a comparison of several state-of-the-art Machine Learning-as-a-Service platforms along with their capabilities in medical informatics. In addition, we performed several analyses to examine the qualitative and quantitative attributes of two Machine Learning-as-a-Service environments namely “BigML” and “Algorithmia”.	informatics;machine learning	Ahmad Pahlavan Tafti;Eric LaRose;Jonathan C. Badger;Ross Kleiman;Peggy L. Peissig	2017		10.1007/978-3-319-62416-7_15	computer science;machine learning;artificial intelligence;gene annotation;sociology of scientific knowledge;health informatics	NLP	-1.5665512402447086	-68.62248956929292	70462
625bccc16e4e5b8d1cb2874628d24c1526a201f5	deriving the expected utility of a predictive model when the utilities are uncertain	bayes theorem;decision support techniques;roc curve;models statistical;artificial intelligence;evaluation studies as topic;decision trees	Predictive models are often constructed from clinical databases with the goal of eventually helping make better clinical decisions. Evaluating models using decision theory is therefore natural. When constructing a model using statistical and machine learning methods, however, we are often uncertain about precisely how the model will be used. Thus, decision-independent measures of classification performance, such as the area under an ROC curve, are popular. As a complementary method of evaluation, we investigate techniques for deriving the expected utility of a model under uncertainty about the model's utilities. We demonstrate an example of the application of this approach to the evaluation of two models that diagnose coronary artery disease.	area under curve;arteriopathic disease;coronary artery disease;database;decision making;decision problem;decision theory;expected utility hypothesis;machine learning;predictive modelling;receiver operator characteristics;receiver operating characteristic;funding grant	Gregory F. Cooper;Shyam Visweswaran	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		econometrics;machine learning;data mining;mathematics	Logic	6.7048043287780725	-76.27195418276496	71450
6373ea60a5687612f7b90722b22ebb1935ddc9b2	computational modeling of interventions and protective thresholds to prevent disease transmission in deploying populations	chickenpox;military personnel;communicable disease control;hepatitis a;models theoretical;communicable diseases;hepatitis b;rubella;vaccination;measles;algorithms;humans;computational biology;computer simulation;military medicine;disease outbreaks	Military personnel are deployed abroad for missions ranging from humanitarian relief efforts to combat actions; delay or interruption in these activities due to disease transmission can cause operational disruptions, significant economic loss, and stressed or exceeded military medical resources. Deployed troops function in environments favorable to the rapid and efficient transmission of many viruses particularly when levels of protection are suboptimal. When immunity among deployed military populations is low, the risk of vaccine-preventable disease outbreaks increases, impacting troop readiness and achievement of mission objectives. However, targeted vaccination and the optimization of preexisting immunity among deployed populations can decrease the threat of outbreaks among deployed troops. Here we describe methods for the computational modeling of disease transmission to explore how preexisting immunity compares with vaccination at the time of deployment as a means of preventing outbreaks and protecting troops and mission objectives during extended military deployment actions. These methods are illustrated with five modeling case studies for separate diseases common in many parts of the world, to show different approaches required in varying epidemiological settings.	adverse event;approximation algorithm;assumed;computation;computer simulation;cryptocurrency tumbler;deploy;epidemiology;goof troop;hearing loss, high-frequency;incidence matrix;interrupt;mathematical model;mathematical optimization;paget's disease, mammary;population;population parameter;religious missions;virus;benefit;disease transmission;interest	Colleen Burgess;Angela Peace;Rebecca Everett;Buena Allegri;Patrick Garman	2014		10.1155/2014/785752	computer simulation;medicine;pathology;computer science;virology;vaccination;immunology;military medicine	ML	7.392333208732169	-70.21399018096784	71637
38f8b93a6b839dea6fbf1a7cd55e287522511bc3	enhancing fingerprint biometrics in automated border control with adaptive cohorts	databases;adaptive systems;data privacy;fingerprint recognition;context;bioinformatics	Automated Border Control (ABC) systems are being increasingly used to perform a fast, accurate, and reliable verification of the travelers' identity. These systems use biometric technologies to verify the identity of the person crossing the border. In this context, fingerprint verification systems are widely adopted due to their high accuracy and user acceptance. Matching score normalization methods can improve the performance of fingerprint recognition in ABC systems and mitigate the effect of non-idealities typical of this scenario without modifying the existing biometric technologies. However, privacy protection regulations restrict the use of biometric data captured in ABC systems and can compromise the applicability of these techniques. Cohort score normalization methods based only on impostor scores provide a suitable solution, due to their limited use of sensible data and to their promising performance. In this paper, we propose a privacy-compliant and adaptive normalization approach for enhancing fingerprint recognition in ABC systems. The proposed approach computes cohort scores from an external public dataset and uses computational intelligence to learn and improve the matching score distribution. The use of a public dataset permits to apply cohort normalization strategies in contexts in which privacy protection regulations restrict the storage of biometric data. We performed a technological and a scenario evaluation using a commercial matcher currently adopted in real ABC systems and we used data simulating different conditions typical of ABC systems, obtaining encouraging results.	biometrics;computation;computational intelligence;database normalization;fingerprint recognition;machine learning;privacy;simulation;time complexity	Abhinav Anand;Ruggero Donida Labati;Angelo Genovese;Enrique Muñoz Ballester;Vincenzo Piuri;Fabio Scotti;Gianluca Sforza	2016	2016 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2016.7850073	computer science;data mining;world wide web;computer security	Security	-1.7214027727564947	-72.1995673812948	71806
181ae661a0673e66df81448c0b03646a80043464	smooth graphs for visual exploration of higher-order state transitions	graph theory;biology computing;splines;graph drawing;time series biology computing graph theory splines mathematics;higher order state transitions;time series;splines mathematics;higher order;large observational time series;animals 0 0 0 1 0 0 0 behavior animal 0 0 0 1 0 0 0 cluster analysis 0 0 0 1 0 0 0 computational biology 0 0 0 1 0 0 0 computer graphics 0 0 0 1 0 0 0 databases factual 0 0 0 1 0 0 0 spheniscidae 0 0 0 1 0 0 0 time factors 0 0 0 1 0 0 0;first order;specific activity;biological data smooth graphs higher order state transitions state sequences large observational time series state graph exploration methods splines;data visualization;state transitions;state graph exploration methods;smooth graphs;biological data state transitions graph drawing time series;clustering algorithms;time series data;state sequences;biological data;frequency;data visualization frequency clustering algorithms;state transition	In this paper, we present a new visual way of exploring state sequences in large observational time-series. A key advantage of our method is that it can directly visualize higher-order state transitions. A standard first order state transition is a sequence of two states that are linked by a transition. A higher-order state transition is a sequence of three or more states where the sequence of participating states are linked together by consecutive first order state transitions. Our method extends the current state-graph exploration methods by employing a two dimensional graph, in which higher-order state transitions are visualized as curved lines. All transitions are bundled into thick splines, so that the thickness of an edge represents the frequency of instances. The bundling between two states takes into account the state transitions before and after the transition. This is done in such a way that it forms a continuous representation in which any subsequence of the timeseries is represented by a continuous smooth line. The edge bundles in these graphs can be explored interactively through our incremental selection algorithm. We demonstrate our method with an application in exploring labeled time-series data from a biological survey, where a clustering has assigned a single label to the data at each time-point. In these sequences, a large number of cyclic patterns occur, which in turn are linked to specific activities. We demonstrate how our method helps to find these cycles, and how the interactive selection process helps to find and investigate activities.	anatomic node;authorization;closure;cluster analysis;clutter;coherence (physics);cyclic redundancy check;daniel g. bobrow;design of experiments;diving (activity);genetic selection;graph - visual representation;graph drawing;ieee xplore;imagery;increment;interactivity;interpolation imputation technique;neuritis, autoimmune, experimental;personnameuse - assigned;product bundling;routing;scott continuity;selection algorithm;silo (dataset);spline (mathematics);spline interpolation;state diagram;state transition table;thickness (graph theory);time complexity;time series;times ascent;tracer;usability testing;exponential;statistical cluster	Jorik Blaas;Charl P. Botha;Edward Grundy;Mark W. Jones;Robert S. Laramee;Frits H. Post	2009	IEEE Transactions on Visualization and Computer Graphics	10.1109/TVCG.2009.181	combinatorics;discrete mathematics;computer science;graph theory;theoretical computer science;time series;data mining;mathematics;geometry;data visualization;statistics	Visualization	-0.8768790159346787	-67.93687271333661	71856
83f587f9ddc782edf569b4e75466868d9ee8e5ce	on the role of the plasmodial cytoskeleton in facilitating intelligent behavior in slime mold physarum polycephalum	physarum polycephalum;cytoskeleton;slime mold;unconventional computing	The plasmodium of slime mold Physarum polycephalum behaves as an amorphous reaction-diffusion computing substrate and is capable of apparently 'intelligent' behavior. But how does intelligence emerge in an acellular organism? Through a range of laboratory experiments, we visualize the plasmodial cytoskeleton-a ubiquitous cellular protein scaffold whose functions are manifold and essential to life-and discuss its putative role as a network for transducing, transmitting and structuring data streams within the plasmodium. Through a range of computer modeling techniques, we demonstrate how emergent behavior, and hence computational intelligence, may occur in cytoskeletal communications networks. Specifically, we model the topology of both the actin and tubulin cytoskeletal networks and discuss how computation may occur therein. Furthermore, we present bespoke cellular automata and particle swarm models for the computational process within the cytoskeleton and observe the incidence of emergent patterns in both. Our work grants unique insight into the origins of natural intelligence; the results presented here are therefore readily transferable to the fields of natural computation, cell biology and biomedical science. We conclude by discussing how our results may alter our biological, computational and philosophical understanding of intelligence and consciousness.	anatomy, regional;automata theory;bespoke;cellular automaton;chondromyces;computation (action);computational intelligence;computer simulation;consciousness;cytoskeleton;dictyosteliida;emergence;experiment;filamentous fungus;incidence matrix;national origin;natural computing;physarum polycephalum;rem sleep behavior disorder;slime;swarm;telecommunications network;transmitter;unconventional computing;funding grant;manifold	Richard Mayne;Andrew Adamatzky;Jeff Jones	2015		10.1080/19420889.2015.1059007	biology;cell biology;cytoskeleton;slime mold;unconventional computing	AI	4.445541482812537	-68.07305700117384	71861
72f53f77bc2f129a78e2c90481f5d43dbc49d571	an integrated framework for risk profiling of breast cancer patients following surgery	biological patents;biomedical journals;text mining;europe pubmed central;citation search;rule extraction;citation networks;artificial neural networks;research articles;abstracts;open access;interface;survival analysis;life sciences;clinical guidelines;full text;breast cancer;rest apis;orcids;europe pmc;biomedical research;artificial neural network;bioinformatics;literature search	OBJECTIVE An integrated decision support framework is proposed for clinical oncologists making prognostic assessments of patients with operable breast cancer. The framework may be delivered over a web interface. It comprises a triangulation of prognostic modelling, visualisation of historical patient data and an explanatory facility to interpret risk group assignments using empirically derived Boolean rules expressed directly in clinical terms.   METHODS AND MATERIALS The prognostic inferences in the interface are validated in a multicentre longitudinal cohort study by modelling retrospective data from 917 patients recruited at Christie Hospital, Wilmslow between 1983 and 1989 and predicting for 931 patients recruited in the same centre during 1990-1993. There were also 291 patients recruited between 1984 and 1998 at the Clatterbridge Centre for Oncology and the Linda McCartney Centre, Liverpool, UK.   RESULTS AND CONCLUSIONS There are three novel contributions relating this paper to breast cancer cases. First, the widely used Nottingham prognostic index (NPI) is enhanced with additional clinical features from which prognostic assessments can be made more specific for patients in need of adjuvant treatment. This is shown with a cross matching of the NPI and a new prognostic index which also provides a two-dimensional visualisation of the complete patient database by risk of negative outcome. Second, a principled rule-extraction method, orthogonal search rule extraction, generates readily interpretable explanations of risk group allocations derived from a partial logistic artificial neural network with automatic relevance determination (PLANN-ARD). Third, 95% confidence intervals for individual predictions of survival are obtained by Monte Carlo sampling from the PLANN-ARD model.		Ian H. Jarman;Terence A. Etchells;José David Martín-Guerrero;Paulo J. G. Lisboa	2008	Artificial intelligence in medicine	10.1016/j.artmed.2007.11.005	computer science;bioinformatics;artificial intelligence;data science;breast cancer;machine learning;interface;data mining;survival analysis;artificial neural network;statistics	AI	5.899185152588517	-74.81767854364824	72590
a8f38cb480dbd2348e0c158a0918bf28c24f5c29	channel capacity analysis of blood capillary-based molecular communication	blood biochemistry mathematical model molecular communication transmitters receivers recycling;wireless channels blood channel capacity molecular communication telecommunication;vesicle propagation process blood capillary based molecular communication channel capacity analysis bio inspired communication method future nano networks diffusion based channel molecule concentration fick law langevin equation microcosmic aspect blood capillary system;molecular communication;receivers;blood;vesicle release molecular communciation diffusion based channel blood capilalry;transmitters;mathematical model;recycling;biochemistry	Molecular communication (MC) is a promising bio-inspired communication method in future nano-networks. Diffusion-based channel is the most basic yet active branch of the MC. This paper introduces a new bio-phenomenon into MC, namely, blood capillary. While most work on diffusion-based channel focus on molecules concentration and described by Fick's law, this paper proposes to use Langevin equation in microcosmic aspect. In this paper, the model of blood capillary system also considers the vesicle release process, and vesicle propagation process. With the modeling of the integrated blood capillary system, this paper analyzes channel capacity for the system.	british informatics olympiad;channel capacity;gnu nano;software propagation	Yue Sun;Kun Yang;Qiang Liu	2015	2015 IEEE International Conference on Computer and Information Technology; Ubiquitous Computing and Communications; Dependable, Autonomic and Secure Computing; Pervasive Intelligence and Computing	10.1109/CIT/IUCC/DASC/PICOM.2015.180	transmitter;telecommunications;mathematical model;recycling	Robotics	3.7029056024063665	-68.99665079411461	72621
4caf6afaefe903ac0c0b39a6b312340f1281ea22	analysis of maryland poisoning deaths using classification and regression tree (cart) analysis		Our study is a cross-sectional analysis of Maryland poisoning deaths for years 2003 and 2004. We used Classification and Regression Tree (CART) methodology to classify undetermined intent Maryland poisoning deaths as either unintentional or suicidal poisonings. The predictive ability of the selected set of variables (i.e., poisoned in the home or workplace, location type, where poisoned, place of death, poison type, victim race and age, year of death) was extremely good. Of the 301 test cases, only eight were misclassified by the CART regression tree. Of 1,204 undetermined intent poisoning deaths, CART classified 903 as suicides and 301 as unintentional deaths. The major strength of our study is the use of CART to differentiate with a high degree of accuracy between unintentional and suicidal poisoning deaths among Maryland undetermined intent poisoning deaths.	cessation of life;ciguatera poisoning;classification;cross-sectional data;decision tree learning;infant, extremely premature;poisons;rom cartridge;test case;gift	Carol Pamer;Tracey Serpi;Joseph Finkelstein	2008	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;toxicology;forensic engineering;computer security	Logic	8.673315018897608	-76.22254266015642	72641
577204197ee4efa3eec79a05177851ecf4c72b19	theory and application of arterial tissue in-host remodelling	biomechanical phenomena;health informatics;regeneration;arteries;tissue engineering blood vessels;biomedical engineering;1707;polymers arteries degradation biological system modeling yttrium production computational modeling;signal processing;arterial tissue engineered constructs arterial tissue in host remodelling biomedicine diseased arterial sections arterial tissue growth synthetic degradable acellularized graft mechanistic models;humans;models cardiovascular;tissue engineering	A central therapeutic goal in many applications of modern Biomedicine is the reconstruction of the diseased arterial sections via robust and viable tissue equivalents. In-host remodelling is an emerging technology that exploits the remodelling ability of the host to regenerate tissue. We develop a general theoretical framework of growth and remodeling of arterial tissue starting from a synthetic, degradable, acellularized graft and we demonstrate the potential of mechanistic models to guide the development and assisting in the design of arterial tissue engineered constructs.	biomedicine;equivalent weight;graft vs tumor effect;grafting (decision trees);robustness (computer science);synthetic intelligence;transplanted tissue	A. Valentin;D. Notaro;P. Zunino;R. Allen;D. Ambrosi;Yue Wang;Anne M Robertson	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318746	health informatics;pathology;signal processing;regeneration;biological engineering;tissue engineering;anatomy	DB	6.873763423503999	-69.23266713490874	72883
03e2eb8fbf063a22221bfe91371ad90d1b6f3ae1	nemesis: which restaurants should you avoid today?	foodborne disease;online social media;language modelling;organic sensor networks	Computational approaches to health monitoring and epidemiology continue to evolve rapidly. We present an end-to-end system, nEmesis, that automatically identifies restaurants posing public health risks. Leveraging a language model of Twitter users’ online communication, nEmesis finds individuals who are likely suffering from a foodborne illness. People’s visits to restaurants are modeled by matching GPS data embedded in the messages with restaurant addresses. As a result, we can assign each venue a “health score” based on the proportion of customers that fell ill shortly after visiting it. Statistical analysis reveals that our inferred health score correlates (r = 0.30) with the official inspection data from the Department of Health and Mental Hygiene (DOHMH). We investigate the joint associations of multiple factors mined from online data with the DOHMH violation scores and find that over 23% of variance can be explained by our factors. We demonstrate that readily accessible online data can be used to detect cases of foodborne illness in a timely manner. This approach offers an inexpensive way to enhance current methods to monitor food safety (e.g., adaptive inspections) and identify potentially problematic venues in near-real time.	computation;computer-mediated communication;embedded system;end system;end-to-end principle;f1 score;global positioning system;language model;mined;nemesis;venue (sound system)	Adam Sadilek;Sean Padraig Brennan;Henry A. Kautz;Vincent Silenzio	2013			simulation;artificial intelligence;computer security	HCI	-1.8829654127313498	-73.90186370776979	72935
5b3da2b5bc8c0c125768af784ca42e192086d980	predicting the risk of low back disorders due to manual handling tasks	quality of life;low risk and high risk jobs;low back disorder;occupational health and safety;support vector machines;training;risk management;manual materials handling;data mining;classification;data model;vegetation;risk management logistics materials handling occupational health occupational safety pattern classification regression analysis;low back disorders;accuracy;artificial neural networks;logistics;data mining methods;materials handling;occupational safety;pattern classification;support vector machines accuracy data models training artificial neural networks neurons vegetation;regression analysis;occupational health;neurons;support vector machine;national institute for occupational health and safety guides risk prediction low back disorder manual material handling task occupational disabling injury industrial working population mmh task logistic regression model manual lifting task classification;logistic regression model;data mining methods low back disorders manual handling tasks classification low risk and high risk jobs;low risk;artificial neural network;data models;manual handling tasks;high risk	Work related low back disorders (LBDs) due to manual material handling (MMH) tasks have long been recognized as one of the main occupational disabling injury that affects the quality of life of the industrial working population in the U.S. One of the efforts to comprehend the nature and phenomenon of LBDs due to MMH tasks was undertaken by Marras [18]. Based on multiple experiments they created a seminal data set and used it to build logistic regression models to identify significant variables and classify manual lifting tasks into high risk and low risk with respect to LBDs. Since then a number of studies have used the same data set to build and test various classification models. This paper analyzes the previous studies and employs the same data set to build and test seven classification methods. Though the performances of our best models are better than those reported in National Institute for Occupational Health and Safety (NIOHS) Guides and two of our previous studies, they are generally less optimistic than those reported in several other studies, this paper proposes a more systematic and reliable approach to creating and validating classifiers to distinguish between low and high risk manual lifting jobs that contribute to LBDs.	experiment;lambda lifting;logistic regression;mmh-badger mac;material handling;performance;the quality of life	Jacek M. Zurada	2012	2012 45th Hawaii International Conference on System Sciences	10.1109/HICSS.2012.482	support vector machine;computer science;artificial intelligence;operations management;machine learning;data mining;occupational safety and health	SE	5.558857616183667	-76.24457620188167	73067
dea404bb21808f5d5a2a69626782b9de5d1d29e4	an intelligent algorithm for identification of optimum mix of demographic features for trust in medical centers in iran	adaptive neuro-fuzzy inference system (anfis);demographic features;healthcare;patient;statistical methods;trust	Healthcare quality is affected by various factors including trust. Patients' trust to healthcare providers is one of the most important factors for treatment outcomes. The presented study identifies optimum mixture of patient demographic features with respect to trust in three large and busy medical centers in Tehran, Iran. The presented algorithm is composed of adaptive neuro-fuzzy inference system and statistical methods. It is used to deal with data and environmental uncertainty. The required data are collected from three large hospitals using standard questionnaires. The reliability and validity of the collected data is evaluated using Cronbach's Alpha, factor analysis and statistical tests. The results of this study indicate that middle age patients with low level of education and moderate illness severity and young patients with high level of education, moderate illness severity and moderate to weak financial status have the highest trust to the considered medical centers. To the best of our knowledge this the first study that investigates patient demographic features using adaptive neuro-fuzzy inference system in healthcare sector. Second, it is a practical approach for continuous improvement of trust features in medical centers. Third, it deals with the existing uncertainty through the unique neuro-fuzzy approach.		Reza Yazdanparast;Saeed Abdolhossein Zadeh;D. Dadras;Ali Azadeh	2018	Artificial intelligence in medicine	10.1016/j.artmed.2018.04.006	illness severity;cronbach's alpha;statistical hypothesis testing;health care;inference;algorithm;computer science;middle age	ML	4.769651615622258	-75.77645889036826	73309
d45f9c174271763cf776feec0cd0e7fc6f7ff10b	on patient's characteristics extraction for metabolic syndrome diagnosis: predictive modelling based on machine learning			machine learning;predictive modelling	Frantisek Babic;Ljiljana Majnaric;Alexandra Lukácová;Ján Paralic;Andreas Holzinger	2014		10.1007/978-3-319-10265-8_11	artificial intelligence;machine learning;pattern recognition	AI	7.872535587934225	-77.85759494869022	73380
f856d40ae162cca57c560c54487937cffef55cf9	a fuzzy logic-based decision support system on anesthetic depth control for helping anesthetists in surgeries	pulse rate;sevoflurane;general anesthesia;depth of anesthesia;fuzzy logic;decision support system;systolic arterial pressure;anesthetic depth control;anesthetic dose level	In this study, a fuzzy logic-based anesthetic depth decision support system (ADDSS) was realized for anesthetic depth control to help anesthetists in surgeries. Depth of anesthesia for a patient can change according to anesthetic agent and characteristic properties of a patient such as age, weight, etc. During the surgery, depth of anesthesia of a patient is determined by the experience of anesthetist controlling of systolic arterial pressure (SAP) and heart pulse rate (HPR) parameters. Anesthetists could have tired and lost attention by inhaling of anesthetic gas leaks in long lasted operations. For that reason, improper anesthetic depth could be applied to the patients. So anesthesia could not be safety and comfortable. To remove this unwanted situation, an ADDSS was proposed for anesthetists. By the help of this system, precise anesthetic depth could have provided. Thus, the anesthetist will spend less time to provide anesthetic and the patient will have a safer and less expensive operation. This study was performed under sevoflurane anesthetic.	anesthetics;cns disorder;decision support systems, clinical;decision support system;fuzzy logic;hl7publishingsubsection <operations>;inspiration function;operative surgical procedures;patients;proteomics;systolic pressure;sevoflurane	Hamdi Melih Saraoglu;Sibel Sanli	2007	Journal of Medical Systems	10.1007/s10916-007-9092-x	fuzzy logic;intensive care medicine;medicine;decision support system;computer science;artificial intelligence;anesthesia;surgery	HCI	5.235179970625013	-79.15065076905933	73503
22c24e4c270e7c1f3361776ea43f44a413f1600d	model formulation: understanding detection performance in public health surveillance: modeling aberrancy-detection algorithms	real time;space time;unified model;evaluation metric;detection algorithm;public health system;evaluation studies;administrative data;public health;public health surveillance;software implementation;disease outbreak	OBJECTIVE Statistical aberrancy-detection algorithms play a central role in automated public health systems, analyzing large volumes of clinical and administrative data in real-time with the goal of detecting disease outbreaks rapidly and accurately. Not all algorithms perform equally well in terms of sensitivity, specificity, and timeliness in detecting disease outbreaks and the evidence describing the relative performance of different methods is fragmented and mainly qualitative.   DESIGN We developed and evaluated a unified model of aberrancy-detection algorithms and a software infrastructure that uses this model to conduct studies to evaluate detection performance. We used a task-analytic methodology to identify the common features and meaningful distinctions among different algorithms and to provide an extensible framework for gathering evidence about the relative performance of these algorithms using a number of evaluation metrics. We implemented our model as part of a modular software infrastructure (Biological Space-Time Outbreak Reasoning Module, or BioSTORM) that allows configuration, deployment, and evaluation of aberrancy-detection algorithms in a systematic manner.   MEASUREMENT We assessed the ability of our model to encode the commonly used EARS algorithms and the ability of the BioSTORM software to reproduce an existing evaluation study of these algorithms.   RESULTS Using our unified model of aberrancy-detection algorithms, we successfully encoded the EARS algorithms, deployed these algorithms using BioSTORM, and were able to reproduce and extend previously published evaluation results.   CONCLUSION The validated model of aberrancy-detection algorithms and its software implementation will enable principled comparison of algorithms, synthesis of results from evaluation studies, and identification of surveillance algorithms for use in specific public health settings.	algorithm;deploy;encode;measurement in quantum mechanics;modular programming;real-time locating system;scientific publication;sensitivity and specificity;sensor;unified model	David L. Buckeridge;Anya Okhmatovskaia;Samson W. Tu;Martin J. O'Connor;Csongor Nyulas;Mark A. Musen	2008	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2799	simulation;medicine;pathology;public health;computer science;space time;unified model;data mining;database;management science;outbreak	Metrics	2.098668917983029	-70.76742823725554	73585
3a319c45e0b120ed396ad8610498a80962eb4bb5	an approach to the engineering of cellular models based on p systems	design principle;synthetic biology;genetics;cellular model;complex system;p system;information processing;gene regulatory network;species distribution	Living cells assembled into colonies or tissues communicate using  complex systems  . These systems consist in the interaction between many molecular species distributed over many compartments. Among the different cellular processes used by cells to monitor their environment and respond accordingly, gene regulatory networks, rather than individual genes, are responsible for the information processing and orchestration of the appropriate response [16].#R##N##R##N#In this respect,  synthetic biology  has emerged recently as a novel discipline aiming at unravelling the design principles in gene regulatory systems by synthetically engineering transcriptional networks which perform a specific and prefixed task [2]. Formal modelling and analysis are key methodologies used in the field to engineer, assess and compare different genetic designs or devices.		Francisco José Romero-Campero;Natalio Krasnogor	2009		10.1007/978-3-642-03073-4_44	gene regulatory network;information processing;computer science;bioinformatics;cellular model;species distribution;synthetic biology;p system	SE	4.51411838663541	-67.72469017075088	73614
d02f31b85ab91db04be02dc50a22411b70fd828d	development of indian weighted diabetic risk score (iwdrs) using machine learning techniques for type-2 diabetes	discretization;data mining;clustering;type 2 diabetes;weighted diabetic risk score	Undetected pre-diabetes and late diagnosis is a major problem in East Asian countries. Diabetes screening tools such as Diabetes Risk Score (DRS) can effectively help in detecting and preventing the disease among pre-diabetes persons. Several Risk Scores for Type -2 Diabetes have been proposed and being used. In current research, researchers have observed certain issues in the available DRS and advocate the need to address the same. In this study researchers propose a novel Indian Weighted Diabetic Risk Score (IWDRS). Machine Learning Techniques such as distance based clustering with Euclidean distance, k-means algorithm and discretization is used to derive weighted risk score for diabetes risk factors like age, BMI, waist circumference, personal history, family history, diet, physical activity, stress and life quality. Result analysis shows that the proposed approach is better than existing approach in scientific literature.	algorithm;brain–computer interface;cluster analysis;discretization;euclidean distance;k-means clustering;machine learning;scientific literature;sensor	Omprakash Chandrakar;Jatinderkumar R. Saini	2016		10.1145/2998476.2998497	computer science;machine learning;discretization;data mining;cluster analysis;statistics	ML	3.9511831686448833	-76.14409772776608	74120
17943db7137838c57814c8ca306cfc10a42cf4b7	prediction of metabolic syndrome using artificial neural network system based on clinical data including insulin resistance index and serum adiponectin	insulin resistance index;artificial neural network system;metabolic syndrome	OBJECTIVE This study aimed to predict the 6-year incidence of metabolic syndrome (MetS) using an artificial neural network (ANN) system and multiple logistic regression (MLR) analysis based on clinical factors, including the insulin resistance index calculated by homeostasis model assessment (HOMA-IR).   DESIGN Subjects were recruited from participants in annual health check-ups in both 2000 and 2006. A total of 410 Japanese male teachers and other workers at Keio University, 30-59 years of age at baseline, participated in this retrospective cohort study.   MEASUREMENTS Clinical parameters were randomly divided into a training dataset and a validation dataset, and the ANN system and MLR analysis were applied to predict individual incidences. The leave some out cross validation method was used for validation.   RESULTS The sensitivity of the prediction was 0.27 for the MLR model and 0.93 for the ANN system, while specificities were 0.95 and 0.91, respectively. Sensitivity analysis employing the ANN system identified BMI, age, diastolic blood pressure, HDL-cholesterol, LDL-cholesterol and HOMA-IR as important predictors, suggesting these factors to be non-linearly related to the outcome.   CONCLUSION We successfully predicted the 6-year incidence of MetS using an ANN system based on clinical data, including HOMA-IR and serum adiponectin, in Japanese male subjects.	adipoq gene;adiponectin measurement;artificial neural network;baseline (configuration management);brain–computer interface;cholesterol;cross-sectional studies;cross-validation (statistics);diastole;digital back-propagation;fifty nine;hardware description language;homeostasis;incidence matrix;insulin resistance;learning to rank;logistic regression;malignant fibrous histiocytoma;metabolic syndrome x;metastatic neoplasm;multiple personality disorder;randomness;silo (dataset)	Hiroshi Hirose;Tetsuro Takayama;Shigenari Hozawa;Toshifumi Hibi;Ikuo Saito	2011	Computers in biology and medicine	10.1016/j.compbiomed.2011.09.005	endocrinology;artificial intelligence;diabetes mellitus	NLP	7.831784690478213	-76.5030022008819	74128
3812a548db6063996dbbe79ce25f1d8aa55f4b6b	use of cumulative information estimations for risk assessment of heart failure patients	telemedicine belief networks cardiology cost reduction decision support systems decision trees fuzzy logic geriatrics patient monitoring regression analysis risk management;pragmatics heart estimation decision trees prediction algorithms blood mutual information;cardiology cumulative information estimation decision tree home telemonitoring e health heart failure;cumulative information estimation logistic regression method nearest neighbor method bayesian network method cost minimization life threatening situation minimization decision node relative mutual cumulative information fuzzy logic fuzzy decision trees intelligent clinical decision support system home telemonitoring diabetes obesity aging population heart failure patient risk assessment	As a consequence of aging population and an increasing prevalence of obesity and diabetes there are more and more patients with heart failure. This leads to a lack of professionals who can treat them and to escalating costs. An interesting solution appears to be home telemonitoring with an intelligent clinical decision support system. In this paper, the use of cumulative information estimations for risk assessment of heart failure patients with such a system is analyzed. These cumulative information estimations are utilized for creation of an algorithmic model using fuzzy decision trees that combine decision trees and notions of fuzzy logic. The algorithmic model employs mutual cumulative information and relative mutual cumulative information for association of an important piece of data about the patients with a decision node. The risk assessment with the presented solution is analyzed from the point of view of minimization of life-threatening situations and minimization of costs. Comparisons with a Bayesian network method, a nearest neighbor method, and a logistic regression method show it is a promising solution.	bayesian network;clinical decision support system;decision tree;fuzzy logic;influence diagram;logistic regression;nearest neighbor search;risk assessment	Jan Bohacik;Chandrasekhar Kambhampati;Darryl N. Davis;John G. F. Cleland	2014	2014 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2014.6891536	computer science;artificial intelligence;machine learning;data mining	Robotics	4.312492500306235	-77.38975569550382	74671
d129010a2775a2986fbb941414bd7c14e8005485	factors affecting automated syndromic surveillance	false positive detection;graphical models;syndromic surveillance;true positive;graphical model;false positive;true positive detection;time series model	OBJECTIVE The increased threat of bioterroristic attacks and epidemic events requires the development of accurate and timely outbreak detection systems for early identification of anomalies in public health data.   MATERIAL AND METHODS We propose an automated outbreak detection system based on syndromic data. This system uses an autoregressive model with seasonal components to monitor, online, the daily counts of chief complaints for respiratory syndromes at the emergency department of two major metropolitan hospitals. We evaluate this system by estimating the false positive rate in real data under the assumption that there were no outbreaks of disease, and the true positive rate in real baseline data in which we injected stochastically simulated outbreaks of different shape and size. We then use directed graphical models to account for the effect of exogenous factors on the detection performance of the system.   RESULTS Our study shows that for a week-long outbreak, our model has an overall 84.8% true detection accuracy across all shapes of outbreaks, while the outbreak size influences the earliness to detection. The false and true positive rates are also associated with the exogenous factors and knowledge about these factors can help to improve the detection accuracy.   CONCLUSION This study suggests that the integration of multiple data sources can significantly improve the detection accuracy of syndromic surveillance systems.	autoregressive model;baseline (configuration management);estimated;graphical model;respiratory distress syndrome, adult;sensitivity and specificity;syndrome	Ling Wang;Marco Ramoni;Kenneth D. Mandl;Paola Sebastiani	2005	Artificial intelligence in medicine	10.1016/j.artmed.2004.11.002	simulation;computer science;machine learning;data mining;graphical model;statistics	AI	6.597763649914827	-72.95938673444078	74844
fe75d9e01d6d38a4f4eb31bfcf91a9935988e1cb	ambient assisted living and daily activities	book	Detection of falls is very important from a health and safety perspective. However, falls occur rarely and infrequently, which leads to either limited or no training data and thus can severely impair the performance of supervised activity recognition algorithms. In this paper, we address the problem of identification of falls in the absence of training data for falls, but with abundant training data for normal activities. We propose two ‘X-Factor’ Hidden Markov Model (XHMMs) approaches that are like normal HMMs, but have “inflated” output covariances (observation models), which can be estimated using cross-validation on the set of ‘outliers’ in the normal data that serve as proxies for the (unseen) fall data. This allows the XHMMs to be learned from only normal activity data. We tested the proposed XHMM approaches on two real activity recognition datasets that show high detection rates for falls in the absence of training data.	activity recognition;algorithm;cross-validation (statistics);hidden markov model;markov chain;supervised learning	Leandro Pecchia;Liming Chen;Chris D. Nugent;José Bravo	2014		10.1007/978-3-319-13105-4	environmental health;business;activities of daily living	Vision	-1.8464104055853416	-73.93709878938832	74956
31b49113bda5118cf40a55813382ba30f24cb741	classification medical data series by artificial immune methods				Wieslaw Wajs	2005			data mining;immune system;computer science	DB	1.8515839524473874	-74.90456405675293	75029
0b61a5c1d7f04e0c614aa0896862747d9b93d201	systems analysis of bone mechanotransduction at cellular level	mechanical loading;modeling technique;signaling network;temporal dynamics;bone resorption;bone remodeling;bone mechanotransduction;computer model;bone formation;loading;biomechanics;signal transduction pathway;osteoclasts;bone resorption systems analysis bone mechanotransduction mechanoregulation intracellular response signal transduction pathways osteoblasts osteoclasts intracellular feedback regulation;intracellular response;bone mass;control system;computational modeling;bones;proteins;systems analysis;network model;bone;system biology;mathematical model;signal transduction pathways;system analysis;simulation study;mechanoregulation;parameter estimation;bones biological system modeling feedback computational modeling systems biology proteins parameter estimation biology computing computer networks control system synthesis;load modeling;intracellular feedback regulation;cellular biophysics;cellular biophysics biomechanics bone;evolutionary computing;osteoblasts	A dasiasystems-levelpsila computational modeling approach is implemented to study the mechano-regulation of bone at cellular level. Issues addressed using this approach include - determining the intra-cellular response of bone cells to mechanical stimulus, bone response to different mechanical loading conditions, the role of intra-cellular feedback regulation in bone remodeling, and the link between reduced mechanical loading and decreased bone mass. An inter-connected network of signal transduction pathways in osteoblasts and osteoclasts is considered for modeling. The salient features of this modeling technique are systems biology based network modeling to simulate the temporal dynamics of the signaling proteins, parameter estimation based on evolutionary computing, and control systems theory to model feedback in the signaling network. The results indicate that signaling networks respond uniquely to different mechanical stimuli, the stimulus signal is gradually attenuated in the signaling cascade, and the disruption of intra-cellular feedback regulation leads to decreased bone formation in osteoblasts and increased bone resorption in osteoclasts. This results in low bone mass, a phenomenon generally observed in reduced loading conditions. It is deduced that reduced mechanical loading leads to disruption in the feedback to result in low bone mass. The results of these simulation studies are expected to serve as useful guidelines for planning relevant experimental work to study the effect of mechanical loading on bone at cellular level.	cell signaling;control system;denial-of-service attack;estimation theory;evolutionary computation;simulation;systems biology;systems theory;transduction (machine learning)	Kalyan C. Mynampati;Peter Lee Vee Sin	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696734	biochemistry;computer science;control system;biomechanics;immunology;signal transduction;anatomy;statistics	Robotics	9.262998279144417	-68.57419927024267	75098
276e3bc7e3e7ed4ba08261b82edf2a2a2ab64fba	an intelligent decision support system for bacterial clinical isolates in vitro utilising an electronic nose	neuro fuzzy systems;microbial analysis;school of no longer in use;intelligent decision support system;electronics and computer science;multiple classifiers;electronic nose	Current clinical diagnostics are based on biochemical, immunological or microbiological methods. However, these methods are operator dependent, time consuming, expensive and require special skills, and are, therefore, not suitable for point-of-care testing. Recent developments in gas-sensing technology and pattern recognition methods make electronic nose technology an interesting alternative for medical point-of-care devices. An electronic nose based on chemo-resistive sensors has been employed to identify in vitro 13 bacterial clinical isolates, collected from patients diagnosed with urinary tract infections, gastrointestinal and respiratory infections in a Public Health Laboratory environment. An intelligent unit consisting of an odour generation mechanism, rapid volatile delivery and recovery system, and a classifier system based on a novel neuro-fuzzy system has been applied in the identification and characterisation of microbial pathogens. The proposed structure constructs its initial rules by clustering, while the final fuzzy rule base is determined by competitive learning. Both error backpropagation and recursive least squares estimation, are applied to the learning scheme. The performance of the model was evaluated in terms of training performance and classification accuracies and the results show the potential for early detection of microbial contaminants in urine samples using electronic nose technology.	intelligent decision support system	Vassilis S. Kodogiannis;Ilias Petrounias;John N. Lygouras	2009	Intelligent Decision Technologies	10.3233/IDT-2009-0057	electronic nose;intelligent decision support system;computer science;engineering;artificial intelligence;machine learning;biological engineering	Robotics	8.646102474703246	-77.48840240070811	75903
27cf81cdc37778199e747a94aa8b9b64c28401a6	maximizing patient coverage through optimal allocation of residents and scribes to shifts in an emergency department	patient volume;residents;scribes;staffing scheduling;workload	Residents and scribes in an Emergency Department (ED) work closely with an attending physician. Residents care for patients under the supervision of the attending physician, whereas scribes assist physicians with documentation contemporaneously with the patient encounter. Optimal allocation of these roles to shifts is crucial to improve patient care, physician productivity, and to increase learning opportunities for residents. Since resident and scribe availability varies on a monthly basis, the allocation of these roles into different shifts within a pre-designed ED physician shift template must be dynamically adjusted. Using historical patient flow timestamp data as well as information about the patient-coverage capacity of an ED care team, a data-driven model was developed for optimally determining which shifts must be staffed by residents and scribes to maximize patient coverage and to calculate the relative importance of a shift. This relative importance metric aids decision-making in adjusting the allocation of residents and scribes to various shifts as their availability fluctuates. Since the model uses historical timestamp data, which all EDs are mandated to collect, the approach is generalizable to all EDs.	accident and emergency department;allocation;clinical use template;decision making;documentation;ehlers-danlos syndrome;mathematical optimization;patients;scribes	Phichet Wutthisirisart;Gabriela Martinez;Heather A. Heaton;Kalyan S. Pasupathy;Moriah S. Thompson;Mustafa Y. Sir	2018	Journal of Medical Systems	10.1007/s10916-018-1080-9	documentation;emergency department;workload;computer security;patient encounter;timestamp;staffing scheduling;medical emergency;medicine	ML	6.34828167083104	-74.607843328916	75904
6f104df21a2a7250577c5c0501cf843748ac61b0	towards intrinsic molecular communication using isotopic isomerism		In this paper we introduce a new approach for molecular communication (MC). The proposed method uses isotopomers as symbols in a communication scenario, and we name this approach isotopic molecular communication (IMC). We propose a modulation scheme based on isotopic isomerism, where symbols are encoded via isotopes in molecules. This can be advantageous in applications where the communication has to be independent from chemical molecular concentration. Application scenarios include nano communications with isotopes in a macroscopic environment, i.e. encoding freshwater flow of rivers or drinking water utilities, or medical applications where blood carries isotopomers used for communication in a human or animal body. We simulate the capacity of communication in the sense of symbols per second and maximum symbol rate for different applications. We provide estimations for the symbol rate per distance and we demonstrate the feasibility to identify isotopes reliably. In summary, this isotopic molecular communication is a new paradigm for data transfer independent from molecular concentrations and chemical reactions, and can provide higher throughput than ordinary molecular communications. TYPE OF PAPER AND	detection theory;gnu nano;memory controller;modulation;nanorobotics;programming paradigm;sensor;simulation;throughput;whole earth 'lectronic link	Gunther Ardelt;Christoph Külls;Horst Hellbrück	2018	OJIOT		throughput;isotopomers;computer network;computer science;modulation;symbol rate;distributed computing;data transmission;molecular communication	Networks	3.5622241503636793	-69.11357002118737	75921
c33c94e62a9c949d1fe63da2ccb41ca7c3b90a0f	lassie: simulating large-scale models of biochemical systems on gpus	computational biology bioinformatics;algorithms;computer appl in life sciences;microarrays;bioinformatics	Abstract Background Mathematical modeling and in silico analysis are widely acknowledged as complementary tools to biological laboratory methods, to achieve a thorough understanding of emergent behaviors of cellular processes in both physiological and perturbed conditions. Though, the simulation of large-scale models—consisting in hundreds or thousands of reactions and molecular species—can rapidly overtake the capabilities of Central Processing Units (CPUs). The purpose of this work is to exploit alternative high-performance computing solutions, such as Graphics Processing Units (GPUs), to allow the investigation of these models at reduced computational costs. Results LASSIE is a “black-box” GPU-accelerated deterministic simulator, specifically designed for large-scale models and not requiring any expertise in mathematical modeling, simulation algorithms or GPU programming. Given a reaction-based model of a cellular process, LASSIE automatically generates the corresponding system of Ordinary Differential Equations (ODEs), assuming mass-action kinetics. The numerical solution of the ODEs is obtained by automatically switching between the Runge-Kutta-Fehlberg method in the absence of stiffness, and the Backward Differentiation Formulae of first order in presence of stiffness. The computational performance of LASSIE are assessed using a set of randomly generated synthetic reaction-based models of increasing size, ranging from 64 to 8192 reactions and species, and compared to a CPU-implementation of the LSODA numerical integration algorithm. Conclusions LASSIE adopts a novel fine-grained parallelization strategy to distribute on the GPU cores all the calculations required to solve the system of ODEs. By virtue of this implementation, LASSIE achieves up to 92× speed-up with respect to LSODA, therefore reducing the running time from approximately 1 month down to 8 h to simulate models consisting in, for instance, four thousands of reactions and species. Notably, thanks to its smaller memory footprint, LASSIE is able to perform fast simulations of even larger models, whereby the tested CPU-implementation of LSODA failed to reach termination. LASSIE is therefore expected to make an important breakthrough in Systems Biology applications, for the execution of faster and in-depth computational analyses of large-scale models of complex biological systems.	algorithm;biological system;black box;cell physiology;central processing unit;computation (action);differential diagnosis;emergence;graphics processing unit;illness behavior;kinetics internet protocol;large;mathematical model;mathematics;memory footprint;numerical analysis;numerical integration;numerical partial differential equations;parallel computing;procedural generation;randomness;runge–kutta methods;runge–kutta–fehlberg method;simulation;solutions;supercomputer;synthetic intelligence;systems biology;time complexity	Andrea Tangherloni;Marco S. Nobile;Daniela Besozzi;Giancarlo Mauri;Paolo Cazzaniga	2017		10.1186/s12859-017-1666-0	memory footprint;numerical integration;graphics processing unit;ordinary differential equation;graphics;ode;computational science;deterministic simulation;general-purpose computing on graphics processing units;bioinformatics;computer science	HPC	5.757964842040347	-67.52311670174569	76279
cd9e49fe12e821ea06f3ec8f05f0cb42842421a4	a deep learning method for icd-10 coding of free-text death certificates		The assignment of disease codes to clinical texts has a wide range of applications, including epidemiological studies or disease surveillance. We address the task of automatically assigning the ICD-10 codes for the underlying cause of death, from the free-text descriptions included in death certificates obtained from the Portuguese Ministry of Health. We specifically propose to leverage a deep neural network based on a two-level hierarchy of recurrent nodes together with attention mechanisms. The first level uses recurrent nodes for modeling the sequences of words given in individual fields of the death certificates, together with attention to weight the contribution of each word, producing intermediate representations for the contents of each field. The second level uses recurrent nodes to model a sequence of fields, using the representations produced by the first level and also leveraging attention in order to weight the contributions of the different fields. The paper reports on experiments with a dataset of 115,406 death certificates, presenting the results of an evaluation of the predictive accuracy of the proposed method, for different ICD-10 levels (i.e., chapter, block, or full code) and for particular causes of death. We also discuss how the neural attention mechanisms can help in interpreting the classification results.	deep learning	Francisco Duarte;Bruno Martins;Cátia Sousa Pinto;Mário J. Silva	2017		10.1007/978-3-319-65340-2_12	coding (social sciences);christian ministry;icd-10;data mining;artificial neural network;hierarchy;deep learning;computer science;artificial intelligence;cause of death	Robotics	3.379205351149726	-73.62204415929581	76503
6731d6f5a8f501eb3024dc78b666fc26a1de69a2	clustering gene expression data using coxian phase-type survival	dna;patient survival distribution;patient management;histograms;pattern clustering;gene expression medical treatment hazards predictive models bioinformatics metastasis dna quality of service neoplasms breast cancer;metastasis;cancer;coxian phase type distribution gene expression data clustering coxian phase type survival acute myeloid leukemia patient gene expression characteristic patient survival distribution patient management financial benefit quality of service;hazards;gene expression data;patient care;transient analysis;gene expression characteristic;medical computing;fitting;gene expression;coxian phase type distributions;coxian phase type distribution;clustering;acute myeloid leukemia patient;survival analysis;diseases;patient treatment;quality of service bioinformatics cancer medical computing patient care patient treatment pattern clustering;predictive models;financial benefit;markov processes;neoplasms;quality of service;bioinformatics clustering survival analysis coxian phase type distributions gene expression data intelligent patient management;medical treatment;gene expression data clustering;breast cancer;coxian phase type survival;intelligent patient management;bioinformatics	The paper is concerned with modeling gene expression data of Acute Myeloid Leukemia patients and their survival. Of particular interest is the development of a clustering approach that will identify clusters of patients that have similar gene expression characteristics. This paper considers one such approach based on the Coxian phase-type distribution which proved to be an adequate representation for the patient survival distribution. If such an approach were introduced it would facilitate better patient management and hence allow the most suitable care plan for the different clusters of patients. This will not only have financial benefit but will provide a better quality of service to the patient who will no longer have to undergo ineffective treatments that may have damaging side effects.	cluster analysis;quality of service	Adele H. Marshall;Laura A. Hill	2010	2010 5th IEEE International Conference Intelligent Systems	10.1109/IS.2010.5548377	medicine;pathology;bioinformatics;data mining	Robotics	4.962009303752446	-72.72967049799945	76826
9bf517a8170e99d7e2381167e8d3f56889c66101	mathematical modeling of diphtheria transmission in thailand	booster vaccination;diphtheria;mathematical model;thailand;waning immunity	In this work, a mathematical model for describing diphtheria transmission in Thailand is proposed. Based on the course of diphtheria infection, the population is divided into 8 epidemiological classes, namely, susceptible, symptomatic infectious, asymptomatic infectious, carrier with full natural-acquired immunity, carrier with partial natural-acquired immunity, individual with full vaccine-induced immunity, and individual with partial vaccine-induced immunity. Parameter values in the model were either directly obtained from the literature, estimated from available data, or estimated by means of sensitivity analysis. Numerical solutions show that our model can correctly describe the decreasing trend of diphtheria cases in Thailand during the years 1977-2014. Furthermore, despite Thailand having high DTP vaccine coverage, our model predicts that there will be diphtheria outbreaks after the year 2014 due to waning immunity. Our model also suggests that providing booster doses to some susceptible individuals and those with partial immunity every 10 years is a potential way to inhibit future diphtheria outbreaks.		Kan Sornbundit;Wannapong Triampo;Charin Modchang	2017	Computers in biology and medicine	10.1016/j.compbiomed.2017.05.031	booster (rocketry);dtp vaccine;immunology;immunity;diphtheria;outbreak;population;transmission (mechanics);medicine	Metrics	8.02127782822832	-73.6105872739238	76884
0c93d9f2f5accee9f74cff24861d9dd35b833aa5	are static fetal growth charts still suitable for diagnostic purposes?	standards;ultrasonic imaging;biological system modeling;fetus;medical services ultrasonic imaging mathematical model standards biological system modeling fetus sociology;medical services;prototype static fetal growth chart validity diagnostic tool biometric data interpretation clinical practice epidemiological study population reshuffling phenomenon diagnostic effectiveness interpretative model global approach data collection multidimensional analysis cloud based data harvesting technique fetal growth curve customization false positive identification false negative identification data processing;mathematical model;patient diagnosis cloud computing data acquisition data analysis medical computing multidimensional systems obstetrics;sociology	Fetal growth curves and the corresponding interpretation of biometric data represent an essential diagnostic tool in clinical practice and for epidemiological studies, but their validity is questioned by the population-reshuffling phenomenon and by other factors. To restore their diagnostic effectiveness, a suitable interpretative model has to be developed based on new parameters and on a global approach for data collection. To achieve this goal we studied the combined adoption of multidimensional analysis and of cloud-based data-harvesting techniques to produce customized curves more suitable for diagnostic purposes. In particular, multidimensional analysis allows to identify and to avoid more effectively false positives and/or false negatives while the use of cloud allows collecting and processing data on a global scale. The feasibility of the proposed approach is discussed referring to a prototype and a test on the field on a small scale.	biometrics;chart;cloud computing;multidimensional analysis;prototype	Mario A. Bochicchio;Lucia Vaira	2014	2014 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2014.6999260	simulation;computer science;bioinformatics;data science;machine learning;mathematical model;data mining;statistics;fetus	Visualization	0.7844899724622787	-79.0292444346969	76891
011223286f00fa664116ef34fc3c86a0b8f08287	stochasticity in pandemic spread over the world airline network explained by local flight connections	internal medicine;medical microbiology;influenza human;gripe humana;brotes de enfermedades;models theoretical;airports;male;pandemics;modelos teoricos;factores de riesgo;humanos;pandemias;risk factors;simulacion por computador;masculino;viaje en avion;aeropuertos;outcome assessment health care;air travel;infectious diseases;transmision de enfermedad infecciosa;evaluacion de resultado atencion de salud;humans;parasitology;disease transmission infectious;tropical medicine;computer simulation;disease outbreaks	BACKGROUND Massive growth in human mobility has dramatically increased the risk and rate of pandemic spread. Macro-level descriptors of the topology of the World Airline Network (WAN) explains middle and late stage dynamics of pandemic spread mediated by this network, but necessarily regard early stage variation as stochastic. We propose that much of this early stage variation can be explained by appropriately characterizing the local network topology surrounding an outbreak's debut location.   METHODS Based on a model of the WAN derived from public data, we measure for each airport the expected force of infection (AEF) which a pandemic originating at that airport would generate, assuming an epidemic process which transmits from airport to airport via scheduled commercial flights. We observe, for a subset of world airports, the minimum transmission rate at which a disease becomes pandemically competent at each airport. We also observe, for a larger subset, the time until a pandemically competent outbreak achieves pandemic status given its debut location. Observations are generated using a highly sophisticated metapopulation reaction-diffusion simulator under a disease model known to well replicate the 2009 influenza pandemic. The robustness of the AEF measure to model misspecification is examined by degrading the underlying model WAN.   RESULTS AEF powerfully explains pandemic risk, showing correlation of 0.90 to the transmission level needed to give a disease pandemic competence, and correlation of 0.85 to the delay until an outbreak becomes a pandemic. The AEF is robust to model misspecification. For 97 % of airports, removing 15 % of airports from the model changes their AEF metric by less than 1 %.   CONCLUSIONS Appropriately summarizing the size, shape, and diversity of an airport's local neighborhood in the WAN accurately explains much of the macro-level stochasticity in pandemic outcomes.	airports;anatomy, regional;information privacy;large;limited stage (cancer stage);network topology;norm (social);schedule (document type);self-replicating machine;simulation;simulators;stochastic process;subgroup;negative regulation of endoplasmic reticulum tubular network organization	Glenn Lawyer	2016	BMC infectious diseases	10.1186/s12879-016-1350-4	computer simulation;tropical medicine;medicine;pathology;medical microbiology;parasitology;pandemic;risk factor	ML	6.905696283223848	-72.23182095959048	77619
216603231174714febbd67998cc9726012f43fee	feature selection methods applied to severe brain damages data		Brain injuries seem to be one of the most widespread diseases. Hence, the main goal of our research was to investigate feature importance in the severe brain damages dataset according to the Glasgow Outcome Scale. This scale is recognized as one of several measures used to evaluate patients' functional ability as well as their conditions after applying brain damage therapy. The current approach is focused on an identification of a relevant subset of features with a similar influence on quality of classification models. According to the results gathered, about 12 from 42 descriptive features could be treated as important without the decrease of classification results.	feature selection	Wieslaw Paja;Krzysztof Pancerz	2017	2017 Federated Conference on Computer Science and Information Systems (FedCSIS)	10.15439/2017F382	computer science;data mining;support vector machine;machine learning;artificial intelligence;brain damage;damages;rough set;feature extraction;statistical classification;glasgow outcome scale;feature selection;pattern recognition	Vision	6.249199737265037	-77.22146038029261	77691
0e7a4aae44e432e8ad65a06f4f7987a517aa87b9	modelling doxorubicin effect in various cancer therapies by means of fractional calculus	drugs;protocols;fractional calculus doxorubicin drug modelling cancer therapy clearance dynamics equidistant bolus administration drug accumulation life threatening side effects power law titration rates logarithmic time spaced boluses;kinetic theory;technology and engineering;blood;fractional calculus;patient treatment calculus cancer drugs;drugs blood fractional calculus kinetic theory protocols	Understanding and capturing the complex kinetics of drugs and their effects in the body plays a key role in optimal and patient-safe medical treatment. Classical models for drug uptake and diffusion are limited in characterizing anomalous diffusion, memory effects and power-law clearance rates. Unlike classical modelling techniques, fractional models can represent all these and reveal unseen dynamics which have the potential to be life-threatening to the patient receiving care. This paper presents a study case of Doxorubicin drug modelling, used for cancer therapy. We present several types of titration methods and their clearance dynamics using classical and fractional models. Our results suggest that continuous titration, or equidistant bolus administration leads to drug accumulation in the body, inducing many life-threatening side-effects in the patient. By applying power-law titration rates, or equivalently, logarithmic time-spaced boluses, unbounded drug accumulation is avoided.	algorithm;bolus tracking;differintegral;emoticon;initial condition;kinetics internet protocol;parametric model;time complexity;tree accumulation	Clara M. Ionescu;Dana Copot;Robin De Keyser	2016	2016 American Control Conference (ACC)	10.1109/ACC.2016.7525094	kinetic theory;communications protocol;fractional calculus;computer science;mathematics;biological engineering;quantum mechanics	ML	9.437147984093617	-69.46881347358442	77794
18067c345a9d0a75fe51f8532dcb2d627e49c9ea	emotion inference from human body motion		The human body has evolved to perform sophisticated tasks from locomotion to the use of tools. At the same time our body movements can carry information indicative of our intentions, inter-personal attitudes and emotional states. Because our body is specialised to perform a variety of everyday tasks, in most situations emotional effects are only visible through subtle changes in the qualities of movements and actions. This dissertation focuses on the automatic analysis of emotional effects in everyday actions. In the past most efforts to recognise emotions from the human body have focused on expressive gestures which are archetypal and exaggerated expressions of emotions. While these are easier to recognise by humans and computational pattern recognisers they very rarely occur in natural scenarios. The principal contribution of this dissertation is hence the inference of emotional states from everyday actions such as walking, knocking and throwing. The implementation of the system draws inspiration from a variety of disciplines including psychology, character animation and speech recognition. Complex actions are modelled using Hidden Markov Models and motion primitives. The manifestation of emotions in everyday actions is very subtle and even humans are far from perfect at picking up and interpreting the relevant cues because emotional influences are usually minor compared to constraints arising from the action context or differences between individuals. This dissertation describes a holistic approach which models emotional, action and personal influences in order to maximise the discriminabil-ity of different emotion classes. A pipeline is developed which incrementally removes the biases introduced by different action contexts and individual differences. The resulting signal is described in terms of posture and dynamic features and classified into one of several emotion classes using statistically trained Support Vector Machines. The system also goes beyond isolated expressions and is able to classify natural action sequences. I use Level Building to segment action sequences and combine component classifications using an incremental voting scheme which is suitable for online applications. The system is comprehensively evaluated along a number of dimensions using a corpus of motion-captured actions. For isolated actions I evaluate the generalisation performance to new subjects. For action sequences I study the effects of reusing models trained on the isolated cases vs. adapting models to connected samples. The dissertation also evaluates the role of modelling the influence of individual user differences. I develop and evaluate a regression-based adaptation scheme. The results bring us an important step closer to …	action potential;computation;fear, uncertainty and doubt;hidden markov model;holism;markov chain;pipeline (computing);poor posture;port knocking;robot locomotion;speech recognition;support vector machine;text corpus;the walking dead: season two	Daniel Bernhardt	2010			computer vision;communication	AI	-2.575573408447534	-76.27942430617647	78123
0fee373a0421191a840580b44940dc7ec4d544f6	a predictive performance comparison of machine learning models for judicial cases		Artificial intelligence is currently in the center of attention of legal professionals. In recent years, a variety of efforts have been made to predict judicial decisions using different machine learning models, but no realistic performance comparison between them is available. In this paper, we conducted experiments comparing five well-known machine learning models: Ã-NN, logistic regression, bagging, random forests and SVM. Our experimental results show that the SVM model outperforms the other models over all the different settings, and the semantic information of the text in cases plays an important role in selecting features for the predicting models.	artificial intelligence;bootstrap aggregating;experiment;logistic regression;machine learning;random forest;support vector machine	Zhenyu Liu;Bingbing Jiang	2017	2017 IEEE Symposium Series on Computational Intelligence (SSCI)	10.1109/SSCI.2017.8285436	support vector machine;logistic regression;random forest;judicial opinion;machine learning;legal profession;artificial intelligence;computer science	AI	0.07279481098817117	-72.46181708238223	78485
5f71b2cac4f80c472043b4576a05df78c5be0d46	principles of bioimage informatics: focus on machine learning of cell patterns	pattern recognition;computational analysis;computer vision literature;cell segmentation;differentiated response;manual analysis infeasible;control setting;bioimage informatics concern;cell pattern;biological image;manual annotation	The field of bioimage informatics concerns the development and use of methods for computational analysis of biological images. Traditionally, analysis of such images has been done manually. Manual annotation is, however, slow, expensive, and often highly variable from one expert to another. Furthermore, with modern automated microscopes, hundreds to thousands of images can be collected per hour, making manual analysis infeasible. This field borrows from the pattern recognition and computer vision literature (which contain many techniques for image processing and recognition), but has its own unique challenges and tradeoffs. Fluorescence microscopy images represent perhaps the largest class of biological images for which automation is needed. For this modality, typical problems include cell segmentation, classification of phenotypical response, or decisions regarding differentiated responses (treatment vs. control setting). This overview focuses on the problem of subcellular location determination as a running example, but the techniques discussed are often applicable to other problems.	bioimage informatics;computer vision;digital image processing;machine learning;modality (human–computer interaction);pattern recognition	Luís Pedro Coelho;Estelle Glory-Afshar;Joshua D Kangas;Shannon Quinn;Aabid Shariff;Robert F. Murphy	2008		10.1007/978-3-642-13131-8_2	computer vision;computer science;bioinformatics;machine learning;data mining	Vision	0.9847506375051428	-68.19018439052726	78732
fb14bf1d5a864892b2bd83615d3cb71d80756c98	multilevel bayesian networks for the analysis of hierarchical health care data	bayesian network;multimorbidity;disease prediction;article letter to editor;cardiovascular disease;inter practice variation;multilevel analysis	OBJECTIVE Large health care datasets normally have a hierarchical structure, in terms of levels, as the data have been obtained from different practices, hospitals, or regions. Multilevel regression is the technique commonly used to deal with such multilevel data. However, for the statistical analysis of interactions between entities from a domain, multilevel regression yields little to no insight. While Bayesian networks have proved to be useful for analysis of interactions, they do not have the capability to deal with hierarchical data. In this paper, we describe a new formalism, which we call multilevel Bayesian networks; its effectiveness for the analysis of hierarchically structured health care data is studied from the perspective of multimorbidity.   METHODS Multilevel Bayesian networks are formally defined and applied to analyze clinical data from family practices in The Netherlands with the aim to predict interactions between heart failure and diabetes mellitus. We compare the results obtained with multilevel regression.   RESULTS The results obtained by multilevel Bayesian networks closely resembled those obtained by multilevel regression. For both diseases, the area under the curve of the prediction model improved, and the net reclassification improvements were significantly positive. In addition, the models offered considerable more insight, through its internal structure, into the interactions between the diseases.   CONCLUSIONS Multilevel Bayesian networks offer a suitable alternative to multilevel regression when analyzing hierarchical health care data. They provide more insight into the interactions between multiple diseases. Moreover, a multilevel Bayesian network model can be used for the prediction of the occurrence of multiple diseases, even when some of the predictors are unknown, which is typically the case in medicine.	area under curve;bayesian network;diabetes mellitus;entity;ephrin type-b receptor 1, human;health care;heart failure;hierarchical database model;interaction;multiple endocrine neoplasia;multiple personality disorder;network model;semantics (computer science)	Martijn Lappenschaar;Arjen Hommersom;Peter J. F. Lucas;Joep Lagro;Stefan Visscher	2013	Artificial intelligence in medicine	10.1016/j.artmed.2012.12.007	marginal model;computer science;multilevel model;machine learning;bayesian network;data mining;statistics	AI	4.978591452110779	-74.20160627184025	78788
2cd72889f1daa4ed95f43d446f405e2e439a3226	diagnosis of alzheimer's disease employing neuropsychological and classification techniques	cancer;data mining;classification algorithms;data visualization;dementia	All over the world, a large number of people are suffering from brain related diseases. Diagnosing of these diseases is the requirement of the day. Dementia is one such brain related disease. This causes loss of cognitive functions such as reasoning, memory and other mental abilities which may be due to trauma or normal ageing. Alzheimer's disease is one of the types of the dementia which accounts to 60-80% of mental disorders [1]. For the diagnosis of such diseases many tests are conducted. In this paper, the authors have collected the data of 466 subjects by conducting neuro psychological test. The subjects are classified as demented or not using machine learning techniques. The authors have preprocessed the data. The data set is classified using Naive Bayes, Jrip and Random Forest. The data set is evaluated using explorer, knowledge flow and API. WEKA tool is used for the analysis purpose. Results show Jrip and Random forest performs better compared to Naive Bayes.	application programming interface;cognition;data pre-processing;embedded system;machine learning;naive bayes classifier;random forest;weka	H. S. Sheshadri;S. R. Bhagya Shree;Murali Krishna	2015	2015 5th International Conference on IT Convergence and Security (ICITCS)	10.1109/ICITCS.2015.7292973	psychology;psychiatry;artificial intelligence;data mining	AI	6.312716053339077	-77.57781079342429	78954
8070930451eb5d1c0b2eaff9be4df45648832f17	in silico simulations of experimental protocols for cardiac modeling	human ventricular ap models experimental protocols cardiac modeling parameter model identification parameter model validation l type calcium current inactivation;cardiology bioelectric potentials calcium;mathematical model logic gates data models protocols electric potential steady state computational modeling	A mathematical model of the AP involves the sum of different transmembrane ionic currents and the balance of intracellular ionic concentrations. To each ionic current corresponds an equation involving several effects. There are a number of model parameters that must be identified using specific experimental protocols in which the effects are considered as independent. However, when the model complexity grows, the interaction between effects becomes increasingly important. Therefore, model parameters identified considering the different effects as independent might be misleading. In this work, a novel methodology consisting in performing in silico simulations of the experimental protocol and then comparing experimental and simulated outcomes is proposed for parameter model identification and validation. The potential of the methodology is demonstrated by validating voltage-dependent L-type calcium current (ICaL) inactivation in recently proposed human ventricular AP models with different formulations. Our results show large differences between ICaL inactivation as calculated from the model equation and ICaL inactivation from the in silico simulations due to the interaction between effects and/or to the experimental protocol. Our results suggest that, when proposing any new model formulation, consistency between such formulation and the corresponding experimental data that is aimed at being reproduced needs to be first verified considering all involved factors.	calcium;communications protocol;computer simulation;data validation;ionic;mathematical model;mathematics;population parameter;protocols documentation;system identification	Jesús Carro;José Félix Rodríguez;Esther Pueyo	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944920	control engineering;simulation;computer science;biological engineering	Visualization	9.85811919617357	-68.98465488415707	79350
77a64b63d7fdfc889f4617b0dcdc5d98bdece9c4	high-performance personalized heartbeat classification model for long-term ecg signal	neural networks;electrocardiography;smoothing methods;feature extraction;graphics processing units;long term ecg classification generalized regression neural network grnn graphics processor unit gpu;heart beat;instruction sets;graphics processing units heart beat electrocardiography feature extraction neural networks instruction sets smoothing methods	Long-term electrocardiogram (ECG) has become one of the important diagnostic assist methods in clinical cardiovascular domain. Long-term ECG is primarily used for the detection of various cardiovascular diseases that are caused by various cardiac arrhythmia such as myocardial infarction, cardiomyopathy, and myocarditis. In the past few years, the development of an automatic heartbeat classification method has been a challenge. With the accumulation of medical data, personalized heartbeat classification of a patient has become possible. For the long-term data accumulation method, such as the holter, it is difficult to obtain the analysis results in a short time using the original method of serial design. The pressure to develop a personalized automatic classification model is high. To solve these challenges, this paper implemented a parallel general regression neural network (GRNN) to classify the heartbeat, and achieved a 95% accuracy according to the Association for the Advancement of Medical Instrumentation. We designed an online learning program to form a personalized classification model for patients. The achieved accuracy of the model is 88% compared to the specific ECG data of the patients. The efficiency of the parallel GRNN with GTX780Ti can improve by 450 times.	artificial neural network;biological neural networks;cardiomyopathies;cardiovascular diseases;electrocardiography;heart diseases;instrument - device;myocardial infarction;myocarditis;neural network simulation;patients;personalization;statistical classification;tree accumulation	Pengfei Li;Yu Wang;Jiangchun He;Lihua Wang;Yu Tian;Tian-Shu Zhou;Tian-Chang Li;Jing-song Li	2016	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2016.2539421	speech recognition;feature extraction;computer science;artificial intelligence;machine learning;instruction set;artificial neural network	Vision	6.682711718507664	-78.58441113391882	79482
33f127fcc7c91a22ceea142be370b37aff79caf8	pattern analysis in drilling reports using optimum-path forest		Well drilling monitoring is an essential task to prevent faults, save resources, and take care of environmental and eco-planning businesses. During drilling, it is required that staff fill out a log to keep track of the activities that are currently occurring. With such data analyzed and processed, it is possible to learn how to prevent faults and take corrective actions in realtime. However, the most important information is usually stored in a free-text format, thus complicating the task of automated text mining. In this work, we introduce the Optimum-Path Forest (OPF) for sentence classification in drilling reports and compare its results against some state-of-art results. We show that OPF combined with text-based features are a compelling source to learn patterns in drilling reports.	care-of address;computation;dimensionality reduction;k-nearest neighbors algorithm;real-time computing;supervised learning;text mining;text-based (computing);tf–idf	G. J. Sousa;Daniel Carlos Guimarães Pedronette;Alexandro Baldassin;H. P. Vasantha Rupasinghe;Ivana Fabiola Rodriguez;Ivan Rizzo Guilherme;Natsume Suzuki;Luis Claudio Sugi Afonso;João Paulo Papa	2018	2018 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2018.8489232	data mining;well drilling;machine learning;feature extraction;drilling;computer science;artificial intelligence;text mining;sentence	SE	5.230487933444237	-77.43428749325756	79566
379ecb4989116817ff75600ef6f9dac535e71a6f	constructing a novel mortality prediction model with bayes theorem and genetic algorithm	model combination;bayesian statistics;intensive care unit;bayes theorem;bayesian statistical model;intensive care;mortality prediction model;glasgow coma score;genetic algorithm;prediction model;binary data;simplified acute physiology system 2nd version;mortality rate;scoring system	Intensive care is one of the most important components of the modern medical system. Healthcare professionals need to utilize intensive care resources effectively. Mortality prediction models help physicians decide which patients require intensive care the most and which do not. The Simplified Acute Physiology System 2nd version (SAPS II) is one of the most popular mortality scoring systems currently available. This study retrospectively collected data on 496 patients admitted to intensive care units from year 2000 to 2001. The average patient age was 59.96 ± 1.83 years old and 23.8% of patients died before discharge. We used these data as training data and constructed an exponential Bayesian mortality prediction model by combining BSM (Bayesian statistical model) and GA (genetic algorithm). The optimal weights and the parameters were determined with GA. Furthermore, we prospectively collected data on 142 patients for testing the new model. The average patient age for this group was 57.80 ± 3.33 years old and 21.8% patients died before discharge. The mortality prediction power of the new model was better than SAPS II (p < 0.001). The new model combining BSM and GA can manage both binary data and continuous data. The mortality rate is predicted to be high if the patient’s Glasgow coma score is less than 5. 2010 Elsevier Ltd. All rights reserved.	bayesian network;binary data;discharger;genetic algorithm;medical decision making;openbsm;software release life cycle;statistical model;time complexity	Chien-Lung Chan;Hsien-Wei Ting	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.10.094	econometrics;genetic algorithm;computer science;glasgow coma scale;machine learning;data mining;predictive modelling;bayesian statistics;bayes' theorem;mortality rate;statistics	AI	6.586196491086189	-76.44807823508415	79701
87254b4fedf1fc8a457d8d5ae2ed83da498e5ac0	a software system for giving clues of medical diagnosis to clinician	disease symptoms;controlled object;medical task analysis;system identification algorithm software system medical diagnostic clues medical decision making medical decision support system clinical dss clinician judgment early disease detection disease signs disease symptoms clinical laboratory data functional depression ecosystem failure clinical diagnosis medical task analysis patient information structuralization medical modelling method patient identification controlled object patient model disease model;medical decision support system;software system;system identification algorithm;early disease detection;clinician judgment;clinical laboratory data;software systems;medical decision making;clinical diagnosis;medical modelling method;support system;identification medical diagnostic computing decision support systems diseases;system identification;clinical dss;ecosystems;identification;disease model;decision support systems;patient identification;diseases;ecosystem failure;patient model;patient information structuralization;functional depression;disease signs;software systems medical diagnosis medical diagnostic imaging diseases decision making laboratories ecosystems clinical diagnosis information analysis medical control systems;medical diagnostic computing;early detection;information analysis;medical diagnosis;medical diagnostic clues;medical diagnostic imaging;medical control systems	The importance of developing a support system for medical decision-making in the clinical field has been pointed out for improving the accuracy and objectivity of the judgment of clinicians, and for early detection of disease. In this paper, we notice that signs, symptoms and clinical laboratory data are essential information which show the functional depression and failure of the ecosystem, and a practical software system is presented that provides physicians with clues to clinical diagnosis. First, by analysing the medical task and the structuralization of patient information, a medical modelling method is provided, where diagnosis can be considered as the identification of a patient who corresponds to the controlled object. Secondly, by using the concept of a patient model and a disease model, a system identification algorithm is proposed, and an actual system constructed for medical diagnosis is described in order to confirm the usefulness of the proposed method.	software system	Tsutomu Matsumoto;Yuki Ueda;Shigeyasu Kawaji	2002		10.1109/CBMS.2002.1011356	medicine;decision support system;pathology;computer science;software system	NLP	4.559215780326732	-78.99092391811244	79959
e2cdca93222f11ebb8cec7d5a3b303aad27e2fbc	introduction: dealing with big data-lessons from cognitive computing		Big Data analytics is an emerging area of fast-growing importance as it provides innovative ways to efficiently analyse large and complex data effectively. Many approaches have been developed to extract meaningful information and knowledge from large multidimensional data sets involving high veracity, volume, velocity, and value. Unlike traditional approaches, cognitive computing systems are not based on predetermined answers or actions to perform Big Data analytics; instead, they are trained using efficient, scalable, natural, and biologically inspired approaches, including computational intelligence and machine learning algorithms without compromising their sophistication and performance. Cognitive computing systems interact with, learn naturally from people and Big data, and thus help increase the productivity of what either humans or machines could do on their own. This Special Issue is aimed at promoting multidisciplinary research, particularly in the complementary areas of cognitive psychology, biology, and computing science for dealing with Big Data challenges across these domains. Accordingly, the articles selected in this Special Issue report a range of cognitively inspired computational approaches to deal with different aspects of Big Data analytics. Following a rigorous peer-review process, ranging from three-to-four rounds of revisions; five articles have been accepted for publication. The reviews of the final article co-authored by the guest editors were handled independently by another editorial board member. In the first article presented in this Special Issue, Wu, Pang, and Coghill propose an integrative qualitative and quantitative modelling framework for inferring biochemical systems that could help, especially biologists, better understand natural biochemical systems. The proposed framework demonstrates how the identification of biochemical systems can be performed and evolved in an integrative manner by reusing, composing, and evolving biochemical modules qualitatively, and by mutating kinetic rates quantitatively. In the next article, Du et al. propose a novel computational method to detect specific biomarkers for different groups of cancer types. The proposed method identifies specific biomarkers for a given cancer group based on different factors, including the same survival rates, the same type, and grade. The proposed methodology is thoroughly evaluated using eight cancer types and a number of benchmark microarray gene expression data sets from public databases. In the next article, Lin et al. describe a cognitively inspired psycho-linguistically motivated approach to build a quiz generation system, which combines similarity information derived from linked data and Resource Description Framework (RDF) resources. The authors effectively exploit this similarity information to gauge the difficulty of quizzes, confirming that a higher similarity among concepts leads to a more difficult quiz formulation, and vice versa. The proposed methodology is evaluated & Ahsan Abdullah aabdullah1@kau.edu.sa	algorithm;benchmark (computing);big data;cognitive computing;computation;computational intelligence;computer science;database;humans;linked data;machine learning;microarray;resource description framework;scalability;velocity (software development);veracity	Ahsan Abdullah;Amir Hussain;Imtiaz Hussain Khan	2015	Cognitive Computation	10.1007/s12559-015-9364-6	data science;machine learning;artificial intelligence;computer science;big data;cognitive computing	AI	-1.7315298226642406	-66.62896914899973	80055
917cb79f587281a693dbe39dd7db946a2c360f3e	topology-driven trend analysis for drug discovery		Abstract The primary goal of the present study is to discover new drug treatments by topology analysis of drug associations and their therapeutic group network. To this end, we collected 19,869 papers dated from 1946 to 2015 that are related to autism treatment from PubMed. We extracted 145 drugs based on MeSH terms and their synonyms (the total number is 6624) within the same ATC classification hierarchy and used them to find drug associations in the collected datasets. We introduced a new topology-driven method that incorporates various network analyses including co-word network, clique percolation, weak component, pathfinding-based analysis of therapeutic groups, and detection of important drug interaction within a clique. The present study showed that the in-depth analysis of the drug relationships extracted from the literature-based network sheds new light on drug discovery research. The results also suggested that certain drugs could be repurposed for autism treatment in the future. In particular, the results indicated that the discovered four drugs such as Tocilizumab, Tacrolimus, Prednisone, and Sulfisoxazole are worthy of further study in laboratory experiments with formal assessment of possible effects on symptoms, which may provide psychologists, physicians, and researchers with data-based scientific hypotheses in autism-drug discovery.	advanced transportation controller;algorithm;clique problem;code;entity;experiment;http 404;pathfinding;percolation;pervasive informatics;pubmed;synergy;web science;world wide web	Yanhua Lv;Ying Ding;Min Song;Zhiguang Duan	2018	J. Informetrics	10.1016/j.joi.2018.07.007	clique;autism;drug;drug discovery;network analysis;drug interaction;trend analysis;computer science;topology;literature-based discovery	ML	-2.1140901329124304	-67.6328281378314	80107
ab758ff1ee3ca69122a104a99449fe71f7b18839	a deep active survival analysis approach for precision treatment recommendations: application of prostate cancer		Abstract Survival analysis has been developed and applied in the number of areas including manufacturing, finance, economics and healthcare. In healthcare domain, usually clinical data are high-dimensional, sparse and complex and sometimes there exists few amount of time-to-event (labeled) instances. Therefore building an accurate survival model from electronic health records is challenging. With this motivation, we address this issue and provide a new survival analysis framework using deep learning and active learning with a novel sampling strategy. First, our approach provides better representation with lower dimensions from clinical features using labeled (time-to-event) and unlabeled (censored) instances and then actively trains the survival model by labeling the censored data using an oracle. As a clinical assistive tool, we introduce a simple effective treatment recommendation approach based on our survival model. In the experimental study, we apply our approach on SEER-Medicare data related to prostate cancer among African–Americans and white patients. The results indicate that our approach outperforms significantly than baseline models.	active learning (machine learning);baseline (configuration management);censoring (statistics);consistency model;deep learning;experiment;oracle database;seer-sem;sampling (signal processing);sparse matrix	Milad Zafar Nezhad;Najibesadat Sadati;Kai Yang;Dongxiao Zhu	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.07.070	active learning;data mining;censoring (statistics);oracle;deep learning;survival analysis;sampling (statistics);computer science;prostate cancer;artificial intelligence	ML	5.120972944244157	-74.72529555765719	80241
73fee316ad281d3bf0a1d059107d01d4b85289ea	predicting high-order directional drug-drug interaction relations		High-order Drug-Drug Interactions (DDI) are common particularly for elderly people. It is highly non-trivial to detect such interactions via in vivo/in vitro experiments. In this paper, we present SVM-based classification methods to predict whether a high-order directional drug-drug interaction (HoDDDI) instance is associated with adverse drug reactions (ADRs) and induced side effects. Specifically, we developed kernels for HoDDDI instances of arbitrary orders that are constructed from various single-drug information. The experiments over datasets extracted from electronic health records demonstrate that our classification methods can achieve the best F1 as 0.793.	experiment;interaction;video-in video-out	Xia Ning;Li Shen;Lang Li	2017	2017 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2017.76	kernel (linear algebra);drug;data mining;support vector machine;drug-drug interaction;in vivo;computer science	Vision	-2.74801900952945	-69.85129848302887	81080
d5caf52d55bf3921ed3e83e0f8cf362a87f58bf9	predicting dire outcomes of patients with community acquired pneumonia	resource utilization;learning algorithm;statistical model;machine learning;quality and cost of healthcare delivery;community acquired pneumonia;outcome prediction;artificial neural network	Community-acquired pneumonia (CAP) is an important clinical condition with regard to patient mortality, patient morbidity, and healthcare resource utilization. The assessment of the likely clinical course of a CAP patient can significantly influence decision making about whether to treat the patient as an inpatient or as an outpatient. That decision can in turn influence resource utilization, as well as patient well being. Predicting dire outcomes, such as mortality or severe clinical complications, is a particularly important component in assessing the clinical course of patients. We used a training set of 1601 CAP patient cases to construct 11 statistical and machine-learning models that predict dire outcomes. We evaluated the resulting models on 686 additional CAP-patient cases. The primary goal was not to compare these learning algorithms as a study end point; rather, it was to develop the best model possible to predict dire outcomes. A special version of an artificial neural network (NN) model predicted dire outcomes the best. Using the 686 test cases, we estimated the expected healthcare quality and cost impact of applying the NN model in practice. The particular, quantitative results of this analysis are based on a number of assumptions that we make explicit; they will require further study and validation. Nonetheless, the general implication of the analysis seems robust, namely, that even small improvements in predictive performance for prevalent and costly diseases, such as CAP, are likely to result in significant improvements in the quality and efficiency of healthcare delivery. Therefore, seeking models with the highest possible level of predictive performance is important. Consequently, seeking ever better machine-learning and statistical modeling methods is of great practical significance.	acquired immunodeficiency syndrome;artificial neural network;data validation;decision making;delivery of health care;machine learning;morbidity - disease rate;patients;robustness (computer science);statistical model;test case;test set;algorithm;community acquired pneumonia;inpatient	Gregory F. Cooper;Vijoy Abraham;Constantin F. Aliferis;John M. Aronis;Bruce G. Buchanan;Rich Caruana;Michael J. Fine;Janine E. Janosky;Gary Livingston;Tom M. Mitchell	2005	Journal of biomedical informatics	10.1016/j.jbi.2005.02.005	statistical model;in situ resource utilization;simulation;medicine;computer science;machine learning;data mining;medical emergency;artificial neural network	ML	6.29898419741117	-75.38058236740002	81090
b6af9e588f8f44871d6791a27f605472415ec1df	hybridization of possibility theory and supervised clustering to build dsss for classification in medicine	multiple sclerosis lesions possibility theory hybridization supervised clustering dss medicine classification fuzzy based decision support systems rule base fuzzy clustering method distance metric probabilistic framework fuzzy rules;pattern clustering;probability;fuzzy set theory;medical computing;probability decision support systems fuzzy set theory medical computing pattern clustering possibility theory;decision support systems;possibility theory;clicaldss probability possibility transformation statistical learning fuzzy clustering classification;decision support systems hybrid intelligent systems zinc	Fuzzy-based Decision Support Systems (DSSs) have gained increasing importance in medicine, since they rely on a transparent and interpretable rule base. A very attractive feature for these systems is to present their results as a set of plausible conclusions, each of them associated with a degree of possibility. In order to face this need, this work proposes a novel approach consisting in hybridization of possibility theory and a classical fuzzy clustering method, based on a distance metric interpretable in a probabilistic framework with the final aim of determining both fuzzy rules and partitions. As a proof of concept, the method has been applied to a real-life application pertaining the classification of Multiple Sclerosis Lesions. Finally, some sophistications are proposed for a future refinement, in order to improve the quality of results and the generality of applications.	cluster analysis;fuzzy clustering;possibility theory;quality of results;real life;refinement (computing);rule-based system	Marco Pota;Massimo Esposito;Giuseppe De Pietro	2012	2012 12th International Conference on Hybrid Intelligent Systems (HIS)	10.1109/HIS.2012.6421383	defuzzification;fuzzy clustering;fuzzy classification;fuzzy number;neuro-fuzzy;machine learning;pattern recognition;fuzzy measure theory;data mining;mathematics;fuzzy set operations	Robotics	1.8239096136622666	-73.88368886562101	81124
4a6c1d45e907a9001f0e8d16f809d268db6046d9	predicting icu mortality: a comparison of stationary and nonstationary temporal models	results: the stationary model usually performed best. however;one nonstationary model using large data sets performed significantly better than the stationary model. conclusion: results suggest that using a combination of stationary and nonstationary models may predict better than either alone.;bayes theorem;bayesian network;computer simulation;comparative study;predictive value of tests;artificial intelligence;roc curve	OBJECTIVE This study evaluates the effectiveness of the stationarity assumption in predicting the mortality of intensive care unit (ICU) patients at the ICU discharge.   DESIGN This is a comparative study. A stationary temporal Bayesian network learned from data was compared to a set of (33) nonstationary temporal Bayesian networks learned from data. A process observed as a sequence of events is stationary if its stochastic properties stay the same when the sequence is shifted in a positive or negative direction by a constant time parameter. The temporal Bayesian networks forecast mortalities of patients, where each patient has one record per day. The predictive performance of the stationary model is compared with nonstationary models using the area under the receiver operating characteristics (ROC) curves.   RESULTS The stationary model usually performed best. However, one nonstationary model using large data sets performed significantly better than the stationary model.   CONCLUSION Results suggest that using a combination of stationary and nonstationary models may predict better than using either alone.	bayesian network;consistency model;discharger;mortality vital statistics;patients;population parameter;projections and predictions;receiver operator characteristics;receiver operating characteristic;stationary process;time complexity;intensive care unit	Mehmet Kayaalp;Gregory F. Cooper;Gilles Clermont	2000	Proceedings. AMIA Symposium		receiver operating characteristic;statistics;bayes' theorem;data set;bayesian network;predictive value of tests;computer science	ML	6.279310546044133	-75.27568724386701	81245
4686b6bdd80a721d052f3da92620c21035aa2c68	agent-based modeling of the spread of influenza-like illness in an emergency department: a simulation study	mathematical model object oriented modeling hospitals parallel processing decision support systems computational modeling;emergency department;decision support tool agent based modeling influenza like illness emergency department virus infection mathematical modeling disease spread antibiotic resistant nosocomial infections infection control systems modeling c qt4 libraries linux operating system ordinary least squares regression patient oriented infection control policies alternate treatment streams symptomatic patient masking;least squares approximations;decision support;decision support tool;object oriented model;system modeling;regression analysis c language decision support systems diseases least squares approximations linux medical computing medical control systems physiological models;simulation agent based modeling abm decision support;computer model;agent based model;simulation;hospitals;nosocomial infection;indexing terms;influenza like illness;medical computing;decision support system;c language;computational modeling;antibiotic resistance;decision support systems;virus infection;ordinary least square;mathematical model;diseases;simulation study;linux;regression analysis;canada communicable diseases computer simulation cross infection decision support techniques emergency service hospital humans infection control influenza human least squares analysis models organizational models statistical;infection control;healthcare worker;physiological models;agent based modeling abm;object oriented modeling;parallel processing;medical control systems	The objective of this paper was to develop an agent based modeling framework in order to simulate the spread of influenza virus infection on a layout based on a representative hospital emergency department in Winnipeg, Canada. In doing so, the study complements mathematical modeling techniques for disease spread, as well as modeling applications focused on the spread of antibiotic-resistant nosocomial infections in hospitals. Twenty different emergency department scenarios were simulated, with further simulation of four infection control strategies. The agent based modeling approach represents systems modeling, in which the emergency department was modeled as a collection of agents (patients and healthcare workers) and their individual characteristics, behaviors, and interactions. The framework was coded in C + + using Qt4 libraries running under the Linux operating system. A simple ordinary least squares (OLS) regression was used to analyze the data, in which the percentage of patients that be came infected in one day within the simulation was the dependent variable. The results suggest that within the given instance con text, patient-oriented infection control policies (alternate treatment streams, masking symptomatic patients) tend to have a larger effect than policies that target healthcare workers. The agent-based modeling framework is a flexible tool that can be made to reflect any given environment; it is also a decision support tool for practitioners and policymakers to assess the relative impact of infection control strategies. The framework illuminates scenarios worthy of further investigation, as well as counterintuitive findings.	agent-based model;behavior;control theory;decision support system;infections, hospital;interaction;large;libraries;linux;mathematical model;mathematics;naruto shippuden: clash of ninja revolution 3;operating system;ordinary least squares;orthomyxoviridae;patients;qt (software);simulation;systems modeling	Marek Laskowski;Bryan C. P. Demianyk;Julia Witt;Shamir Mukhi;Marcia R. Friesen;Robert D. McLeod	2011	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2011.2163414	antibiotic resistance;simulation;systems modeling;index term;decision support system;ordinary least squares;computer science;artificial intelligence;infection control;machine learning;mathematical model;data mining;computational model;linux kernel;regression analysis;statistics	Web+IR	6.406189752870134	-72.79567542854211	81314
b33b0ae8d139a7c07382e1ab38517a9a477aec58	the logic of hypothesis generation in kinetic modeling of system biology	hypothesis generation;pathway identification hypothesis generation logic kinetic modeling system biology;nonmonotonic logic;kinetic model;computer model;biological system modeling;diseases artificial life;system biology;mathematical model;diseases;biological systems;system biology nonmonotonic logic logic of hypothesis logical model;cognition biological systems biological system modeling conferences mathematical model computational modeling;artificial life	In the past decades a lot of researches have been conducted to investigate pathways in order to increase the understanding of the mechanisms responsible for diseases and thus to find new treatments. One strategy to achieve this goal would be to inhibit or to trigger a part of the pathway that produces the disease. A strategy would be to first identify the pathway whose activity contributes to the disease and then look for drugs that inhibit or trigger the pathway acting on its internal components and reactions.	gene regulatory network;systems biology	Andrei Doncescu;Pierre Siegel	2011	2011 IEEE 23rd International Conference on Tools with Artificial Intelligence	10.1109/ICTAI.2011.158	computer science;artificial intelligence;non-monotonic logic;mathematical model;systems biology;algorithm;artificial life	Robotics	6.536134387638472	-67.53299916828212	81545
fcf8ec198e0c9f9355138f374cf7ae11080a6be1	handling temporality of clinical events for drug safety surveillance	information systems	Using longitudinal data in electronic health records (EHRs) for post-marketing adverse drug event (ADE) detection allows for monitoring patients throughout their medical history. Machine learning methods have been shown to be efficient and effective in screening health records and detecting ADEs. How best to exploit historical data, as encoded by clinical events in EHRs is, however, not very well understood. In this study, three strategies for handling temporality of clinical events are proposed and evaluated using an EHR database from Stockholm, Sweden. The random forest learning algorithm is applied to predict fourteen ADEs using clinical events collected from different lengths of patient history. The results show that, in general, including longer patient history leads to improved predictive performance, and that assigning weights to events according to time distance from the ADE yields the biggest improvement.	alyref gene;adverse reaction to drug;algorithm;bin;dav regimen;data mining;electronic health records;fourteen;handling (psychology);machine learning;personnameuse - assigned;random forest;screening procedure;sensor;weight	Jing Zhao;Aron Henriksson;Maria Kvist;Lars Asker;Henrik Boström	2015	AMIA ... Annual Symposium proceedings. AMIA Symposium		medical history;data science;random forest;temporality;medical emergency;information system;safety surveillance;computer science	ML	4.027003531378563	-75.70845386540643	81956
ce1ef37eacf1b8ff43fbe3c4d2365b40c0294ef5	investigating eating behaviours using topic models	unsupervised learning;routine behaviours;snacks topic models nutrition routine behaviours unsupervised learning;snacks;mathematical model data models vocabulary machine learning algorithms graphical models resource management;food diary data eating behaviours topic models chronic conditions diabetes obesity diet quality machine learning algorithm nutrition application uk national diet and nutrition survey rolling programme dataset;topic models;learning artificial intelligence health care;nutrition	Chronic conditions, such as diabetes and obesity are related to quality of diet. However, current research findings are conflicting with regards to the impact of snacking on diet quality. One reason for this is the lack of a clear definition of a snack or a meal. This paper presents a novel approach to understanding how foods are grouped together in eating events using a machine learning algorithm, topic models. Approaches for applying topic models to a nutrition application are discussed. A topic model is implemented for the UK National Diet and Nutrition Survey Rolling Programme dataset. The results demonstrate that the topics found are representative of typical eating events in terms of food group content and associated time of day. There is a strong potential for topic models to reveal useful patterns in food diary data that have not previously been considered.	algorithm;machine learning;topic model;vocabulary	Ruth White;William S. Harwin;William Holderbaum;Laura Johnson	2015	2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)	10.1109/ICMLA.2015.50	unsupervised learning;simulation;computer science;artificial intelligence;machine learning;topic model;nutrition	SE	3.932245071281186	-75.38501099461297	81983
1831d8626c902c2b04e450222837a9abcb89425e	strategies for containing an influenza pandemic: the case of italy	incidence reduction influenza pandemic italy antiviral prophylaxis quarantine measures;influenza vaccines personnel humans phase measurement drugs educational institutions employment surveillance testing;influenza pandemic;antiviral prophylaxis;probability;probability diseases;component;urology;individual based model;decision support systems;diseases;italy;quarantine measures;incidence reduction;social distance;artificial neural network	In this paper we introduce an individual based model for simulating the spread of an emerging influenza pandemic in Italy and testing the effectiveness of some containing strategies including vaccination, antiviral prophylaxis and quarantine measures to increase social distances. Our results show that while the probability of interrupting a large outbreak is negligible, a combination of the control measures can be effective in reducing the incidence of infection. In particular, in the worst case, an incidence reduction from about 50% to about 10% can be hopefully achieved.	best, worst and average case;incidence matrix;interrupt;simulation	Stefano Merler;Giuseppe Jurman;Cesare Furlanello;Caterina Rizzo;Antonino Bella;Marco Massari;Marta Luisa Ciofi degli Atti	2006	2006 1st Bio-Inspired Models of Network, Information and Computing Systems	10.1145/1315843.1315857	decision support system;computer science;machine learning;probability;component;artificial neural network;statistics;social distance	AI	7.586851603397312	-74.23786917333511	82204
ca386ea5f5c31705ac75cd1863faf03b6987cd0a	testing biological models for non-linear sensitivity with a programmability test		We address the question of what kind of test could be implemented to establish whether an artificial system is living or not with a computability and programmability test as advanced in [2]. We claim that necessary conditions for life are non-linear behavioural variability and sensitivity to external stimuli. We advance an algorithmic information concept of programmability as a combination of behavioural change and external control similar to the one presented in [1] in the context of self-assembly and non-DNA spatial computation. We look into evaluating, classifying and discriminating biological models from the BioModels Database (http://www.ebi.ac.uk/biomodels-main/), a centralized database of curated quantitative models of biochemical interest whose dynamical space, time-evolution and model’s reaction to their “environment” (sensitivity) is studied, leading to questions such as the robustness and param-	algorithmic information theory;biomodels database;centralized computing;computability;computation;dynamical system;heart rate variability;nonlinear system;self-assembly	Gordon Ball;Jesper Tegnér	2013		10.7551/978-0-262-31709-2-ch188	robustness (computer science);centralized database;param;theoretical computer science;nonlinear system;biomodels database;computation;computability;computer science	SE	5.498263177046029	-67.72608544341499	82233
a573b3c9b6144a4ced84c6ec80ccea54452392b2	integration of cluster analysis and granular computing for imbalanced data classification: a case study on prostate cancer prognosis in taiwan			cluster analysis;granular computing	R. J. Kuo;L. Lin;Ferani E. Zulvia;C. C. Lin	2017	Journal of Intelligent and Fuzzy Systems	10.3233/JIFS-16236	bioinformatics;data science;data mining	Robotics	3.3441053279996944	-75.47903356228763	82474
e52256377101c42b8a537661de0fd5fb0b10c3c4	description and evaluation of the variability of human color vision in an anti-piracy context			color vision;heart rate variability	Didier Doyen;Jean-Jacques Sacré;Laurent Blondé	2009			artificial intelligence;color vision;computer vision;computer science	Vision	-2.895120400485941	-79.26618933806685	82841
ebff789a1f0f6b9a91b10902cc5ea5ef753928f3	lost in localization: the need for a universal coordinate database	article letter to editor;coordinate system	One of the great advantages of neuroimaging research is the use of an established and uniform coordinate system. This 3-D coordinate system allows for the comparison of activation locations across studies. In order to capitalize upon this advantage, however, researchers must be able to find relevant studies based upon activation locations. A number of research groups have embarked upon solutions to this problem, but to date there exists no exhaustive, universal coordinate database. In this commentary we outline the nature of the problem, its current solutions, and propose alternate solutions. We close with suggestions on how those in the field can facilitate the process of developing a universal coordinate database.	neuroimaging;published comment;solutions	Jan Derrfuss;Raymond A. Mar	2009	NeuroImage	10.1016/j.neuroimage.2009.01.053	psychology;computer science;artificial intelligence;coordinate system;data mining	DB	-1.2374408356566342	-69.95891406023753	82847
556c7490316027521c674c4c20cbd00531bdf8f9	diagnosing lassa virus infection by tracking the antiviral response	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;algorithms;combinatorial libraries;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;microarrays;bioinformatics	Background Lassa fever is an acute viral hemorrhagic fever caused by the Lassa virus. It infects 300,000 to 500,000 West Africans every year, with a mortality rate among hospitalized patients of 15%. It is difficult to diagnose because its early symptoms (fever, sore throat, general malaise) often go unnoticed, or are confused with those of the common flu, malaria, or other febrile diseases. For this reason, there is considerable interest in developing tests that can detect the presence of the virus at the earliest stages of infection, when treatment is most effective. Methods such as the enzyme-linked immunosorbent assay (ELISA) or the		Ignacio S. Caballero;Gracia Bonilla;Judy Y. Yen;John H. Connor	2012		10.1186/1471-2105-13-S18-A13	biology;dna microarray;computer science;bioinformatics;virology	AI	7.968273579722972	-70.76146792857492	82977
ee11fb594c8ee7c2628b8d2c7441852637ccd7cf	efficient algorithms for fast integration on large data sets from multiple sources	health informatics;data collection;data mining;information systems and communication service;cluster analysis;management of computing and information systems;algorithms;humans;databases factual;electronic health records	BACKGROUND Recent large scale deployments of health information technology have created opportunities for the integration of patient medical records with disparate public health, human service, and educational databases to provide comprehensive information related to health and development. Data integration techniques, which identify records belonging to the same individual that reside in multiple data sets, are essential to these efforts. Several algorithms have been proposed in the literatures that are adept in integrating records from two different datasets. Our algorithms are aimed at integrating multiple (in particular more than two) datasets efficiently.   METHODS Hierarchical clustering based solutions are used to integrate multiple (in particular more than two) datasets. Edit distance is used as the basic distance calculation, while distance calculation of common input errors is also studied. Several techniques have been applied to improve the algorithms in terms of both time and space: 1) Partial Construction of the Dendrogram (PCD) that ignores the level above the threshold; 2) Ignoring the Dendrogram Structure (IDS); 3) Faster Computation of the Edit Distance (FCED) that predicts the distance with the threshold by upper bounds on edit distance; and 4) A pre-processing blocking phase that limits dynamic computation within each block.   RESULTS We have experimentally validated our algorithms on large simulated as well as real data. Accuracy and completeness are defined stringently to show the performance of our algorithms. In addition, we employ a four-category analysis. Comparison with FEBRL shows the robustness of our approach.   CONCLUSIONS In the experiments we conducted, the accuracy we observed exceeded 90% for the simulated data in most cases. 97.7% and 98.1% accuracy were achieved for the constant and proportional threshold, respectively, in a real dataset of 1,083,878 records.	algorithm;apoptosis;blocking (computing);cluster analysis;computation;dna integration;database;dendrogram;edit distance;experiment;greater than;hierarchical clustering;information sciences;literature;patients;preprocessor;process-centered design;reside;solutions;statistical cluster	Tian Mi;Sanguthevar Rajasekaran;Robert Aseltine	2012		10.1186/1472-6947-12-59	health informatics;medicine;computer science;data science;nursing;data mining;cluster analysis;information retrieval;statistics;data collection	ML	-4.130347394139628	-67.67032722309169	83502
6ab506185d4d2496bd8a15007ab41341f92788f6	analysis of adverse drug reactions using drug and drug target interactions and graph-based methods	adverse drug event;drug targeting;drug target;food and drug administration;drug drug interaction;clustering coefficient;topological methods;drug interaction;drug target ontology;adverse drug reaction	OBJECTIVE The purpose of this study was to integrate knowledge about drugs, drug targets, and topological methods. The goals were to build a system facilitating the study of adverse drug events, to make it easier to find possible explanations, and to group similar drug-drug interaction cases in the adverse drug reaction reports from the US Food and Drug Administration (FDA).   METHODS We developed a system that analyses adverse drug reaction (ADR) cases reported by the FDA. The system contains four modules. First, we integrate drug and drug target databases that provide information related to adverse drug reactions. Second, we classify drug and drug targets according to anatomical therapeutic chemical classification (ATC) and drug target ontology (DTO). Third, we build drug target networks based on drug and drug target databases. Finally, we apply topological analysis to reveal drug interaction complexity for each ADR case reported by the FDA.   RESULTS We picked 1952 ADR cases from the years 2005-2006. Our dataset consisted of 1952 cases, of which 1471 cases involved ADR targets, 845 cases involved absorption, distribution, metabolism, and excretion (ADME) targets, and 507 cases involved some drugs acting on the same targets, namely, common targets (CTs). We then investigated the cases involving ADR targets, ADME targets, and CTs using the ATC system and DTO. In the cases that led to death, the average number of common targets (NCTs) was 0.879 and the average of average clustering coefficient (ACC) was 0.067. In cases that did not lead to death, the average NCTs was 0.551, and the average of ACC was 0.039.   CONCLUSIONS We implemented a system that can find possible explanations and cluster similar ADR cases reported by the FDA. We found that the average of ACC and the average NCTs in cases leading to death are higher than in cases not leading to death, suggesting that the interactions in cases leading to death are generally more complicated than in cases not leading to death. This indicates that our system can help not only in analysing ADRs in terms of drug-drug interactions but also by providing drug target assessments early in the drug discovery process.		Shih-Fang Lin;Ke-Ting Xiao;Yu-Ting Huang;Chung-Cheng Chiu;Von-Wun Soo	2010	Artificial intelligence in medicine	10.1016/j.artmed.2009.11.002	targeted drug delivery;clustering coefficient;drug interaction	ML	-2.059654544900285	-67.48439989084693	83572
fd55972dd392789f0b4206adfc97d1df910f96ba	assessing the relevance of specific response features in the neural code				Hugo Gabriel Eyherabide;Inés Samengo	2018	Entropy	10.3390/e20110879		NLP	-2.936057595846165	-78.93368839684398	83626
0a4a81cec3fba59da6109b6527636dce44339c8d	an application of a generalized additive model for an identification of a nonlinear relation between a course of menstrual cycles and a risk of endometrioid cysts	risk factors;generalized additive model;menstrual cycle;logistic regression model;neural network	Standard methods used for an identification of risk factors are based on logistic regression models. These models disabled  to assessment a nonlinearity between a study factors and a disease occurrence. This paper presents an application of generalized  additive models for modeling of reproductive risk factors associated with endometrioid cysts. Moreover theoretical similarity  and differences between generalized additive models and neural networks was discussed. The obtained results enabled to propose  a new etiological aspect for endometrioid cysts.  	generalized additive model	Dariusz S. Radomski;Zbigniew Lewandowski;Piotr I. Roszkowski	2008		10.1007/978-3-540-68168-7_54	econometrics;machine learning;mathematics;statistics	Crypto	8.134743108634979	-74.53821054664367	83656
cc2b6372d7ece683e785574de279acbfdafc4a63	analog cmos charge model for molecular redox electron-transfer reactions and bio-chemical pathways	molecular redox electron transfer reactions;organisms;cmos integrated circuits;integrated circuit;electron transfer chain;systems biology;electron transfer;biological system modeling;cmos integrated circuit model;mathematical analysis;electrons;oxidation;mathematical modeling analog cmos charge model molecular redox electron transfer reactions biochemical pathways cmos integrated circuit model electron transfer chain biocellular mitochondrial respiration neutral oxygen reduced oxygen;semiconductor device modeling;integrated circuit modeling;system biology;mathematical model;analog cmos charge model;biochemical pathways;mathematical modeling;mathematical analysis biomolecular electronics cellular biophysics cmos integrated circuits;biocellular mitochondrial respiration;neutral oxygen;metabolic pathway;reduced oxygen;cellular biophysics;semiconductor device modeling biological system modeling integrated circuit modeling oxidation electrons mathematical model systems biology organisms cmos integrated circuits protons;biomolecular electronics;protons	Oxidation and reduction (Redox) reactions are known to constitute an electron transfer chain in many biochemical pathways in living organisms (systems). Mathematical modeling of these bio-chemical pathways is of growing focus in the emerging area of systems biology. In this brief paper an effort is made to develop a CMOS integrated circuit model for the electron transfer chain in the biochemical pathway of bio-cellular mitochondrial respiration. Integrated circuit models for protons (H+), neutral oxygen, reduced oxygen (oxygen an-ion), water (H2O) molecule formation, oxidation and reduction of molecules has been developed in the process. This new technique compares favorably to other mathematical modeling efforts in systems biology for signaling and metabolic pathways in bio-cells.	british informatics olympiad;cmos;electron;gene regulatory network;h+: the digital series;integrated circuit;mathematical model;systems biology	S. M. Rezaul Hasan;Nazmul Ula	2008	2008 IEEE International Symposium on Circuits and Systems	10.1109/ISCAS.2008.4541848	metabolic pathway;electronic engineering;computer science;mathematical model;systems biology	EDA	7.092045907584289	-66.36502832775929	83765
98de9ad3dc641287bce9fba6f41da735dd6432dc	a bayesian approach to the probability of coronary heart disease subject to the -308 tumor necrosis factor-α snp	bottom up;tumor necrosis factor alpha;functional form;tumor necrosis factor;coronary heart disease;bayesian approach;top down;bayesian inference;risk factors;signaling pathway;type 2 diabetes;quantitative method;single nucleotide polymorphism	We study the correlation of the occurrence of coronary heart disease (CHD) with the presence of the single-nucleotide polymorphism (SNP) at the -308 position of the tumor necrosis factor alpha (TNF-α) gene. We also consider the influence of the occurrence of type 2 diabetes (t2DM). Using Bayesian inference, we first pursue a bottom-up approach to compute the working hypothesis and the probabilities derivable from the data. We then pursue a top-down approach by modelling the signal pathway that causally connects the SNP with the emergence of CHD. We compute the functional form of the probability of CHD conditional on the presence of the SNP in terms of both the statistical and biochemical properties of the system. From the probability of occurrence of a disease conditional on a given risk factor, we explore the possibility of extracting information on the pathways involved in the occurrence of the disease. This is a first study that we want to systematise into a comprehensive formalism to be applied to the inference of the mechanism connecting the risk factors to the disease.	bayesian approaches to brain function;bottom-up parsing;choline dehydrogenase, mitochondrial;coronary heart disease;diabetes mellitus;diabetes mellitus, non-insulin-dependent;emergence;gene regulatory network;heart diseases;higher-order function;inference;nsa product types;neoplasms;nitroprusside;nucleotides;probability;risk factor (computing);semantics (computer science);tomography, emission-computed, single-photon;top-down and bottom-up design;tumor necrosis	Ekaterini Vourvouhaki;C. Sofia Carvalho	2011	Bio Systems	10.1016/j.biosystems.2011.03.010	biology;bioinformatics;tumor necrosis factor alpha;top-down and bottom-up design;data mining;statistics	ML	1.8101791648694376	-66.69032897197904	83788
29dbb3b0bd4bb78a2b1e7b5b88e911c63c04713f	instance driven clustering for the imputation of missing data in kdd	imputation methods;data mining;inst_clust_impute;machine learning;average imputation error;missing data	Ongoing research and development process in medical data mining have opened up versatile computer assisted approaches for effective clinical decisions. The nature and quality of the selected sample for training is largely responsible for the performance of the data mining algorithms. The large quantities of cumulative data collected from various sources suffer from qualitative deficiency factors such as inconsistency, incompleteness and redundancy. Addressing the prime problem of missing data is vital as it may introduce a bias into the model under evaluation, at times leading to inaccurate results. Imputation of missing data through instance-based clustering methodology is proposed in this paper. A complete dataset, Pima Indian Type II Diabetes, is considered for evaluation of the proposed method and its usefulness and performance are estimated through average imputation error E. The results illustrate that the proposed clustering method gives a lesser and stable error rate compared to other existing imputation methods.	cluster analysis;data mining;geo-imputation;missing data	P. Ilango;K. Vijayakumar;M. Rajasekhara Babu	2014	IJCNDS	10.1504/IJCNDS.2014.057988	missing data;computer science;data mining;imputation;statistics	ML	5.121999230107459	-75.39793782131451	83842
5336a75d8ed0aa0c8af509aceb9caff592b3b8c6	predicting disease complications using a stepwise hidden variable approach for learning dynamic bayesian networks		Predicting Diabetes Type 2 Mellitus (T2DM) complications such as retinopathy and liver disease is still a challenge despite being a growing public health concern worldwide. This is due to the complex interactions between complications and other features, as well as between the different complications, themselves. What is more, there are likely to be many unmeasured effects that impact the disease progression of different patients. Probabilistic graphical models such as Dynamic Bayesian Networks (DBNs) have demonstrated much promise in the modeling of disease progression and they can naturally incorporate hidden (latent) variables using the EM algorithm. Unlike deep learning approaches that attempt to model complex interactions in data by using a large number of hidden variables, we adopt a different approach. We are interested in models that not only capture unmeasured effects but are also transparent in how they model data so that knowledge about disease processes can be extracted and trust in the model can be maintained by clinicians. As a result, we have developed a step-wise hidden variable structure learning process that incrementally adds hidden variables based on the IC* algorithm. To the best of our knowledge, this is the first study for classifying disease complication using a step-wise learning methodology for identifying hidden and T2DM features with a DBN structure from clinical data. Our extensive set of experiments show that the proposed method improves classification accuracy, identifying the correct number of hidden variables, and targeting their precise location within the network structure.	color gradient;deep learning;dynamic bayesian network;expectation–maximization algorithm;experiment;graphical model;hidden variable theory;interaction;stepwise regression	Leila Yousefi;Allan Tucker;Mashael Al-luhaybi;Lucia Sacchi;Riccardo Bellazzi;Luca Chiovato	2018	2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)	10.1109/CBMS.2018.00026	expectation–maximization algorithm;dynamic bayesian network;data modeling;deep learning;data mining;hidden variable theory;computer science;disease complication;disease;graphical model;artificial intelligence	ML	3.9115076993657696	-73.61575514254123	84060
f26fbd4d769a3c003a804e15a277595a21637c0c	a simulation model of colorectal cancer surveillance and recurrence	lung cancer;sensitivity and specificity;health informatics;treatment;disease progression;surveillance;outcomes;barrett s;neoplasm recurrence local;randomized controlled trials as topic;information systems and communication service;recurrence;epidemiological monitoring;management of computing and information systems;reproducibility of results;predictive value of tests;colorectal cancer;model;cost effectiveness;medicine;humans;patient outcome assessment;follow up;modeling;prognosis;computer simulation;breast cancer;calibration;colorectal neoplasms	BACKGROUND Approximately one-third of those treated curatively for colorectal cancer (CRC) will experience recurrence. No evidence-based consensus exists on how best to follow patients after initial treatment to detect asymptomatic recurrence. Here, a new approach for simulating surveillance and recurrence among CRC survivors is outlined, and development and calibration of a simple model applying this approach is described. The model's ability to predict outcomes for a group of patients under a specified surveillance strategy is validated.   METHODS We developed an individual-based simulation model consisting of two interacting submodels: a continuous-time disease-progression submodel overlain by a discrete-time Markov submodel of surveillance and re-treatment. In the former, some patients develops recurrent disease which probabilistically progresses from detectability to unresectability, and which may produce early symptoms leading to detection independent of surveillance testing. In the latter submodel, patients undergo user-specified surveillance testing regimens. Parameters describing disease progression were preliminarily estimated through calibration to match five-year disease-free survival, overall survival at years 1-5, and proportion of recurring patients undergoing curative salvage surgery from one arm of a published randomized trial. The calibrated model was validated by examining its ability to predict these same outcomes for patients in a different arm of the same trial undergoing less aggressive surveillance.   RESULTS Calibrated parameter values were consistent with generally observed recurrence patterns. Sensitivity analysis suggested probability of curative salvage surgery was most influenced by sensitivity of carcinoembryonic antigen assay and of clinical interview/examination (i.e. scheduled provider visits). In validation, the model accurately predicted overall survival (59% predicted, 58% observed) and five-year disease-free survival (55% predicted, 53% observed), but was less accurate in predicting curative salvage surgery (10% predicted; 6% observed).   CONCLUSIONS Initial validation suggests the feasibility of this approach to modeling alternative surveillance regimens among CRC survivors. Further calibration to individual-level patient data could yield a model useful for predicting outcomes of specific surveillance strategies for risk-based subgroups or for individuals. This approach could be applied toward developing novel, tailored strategies for further clinical study. It has the potential to produce insights which will promote more effective surveillance-leading to higher cure rates for recurrent CRC.	agent-based model;cea assay;calibration;color gradient;colorectal carcinoma;cyclic redundancy check;data recovery;disease-free survival;fifty nine;interaction;markov chain;overall survival;patients;population parameter;progressive disease;randomized algorithm;recurrence relation;recurrent disease;recurrent neural network;schedule (document type);scientific publication;simulation;stand up to cancer;survivors	Johnie Rose;Knut Augestad;Chung Yin Kong;Neal J. Meropol;Michael W. Kattan;Qingqing Hong;Xuebei An;Gregory S. Cooper	2014		10.1186/1472-6947-14-29	computer simulation;health informatics;calibration;simulation;systems modeling;cost-effectiveness analysis;medicine;pathology;colorectal cancer;breast cancer;nursing;predictive value of tests;surgery	AI	7.110143916373151	-74.03594535505128	84255
31ca5df6712cdbca30ca231d2631c270b78f402c	virapops2 supports the influenza virus reassortments	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;computational biology bioinformatics;uk phd theses thesis;life sciences;uk research reports;medical journals;computer appl in life sciences;europe pmc;biomedical research;bioinformatics	For over 400 years, due to the reassortment of their segmented genomes, influenza viruses evolve extremely quickly and cause devastating epidemics. This reassortment arises because two flu viruses can infect the same cell and therefore the new virions’ genomes will be composed of segment reassortments of the two parental strains. A treatment developed against parents could then be ineffective if the virions’ genomes are different enough from their parent’s genomes. It is therefore essential to simulate such reassortment phenomena to assess the risk of apparition of new flu strain. So we decided to upgrade the forward simulator VIRAPOPS, containing already the necessary options to handle non-segmented viral populations. This new version can mimic single or successive reassortments, in birds, humans and/or swines. Other options such as the ability to treat populations of positive or negative sense viral RNAs, were also added. Finally, we propose output options giving statistics of the results. In this paper we present a new version of VIRAPOPS which now manages the viral segment reassortments and the negative sense single strain RNA viruses, these two issues being the cause of serious public health problems.	genome;humans;orthomyxoviridae;population;simulation;simulators;virus	Michel Petitjean;Anne Vanet	2014		10.1186/1751-0473-9-18	biology;medical research;computer science;bioinformatics;virology	Comp.	7.33378807128991	-69.75682609551721	84629
6a3a3da3ed4abfbeb15751f5286ce0d0187cdf47	three-dimensional cardiovascular imaging-genetics: a mass univariate framework		Motivation Left ventricular (LV) hypertrophy is a strong predictor of cardiovascular outcomes, but its genetic regulation remains largely unexplained. Conventional phenotyping relies on manual calculation of LV mass and wall thickness, but advanced cardiac image analysis presents an opportunity for high-throughput mapping of genotype-phenotype associations in three dimensions (3D).   Results High-resolution cardiac magnetic resonance images were automatically segmented in 1124 healthy volunteers to create a 3D shape model of the heart. Mass univariate regression was used to plot a 3D effect-size map for the association between wall thickness and a set of predictors at each vertex in the mesh. The vertices where a significant effect exists were determined by applying threshold-free cluster enhancement to boost areas of signal with spatial contiguity. Experiments on simulated phenotypic signals and SNP replication show that this approach offers a substantial gain in statistical power for cardiac genotype-phenotype associations while providing good control of the false discovery rate. This framework models the effects of genetic variation throughout the heart and can be automatically applied to large population cohorts.   Availability and implementation The proposed approach has been coded in an R package freely available at https://doi.org/10.5281/zenodo.834610 together with the clinical data used in this work.   Contact declan.oregan@imperial.ac.uk.   Supplementary information Supplementary data are available at Bioinformatics online.	academy;atlases;bioinformatics;crew resource management, healthcare;eaf2 gene;experiment;gene expression regulation;genome-wide association study;geographic information systems;gtk#;heart;high-throughput computing;hypertrophy;image analysis;interaction;kerrison predictor;logical volume management;mental association;nitroprusside;r language;resonance;ross quinlan;snp array;science;sensor;solid modeling;synthetic intelligence;thickness (graph theory);throughput;variation (genetics);ventricular dysfunction, left;vertex;funding grant	Carlo Biffi;Antonio de Marvao;Mark I Attard;Timothy Dawes;Nicola Whiffin;Wenjia Bai;Wenzhe Shi;Catherine Francis;Hannah Verena Meyer;Rachel Buchan;Stuart A. Cook;Daniel Rueckert;Declan P. O'Regan	2018		10.1093/bioinformatics/btx552	contiguity;data mining;false discovery rate;statistical power;computer science;imaging genetics;regression;population;magnetic resonance imaging;univariate	Comp.	9.697875535763975	-73.90446843223965	84776
393fe43bf1bf28f1e7636cc9d74716ecaf00335f	bayesian modeling of unknown diseases for biosurveillance	simulation experiment;disease outbreak;bayes theorem;bayesian approach;algorithms;bayesian model;computer simulation	This paper investigates Bayesian modeling of unknown causes of events in the context of disease-outbreak detection. We introduce a Bayesian approach that models and detects both (1) known diseases (e.g., influenza and anthrax) by using informative prior probabilities and (2) unknown diseases (e.g., a new, highly contagious respiratory virus that has never been seen before) by using relatively non-informative prior probabilities. We report the results of simulation experiments which support that this modeling method can improve the detection of new disease outbreaks in a population. A key contribution of this paper is that it introduces a Bayesian approach for jointly modeling both known and unknown causes of events. Such modeling has broad applicability in medical informatics, where the space of known causes of outcomes of interest is seldom complete.	bayesian network;biosurveillance;experiment;informatics (discipline);information;murine sarcoma viruses;parkinson disease;probability;simulation	Yanna Shen;Gregory F. Cooper	2009	AMIA ... Annual Symposium proceedings. AMIA Symposium		biosurveillance;data mining;bayes' theorem;health informatics;disease;population;bayesian probability;bayesian inference;computer science	Logic	5.109872670138653	-70.32152801872081	84848
680c8ff2e47f2659d665e5a7f5ecf53b4fd87712	predictive models for integrating clinical and genomic data			predictive modelling	Sanjoy Dey;Rohit Gupta;Michael Steinbach;Vipin Kumar	2015		10.1201/b18588-16	computer science	ML	0.9543086631396097	-66.73580127725549	85631
af30d276ad004dc64df61c07be4d48adf5231d74	dynamical modeling of chronic myeloid leukemia progression and the development of mutations	drugs;drug delivery systems;mathematical model immune system cancer drugs stem cells biological system modeling;clinical data dynamical modeling chronic myeloid leukemia progression mutation development mathematical models short term dynamics drugs imatinib cessation treatment human cancer molecularly targeted therapy biology knowledges dynamical properties theoretical analysis;cancer;chronic myeloid leukemia;resistance mutation;mathematical analysis;blood;mathematical modeling;resistance mutation chronic myeloid leukemia mathematical modeling;mathematical analysis blood cancer drug delivery systems drugs	Mathematical models have already been developed to describe the dynamics of the chronic myeloid leukemia (CML). However, most of them mainly focus on the short term dynamics and response to the treatment with drugs such as imatinib or the relapse upon cessation of treatment. In this paper, we aim to study the dynamics of CML which represents the first human cancer for the molecularly targeted therapy again. First, we propose a new mathematical model of CML which based on the known biology knowledges. And then we analyze its qualitative and dynamical properties. Combining the clinical data and theoretical analysis, we demonstrate that our model is biologically plausible and also computationally viable. Second, we demonstrate that this model can characterize the dynamics of the CML infection and the development of mutation, and is also validated by clinical data.	chronic electrode implant;color gradient;mathematical model;mutation (genetic algorithm)	Ben-gong Zhang;Gouhei Tanaka;Luonan Chen;Kazuyuki Aihara	2012	2012 IEEE International Conference on Bioinformatics and Biomedicine Workshops	10.1109/BIBMW.2012.6470294	pharmacology;biology;virology;resistance mutation;mathematical model;immunology;cancer	Robotics	8.594111854112658	-68.4680510775857	85945
39ae6448f678ba399d6a391da2d0a06d76245bb7	detection of anomalous activity in diabetic patients using graph-based approach.				Ramesh Paudel;William Eberle;Douglas A. Talbert	2017			artificial intelligence;machine learning;computer science;graph	Vision	7.5848507362588515	-78.75325400082889	86515
53953ed97fe0977397b1035acca1e1f11f9a7cb8	subtyping: what it is and its role in precision medicine	analytical models;electronic health record precision medicine disease subtyping verotype;disease subtyping;precision control;lungs;electronic health record;genetics;trajectory;medical services;open systems electronic health records genetics medicine;open computational problem subtyping precision medicine gene variability electronic health record ehr data;precision medicine;diseases;verotype;medical diagnostic imaging;diseases precision control trajectory medical diagnostic imaging analytical models genetics medical services	Precision medicine is an emerging approach that considers variability in genes, environment, and lifestyle in order to better treat individuals. This article gives an overview of the diverse approaches to subtyping, from early accounts based on clinical practice to more recent approaches that focus on computationally derived subtypes based on molecular and electronic health record (EHR) data. This field is expansive and growing rapidly; the authors juxtapose approaches taken by different communities and highlight examples of significant open computational problems.	computation;computational problem;heart rate variability;precision medicine	Suchi Saria;Anna Goldenberg	2015	IEEE Intelligent Systems	10.1109/MIS.2015.60	bioinformatics;trajectory;precision medicine	SE	6.445078617704594	-69.10480924127427	86706
b78ddb95df598dbbe7b96f8aae1b20feeba8939f	a gene–protein–mirna electronic oscillator	transcription gene mirna mrna degradation protein oscillator repression;oscillators rna proteins voltage control transistors integrated circuit modeling	This brief presents an electronic oscillator circuit model mimicking a unique relationship among six genes. Three out of the six genes are employed to synthesize proteins, while the remaining three are their complementary micro-ribonucleic acid (miRNA) genes degrading the messenger-RNA transcripts at the translational level of gene expression. The circuit runs without any external stimulus because the transcription of each gene is controlled by another gene in forming the oscillatory network. A feasibility procedure is also presented, which can be used to fabricate the proposed gene–protein–miRNA electronic oscillator in the biological domain. This research is valuable in designing complex synthetic gene regulatory networks and analyzing their biological behavior.	acid;amplifier;artificial gene synthesis;electronic circuit;electronic oscillator;emitter-coupled logic;gene regulatory network;interaction;lambert's cosine law;network switch;protein data bank;synthetic intelligence;transcription (software)	Sadia Alam;S. M. Rezaul Hasan	2017	IEEE Transactions on Circuits and Systems II: Express Briefs	10.1109/TCSII.2016.2618366	electronic engineering;computer science;bioinformatics	Visualization	7.010946647953104	-66.1551069439567	86992
3cb5df4418f8567cda4f683a58c8e23f5aa86eb1	alternating decision trees for early diagnosis of dengue fever		Dengue fever is a flu-like illness spread by the bite of an infected mosquito which is fast emerging as a major health problem. Timely and cost effective diagnosis using clinical and laboratory features would reduce the mortality rates besides providing better grounds for clinical management and disease surveillance. We wish to develop a robust and effective decision tree based approach for predicting dengue disease. Our analysis is based on the clinical characteristics and laboratory measurements of the diseased individuals. We have developed and trained an alternating decision tree with boosting and compared its performance with C4.5 algorithm for dengue disease diagnosis. Of the 65 patient records a diagnosis establishes that 53 individuals have been confirmed to have dengue fever. An alternating decision tree based algorithm was able to differentiate the dengue fever using the clinical and laboratory data with number of correctly classified instances as 89%, F-measure of 0.86 and receiver operator characteristics (ROC) of 0.826 as compared to C4.5 having correctly classified instances as 78%,h F-measure of 0.738 and ROC of 0.617 respectively. Alternating decision tree based approach with boosting has been able to predict dengue fever with a higher degree of accuracy than C4.5 based decision tree using simple clinical and laboratory features. Further analysis on larger data sets is required to improve the sensitivity and specificity of the alternating decision trees. 1 ar X iv :1 30 5. 73 31 v2 [ cs .L G ] 5 J un 2 01 3	alternating decision tree;c4.5 algorithm;sensitivity and specificity	M. Naresh Kumar	2013	CoRR		simulation;data mining	ML	6.820697967003151	-77.07066172891128	87580
83b1b934bd38bbcee39678569106ad66e1b7588e	on the prediction of glucose concentration under intra-patient variability in type 1 diabetes: a monotone systems approach	type 1 diabetes;compartmental models;glucose insulin models;blood glucose prediction;interval simulation	Insulin therapy in type 1 diabetes aims to mimic the pattern of endogenous insulin secretion found in healthy subjects. Glucose-insulin models are widely used in the development of new predictive control strategies in order to maintain the plasma glucose concentration within a narrow range, avoiding the risks of high or low levels of glucose in the blood. However, due to the high variability of this biological process, the exact values of the model parameters are unknown, but they can be bounded by intervals. In this work, the computation of tight glucose concentration bounds under parametric uncertainty for the development of robust prediction tools is addressed. A monotonicity analysis of the model states and parameters is performed. An analysis of critical points, state transformations and application of differential inequalities are proposed to deal with non-monotone parameters. In contrast to current methods, the guaranteed simulations for the glucose-insulin model are carried out by considering uncertainty in all the parameters and initial conditions. Furthermore, no time-discretisation is required, which helps to reduce the computational time significantly. As a result, we are able to compute a tight glucose envelope that bounds all the possible patient's glycemic responses with low computational effort.	computation;diabetes mellitus;diabetes mellitus, insulin-dependent;discretization;glucose;initial condition;patients;plasma active;simulation;spatial variability;time complexity;cell transformation;insulin secretion;monotone	Diego de Pereda;Sergio Romero-Vivo;Beatriz Ricarte;Jorge Bondia	2012	Computer methods and programs in biomedicine	10.1016/j.cmpb.2012.05.012	mathematical optimization;mathematics;diabetes mellitus	ML	9.305961752741343	-71.53236374016271	87702
3aaa6e31de6a631afe866ae9a348c33f91d6f439	sequential classification for microarray and clinical data	dna;clinical data;prolonged intensive care sequential classification object classification surgical decision making clinical patient data lab data cdna microarray expression data tissue samples;surgery error analysis decision making neoplasms costs statistical analysis clinical trials neurons artificial neural networks bioinformatics;patient care;medical computing;patient care pattern classification surgery medical computing decision making dna;intensive care;surgery;pattern classification;cdna microarray;high risk	Sequential classification uses in a stepwise process only part of the data (evidence) for partial classification, i.e., classifying only objects with sufficient evidence and leaving the rest unclassified. In the following steps the procedure is repeated using additional data until all objects are classified. This is especially useful when data become available only at certain points in time, as in surgical decision making, i.e., clinical patient data, lab data, or cDNA microarray expression data from tissue samples become available before, during and after the operation. Surgeons are interested in classifying patients into low or high risk groups, which might need special measures, e.g., prolonged intensive care.	microarray;stepwise regression	Günter Tusch	2005	2005 IEEE Computational Systems Bioinformatics Conference - Workshops (CSBW'05)	10.1109/CSBW.2005.123	biology;bioinformatics;data mining;genetics;dna	Visualization	8.359664352934002	-76.59944713325945	88034
812f5b93afc47e4d0526ae664ed8c97860b5b9b5	using kernel methods and model selection for prediction of preterm birth		We describe an application of machine learning to the problem of predicting preterm birth. We conduct a secondary analysis on a clinical trial dataset collected by the National Institute of Child Health and Human Development (NICHD) while focusing our attention on predicting different classes of preterm birth. We compare three approaches for deriving predictive models: a support vector machine (SVM) approach with linear and non-linear kernels, logistic regression with different model selection along with a model based on decision rules prescribed by physician experts for prediction of preterm birth. Our approach highlights the pre-processing methods applied to handle the inherent dynamics, noise and gaps in the data and describe techniques used to handle skewed class distributions. Empirical experiments demonstrate significant improvement in predicting preterm birth compared to past work.	experiment;kernel method;logistic regression;machine learning;model selection;nonlinear system;predictive modelling;preprocessor;support vector machine	Ilia Vovsha;Ansaf Salleb-Aouissi;Anita Raja;Thomas Koch;Alex Rybchuk;Axinia Radeva;Ashwath Rajan;Yiwen Huang;Hatim Diab;Ashish Tomar;Ronald J. Wapner	2016			econometrics;computer science;machine learning;data mining	ML	5.707597550540821	-75.54068368231307	88195
e001a6a9b447753a69ef708a1ce25b2c3ec2b3d9	cancer therapy design based on pathway logic	cancer therapy	MOTIVATION Cancer encompasses various diseases associated with loss of cell cycle control, leading to uncontrolled cell proliferation and/or reduced apoptosis. Cancer is usually caused by malfunction(s) in the cellular signaling pathways. Malfunctions occur in different ways and at different locations in a pathway. Consequently, therapy design should first identify the location and type of malfunction to arrive at a suitable drug combination.   RESULTS We consider the growth factor (GF) signaling pathways, widely studied in the context of cancer. Interactions between different pathway components are modeled using Boolean logic gates. All possible single malfunctions in the resulting circuit are enumerated and responses of the different malfunctioning circuits to a 'test' input are used to group the malfunctions into classes. Effects of different drugs, targeting different parts of the Boolean circuit, are taken into account in deciding drug efficacy, thereby mapping each malfunction to an appropriate set of drugs.	apoptosis;boolean algebra;boolean circuit;cell cycle control;cell proliferation;cell signaling;class;drug combinations;gene regulatory network;grammatical framework;growth factor;interaction;logic gate;logical connective;neoplasms;uncontrolled format string;cellular targeting	Ritwik Layek;Aniruddha Datta;Michael L. Bittner;Edward R. Dougherty	2011	Bioinformatics	10.1093/bioinformatics/btq703	biology	Comp.	7.868253051583562	-68.2491247255634	88334
924fe524eb360c5dfacc4a209001acc1ad10f682	a comparison of models for predicting early hospital readmissions	penalized methods;deep learning;random forest;predictive models;electronic health records;early readmission	Risk sharing arrangements between hospitals and payers together with penalties imposed by the Centers for Medicare and Medicaid (CMS) are driving an interest in decreasing early readmissions. There are a number of published risk models predicting 30day readmissions for particular patient populations, however they often exhibit poor predictive performance and would be unsuitable for use in a clinical setting. In this work we describe and compare several predictive models, some of which have never been applied to this task and which outperform the regression methods that are typically applied in the healthcare literature. In addition, we apply methods from deep learning to the five conditions CMS is using to penalize hospitals, and offer a simple framework for determining which conditions are most cost effective to target.	deep learning;financial risk modeling;medicare;patients;population;predictive modelling;scientific publication	Joseph Futoma;Jonathan Morris;Joseph Lucas	2015	Journal of biomedical informatics	10.1016/j.jbi.2015.05.016	random forest;simulation;medicine;computer science;machine learning;data mining;deep learning;predictive modelling	ML	6.36676840736031	-75.55282994892845	88454
33e79061345188ee7e6a92597023e376e455720f	heart rate topic models		A key challenge in reducing the burden of cardiovascular disease is matching patients to treatments that are most appropriate for them. Different cardiac assessment tools have been developed to address this goal. Recent research has focused on heart rate motifs, i.e., short-term heart rate sequences that are overor under-represented in long-term electrocardiogram (ECG) recordings of patients experiencing cardiovascular outcomes, which provide novel and valuable information for risk stratification. However, this approach can leverage only a small number of motifs for prediction and results in difficult to interpret models. We address these limitations by identifying latent structure in the large numbers of motifs found in long-term ECG recordings. In particular, we explore the application of topic models to heart rate time series to identify functional sets of heart rate sequences and to concisely describe patients using task-independent features for various cardiovascular outcomes. We evaluate the approach on a large collection of real-world ECG data, and investigate the performance of topic mixture features for the prediction of cardiovascular mortality. The topics provided an interpretable representation of the recordings and maintained valuable information for clinical assessment when compared with motif frequencies, even after accounting for commonly used clinical risk scores.	level structure;sequence motif;sparse matrix;time series;topic model	Alexander Van Esbroeck;Chih-Chun Chia;Zeeshan Syed	2012			bioinformatics;data science;data mining	AI	4.276618647818252	-74.12449276009758	89106
0071b3ec0eb62d81af94e821eb2f272a230fe3fd	development of the cardiac failure expert system for chinese medicine diagnosis based on database	patient diagnosis;intelligence information processing technology cardiac failure expert system chinese medicine diagnosis traditional chinese medicine database technology;database management systems;cardiology;patient diagnosis cardiology database management systems medical expert systems;medical expert systems;medical diagnostic imaging standards databases computers expert systems heart hospitals;chinese medicine expert system computer and database technology cardiac failure	On the basis of the modern literature and clinical cases investigation of CF, and under the guidance of the theory of traditional Chinese medicine, we screen the diagnosis factors and kinds of syndromes and symptoms by using the computer technology, database technology and intelligence information processing technology (as: vague-mathematics, Bayes'method and artificial neural network, and so on) and establish the respective database, and develop the cardiac failure expert system. This study refered to a Cardiac failure Chinese medicine expert system under the guidance of the theory of traditional Chinese medicine, which combined the computer technology, intelligent information processing technology and Chinese medicine theory, it achieves intelligentification of Chinese medicine symptoms' diagnosis of Cardiac failure. The core-modular of this system is the modular of symptoms' diagnosis, it can do distinguish diagnosis of the Chinese medicine symptoms by coding and defmiting the diagnosis factors and using the computer programme. This system is profit to elevate the clinical diagnosis' normalization of medical worker, and the clinician's accuratissime and speed of differentiation of symptoms and signs.	artificial neural network;computer program;expert system;information processing;vagueness	Jiancheng He;Xiaoqian Li;Fang Hong;Hongjuan Ding;Xuebin Cao;Yuanhui Hu	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732639	computer science	Robotics	4.641493621471516	-78.97042789721213	89340
ce587e6f68df23fc81aa70f4a8c8bc6effc23c21	modelling gradients using petri nets	professor maciej koutny;eprints newcastle university;open access	Motivated by the graded posteriorization during the AP axis development in the frog Xenopus laevis, we propose an abstract Petri net model for the formation of a gradient of proteins in a chain of cells.	apache axis;image gradient;petri net	Laura M. F. Bertens;Jetty Kleijn;Maciej Koutny;Fons J. Verbeek	2010			simulation;computer science;artificial intelligence;process architecture;petri net	Robotics	6.270034989717406	-67.99440235699076	89530
043502d9e51428c11fe17fe7a8c6ad0f65d882ff	analyzing occupational stress based on inner composition alignment algorithm	complex networks;occupational stress;standards;electrodes;time series analysis;electroencephalography;algorithm design and analysis	To distinguish different level of occupational stress of students and nurses, this paper applied inner composition alignment (IOTA) algorithm to analyse differences between the students' Electroencephalogram (EEG) and the nurses' EEG and to explore the influence of occupational stress on EEG. First, we selected nine cases of students EEG data and the same quantity of nurses EEG data from General Hospital of Nanjing Military Region, then, constructed two different brain networks by utilizing IOTA method and analyzed the networks characteristics which include IOTA coefficient, Clustering coefficient and Average degree. In order to verify the accuracy of the results, we used SPSS to analyse the significant difference hypothesis testing. The IOTA coefficients are inconsistent between students' EEG and nurses' EEG. Compared with the students, nurses' brain network is more complex. The level of occupational stress of nurses is higher than students. The IOTA algorithm can significantly distinguish occupational stress between students and nurses, and all results demonstrated the validity of the algorithm.	algorithm;clustering coefficient;electroencephalography;spss	Zhen Shi;Shitong Wang;Jun Wang;Jiafei Dai;Fengzhen Hou;Jin Li	2016	2016 9th International Congress on Image and Signal Processing, BioMedical Engineering and Informatics (CISP-BMEI)	10.1109/CISP-BMEI.2016.7852947	algorithm design;electroencephalography;computer science;electrode;artificial intelligence;time series;mathematics;complex network;statistics	ML	4.556851358929124	-76.03364726740298	90029
69cce9edb5e0981d015d12b47bb6b058485efd6f	multiple-time-series clinical data processing for classification with merging algorithm and statistical measures	time series biomedical measurement data mining medical signal processing signal classification statistical analysis support vector machines;support vector machines;prediction algorithms;accuracy;classification algorithms;time 120 d single measurement results multiple measurements random forest regression hcc recurrence mmsvm radial basis function kernels multiple measurements support vector machine hepatocellular carcinoma patients multiple measurements clinical outcome prediction classification algorithms traditional data processing method patient conditions statistical measures merging algorithm multiple time series clinical data processing;merging;predictive models;biomedical measurement predictive models support vector machines merging classification algorithms prediction algorithms accuracy;biomedical measurement	A description of patient conditions should consist of the changes in and combination of clinical measures. Traditional data-processing method and classification algorithms might cause clinical information to disappear and reduce prediction performance. To improve the accuracy of clinical-outcome prediction by using multiple measurements, a new multiple-time-series data-processing algorithm with period merging is proposed. Clinical data from 83 hepatocellular carcinoma (HCC) patients were used in this research. Their clinical reports from a defined period were merged using the proposed merging algorithm, and statistical measures were also calculated. After data processing, multiple measurements support vector machine (MMSVM) with radial basis function (RBF) kernels was used as a classification method to predict HCC recurrence. A multiple measurements random forest regression (MMRF) was also used as an additional evaluation/classification method. To evaluate the data-merging algorithm, the performance of prediction using processed multiple measurements was compared to prediction using single measurements. The results of recurrence prediction by MMSVM with RBF using multiple measurements and a period of 120 days (accuracy 0.771, balanced accuracy 0.603) were optimal, and their superiority to the results obtained using single measurements was statistically significant (accuracy 0.626, balanced accuracy 0.459, P <; 0.01). In the cases of MMRF, the prediction results obtained after applying the proposed merging algorithm were also better than single-measurement results (P <; 0.05). The results show that the performance of HCC-recurrence prediction was significantly improved when the proposed data-processing algorithm was used, and that multiple measurements could be of greater value than single	algorithm;forecast of outcome;human-centered computing;liver carcinoma;merge;multiple endocrine neoplasia;multiple congenital anomalies;numerical weather prediction;patients;radial (radio);radial basis function;random forest;statistical model;support vector machine;time series	Yi-Ju Tseng;Xiao-Ou Ping;Ja-Der Liang;Pei-Ming Yang;Guan-Tarn Huang;Feipei Lai	2015	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2014.2357719	statistical classification;support vector machine;prediction;computer science;machine learning;pattern recognition;data mining;accuracy and precision;predictive modelling	ML	7.805644922921591	-77.7235828273052	90315
c965f3afc3f2a6733088265b383f0a5bb4640c89	medical data mining for early deterioration warning in general hospital wards	survival rate;early warning system;measurement error;high dimensionality;patient treatment alarm systems data mining;data stream;real time;class imbalance;clinical trial;data mining;logistic regression;cyber physical systems;exploratory undersampling;early warning;moving average;retrospective study;hospitals bagging prediction algorithms logistics data mining accuracy real time systems;early warning algorithm medical data mining early deterioration warning clinical deterioration early warning system general hospital wards electronic medical record retrospective study multidisciplinary cyber physical systems clinical science nursing staff;clinical study;patient treatment;bootstrap aggregating;ema exponential moving average early warning system logistic regression bootstrap aggregating exploratory undersampling;electronic medical record;alarm systems;early detection;ema exponential moving average;real time systems	Data mining on medical data has great potential to improve the treatment quality of hospitals and increase the survival rate of patients. Every year, $4$ -- $17\%$ of patients undergo cardiopulmonary or respiratory arrest while in hospitals. Early prediction techniques have become an apparent need in many clinical area. Clinical study has found early detection and intervention to be essential for preventing clinical deterioration in patients at general hospital units. In this paper, based on data mining technology, we propose an early warning system (EWS) designed to identify the signs of clinical deterioration and provide early warning for serious clinical events. Our EWS is designed to provide reliable early alarms for patients at the general hospital wards (GHWs). EWS automatically identifies patients at risk of clinical deterioration based on their existing electronic medical record. The main task of EWS is a challenging classification problem on high-dimensional stream data with irregular, multi-scale data gaps, measurement errors, outliers, and class imbalance. In this paper, we propose a novel data mining framework for analyzing such medical data streams. The framework addresses the above challenges and represents a practical approach for early prediction and prevention based on data that would realistically be available at GHWs. We assess the feasibility of the proposed EWS approach through retrospective study that includes data from 28,927 visits at a major hospital. Finally, we apply our system in a real-time clinical trial and obtain promising results. This project is an example of multidisciplinary cyber-physical systems involving researchers in clinical science, data mining, and nursing staff in the hospital. Our early warning algorithm shows promising result: the transfer of patients to ICU was predicted with sensitivity of 0.4127 and specificity of 0.950 in the real time system.	and gate;algorithm;bootstrap aggregating;chi;cyber-physical system;data mining;exploratory testing;international components for unicode;logistic regression;missing data;overfitting;principle of good enough;real-time clock;real-time computing;sensitivity and specificity;smoothing;undersampling;volatility	Yi Mao;Yixin Chen;Gregory Hackmann;Minmin Chen;Chenyang Lu;Marin H Kollef;Thomas C. Bailey	2011	2011 IEEE 11th International Conference on Data Mining Workshops	10.1109/ICDMW.2011.117	computer science;data science;machine learning;warning system;data mining;computer security;statistics	ML	5.129686767639821	-76.91930521974939	90389
557835475f51b31a26552598a6062bc3a6f548c5	the timing and targeting of treatment in influenza pandemics influences the emergence of resistance in structured populations	influenza human;drug resistance viral;models biological;pandemics;influenza;network analysis;cancer treatment;stochastic processes;drug therapy;antimicrobial resistance;morbidity;humans;antiviral agents;spatial epidemiology;infectious disease epidemiology	Antiviral resistance in influenza is rampant and has the possibility of causing major morbidity and mortality. Previous models have identified treatment regimes to minimize total infections and keep resistance low. However, the bulk of these studies have ignored stochasticity and heterogeneous contact structures. Here we develop a network model of influenza transmission with treatment and resistance, and present both standard mean-field approximations as well as simulated dynamics. We find differences in the final epidemic sizes for identical transmission parameters (bistability) leading to different optimal treatment timing depending on the number initially infected. We also find, contrary to previous results, that treatment targeted by number of contacts per individual (node degree) gives rise to more resistance at lower levels of treatment than non-targeted treatment. Finally we highlight important differences between the two methods of analysis (mean-field versus stochastic simulations), and show where traditional mean-field approximations fail. Our results have important implications not only for the timing and distribution of influenza chemotherapy, but also for mathematical epidemiological modeling in general. Antiviral resistance in influenza may carry large consequences for pandemic mitigation efforts, and models ignoring contact heterogeneity and stochasticity may provide misleading policy recommendations.	approximation;bistability;emergence;epidemiology;genetic heterogeneity;infection;mathematics;mean field particle methods;morbidity - disease rate;network model;population;propagation constant;simulation;stochastic process	Benjamin M. Althouse;Oscar Patterson-Lomba;Georg M. Goerg;Laurent Hébert-Dufresne	2013		10.1371/journal.pcbi.1002912	biology;antibiotic resistance;network analysis;virology;spatial epidemiology;immunology;pandemic	ML	8.635721329505621	-67.51263410616066	90478
6e2c9da857f1c70a2d8cfa4fbd3bcfdc6442a27e	job performance prediction in a call center using a naive bayes classifier	employee turnover;naive bayesian classifier;job performance;call center	This study presents an approach to predict the performance of sales agents of a call center dedicated exclusively to sales and telemarketing activities. This approach is based on a naive Bayesian classifier. The objective is to know what levels of the attributes are indicative of individuals who perform well. A sample of 1037 sales agents was taken during the period between March and September of 2009 on campaigns related to insurance sales and service pre-paid phone services, to build the naive Bayes network. It has been shown that, socio-demographic attributes are not suitable for predicting performance. Alternatively, operational records were used to predict production of sales agents, achieving satisfactory results. In this case, the classifier training and testing is done through a stratified tenfold cross-validation. It classified the instances correctly 80.60% of times, with the proportion of false positives of 18.1% for class no (does not achieve minimum) and 20.8% for the class yes (achieves equal or above minimum acceptable). These results suggest that socio-demographic attributes has no predictive power on performance, while the operational information of the activities of the sale agent can predict the future performance of the	computer performance;cross-validation (statistics);job stream;kerrison predictor;naive bayes classifier;partial template specialization;performance prediction;predictive modelling	Mauricio A. Valle;Samuel Varas;Gonzalo A. Ruz	2012	Expert Syst. Appl.	10.1016/j.eswa.2011.11.126	machine learning;job performance;data mining;turnover	Web+IR	1.1089887260502036	-72.36690084574367	90626
4e744089b9390331d17ccc622fcb9495d5b8a4bd	visual analytics of image-centric cohort studies in epidemiology		Epidemiology characterizes the influence of causes to disease and health conditions of defined populations. Cohort studies are population-based studies involving usually large numbers of randomly selected individuals and comprising numerous attributes, ranging from self-reported interview data to results from various medical examinations, e.g., blood and urine samples. Since recently, medical imaging has been used as an additional instrument to assess risk factors and potential prognostic information. In this chapter, we discuss such studies and how the evaluation may benefit from visual analytics. Cluster analysis to define groups, reliable image analysis of organs in medical imaging data and shape space exploration to characterize anatomical shapes are among the visual analytics tools that may enable epidemiologists to fully exploit the potential of their huge and complex data. To gain acceptance, visual analytics tools need to complement more classical epidemiologic tools, primarily hypothesis-driven statistical analysis.	cluster analysis;image analysis;medical imaging;population;randomness;visual analytics	Bernhard Preim;Paul Klemm;Helwig Hauser;Katrin Hegenscheid;Steffen Oeltze-Jafra;Klaus D. Tönnies;Henry Völzke	2016		10.1007/978-3-319-24523-2_10	computer science;bioinformatics;data science;data mining;statistics	Visualization	3.312549699928025	-76.08008837601339	91477
54226362520d3247158d7db9346ecf29ea9a155e	unfolding physiological state: mortality modelling in intensive care units	health research;uk clinical guidelines;biological patents;text;support vector machines;graphical and latent variable models;europe pubmed central;citation search;data mining for social good;healthcare and medicine;uk phd theses thesis;life sciences;uk research reports;medical journals;topic graphical and latent variable models;europe pmc;biomedical research;topic;bioinformatics	Accurate knowledge of a patient's disease state and trajectory is critical in a clinical setting. Modern electronic healthcare records contain an increasingly large amount of data, and the ability to automatically identify the factors that influence patient outcomes stand to greatly improve the efficiency and quality of care.  We examined the use of latent variable models (viz. Latent Dirichlet Allocation) to decompose free-text hospital notes into meaningful features, and the predictive power of these features for patient mortality. We considered three prediction regimes: (1) baseline prediction, (2) dynamic (time-varying) outcome prediction, and (3) retrospective outcome prediction. In each, our prediction task differs from the familiar time-varying situation whereby data accumulates; since fewer patients have long ICU stays, as we move forward in time fewer patients are available and the prediction task becomes increasingly difficult.  We found that latent topic-derived features were effective in determining patient mortality under three timelines: in-hospital, 30 day post-discharge, and 1 year post-discharge mortality. Our results demonstrated that the latent topic features important in predicting hospital mortality are very different from those that are important in post-discharge mortality. In general, latent topic features were more predictive than structured features, and a combination of the two performed best.  The time-varying models that combined latent topic features and baseline features had AUCs that reached 0.85, 0.80, and 0.77 for in-hospital, 30 day post-discharge and 1 year post-discharge mortality respectively. Our results agreed with other work suggesting that the first 24 hours of patient information are often the most predictive of hospital mortality. Retrospective models that used a combination of latent topic features and structured features achieved AUCs of 0.96, 0.82, and 0.81 for in-hospital, 30 day, and 1-year mortality prediction.  Our work focuses on the dynamic (time-varying) setting because models from this regime could facilitate an on-going severity stratification system that helps direct care-staff resources and inform treatment strategies.	baseline (configuration management);discharger;forecast of outcome;latent dirichlet allocation;latent variable;net (polyhedron);note (document);patients;stratification;synapomorphy;timeline;viz: the computer game;intensive care unit	Marzyeh Ghassemi;Tristan Naumann;Finale Doshi-Velez;Nicole Brimmer;Rohit Joshi;Anna Rumshisky;Peter Szolovits	2014	KDD : proceedings. International Conference on Knowledge Discovery & Data Mining	10.1145/2623330.2623742	support vector machine;simulation;computer science;data science;machine learning;data mining	ML	5.089165055774704	-74.41578170272614	91513
9599153696a1557a9b5b4765079a08a1769612bb	identifying patients in target customer segments using a two-stage clustering-classification approach: a hospital-based assessment	target customer segment tcs;recency frequency monetary rfm analysis model;rough set theory rst;customer relationship management crm;k means clustering algorithm	Identifying patients in a Target Customer Segment (TCS) is important to determine the demand for, and to appropriately allocate resources for, health care services. The purpose of this study is to propose a two-stage clustering-classification model through (1) initially integrating the RFM attribute and K-means algorithm for clustering the TCS patients and (2) then integrating the global discretization method and the rough set theory for classifying hospitalized departments and optimizing health care services. To assess the performance of the proposed model, a dataset was used from a representative hospital (termed Hospital-A) that was extracted from a database from an empirical study in Taiwan comprised of 183,947 samples that were characterized by 44 attributes during 2008. The proposed model was compared with three techniques, Decision Tree, Naive Bayes, and Multilayer Perceptron, and the empirical results showed significant promise of its accuracy. The generated knowledge-based rules provide useful information to maximize resource utilization and support the development of a strategy for decision-making in hospitals. From the findings, 75 patients in the TCS, three hospital departments, and specific diagnostic items were discovered in the data for Hospital-A. A potential determinant for gender differences was found, and the age attribute was not significant to the hospital departments.	action potential;algorithm;classification;cluster analysis;database;decision making;decision tree;discretization;extraction;health care;health services;hospital departments;k-means clustering;mandibulofacial dysostosis;multilayer perceptron;naive bayes classifier;norm (social);patients;rough set;rule (guideline);set theory;sex characteristics;silo (dataset);statistical cluster	You-Shyang Chen;Ching-Hsue Cheng;Chien-Jung Lai;Cheng-Yi Hsu;Han-Jhou Syu	2012	Computers in biology and medicine	10.1016/j.compbiomed.2011.11.010	computer science;machine learning;data mining;k-means clustering	ML	4.242220787092621	-76.72972794544528	92457
d569c20263763698dbaa0d83cacf82eb3804fa88	body -- buckets of disease symptoms for disease outbreak analysis	hospital record entry;vast 2010 challenge data;health authority;manuals;medical administrative data processing;disease symptom;diseases hospitals cities and towns time frequency analysis pregnancy blood manuals;body;hospitals;medical data;records management;pregnancy;data visualisation;data analysis;patient record;medical data data analytics visual analytics;blood;vast 2010 challenge data disease symptom disease outbreak analysis hospital record entry health authority patient record body data analysis;data visualization;diseases;cities and towns;data analytics;disease outbreak analysis;records management data analysis data visualisation diseases health care medical administrative data processing;visual analytics;mortality rate;time frequency analysis;disease outbreak;health care	Active day-to-day analysis of hospital record entries can help alert possible disease outbreaks. Such information could help health authorities in planning prevention and minimizing the influence of the disease outbreak. Given a set of hospital entry records of patients, we developed a toolkit named BODY to analyse the data - historical and current - to present insights into disease outbreak patterns and abnormal patterns in symptom incidences. From the analysis, we aim to predict a possible outbreak of diseases detailing symptoms, time zones and mortality rates. We illustrate BODY's utility on the VAST 2010 Challenge Data containing hospital data entries from 11 cities across the world.	algorithm;compiler;data mining	Hanisha Veeramachaneni;Soujanya Vadapalli;Kamalakar Karlapalem	2010	2010 IEEE International Conference on Data Mining Workshops	10.1109/ICDMW.2010.193	computer science;data science;data mining;data analysis;data visualization;statistics	ML	3.967087499679567	-76.36821768441587	92531
553ceb683a397d208b78169b2fdb9a4400d70ce0	a note on relevance of diagnostic classification and rating scales used in psychiatry	unsupervised learning;scale;cluster analysis;machine learning;relevance	In the clinical practice of psychology and psychiatry, presence or absence of particular disorder or syndromes is based on the subjective interpretation of mental and behavioral descriptions offered by the patient. This is often done by questionnaires (also called instruments or scales) or by interviews. This subjectivity of the diagnostic decision-making process may limit the reliability of diagnosis. In the present study, a new method of scale relevance, based on double cluster analysis, is proposed as it is important to verify what we are trying to find with the proposed scale. If two data sets cluster differently, we must consider them as different.	cluster analysis;decision making;description;instrument - device;patients;psychiatry specialty;relevance;statistical classification;syndrome	Yiqun Gan;Tamar Kakiashvili;Waldemar W. Koczkodaj;Feng Li	2013	Computer methods and programs in biomedicine	10.1016/j.cmpb.2013.05.016	unsupervised learning;scale;psychiatry;relevance;computer science;machine learning;cluster analysis	NLP	4.066884991399849	-77.1084422216943	92666
6213b55d2c1363de6a2d3d218fe78ad6e0ffbefb	using meta-differential evolution to enhance a calculation of a continuous blood glucose level	differential evolution;blood glucose;interstitial fluid;continuous glucose monitoring;glucose modeling	We developed a new model of glucose dynamics. The model calculates blood glucose level as a function of transcapillary glucose transport. In previous studies, we validated the model with animal experiments. We used analytical method to determine model parameters. In this study, we validate the model with subjects with type 1 diabetes. In addition, we combine the analytic method with meta-differential evolution. To validate the model with human patients, we obtained a data set of type 1 diabetes study that was coordinated by Jaeb Center for Health Research. We calculated a continuous blood glucose level from continuously measured interstitial fluid glucose level. We used 6 different scenarios to ensure robust validation of the calculation. Over 96% of calculated blood glucose levels fit A+B zones of the Clarke Error Grid. No data set required any correction of model parameters during the time course of measuring. We successfully verified the possibility of calculating a continuous blood glucose level of subjects with type 1 diabetes. This study signals a successful transition of our research from an animal experiment to a human patient. Researchers can test our model with their data on-line at https://diabetes.zcu.cz.		Tomas Koutny	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.05.011	differential evolution;econometrics;computer science;mathematics;diabetes mellitus	ML	9.572320719783987	-70.81959284012731	93362
4816586fce925ea387a13b83e26c6dacc114e0b4	a hybrid case-based system in clinical diagnosis and treatment	patient diagnosis;biofeedback;domain theory;case base reasoning;information retrieval;clinical diagnosis;biological control systems;artificial intelligent;fuzzy logic;decision support system;stress measurement;decision support systems;artificial intelligence method;patient treatment;artificial intelligence;fuzzy logic hybrid case based system clinical diagnosis clinical treatment computer aided decision support systems stress diagnosis artificial intelligence methods case based reasoning textual information retrieval rule based reasoning;case based reasoning;diagnosis;patient treatment case based reasoning decision support systems fuzzy logic information retrieval patient diagnosis;biological control;stress measurement artificial intelligence biofeedback case based reasoning diagnosis information retrieval rule based reasoning;rule based reasoning	Computer-aided decision support systems play an increasingly important role in clinical diagnosis and treatment. However, they are difficult to build for domains where the domain theory is weak and where different experts differ in diagnosis. Stress diagnosis and treatment is an example of such a domain. This paper explores several artificial intelligence methods and techniques and in particular case-based reasoning, textual information retrieval, rule-based reasoning, and fuzzy logic to enable a more reliable diagnosis and treatment of stress. The proposed hybrid case-based approach has been validated by implementing a prototype in close collaboration with leading experts in stress diagnosis. The obtained sensitivity, specificity and overall accuracy compared to an expert are 92%, 86% and 88% respectively.	artificial intelligence;case-based reasoning;decision support system;domain theory;fuzzy logic;heart rate variability;information retrieval;logic programming;prototype;sensitivity and specificity;software diagnosis;stress testing (software)	Mobyen Uddin Ahmed;Shahina Begum;Peter Funk	2012	Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics	10.1109/BHI.2012.6211679	computer science;artificial intelligence;machine learning;data mining	AI	3.658577985409121	-79.26435815909224	93606
0074aede8358623d3bd49f37c6a31616f933f6e8	modeling estrogen receptor pathways in breast cancer using an artificial neural networks based inference approach	analytical models;feed forward;metastasis;cancer;multilayer perceptrons;biological system modeling;inference mechanisms;holistic approach;backpropagation;three layer multilayer perceptron based approach estrogen receptor pathways modeling artificial neural networks based inference approach estrogen receptor status breast cancer patients treatment management patient management tamoxifen artificial neural network feed forward back propagation algorithm;immune system;back propagation algorithm;biological system modeling analytical models metastasis immune system;patient treatment;multi layer perceptron;patient treatment backpropagation cancer inference mechanisms multilayer perceptrons;estrogen receptor;breast cancer;analytical model;artificial neural network	Estrogen receptor (ER) status is an important consideration in the prognosis and management of breast cancer patients, dictating treatment and patient management. While the prognosis of ER positive patients is generally poorer because of treatments such as Tamoxifen this situation has been reversed. Some detail is known of the ER pathway, however this has been based on reductionist studies of small numbers of markers. Here we present an Artificial Neural Network (ANN) using a feed forward back-propagation algorithm applied to a three layer multi-layer perceptron based approach that facilitates a wider more holistic approach to the identification of genes associated with ER status and the modeling of their interactions with one another in the context of a pathway.	algorithm;artificial neural network;backpropagation;data mining;erdős–rényi model;gene regulatory network;holism;interaction;layer (electronics);multilayer perceptron;multitier architecture;neural networks;reductionism;software propagation	Gopal K. Dhondalay;Christophe Lemetre;Graham R. Ball	2012	Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics	10.1109/BHI.2012.6211745	bioinformatics;engineering;artificial intelligence;machine learning	SE	8.494669010075285	-69.74217229461118	93688
1aec322ed898505592528d2099bdd04d16968bfc	phagocyte transmigration modeling using system dynamic controls	control theory;cell motility;system theory cell motility control theory physiological models prosthetics;system dynamics;system dynamic controls;system theory;prosthetics;implanted medical devices;control theory phagocyte transmigration modeling system dynamic controls pathogenesis flbrotic responses implanted medical devices system theory;phagocyte transmigration modeling;pathogenesis;medical device;mathematical model;control system synthesis control theory mathematical model implants biological system modeling medical simulation predictive models pathogens computer science biomedical engineering;molecular mechanics;physiological models;flbrotic responses	Phagocyte transmigration is one of major phagocyte responses which are important in pathogenesis of flbrotic responses to implanted medical devices. Excessive flbrotic responses may lead to the failure of various medical implants. Therefore, better understanding of molecular mechanisms governing phagocyte transmigration is important in the success of development of implanted medical devices. This paper hypothesizes the biological components involved in phagocyte transmigration and the relationship between these components. A mathematical model based on system theory and control theory is built to support the hypothesis, so as to provide the prediction of some un-measured dynamic features in the phagocyte transmigration process. External controls are also supplied by the mathematical model to analogize diverse knockout experiments which contribute the construction of the biological hypothesis. Simulation results demonstrate the effectiveness of the mathematical model.	control theory;experiment;knockout;mathematical model;reincarnation;simulation;systems theory	Jiaxing Xue;Jean Gao;Liping Tang	2007	2007 IEEE 7th International Symposium on BioInformatics and BioEngineering	10.1109/BIBE.2007.4375604	biology;molecular mechanics;computer science;mathematical model;immunology;system dynamics;motility;systems theory	Embedded	9.157489834310537	-68.75949732730953	93866
c1b6bb26aaee234917e9648a08608759c5a8de57	when can multi-site datasets be pooled for regression? hypothesis tests, $\ell_2$-consistency and neuroscience applications		Many studies in biomedical and health sciences involve small sample sizes due to logistic or financial constraints. Often, identifying weak (but scientifically interesting) associations between a set of predictors and a response necessitates pooling datasets from multiple diverse labs or groups. While there is a rich literature in statistical machine learning to address distributional shifts and inference in multi-site datasets, it is less clear when such pooling is guaranteed to help (and when it does not) – independent of the inference algorithms we use. In this paper, we present a hypothesis test to answer this question, both for classical and high dimensional linear regression. We precisely identify regimes where pooling datasets across multiple sites is sensible, and how such policy decisions can be made via simple checks executable on each site before any data transfer ever happens. With a focus on Alzheimer’s disease studies, we show empirical results in regimes suggested by our analysis, where pooling a locally acquired dataset with data from an international study improves power.	algorithm;convolutional neural network;executable;machine learning	Hao Henry Zhou;Yilin Zhang;Vamsi K. Ithapu;Sterling C. Johnson;Grace Wahba;Vikas Singh	2017			econometrics;artificial intelligence;statistical hypothesis testing;pattern recognition;statistics;data mining;sample size determination;linear regression;executable;inference;pooling;computer science	ML	2.9171009460264705	-72.73747932071139	93891
c75543ea80bc6bfae2d6775e9b2ec1174846b094	mortality prediction of septic shock patients using probabilistic fuzzy systems	sequential forward feature selection;mortality prediction;area under the roc curve;probabilistic fuzzy systems;septic shock	Graphical abstractDisplay Omitted HighlightsProbabilistic fuzzy systems (PFS) are used to predict mortality of septic shock patients.PFS models are compared with Takagi-Sugeno fuzzy models and logistic regression models.The methods are tested using ICU patients with abdominal septic shock.PFS models increase the transparency of the learned system using fuzzy rules.By providing estimates for the mortality risk, PFS help clinical decision making. Mortality scores based on multiple regressions are common in critical care medicine for prognostic stratification of patients. However, to be used at the point of care, they need to be both accurate and easily interpretable. In this work, we propose the application of one existent type of rule base system using statistical information - probabilistic fuzzy systems (PFS) - to predict mortality of septic shock patients. To assess its accuracy and interpretability, these models are compared to methodologies previously proposed in this domain: Takagi-Sugeno fuzzy models and logistic regression models. The methods are tested using a retrospective cohort study including ICU patients with abdominal septic shock. Regarding accuracy, PFS models are comparable to fuzzy modeling and logistic regression. In terms of interpretability, results indicate that PFS models increase the transparency of the learned system (using fuzzy rules), but at the same time, provide additional means for validating the fuzzy classifier using expert knowledge (from physicians in this paper). By providing accurate and interpretable estimates for the mortality risk, results suggest the usefulness of PFS to develop scores for critical care medicine.	fuzzy control system;septic equation	André S. Fialho;Susana M. Vieira;Uzay Kaymak;Rui Jorge Almeida;Federico Cismondi;Shane R. Reti;Stan N. Finkelstein;João Miguel da Costa Sousa	2016	Appl. Soft Comput.	10.1016/j.asoc.2016.01.005	econometrics;data mining;mathematics;statistics	NLP	5.937877448428989	-76.14872349111728	94045
8624e312312b3f08a3a78500db4c8f71555fd6e9	cellular automata simulation of signal transduction and calcium dynamics with healthy and faulty receptor trafficking	lattices;extracellular;calcium mathematical model automata insulin extracellular difference equations lattices;calcium;automata;difference equations;mathematical model;insulin;defective receptors signal transduction cytosolic calcium cellular automata dimerization trafficking	The signal transduction process is one by which an external signal is detected by the cell and converted into cellular responsive actions leading to changes in secondary messengers, such as cAMP (cyclic adenosine monophosphate) or intracellular calcium. Calcium is an important second messenger and has been the subject of numerous experimental investigations and dynamic studies in intracellular signaling. Defects in the signaling process involving the calcium-sensing receptors can lead to faulty re-adjustments of circulating calcium level. Dimerization process has also been proposed to play an important role in the efficiency of the receptor responses. Here, we construct and simulate a Monte Carlo cellular automata model coupled with a system of difference equations to model receptor binding, trafficking, and dimerization triggered by an agonist, while keeping track of the concentration of cytosolic free calcium by using a system of difference equations.	automata theory;cell signaling;cellular automaton;emoticon;fractal dimension;monte carlo method;recurrence relation;simulation;transduction (machine learning);xfig	Chontita Rattanakul;Yongwimon Lenbury	2016	2016 Annual IEEE Systems Conference (SysCon)	10.1109/SYSCON.2016.7490536	biology;biochemistry;cell biology;communication	Embedded	7.455845777727267	-67.03060491004084	94130
a9f94652bc58addf48f130894de4c9d61bf1f061	a mathematical and computational model for simulating complex dynamic cancer growth and metastasis			computation;computational model	Marc Colangelo;Miroslav Lovric;Johathan R. Stone	2012	I. J. Comput. Appl.		cancer research;cancer;metastasis;computer science	Theory	6.426644033755446	-68.61604949983627	94232
b7032789e4b5eb5887f8b52efc595ca78763751a	coupled analytical and numerical approach to uncovering new regulatory mechanisms of intracellular processes	ściezka sygnalizacyjna;signaling pathways;simulation;punkt rownowagi;equilibrium points;symulacja	The paper deals with the analysis of signaling pathways aimed at uncovering new regulatory processes regulating cell responses. First, general issues of comparing simulation and experimental data are discussed, and various aspects of data normalization are covered. Then, a model of a particular signaling pathway, induced by Interferon-β, is briefly introduced. It serves as an example illustrating how mathematical modeling can be used for inferring the structure of a regulatory system governing the dynamics of intracellular processes. In this pathway, experimental results suggest that a hitherto unknown process is responsible for a decrease in the levels of one of the important molecules used in the pathway. Then, equilibrium points of the model are analyzed, allowing the rejection of all but one explanation of the phenomena observed experimentally. Numerical simulations confirm that the model can mimic the dynamics of the processes in the pathway under consideration. Finally, some remarks about the applicability of the method based on an analysis of equilibrium points are made.	computational fluid dynamics;emoticon;experiment;gene regulatory network;mathematical model;numerical analysis;numerical linear algebra;reduction (complexity);rejection sampling;rough set;simulation;unified model	Jaroslaw Smieja	2010	Applied Mathematics and Computer Science	10.2478/v10006-010-0060-0	equilibrium point;econometrics;computer science;bioinformatics;mathematics;operations research;signal transduction	Comp.	9.140094592677816	-67.60689163007031	94387
917e7808b6a8529c018aac4d445368bc90c07edc	nonlinear dynamic modelling of platelet aggregation via microfluidic devices	shear flow aggregation biochemistry biomems blood cellular transport enzymes haemodynamics medical disorders microfluidics molecular biophysics patient diagnosis;nonlinear dynamic modelling platelet hyperfunction automatic controllers biomechanical dynamics thrombin txa2 adp platelet activation blood born chemical pathways shear stress rates severe arterial stenosis platelet disaggregation complex biological phenomenon biochemical factors mechanical factors nonlinear dynamic systems analysis bioengineering approaches pathological thrombus formation blood platelet function microfluidic devices platelet aggregation;system identification blood diagnosis disturbed flow microfluidics platelet aggregate platelet function shear rate;platelet aggregate;platelet function;disturbed flow;system identification;blood;microfluidics;aggregates blood biological system modeling data models mathematical model biomechanics stress;diagnosis;shear rate	The recent application of new microfluidic technologies and methods has facilitated significant progress in the understanding of the fundamental mechanisms governing blood platelet function and how these parameters affect pathological thrombus formation. In-line with these new bioengineering approaches, the application of nonlinear dynamic systems analysis holds particular potential to extend our understanding of the complex interplay between mechanical and biochemical factors that underlie this complex biological phenomenon. In this paper we propose a simple mathematical model of the main dynamics of platelet aggregation/disaggregation observed experimentally in a novel microfluidic device that approximates a severe arterial stenosis. We apply dynamic systems theory (system identification) to explore the dynamics of the biomechanical platelet aggregation response to a range of shear stress rates, inhibiting blood-born chemical pathways of platelet activation (ADP, TXA2, and thrombin). We demonstrate that the proposed model is able to replicate experimental results with low variation, and suggest that the reduced set of model parameters has the potential to be used as a simplified way to evaluate the biomechanical dynamics of platelet aggregation. The proposed model has application to the development of automatic controllers within the context of microfluidic systems that may show great utility in the clinical assessment of platelet hyperfunction.	adenosine diphosphate;blood clot;blood platelet disorders;blood platelets;cardiomyoplasty;controllers;dynamical system;dynamical systems theory;experiment;inhibition;intrauterine devices;mathematical model;mathematics;microchip analytical devices;microfluidics;nonlinear dynamics;nonlinear system;platelet aggregation;raynaud phenomenon;review aggregator;self-replication;system identification;thromboxane a2;usb on-the-go;shear stress	Miguel E. Combariza;Xinghuo Yu;Warwick S. Nesbitt;Arnan Mitchell;Francisco J. Tovar-Lopez	2015	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2015.2403266	shear rate;microfluidics;medicine;system identification;nanotechnology;mathematics;biological engineering;immunology;physics	Robotics	9.520584175431134	-69.05850628103794	95090
ebe02ad2d3bba926695276a6a4f00ca397f22648	local linear wavelet neural network for breast cancer recognition		Breast cancer is the major cause of cancer deaths in women today and it is the most common type of cancer in women. Many sophisticated algorithm have been proposed for classifying breast cancer data. This paper presents some experiments for classifying breast cancer tumor and proposes the use local linear wavelet neural network for breast cancer recognition by training its parameters using Recursive least square (RLS) approach to improve its performance. The difference of the local linear wavelet network with conventional wavelet neural network (WNN) is that the connection weights between hidden layer and output layer of conventional WNN are replaced by a local linear model. The result quality has been estimated and compared with other experiments. Results on extracted breast cancer data from University of Wisconsin Hospital Madison show that the proposed approach is very robust, effective and gives better classification.	algorithm;artificial neural network;data mining;database;display resolution;experiment;linear model;radiology;recursion (computer science);recursive least squares filter;statistical classification;time series;wavelet	Manas Ranjan Senapati;Aswini Kumar Mohanty;S. Dash;Pradipta Kishore Dash	2011	Neural Computing and Applications	10.1007/s00521-011-0670-y	artificial intelligence;machine learning;data mining	ML	7.204239414005784	-78.36603948252029	95443
af82eb0955f835678620ff49c5d38b8cdbb1312e	a new flexible direct roc regression model: application to the detection of cardiovascular risk factors by anthropometric measures	diagnostic test;bootstrap;roc curve generalised additive models bootstrap cardiovascular risk factors anthropometric measures;citations;receiver operator characteristic;impact factor;cardiovascular risk factors;regression model;h index;generalised additive models;cardiovascular risk factor;anthropometric measures;roc curve;rankings;simulation study;economics;false positive;generalised additive model;research impact;research production	The receiver operating characteristic (ROC) curve is the most widely used measure for evaluating the accuracy of diagnostic tests in terms of differentiating between two conditions. It is known that, in certain circumstances, the characteristics of the patient or the place where the diagnostic test is performed can modify the test's accuracy. A new estimator for the conditional ROC curve, based on direct modelling, is proposed. In this approach, the effect of covariates and false positive fraction on the ROC curve is modelled non-parametrically using generalised additive models (GAM) combined with local polynomial kernel smoothers. The method allows for incorporation of more than one covariate in the regression model for the ROC curve and the possible interaction between them. The proposed model's performance is examined in an in-depth simulation study. Finally, endocrine data are analysed with the aim of assessing the performance of several anthropometric measures in predicting clusters of cardiovascular risk factors in an adult population in Galicia (NW Spain), with adjustment for age and gender.	anthropometry	María Xosé Rodríguez-Álvarez;Javier Roca-Pardiñas;Carmen Cadarso-Suárez	2011	Computational Statistics & Data Analysis	10.1016/j.csda.2011.06.008	econometrics;machine learning;data mining;mathematics;receiver operating characteristic;statistics	ML	8.62381336101787	-74.45252416327614	95568
4d8c70c2b367a7742b965d543cd22b7b58971815	cardiac image modelling: breadth and depth in heart disease	computational modelling;biomechanics;journal article;cardiac atlases	With the advent of large-scale imaging studies and big health data, and the corresponding growth in analytics, machine learning and computational image analysis methods, there are now exciting opportunities for deepening our understanding of the mechanisms and characteristics of heart disease. Two emerging fields are computational analysis of cardiac remodelling (shape and motion changes due to disease) and computational analysis of physiology and mechanics to estimate biophysical properties from non-invasive imaging. Many large cohort studies now underway around the world have been specifically designed based on non-invasive imaging technologies in order to gain new information about the development of heart disease from asymptomatic to clinical manifestations. These give an unprecedented breadth to the quantification of population variation and disease development. Also, for the individual patient, it is now possible to determine biophysical properties of myocardial tissue in health and disease by interpreting detailed imaging data using computational modelling. For these population and patient-specific computational modelling methods to develop further, we need open benchmarks for algorithm comparison and validation, open sharing of data and algorithms, and demonstration of clinical efficacy in patient management and care. The combination of population and patient-specific modelling will give new insights into the mechanisms of cardiac disease, in particular the development of heart failure, congenital heart disease, myocardial infarction, contractile dysfunction and diastolic dysfunction.		Avan Suinesiaputra;Andrew D. McCulloch;Martyn P. Nash;Beau Pontre;Alistair A. Young	2016	Medical image analysis	10.1016/j.media.2016.06.027	simulation;medicine;pathology;biomechanics	ML	6.63775848656743	-69.26559144958014	95592
83351f4764ce5acf6c4695c50c1faee9d420235b	the prognostic scale crash in the treatment of children with severe traumatic brain injury	pedestrian safety;poison control;injury prevention;safety literature;traffic safety;injury control;home safety;injury research;safety abstracts;human factors;occupational safety;safety;safety research;accident prevention;violence prevention;bicycle safety;poisoning prevention;falls;ergonomics;suicide prevention	The aim of the present study was to assess the effectiveness and validity of prognostic scale CRASH which is calculated using on-line resources and which may serve as a decision support for physicians in treating severe traumatic brain injury (TBI) in children. This retrospective study was conducted using clinical and physiological data of 168 hospitalized pediatric patients with severe traumatic brain injury (GCS score less than or equal to 8). CRASH scale was used for calculating the severity of patients' state and for prognosing death outcomes at 14 days and at 6 months using the on-line resource. Our research has shown that the prognostic scale CRASH has an excellent discrimination ability (AUROC=0.816) in each version. The study has also shown that the scale has a satisfactory calibration ability in the option of 14 days with CT (χ2 equal 8.7 and p-value equal to 0.368). Calibration ability for other options was unsatisfactory. Thus, CRASH scale with CT scan has turned to be useful for assessing death outcomes at 14 days in children with severe TBI.		Irina Vitalevna Vasilyeva;Semen Vladimirivich Mesherakov;Sergey Borosovich Arseniev;Zanna Borisovna Semenova	2015	Studies in health technology and informatics	10.3233/978-1-61499-538-8-71	medicine;medical emergency;forensic engineering;computer security	HCI	7.731936417528109	-75.71446722846898	96310
9788449aadca9db80b8f0cda63af3ac53fe0930e	abstract interpretation of cellular signalling networks	post translational modification;egf receptor;signalling pathway;abstract interpretation	interpretation of cellular signaling networks Vincent Danos, Jerome Feret, Walter Fontana, Jean Krivine Davis Buenger Biological Signaling Pathways • Signaling pathways regulate sending and receiving extra-cellular signals and trigger cell activities • Understanding the pathways will allow greater incite on controlling the inner workings of the cell Signaling Pathways • Pathways function through kinetic interactions between ligands and proteins • Often not possible due to the high number of combinations that the pathways can form Overview • Like BioNetGen k-Calculus Uses a , formalized mathematical language to look at signaling pathways through a collection of rules • Danos worked within the frame work of k Calculus to provide a criteria to prove a pathway finite	abstract interpretation;cell signaling;gene regulatory network;interaction;jean	Vincent Danos;Jérôme Feret;Walter Fontana;Jean Krivine	2008		10.1007/978-3-540-78163-9_11	posttranslational modification;algorithm	Comp.	2.9555812555643652	-66.98168258848897	96495
80c64b11da19bb570aee42dc669eaffaccbd7626	editorial: methods in functional genomics	machine learning;functional genomics	In June 2000, leaders of the Human Genome Project, Craig Venter of Celera Genomics, and U.S. President Clinton announced the completion of a “working draft” of the human genome: the genetic blueprint of a human being. Today, the legacy of that announcement is the challenge to annotate this map, by understanding the roles and functions of genes—and their interplay with proteins and the environment—to create complex, dynamic living systems. This understanding is the goal of functional genomics. Functional genomics has recently become a major focus of machine learning applications thanks to the development of the new technology of DNA or expression microarray (Schena et al., 1995; Lockhart et al., 1996). Microarrays enable investigators to observe the genome of entire organisms in action by simultaneously measuring the level of activation of thousands of genes under the same experimental conditions. This technology provides today unprecedented discovery opportunities and is reshaping biomedical sciences by shifting its paradigm from a hypothesis driven to a data driven approach (Lander, 1999). Not surprisingly, parallel to these technological advances has been the development of machine learning methods able to integrate and understand the data generated by this new kind of experiments. However, most of this research has been conducted outside the traditional machine learning research community. The aim of this special issue is to bridge this divide by reporting methodological advances in automated learning from functional genomic data to the core machine learning community.	blueprint;dna microarray;experiment;functional genomics;living systems;lunar lander (video game series);machine learning;programming paradigm	Paola Sebastiani;Isaac S. Kohane;Marco Ramoni	2003	Machine Learning	10.1023/A:1023904205853	functional genomics;computer science;bioinformatics;artificial intelligence;data science;machine learning	ML	-1.3017227987381252	-66.43069221146511	96600
e3e37bbc2d18a3bb6484564727bf83c6f2ea1e90	risk factors rule mining in hypertension: korean national health and nutrient examinations survey 2007–2014	prognostics and health management;hypertension;data mining;feature extraction;statistics;diseases;sociology	The prevention of hypertension is one of the most important topics in health research. In the most of the previous studies used statistical methods for analyzing the association between hypertension prevalence and dietary. However, statistical methods have some limitation which are, it is difficult to interpret variables interaction at a time. Thus we apply the data mining techniques for generation of prognosis factors based on association rule mining. In our experiment, we conducted Korean National Health and Nutrient Examination Survey (KNHANES) data from 2007 to 2014. We used to filter-based feature selection method for find prognosis factors and we generate the rules based on discovered risk factors of prognosis in hypertension. We evaluated discovered rules by support and confidence. In the results shows that, we can find useful rules for prognosis of hypertension. We expected to support medical decision making and easy to interpret prognosis of hypertension.	association rule learning;data mining;feature selection;medical decision making	Hyun Woo Park;Erdenebileg Batbaatar;Dingkun Li;Keun Ho Ryu	2016	2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)	10.1109/CIBCB.2016.7758128	feature extraction;computer science;bioinformatics;data science;data mining;prognostics	AI	4.348885522015252	-75.93999962814961	96668
56d4ea4a8ad3bbb80e85dc1e518208e094d11ed3	semantic associative relations and conceptual processing		We analysed the organisation of semantic network using associative mechanisms between different types of information and studied the progression of the use of these associative relations during development. We aimed to verify the linkage of concepts with the use of semantic associative relations. The goal of this study was to analyse the cognitive ability to use associative relations between various items when describing old and/or new concepts. We examined the performance of 100 subjects between the ages of 4 and 7 years on an experimental task using five associative relations based on verbal encoding. The results showed that children are able to use the five semantic associative relations at age 4, but performance with each of the different associative relations improves at different times during development. Functional and part/whole relations develop at an early age, whereas the superordinate relations develop later. Our study clarified the characteristics of the progression of semantic associations during development as well as the roles that associative relations play in the structure and improvement of the semantic store.	aging;brain injuries;categorization;clarify;cognition;cognition disorders;color gradient;linkage (software);mental retardation;mental association;patients;priming exercise;semantic network;statistical classification;traumatic brain injury;genetic linkage	Dina Di Giacomo;Lucia Serenella De Federicis;Manuela Pistelli;Daniela Fiorenzi;Domenico Passafiume	2011	Cognitive Processing	10.1007/s10339-011-0399-7	natural language processing;computer science;artificial intelligence;communication	AI	1.7291659089714189	-79.50446262780765	96803
eab2b5df3ced145501fc663befd821475b45e4ce	telemonitoring in heart failure patients with clinical decision support to optimize medication doses based on guidelines	guidelines hafnium medical services medical diagnostic imaging heart blood pressure biomedical monitoring;heart failure patients telemonitoring intense hf weight changes heart rate blood pressure heart failure management medication dose optimization clinical decision support;telemedicine blood pressure measurement cardiology decision support systems diseases patient monitoring	The European Society of Cardiology guidelines for heart failure management are based on strong evidence that adherence to optimal medication is beneficial for heart failure patients. Telemonitoring with integrated clinical decision support enables physicians to adapt medication dose based on up to date vital parameters and reduces the number of hospital visits needed solely for up-titration of heart failure medication. Although keeping track of weight and blood pressure changes is recommended during unstable phases, e.g. post-discharge and during up-titration of medication, guidelines are rather vague regarding telehealth aspects. In this paper, we focus on the evaluation of a clinical decision support system for adaption of heart failure medication and for detecting early deteriorations through monitoring of blood pressure, heart rate and weight changes. This clinical decision support system is currently used in INTENSE-HF, a large scale telemonitoring trial with heart failure patients. The aim of this paper was to apply the decision support algorithm to an existing telemonitoring dataset, to assess the ability of the decision support concept to adhere to the guidelines and to discuss its limitations and potential improvements.	acclimatization;cns disorder;cardiology discipline;clinical decision support system;control theory;discharger;heart failure;patients;sensor;silo (dataset);titration method;unstable medical device problem;vagueness;algorithm	Martin Kropf;Robert Modre-Osprian;Dieter Hayn;Friedrich Fruhwald;Günter Schreier	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944295	intensive care medicine;medicine;medical emergency;cardiology	DB	6.3486805793832835	-79.64275953511351	96910
39c6234fbc1e593ea1a0fd3e2abf0af8af93ae00	a machine learning approach to prediction of exacerbations of chronic obstructive pulmonary disease		Chronic Obstructive Pulmonary Disease (COPD) places an enormous burden on the health care systems and causes diminished health related quality of life. The highest proportion of human and economic cost is associated to admissions for acute exacerbation of respiratory symptoms. The remote monitoring of COPD patients with the view of early detection of acute exacerbation of COPD (AECOPD) is one of the goals of the respiratory community. In this study, machine learning was used to develop predictive models. Models robustness to exacerbation definition was analyzed. A non-knowled-ge based approach was followed on data self-reported by patients using a multimodal tool during a remote monitoring 6 months trial. Comparison of different classifier algorithms operating with different AECOPD definitions was performed. Significant results were obtained for AECOPD prediction, regardless of the definition of exacerbation used. Best accuracy was achieved using a PNN classifier independently of the selected AECOPD definition. Our study suggests that the proposed data-driven methodology could help to design reliable predictive algorithms aimed to predict COPD exacerbations and therefore could provide support both to physicians and patients.	machine learning	Miguel Ángel Fernández Granero;Daniel Sánchez Morillo;Miguel Angel Lopez-Gordo;Antonio León	2015		10.1007/978-3-319-18914-7_32	exacerbation;artificial intelligence;computer science;machine learning;predictive analytics;health care;telehealth;disease;quality of life;copd	NLP	6.379855854859082	-76.06944248000916	97006
3c5ecae71ceac75e9a5e44133b46d008b5581756	the emergence of biological coding theory as a mathematical framework for modeling, monitoring, and modulating biomolecular systems	error correction codes;information transmission;synthetic biology;mathematical analysis;genetics;coding theory;error correction;molecular biophysics;communication theory;error control coding;synthetic dna biological coding theory mathematical framework biomolecular systems error control coding theory information transmission genetic communication theory paradigm biosensors;dna computing;codes biological system modeling mathematical model monitoring modulation coding error correction biological information theory biosensors information analysis genetic communication;information theoretic;biosensors;single nucleotide polymorphism;in silico;molecular biophysics biosensors error correction codes mathematical analysis	In this work we will discuss the implications of understanding hybridization using the mathematical framework of error control coding theory and how this requires not only the use of information theoretic analysis tools, but compels us to view and model biomolecular systems as information transmission and processing systems. Using the genetic communication theory paradigm, we investigate coding theory algorithms for in silico categorization of single nucleotide polymorphisms based on the calculation of syndromes [1]. We explore the use of coding theory frameworks in the design of in vitro computational biosensors [2] and design of error correcting biosensors [3] for monitoring biomolecular systems. We conclude by briefly investigating the necessity of biological coding theory in the emerging field of synthetic biology [4] where incorporation of error control codes into synthetic DNA, an established area of research in the field of DNA computing [5], is becoming an area of growing interest in order to increase the robustness of engineered biomolecular systems.	algorithm;categorization;code;coding theory;dna computing;emergence;entropy (information theory);error detection and correction;information processing;information theory;model-driven architecture;mutual information;parallels desktop for mac;programming paradigm;shannon (unit);synthetic biology;synthetic intelligence	Elebeoba May	2009	2009 43rd Annual Conference on Information Sciences and Systems	10.1109/CISS.2009.5054838	single-nucleotide polymorphism;error detection and correction;computer science;bioinformatics;theoretical computer science;mathematics;synthetic biology;dna computing;biosensor;communication theory;coding theory;molecular biophysics	Theory	3.436562329992406	-69.02350811403014	97041
894bf83b2bee59a0ea6c2e68234bb7c6e37a4099	regression methods for parameter sensitivity analysis: applications to cardiac arrhythmia mechanisms	analytical models;mathematical model computational modeling predictive models biological system modeling analytical models data models transient analysis;sensitivity analysis bioelectric phenomena cardiology diseases physiological models regression analysis;cardiology;computer model;biological system modeling;cardiac electrophysiology regression method parameter sensitivity analysis cardiac arrhythmia mechanism mathematical model cardiac myocytes;cardiac myocyte;transient analysis;data model;arrhythmias cardiac humans regression analysis;computational modeling;sensitivity analysis;mathematical model;diseases;predictive models;regression analysis;prediction model;bioelectric phenomena;arrhythmia;physiological models;analytical model;cardiac arrhythmia;data models	Mathematical models are used extensively in studies of cardiac electrophysiology and arrhythmia mechanisms. Models can generate novel predictions, suggest experiments, and provide a quantitative understanding of underlying mechanisms. Limitations of present modeling approaches, however, include non-uniqueness of both parameters and the models themselves, and difficulties in accounting for experimental variability. We describe new approaches that can begin to address these limitations, and show how these can provide novel insight into mathematical models of cardiac myocytes.	electrophysiology (science);experiment;heart rate variability;mathematical model;mathematics;muscle cells;myocytes, cardiac;population parameter;sinoatrial node	Eric A. Sobie;Amrita X. Sarkar	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6091153	computer simulation;toxicology;computer science;machine learning;predictive modelling;biological engineering;statistics;cardiology	SE	9.742489942280223	-69.21748127342543	98251
e7e6c58ffdb0b19d6fbd8172948157264a9d65ce	towards improving diagnosis of skin diseases by combining deep neural network and human knowledge	deep learning;dermatology;image classification;semantic data analytics	BACKGROUND The emergence of the deep convolutional neural network (CNN) greatly improves the quality of computer-aided supporting systems. However, due to the challenges of generating reliable and timely results, clinical adoption of computer-aided diagnosis systems is still limited. Recent informatics research indicates that machine learning algorithms need to be combined with sufficient clinical expertise in order to achieve an optimal result.   METHODS In this research, we used deep learning algorithms to help diagnose four common cutaneous diseases based on dermoscopic images. In order to facilitate decision-making and improve the accuracy of our algorithm, we summarized classification/diagnosis scenarios based on domain expert knowledge and semantically represented them in a hierarchical structure.   RESULTS Our algorithm achieved an accuracy of 87.25 ± 2.24% in our test dataset with 1067 images. The semantic summarization of diagnosis scenarios can help further improve the algorithm to facilitate future computer-aided decision support.   CONCLUSIONS In this paper, we applied deep neural network algorithm to classify dermoscopic images of four common skin diseases and archived promising results. Based on the results, we further summarized the diagnosis/classification scenarios, which reflect the importance of combining the efforts of both human expertise and computer algorithms in dermatologic diagnoses.	algorithm;archive;artificial neural network;biological neural networks;computer assisted diagnosis;convolutional neural network;cutaneous candidiasis;decision making;decision support systems, clinical;decision support system;deep learning;dermatologic disorders;emergence;informatics (discipline);machine learning;silo (dataset);subject-matter expert	Xinyuan Zhang;Shiqi Wang;Jie Liu;Cui Tao	2018		10.1186/s12911-018-0631-9	data mining;convolutional neural network;artificial neural network;deep learning;informatics;contextual image classification;health informatics;medicine;artificial intelligence	AI	1.9983417398795236	-73.38091719385797	98740
59d76e1a50201ea044741ee9c06edf0f7405de16	digital pathology imaging - the next frontier in medical imaging	radiology;radiology digital pathology imaging medical imaging disease dignosis diagnostic biomarkers image based technology digital workflow;medical image analysis digital pathology medical imaging tissue diagnostics;radiology diseases medical image processing;medical image processing;diseases;pathology tumors multiplexing image analysis biomedical imaging image color analysis	Pathologists have practiced medicine relatively unchanged over the last century to render diagnosis of disease. However, over the last decade, the practice of Pathology is undergoing a foundational change. In addition to the revolutions in diagnostic biomarkers, we are seeing a ground-swell in new imaging technologies. This new image based technology offers significant opportunities to the practice. However, it comes at a cost - there are technology, regulatory, and methodological challenges which may derail the adoption process if not addressed proactively and innovatively. Pathology lags behind other medicine practice such as Radiology in the adoption of digital workflow. Recently, significant advances have been made in the capture and management of the whole slide images used in pathology practice and this is leading to an explosion in the data volume that completely eclipses the vast quantities of data being produced in Radiology. Another area of technology challenges is related to the analysis of the imagery data for detection, identification, recognition and quantification of the pathology in the slide. This clearly is the bleeding edge of digital pathology which will enable the creation of a paradigm shift in the practice of medicine. This paper explores the technological challenges emerging in different areas of digital pathology. We present image capture technologies, the image analysis techniques, and the key computational challenges.	image analysis;imaging technology;medical imaging;programming paradigm;radiology	Bikash Sabata	2012	2012 International Conference on Advanced Computer Science and Information Systems (ICACSIS)		data science;paradigm shift;workflow;medical imaging;digital pathology;frontier;computer science	EDA	2.116958160995162	-68.8884160118297	98797
ab017362ee59b3489abdf866dd686cc1b8d74aec	improving the modeling of disease data from the government surveillance system: a case study on malaria in the brazilian amazon	incidence;models biological;prevalence;monte carlo method;reproducibility of results;brazil;malaria;algorithms;humans;spatial analysis;computational biology;computer simulation;public health surveillance;markov chains	The study of the effect of large-scale drivers (e.g., climate) of human diseases typically relies on aggregate disease data collected by the government surveillance network. The usual approach to analyze these data, however, often ignores a) changes in the total number of individuals examined, b) the bias towards symptomatic individuals in routine government surveillance, and; c) the influence that observations can have on disease dynamics. Here, we highlight the consequences of ignoring the problems listed above and develop a novel modeling framework to circumvent them, which is illustrated using simulations and real malaria data. Our simulations reveal that trends in the number of disease cases do not necessarily imply similar trends in infection prevalence or incidence, due to the strong influence of concurrent changes in sampling effort. We also show that ignoring decreases in the pool of infected individuals due to the treatment of part of these individuals can hamper reliable inference on infection incidence. We propose a model that avoids these problems, being a compromise between phenomenological statistical models and mechanistic disease dynamics models; in particular, a cross-validation exercise reveals that it has better out-of-sample predictive performance than both of these alternative models. Our case study in the Brazilian Amazon reveals that infection prevalence was high in 2004-2008 (prevalence of 4% with 95% CI of 3-5%), with outbreaks (prevalence up to 18%) occurring during the dry season of the year. After this period, infection prevalence decreased substantially (0.9% with 95% CI of 0.8-1.1%), which is due to a large reduction in infection incidence (i.e., incidence in 2008-2010 was approximately one fifth of the incidence in 2004-2008).We believe that our approach to modeling government surveillance disease data will be useful to advance current understanding of large-scale drivers of several diseases.	aggregate data;amazona;cross reactions;cross-validation (statistics);emoticon;incidence matrix;infection;inference;malaria;sampling (signal processing);simulation;statistical model	Denis Valle;James S. Clark	2013		10.1371/journal.pcbi.1003312	computer simulation;markov chain;incidence;simulation;prevalence;spatial analysis;operations research;statistics;monte carlo method	Metrics	6.659020444521071	-72.55266030764935	98864
1f4792fc8332da39c613662f0c25d5dff2a29101	explicit predictions for illness statistics		People’s predictions for real-world events have been shown to be well-calibrated to the true environmental statistics (e.g. Griffiths and Tenenbaum 2006). Previous work, however, has focused on predictions for these events by aggregating across observers, making a single estimate for the total duration given a current duration. Here, we focus on assessing predictions for both the mean and form of distributions in the domain of illness duration prediction at the individual level. We assess understanding for both acute illnesses for which people might have experience, as well as chronic conditions for which people are less likely to have knowledge. Our data suggests that for common acute illnesses people can accurately estimate both the mean and form of the distribution. For less common acute illnesses and chronic illnesses, people have a tendency to overestimate the mean duration, but still accurately predict the distribution form.		Talia Robbins;Pernille Hemmer	2017			social psychology;psychology	HCI	7.22077462760864	-73.04644602420744	99245
c619498890f0890707fe210c48dbfee0dfae6529	analyzing patterns of numerously occurring heart diseases using association rule mining		The use of technology and science in Healthcare has made services available to all the people along with ensuring the best care for the people. Data Mining provides us such useful techniques, which can help the medical practitioners to effectively analyze and discover large amount of data in a more efficient and convenient way as now electronic recording system of data has come into existence. Therefore, millions of data are now available and majority of them would have been remained undiscovered, if the data mining techniques were not introduced. In our work, an association based rule mining technique has been used to identify such hidden patterns of the most commonly occurring heart diseases namely Unstable Angina(UA), Myocardial Infarction(MI), Coronary Heart Disease(CHD) etc. among Bangladeshi people and unravelling the hidden information by analyzing the results. Basically, other researchers in this field used the classification and clustering methods of data mining by which they could predict the chance of occurring heart diseases and clustered them to identify the dependency of one attribute to another. The trends or patterns for heart diseases may vary depending on sex, age, socioeconomic condition, demographic regions and so on. The objective of our work is to find out those hidden trends or patterns. Therefore, we have chosen association rule mining technique to find those patterns or trends among patients depending on their age, sex, regions and socioeconomic condition.	association rule learning;cluster analysis;control theory;data mining;kripke semantics	K. M. Mehedi Hasan Sonet;Md. Mustafizur Rahman;Pritom Mazumder;Abid Reza;Rashedur M. Rahman	2017	2017 Twelfth International Conference on Digital Information Management (ICDIM)	10.1109/ICDIM.2017.8244690	data mining;heart disease;myocardial infarction;cluster analysis;computer science;health care;socioeconomic status;association rule learning	ML	4.08138761199631	-76.6710898829038	99322
4b341b80e3a536f42e60f92d3f86bed9f153389f	imputing missing values in unevenly spaced clinical time series data to build an effective temporal classification framework	inverse distance weight;time series;particle swarm optimization;missing value;tolerance rough set	BACKGROUND: In healthcare domain, clinical trials generate time-stamped data that record set of observations on patient health status. These data are liable to missing values since there are situations, where the patient observations are neither done regularly nor updated correctly.#R##N##R##N#OBJECTIVE: This paper aims to impute missing values in an unevenly spaced clinical time-series data by proposing a tolerance rough set induced bio-statistical (TRiBS) framework. The proposed framework adopts an inverse distance weight (IDW) interpolation technique and improves it using the concept of tolerance rough set (TR) and particle swarm optimization (PSO).#R##N##R##N#METHOD: To interpolate an unknown data point, the classical IDW interpolation suffers from two major drawbacks: first, in selecting the known data points and second, choosing an optimal influence factor. TRiBS framework overcomes the first limitation using TR and the second using PSO. TR derives the dependent attributes for each attribute using non-missing records. The nearest significant set is then generated for each missing value based on its attribute dependencies. The PSO technique fixes the weights for the data in a nearest significant set by finding an optimized influence factor. The obtained significant set and its influence factor are used in IDW computations to impute missing value.#R##N##R##N#RESULT: The proposed work is experimented using clinical time series dataset of hepatitis and thrombosis patients. However, the proposed system can support other clinical time series dataset with minor domain specific changes.#R##N##R##N#CONCLUSION: The performance of the imputed results proves the effectiveness of TRiBS. Experimental evaluation with the classifiers such as neural networks, support vector machine (SVM) and decision tree have shown an improvement in the classification accuracy when a missing data is pre-processed with the proposed framework.	missing data;unevenly spaced time series	Y. Nancy Jane;Harichandran Khanna Nehemiah;Arputharaj Kannan	2017	Computational Statistics & Data Analysis	10.1016/j.csda.2017.02.012	missing data;machine learning;time series;data mining;mathematics;particle swarm optimization;statistics	ML	5.211195316584785	-76.47292972205449	99645
e456bfa35752749e74f17779f8f73dbb246b62eb	predicting cardiovascular risks using pattern recognition and data mining	computer science		data mining;pattern recognition	Thuy Thi Thu Nguyen	2009			data science;data mining;computer science	ML	3.6291748167960787	-75.94858946685721	99660
18fca82d6fdac7b0e2f2de1a821bb4602bffbc57	ovarian volume throughout life: a validated normative model	female;growth hormone;rg gynecology and obstetrics;middle aged;polycystic ovary syndrome;adolescent;child preschool;models biological;aging;infant;journal article;ovary;ultrasound imaging;ovaries;adults;infant newborn;adult;child;cancer screening;menopause;puberty;humans;young adult;aged;aged 80 and over	The measurement of ovarian volume has been shown to be a useful indirect indicator of the ovarian reserve in women of reproductive age, in the diagnosis and management of a number of disorders of puberty and adult reproductive function, and is under investigation as a screening tool for ovarian cancer. To date there is no normative model of ovarian volume throughout life. By searching the published literature for ovarian volume in healthy females, and using our own data from multiple sources (combined n=59,994) we have generated and robustly validated the first model of ovarian volume from conception to 82 years of age. This model shows that 69% of the variation in ovarian volume is due to age alone. We have shown that in the average case ovarian volume rises from 0.7 mL (95% CI 0.4-1.1 mL) at 2 years of age to a peak of 7.7 mL (95% CI 6.5-9.2 mL) at 20 years of age with a subsequent decline to about 2.8 mL (95% CI 2.7-2.9 mL) at the menopause and smaller volumes thereafter. Our model allows us to generate normal values and ranges for ovarian volume throughout life. This is the first validated normative model of ovarian volume from conception to old age; it will be of use in the diagnosis and management of a number of diverse gynaecological and reproductive conditions in females from birth to menopause and beyond.	best, worst and average case;malignant neoplasm of ovary;menopause;ovarian reserve;puberty;reproduction;scientific publication;sixty nine;ovarian neoplasm	Thomas W. Kelsey;Sarah K. Dodwell;A. Graham Wilkinson;Tine Greve;Claus Y. Andersen;Richard A. Anderson;W. Hamish B. Wallace	2013		10.1371/journal.pone.0071465	endocrinology;andrology;medicine;young adult;gynecology;ovary	Networks	9.542270460305108	-74.36276424950368	99998
f7be3c8dc2bb486970512dcf8d298ce4577d1922	applying bayesian changepoint model and hierarchical divisive model for detecting anomalies in clinical decision support alert firing		Clinical Decision Support (CDS) Systems are widely used to support efficient evidence-based care and have become an important aspect of healthcare. CDS systems are complex, and sometimes malfunction or exhibit anomalous behavior. We have previously shown how anomaly detection models can be used to successfully identify malfunctions in CDS systems. We have extended this work and applied two new anomaly detection models on CDS alert firing data from a large health system.	anomaly detection;bayesian network;clinical decision support system;sensor	Soumi Ray;Adam Wright	2017		10.1145/3107411.3108200	anomaly detection;clinical decision support system;machine learning;data mining;artificial intelligence;computer science;bayesian probability;pattern recognition	ML	3.800245751329716	-75.80953554906586	100162
e1a7d728548695907efea676831c3b5384d511e7	simulation studies on the dynamics of insulin-glucose in diabetic mellitus patients	renal glucose excretion;drugs;insulin glucose dynamics;sugar biochemistry blood diseases drugs molecular biophysics;liver;glucose insulin interaction;renal glucose excretion insulin glucose dynamics diabetic mellitus patients glucose insulin interaction pharmacokinetic diagrams blood glucose;diabetes;blood glucose;diabetes mellitus;glucose uptake;biomedical engineering;blood;molecular biophysics;pancreas;mathematical model;diseases;production;insulin;diabetic mellitus patients;simulation study;diabetic;sugar;diabetes sugar insulin blood pancreas mathematical model liver muscles biomedical engineering production;biochemistry;pharmacokinetic diagrams;insulin glucose dynamics diabetic;muscles	Glucose-insulin interaction in an insulin-dependent diabetic patient has been simulated using an overall model based on pharmacokinetic diagrams of insulin and glucose. Model is capable of predicting the blood glucose and insulin levels, total glucose uptake and the renal glucose excretion. The treatment strategy is based on a four-daily dose of regular insulin, which is applied through a subcutaneous route 30 min prior to each meal.	diagram;mathematical model;maxima and minima;simulation	Laleh Kardar;Ali Fallah	2008	2008 International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2008.265	endocrinology;biochemistry;medicine;mathematical model;diabetes mellitus;proinsulin;molecular biophysics	Robotics	10.00360895991887	-70.95037395591248	100552
9f66e397c9941e3cbe45c7063004e28741ee8902	development of an optimized multi-biomarker panel for the detection of lung cancer based on principal component analysis and artificial neural network modeling	lung cancer;principal component analysis;biomarkers;diagnosis;artificial neural network	Lung cancer is a public health priority worldwide due to the high mortality rate and the costs involved. Early detection of lung cancer is important for increasing the survival rate, however, frequently its diagnosis is not made opportunely, since detection methods are not sufficiently sensitive and specific. In recent years serum biomarkers have been proposed as a method that might enhance diagnostic capabilities and complement imaging studies. However, when used alone they show low sensitivity and specificity because lung cancer is a heterogeneous disease. Recent reports have shown that simultaneous analysis of biomarkers has the potential to separate lung cancer patients from control subjects. However, it has become clear that a universal biomarker panel does not exist, and optimized panels need to be developed and validated in each population before their application in a clinical setting. In this study, we selected 14 biomarkers from literature, whose diagnostic or prognostic value had been previously demonstrated for lung cancer, and evaluated them in sera from 63 patients with lung cancer and 87 non-cancer controls (58 Chronic Obstructive Pulmonary Disease (COPD) patients and 29 current smokers). Principal component analysis and artificial neural network modeling allowed us to find a reduced biomarker panel composed of Cyfra 21.1, CEA, CA125 and CRP. This panel was able to correctly classify 135 out of 150 subjects, showing a correct classification rate for lung cancer patients of 88.9%, 93.3% and 90% in training, validation and testing phases, respectively. Thus, sensitivity was increased 18.31% (sensitivity 94.5% at specificity 80%) with respect to the best single marker Cyfra 21.1. This optimized panel represents a potential tool for assisting lung cancer diagnosis, therefore it merits further consideration.	artificial neural network;principal component analysis	José Miguel Flores-Fernández;Enrique J. Herrera-López;Francisco Sánchez-Llamas;Antonio Rojas-Calvillo;Paula Anel Cabrera-Galeana;Gisela Leal-Pacheco;María Guadalupe González-Palomar;Ricardo Femat;Moisés Martínez-Velázquez	2012	Expert Syst. Appl.	10.1016/j.eswa.2012.03.008	computer science;machine learning;artificial neural network;biomarker;principal component analysis	ML	7.539082110720281	-76.47600624191423	101667
f29c3cce9a921652126c8f6190a0d2b2ae39fec0	from prescription drug purchases to drug use periods – a second generation method (pre2dup)	health research;uk clinical guidelines;biological patents;health informatics;europe pubmed central;citation search;information systems and communication service;uk phd theses thesis;management of computing and information systems;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	BACKGROUND Databases of prescription drug purchases are now widely used in pharmacoepidemiologic studies. Several methods have been used to generate drug use periods from drug purchases to investigate various aspects; e.g., to study associations between exposure and outcome. Typically, such methods have been fairly simplistic, with fixed assumptions of drug use pattern and or dose (for example, the assumed usage of 1 tablet per day). This paper describes a novel PRE2DUP method that constructs drug use periods from purchase histories, and verified by a validation based on an expert evaluation of the drug use periods generated by the method.   METHODS The PRE2DUP method is a novel approach based on mathematical modelling of personal drug purchasing behaviors. The method uses a decision procedure that includes each person's purchase history for each ATC code, processed in a chronological order. The method constructs exposure time periods and estimates the dose used during the period by considering the purchased amount in Defined Daily Doses (DDDs), which is recorded in the prescription register database. This method takes account of stockpiling of drugs, personal purchasing pattern; i.e., regularity of the purchases, and periods of hospital or nursing home care where drug use is not recorded in the prescription register. The method can be applied to a variety of drug classes with different doses and use patterns by controlling restriction parameters for each ATC class, or even each drug package. In the presented example, the PRE2DUP method was applied to a register-based MEDALZ-2005 study cohort. All drug purchases (3,793,085) recorded in the Finnish prescription register between 2002 and 2009 for persons with Alzheimer's disease (28,093) were included.   RESULTS Results of the expert-opinion based validation indicate that PRE2DUP method creates drug use periods with a relatively high correctness. Drugs with varying patterns of use and drugs used on a short-term basis only require more precise parameters.   CONCLUSIONS PRE2DUP method gives highly accurate drug use periods for most drug classes, especially those meant for long-term use.	advanced transportation controller;adverse reaction to drug;alzheimer's disease;behavior;class;correctness (computer science);database;decision problem;drug packaging;estimated;home care of patient;mathematical model;mathematics;mental association;newton's method;nursing homes;prescription drugs;purchasing;second generation multiplex plus;tablet dosage form;tablet computer;x-ray exposure time	Antti Tanskanen;Heidi Taipale;Marjaana Koponen;Anna-Maija Tolppanen;Sirpa Hartikainen;Riitta Ahonen;Jari Tiihonen	2015		10.1186/s12911-015-0140-z	health informatics;medical research;medicine;nursing;data mining	Comp.	8.138247150662874	-73.08431790723202	101828
056610f2d1e177798fdf7aa35a373f4771c461be	simultaneous assessment of teams in collaborative virtual environments using fuzzy naive bayes	groupware;bayes methods;fuzzy rule based expert systems collaborative virtual environments fuzzy naive bayes computational architectures collaborative training real surgical rooms assessment systems virtual reality system;virtual reality;fuzzy set theory;training virtual reality solid modeling collaboration surgery bayes methods monitoring;computer based training;virtual reality bayes methods computer based training fuzzy set theory groupware	In the recent years, computational architectures have been proposed to allow teams assessment in collaborative training based on virtual reality. In the virtual environments, procedures are performed by a team of professionals acting simultaneously, as in real surgical rooms. It is important to verify if the group performed the procedure correctly or not. The assessment systems utilizes user and users data from the execution of the virtual procedure, generated by the virtual reality system, to be compared with predefined classes of performance. Previous approaches basically used fuzzy rule based expert systems and presented some problems with respect to calibration which was performed in phases. In this paper, we propose a new approach based on Fuzzy Naive Bayes to perform the calibration in a single phase, without lost of accuracy in the assessment of the performance.	collaborative virtual environment;emoticon;expert system;fuzzy rule;interaction;multi-user;naive bayes classifier;simulation;statistical model;virtual reality	Ronei Marcos de Moraes;Liliane S. Machado	2013	2013 Joint IFSA World Congress and NAFIPS Annual Meeting (IFSA/NAFIPS)	10.1109/IFSA-NAFIPS.2013.6608596	simulation;computer science;knowledge management;data mining	Visualization	3.216025914173019	-80.18398703976173	101873
6264f57415269255a9d5cc4e191b4c6db54b0443	an open domain-extensible environment for simulation-based scientific investigation (odessi)	uncertainty quantification;experimental analysis;source localization;computer model;sensitivity analysis;simulation model;domain specificity	In scientific domains where discovery is driven by simulation modeling there are found common methodologies and procedures applied for scientific investigation. ODESSI (Open Domain-extensible Environment for Simulation-based Scientific Investigation) is an environment to facilitate the representation and automatic conduction of scientific studies by capturing common methods for experimentation, analysis, and evaluation used in simulation science. Specific methods ODESSI will support include  parameter studies  ,  optimization  ,  uncertainty quantification  , and  sensitivity analysis  . By making these methods accessible in a programmable framework, ODESSI can be used to capture and run domain-specific investigations. ODESSI is demonstrated for a problem in the neuroscience domain involving computational modeling of human head electromagnetics for conductivity analysis and source localization.	simulation	Adnan Salman;Allen D. Malony;Matthew J. Sottile	2009		10.1007/978-3-642-01970-8_3	computer simulation;uncertainty quantification;simulation;human–computer interaction;computer science;simulation modeling;sensitivity analysis;experimental analysis of behavior	HPC	4.988762136569696	-68.33656153437786	102592
f1ec9d7a3b1c44ade4ebee2f945b731de72ce9e8	proposed new requirements for testing and reporting performance results of arrhythmia detection algorithms	medical signal detection;medical disorders;electrocardiography;statistical analysis;patient monitoring	Several arrhythmia performance measures as specified in the current AAMI recommended practice document do not adequately reflect actual clinical experience in real-time patient monitoring. Additional reporting requirements that are more clinical relevant are thus needed: 1) Due to the large number of QRS complexes that need to be analyzed, even an algorithm with high specificity will generate a large number of false positives, which is not directly reflected by the reported false positive rate. A new recommendation is to report the actual number of false positives. 2) Due to the low PVC prevalence in most monitored patients, the high positive predictive values (PPV) reported using databases with high PVC prevalence are not clinically relevant. A proposed recommendation is to report PPVs at much lower PVC rates. 3) Arrhythmia performance measures as specified in the recommended practice do not include performance bounds. A new proposal is to use the bootstrap method with sample replacement to generate the mean values and 95% confident intervals for all the reported arrhythmia performance measures. Conclusions: Additional performance measures are proposed to further improve the clinical relevance of the reported results. These measures should be considered for inclusion as part of the standard reporting.	algorithm;approximation algorithm;bootstrapping (statistics);database;patients;polyvinyl chloride;premature ventricular contractions;real-time transcription;relevance;requirement;sensitivity and specificity;technical standard	John J. Wang	2013	Computing in Cardiology 2013		medicine;data mining;social psychology;statistics	Web+IR	7.582423860999693	-75.07092505749955	103220
2093e684e33959082ea13209a2d3d11ba6c288df	identifying relatively high-risk group of coronary artery calcification based on progression rate: statistical and machine learning methods	patient diagnosis;regression analysis blood vessels cardiovascular system diseases learning artificial intelligence patient diagnosis;algorithms artificial intelligence calcinosis coronary artery disease diagnosis computer assisted female humans male middle aged pattern recognition automated risk assessment;solid modeling accuracy arteries logistics calcium diseases design automation;diseases;regression analysis;cardiovascular system;learning artificial intelligence;ensemble voting method relatively high risk group identification coronary artery calcification progression rate statistical method machine learning method cac score coronary artery disease death symptom performance accuracy linear regression based classifier logistic regression model;blood vessels	Coronary artery calcification (CAC) score is an important predictor of coronary artery disease (CAD), which is the primary cause of death in advanced countries. Early prediction of high-risk of CAC based on progression rate enables people to prevent CAD from developing into severe symptoms and diseases. In this study, we developed various classifiers to identify patients in high risk of CAC using statistical and machine learning methods, and compared them with performance accuracy. For statistical approaches, linear regression based classifier and logistic regression model were developed. For machine learning approaches, we suggested three kinds of ensemble-based classifiers (best, top-k, and voting method) to deal with imbalanced distribution of our data set. Ensemble voting method outperformed all other methods including regression methods as AUC was 0.781.	area under curve;arteriopathic disease;cessation of life;color gradient;common access card;computer-aided design;coronary artery disease;coronary artery calcification;kerrison predictor;logistic regression;machine learning;patients	Ha Young Kim;Sanghyun Yoo;Jihyun Lee;Hye Jin Kam;Kyoung-Gu Woo;Yoon-Ho Choi;Jidong Sung;Mira Kang	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6346399	medicine;computer science;engineering;artificial intelligence;machine learning;circulatory system;surgery;regression analysis;statistics	ML	7.044919914015599	-76.72846877420788	103324
29cc6b484a13965cc8c3d6ec57c5003b24446cd7	bone metastasis modeling based on the interactions between the bmu and tumor cells	nonlinear differential model;stability;bone metastasis;osteoblast;osteoclast	In recent years there has been a multidisciplinary effort by the scientific community to understand the mechanisms of bone metastasis. In order to obtain more information about the significant factors that give support to the cancer invasion, we propose a nonlinear differential model to describe the dynamics between the tumor cells and the main BMU cells (osteoclasts and osteoblasts). The model is based on a power law functional that represents the paracrine signaling between the BMU cells along with a logistic growth for cancer cells. A stability analysis is carried out and the obtained results provide biological information regarding the success or failure of the invasion of bone metastasis. Numerical simulations are performed in order to illustrate theoretical results. Diverse dynamical interactions of the tumor–osteoclast–osteoblast cells are shown which are consistent with observed experimental behavior.	interaction	Silvia Jerez;Ariel Camacho	2018	J. Computational Applied Mathematics	10.1016/j.cam.2016.12.026	stability;mathematics;statistics	Comp.	7.407371269658759	-67.85793558256277	103388
1b34652da206f9ba53cbdd3fa914a6906861c669	optimal vaccine allocation for the early mitigation of pandemic influenza	influenza vaccines;influenza human;models biological;pandemics;cluster analysis;humans;influenza a virus h5n1 subtype;computational biology;asia southeastern	With new cases of avian influenza H5N1 (H5N1AV) arising frequently, the threat of a new influenza pandemic remains a challenge for public health. Several vaccines have been developed specifically targeting H5N1AV, but their production is limited and only a few million doses are readily available. Because there is an important time lag between the emergence of new pandemic strain and the development and distribution of a vaccine, shortage of vaccine is very likely at the beginning of a pandemic. We coupled a mathematical model with a genetic algorithm to optimally and dynamically distribute vaccine in a network of cities, connected by the airline transportation network. By minimizing the illness attack rate (i.e., the percentage of people in the population who become infected and ill), we focus on optimizing vaccine allocation in a network of 16 cities in Southeast Asia when only a few million doses are available. In our base case, we assume the vaccine is well-matched and vaccination occurs 5 to 10 days after the beginning of the epidemic. The effectiveness of all the vaccination strategies drops off as the timing is delayed or the vaccine is less well-matched. Under the best assumptions, optimal vaccination strategies substantially reduced the illness attack rate, with a maximal reduction in the attack rate of 85%. Furthermore, our results suggest that cooperative strategies where the resources are optimally distributed among the cities perform much better than the strategies where the vaccine is equally distributed among the network, yielding an illness attack rate 17% lower. We show that it is possible to significantly mitigate a more global epidemic with limited quantities of vaccine, provided that the vaccination campaign is extremely fast and it occurs within the first weeks of transmission.	allocation;attack rate;aves;bipolar disorder;emergence;genetic algorithm;illness (finding);influenza in birds;influenza virus vaccine;mathematical model;maximal set;optimizing compiler;quantity;recursion;stmn1 gene;cellular targeting	Laura Matrajt;M. Elizabeth Halloran;Ira M. Longini	2013		10.1371/journal.pcbi.1002964	computational biology;simulation;virology;immunology;cluster analysis;pandemic	Web+IR	7.22369898753107	-70.03418619418824	103599
57563bb8110b3dcc75a57fc5d3d74dcd4b176349	modeling cell-fractone dynamics using mathematical control theory	mathematical model cell fractone dynamics mathematical control theory biological structures fractal like appearance cell biologists cell division cell migration cell differentiation;control systems;control theory;life cycle;growth factor;biological system modeling;mathematical model aerospace electronics diffusion processes biological system modeling control systems biological systems;diffusion processes;control system;control theory cellular biophysics;aerospace electronics;mathematical model;biological systems;cell division;diffusion process;growth process;cellular biophysics	New biological structures called fractones, named in honor of the late Dr. Benoit Mandelbrot due to their fractal-like appearance, have been discovered by cell biologists. Their primary purposes are theorized to pertain to the major processes of the life cycle of cells, namely cell division, migration, and differentiation. In this paper, we build a mathematical model of how fractones interact with the cells and the associated growth factors to gain insight into the growth process.	control theory;fractal;mandelbrot set;mathematical model	Monique Chyba;John Marriott;Frederic Mercier;John Rader;Giulio Telleschi	2011	IEEE Conference on Decision and Control and European Control Conference	10.1109/CDC.2011.6160966	biological life cycle;computer science;control system;diffusion process;mathematical model;control theory;mathematics;cell division	Visualization	6.6926346506977765	-68.19614650356574	103775
845015e4b7aa0d6f7deefc1368ad1a32440b8a1c	context-sensitivity of human memory: episode connectivity and its influence on memory reconstruction	human memory;episodic memory;cognitive science;psychology;cognitive modeling;context effects;cognitive model;context effect	This paper is testing a DUAL-based model of memory. The model assumes decentralized representation of episodes as a coalition of agents and analogical transfer processes as the basis for memory reconstruction of our past. It is a model of active reconstruction thereby allowing memory insertions and blending of episodes. The experiment explores the role of the degree of internal connectivity of the coalition representing the episode on the outcome of the reconstruction process. It demonstrates that the more the links between the elements of the episode are, the higher the number of details we recall, and the lesser the intruded elements and the context influence.	alpha compositing;connectivity (graph theory);context-sensitive language;mike lesser;schematic;strongly connected component	Boicho N. Kokinov;Georgi Petkov;Nadezhda Petrova	2007		10.1007/978-3-540-74255-5_24	cognitive model;childhood memory;context effect;semantic memory;artificial intelligence;explicit memory;memory errors;episodic memory;adaptive memory;memory;social psychology;autobiographical memory;verbal memory;reconstructive memory;implicit memory	AI	-4.149014142456487	-75.71569710457291	104566
1f2739fa9f457d0d7a1a185010576995b39efd13	a novel fuzzy logic inference system for decision support in weaning from mechanical ventilation	decision support;mechanical ventilation;weaning;fuzzy logic;indexation;expert opinion;multi attribute decision making;critically ill patient;weaning predictors	Weaning from mechanical ventilation represents one of the most challenging issues in management of critically ill patients. Currently used weaning predictors ignore many important dimensions of weaning outcome and have not been uniformly successful. A fuzzy logic inference system that uses nine variables, and five rule blocks within two layers, has been designed and implemented over mathematical simulations and random clinical scenarios, to compare its behavior and performance in predicting expert opinion with those for rapid shallow breathing index (RSBI), pressure time index and Jabour’ weaning index. RSBI has failed to predict expert opinion in 52% of scenarios. Fuzzy logic inference system has shown the best discriminative power (ROC: 0.9288), and RSBI the worst (ROC: 0.6556) in predicting expert opinion. Fuzzy logic provides an approach which can handle multi-attribute decision making, and is a very powerful tool to overcome the weaknesses of currently used weaning predictors.	branch predictor;computer simulation;critical illness;decision making;fuzzy logic;inference engine;intermittent positive-pressure breathing;mathematics;mechanical ventilation;numerous;patients;respiration;weakness;anatomical layer	Yusuf Alper Kilic;Ilke Zekiye Kilic	2009	Journal of Medical Systems	10.1007/s10916-009-9327-0	fuzzy logic;intensive care medicine;decision support system;computer science;artificial intelligence	AI	6.1809490120975985	-76.28773673089727	104678
9923ec0ced5dd85ed0ace88d81e97f67fdf9722f	application of artificial neural networks in dementia and alzheimer's diagnosis		Diagnosis in the early phases of many diseases makes it possible to treat the disease and affects the treatment process positively. This is especially important for diseases like Alzheimer in the field of neurology. The use of a computerized support system, which can autonomously perform the diagnostic process by the expert in this process, saves time and helps to reduce the most human errors. In this study, machine learning models with the ability to diagnose dementia and Alzheimeru0027s disease were developed by predicting the Clinical Dementia Rating (CDR) value. Artificial Neural Networks (ANN), Logistic Regression (LR), k-nearest neighbors (KNN), and Decision Tree (DT) classifiers were applied to compare the classification performances. The Open Access Series of Imaging Studies (OASIS) longitudinal and cross-sectional datasets have been used to train models. As a result of the tests, best performance of the detection and identification of Alzheimeru0027s disease has been shown by LR and YSA models.	artificial neural network	Altug Yigit;Zerrin Isik	2018		10.1109/SIU.2018.8404447	artificial intelligence;artificial neural network;pattern recognition;computer science;decision tree;neurology;logistic regression;electroencephalography;clinical dementia rating;dementia	AI	6.722665416519666	-77.05795690189088	104765
576af01083e86a30bbb49bec8ce6096d5722cfbb	on predicting glycaemia in type 1 diabetes mellitus patients by using support vector machines		Type 1 Diabetes Mellitus (DM1) is a chronic disease that forces patients to continuously monitor their status manually, and try to make a prediction, taking into account numerous considerations, to anticipate blood glucose variability and, consequently, make decisions about insulin dosages. In this context, well-known artificial intelligence regression techniques like Support Vector Machines reach new possibilities, achieving high levels of representativeness, informativeness, and accuracy.	artificial intelligence;heart rate variability;support vector machine	Ignacio Rodríguez-Rodríguez;José-Victor Rodríguez	2017		10.1145/3109761.3158404	support vector machine;representativeness heuristic;data mining;computer security;type 1 diabetes;computer science;disease;insulin	AI	6.083338611008812	-77.6674968111906	104880
819e2278b98d2786c26095e0a606584c9c52c638	an iterative method for analysis of joint visit model at dean east clinic	analytical models;cybernetics;medical services;markov processes;conferences;steady state	This paper introduces a case study at Dean East Clinic to model patient flow with joint visits by provider and medical assistant (MA). To reduce the state space dimension, a convergent iterative procedure based on Markov chain model of patient flow is proposed. The study is extended to non-Markovian case by introducing an empirical formula based on the mean and coefficient of variation (CV) of service times. The results have been validated with good accuracy using both collected and randomly generated data.	coefficient;iterative method;markov chain;procedural generation;state space	Hyo-Kyung Lee;Xiang Zhong;Jingshan Li;Albert J. Musa;Philip A. Bain	2016	2016 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/SMC.2016.7844405	simulation;cybernetics;computer science;artificial intelligence;markov process;steady state;operations research;statistics	Robotics	5.5419968112607805	-72.87842259050153	104961
c72680d5efb2de009975354e2a26ce5935f9ae00	artificial neural network prediction of clozapine response with combined pharmacogenetic and clinical data	clinical data;genetic variability;feed forward;decision support;genetic polymorphism;clozapine;multilayer perceptron;logistic regression;receiver operating characteristic curve;schizophrenia;gene polymorphism;neural network models;neural network model;back propagation;leave one out;artificial neural network	Although one third to one half of refractory schizophrenic patients responds to clozapine, however, there are few evidences currently that could predict clozapine response before the use of the medication. The present study aimed to train and validate artificial neural networks (ANN), using clinical and pharmacogenetic data, to predict clozapine response in schizophrenic patients. Five pharmacogenetic variables and five clinical variables were collated from 93 schizophrenic patients taking clozapine, including 26 responders. ANN analysis was carried out by training the network with data from 75% of cases and subsequently testing with data from 25% of unseen cases to determine the optimal ANN architecture. Then the leave-one-out method was used to examine the generalization of the models. The optimal ANN architecture was found to be a standard feed-forward, fully-connected, back-propagation multilayer perceptron. The overall accuracy rate of ANN was 83.3%, which is higher than that of logistic regression (LR) (70.8%). By using the area under the receiver operating characteristics curve as a measure of performance, the ANN outperformed the LR (0.821+/-0.054 versus 0.579+/-0.068; p<0.001). The ANN with only genetic variables outperformed the ANN with only clinical variables (0.805+/-0.056 versus 0.647+/-0.066; p=0.046). The gene polymorphisms should play an important role in the prediction. Further validation of ANN analysis is likely to provide decision support for predicting individual response.		Chao-Cheng Lin;Ying-Chieh Wang;Jen-Yeu Chen;Ying-Jay Liou;Yamei Bai;I-Ching Lai;Tzu-Ting Chen;Hung-Wen Chiu;Yu-Chuan Li	2008	Computer methods and programs in biomedicine	10.1016/j.cmpb.2008.02.004	genetic variability;econometrics;gene polymorphism;computer science;artificial intelligence;machine learning;schizophrenia;logistic regression;multilayer perceptron;feed forward;artificial neural network	ML	7.0868548212813165	-76.14309228459037	105226
a8c4f1a31072450d5a4b73b6a9975b5fe5a7a265	algorithmic challenges in mass spectrometry and systems biology	system approach;mass spectra;mass spectrometry;heterogeneous data;system biology;cell biology	Cell biology is in the middle of a paradigm change where approaches focusing on the biochemically oriented understanding of single genes are slowly replaced by the systems approach that views systems of genes and proteins in their network context [1]. In this talk we will examine a number of new computational challenges associated with this approach, from various aspects of interpreting peptide mass spectra, to the visualization and integration of heterogenous data types in the molecular network context [2-5].	programming paradigm;systems biology	Benno Schwikowski	2002			chemistry;bioinformatics;analytical chemistry;environmental chemistry	Comp.	2.7301150893704285	-66.21667616406432	105319
658790592811c2afc7f9a305e30f1a1f1aee97a2	investigation of the role of ion channels in human pancreatic β-cell hubs: a mathematical modeling study	gap junction coupling;hub cells;mathematical modeling;pancreatic β-cells	In many cellular networks, the structure of the network follows a scale-free organization, where a limited number of cells are strongly coupled to other cells. These cells are called hub cells and their critical roles are well accepted. Despite their importance, there have been only a few studies investigating the characteristic features of these cells. In this paper, a computational approach is proposed to study the possible role of different ion channels in distinguishing between the hub and non-hub cells. The results show that the P/Q-type and T-type calcium channels may have an especial role in the β-cell hubs because the high-level expressions of these channels make a pancreatic β-cell more potent to force other coupled cells to follow it. In addition, in order to consider the variation of the coupling strength with voltage, a novel mathematical model is proposed for the gap junction coupling between the pancreatic β-cells. The proposed approach is validated based on the data from the literature.		Sajjad Farashi;Pezhman Sasanpour;Hashem Rafii-Tabar	2018	Computers in biology and medicine	10.1016/j.compbiomed.2018.04.006	cell;pattern recognition;cell biology;ion channel;artificial intelligence;computer science;voltage-dependent calcium channel;gap junction;cellular network;expression (mathematics)	SE	7.173735001640381	-67.30913640886551	106115
cb8dc6e785779120c7827017262b669ab45f4ab1	a dynamical model of an aeration plant for wastewater treatment using a phenomenological based semi-physical modeling methodology		Diffused aeration is a sensitive process for wastewater treatment. Because of the nonlinearity and complexity of aerator dynamics due to microorganism metabolism and oxygen transfer, reliable mathematical models are needed to perform control-oriented tasks. To this end, in this study we develop a Phenomenological based Semi-physical Model (PBSM) to predict and describe the dynamic behavior of the oxygen transfer in a diffused aeration process by means of a formal modeling methodology. This model will then be validated by using data from an aeration pilot plant. In this paper, we also show a lack of agreement in the literature in terms of the different available ways to represent the volumetric oxygen transfer coefficient kLa. Reasonable agreement between the developed model and plant data is found by considering a phenomenological approach of the kLa instead of considering many of the available empirical correlations in the literature.		Christian Zuluaga-Bedoya;Maribel Ruiz-Botero;Manuel Ospina-Alarcon;J. Garcia-Tirado	2018	Computers & Chemical Engineering	10.1016/j.compchemeng.2018.07.008	aeration;biochemical engineering;mathematical optimization;sewage treatment;mathematical model;microorganism metabolism;pilot plant;nonlinear system;phenomenology (psychology);mathematics	SE	9.799844579327567	-69.05703005951925	106563
2a850b7d5468df92b469b7697f94f85855a712f9	artificial neural networks applications in computer aided diagnosis: system design and use as an educational tool	radiology;computed tomography;computer aided diagnosis;image database;radiography;doctoral consortium;machine learning;medical imaging;computer aided detection;radiology training;convolutional neural network;mammography;medical education;artificial neural network	"""This paper describes the motivation, state-of-the-art, hypotheses and research objectives of the Doctoral Thesis """"Artificial Neural Networks applications in Computer Aided Diagnosis. System design and use as an educational tool"""". A description of the investigation approaches and methodologies, the current dissertation status and expected contributions is also presented. At the time of writing, this dissertation is in its first year of development. Its central topic is Computer Aided Diagnosis and Detection (CAD), a valuable automated tool for specialists who interpret medical images, that provides information which can be used as a """"second opinion"""" or supplementary data in their decision making process. Developing CAD schemes based in the machine learning models called Artificial Neural Networks (ANNs), which could be applied to different image modalities, is the main objective of the first phase of the dissertation. Their integration in a software environment that allows the user to handle and access to information efficiently is of key importance in the process. The validation of the system in clinical practice and the investigation of their possible uses as an educational tool for trainees during residency programs is the second phase."""	artificial neural network;computer-aided design;freedom of information laws by country;machine learning;neural networks;systems design	Jorge Hernández Rodríguez;María José Rodriguez-Conde;Francisco Javier Cabrero Fraile	2016		10.1145/3012430.3012670	simulation;computer science;artificial intelligence;biological engineering	AI	0.012932885306244297	-77.58800965220004	106780
e48d0006ad3fd11d3d2b3197f7d4ec04d02b2f2e	programming strategy for efficient modeling of dynamics in a population of heterogeneous cells		MOTIVATION Heterogeneity is a ubiquitous property of biological systems. Even in a genetically identical population of a single cell type, cell-to-cell differences are observed. Although the functional behavior of a given population is generally robust, the consequences of heterogeneity are fairly unpredictable. In heterogeneous populations, synchronization of events becomes a cardinal problem-particularly for phase coherence in oscillating systems.   RESULTS The present article presents a novel strategy for construction of large-scale simulation programs of heterogeneous biological entities. The strategy is designed to be tractable, to handle heterogeneity and to handle computational cost issues simultaneously, primarily by writing a generator of the 'model to be simulated'. We apply the strategy to model glycolytic oscillations among thousands of yeast cells coupled through the extracellular medium. The usefulness is illustrated through (i) benchmarking, showing an almost linear relationship between model size and run time, and (ii) analysis of the resulting simulations, showing that contrary to the experimental situation, synchronous oscillations are surprisingly hard to achieve, underpinning the need for tools to study heterogeneity. Thus, we present an efficient strategy to model the biological heterogeneity, neglected by ordinary mean-field models. This tool is well posed to facilitate the elucidation of the physiologically vital problem of synchronization.   AVAILABILITY The complete python code is available as Supplementary Information.   CONTACT bjornhald@gmail.com or pgs@kiku.dk   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.	algorithmic efficiency;bioinformatics;biological system;cobham's thesis;computer simulation;entity;genetic heterogeneity;guarded suspension;jacobian matrix and determinant;medicine;neural oscillation;one thousand;oscillator device component;population;python;robustness (computer science);run time (program lifecycle phase);science;simulation;suspensions;yeast cell measurement	Bjørn Olav Hald;Morten Garkier Hendriksen;Preben Graae Sørensen	2013	Bioinformatics	10.1093/bioinformatics/btt132	biology;simulation;computer science;bioinformatics;artificial intelligence;mathematics;statistics	Comp.	5.767943481406401	-67.29436993149923	107058
9cc73783d8a71150f6dcb78dfabf8b7ffa943dbd	linear time-invariant system based assessment model for coronary heart disease	cardiology;会议论文;mathematical model data models measurement heart real time systems arteries diseases;medical computing;regression analysis cardiology digital simulation medical computing real time systems;linear time invariant system risk regression models black box system identification process real time assessment chd risk lti system coronary heart disease assessment model;regression analysis;digital simulation;real time systems	This study proposes a linear time-invariant (LTI) system based assessment approach for coronary heart disease (CHD) risk. Unlike traditional risk regression models, the new approach considers accumulated effects of CHD factors with time and can thus perform time-based simulation and real time assessment for the progression of coronary heart disease. There are several LTI-based models achieved in this study via black box system identification process on a 1,549-men cohort. These models have good fitting on the sample data and can adequately reproduce results of the current dominant risk models. Our findings verified that the LTI-based modeling approach works for time-based CHD assessment - such models can be used for time-based simulation and real time evaluation if sufficient sample data is observed.	black box;color gradient;experiment;financial risk modeling;linear time-invariant theory;riskmetrics;simulation;system identification;time complexity;time-invariant system	Zening Qu;Yongqiang Lyu;Yida Tang;Wenyao Wang;Zihan Wang;Jiaming Hong;Nazim Agoulmine	2013	2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013)	10.1109/HealthCom.2013.6720713	simulation;engineering;operations management;biological engineering	SE	8.957723263821077	-72.10847232301657	107191
54c2767e68d62fd5d1a6fc2f1fa689273ce44e1e	sequential representation of clinical data for full-fitting survival prediction	hazards;sequential analysis;maximum likelihood estimation;robustness;predictive models;context;data models	Survival prediction on time-to-event data associated with patients is crucial in clinical research. Cox-type regression models are widely used for such prediction, but their performance for practical survival prediction suffers due to their use of a maximum partial likelihood estimator, which undermines the effectiveness and robustness of such models. To address this problem, we propose to maximize a new full likelihood that fits the model to all of the data for both failed and censored patients. We also represent time-to-event data by a new sequencing structure, which allows the proposed likelihood to be estimated by predicting event occurrence across the unit time intervals in practice. Furthermore, the likelihood is regularized to prevent overfitting from arising in the model learning step. We investigate the new approach via experimental studies on real-life clinical data and its superior performance compared to other popular state-of-the-art models reveals the great promise of our approach for clinical prediction.	curve fitting;fits;overfitting;predictive modelling;real life	Jianfei Zhang;Lifei Chen;Aurélien Bach;Josiane Courteau;Alain Vanasse;Shengrui Wang	2017	2017 31st International Conference on Advanced Information Networking and Applications Workshops (WAINA)	10.1109/WAINA.2017.90	data modeling;hazard;computer science;sequential analysis;data mining;predictive modelling;maximum likelihood;quasi-maximum likelihood;statistics;robustness	ML	4.8051544703077305	-74.0519399617895	107534
999ab5e88fc519b7fca9510e5ca882f341f1bb8e	neural field simulator: two-dimensional spatio-temporal dynamics involving finite transmission speed	neural field;biological patents;biomedical journals;text mining;3d visualization;europe pubmed central;pattern formation;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;numerical simulation;bioinformatics;literature search	Neural Field models (NFM) play an important role in the understanding of neural population dynamics on a mesoscopic spatial and temporal scale. Their numerical simulation is an essential element in the analysis of their spatio-temporal dynamics. The simulation tool described in this work considers scalar spatially homogeneous neural fields taking into account a finite axonal transmission speed and synaptic temporal derivatives of first and second order. A text-based interface offers complete control of field parameters and several approaches are used to accelerate simulations. A graphical output utilizes video hardware acceleration to display running output with reduced computational hindrance compared to simulators that are exclusively software-based. Diverse applications of the tool demonstrate breather oscillations, static and dynamic Turing patterns and activity spreading with finite propagation speed. The simulator is open source to allow tailoring of code and this is presented with an extension use case.	algorithm;computer simulation;directx video acceleration;graphical user interface;graphics hardware;hardware acceleration;mesoscopic physics;neural ensemble;open-source software;population dynamics;simulators;software propagation;synaptic package manager;text-based (computing);text-based user interface;the chemical basis of morphogenesis;turing machine	Eric J. Nichols;Axel Hutt	2015		10.3389/fninf.2015.00025	text mining;neuroscience;simulation;visualization;computer science;bioinformatics;theoretical computer science;pattern formation	Visualization	5.313264324951223	-68.873204828423	108176
b4c8f36463188903d46ed6f4b6de690054b1734e	nucleic acids for ultra-sensitive protein detection	aptamer;nucleic acids;pcr;biosensor;proteins;aptamers nucleotide;biomarker;humans;proteomics;protein;nucleic acid	"""Major advancements in molecular biology and clinical diagnostics cannot be brought about strictly through the use of genomics based methods. Improved methods for protein detection and proteomic screening are an absolute necessity to complement to wealth of information offered by novel, high-throughput sequencing technologies. Only then will it be possible to advance insights into clinical processes and to characterize the importance of specific protein biomarkers for disease detection or the realization of """"personalized medicine"""". Currently however, large-scale proteomic information is still not as easily obtained as its genomic counterpart, mainly because traditional antibody-based technologies struggle to meet the stringent sensitivity and throughput requirements that are required whereas mass-spectrometry based methods might be burdened by significant costs involved. However, recent years have seen the development of new biodetection strategies linking nucleic acids with existing antibody technology or replacing antibodies with oligonucleotide recognition elements altogether. These advancements have unlocked many new strategies to lower detection limits and dramatically increase throughput of protein detection assays. In this review, an overview of these new strategies will be given."""	biological markers;biopolymer sequencing;cell nucleus;complement system proteins;high-throughput computing;molecular biology;nucleic acids;personalization;precision medicine;proteomics;requirement;throughput	Kris P. F. Janssen;Karel Knez;Dragana Spasic;Jeroen Lammertyn	2013		10.3390/s130101353	biochemistry;nucleic acid;molecular biology;bioinformatics;proteomics	Comp.	2.2995730772810994	-68.12501839905927	108917
372157c016f6581f1cd54f8091bed2e6b75df140	constructed charts of vaccination strategies	selected works;statistical significance;multidimensional scaling;vietnam;disease prevention;mathematical model;developing country;bepress;tuberculosis	Various vaccination rates are mathematically modeled as if they simply spread from area to next nearest area across a 'constructed landscape' of developing countries. A technique that had previously been used to model the spread of diseases effectively models the spread of disease prevention. Multidimensional scaling successfully summarizes complex patterns in vaccination rates in developing countries as a 'constructed chart' from 'functional distances'. Countries that have similar vaccination rates are close together in this constructed chart, and countries that have different rates are far apart, regardless of the physical distance between the countries. A statistically significant (p < 0.001) chart was made of seven different vaccination rates (diphtheria, polio, measles in adults, measles in infants, tuberculosis in adults, tuberculosis in infants, and total percentage of routine epidemic vaccines financed by government) in 49 developing countries (from Belarus, Belize and Benin to Vietnam, Yemen and Zimbabwe).	chart;diphtheria;forty nine;image scaling;measles vaccine;multidimensional scaling;poliomyelitis;test scaling;tuberculosis	Lincoln Gray;Jennifer A. McCabe;David Bernstein	2010	Health informatics journal	10.1177/1460458209353557	medicine;multidimensional scaling;developing country;computer science;mathematical model;statistical significance;statistics	ML	7.100748717557942	-70.33316215001416	108934
ce5e67e3d17645302f7a91d7d0c8bc3428884187	the spatial resolution of epidemic peaks	stochastic processes;epidemics;geography	The emergence of novel respiratory pathogens can challenge the capacity of key health care resources, such as intensive care units, that are constrained to serve only specific geographical populations. An ability to predict the magnitude and timing of peak incidence at the scale of a single large population would help to accurately assess the value of interventions designed to reduce that peak. However, current disease-dynamic theory does not provide a clear understanding of the relationship between: epidemic trajectories at the scale of interest (e.g. city); population mobility; and higher resolution spatial effects (e.g. transmission within small neighbourhoods). Here, we used a spatially-explicit stochastic meta-population model of arbitrary spatial resolution to determine the effect of resolution on model-derived epidemic trajectories. We simulated an influenza-like pathogen spreading across theoretical and actual population densities and varied our assumptions about mobility using Latin-Hypercube sampling. Even though, by design, cumulative attack rates were the same for all resolutions and mobilities, peak incidences were different. Clear thresholds existed for all tested populations, such that models with resolutions lower than the threshold substantially overestimated population-wide peak incidence. The effect of resolution was most important in populations which were of lower density and lower mobility. With the expectation of accurate spatial incidence datasets in the near future, our objective was to provide a framework for how to use these data correctly in a spatial meta-population model. Our results suggest that there is a fundamental spatial resolution for any pathogen-population pair. If underlying interactions between pathogens and spatially heterogeneous populations are represented at this resolution or higher, accurate predictions of peak incidence for city-scale epidemics are feasible.	emergence;genetic heterogeneity;health care;image resolution;incidence matrix;interaction;pathogenic organism;population density;population model;sampling (signal processing);intensive care unit	Harriet L. Mills;Steven Riley	2014		10.1371/journal.pcbi.1003561	stochastic process;simulation;statistics	HCI	6.495874275675644	-71.53169062271562	109317
592c822a847b5899fef4dad9a8137df7a8137430	disease surveillance, case study		Event Detection Identifying patterns of interest in large temporal datasets Spatial Scan Statistic A method for identifying hotspots in spatial data, widely used in epidemiology and biosurveillance Scoring Function An objective function that measures the anomalousness of a subset of data LTSS Linear-time subset scanning Time to Detect Evaluation metric; time delay before detecting an event Overlap Evaluation metric; accuracy of detected subsets of data Detection Power Evaluation metric; proportion of detected events	broadcast delay;hotspot (wi-fi);loss function;optimization problem;overlap–add method;scoring functions for docking;sensor;time complexity	Skyler Speakman;Sriram Somanchi;Edward McFowland;Daniel B. Neill	2014		10.1007/978-1-4614-6170-8_283	medical emergency;disease surveillance;medicine	SE	9.036300629539173	-78.37991599454463	110116
76feb07f8f0e2b2266bf04bd0af8d5bdc52d1af6	computer scientists at the biology lab bench	genetics;bioinformatics	We present the development, implementation and evaluation of a new team-taught introductory computer science course focused on the topic of bioinformatics. Our course is unique when compared to other bioinformatics and interdisciplinary (biology and computer science) courses taught to undergraduates. Instead of analyzing data provided to them, students collect their own data at the biology lab bench and then analyze their data utilizing various bioinformatics tools in the computer lab. In addition, while other bioinformatics classes focus on programming, our course introduces other computer science topics relevant to the biological problems under investigation such as artificial intelligence, networks and databases. Our approach resulted in students acquiring an appreciation for how biology and computer science function synergistically.	artificial intelligence;bioinformatics;computer lab;computer scientist;database;synergy	Andrea Tartaro;Renee J. Chosed	2015		10.1145/2676723.2677246	computational biology;computer science;bioinformatics	Theory	-1.4116297811306904	-66.42431906483449	110517
57fea4535c34b6a40f201bb2d66a072c20ee3bc4	monte carlo simulation experiments for analysis of hiv vaccine effects and vaccine trial design	monte carlo simulation;vaccine trial design characteristic;monte carlo simulation experiment;simulation system;discrete-event simulation;hiv vaccine effect;plausible hiv vaccine trial;hiv vaccine trial design;explicit simulation;vaccine effect;rpt design vaccine trial;vaccine effect estimate;medical simulation;statistical power;statistical analysis;computational modeling;bias;statistical independence;stochastic simulation;discrete event simulation;infectious disease;design methodology;monte carlo methods;mathematical model	The field of infectious disease epidemiology has increasingly adopted stochastic simulation technologies to simulate complex infectious disease transmission systems. Such simulations have both increased the scientific understanding of infectious disease transmission dynamics and served as important tools for evaluating epidemiologic study designs and statistical methods. This paper reports on a discrete-event simulation to analyze the recently developed Retrospective Partner Trials (RPT) HIV vaccine trial design. A specially designed simulation system, HIVSIM, was used to simulate data resulting from the RPT design vaccine trials. HIVSIM explicitly models complex HIV transmission dynamics (e.g., sexual partner mixing patterns and concurrent sexual partnerships) and vaccine trial design characteristics. Monte Carlo simulation analyses conducted with HIVSIM indicate that the RPT design is able to produce vaccine effect estimates with acceptably small bias, high precision and excellent statistical power under plausible HIV vaccine trial conditions. Additionally, the explicit simulation of HIV transmission dynamics permits investigations into the common, but unwarranted, statistical independence assumptions routinely used in the estimation of vaccine effects.	computer simulation;experiment;mixing patterns;monte carlo method	Daniel C. Barth-Jones;Andrew L. Adams;James S. Koopman	2000			medical simulation;econometrics;simulation;infectious disease;mathematics;statistics;monte carlo method	EDA	9.31287103159743	-70.25243145343724	110529
2844173d782c5c17d53b79fab2430fbd1b2888de	soft computing for recognition based on biometrics	fuzzy logic;information source;bio-inspired optimization algo-rithms;powerful hybrid intelligent system;intelligent computing paradigm;soft computing;bio-inspired model;soft computing technique;image analysis;hybrid intelligent system	Find the secret to improve the quality of life by reading this soft computing for recognition based on biometrics. This is a kind of book that you need now. Besides, it can be your favorite book to read after having this book. Do you ask why? Well, this is a book that has different characteristic with others. You may not need to know who the author is, how well-known the work is. As wise word, never judge the words from who speaks, but make the words as your good value to your life.	biometrics;need to know;soft computing;the quality of life	Patricia Melin;Janusz Kacprzyk;A. Pedrycz	2010		10.1007/978-3-642-15111-8	computer science;artificial intelligence;theoretical computer science;machine learning;soft computing	NLP	-4.218150617865127	-71.9757257902356	110585
083a8c0a9937a04f013218c9a458af645f11aa89	high-level modeling for computer-aided clinical trials of medical devices	heart;physiology;computational modeling;medical diagnostic imaging;clinical trials	Medical devices like the Implantable Cardioverter Defibrillator (ICD) are life-critical systems. Malfunctions of the device can cause serious injury or death of the patient. In addition to rigorous testing and verification during the development process, new medical devices often go through clinical trials to evaluate their safety and performance on sample populations. Clinical trials are costly and prone to failure if not planned and executed properly. Evaluating devices on computer models of the relevant physiological systems can provide helpful insights into the safety and efficacy of the device, thus helping to plan and execute a clinical trial. In this paper, we demonstrate how to develop high-level physiological models of cardiac electrophysiology and how to apply them to the Rhythm ID Head to Head Trial (RIGHT), a 5-year long clinical trial for comparing two ICDs. We refer to this as a Computer-Aided Clinical Trial (CACT). We explored two modeling options, a white-box model capturing the mechanisms of the physiological behaviors, and a blackbox model which uses machine learning methods to synthesize physiological input signals. Both models were able to generate physiological inputs to the ICDs and we discuss the challenges and appropriateness of the two modeling options.	computer simulation;debugging;high- and low-level;high-level programming language;implantable cardioverter-defibrillator;machine learning;norm (social);population	Houssam Abbas;Zhihao Jiang;Kuk Jin Jang;Marco Beccani;Jackson J Liang;Rahul Mangharam	2016	2016 IEEE International High Level Design Validation and Test Workshop (HLDVT)	10.1109/HLDVT.2016.7748260	simulation;computational model;heart	EDA	4.9237737408140765	-73.80913340686071	110632
b191fe28da072153727eef00a2453e0a505e8279	gene- and evidence-based candidate gene selection for schizophrenia and gene feature analysis	psychiatric disorder;candidate genes;association study;schizophrenia;association studies;odd ratio;feature analysis;odds ratio;candidate gene	OBJECTIVE Schizophrenia is a chronic psychiatric disorder that affects about 1% of the population globally. A tremendous amount of effort has been expended in the past decade, including more than 2400 association studies, to identify genes influencing susceptibility to the disorder. However, few genes or markers have been reliably replicated. The wealth of this information calls for an integration of gene association data, evidence-based gene ranking, and follow-up replication in large sample. The objective of this study is to develop and evaluate evidence-based gene ranking methods and to examine the features of top-ranking candidate genes for schizophrenia.   METHODS We proposed a gene-based approach for selecting and prioritizing candidate genes by combining odds ratios (ORs) of multiple markers in each association study and then combining ORs in multiple studies of a gene. We named it combination-combination OR method (CCOR). CCOR is similar to our recently published method, which first selects the largest OR of the markers in each study and then combines these ORs in multiple studies (i.e., selection-combination OR method, SCOR), but differs in selecting representative OR in each study. Features of top-ranking genes were examined by Gene Ontology terms and gene expression in tissues.   RESULTS Our evaluation suggested that the SCOR method overall outperforms the CCOR method. Using the SCOR, a list of 75 top-ranking genes was selected for schizophrenia candidate genes (SZGenes). We found that SZGenes had strong correlation with neuro-related functional terms and were highly expressed in brain-related tissues.   CONCLUSION The scientific landscape for schizophrenia genetics and other complex disease studies is expected to change dramatically in the next a few years, thus, the gene-based combined OR method is useful in candidate gene selection for follow-up association studies and in further artificial intelligence in medicine. This method for prioritization of candidate genes can be applied to other complex diseases such as depression, anxiety, nicotine dependence, alcohol dependence, and cardiovascular diseases.	alcoholic intoxication, chronic;anxiety disorders;applications of artificial intelligence;body tissue;candidate disease gene;cardiovascular diseases;depressive disorder;ethanol;gene expression;gene ontology term enrichment;genes, vif;kde applications;mental disorders;name;neuronal plasticity;nicotine dependence;odds ratio;p-value;population;replication (computing);schizophrenia;scientific publication;substance-related disorders;united states national institutes of health;funding grant	Jingchun Sun;Leng Han;Zhongming Zhao	2010	Artificial intelligence in medicine	10.1016/j.artmed.2009.07.009	bioinformatics;candidate gene;odds ratio	Comp.	8.844013322436991	-74.17384025243625	110835
fae217ab21ec38bf182f1645b8c26c844d24388d	high-performance computational analysis of glioblastoma pathology images with database support identifies molecular and survival correlates	health research;uk clinical guidelines;large scale image analysis;biological patents;genomics;translational research glioblastoma pathology images microscopic image analysis data management high performance computational analysis nuclei segmentation feature computation multicore cpus graphical processor units spatial relational database support cancer genome atlas dataset patient survival molecular data proneural subtype glioblastomas nuclear eccentricity nuclear extent minoraxislength gene expressions stem cell marker myc cell proliferation maker mki67;brain;image segmentation;europe pubmed central;glioblastoma;citation search;translational research glioblastoma large scale image analysis survival analysis phenotype genotype integration;tumours;microscopy;tumours bioinformatics brain cellular biophysics feature extraction genetics genomics graphics processing units image segmentation medical image processing molecular biophysics neurophysiology;genetics;gene expression;translational research;uk phd theses thesis;feature extraction;image reconstruction;medical image processing;graphics processing units;spatial databases;molecular biophysics;survival analysis;life sciences;image analysis;neurophysiology;uk research reports;medical journals;cellular biophysics;pathology;europe pmc;biomedical research;phenotype genotype integration;bioinformatics;image analysis pathology image segmentation image reconstruction microscopy spatial databases gene expression	In this paper, we present a novel framework for microscopic image analysis of nuclei, data management, and high performance computation to support translational research involving nuclear morphometry features, molecular data, and clinical outcomes. Our image analysis pipeline consists of nuclei segmentation and feature computation facilitated by high performance computing with coordinated execution in multi-core CPUs and Graphical Processor Units (GPUs). All data derived from image analysis are managed in a spatial relational database supporting highly efficient scientific queries. We applied our image analysis workflow to 159 glioblastomas (GBM) from The Cancer Genome Atlas dataset. With integrative studies, we found statistics of four specific nuclear features were significantly associated with patient survival. Additionally, we correlated nuclear features with molecular data and found interesting results that support pathologic domain knowledge. We found that Proneural subtype GBMs had the smallest mean of nuclear Eccentricity and the largest mean of nuclear Extent, and MinorAxisLength. We also found gene expressions of stem cell marker MYC and cell proliferation maker MKI67 were correlated with nuclear features. To complement and inform pathologists of relevant diagnostic features, we queried the most representative nuclear instances from each patient population based on genetic and transcriptional classes. Our results demonstrate that specific nuclear features carry prognostic significance and associations with transcriptional and genetic classes, highlighting the potential of high throughput pathology image analysis as a complementary approach to human-based review and translational research.	cell proliferation;central processing unit;class;complement system proteins;computation (action);distance (graph theory);gene expression;genetic translation process;glioblastoma;graphics processing unit;image analysis;largest;mathematical morphology;mental association;morphometric analysis;morphometrics;multi-core processor;non-small cell lung carcinoma;numerous;patients;relational database;silo (dataset);stem cells;supercomputer;throughput;transcription, genetic;translational research	Jun Kong;Fusheng Wang;George Teodoro;Lee A. D. Cooper;Carlos S. Moreno;Tahsin M. Kurç;Tony Pan;Joel H. Saltz;Daniel J. Brat	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732495	iterative reconstruction;biology;genomics;image analysis;gene expression;feature extraction;computer science;bioinformatics;microscopy;data mining;survival analysis;image segmentation;translational research;neurophysiology;genetics;molecular biophysics	HPC	0.4475923793639301	-67.12997551862102	111107
a844e96324cd80aa1136bf3ca8f47dd732ee5f65	cluster analytic strategy for identification of metagenes relevant for prognosis of node negative breast cancer		Worldwide, breast cancer is the second leading cause of cancer deaths in women. To gain insight into the processes related to the course of the disease, human genetic data can be used to identify associations between gene expression and prognosis. Moreover, the expression data of groups of genes may be aggregated to metagenes that may be used for investigating complex diseases like breast cancer. Here we introduce a cluster analytic approach for identification of potentially relevant metagenes. In a first step of our approach we used gene expression patterns over time of erbB2 breast cancer MCF7 cell lines to obtain promising sets of genes for a metagene calculation. For this purpose, two cluster analytic approaches for short time-series of gene expression data – DIB-C and STEM – were applied to identify gene clusters with similar expression patterns. Among these we next focussed on groups of genes with transcription factor (TF) binding site enrichment or associated with a GO group. These gene clusters were then used to calculate metagenes of the gene expression data of 766 breast cancer patients from three breast cancer studies. In the last step of our approach Cox models were applied to determine the effect of the metagenes on the prognosis. Using this strategy we identified new metagenes that were associated with metastasis-free survival patients. E. Freis ( ) Department of Statistics, Dortmund University of Technology, Dortmund, Germany Leibniz Research Centre for Working Environment and Human Factors (IfADo), Dortmund University of Technology, Dortmund, Germany e-mail: freis@statistik.tu-dortmund.de S. Selinski J.G. Hengstler Leibniz Research Centre for Working Environment and Human Factors (IfADo), Dortmund University of Technology, Dortmund, Germany K. Ickstadt Department of Statistics, Dortmund University of Technology, Dortmund, Germany W. Gaul et al. (eds.), Challenges at the Interface of Data Analysis, Computer Science, and Optimization, Studies in Classification, Data Analysis, and Knowledge Organization, DOI 10.1007/978-3-642-24466-7 48, © Springer-Verlag Berlin Heidelberg 2012 475	computer science;email;gene ontology term enrichment;human factors and ergonomics;knowledge organization;program optimization;springer (tank);time series;transcription (software)	Evgenia Freis;Silvia Selinski;Jan G. Hengstler;Katja Ickstadt	2010		10.1007/978-3-642-24466-7_48	oncology	Comp.	0.546508063935853	-74.47581698917419	111824
53e28e1c1650133f1e78cb6d985ecf13530319e3	from micro to macro: data driven phenotyping by densification of longitudinal electronic medical records	medical informatics;sparse learning;phenotyping;matrix completion;densification	Inferring phenotypic patterns from population-scale clinical data is a core computational task in the development of personalized medicine. One important source of data on which to conduct this type of research is patient Electronic Medical Records (EMR). However, the patient EMRs are typically sparse and noisy, which creates significant challenges if we use them directly to represent patient phenotypes. In this paper, we propose a data driven phenotyping framework called Pacifier (PAtient reCord densIFIER), where we interpret the longitudinal EMR data of each patient as a sparse matrix with a feature dimension and a time dimension, and derive more robust patient phenotypes by exploring the latent structure of those matrices. Specifically, we assume that each derived phenotype is composed of a subset of the medical features contained in original patient EMR, whose value evolves smoothly over time. We propose two formulations to achieve such goal. One is Individual Basis Approach (IBA), which assumes the phenotypes are different for every patient. The other is Shared Basis Approach (SBA), which assumes the patient population shares a common set of phenotypes. We develop an efficient optimization algorithm that is capable of resolving both problems efficiently. Finally we validate Pacifier on two real world EMR cohorts for the tasks of early prediction of Congestive Heart Failure (CHF) and End Stage Renal Disease (ESRD). Our results show that the predictive performance in both tasks can be improved significantly by the proposed algorithms (average AUC score improved from 0.689 to 0.816 on CHF, and from 0.756 to 0.838 on ESRD respectively, on diagnosis group granularity). We also illustrate some interesting phenotypes derived from our data.	algorithm;cryptographic hash function;excalibur: morgana's revenge;infiniband;mathematical optimization;personalization;smoothing;sparse matrix	Jiayu Zhou;Fei Wang;Jianying Hu;Jieping Ye	2014		10.1145/2623330.2623711	urban density;health informatics;computer science;bioinformatics;data science;phenotype;data mining	ML	2.7004181683825754	-72.93534028364829	112207
63e68810de32c978da07ee22015b2e7c96fb735c	retrieval of clinical science information using an interactive activation and competition network	parallel distributed processing;cardiology;information storage and retrieval	A method for storing and retrieving tabulated clinical science information is described. This method uses the interactive activation and competition network and belongs to the realm of parallel distributed processing. The advantages of this method are that information is readily retrievable by name and by any content, and that the best matched information is automatically returned when partially incorrect cues are given. Furthermore, it allows a variable degree of association among different units of information. The basic principles of this method are illustrated through a simple example from clinical bacteriology. In addition an application of this method to the retrieval of information in cardiology is presented.		K. J. Cheng	1996	Artificial intelligence in medicine	10.1016/0933-3657(95)00041-0	connectionism;computer science;artificial intelligence;theoretical computer science;information retrieval;human–computer information retrieval	AI	-1.34921707280574	-69.87819756866514	112545
da4a6267f2bdab48e318e4022fb2c8986ce3bbe4	a four-compartment model for ca2+ dynamics: an interpretation of ca2+ decay after repetitive firing of intact nerve terminals	eigenfunction expansion;exponential function;differential equation;nerve terminal;mathematical model;compartment model;ryanodine receptor;fitness function	In the presynaptic nerve terminals of the bullfrog sympathetic ganglia, repetitive nerve firing evokes [Ca2+] transients that decay monotonically. An algorithm based on an eigenfunction expansion method was used for fitting these [Ca2+] decay records. The data were fitted by a linear combination of two to four exponential functions. A mathematical model with three intraterminal membrane-bound compartments was developed to describe the observed Ca2+ decay. The model predicts that the number of exponential functions, n, contained in the decay data corresponds to n − 1 intraterminal Ca2+ stores that release Ca2+ during the decay. Moreover, when a store stops releasing or starts to release Ca2+, the decay data should be fitted by functions that contain one less exponential component for the former and one more for the latter than do the fitting functions for control data. Because of the current lack of a parameter by which quantitative comparisons can be made between two decay processes when at least one of them contained more than one exponential components, we defined a parameter, the overall rate (OR) of decay, as the trace of the coefficient matrix of the differential equation systems of our model. We used the mathematical properties of the model and of the OR to interpret effects of ryanodine and of a mitochondria uncoupler on Ca2+ decay. The results of the analysis were consistent with the ryanodine-sensitive store, mitochondria, and another, yet unidentified store release Ca2+ into the cytosol of the presynaptic nerve terminals during Ca2+ decay. Our model also predicts that mitochondrial Ca2+ buffering accounted for more than 86% of all the flux rates across various membranes combined and that there are type 3 and type 1 and/or type 2 ryanodine receptors in these terminals.	abducens nerve diseases;anatomical compartments;calcium ion;coefficient;contain (action);cytoplasmic matrix;greater than;mathematical model;mathematics;mitochondrial diseases;multi-compartment model;nsa product types;nerve endings;population parameter;rana catesbeiana;ryanodine;seizures;sympathetic ganglia;tissue membrane;transient (computer programming);algorithm;exponential	Yan-Yi Peng;Kwang-Shang Wang	2000	Journal of Computational Neuroscience	10.1023/A:1008954127682	mathematical optimization;exponential function;mathematical model;mathematics;ryanodine receptor;fitness function;differential equation;quantum mechanics	ML	9.519168480659339	-66.32098896006269	113042
908cb19cdb4f58a514625fc60ae57cf5e4741da4	cleansing and imputation of body mass index data and its impact on a machine learning based prediction model	electronic health records;body mass index;data cleansing;data imputation;machine learning;predictive modelling	BACKGROUND A challenge of using electronic health records for secondary analyses is data quality. Body mass index (BMI) is an important predictor for various diseases but often not documented properly.   OBJECTIVES The aim of our study is to perform data cleansing on BMI values and to find the best method for an imputation of missing values in order to increase data quality. Further, we want to assess the effect of changes in data quality on the performance of a prediction model based on machine learning.   METHODS After data cleansing on BMI data, we compared machine learning methods and statistical methods in their accuracy of imputed values using the root mean square error. In a second step, we used three variations of BMI data as a training set for a model predicting the occurrence of delirium.   RESULTS Neural network and linear regression models performed best for imputation. There were no changes in model performance for different BMI input data.   CONCLUSION Although data quality issues may lead to biases, it does not always affect performance of secondary analyses.		Stefanie Jauk;Diether Kramer;Werner Leodolter	2018	Studies in health technology and informatics	10.3233/978-1-61499-858-7-116	body mass index;imputation (statistics);statistics;computer science	DB	5.617220368431658	-75.65498593199352	113055
c46ac3c61a9c109691befddf175ee3638d027c92	applying statistical, uncertainty-based and connectionist approaches to the prediction of fetal outcome: a comparative study	prediction method;discriminant analysis;expert prediction systems;comparative study;intelligent system;validation of intelligent systems;stress testing;artificial neural network;knowledge discovery	A common situation in the field of medicine is the availability of a huge quantity of data and knowledge relevant to a problem which is nevertheless, to a greater or lesser degree, incomplete or imprecise. This kind of problem occurs, for example, with respect to the information available for the prognostic tasks required for pregnancy monitoring, where decision-making by physicians calls for the incorporation of predictive skills. There are available however, several knowledge discovery methods that can be applied to data resulting from the performance of one or several of the non-stress tests (NSTs) that are used to evaluate a pregnant patient's antenatal status. This paper presents, discusses and compares the results obtained as a consequence of the application of different prediction methods, namely the Bayes' model, discriminant analysis, artificial neural networks (ANNs) and the Shortliffe and Buchanan uncertainty-based model.		Amparo Alonso-Betanzos;Eduardo Mosqueira-Rey;Vicente Moret-Bonillo;Belén Baldonedo del Río	1999	Artificial intelligence in medicine	10.1016/S0933-3657(99)00013-5	computer science;artificial intelligence;machine learning;comparative research;data mining;artificial neural network;stress testing;statistics	ML	5.4390880362524054	-76.88475170427219	114131
02a9df8bb3ddb1077e606216d1886ce6b670dede	a review of medical applications of genetic and evolutionary computation	clinical expert systems and knowledge based systems;data mining obtaining information identifying patterns and relationships;medical imaging special case of signal processing two or three dimensional data;data mining medical data and patient records;application of gec to medicine;medical application review genetic and evolutionary computation;clinical diagnosis and therapy;medical imaging and signal processing;modelling and simulation of medical processes			Stephen L. Smith	2010		10.1002/9780470973134.ch3	computer science;bioinformatics;data science;data mining;medical algorithm	Theory	1.9102370172764607	-74.93580069617524	114254
401eeee457259bf017ef97eee347ee936b37a22a	determining individual variation in growth and its implication for life-history and population processes using the empirical bayes method	health research;uk clinical guidelines;biological patents;animals;simulation and modeling;habitats;ecology evolution behavior and systematics;modeling and simulation;europe pubmed central;growth and development;citation search;bayes theorem;models biological;ecology;journal article;genetics;body size;trout;aut;physiological parameters;population density;uk phd theses thesis;computational theory and mathematics;molecular biology;fish farming;life sciences;freshwater fish;models statistical;computational biology;uk research reports;demography;medical journals;cellular and molecular neuroscience;europe pmc;biomedical research;bioinformatics	The differences in demographic and life-history processes between organisms living in the same population have important consequences for ecological and evolutionary dynamics. Modern statistical and computational methods allow the investigation of individual and shared (among homogeneous groups) determinants of the observed variation in growth. We use an Empirical Bayes approach to estimate individual and shared variation in somatic growth using a von Bertalanffy growth model with random effects. To illustrate the power and generality of the method, we consider two populations of marble trout Salmo marmoratus living in Slovenian streams, where individually tagged fish have been sampled for more than 15 years. We use year-of-birth cohort, population density during the first year of life, and individual random effects as potential predictors of the von Bertalanffy growth function's parameters k (rate of growth) and L∞ (asymptotic size). Our results showed that size ranks were largely maintained throughout marble trout lifetime in both populations. According to the Akaike Information Criterion (AIC), the best models showed different growth patterns for year-of-birth cohorts as well as the existence of substantial individual variation in growth trajectories after accounting for the cohort effect. For both populations, models including density during the first year of life showed that growth tended to decrease with increasing population density early in life. Model validation showed that predictions of individual growth trajectories using the random-effects model were more accurate than predictions based on mean size-at-age of fish.	aicardi's syndrome;akaike information criterion;birth cohort;diploid cell;ecology;evolutionary algorithm;population density;random effects model;salmo marmoratus;salmo salar;tagged architecture;tracer	Simone Vincenzi;Marc Mangel;Alain J. Crivelli;Stephan Munch;Hans J. Skaug	2014		10.1371/journal.pcbi.1003828	biology;fish farming;habitat;computer science;bioinformatics;modeling and simulation;bayes' theorem;ecology;population density;statistics	ML	8.552275302276557	-67.18974917991265	114547
edf2fc406f393e28c78a4619b2358f085e4c0339	a novel telecommunications-based approach to hiv modeling and simulation	hiv;nano communication;simulation;biological communication;modeling	It is well known that biological systems utilize communication in some form; one prolific example of this is the propagation of HIV (Human Immunodeficiency Virus) in the human body. By modeling HIV infection as a communication system, we hope to gain a unique insight into HIV and biological communication systems in general. Such a model would provide researchers a platform for experimenting and simulating various biological communication systems.Wehave previously developed a layered communication protocol for interpreting biological communication systems using telecommunications paradigms and will apply said model to HIV proliferation. We will also demonstrate the effectiveness of the model by implementing a communication-based simulation of HIV infection based on direct interpretation of this layered protocol. © 2012 Elsevier Ltd. All rights reserved.	biological system;communications protocol;experiment;osi model;simulation;software propagation;telecommunications network	Aaron T. Sharp;Angela K. Pannier;Beata J. Wysocki;Tadeusz A. Wysocki	2012	Nano Comm. Netw.	10.1016/j.nancom.2012.01.003	simulation;systems modeling;computer science;engineering;virology	AI	3.7274236383822466	-68.95937808523612	114584
3f382b9c37844ab4e9788345334d0bb1d89e8aa8	using a normalized score multi-label knn to classify multi-label herbal formulae	multi label document;text classification;k nn classifier;herbal formula;text categorization	The popularity of herbal medicines has greatly increased in worldwide countries over recent years. Herbal formula is a form of traditional medicine where herbs are combined to heal patient to heal faster and more efficiency. Herbal formulae can be divided into categories. Some formulae can be classified as more than one category. The categories are usually based on indications of herbs in formulae. To support experts for classifying a formula to one or more therapeutic categories, the normalized score multi-label k-nearest neighbors (NSML k-NN) algorithm, is proposed for multi-label herbal formulae classification. The k-NN classifiers with several term weight schemes are explored. The normalized scores are calculated. The values of k, strategies to assign categories are investigated to adjust the decision for multi-label herbal formulae. The experiment is done using a mixed data set of herbal formulae collected from the Natural List of Essential Medicine and the list of common household remedies for traditional medicine. Moreover, a set of well-known commercial products are used for evaluating the effectiveness of the proposed method. From the results, the NSML k-NN is an efficient method to classify multi-label herbal formulae.	k-nearest neighbors algorithm;multi-label classification	Verayuth Lertnattee;Sinthop Chomya;Chanisara Lueviphan	2013		10.1007/978-3-319-03844-5_6	arithmetic;medicine;data mining;traditional medicine	Vision	2.922525907648361	-75.38270676500193	114616
799721bd8d2babd44592fd089b1d4f87cf0b36df	predictive models for hospital bed management using data mining techniques	data mining;hospital management;management of beds;management of patients	It is clear that the failures found in hospital management are usually related to the lack of information and insufficient resources management. The use of Data Mining (DM) can contribute to overcome these limitations in order to identify relevant data on patient’s management and providing important information for managers to support their decisions. Throughout this study, were induced DM models capable to make predictions in a real environment using real data. For this, was adopted the Cross-Industry Standard Process for Data Mining (CRISP-DM) methodology. Three distinct techniques were considered: Decision Trees (DT), Naïve Bayes (NB) and Support Vector Machine (SVM) to perform classification tasks. With this work it was explored and assessed the possibility to predict the number of patient discharges using only the number and the respective date. The models developed are able to predict the number of patient discharges per week with acuity values ranging from ≈82.69% to ≈94.23%. The use of this models can contribute to improve the hospital bed management because having the discharges number it is possible forecasting the beds available for the following weeks in a determinated service.	cross industry standard process for data mining;data (computing);decision tree;discharger;experiment;freedman–diaconis rule;international symposium on fundamentals of computation theory;naive bayes classifier;predictive modelling;sampling (signal processing);statistical classification;support vector machine	Sérgio Oliveira;Filipe Portela;Manuel Filipe Santos;José Machado;António Abelha	2014		10.1007/978-3-319-05948-8_39	engineering;data science;operations management;data mining	ML	5.604934729370024	-76.15188716631181	114859
79d41e6a2cb973a2c955b35ed52ef46341e49203	comparison of variable selection methods for clinical predictive modeling	data interpretation;electronic health records;machine learning;models;regression analysis;statistical;variable selection	OBJECTIVE Modern machine learning-based modeling methods are increasingly applied to clinical problems. One such application is in variable selection methods for predictive modeling. However, there is limited research comparing the performance of classic and modern for variable selection in clinical datasets.   MATERIALS AND METHODS We analyzed the performance of eight different variable selection methods: four regression-based methods (stepwise backward selection using p-value and AIC, Least Absolute Shrinkage and Selection Operator, and Elastic Net) and four tree-based methods (Variable Selection Using Random Forest, Regularized Random Forests, Boruta, and Gradient Boosted Feature Selection). We used two clinical datasets of different sizes, a multicenter adult clinical deterioration cohort and a single center pediatric acute kidney injury cohort. Method evaluation included measures of parsimony, variable importance, and discrimination.   RESULTS In the large, multicenter dataset, the modern tree-based Variable Selection Using Random Forest and the Gradient Boosted Feature Selection methods achieved the best parsimony. In the smaller, single-center dataset, the classic regression-based stepwise backward selection using p-value and AIC methods achieved the best parsimony. In both datasets, variable selection tended to decrease the accuracy of the random forest models and increase the accuracy of logistic regression models.   CONCLUSIONS The performance of classic regression-based and modern tree-based variable selection methods is associated with the size of the clinical dataset used. Classic regression-based variable selection methods seem to achieve better parsimony in clinical prediction problems in smaller datasets while modern tree-based methods perform better in larger datasets.		L. Nelson Sanchez-Pinto;Laura Ruth Venable;John Fahrenbach;Matthew M. Churpek	2018	International journal of medical informatics	10.1016/j.ijmedinf.2018.05.006	regression analysis;data mining;statistics;logistic regression;feature selection;random forest;elastic net regularization;cohort;medicine	ML	6.038744877574971	-75.92413639753384	115081
2179bd7776e240d4c003af60c027e8ff6dec2cb8	data driven inferrence of unmodelled dynamical processes in systems biology models		Mathematical models are at the heart of systems biology. They are used to gain a deeper understanding of biological problems and to guide the selection of informative experiments. Model development for a particular process starts with a number of decisions regarding the specification of the systems border, the specification of dynamical variables and assumptions about the interactions between these variables. Processes or quantities believed to be approximately constant at the time scale of interest are then treated as parameters. These decisions require detailed biological knowledge, which is, however, not always available. But even if it is well known that there is crosstalk between the system to be modelled and another biological process, the time course of these interactions is often not known or highly context dependent. Unmodelled dynamics can have strong effects on the quantitative and qualitative dynamic features of the system. During model development, this might exhibit itself through an unsatisfactory fit of the model to the training data or by unrealistic parameter estimates [1]. The question of where the model should be modified and where the points of application of these unmodelled processes are, is often handled by labour intensive educated guessing or trial and error. We have devised a data driven strategy, which we call the Reverse Tracking Algorithm (RTA), to handle unmodelled dynamics in ordinary differential equation models. The RTA (i) identifies candidate points of application of the unmodelled dynamical inputs and (ii) estimates the time course of these interfering signals. An early version of this algorithm has already been successful in predicting regulatory signals in response to potassium starvation in the yeast Saccharomyces cerevisiae [2]. However, this early version suffered from numerical instabilities and required careful calibration of the algorithm’s parameter. In our presentation we discuss a much more robust version of the RTA. We present the computational method of this improved and now easy to use RTA and demonstrate its application to three additional examples. The first example illustrates, how the influence of circadian rhythms and other umodelled dynamics can be handled in pharmacokinetic models. In contrast to the standard approach we study the case where the elimination rate of a drug is subject to unknown oscillatory perturbations caused by time-of-day variations. We demonstrate, how the RTA can be applied to infer the time course of the elimination rate oscillations when starting from the standard single compartment model. IWBBIO 2013. Proceedings Granada, 18-20 March, 2013 527 2 Maik Kschischo and Matthias Kahm The second example is a biochemical reaction cascade with an unmodelled feedback mechanism. The RTA is applied to infer the cascade stage where the feedback applies and the unknown time course of the feedback. A simple phase plane analysis is then sufficient to discriminate between different models for the feedback kinetics, leading to an improved version of the cascade model. The third example considers a well known model of the JAK/STAT signalling pathway, where the authors compared an initial model to a more accurate model involving rapid nucleocytoplasmic cycles of STAT5 species [4]. Starting from the initial model ignoring these cycles, we used the RTA to identify the unmodelled dynamics as well as the involved species (cycloplasmatic and nuclear STAT5). This was found by Swameye et al. using conventional manual model building strategies. We show that the RTA could have provided the same result in a semi-automatic way underpinning its utility for systematic model building and extension.	algorithm;crosstalk;dynamical system;experiment;feedback;gene regulatory network;granada;information;interaction;kinetics internet protocol;mathematical model;multi-compartment model;numerical analysis;semiconductor industry;stat (system call);systems biology	Maik Kschischo;Matthias Kahm	2013			phase plane;systems biology;control theory;ordinary differential equation;cascade;mathematical model;model building;trial and error;data-driven;computer science	AI	7.93259595097082	-66.47652278211919	115668
3d29c0ad44f3adde20343048e978cac5dde5d01f	a 3d insulin sensitivity prediction model enables more patient-specific prediction and model-based glycaemic control		Abstract Background Insulin therapy for glycaemic control (GC) in critically ill patients may improve outcomes by reducing hyperglycaemia and glycaemic variability, which are both associated with increased morbidity and mortality. However, initial positive results have proven difficult to repeat or achieve safely. STAR (Stochastic TARgeted) is a model-based glycaemic control protocol using a risk-based dosing approach. STAR uses a 2D stochastic model to predict distributions of likely future changes in model-based insulin sensitivity ( SI ) based on its current value, and determines the optimal intervention. Objectives This study investigates the impact of a new 3D stochastic model on the ability to predict more accurate future SI distributions, which would allow more aggressive insulin dosing and improved glycaemic control. Methods The new 3D stochastic model is built using both current SI and its prior variation to predict future SI distribution from 68,629 h of clinical data (819 GC episodes). The 5 th –95 th percentile range of predicted SI distribution are calculated and compared with the 2D model. Results Results show the 2D model is over-conservative compared to the 3D case for more than 77% of the data, predominantly where SI is stable (| % Δ SI | ≤ 25%). These formerly conservative prediction ranges are now ∼30% narrower with the 3D model, which safely enables more aggressive insulin dosing for these patient hours. In addition, distributions of predicted SI within the 5 th –95 th percentile range are much closer to the ideal value of 90% for more patients with the 3D model. Conclusions The new 3D model better characterises patient specific metabolic variability and patient specific response to insulin, allowing more optimal insulin dosing to increase performance and safety.		Vincent Uyttendaele;Jennifer L. Dickson;Kent W. Stewart;Thomas Desaive;Balázs Benyó;Noémi Szabó-Némedi;Attila Illyés;Geoffrey M. Shaw;J. Geoffrey Chase	2018	Biomed. Signal Proc. and Control	10.1016/j.bspc.2018.05.032	statistics;stochastic modelling;artificial intelligence;pattern recognition;mathematics;insulin;percentile	Robotics	7.792259696974953	-73.76783867368243	115869
371e27a8f18ce3a8fd6b6edce89ad660ccc4068f	hybrid spreading mechanisms and t cell activation shape the dynamics of hiv-1 infection	female;ucl;cd4 positive t lymphocytes;hiv infections;male;discovery;theses;conference proceedings;hiv 1;digital web resources;models immunological;ucl discovery;open access;host pathogen interactions;ucl library;humans;book chapters;open access repository;computational biology;ucl research	HIV-1 can disseminate between susceptible cells by two mechanisms: cell-free infection following fluid-phase diffusion of virions and by highly-efficient direct cell-to-cell transmission at immune cell contacts. The contribution of this hybrid spreading mechanism, which is also a characteristic of some important computer worm outbreaks, to HIV-1 progression in vivo remains unknown. Here we present a new mathematical model that explicitly incorporates the ability of HIV-1 to use hybrid spreading mechanisms and evaluate the consequences for HIV-1 pathogenenesis. The model captures the major phases of the HIV-1 infection course of a cohort of treatment naive patients and also accurately predicts the results of the Short Pulse Anti-Retroviral Therapy at Seroconversion (SPARTAC) trial. Using this model we find that hybrid spreading is critical to seed and establish infection, and that cell-to-cell spread and increased CD4+ T cell activation are important for HIV-1 progression. Notably, the model predicts that cell-to-cell spread becomes increasingly effective as infection progresses and thus may present a considerable treatment barrier. Deriving predictions of various treatments' influence on HIV-1 progression highlights the importance of earlier intervention and suggests that treatments effectively targeting cell-to-cell HIV-1 spread can delay progression to AIDS. This study suggests that hybrid spreading is a fundamental feature of HIV infection, and provides the mathematical framework incorporating this feature with which to evaluate future therapeutic strategies.	acquired immunodeficiency syndrome;color gradient;hiv infections;leukemia, b-cell;mathematical model;mathematics;patients;t-cell activation;video-in video-out;virion	Changwang Zhang;Shi Zhou;Elisabetta Groppelli;Pierre Pellegrino;Ian Williams;Persephone Borrow;Benjamin M. Chain;Clare Jolly	2015		10.1371/journal.pcbi.1004179	computational biology;biology;bioinformatics;immunology	ECom	8.417522790882854	-68.29064528604336	116036
8a1b825327f08c0ecae50c5df885bc725a2c76ca	a cell state splitter and differentiation wave working-model for embryonic stem cell development and somatic cell epigenetic reprogramming		Cell fate determination and development is a biology question that has yet to be fully answered. During embryogenesis and in vivo stem cell differentiation, cells/tissues deploy epigenetic mechanisms to accomplish differentiation and give rise to the fully developed organism. Although a biochemistry description of cellular genetics and epigenetics is important, additional mechanisms are necessary to completely solve the problem of embryogenesis, especially differentiation and the spatiotemporal coordination of cells/tissues during morphogenesis. The cell state splitter and differentiation wave working-model was initially proposed to explain the homeostatic primary neural induction in amphibian embryos. Here the model is adopted to explain experimental findings on in vitro embryonic stem cell, pluripotency and differentiation. Moreover, since somatic cells can be reverted to a stem-cell-like pluripotent state through the laboratory procedure called epigenetic reprogramming, erection of a cell state splitter could be a key event in their successful reprogramming. Overall, the cell state splitter working-model introduces a bistable cytoskeletal mechanism that partially explains cell fate determination and biological development. It offers an interdisciplinary framework that bridges the gap between molecular epigenetics and embryogenesis.		Kai Lu;Tong Cao;Richard Gordon	2012	Bio Systems	10.1016/j.biosystems.2012.06.001	biology;cell biology;cell fate determination;cellular differentiation;anatomy;cell potency	ML	6.124176866254464	-66.26732041031212	116337
33a34c9e831900331a8275eecbdc771a551e28e7	an efficient pattern mining approach for event detection in multivariate temporal data	event detection;recent temporal patterns;temporal abstractions;time interval patterns;temporal data mining;electronic health records	This work proposes a pattern mining approach to learn event detection models from complex multivariate temporal data, such as electronic health records. We present recent temporal pattern mining, a novel approach for efficiently finding predictive patterns for event detection problems. This approach first converts the time series data into time-interval sequences of temporal abstractions. It then constructs more complex time-interval patterns backward in time using temporal operators. We also present the minimal predictive recent temporal patterns framework for selecting a small set of predictive and non-spurious patterns. We apply our methods for predicting adverse medical events in real-world clinical data. The results demonstrate the benefits of our methods in learning accurate event detection models, which is a key step for developing intelligent patient monitoring and decision support systems.	data mining;decision support systems, clinical;decision support system;electronic health records;time series;benefit	Iyad Batal;Gregory F. Cooper;Dmitriy Fradkin;James H. Harrison;Fabian Mörchen;Milos Hauskrecht	2015	Knowledge and Information Systems	10.1007/s10115-015-0819-6	computer science;data science;pattern recognition;data mining	ML	4.047497941398796	-77.03116264795554	116745
7068737bccac265bd0d03cc60a523cd28bbb7ed5	a study on data-driven novel cancer staging methods	colorectal neoplasms;machine learning;survival analysis	This paper presents a data-driven method to study the relationship of survival and clinical information of patients. The machine learning models were established to study the survival situation at the time of interest based on survival analysis. The way to determine the time of interest is an innovation of this paper. The distribution of survival time is considered, namely the three quartiles, as well as the traditional analysis experience is taken into consideration.	disk staging;machine learning;non-small cell lung carcinoma;patients	Yuan Gao;Yu Tian;Shengqiang Chi;Yao Lu;Xinhang Li;Tianshu Zhou;Jing-song Li	2017	Studies in health technology and informatics	10.3233/978-1-61499-830-3-1263	cancer research;cancer staging;computer science	PL	5.181103323237607	-75.25609553433708	116860
3770748b0198053f8cd2dcd96d330128bb45dd39	so you think you understand tautomerism?	theoretical model;drug design;physical environment;database search;organic compound	"""It appears so simple at first glance, """"tautomers are isomers of organic compounds that readily interconvert, usually by the migration of hydrogen from one atom to another"""". If a chemist can describe the problem so succinctly, one might question why the complication of tautomerism remains a considerable challenge to cheminformatics and computer-assisted drug design. With a half-century of experience with representing molecules in computers, and almost limitless modern computational power, the problem should have been solved by now. The unfortunate answer is that the frustration and inconvenience of a database search failing to find matches due to differences in the tautomeric forms of the query and registered compounds is but the tip of an iceberg. Prototropic tautomerism, the movement of hydrogens around a molecule, is but just one aspect of an interconnected web of complications. These include mesomerism, aromaticity, protonation state, stereochemistry, conformation, polymerization, photostability, hydrolysis, metabolism and EOCWR (explodes on contact with reality). The common theme is that valence theory, which underlies all modern chemical informatics systems, is an approximate theoretical model for representing molecules mathematically, and, as with all models, it has limitations and domains of applicability. In the physical environments that chemists care about, small organic molecules are often dynamic, existing in multiple equivalent or interconvertible forms. A single connection table can at best represent a snapshot or sample from these populations. Although partial algorithmic solutions exist for handling the most common cases of tautomerism, this perspective hopes to argue that the underlying problems perhaps make tautomerism more complex than it might first appear."""	approximation algorithm;cheminformatics;computation;computer;computers;drug design;failure;handling (psychology);hope (emotion);hydrogen;informatics (discipline);isomerism;organic chemicals;population;question (inquiry);registration;snapshot (computer storage);stereochemistry (discipline);theory	Roger A. Sayle	2010	Journal of computer-aided molecular design	10.1007/s10822-010-9329-5	biochemistry;stereochemistry;database search engine;chemistry;bioinformatics;organic chemistry;computational chemistry;nanotechnology;drug design		2.4434715519189965	-66.82450084460488	117101
cd8c05fc4950581c7f5cb6108d50c92f478e6d60	digital and biological storage systems — a quantitative comparison	dna;computers;data organization;dna neurons media computers hard disks encoding power demand;biology computing;biocomputing;brain memory;storage system;data integrity;storage management;data density;storage management biocomputing biology computing content addressable storage data integrity;hard disks;bioinformatics data storage systems and media redundancy integrity;media;bioinformatics research digital storage system biological storage system quantitative comparison electronic storage system brain memory data organization functionality data density data capacity power consumption redundancy data integrity data access time data transfer rate dna storage;data storage;data transfer rate;digital storage system;integrity;redundancy;biological systems;bioinformatics research;electronic storage system;functional data;functionality;neurons;power consumption;data access time;content addressable storage;encoding;power demand;dna storage;biological storage system;data transfer;quantitative comparison;bioinformatics;data storage systems and media;data capacity	The paper presents a quantitative comparison of digital/electronic and biological storage systems. Two biological storage systems are included: DNA and brain memory. First we will show some examples of digital-biological systems integration. In the main part of the paper, we discuss different storage aspects, mostly quantitative, such as: organization, functionality, data density, capacity, power consumption, redundancy, integrity, access time and data transfer rate. Numerous analogies between biological and electronic storage systems are pointed out. Finally, we will try to answer the question: which digital storage systems and media are the best equivalents for brain and DNA storage? The analysis of the storage systems resemblances and differences may facilitate to carry bioinformatics research.	access time;areal density (computer storage);bioinformatics;biological system;redundancy (engineering);system integration	Tomasz Bilski	2007	2007 2nd Bio-Inspired Models of Network, Information and Computing Systems	10.1109/BIMNICS.2007.4610074	media;computer hardware;computer science;theoretical computer science;operating system;computer data storage;data integrity;database;redundancy;dna;encoding	HPC	3.1014430309134844	-69.10128658243217	117144
265a0260d19588e5706523daa0c2531eba0ecb02	a neurofuzzy-based expert system for disease diagnosis	fuzzy neural nets;medical records;diseases medical diagnostic computing medical expert systems fuzzy neural nets;rule based;electrical medical record medical expert system neurofuzzy fuzzy logic diagnosis;medical expert systems;artificial intelligent;fuzzy logic;medical expert system;fuzzy expert system;diagnostic expert systems biomedical imaging medical diagnostic imaging medical expert systems medical diagnosis cardiology testing humans cardiac disease cardiovascular diseases;diseases;electrical medical record neurofuzzy based expert system disease diagnosis medical diagnosis expert system differential artificial intelligence techniques diagnosis process linear scoring system subjective analysis rule based fuzzy expert system patient information fuzzy logic;medical diagnostic computing;medical diagnosis;scoring system;expert system	This paper describes the development of a medical diagnosis expert system that can be used by physicians in their daily practices. Differential artificial intelligence techniques are incorporated into the expert system to best represent the various stages of the diagnosis process. A linear scoring system is used to represent the initial subjective analysis stage, while a rule-based fuzzy expert system is used to interpret lab tests and imaging studies to confirm final diagnosis. An actual example of patient walkthrough is used to demonstrate various computation steps from embedding the patient information to reaching the final diagnosis.	artificial intelligence;computation;expert system;logic programming;medical imaging;software walkthrough	William W. Melek;Alireza Sadeghian;Homayoun Najjaran;Mina Hoorfar	2005	2005 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2005.1571727	fuzzy logic;computer science;artificial intelligence;medical diagnosis;data mining;expert system;medical record	AI	3.8139403332256063	-79.31630322222537	117172
a7b46bae67d3908e43b52a0c29944eb1664f95af	a study of complication identification based on weighted association rule mining		With the fast development of big data technology, data mining algorithms are widely used to process the medical data and support clinical decision-making. In this paper, a new method is proposed to mine the disease association rule and predict the possible complications. The concept of disease concurrent weight is proposed and Back Propagation (BP) neural network model is applied to calculate the disease concurrent weight. Adopting the weighted association rule mining algorithm, diseases complication association rule are derived, which can help to remind doctors about patients’ potential complications. The empirical evaluation using hospital patients’ medical information shows that the proposed method is more effective than two baseline methods.		Zhijun Yan;Kai Liu;Meiming Xing;Tianmei Wang;Baowen Sun	2016		10.1007/978-3-319-42102-5_17	data mining	ML	5.0640444682006205	-76.72503330054634	117196
b605da031b80efc33c32a0fd6d406d40af3908d2	predicting surgical skill from the first n seconds of a task: value over task time using the isogony principle	surgical skill evaluation;computer aided decision;tracking systems	Most evaluations of surgical workflow or surgeon skill use simple, descriptive statistics (e.g., time) across whole procedures, thereby deemphasizing critical steps and potentially obscuring critical inefficiencies or skill deficiencies. In this work, we examine off-line, temporal clustering methods that chunk training procedures into clinically relevant surgical tasks or steps during robot-assisted surgery. Features calculated from the isogony principle are used to train four common machine learning algorithms from dry-lab laparoscopic data gathered from three common training exercises. These models are used to predict the binary or ternary skill level of a surgeon. K-fold and leave-one-user-out cross-validation are used to assess the accuracy of the generated models. It is shown that the proposed scalar features can be trained to create 2-class and 3-class classification models that map to fundamentals of laparoscopic surgery skill level with median 85 and 63% accuracy in cross-validation, respectively, for the targeted dataset. Also, it is shown that the 2-class models can discern class at 90% of best-case mean accuracy with only 8 s of data from the start of the task. Novice and expert skill levels of unobserved trials can be discerned using a state vector machine trained with parameters based on the isogony principle. The accuracy of this classification comes within 90% of the classification accuracy from observing the full trial within 10 s of task initiation on average.	cluster analysis;cross infection;cross reactions;cross-validation (statistics);description;dry lab;evaluation;exercise;machine learning;online and offline;silo (dataset);surgical procedures, laparoscopic;algorithm;statistical cluster	Anna French;Thomas S. Lendvay;Robert M. Sweet;Timothy M. Kowalewski	2017	International Journal of Computer Assisted Radiology and Surgery	10.1007/s11548-017-1606-5	data mining;artificial intelligence;computer vision;tracking system;management science;cluster analysis;workflow;descriptive statistics;computer science	ML	7.158889394388522	-76.4151428240041	118145
43930be72c51e33b80b4b872e9071d3d930fff50	multistage classification for cardiovascular disease risk prediction	ecg;mit bih;classification;random forest	Cardiovascular diseases are the prominent causes of death each year. Data mining is an emerging area which has numerous applications specifically in healthcare. Our work suggests a system for predicting the risk of a cardiovascular disease using data mining techniques and is based on the ECG tests. It further recommends nearby relevant hospitals based on the prediction. We propose a multistage classification algorithm in which the first stage is used to classify normal and abnormal ECG beats and the next stage is used to refine the prediction done by the first stage by reducing the number of false negatives. In this work experiments have been conducted on the MIT-BIH Arrhythmia dataset which is a benchmark dataset. The results of the experiments show that the proposed technique is very promising.	multistage amplifier	Durga Toshniwal;Bharat Goel;Hina Sharma	2015		10.1007/978-3-319-27057-9_18	engineering;data science;machine learning;data mining	ML	6.55683456730256	-77.65273694067139	118195
6a2b7979f4cef4a4d3cb4b6367642b566e0b2abf	a comparison of non-symmetric entropy-based classification trees and support vector machine for cardiovascular risk stratification	kernel;acute coronary syndrome diagnosis computer assisted entropy humans pattern recognition automated prevalence proportional hazards models reproducibility of results risk assessment risk factors sensitivity and specificity support vector machines survival analysis survival rate;classification tree;history;support vector machines;model generation;training;class imbalance;acute coronary syndrome;machine learning;classification rules;stratification;support vector machines training entropy history machine learning kernel algorithm design and analysis;entropy;risk stratification;support vector machine;algorithm design;cardiovascular risk;algorithm design and analysis	Classification tree-based risk stratification models generate easily interpretable classification rules. This feature makes classification tree-based models appealing for use in a clinical setting, provided that they have comparable accuracy to other methods. In this paper, we present and evaluate the performance of a non-symmetric entropy-based classification tree algorithm. The algorithm is designed to accommodate class imbalance found in many medical datasets. We evaluate the performance of this algorithm, and compare it to that of SVM-based classifiers, when applied to 4219 non-ST elevation acute coronary syndrome patients. We generated SVM-based classifiers using three different strategies for handling class imbalance: cost-sensitive SVM learning, synthetic minority oversampling (SMOTE), and random majority undersampling. We used both linear and radial basis kernel-based SVMs. Our classification tree models outperformed SVM-based classifiers generated using each of the three techniques. On average, the classification tree models yielded a 14% improvement in G-score and a 21% improvement in F-score relative to the linear SVM classifiers with the best performance. Similarly, our classification tree models yielded a 12% improvement in G-score and a 21% improvement in the F-score over the best RBF kernel-based SVM classifiers.	acute coronary syndrome;algorithm;classification chart;decision tree learning;f1 score;handling (psychology);list of algorithms;oversampling;patients;radial (radio);radial basis function kernel;rule (guideline);stratification;support vector machine;synthetic intelligence;trees (plant);undersampling	Anima Singh;John V. Guttag	2011	2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/IEMBS.2011.6089901	support vector machine;algorithm design;computer science;machine learning;linear classifier;pattern recognition;data mining;mathematics	Vision	7.10268751483237	-76.91680921066747	118503
e0ee787da06e5b0d3cc0baa2f8bc295f5b9b8eb7	computational predictions of structures of multichromosomes of budding yeast	budding yeast genome computational predictions multichromosome structures global architecture spatial organization gene expression nuclear function higher order genome organization multichromosome constrained self avoiding chromatin model genome structural models budding yeast cell nucleus chromatin fiber diameter persistence length spatial confinement single cell imaging;microorganisms cellular biophysics genomics;bioinformatics genomics biological cells computer architecture microprocessors organizations computational modeling	Knowledge of the global architecture of the cell nucleus and the spatial organization of genome is critical for understanding gene expression and nuclear function. Single-cell imaging techniques provide a wealth of information on the spatial organization of chromosomes. Computational tools for modelling chromosome structure have broad implications in studying the effect of cell nucleus on higher-order genome organization. Here we describe a multichromosome constrained self-avoiding chromatin model for studying ensembles of genome structural models of budding yeast nucleus. We successfully generated a large number of model genomes of yeast with appropriate chromatin fiber diameter, persistence length, and excluded volume under spatial confinement. By incorporating details of the constraints from single-cell imaging studies, our method can model the budding yeast genome realistically. The model developed here provides a general computational framework for studying the overall architecture of budding yeast genome.	30 nm chromatin fiber;cell nucleus;centromere;chromosome structures;chromosomes;computation;diameter (qualifier value);exclusion;gene expression;genome;imaging techniques;medical imaging;persistence (computer science);saccharomycetales;spatial organization;telomere	Gamze Gürsoy;Yun Xu;Jie Liang	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944487	biology;bioinformatics;genetics	Comp.	5.529034993116628	-67.11598566504517	118691
2c60293a8c65d8f9d68872fe23c70b64cb5be53f	bo's abdominal acupuncture mitigate post-stroke fatigue: a clinical center retrospective analysis	post stroke fatigue bo s abdominal acupuncture rehabilitation exercise;rehabilitation exercise;post stroke fatigue;life scale bo s abdominal acupuncture post stroke fatigue clinical center retrospective analysis post stroke patients therapeutic tool chinese medicine therapeutic efficacy rehabilitation exercise fatigue severity scale stroke specific quality;fatigue hemorrhaging hypertension diabetes diseases;bo s abdominal acupuncture;patient treatment diseases patient rehabilitation	Backgrounds: Post-stroke fatigue is a common and devastating malady of post-stroke patients. Bo's abdominal acupuncture is an effective therapeutic tool of Chinese medicine and widely used in practice in China. In this clinical study, we evaluated the therapeutic efficacy and safety of Bo's abdominal acupuncture for post-stroke fatigue. Methods: sixty participants with post-stroke fatigue were recruited and randomized to observational and acupuncture groups. The patients of acupuncture group were treated with Bo's abdominal acupuncture and rehabilitation exercise. The patients of observational group were treated with rehabilitation exercise. Two scoring systems, such as fatigue severity scale and stroke-specific quality of life scale, were used to analyze the therapeutic efficacy and safety of Bo's abdominal acupuncture, administered before and after treatment. Results: After four weeks, the patients with acupuncture displayed milder post-stroke fatigue in comparison with patients without acupuncture. Conclusion: Bo's abdominal acupuncture and rehabilitation exercise markedly mitigate post-stroke fatigue.	bo-taoshi;call of duty: black ops;randomized algorithm	Ruihuan Pan;Mingfeng He;Lechang Zhan;Zhen Huang;Jie Zhan;Hongxia Chen	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359931	acupuncture	SE	9.937104383876873	-77.53883937759008	119008
801f97ddb970211d823b3e8f52a79ee3915ba531	a comparative analysis of classification algorithms in diabetic retinopathy screening		Automated screening of diabetic retinopathy plays an important role in diagnosis of the disease in early stages and preventing blindness in patients with diabetes. Various machine learning approaches have been studied in literature with the purpose of improving the accuracy of the screening methods. Although the performance of the machine learning algorithm depends on the application and the type of data, yet there is no comprehensive analysis of different approaches in the diabetic retinopathy screening to choose the best approach. To this end, in this study a comparative analysis of nine common classification algorithms is performed to select the most applicable approach for the specific problem of screening diabetic retinopathy patients. Individual algorithms are optimized with respect to their tunable parameters, and are compared together in terms of their accuracy, precision, recall, and F1-score. Simulation results demonstrate the difference between the performances of individual classification algorithms and can be used as a deciding factor in method selection for further research.	algorithm;f1 score;gaussian process;machine learning;performance;qualitative comparative analysis;sensor;simulation	Saboora Mohammadian;Ali Karsaz;Yaser M. Roshan	2017	2017 7th International Conference on Computer and Knowledge Engineering (ICCKE)	10.18293/SEKE2017-207	machine learning;computer science;real-time computing;diabetic retinopathy screening;statistical classification;artificial intelligence	ML	7.76530367364004	-77.64544649530568	119371
fa056345e9065e2c1401afbd88cff1e09c8ccb57	improving the classification of multiple disorders with problem decomposition	diagnostic test;neural networks;modular networks;abductive networks;data analysis;problem decomposition;decision making process;network model;multiple disorders;clinical practice;differential diagnosis;classifiers;data reduction;dermatology;classification accuracy;divide and conquer;medical diagnosis;neural network	Differential diagnosis of multiple disorders is a challenging problem in clinical medicine. According to the divide-and-conquer principle, this problem can be handled more effectively through decomposing it into a number of simpler sub-problems, each solved separately. We demonstrate the advantages of this approach using abductive network classifiers on the 6-class standard dermatology dataset. Three problem decomposition scenarios are investigated, including class decomposition and two hierarchical approaches based on clinical practice and class separability properties. Two-stage classification schemes based on hierarchical decomposition boost the classification accuracy from 91% for the single-classifier monolithic approach to 99%, matching the theoretical upper limit reported in the literature for the accuracy of classifying the dataset. Such models are also simpler, achieving up to 47% reduction in the number of input variables required, thus reducing the cost and improving the convenience of performing the medical diagnostic tests required. Automatic selection of only relevant inputs by the simpler abductive network models synthesized provides greater insight into the diagnosis problem and the diagnostic value of various disease markers. The problem decomposition approach helps plan more efficient diagnostic tests and provides improved support for the decision-making process. Findings are compared with established guidelines of clinical practice, results of data analysis, and outcomes of previous informatics-based studies on the dataset.	abductive reasoning;classification;consistency model;decision making;dermatology field;diagnostic tests;fibrosis;group method of data handling;handling (psychology);informatics (discipline);linear separability;monolithic kernel;ninety nine;papillary thyroid carcinoma;seborrheic dermatitis;signs and symptoms;silo (dataset);small;multiple pathologies	Radwan E. Abdel-Aal;Mona R. E. Abdel-Halim;Safa Abdel-Aal	2006	Journal of biomedical informatics	10.1016/j.jbi.2005.12.001	decision-making;data reduction;divide and conquer algorithms;pathology;computer science;bioinformatics;artificial intelligence;network model;machine learning;medical diagnosis;data mining;data analysis;artificial neural network;diagnostic test;statistics	ML	6.433320006778221	-76.13604191834463	119642
0924ee6ee96ffe7f3b2d7681b30860eae30829f2	application of decisional models to the health-economic assessment of new interactive clinical software	decisional models healthcare sector it related innovations clinical advantage estimation economic estimation markov health state based model pad peripheral artery disease treatment patient progression simulation minimally invasive stenting procedures way limb saving improvement endovascular stenting procedure preplanning software tool eu funded research project real time simulations for safer vascular stenting rt3s interactive clinical software health economic assessment;medical computing;biological system modeling economics software diseases analytical models educational institutions;diseases;patient treatment;software tools;markov processes;software tools blood vessels diseases health care markov processes medical computing patient treatment;blood vessels;health care	RT3S (Real Time Simulations for Safer vascular Stenting) is a partially EU-funded research project aiming to develop a software tool for supporting physicians during the preplanning of endovascular stenting procedures. The project is expected to improve the way limb-saving, minimally-invasive stenting procedures are currently performed, with positive clinical and economic impact. A hypothetical cohort of patients was modeled and used to simulate the patient's progression through the treatment of Peripheral Artery Disease (PAD). A Markov health state-based model was implemented, based on clinical and economic parameters derived from the literature and clinicians' feedback. The health-economic analysis allowed quantitative estimation of the economic and clinical advantages related to the implementation of the clinical software. A quantitative estimation of the potential health-economic impact was achieved. The model proved to accord well with observed predictions from endovascular experts in the field. It represents an important reference for future assessment of IT-related innovations in the healthcare sector.	color gradient;computer simulation;markov chain;peripheral;programming tool	Claudio Silvestro;Jonathan Michaels;Spiridoula Dimou;Evanthia E. Tripoliti;Euripides G. M. Petrakis	2013	13th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2013.6701638	simulation;management science;markov process;health care;statistics	Robotics	5.520202788926046	-72.99278069717532	119762
6c77fe32989eb3eb5d23aa71eb8a3044d4035995	emergence of pathway-level composite biomarkers from converging gene set signals of heterogeneous transcriptomic responses		"""Recent precision medicine initiatives have led to the expectation of improved clinical decisionmaking anchored in genomic data science. However, over the last decade, only a handful of new single-gene product biomarkers have been translated to clinical practice (FDA approved) in spite of considerable discovery efforts deployed and a plethora of transcriptomes available in the Gene Expression Omnibus. With this modest outcome of current approaches in mind, we developed a pilot simulation study to demonstrate the untapped benefits of developing disease detection methods for cases where the true signal lies at the pathway level, even if the pathway's gene expression alterations may be heterogeneous across patients. In other words, we relaxed the crosspatient homogeneity assumption from the transcript level (cohort assumptions of deregulated gene expression) to the pathway level (assumptions of deregulated pathway expression). Furthermore, we have expanded previous single-subject (SS) methods into cohort analyses to illustrate the benefit of accounting for an individual's variability in cohort scenarios. We compare SS and cohort-based (CB) techniques under 54 distinct scenarios, each with 1,000 simulations, to demonstrate that the emergence of a pathway-level signal occurs through the summative effect of its altered gene expression, heterogeneous across patients. Studied variables include pathway gene set size, fraction of expressed gene responsive within gene set, fraction of expressed gene responsive up- vs down-regulated, and cohort size. We demonstrated that our SS approach was uniquely suited to detect signals in heterogeneous populations in which individuals have varying levels of baseline risks that are simultaneously confounded by patient-specific """"genome -by-environment"""" interactions (G×E). Area under the precision-recall curve of the SS approach far surpassed that of the CB (1st quartile, median, 3rd quartile: SS = 0.94, 0.96, 0.99; CB= 0.50, 0.52, 0.65). We conclude that single-subject pathway detection methods are uniquely suited for consistently detecting pathway dysregulation by the inclusion of a patient's individual variability. http://www.lussiergroup.org/publications/PathwayMarker/."""	biological markers;convergence (action);decision making;deregulation;gene expression;genetic heterogeneity;manuscripts;medicine, east asian traditional;patients;proteins;proteomics;relaxation;single-chain antibodies;subgroup;systems biology;transcript;transcriptome;benefit	Samir Rachid Zaim;Qike Li;A. Grant Schissler;Yves A. Lussier	2018	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		precision medicine;bioinformatics;genetics;gene;transcriptome;biomarker (medicine);biology	Comp.	0.6723318547710663	-68.61335496019406	120305
a5d44bcab496d1bbe9b2f4fa91337adf6100b408	predicting glaucoma visual field loss by hierarchically aggregating clustering-based predictors		This study addresses the issue of predicting the glaucomatous visual field loss from patient disease datasets. Our goal is to accurately predict the progress of the disease in individual patients. As very few measurements are available for each patient, it is difficult to produce good predictors for individuals. A recently proposed clustering-based method enhances the power of prediction using patient data with similar spatiotemporal patterns. Each patient is categorized into a cluster of patients, and a predictive model is constructed using all of the data in the class. Predictions are highly dependent on the quality of clustering, but it is difficult to identify the best clustering method. Thus, we propose a method for aggregating cluster-based predictors to obtain better prediction accuracy than from a single cluster-based prediction. Further, the method shows very high performances by hierarchically aggregating experts generated from several cluster-based methods. We use real datasets to demonstrate that our method performs significantly better than conventional clustering-based and patient-wise regression methods, because the hierarchical aggregating strategy has a mechanism whereby good predictors in a small community can thrive. keywords: glaucoma, hierarchical aggre-	aggregate data;algorithm;batch processing;categorization;cluster analysis;color gradient;data mining;performance;predictive modelling;spatiotemporal pattern	Motohide Higaki;Kai Morino;Hiroshi Murata;Ryo Asaoka;Kenji Yamanishi	2016	CoRR		computer science;data science;machine learning;data mining;cluster analysis	ML	5.603227680055935	-75.52639752489384	120738
ee396d62e6c6a89d423c4400e218556cc8603734	methodologies for the modeling and simulation of biochemical networks, illustrated for signal transduction pathways: a primer	modeling and simulation;ordinary differential equations;partial differential equations;signal transduction pathways;stochastic methods;biochemical networks	Biochemical networks depict the chemical interactions that take place among elements of living cells. They aim to elucidate how cellular behavior and functional properties of the cell emerge from the relationships between its components, i.e. molecules. Biochemical networks are largely characterized by dynamic behavior, and exhibit high degrees of complexity. Hence, the interest in such networks is growing and they have been the target of several recent modeling efforts. Signal transduction pathways (STPs) constitute a class of biochemical networks that receive, process, and respond to stimuli from the environment, as well as stimuli that are internal to the organism. An STP consists of a chain of intracellular signaling processes that ultimately result in generating different cellular responses. This primer presents the methodologies used for the modeling and simulation of biochemical networks, illustrated for STPs. These methodologies range from qualitative to quantitative, and include structural as well as dynamic analysis techniques. We describe the different methodologies, outline their underlying assumptions, and provide an assessment of their advantages and disadvantages. Moreover, publicly and/or commercially available implementations of these methodologies are listed as appropriate. In particular, this primer aims to provide a clear introduction and comprehensive coverage of biochemical modeling and simulation methodologies for the non-expert, with specific focus on relevant literature of STPs.		Nesma ElKalaawy;Amr Wassal	2015	Bio Systems	10.1016/j.biosystems.2015.01.008	biology;ordinary differential equation;biochemistry;bioinformatics;modeling and simulation;mathematics;partial differential equation;signal transduction	ML	5.190548251348262	-67.34911369220553	120939
31636b4259d611573ad1b94eb685dcc6cad73f3f	kame: knowledge-based attention model for diagnosis prediction in healthcare		The goal of diagnosis prediction task is to predict the future health information of patients from their historical Electronic Healthcare Records (EHR). The most important and challenging problem of diagnosis prediction is to design an accurate, robust and interpretable predictive model. Existing work solves this problem by employing recurrent neural networks (RNNs) with attention mechanisms, but these approaches suffer from the data sufficiency problem. To obtain good performance with insufficient data, graph-based attention models are proposed. However, when the training data are sufficient, they do not offer any improvement in performance compared with ordinary attention-based models. To address these issues, we propose KAME, an end-to-end, accurate and robust model for predicting patients' future health information. KAME not only learns reasonable embeddings for nodes in the knowledge graph, but also exploits general knowledge to improve the prediction accuracy with the proposed knowledge attention mechanism. With the learned attention weights, KAME allows us to interpret the importance of each piece of knowledge in the graph. Experimental results on three real world datasets show that the proposed KAME significantly improves the prediction performance compared with the state-of-the-art approaches, guarantees the robustness with both sufficient and insufficient data, and learns interpretable disease representations.	artificial neural network;end-to-end principle;gene prediction;knowledge graph;recurrent neural network	Fenglong Ma;Quanzeng You;Houping Xiao;Radha Chitta;Jing Zhou;Jing Gao	2018		10.1145/3269206.3271701	data mining;robustness (computer science);health informatics;recurrent neural network;exploit;kame;general knowledge;health care;computer science;graph	ML	2.598021777635074	-73.18487488247261	121670
d3e7a81540c59836e901dbd4efc0692390abbd4c	ann validation system for icu neonatal data	data integrity;patient care data integrity medical computing neural nets;neural nets;nicu ann validation system icu neonatal data intensive care environment data validation process artificial intelligence approach artificial neural networks beth israel deaconess medical center neonatal intensive care unit;patient care;medical computing;icu databases;artificial neural networks;pediatrics monitoring biomedical monitoring neurons training biological neural networks databases;monitoring signals;critical care;icu databases artificial neural networks critical care monitoring signals	The amount of data generated in the intensive care environment nowadays prohibits the storage of all the information available. The validation process is time consuming, since nurses have to check every certain periods the data acquired from bedside monitors in order to assess their validity and integrity. This work presents an automatic method for data validation in the intensive care environment, based on an artificial intelligence approach, namely artificial neural networks (ANNs). A real world dataset acquired at Beth Israel Deaconess Medical Center (BIDMC) neonatal intensive care unit (NICU) is used to obtain the validation model and assess its performance. The dataset consists of high frequency sampled data of the level of oxygen saturation (SpO2) of neonates. A subset of 100 neonates was considered for modeling purposes. A total of 7,018,662 samples were available, containing 129,075 validated ones. The performance of the validation model, assessed in terms of its AUC, was of up to 0.75. Both the sensitivity and specificity reached acceptable values according to medical review. Future work would involve a prospective study and validation of the methods proposed in this work.	artificial intelligence;artificial neural network;data validation;evert willem beth;international components for unicode;prospective search;sensitivity and specificity	Federico Cismondi;André S. Fialho;Xiaoning Lu;Susana M. Vieira;James E. Gray;Shane R. Reti;João Miguel da Costa Sousa;Stan N. Finkelstein	2012	The 2012 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2012.6252782	computer science;machine learning;data integrity;artificial neural network	HPC	6.170789285972687	-77.99987276552125	121832
bb9f78ef95154c1f6aee09c846fbc922efaa205e	local linear wavelet neural network based breast tumor classification using firefly algorithm		Breast cancer is the major cause of cancer deaths in women today and it is the most common type of cancer in women. This paper presents some experiments for classifying breast cancer tumor and proposes the use of firefly algorithm (FA) to improve the performance of Local linear wavelet neural network. This work in fact uses FA to optimize the parameters of local linear wavelet neural network. The experiments were conducted on extracted breast cancer data from University of Winconsin Hospital, Madison. The result has been compared with a wide range of classifiers to evaluate its performance. The evaluations show that the proposed approach is very robust, effective and gives better correct classification as compared to other classifiers.	artificial neural network;experiment;firefly (cache coherence protocol);firefly algorithm;wavelet	Manas Ranjan Senapati;Pradipta Kishore Dash	2012	Neural Computing and Applications	10.1007/s00521-012-0927-0	artificial intelligence;machine learning;data mining	ML	7.334131233052638	-78.19424267161673	122077
1670fc1fb6f59b249772bb557f7f6033781ff13b	classification models to predict vasopressor administration for septic shock in the emergency department		Optimal management of sepsis and septic shock in the emergency department (ED) involves timely decisions related to intravenous fluid resuscitation and initiation of vasoactive medication support. A decision-support tool trained on electronic health record data, can help improve this complex decision. We retrospectively extracted vital signs, lab measurements, and fluid administration information from 807 patient visits over a two-year period to a major ED. Patients selected for inclusion had a high likelihood of septic shock. We trained binary classifiers to discriminate between patients administered vasopressors in the ED and those not administered vasopressors at any point. Using features extracted from the entire ED visit record yielded a maximum area under the receiver-operating characteristic curve (AUC) of 0.798 (95% CI 0.725–0.849) in a hold-out test set. In a separate task, we used individual vital signs observations with lab results to predict vasopressor administration, yielding a maximum AUC of 0.762 (95% CI 0.748–0.777). Lastly, we trained separate classifiers for different subgroups of vital signs observations. These subgroups were defined by the cumulative number of fluid boluses delivered at the time of the observation. The maximum AUC achieved by any of these classifiers was 0.815 (95% CI 0.784–0.853), occurring for vital signs observations made after 2 bolus administrations. Classifiers in all tasks significantly outperformed existing clinical tools for assessing prognosis in ED sepsis. This work shows how relatively few features can provide instantaneous and accurate prediction of need for an intervention that is typically a complex clinical decision.	accident and emergency department;area under curve;binary classification;bolus tracking;cooley's anemia;decision making;decision support system;electronic health records;erectile dysfunction;extraction;forecast of outcome;patient visit;patients;resuscitation procedure;sepsis;sepsis-associated encephalopathy;septic shock;septic equation;test set;vasoconstrictor agents;intravenous (iv) fluid	Varesh Prasad;James C. Lynch;Corey L. Pasakarnis;Jill E. Thorsen;Michael R. Filbin;Andrew T. Reisner;Thomas Heldt	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037402	emergency department;septic shock;bolus (medicine);vital signs;vasoactive;resuscitation;medical emergency;test set;sepsis;intensive care medicine;medicine	ML	6.975575898864142	-76.27855789159285	122191
561fc575e04d66c046b226dafe9732d69b697336	using artificial intelligence to improve hospital inpatient care	biomedical monitoring;patient treatment artificial intelligence hospital inpatient care ai america healthcare system disease detection predictive models;artificial intelligence medical diagnostic imaging hospitals real time systems biomedical monitoring;fast subset scan electronic health records ehr intelligent systems machine learning;hospitals;patient care;machine learning;intelligent systems;diseases;ehr;patient treatment artificial intelligence diseases health care hospitals patient care;patient treatment;artificial intelligence;fast subset scan;electronic health records;medical diagnostic imaging;real time systems;health care	AI can be used to address many challenges facing America's healthcare system - from disease detection to building predictive models for treatment - thereby improving the quality and lowering the cost of patient care.	artificial intelligence;predictive modelling	Daniel B. Neill	2013	IEEE Intelligent Systems	10.1109/MIS.2013.51	intelligent decision support system;computer science;artificial intelligence;health care	AI	4.368933753791655	-77.84856806333316	122230
3150ddc5cdd96b721dad266705fc6763b658b2c8	genetic circuit building blocks for cellular computation, communications, and signal processing	genetic engineering;directed evolution;building block;circuit design;gene network;analog computation;genetic signal processing;genetics;cellular computation;signal processing;cell communication;cell cell communications;synthetic gene networks;simulation tool;design methodology	In this paper, we review an emerging engineering discipline to programcell behaviors by embedding synthetic gene networks that performcomputation, communications, and signal processing. To accomplishthis goal, we begin with a genetic component library and a biocircuitdesign methodology for assembling these components into compoundcircuits. The main challenge in biocircuit design lies in selectingwell-matched genetic components that when coupled, reliably producethe desired behavior. We use simulation tools to guide circuitdesign, a process that consists of selecting the appropriatecomponents and genetically modifying existing components until thedesired behavior is achieved. In addition to such rational design, wealso employ directed evolution to optimize genetic circuitbehavior. Building on Nature's fundamental principle of evolution,this unique process directs cells to mutate their own DNA until theyfind gene network configurations that exhibit the desired systemcharacteristics. The integration of all the above capabilities infuture synthetic gene networks will enable cells to performsophisticated digital and analog computation, both asindividual entities and as part of larger cell communities. Thisengineering discipline and its associated tools will advance thecapabilities of genetic engineering, and allow us to harness cells fora myriad of applications not previously achievable.	analog computer;artificial gene synthesis;assembly language;british informatics olympiad;cell (microprocessor);cell signaling;circuit design;complex systems;computation;embedded system;entity;gene regulatory network;information processing;programming paradigm;prototype;robot;signal processing;simulation;synthetic intelligence	Ron Weiss;Subhayu Basu;Sara Hooshangi;Abigail Kalmbach;David K. Karig;Rishabh Mehreja;Ilka Netravali	2003	Natural Computing	10.1023/A:1023307812034	genetic engineering;gene regulatory network;cell signaling;design methods;directed evolution;computer science;bioinformatics;artificial intelligence;theoretical computer science;machine learning;circuit design;signal processing;genetics;algorithm	EDA	4.130273088095156	-67.5502935558601	122535
361e0c5373e53b1535572f3991b7c5a170e42179	bayesian averaging over decision tree models: an application for estimating uncertainty in trauma severity scoring	bayesian model averaging;decision tree;predictive posterior distribution;triss;trauma and injury severity scoring	INTRODUCTION For making reliable decisions, practitioners need to estimate uncertainties that exist in data and decision models. In this paper we analyse uncertainties of predicting survival probability for patients in trauma care. The existing prediction methodology employs logistic regression modelling of Trauma and Injury Severity Score (TRISS), which is based on theoretical assumptions. These assumptions limit the capability of TRISS methodology to provide accurate and reliable predictions.   METHODS We adopt the methodology of Bayesian model averaging and show how this methodology can be applied to decision trees in order to provide practitioners with new insights into the uncertainty. The proposed method has been validated on a large set of 447,176 cases registered in the US National Trauma Data Bank in terms of discrimination ability evaluated with receiver operating characteristic (ROC) and precision-recall (PRC) curves.   RESULTS Areas under curves were improved for ROC from 0.951 to 0.956 (p = 3.89 × 10-18) and for PRC from 0.564 to 0.605 (p = 3.89 × 10-18). The new model has significantly better calibration in terms of the Hosmer-Lemeshow Hˆ statistic, showing an improvement from 223.14 (the standard method) to 11.59 (p = 2.31 × 10-18).   CONCLUSION The proposed Bayesian method is capable of improving the accuracy and reliability of survival prediction. The new method has been made available for evaluation purposes as a web application.		Vitaly Schetinin;Livia Jakaite;Wojtek J. Krzanowski	2018	International journal of medical informatics	10.1016/j.ijmedinf.2018.01.009	data mining;statistic;logistic regression;statistics;decision tree;decision model;receiver operating characteristic;injury severity score;medicine;bayesian probability;bayesian inference	ML	6.7730198706861575	-75.02483173340468	122645
55340c15d6c549314442a2bf251b81d89e4f5342	grey correlation analysis on the influential factors the hospital medical expenditure	internal medicine;laboratory tests;factors;medical expenses;grey correlation analysis;correlation analysis	To investigate the implementation of Grey Correlation Analysis in the research of the hospitalization fee, to analyze the influential factors of the hospitalization expenses, and to provide proof for medical expenses containing measurements.To analyzed the in patient hospital fee of the top ten diseases of internal medicine inpatient in a hospital by Grey correlation analysis The grey correlation analysis shows the correlative degree between each kind of expense and their rankings are as follows: pharmaceutical fee 0.938, bed fee 0.8411, laboratory test fee 0.8331, radiation diagnosis fee 0.7655, and examination fee 0.6108. It indicates that for the internal medical disease, the dominating factor in hospital expenditure is the pharmaceutical fee. It is the medicine expenses which should be cut mostly in order to control the excessive increase of hospital medical expenditure. In the study of hospital medical expenditure, the grey correlation analysis fully utilized information and data, and led to more reasonable conclusions.		Su-feng Yin;Xiao-jing Wang;Jian-hui Wu;Guo-li Wang	2010		10.1007/978-3-642-16167-4_10	actuarial science;medicine;operations management;emergency medicine	NLP	7.881708194821235	-74.8244415001678	123716
59461627f83b91dd4ff239525f59663baad23ce2	genomics and metabolomics research for brain tumour diagnosis based on machine learning	glioblastoma multiforme;truncated data;magnetic resonance spectroscopy;clinical diagnosis;support system;machine learning;mixture model;dna microarray;proof of principle;brain tumour	The incorporation of new biomedical technologies in the diagnosis and prognosis of cancer is changing medicine to an evidence-based diagnosis. We summarize some studies related to brain tumour research in Europe, based on the metabolic information provided by in vivo Magnetic Resonance Spectroscopy (MRS) and transcriptomic profiling observed by DNA microarrays. The first result presents the improvement in brain tumour diagnosis by combining Long TE and Short TE single voxel MR Spectra. Afterwards, a mixture model for binned and truncated data to characterize and classify MRS is reviewed. The classification of Glioblastomas Multiforme and Meningothelial Meningiomas using single-labeling cDNA-based microarrays was studied as proof of principle in the incorporation of genomic information to clinical diagnosis. Finally, we present a Decision Support System for in-vivo classification of brain tumours were the best inferred classifiers are deployed for their clinical use.	dna microarray;decision support system;linear algebra;machine learning;metabolomics;minimal recursion semantics;mixture model;rca spectra 70;resonance;software diagnosis;test engineer;test set;truncation (statistics);video-in video-out;voxel	Juan Miguel García-Gómez;Salvador Tortajada;Javier Vicente;Carlos Sáez;Xavier Castells;Jan Luts;Margarida Julià-Sapé;Alfons Juan-Císcar;Sabine Van Huffel;Anna Barceló;Joaquín Ariño;Carles Arús;Montserrat Robles	2007		10.1007/978-3-540-73007-1_122	truncation;nuclear magnetic resonance spectroscopy;dna microarray;computer science;bioinformatics;machine learning;mixture model;proof of concept	ML	2.771404889221689	-74.237280293171	124200
d73598caa98346df22966278572cd3d64ad4d5b9	a computational model for telomere-dependent cell-replicative aging	cultured stem cells;computer model;human mesenchymal stem cells;telomere;stem cell;stochastic;random process;model;cell division;growth;computer simulation;stochastic growth model	Telomere shortening provides a molecular basis for the Hayflick limit. Recent data suggest that telomere shortening also influence mitotic rate. We propose a stochastic growth model of this phenomena, assuming that cell division in each time interval is a random process which probability decreases linearly with telomere shortening. Computer simulations of the proposed stochastic telomere-regulated model provides good approximation of the qualitative growth of cultured human mesenchymal stem cells.	approximation;cell division;computational model;mesenchymal stem cells;population dynamics;simulation;stochastic process;stretch-shortening exercise;telomere shortening;url shortening;negative regulation of telomere single strand break repair	R. D. Portugal;Marcelo G P Land;Benar Fux Svaiter	2008	Bio Systems	10.1016/j.biosystems.2007.10.003	computer simulation;biology;telomere;stem cell;bioinformatics;stochastic;genetics;cell division;statistics	AI	8.797393019906748	-66.70917755101465	124661
9393afe2f86ce31e4b42fd8bff1ec35ff1436381	dynamic simulation modeling of icu bed availability	erbium;healthcare systems;heart;medical bed utilization;medical administrative data processing;excess capacity;intensive care unit;availability;computer model;icu bed availability;psychology;telemetry availability medical services floors heart erbium personnel surgery psychology computational modeling;length of stay;medical floor beds dynamic simulation modeling icu bed availability intensive care unit healthcare systems excess capacity bed space personnel medical bed utilization computer model telemetry;computational modeling;medical services;emergency room;healthcare system;personnel;surgery;patient treatment;quality of care;acute care;dynamic simulation;patient satisfaction;telemetry;medical floor beds;patient treatment medical administrative data processing;bed space;floors;dynamic simulation modeling	The intensive care unit accounts for nearly 30% of inpatient expenditures while representing only 8% of t patient population. All healthcare systems must balance need for access and availability of intensive care unit b (ICU) versus excess capacity that wastes increasin limited healthcare resources including bed space personnel. The Cincinnati VA Medical Center is an acu care, university affiliated 220-bed facility serving eligibl veterans with medical, surgical, neurological a psychiatric care needs. ICU beds are unavailable ne one third of the time, eliminating new ICU admissions, a requiring diversion of ambulance traffic. Divertin ambulance traffic adversely impacts patient satisfact and community perception of quality of care delivered this center. Phased construction to relieve the problem planned, including additional telemetry beds, move ventilator dependent patients out of the ICU to Respiratory Care Unit (Tele/RCU), and development ICU swing beds in the emergency room area (Heart E We assessed the likelihood that the planned changes w result in the desired outcomes. A computer model representing medical be utilization at this facility was developed using dynam simulation software (Arena ). This model analyzed the flow of patients through the ICU, telemetry and medic floor beds under current bed allocation. The model w then used to evaluate the effects of the planned pha construction. The model demonstrated improv availability of ICU beds with the addition of the telemetr and respiratory care unit beds. Resolving ICU acc problems required addition of Heart ER bed Unexpectedly, increased ICU bed availability resulted increased telemetry and medical floor bed utilizati downstream and increased length of stay on the med service as the proportion of post-ICU patients increased the floors.	computer simulation;downstream (software development);dynamic simulation;international components for unicode;lotus improv;numerical weather prediction;offset binary;read-copy-update;simulation software;television	William Cahill;Marta Render	1999		10.1145/324898.325327	availability;dynamic simulation;erbium;simulation;capacity utilization;computer science;telemetry;computational model;heart	ML	6.16805321824911	-73.83623872054919	124792
04c6280b819f5a11975b8ac278d195dde6066785	generation of an intelligent medical system, using a real database, to diagnose bacterial infection in hospitalized patients	sensibilite;aide diagnostic;base relacional dato;correlacion;escherichia coli;informatica biomedical;biomedical data processing;systeme intelligent;diagnostic tool;hospital;infeccion;decision aid;genie biomedical;fuzzy rules;sistema inteligente;informatique biomedicale;predisposicion;bacterial infections;hombre;ayuda decision;relational database;demografia;hopital;prospectiva;sensitivity;intelligent medical system;bacteriose;predisposition;biomedical engineering;prospective;human;intelligent system;computer aid;base donnee relationnelle;aide decision;asistencia ordenador;evaluation;ingenieria biomedica;evaluacion;correlation;bacterial infection;diagnosis;infection;demography;assistance ordinateur;diagnostic aid;bacteriosis;sensibilidad;demographie;homme;ayuda diagnostica	The initial diagnosis of bacterial infections in the absence of laboratory microbiological data requires physicians to use clinical algorithms based on symptoms, patient history and infection site. Optimization of such algorithms would be achieved by including as many variables associated with bacterial infection as possible. Demographic data are easily available and frequently used to sub-group human populations. A prospective investigation was, therefore, undertaken to examine the influence of demographic variables on bacterial infection rates, using data obtained from 173 patients presenting to Albert Einstein Medical Center. Data was randomly selected from 149 of these patients and used to generate fuzzy rules to model an intelligent medical system. To test the accuracy of this system at determining bacterial infection, based solely on demographic data, the program was given the remaining 24 patients' information. All 18 patients with either streptococcal, staphylococcal or Escherichia coli infections were correctly diagnosed. Non-E.coli GNR were misdiagnosed as E. coli infections in two patients resulting in an overall prediction rate for the 24 patients of 91.66%. This study suggests that the direct correlation of demographic variables with a predisposition to bacterial infection allow the design of an intelligent medical system, which shows great future potential as a diagnostic tool for all physicians.		Diana R. Cundell;Randy S. Silibovsky;Robyn Sanders;Les M. Sztandera	2001	International journal of medical informatics	10.1016/S1386-5056(01)00169-1	medicine;pathology;sensitivity;relational database;computer science;evaluation;escherichia coli;correlation;surgery	ML	4.810039527978637	-79.30463939397596	125095
d4a1a326c4ced5102bb65bd281010695b455e392	pattern recognition using labelled and unlabelled data		This thesis presents the results of a three year investigation into combining labelled and unlabelled data for data classification. In the present world, there are many fields in which the quantity of data available to workers in that field has increased exponentially over the last few years. This has in part been due to improved methods of automatic data capture and in part due to improved electronic communication particularly via the internet. These vast quantities of data require some form of processing in order to transform the data into information. This is often a costly business requiring human (often expert) intervention. Our rationale for this investigation is that we wish to augment the information provided by the human experts with data which has not been processed by human experts. The actual method we investigate is classification using both processed (labelled) and unprocessed (unlabelled) data in order to reduce the requirement for human intervention. In Chapter 2 of the thesis we review several aspects of this problem as it features in the current literature. We discuss • Classification versus clustering • Error estimation training, testing and validation data. • Generalisation • Existing methods for combining labelled and unlabelled data • Combining classifiers • Artificial neural networks • The sufficient level of labelled samples for a classification task. • Active selection	artificial neural network;automatic identification and data capture;cluster analysis;design rationale;internet;pattern recognition;statistical classification	Lina Petrakieva	2004				ML	0.2730757632912705	-71.69755721589405	125376
f889177f0b93b975bfbd75be28e90b8e81a5cc64	optimal control of anti-hbv treatment based on combination of traditional chinese medicine and western medicine	traditional chinese medicine;optimal control;therapy;chronic hepatitis b;viral dynamics;efficacy;virus infection;model;lamivudine;combination treatment;adefovir dipivoxil;monotherapy;dynamics in vivo	In this paper, a Hepatitis B virus model with standard incidence rate and logistic proliferation of healthy and infected cells is presented. Based on this model, we study an optimal control problem about anti-HBV infection combination therapy of Traditional Chinese Medicine and Western Medicine, the optimal strategies of taking medicine are given by simulation. Two optimal strategies with or without the impact to the infection rate by treatment are compared, simulation shows the impact to the reduction of infection may be omitted when mathematical model is used to study the anti-HBV therapy which is consistent with some references. What is more, optimal control strategy with other constant control strategies are also compared, and the simulation shows the optimal control strategy is better than constant control strategies.	optimal control	Yongmei Su;Deshun Sun	2015	Biomed. Signal Proc. and Control	10.1016/j.bspc.2014.09.007	traditional chinese medicine;alternative medicine;medicine;optimal control;virology;therapy;efficacy	Robotics	8.537270625970057	-73.31489439398402	125468
aad242f919b55f815bbfc4719234461643513015	disease diagnosis validation in tropix using cbr	aggregation function;tropix;case base reasoning;disease;singular value decomposition;health care delivery;data filtering;classification;clustering;validation;diagnosis;domain specificity;case library	TROPIX is a practical application project initially designed to help improve health care delivery in the rural/semi urban clinics and public hospitals in Nigeria due largely to limited laboratory facilities, medical doctors, and expertise. This paper is devoted to the use of case-based reasoning (CBR) paradigm in concert with statistical association-based reasoning (ABR) for disease diagnosis, validation and therapy selection components of the research. Essentially, tentative disease diagnosis arrived at by some classification method using similarity and dissimilarity aggregate functions, the matched vector functions (MVF), aided by the application of evidence ratio factors (ERF) for tied match cases is passed to the CBR model for validation by reusing past similar cases. The design and organization of the case-library using singular value decomposition (SVD) technique on the disease-attribute decision matrix to generate primary/secondary storage key clusters, as well as the use of domain-specific case-object properties that help to build a good case-base are described in some detail. The paper presents a disease case validation algorithm for appropriate data filtering and therapy selection enhancement from the new case-base.		Ambrose Sunny Ochi-Okorie	1998	Artificial intelligence in medicine	10.1016/S0933-3657(97)00039-0	simulation;biological classification;computer science;artificial intelligence;machine learning;data mining;cluster analysis;singular value decomposition	AI	1.7846741628440892	-77.69801513536643	125479
cb415782c4da346b075b5e031ad01ef1c4c93a0f	preprocessing of automated blood cell counter data and discretization of data using chi merge algorithm in clinical pathology		This paper applies the preprocessing phases of the Knowledge Discovery in Databases to the automated blood cell counter data and creates discrete ranges of blood cell counter data that can be used in grouping data using classification, clustering and association rule generation. The functions of an automated blood cell counter from a clinical pathology laboratory and the phases in Knowledge Discovery in Databases are explained briefly. Twelve thousand records are taken from a clinical laboratory for processing. The preprocessing steps of the KDD process are applied on the blood cell counter data. This paper applies the Chi Merge algorithm on the blood cell counter data and generates discretized data representing ranges of values for the data.	chi;discretization;merge algorithm;merge sort;preprocessor	D. Minnie;S. Srinivasan	2012		10.1007/978-3-642-31600-5_50	computer science;theoretical computer science;pattern recognition;data mining	ML	2.6023831175501857	-77.5240588116147	125789
2a64a840d32266fe03892364f0dd19d4adb3710b	monitoring pharmacy expert system performance using statistical process control methodology	drug therapy computer assisted;expert systems;clinical pharmacy information systems;equipment failure;medical records systems computerized;statistics as topic;medication errors;humans;quality control;automatic data processing	Automated expert systems provide a reliable and effective way to improve patient safety in a hospital environment. Their ability to analyze large amounts of data without fatigue is a decided advantage over clinicians who perform the same tasks. As dependence on expert systems increase and the systems become more complex, it is important to closely monitor their performance. Failure to generate alerts can jeopardize the health and safety of patients, while generating excessive false positive alerts can lead to valid alerts being dismissed as noise. In this study, statistical process control charts were used to monitor an expert system, and the strengths and weaknesses of this technology are presented.	chart;dismiss;expert system;fatigue;hospitals;image noise;patients;weakness	Joshua A. Doherty;Richard M. Reichley;Laura A. Noirot;Ervina Resetar;Michael R. Hodge;Robert D. Sutter;W. Claiborne Dunagan;Thomas C. Bailey	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		reliability engineering;medicine;data mining;biological engineering	Embedded	4.404976363794523	-78.0941816162377	126638
75fe0f1cedf1f744483ca2081da66499a21f6622	reachability-related analysis of double-deficient k-th order systems of petri nets in terms of a closed-form solution		Earlier, Chao pioneered the very first closed-form solution (CFS) of the number of control related states (CRSs) for k-th order systems, which was the first step in facilitating an exponential computation time for enumerating reachable states of a particular large Petri net (PN), reducing this to within a second. This paper progresses a step further in enumerating the control related states for double-deficient k-th order systems (the initial tokens of each idle place in both processes are lower than k), which is an essential element of a non-sharing subnet in PNs. From the closed-form solution for a double-deficient k-th order system, we can extend the modeling system capability to a dynamic non-sharing sub-net allocation k-th order system, by constructing a dynamic initial token allocation mechanism for whole-system performance calibration according to the modified deficient reachablility ratio (DRO) derived by the solution in real time.	allocation;cns disorder;calibration;chao (sonic);chronic fatigue syndrome;climate forecast system;computation;ephrin type-b receptor 1, human;petri net;reachability;subnetwork;exponential	Tsung Hsien Yu	2017	IECON 2017 - 43rd Annual Conference of the IEEE Industrial Electronics Society	10.1109/IECON.2017.8216521	subnet;closed-form expression;petri net;control theory;engineering;dynamic priority scheduling;computation;exponential function;reachability;real-time computing;idle	Robotics	10.023686921383328	-71.61841240889036	127089
9ab5a365b36e43cd7d354a9aca85814820c2b7a4	evaluating predictive modeling's potential to improve teleretinal screening participation in urban safety net clinics	risk assessment;proportional hazards models;young adult;bayes theorem;prevalence	INTRODUCTION Screening guidelines for diabetic patients recommend yearly eye examinations to detect diabetic retinopathy and other forms of diabetic eye disease. However, annual screening rates for retinopathy in US urban safety net settings remain low.   METHODS Using data gathered from a study of teleretinal screening in six urban safety net clinics, we assessed whether predictive modeling could be of value in identifying patients at risk of developing retinopathy. We developed and examined the accuracy of two predictive modeling approaches for diabetic retinopathy in a sample of 513 diabetic individuals, using routinely available clinical variables from retrospective medical record reviews. Bayesian networks and radial basis function (neural) networks were learned using ten-fold cross-validation.   RESULTS The predictive models were modestly predictive with the best model having an AUC of 0.71.   DISCUSSION Using routinely available clinical variables to predict patients at risk of developing retinopathy and to target them for annual eye screenings may be of some usefulness to safety net clinics.	area under curve;artificial neural network;bayesian network;clinic;cross reactions;cross-validation (statistics);diabetes mellitus;diabetic retinopathy;disorder of eye;ephrin type-b receptor 1, human;medical records;patients;predictive modelling;radial (radio);radial basis function;retinal diseases;screening procedure;annual screening;diabetic eye disease	Omolola Ogunyemi;Senait Teklehaimanot;Lauren Daskivich;Erin Moran;Sheba M. George	2013	Studies in health technology and informatics	10.3233/978-1-61499-289-9-162	emergency medicine;knowledge management;safety net;optometry;diabetic eye disease;diabetic retinopathy;annual screening;risk assessment;retinopathy;diabetes mellitus;telemedicine;medicine	ML	6.902065948711067	-75.5889661209478	127330
35b411e4bdbe1fc30bd0f246b0ce3a3f88de819d	combining fuzzy experts' decisions fusion with linguistic summarization of mammograms for computer-aided breast diagnosis	pragmatics;cancer;patient diagnosis decision theory fuzzy set theory human computer interaction mammography medical computing;breast;breast pragmatics mammography medical diagnostic imaging computational modeling cancer;computational modeling;bi rads linguistic summarization of data fuzzy decisions fusion computational theory of perceptions granular linguistic model of a phenomenon computer aided breast diagnosis;mammography;medical diagnostic imaging;fuzzy expert decisions fusion breast lesion findings breast imaging reporting and data system information fusion bi rads radiology standard breast image mammograms ctp based system human computer interaction linguistic expressions linguistic data summarization computational theory of perceptions computer aided breast diagnosis	The Computational Theory of Perceptions (CTP) provides capabilities for linguistic summarization of data and it aims the description of patterns emerging from these data by means of linguistic expressions. This technique is particularly well suited in applications where there is the need of understanding the information at different levels of expertise and/or when intense human-computer interaction is required. In this paper, we present a CTP-based system able to generate valuable linguistic reports from findings in breast image mammograms using the BI-RADS radiology standard. The implemented framework uses data obtained through the fusion of information provided by different medical experts on the same mammography. Then, our system automatically produces a collection of valid sentences describing: the breast lesion findings (i.e. features of masses or calcifications), the BI-RADS category rating for the analyzed mammography and the recommendations for patients. The system validation by several medical specialists has produced promising results, which make it useful for its practical deployment.	automatic summarization;bi-rads;computation;fuzzy concept;human–computer interaction;ion implantation;online and offline;prototype;radiology;software deployment	Éldman de Oliveira Nunes;Ángel Sánchez;Ana Belén Moreno;Aura Conci	2016	2016 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)	10.1109/FUZZ-IEEE.2016.7737738	computer vision;computer science;artificial intelligence;data mining;computational model;cancer;pragmatics	Robotics	3.176332277247801	-79.16076781314962	127459
41edcf3c320516fad8a247f83682cf752697f6a9	foreseeing promising bio-medical findings for effective applications of data mining	automated data collection;data mining;information and communication technology;knowledge discovery in database;medical informatic;health care	The increasing availability of automated data collection tools, database technologies and Information and Communication Technologies in biomedicine and health care have led to huge amounts of biomedical and healthcare data accumulated in several repositories. Unfortunately, the process of analysis of such data represents a complex task also because data volumes grow exponentially so manual analysis and interpretation become impractical. Fortunately, knowledge discovery in databases (KDD) and data mining (DM) are powerful tools available to medical and research people for help them in explore data and discover useful knowledge. To assess the spread of DM and KDD in biomedicine and health care, we designed and performed a search database of biomedical and health-care scientific literature, for the year interval 1997-2004, and analyzed the obtained results. There has been an increase of application of DM methods in literature of bio-medical informatics research most of which in bioinformatics and genomic area.	british informatics olympiad;data mining;database;scientific literature	Stefano Bonacina;Marco Masseroli;Francesco Pinciroli	2005		10.1007/11573067_14	information and communications technology;computer science;bioinformatics;data science;data mining;database;health care	ML	-3.1543830695459945	-66.20196339855838	127495
2c675e7adc18243466ff1535adf35fc848e87d99	design automation for synthetic biological systems	biology computing;dna proteins biological systems biological information theory molecular biophysics;cad biology computing;cad;pathways synthetic biology systems biology metabolic engineering genetic regulatory network electronic design automation parts devices systems;pathway based approach synthetic biological system engineering method specialized biological component complex system biodesign automation bottom up modular construction biological primitives	Through principled engineering methods, synthetic biology aims to build specialized biological components that can be modularly composed to create complex systems. This article outlines bio-design automation using two complementary design approaches, bottom-up modular construction from biological primitives and pathway-based approaches. The article also highlights future challenges for both.	biological system;bottom-up proteomics;british informatics olympiad;complex systems;electronic design automation;gene regulatory network;modular design;synthetic biology;synthetic intelligence	Douglas Densmore;Soha Hassoun	2012	IEEE Design & Test of Computers	10.1109/MDT.2012.2193370	bioinformatics;systems engineering;engineering;cad;systems biology	EDA	4.02829108700409	-67.4867305464762	127830
33563db8fbe30da9855bcc6778bea280b2320c30	causality refined diagnostic prediction		Applying machine learning in the health care domain has shown promising results in recent years. Interpretable outputs from learning algorithms are desirable for decision making by health care personnel. In this work, we explore the possibility of utilizing causal relationships to refine diagnostic prediction. We focus on the task of diagnostic prediction using discomfort drawings, and explore two ways to employ causal identification to improve the diagnostic results. Firstly, we use causal identification to infer the causal relationships among diagnostic labels which, by itself, provides interpretable results to aid the decision making and training of health care personnel. Secondly, we suggest a post-processing approach where the inferred causal relationships are used to refine the prediction accuracy of a multi-view probabilistic model. Experimental results show firstly that causal identification is capable of detecting the causal relationships among diagnostic labels correctly, and secondly that there is potential for improving pain diagnostics prediction accuracy using the causal relationships.	algorithm;causality;machine learning;sensor;statistical model;video post-processing	Marcus Klasson;Kun Zhang;Bo C. Bertilson;Cheng Zhang;Hedvig Kjellström	2017	CoRR		machine learning;artificial intelligence;mathematics;health care;causality;statistical model	ML	1.9293142494094078	-74.00568599990478	127880
e569131d813be4ed3b027162acf6f130301d3172	gamenet: graph augmented memory networks for recommending medication combination		Recent progress in deep learning is revolutionizing the healthcare domain including providing solutions to medication recommendations, especially recommending medication combination for patients with complex health conditions. Existing approaches either do not customize based on patient health history, or ignore existing knowledge on drug-drug interactions (DDI) that might lead to adverse outcomes. To fill this gap, we propose the Graph Augmented Memory Networks (GAMENet), which integrates the drug-drug interactions knowledge graph by a memory module implemented as a graph convolutional networks, and models longitudinal patient records as the query. It is trained end-to-end to provide safe and personalized recommendation of medication combination. We demonstrate the effectiveness and safety of GAMENet by comparing with several state-of-the-art methods on real EHR data. GAMENet outperformed all baselines in all effectiveness measures, and also achieved 3.60% DDI rate reduction from existing EHR data.	deep learning;end-to-end principle;find-a-drug;interaction;knowledge graph;memory bank;memory management;memory module;personalization	Junyuan Shang;Cao Xiao;Tengfei Ma;Hongyan Li;Jimeng Sun	2018	CoRR		memory module;data mining;machine learning;baseline (configuration management);deep learning;computer science;artificial intelligence;graph	ML	2.6246051496569107	-73.15495866024226	127954
9cc3e53b3d2b774a4576c1e489bca570c8c6c72e	how does age affect baseline screening mammography performance measures? a decision model	performance measure;age factors;decision models;sensitivity and specificity;breast neoplasms;female;health informatics;decision tree;mass screening;middle aged;medical decision making;information systems and communication service;prevalence;adult;management of computing and information systems;predictive value of tests;positive predictive value;detection rate;outcome assessment health care;humans;mammography;decision trees;surveillance epidemiology and end results;breast cancer;aged	BACKGROUND In order to promote consumer-oriented informed medical decision-making regarding screening mammography, we created a decision model to predict the age dependence of the cancer detection rate, the recall rate and the secondary performance measures (positive predictive values, total intervention rate, and positive biopsy fraction) for a baseline mammogram.   METHODS We constructed a decision tree to model the possible outcomes of a baseline screening mammogram in women ages 35 to 65. We compared the single baseline screening mammogram decision with the no screening alternative. We used the Surveillance Epidemiology and End Results national cancer database as the primary input to estimate cancer prevalence. For other probabilities, the model used population-based estimates for screening mammography accuracy and diagnostic mammography outcomes specific to baseline exams. We varied radiologist performance for screening accuracy.   RESULTS The cancer detection rate increases from 1.9/1000 at age 40 to 7.2/1000 at age 50 to 15.1/1000 at age 60. The recall rate remains relatively stable at 142-157/1000, which varies from 73-236/1000 at age 50 depending on radiologist performance. The positive predictive value of a screening mammogram increases from 1.3% at age 40 to 9.8% at age 60, while the positive predictive value of a diagnostic mammogram varies from 2.9% at age 40 to 19.2% at age 60. The model predicts the total intervention rate = 0.013*AGE2 - 0.67*AGE + 40, or 34/1000 at age 40 to 47/1000 at age 60. Therefore, the positive biopsy (intervention) fraction varies from 6% at age 40 to 32% at age 60.   CONCLUSION Breast cancer prevalence, the cancer detection rate, and all secondary screening mammography performance measures increase substantially with age.	baseline dental cement;baseline (configuration management);decision making;decision tree;diagnostic mammography;estimated;malignant neoplasm of breast;mammary neoplasms;positive predictive value of diagnostic test;probability;radiology;screening mammography;sensitivity and specificity	John D. Keen;James E. Keen	2008	BMC Medical Informatics and Decision Making	10.1186/1472-6947-8-40	health informatics;medicine;pathology;gynecology;decision tree	ML	7.320171095586218	-75.23764715049175	128264
00cc23c6302980660f826f2947b4dfe6210b7e09	hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification		Network biology has been successfully used to help reveal complex mechanisms of disease, especially cancer. On the other hand, network biology requires in-depth knowledge to construct disease-specific networks, but our current knowledge is very limited even with the recent advances in human cancer biology. Deep learning has shown a great potential to address the difficult situation like this. However, deep learning technologies conventionally use grid-like structured data, thus application of deep learning technologies to the classification of human disease subtypes is yet to be explored. Recently, graph based deep learning techniques have emerged, which becomes an opportunity to leverage analyses in network biology. In this paper, we proposed a hybrid model, which integrates two key components 1) graph convolution neural network (graph CNN) and 2) relation network (RN). We utilize graph CNN as a component to learn expression patterns of cooperative gene community, and RN as a component to learn associations between learned patterns. The proposed model is applied to the PAM50 breast cancer subtype classification task, the standard breast cancer subtype classification of clinical utility. In experiments of both subtype classification and patient survival analysis, our proposed method achieved significantly better performances than existing methods. We believe that this work is an important starting point to realize the upcoming personalized medicine.	artificial neural network;bioinformatics;biological network;convolution;deep learning;performance;semantic network;synthetic data	SungMin Rhee;Seokjun Seo;Sun Kim	2018		10.24963/ijcai.2018/490	pattern recognition;convolutional neural network;filter (signal processing);machine learning;breast cancer;personalized medicine;biological network;computer science;data model;deep learning;graph;artificial intelligence	AI	2.8916545523341304	-70.4426934236414	128595
ac104cbaf90aebad23096cb49f3d58ac116f6f70	use of clustering algorithms and extreme learning machine in determining arrhythmia types		"""This study examined the electrocardiographic data set recorded by Boston's Beth Israel Hospital for the work of the cardiac and neurotransmitter system and for normal and various irregular heartbeat patterns of electrical activity in the heart. Seven different types of arrhythmia, available in this data set, were classified using four different, widely used, classifiers (Fuzzy C-Means, Naive Bayes, Extreme Learning Machine and K-Means) by multiple classification methods. Classifier performances were evaluated using accuracy, sensitivity, and selectivity classification performance measures. The results of the study showed that classification achievements for four classifiers had the highest success rate of 99% of """"Normal"""" beat type compared to other types of arrhythmia. The average classification performances of Naive Bayes and Extreme Learning Machine classifiers were found to be higher when the classifiers were compared among themselves. When the averages of all the arrhythmia types were taken the most successful classifier was detected as Naive Bayes classifier with 92% accuracy and 95% selectivity values."""	algorithm;cluster analysis;evert willem beth;fuzzy clustering;k-means clustering;naive bayes classifier;performance;selectivity (electronic);statistical classification	Ebru Sayilgan;Ozlem Karabiber Cura;Yalcin Isler	2017	2017 25th Signal Processing and Communications Applications Conference (SIU)	10.1109/SIU.2017.7960236	computer science;pattern recognition;naive bayes classifier;fuzzy logic;artificial intelligence;extreme learning machine;cluster analysis;statistical classification;machine learning;probabilistic classification;bayes error rate;bayes classifier	ML	8.073004883421738	-77.96167917347115	129023
0f2f971d6c5db73192c3ced26e5928004d955d25	robust conclusions in mass spectrometry analysis	robust decisions;mass spectrometry;computer science all;data analysis;inf 01 informatica;graph;inference	A central issue in biological data analysis is that uncertainty, resulting from different factors of variability, may change the effect of the events being investigated. Therefore, robustness is a fundamental step to be considered. Robustness refers to the ability of a process to cope well with uncertainties, but the different ways to model both the processes and the uncertainties lead to many alternative conclusions in the robustness analysis. In this paper we apply a framework allowing to deal with such questions for mass spectrometry data. Specifically, we provide robust decisions when testing hypothesis over a case/control population of subject measurements (i.e. proteomic profiles). To this concern, we formulate (i) a reference model for the observed data (i.e., graphs), (ii) a reference method to provide decisions (i.e., test of hypotheses over graph properties) and (iii) a reference model of variability to employ sources of uncertainties (i.e., random graphs). We apply these models to a realcase study, analyzing the mass spectrometry profiles of the most common type of Renal Cell Carcinoma; the Clear Cell variant.	biological system;british informatics olympiad;cluster analysis;computation;function (biology);graph property;heart rate variability;machine learning;newton's method;prospective search;proteomics;random graph;reference model;robustness (computer science);spatial variability	Italo Zoppis;Riccardo Dondi;Massimiliano Borsani;Erica Gianazza;Clizia Chinello;Fulvio Magni;Giancarlo Mauri	2015		10.1016/j.procs.2015.05.185	econometrics;mathematical optimization;mass spectrometry;computer science;machine learning;data mining;graph;data analysis;statistics	ML	5.075219660676566	-70.11527827497717	129411
07bb8c949bd7adf8dbfdd858c68ac8e1011900fb	use of simulation to determine resource requirements for end-stage renal failure	discrete event simulation;patient treatment;discrete event simulation;end-stage renal failure;modeling methods;resource requirements	All Western countries are taking increasing numbers of patients onto their renal programs. Physicians now accept older and sicker patients than they would have done in the past. A discrete event simulation describes patient arrivals and the transfer between three modalities of treatment for different age and risk groups in order to project future demands for treatment. In the UK, at the national level the main uncertainty arises from the expected number of new patients, which is on an upward trajectory and has not yet reached the level of most other European countries or the USA. At a local level the uncertainties are much greater because of the inherent randomness in smaller populations. In these smaller populations, simpler modeling methods that only take account of new arrivals, transplant and death rates may be equally valuable, providing that the standard deviations of the estimates can be calculated.	population;randomness;requirement;simulation	Ruth Davies	2006	Proceedings of the 2006 Winter Simulation Conference		simulation;computer science;discrete event simulation;mathematics;standard deviation;operations research;statistics	Comp.	6.713803694632527	-73.73636195643509	129481
c7b0334d1763c27b49d21ffdb8a657af0d3d7d14	balance-bagging-prfs algorithm for feature optimization on insomnia data intervened by traditional chinese medicine	control group;traditional chinese medicine;bagging;information technology;feature optimization;portable document format;insomnia;ieee xplore;tcm;prediction risk;prediction risk tcm insomnia feature optimization bagging	"""Goal: Traditional Chinese Medicine (TCM) focuses on individual diagnosis. Besides the analysis methods on group level, clinical experimental data could also be researched with Information Technology to optimize the feature for individual healing effect; Method: we propose and apply a new method of feature optimization — Balance-Bagging-PRFS — to optimize the feature of insomnia intervened by TCM, aiming at solving problems typically in TCM data, such as mixing of discrete and continuous features and data imbalance; Result: from the view of all data, it is found that different levels of """"ISI baseline score"""" and """"Insomnia severity"""" have important influence on the curative effect. In treat group, different values of """"environment"""" and """"social field baseline"""" make remarkable difference on curative effect; while in control group, in which patients are treated with the placebo, """"social field baseline"""", """"survival quality baseline"""", and """"classification of constitution"""" make sense; Conclusion: the method of Balance-Bagging-PRFS achieves good results in feature optimization for data from insomnia interfered by TCM, and it provides a basis for TCM individual diagnosis and for further optimization of symptom."""	algorithm;baseline (configuration management);information sciences institute;mathematical optimization;toolkit for conceptual modeling	Xiao-bo Yang;Shixing Yan;Zheng-yang Zhou;Guozheng Li;Yan Li;Xin-feng Guo	2011	2011 IEEE International Conference on Bioinformatics and Biomedicine Workshops (BIBMW)	10.1109/BIBMW.2011.6112485	traditional chinese medicine;bootstrap aggregating;computer science;artificial intelligence;data mining;scientific control	DB	6.060431758592283	-76.84480501166692	129910
4e10f9b1d6f3bdc3dedc435657f8a37a1df6eb90	a human-centric user model for intelligent healthcare	user modelling;user modelling case based reasoning decision support systems diseases health care knowledge based systems learning artificial intelligence patient diagnosis;intelligent healthcare systems healthcare decision making case based reasoning techniques rule based reasoning techniques reasoning approach semantic learning human centric semantic user modelling medical accidents medical complains communication gap medical related user data disease detection disease prevention real time monitoring health maintenance disease diagnosis;human centric;semantic learning;semantic;intelligent healthcare;semantic learning intelligent healthcare semantic human centric user modelling;context semantics diseases artificial intelligence cognition medical diagnostic imaging	Making use of IT technologies, various intelligent healthcare systems are designed which facilitate disease diagnosis, health maintenance and real-time monitoring. To detect and prevent the disease intelligently, user modelling of these system focuses on medical-related user data. However a communication gap between patients and doctors is still so large that yielding amounts of medical complains and accidents. This paper proposes a human-centric semantic user model and developed a novel semantic learning and reasoning approach using a combination of rule-based and case-based reasoning techniques to lessen the communication gap between doctors and patients by focusing on the dedicated user model for the purpose of providing assistants for healthcare decision making.	case-based reasoning;logic programming;real-time transcription;user modeling	Yiye Zhu;Jianping Shen;Yinsheng Li	2014	2014 IEEE 11th International Conference on e-Business Engineering	10.1109/ICEBE.2014.15	computer science;knowledge management;data science;model-based reasoning;data mining	Robotics	4.055563261638565	-78.87598001653552	129950
95a50a6440f1478ceafe816847ea6f276bf7f231	learning to predict life and death from go game records	evaluation function;learning;life and death;go;learning system;neural net;game records;game of go	This paper presents a learning system for predicting life and death in the game of Go. Learning examples are extracted from game records. On average our system correctly predicts life and death for 88% of all blocks. Towards the end of a game the performance increases up to 99%. Clearly, such a predictor will be an important component for building a full-board evaluation function.	evaluation function;kerrison predictor;life & death	Erik C. D. van der Werf;Mark H. M. Winands;H. Jaap van den Herik;Jos W. H. M. Uiterwijk	2005	Inf. Sci.	10.1016/j.ins.2004.04.013	simulation;computer science;artificial intelligence;machine learning;evaluation function;artificial neural network	AI	6.218006225345204	-78.48703803057995	130075
aa5293403da76c8f4e2711104e2abb43b13abe48	attributes for causal inference in longitudinal observational databases		The pharmaceutical industry is plagued by the problem of side effects that can occur anytime a prescribed medication is ingested. There has been a recent interest in using the vast quantities of medical data available in longitudinal observational databases to identify causal relationships between drugs and medical events. Unfortunately the majority of existing post marketing surveillance algorithms measure how dependant or associated an event is on the presence of a drug rather than measuring causal-ity. In this paper we investigate potential attributes that can be used in causal inference to identify side effects based on the Bradford-Hill causality criteria. Potential attributes are developed by considering five of the causality criteria and feature selection is applied to identify the most suitable of these attributes for detecting side effects. We found that attributes based on the specificity criterion may improve side effect signalling algorithms but the experiment and dosage criteria attributes investigated in this paper did not offer sufficient additional information.	akaike information criterion;anytime algorithm;causal inference;causality;database;feature selection;flight recorder;list of code lyoko episodes;machine learning;plausibility structure;sensitivity and specificity;sensor;side effect (computer science)	Jenna Reps;Jonathan M. Garibaldi;Uwe Aickelin;Daniele Soria;Jack E. Gibson;Richard B. Hubbard	2014	CoRR		econometrics;data mining;statistics	NLP	6.929161136905194	-73.33835074962977	130319
1fbc13d5302ce67c44d8a796080b3349029fc7d7	customizing a commercial rule base for detecting drug-drug interactions		We developed and implemented an adverse drug event system (PharmADE) that detects potentially dangerous drug combinations using a commercial rule base. While commercial rule bases can be useful for rapid deployment of a safety net to screen for drug-drug interactions, they sometimes do not provide the desired rule sensitivity. We implemented methods for enhancing commercial drug-drug interaction rules while preserving the original rule base architecture for easy and low cost maintenance.	adverse reaction to drug;base;drug interactions;drug screening assays, antitumor;ephrin type-b receptor 1, human;interaction;rule (guideline);rule 184;rule-based system;software deployment	Ervina Resetar;Richard M. Reichley;Laura A. Noirot;W. Claiborne Dunagan;Thomas C. Bailey	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		drug;software deployment;safety net;architecture;real-time computing;pharmacology;medicine	Embedded	1.1039175944252744	-69.65025130926693	130325
664c1a2fae50fa402feb8ad0d76427f8f08e58b8	evolutionary-driven support vector machines for determining the degree of liver fibrosis in chronic hepatitis c	support vector machines;liver fibrosis;liver disease;chronic hepatitis c;liver fibrosis stage;evolutionary algorithms;feature selection;formula for class prediction;support vector machine;evolutionary algorithm;hepatic fibrosis	OBJECTIVE Hepatic fibrosis, the principal pointer to the development of a liver disease within chronic hepatitis C, can be measured through several stages. The correct evaluation of its degree, based on recent different non-invasive procedures, is of current major concern. The latest methodology for assessing it is the Fibroscan and the effect of its employment is impressive. However, the complex interaction between its stiffness indicator and the other biochemical and clinical examinations towards a respective degree of liver fibrosis is hard to be manually discovered. In this respect, the novel, well-performing evolutionary-powered support vector machines are proposed towards an automated learning of the relationship between medical attributes and fibrosis levels. The traditional support vector machines have been an often choice for addressing hepatic fibrosis, while the evolutionary option has been validated on many real-world tasks and proven flexibility and good performance.   METHODS AND MATERIALS The evolutionary approach is simple and direct, resulting from the hybridization of the learning component within support vector machines and the optimization engine of evolutionary algorithms. It discovers the optimal coefficients of surfaces that separate instances of distinct classes. Apart from a detached manner of establishing the fibrosis degree for new cases, a resulting formula also offers insight upon the correspondence between the medical factors and the respective outcome. What is more, a feature selection genetic algorithm can be further embedded into the method structure, in order to dynamically concentrate search only on the most relevant attributes. The data set refers 722 patients with chronic hepatitis C infection and 24 indicators. The five possible degrees of fibrosis range from F0 (no fibrosis) to F4 (cirrhosis).   RESULTS Since the standard support vector machines are among the most frequently used methods in recent artificial intelligence studies for hepatic fibrosis staging, the evolutionary method is viewed in comparison to the traditional one. The multifaceted discrimination into all five degrees of fibrosis and the slightly less difficult common separation into solely three related stages are both investigated. The resulting performance proves the superiority over the standard support vector classification and the attained formula is helpful in providing an immediate calculation of the liver stage for new cases, while establishing the presence/absence and comprehending the weight of each medical factor with respect to a certain fibrosis level.   CONCLUSION The use of the evolutionary technique for fibrosis degree prediction triggers simplicity and offers a direct expression of the influence of dynamically selected indicators on the corresponding stage. Perhaps most importantly, it significantly surpasses the classical support vector machines, which are both widely used and technically sound. All these therefore confirm the promise of the new methodology towards a dependable support within the medical decision-making.	act relationship join - detached;artificial intelligence;chemical and drug induced liver injury;chronic lymphocytic leukemia;class;clinical decision-making;coefficient;decision making;dependability;disk staging;embedded system;embedding;evolutionary algorithm;feature selection;fibrosis;fibrosis, liver;genetic algorithm;hepatitis b;hepatitis c, chronic;hepatitis, chronic;iterative and incremental development;mathematical optimization;mechatronics;nucleic acid hybridization;patients;pointer (computer programming);pointer <dog>;power (psychology);precipitating factors;support vector machine	Ruxandra Stoean;Catalin Stoean;Monica Lupsor Paton;Horia Stefanescu;Radu Badea	2011	Artificial intelligence in medicine	10.1016/j.artmed.2010.06.002	support vector machine;computer science;machine learning;evolutionary algorithm	AI	7.242736120926892	-78.36547515995949	130449
0e1e40bf593ec2423da489b2f5c493833a4346d8	a framework for individualizing predictions of disease trajectories by exploiting multi-resolution structure		For many complex diseases, there is a wide variety of ways in which an individual can manifest the disease. The challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual’s disease, which can in turn enable clinicians to optimize treatments. We represent an individual’s disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time. We propose a hierarchical latent variable model that individualizes predictions of disease trajectories. This model shares statistical strength across observations at different resolutions–the population, subpopulation and the individual level. We describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters. Finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma. We compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy.	algorithm;baseline (configuration management);hierarchical database model;interstitial webpage;latent variable model;online and offline;personalization	Peter Schulam;Suchi Saria	2015			simulation;bioinformatics;data mining	ML	8.918827205911363	-72.20625178885753	130809
f6674c36593c4362a390a74b14605d0e5c1a7f99	multicentre study design in survival analysis	adjuvant therapy;breast cancer diseases tumors hazards surgery artificial neural networks;study design;medical administrative data processing;cancer;hazards;multicentre study design;medical computing;data analysis;artificial neural networks;survival analysis;surgery;multicentre study design survival analysis artificial neural networks;tumors;diseases;patient treatment;reliability analysis;data acquisition protocol multicentre study design medical statistics prognostic index disease recurrence treatment outcome medical survival analysis cancer disease prognostic algorithm benchmark mathematical algorithm post operative adjuvant therapy;mechanical systems;patient treatment benchmark testing cancer data acquisition data analysis medical administrative data processing medical computing;data acquisition;breast cancer;benchmark testing;artificial neural network	Survival analysis is an important part of medical statistics or for the study of failures in mechanical systems. The latter is called reliability analysis in engineering. This paper addresses the survival analysis in medical research. In this context of medical research, the survival analysis is frequently used to define prognostic indices for survival or recurrence of a disease, and to study outcome of treatment. Within the broader domain of medical survival analysis or more specifically for the study of patients with cancer disease, there are a number of clinical/prognostic techniques developed over the last half of century in many research groups, universities and hospitals around the globe. Despite numerous publications on each of these techniques, there is still a need of evaluating the numerical outputs of the different prognostic algorithms on identical medical datasets. Hence the need to benchmark the mathematical algorithms against the available medical and other cancer datasets held by various research groups from universities or hospitals. The main scope of a benchmark process is to specifically inform and to provide prognostic advice and choice of post-operative adjuvant therapy. It is also essential to define data acquisition protocols, to create the necessary databases on which the benchmarked methodologies can be tested. In case there are several research groups which are taking part in a benchmark study, then they form a multicentre study. This paper presents the steps to be followed in order to realize such a multicentre study.	algorithm;benchmark (computing);data acquisition;database;numerical analysis	Corneliu T. C. Arsene;Paulo J. G. Lisboa	2012	2012 IEEE Symposium on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)	10.1109/CIBCB.2012.6217222	computer science;machine learning;data mining;survival analysis;artificial neural network;statistics	Metrics	4.470146749228142	-74.46635530740447	130845
6108c586b09ebee917338ff5db8e5dfefec83c54	outbreak detection algorithms for seasonal disease data: a case study using ross river virus disease	seasonal adjustment;sensitivity and specificity;local government area;negative binomial;health informatics;disease surveillance;false negative;alphavirus infections;gold standard;seasons;information systems and communication service;population surveillance;seasonality;management of computing and information systems;ross river virus;detection algorithm;true positive;algorithms;humans;false positive;disease notification;australia;disease outbreaks	BACKGROUND Detection of outbreaks is an important part of disease surveillance. Although many algorithms have been designed for detecting outbreaks, few have been specifically assessed against diseases that have distinct seasonal incidence patterns, such as those caused by vector-borne pathogens.   METHODS We applied five previously reported outbreak detection algorithms to Ross River virus (RRV) disease data (1991-2007) for the four local government areas (LGAs) of Brisbane, Emerald, Redland and Townsville in Queensland, Australia. The methods used were the Early Aberration Reporting System (EARS) C1, C2 and C3 methods, negative binomial cusum (NBC), historical limits method (HLM), Poisson outbreak detection (POD) method and the purely temporal SaTScan analysis. Seasonally-adjusted variants of the NBC and SaTScan methods were developed. Some of the algorithms were applied using a range of parameter values, resulting in 17 variants of the five algorithms.   RESULTS The 9,188 RRV disease notifications that occurred in the four selected regions over the study period showed marked seasonality, which adversely affected the performance of some of the outbreak detection algorithms. Most of the methods examined were able to detect the same major events. The exception was the seasonally-adjusted NBC methods that detected an excess of short signals. The NBC, POD and temporal SaTScan algorithms were the only methods that consistently had high true positive rates and low false positive and false negative rates across the four study areas. The timeliness of outbreak signals generated by each method was also compared but there was no consistency across outbreaks and LGAs.   CONCLUSIONS This study has highlighted several issues associated with applying outbreak detection algorithms to seasonal disease data. In lieu of a true gold standard, a quantitative comparison is difficult and caution should be taken when interpreting the true positives, false positives, sensitivity and specificity.	disease notification;emerald;incidence matrix;local area networks;local government;novobiocin;offset binary;population parameter;redland rdf application framework;ross chicken;ross river virus;seasonality;sensitivity and specificity;sensor;tick-borne encephalitis;virus diseases;algorithm;hierarchical linear modeling	Anita M. Pelecanos;Peter A. Ryan;Michelle L. Gatton	2010		10.1186/1472-6947-10-74	health informatics;seasonal adjustment;medicine;pathology;type i and type ii errors;gold standard;negative binomial distribution;operations research;seasonality;statistics	ML	6.727306927168401	-72.6668521301211	131007
6660ba49c5faf55b71779dc3c5ce664a759c060a	semantic analysis on medical images: a case study	radiology;image processing;computer aided diagnosis;ontologies artificial intelligence;medical image analysis;radiology formal logic medical image processing ontologies artificial intelligence;machine agents;semantic information;medical image;medical image processing;image analysis biomedical imaging computer aided software engineering medical diagnostic imaging ontologies cancer humans vocabulary displays image processing;formal logic;radiologists;ontology semantic analysis computer aided diagnosis systems radiologists semantic information image processing machine agents description logic semantic medical image analysis;description logic;semantic medical image analysis;ontology;semantic analysis;computer aided diagnosis systems	Computer aided diagnosis (CAD) systems, as second-opinion assistance to radiologists, are being developed. However, the lack of semantic information at the image processing stage is expected to limit the ultimate performance of such CAD systems. A solution can be provided by a specific medical domain terminology, which contains several concepts that describe features of abnormalities and the relationships (properties) between these concepts; as such is well readable by both human beings (radiologists) and machine agents (CAD systems). Here we investigate a specific ontology and provide a clear evaluation based on string and description logic (DL). The results provide a clear understanding of semantic medical image analysis with which CAD systems are expected to improve their performance	computer-aided design;computer-aided software engineering;description logic;human-readable medium;image analysis;image processing;medical image computing;medical imaging;radiology	Da Qi;Erika R. E. Denton;Reyer Zwiggelaar	2006	18th International Conference on Pattern Recognition (ICPR'06)	10.1109/ICPR.2006.1042	natural language processing;computer vision;description logic;image processing;computer science;ontology;logic;information retrieval	Robotics	-0.06940151075166247	-77.59547847256827	131035
b8db947e58f214b45eca918438e0c69274d7c06a	using bayesian networks and rule-based trending to predict patient status in the intensive care unit	area under curve;roc curve;bayes theorem;artificial intelligence;algorithms	Multivariate Bayesian models trained with machine learning, in conjunction with rule-based time-series statistical techniques, are explored for the purpose of improving patient monitoring. Three vital sign data streams and known outcomes for 36 intensive care unit (ICU) patients were captured retrospectively and used to train a set of Bayesian net models and to construct time-series models. Models were validated on a reserved dataset from 16 additional patients. Receiver operating characteristic (ROC) curves were calculated. Area under the curve (AUC) was 91% for predicting improving outcome. The model's AUC for predicting declining outcome increased from 70% to 85% when the model was indexed to personalized baselines for each patient. The rule-based trending and alerting system was accurate 100% of the time in alerting a subsequent decline in condition. These techniques promise to improve the monitoring of ICU patients with high-sensitivity alerts, fewer false alarms, and earlier intervention.	alert brand of caffeine;area under curve;baseline (configuration management);bayesian network;ephrin type-b receptor 1, human;hypothalamic area, lateral;index;logic programming;machine learning;patients;personalization;receiver operating characteristic;receiver operator characteristics;silo (dataset);time series;vital signs;intensive care unit	Cindy Crump;Sunil Saxena;Bruce Wilson;Patrick Farrell;Azhar Rafiq;Christine Tsien Silvers	2009	AMIA ... Annual Symposium proceedings. AMIA Symposium		rule-based system;receiver operating characteristic;remote patient monitoring;data mining;bayesian network;bayes' theorem;multivariate statistics;bayesian probability;computer science;intensive care unit	ML	6.6985761092348595	-76.48520956834241	131225
22f7daf82d0256eb1f25472ab06a6c34eac15a55	dynamics of an age-structured two-strain model for malaria transmission	reproduction number;stability;strains;malaria	A new age-structured deterministic model for assessing the impact of anti-malaria drugs on the transmission dynamics of malaria is designed and qualitatively analysed. The resulting two-strain age-structured model undergoes backward bifurcation, which arises due to malaria-induced mortality in humans. Conditions for the existence of unique resistant strain-only and low-endemicity equilibria are derived for special cases. It is shown, for the case when treatment does not cause drug resistance, that the disease-free equilibrium of the wild strain-only component of the model is globally-asymptotically stable whenever the associated reproduction number of the model is less than unity. Similar result is established for the resistant strain-only component of the model for this case. Numerical simulations of the model, for the case when treatment does not cause drug resistance, show that the model undergoes competitive exclusion (where the malaria strain with the higher reproduction number drives the other to extinction).		Farinaz Forouzannia;Abba B. Gumel	2015	Applied Mathematics and Computation	10.1016/j.amc.2014.09.117	stability;mathematics;statistics	Theory	8.538299326340676	-67.94664515467687	131507
00fb804b127bbbad108a8ec2e27311396489a596	how much is a health insurer willing to pay for colorectal cancer screening tests?	additional life year;computer simulation model;analytical model;crc screening test utilization;health insurer;crc screening test;colorectal cancer;recent study;colorectal cancer screening test;asymmetric information;computer simulation;mean squared error;simulation model;cancer;simulation;health care;cost effectiveness	Colorectal Cancer (CRC) screening tests have proven to be cost-effective in preventing cancer incidence. Yet, as recent studies have shown, CRC screening tests are noticeably underutilized. Among the factors influencing CRC screening test utilization, the role of health insurers has gained considerable attention in recent studies. In this paper, we propose an analytical model for the market of CRC screening tests and show how the insurer can benefit from a computer simulation model to cope with the problem of incomplete and asymmetric information inherent in this market. Our estimates reveal that promoting CRC screening tests is not necessarily economically attractive to the insurer, unless the insurer's valuation of life is greater than a certain limit. We use the proposed model to estimate such a threshold - the insurer's willingness-to-pay to acquire one additional life year by covering the CRC screening tests.	computer simulation;cyclic redundancy check;incidence matrix;stand up to cancer;value (ethics);virtual screening	Reza Yaesoubi;Stephen D. Roberts	2008	2008 Winter Simulation Conference			AI	7.6598350384791525	-74.62634026150857	131666
9f125b5e8733f3fdf1d3465b74931ed995044719	behavioral informatics: dynamical models for measuring and assessing behaviors for precision interventions	fats;biological system modeling;computational modeling;hidden markov models;mathematical model;informatics;data models	Poor health-related behaviors represent a major challenge to healthcare due to their significant impact on chronic and acute diseases and their effect on the quality of life. Recent advances in technology have enabled an unprecedented opportunity to assess objectively, unobtrusively and continuously human behavior and have opened the possibility of optimizing individual-tailored, precision interventions within the framework of behavioral informatics. A key prerequisite for this optimization is the ability to assess and predict effects of interventions. This is potentially achievable with computational models of behavior and behavior change. In this paper we describe various approaches to computational modeling and describe a new hybrid model based on a dual process theoretical framework for behavior change. The model leverages cognitive learning theories and is shown to be consistent with mobile intervention data. We also illustrate how system-theoretic approaches can be used to assess the effect of coaching and participants' health behaviors.	acute disease;computation;computational model;dual;dynamical system;health behavior;informatics (discipline);mathematical optimization;predictive modelling;the quality of life;theory	Misha Pavel;Holly Brügge Jimison;Bonnie Spring	2016	2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2016.7590672	data modeling;simulation;computer science;engineering;data science;machine learning;mathematical model;management science;informatics;computational model;hidden markov model;statistics	Robotics	8.953270191130347	-72.0303301491447	131787
e13bac97b46ebc18c15cfb47f6d20668c043ec6c	discover high-risk factor combinations using bayesian network from national screening data in china		Stroke, characterized by high incidence, high prevalence and high mortality, has a serious impact on the health of the residents, and brings heavy burden to the family and the society in China. It is proved that early screening is an important and effective method in stroke prevention and control. Based on the 2012 screening data from China stroke screening and intervention program, we manually define a network and use Bayesian network model to study the network parameter and stroke probability with risk factors in order to explore the rationality of the screening standard proposed by the program mentioned above. We find that it is not reasonable enough to simply consider those with three risk factors as of high-risk for stroke without distinguishing the types and combinations of risk factors. Some two-factors combinations are with higher stroke probability than three-factors combinations. Therefore, in the screening, some new criteria should be added to avoid missing person with high risk of stroke, and a threshold is needed to remove low-risk combinations in order to save economic costs.	bayesian network;effective method;incidence matrix;network model;rationality;screening effect	Xuemeng Li;Jinghui Yu;Mei Li;Dongsheng Zhao	2017	2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2017.8217801	artificial intelligence;risk analysis (engineering);economic cost;machine learning;computer science;china;risk factor;bayesian network;rationality;effective method;stroke	Visualization	7.882776561092631	-74.87582551721191	132035
978d3c21e125af04c93d5b225a0673c5403dca9c	predicting yeast gene function based on hidden markov models	hidden markov model;gene expression;function prediction;gene function	The prediction of function classes for unannotated genes or Open Reading Frames (ORFs) is important for understanding the function role of ge nes and gene networks. Existing data mining tools, such as Support Vector Machines (SVMs) and K-Nearest Neighbors (KNNs), can only achieve about 40% precision. We developed a gene function prediction ool based on profile Hidden Markov Models (HMMs). HMMs have shown great successes in modeling noisy sequential data sets in speech recognition and prot ein sequence profiling. Results from contingency test s howed significant Markov dependency in time-series expres sion data, and therefore HMMs would be especially appropriate for modeling gene expressions. Each fun ction class is associated with a distinct HMM whose param eters are trained using yeast time-series gene expression data. The function annotations of the HMM training set we re obtained from the Munich Information Centre for Pro tein Sequences (MIPS) data base. We designed two structu ral variants of HMMs (chain HMM, split HMM) and tested each of them on 40 function classes. The highest ov erall prediction precision achieved was 67% using doublesp it HMM with n-fold cross-validation. We also attempted to generalize HMMs to Dynamic Bayesian Networks (DBNs) for gene function prediction using heterogen eous data sets.	cross-validation (statistics);data mining;database;dynamic bayesian network;gene regulatory network;hidden markov model;k-nearest neighbors algorithm;markov chain;open reading frame;param;sion's minimax theorem;speech recognition;support vector machine;test set;time series;uniprot	Xutao Deng;Huimin Geng;Hesham H. Ali	2005			gene expression;computer science;bioinformatics;machine learning;pattern recognition;hidden markov model;variable-order markov model	ML	-3.786759292417854	-71.21676559533238	132246
1330fdf453afc58a801221c99dcf717babe33c96	classification tree for risk assessment in patients suffering from congestive heart failure via long-term heart rate variability	heart rate variability hrv congestive heart failure chf data mining decision tree;risk assessment classification tree specificity depressed hrv classification tree sensitivity cart method classification and regression tree method satisfactory signal quality normal to normal interval total heartbeat interval classification nyha iv classification nyha iii classification severe chf nyha ii classification nyha i classification mild chf data analysis public holter database retrospective analysis nyha classification new york heart association classification hrv measurement higher risk patient lower risk patient automatic classifier long term heart rate variability congestive heart failure patient;cardiology;data analysis;signal classification cardiology data analysis diseases medical signal processing regression analysis;heart rate variability standards accuracy databases sensitivity decision trees;signal classification;diseases;regression analysis;medical signal processing	This study aims to develop an automatic classifier for risk assessment in patients suffering from congestive heart failure (CHF). The proposed classifier separates lower risk patients from higher risk ones, using standard long-term heart rate variability (HRV) measures. Patients are labeled as lower or higher risk according to the New York Heart Association classification (NYHA). A retrospective analysis on two public Holter databases was performed, analyzing the data of 12 patients suffering from mild CHF (NYHA I and II), labeled as lower risk, and 32 suffering from severe CHF (NYHA III and IV), labeled as higher risk. Only patients with a fraction of total heartbeats intervals (RR) classified as normal-to-normal (NN) intervals (NN/RR) higher than 80% were selected as eligible in order to have a satisfactory signal quality. Classification and regression tree (CART) was employed to develop the classifiers. A total of 30 higher risk and 11 lower risk patients were included in the analysis. The proposed classification trees achieved a sensitivity and a specificity rate of 93.3% and 63.6%, respectively, in identifying higher risk patients. Finally, the rules obtained by CART are comprehensible and consistent with the consensus showed by previous studies that depressed HRV is a useful tool for risk assessment in patients suffering from CHF.	classification;congestive heart failure;decision tree learning;depressive disorder;eighty;heart rate variability;patients;published database;rom cartridge;rapid refresh;risk assessment;rule (guideline);sensitivity and specificity;tracer;trees (plant)	Paolo Melillo;Nicola De Luca;Marcello Bracale;Leandro Pecchia	2013	IEEE Journal of Biomedical and Health Informatics	10.1109/JBHI.2013.2244902	medicine;computer science;artificial intelligence;data mining;medical emergency;data analysis;regression analysis;statistics	ML	7.306816137442112	-76.3168933864816	132573
c55f19b96efcc92e5013801684271dd72d520267	web-tool to support medical experts in probabilistic modelling using large bayesian networks with an example of hinosinusitis		For many complex diseases, finding the best patient-specific treatment decision is difficult for physicians due to limited mental capacity. Clinical decision support systems based on Bayesian networks (BN) can provide a probabilistic graphical model integrating all necessary aspects relevant for decision making. Such models are often manually created by clinical experts. The modeling process consists of graphical modeling conducted by collecting of information entities, and probabilistic modeling achieved through defining the relations of information entities to their direct causes. Such expert-based probabilistic modelling with BNs is very time intensive and requires knowledge about the underlying modeling method. We introduce in this paper an intuitive web-based system for helping medical experts generate decision models based on BNs. Using the tool, no special knowledge about the underlying model or BN is necessary. We tested the tool with an example of modeling treatment decisions of Rhinosinusitis and studied its usability.		Mario A. Cypko;David Hirsch;Lucas Koch;Matthaeus Stoehr;Gero Strauß;Kerstin Denecke	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-259	clinical decision support system;data mining;probabilistic logic;usability;decision model;bayesian network;influence diagram;text mining;graphical model;computer science	AI	1.4285174065577364	-76.15408167059954	132863
401d3ec966761febb97e1f248f87cc83fc0ad908	comparison of robustness against missing values of alternative decision tree and multiple logistic regression for predicting clinical data in primary breast cancer	patient diagnosis;cancer;missing value robustness alternative decision tree multiple logistic regression breast cancer pathological complete response neoadjuvant therapy ensembled adtree models decision making;breast cancer predictive models robustness boosting data models decision trees;regression analysis cancer decision making decision trees medical diagnostic computing patient diagnosis patient treatment;patient treatment;regression analysis;decision trees;medical diagnostic computing	Nomogram based on multiple logistic regression (MLR) is a standard technique for predicting diagnostic and treatment outcomes in medical fields. However, the applicability of MLR to data mining of clinical information is limited. To overcome these issues, we have developed prediction models using ensembles of alternative decision trees (ADTree). Here, we compare the performance of MLR and ADTree models in terms of robustness against missing values. As a case study, we employ datasets including pathological complete response (pCR) of neoadjuvant therapy, one of the most important decision-making factors in the diagnosis and treatment of primary breast cancer. Ensembled ADTree models are more robust against missing values than MLR. Sufficient robustness is attained at low boosting and ensemble number, and is compromised as these numbers increase.	alternating decision tree;data mining;decision making;electroconvulsive therapy;learning to rank;logistic regression;mammary neoplasms;missing data;multiple congenital anomalies;neoadjuvant therapy;nomogram;polymerase chain reaction;trees (plant)	Masahiro Sugimoto;Masahiro Takada;Masakazu Toi	2013	2013 35th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2013.6610185	econometrics;medicine;computer science;machine learning;decision tree;data mining;regression analysis;statistics;cancer	ML	6.268746491658572	-76.22756397570465	132928
1ae74dcb042d72d0bb323f0aa28d77f2261cdc16	prediction of breast cancer using artificial neural networks	breast cancer prediction;bi rads;breast cancer;artificial neural network	In this study, an artificial neural network (ANN) was developed to determine whether patients have breast cancer or not. Whether patients have cancer or not and if they have its type can be determined by using ANN and BI-RADS evaluation and based on the age of the patient, mass shape, mass border and mass density. Though this system cannot diagnose cancer conclusively, it helps physicians in deciding whether a biopsy is required by providing information about whether the patient has breast cancer or not. Data obtained from 800 patients who were diagnosed with cancer definitively through biopsy. The definitive diagnosis corresponding to each patient and the data from ANN model results were investigated using Confusion matrix and ROC analyses. In the test data of the ANN model that was implemented as a result of these analyses, disease prediction rate was 90.5% and the health ratio was 80.9%. It is seen from these high predictive values that the ANN model is fast, reliable and without any risks and therefore can be of great help to physicians.	artificial neural network;bi-rads;breast carcinoma;confusion matrix;malignant neoplasm of breast;mammary neoplasms;mass density;patients;receiver operator characteristics;test data;wavelet analysis	Ismail Saritas	2011	Journal of Medical Systems	10.1007/s10916-011-9768-0	medicine;pathology;computer science;gynecology;breast cancer;machine learning;artificial neural network;surgery	ML	7.098414980030471	-76.68427246251568	132963
3c17ab5d01bdabfd883f1d6d5b4490b12ca380fb	shrink: a breast cancer risk assessment model based on medical social network		Breast cancer risk assessment model can assess whether a people is at a high risk of developing breast cancer disease or not and confirm a breast cancer high-risk group. Because the etiology of breast cancer disease is different in different country and region, the existing risk assessment model is only adaptive to certain countries and regions. And the parameters of these models are fixed, so these models have poor generality. Aiming at these problems, the paper puts forward a new breast cancer risk assessment model named as Shrink. Using the idea of social network, Shrink constructs a medical social network to show the similarity among people, and uses group division algorithm to divide the network into breast cancer high-risk group and low-risk group. The parameters of this model can be set according to the needs of the breast census, and these parameters can be directly acquired through questionnaire, therefore Shrink has good generality. Moreover, under the uncertain classification standard, Shrink adopts a new classification method to discover breast cancer high-risk group. Based on the real data from questionnaires, we make experiments in Matlab, and obtain the evaluation index of the model. The experiment proves that the model itself has good evaluation result and is better than classic Gail model.	chinese room;division algorithm;experiment;matlab;mathematical model;risk assessment;social network	Ali Li;Rui Wang;Lei Xu	2017	2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)	10.1109/ICDCS.2017.168	risk management;breast cancer;data mining;computer science;generality;risk assessment;disease;gail model;social network	DB	4.0423917515954955	-76.05672013784323	133370
d16066a707ccd724877628d3458c098d7b271ee9	unusual presentation of primary hyperparathyroidism: report of three cases	hyperparathyroidism primary;diagnostic imaging;male;treatment outcome;diagnosis differential;imaging radiology;adult;parathyroid neoplasms;humans	BACKGROUND Primary hyperparathyroidism is an endocrinopathic condition characterized by hypersecretion of parathyroid hormone. Excess parathyroid hormone results in an altered state of osseous metabolism involving bone resorption and tissue change known as osteitis fibrosa cystica, which is the end stage of primary hyperparathyroidism. Osteitis fibrosa cystica is associated with the development of brown tumors, which are rare because hyperparathyroidism is now usually diagnosed and treated before symptoms develop. Brown tumors are rarely the first symptom of hyperparathyroidism and can occasionally be mistaken for malignancy.   CASE PRESENTATION We herein report three cases of primary hyperparathyroidism with an unusual presentation of brown tumors. All three patients were Asian. In the first case, a 42-year-old man was admitted with a mass mimicking a malignant bone neoplasm in the right mandible as the first manifestation of primary hyperparathyroidism. The second case involved a 25-year-old man admitted with a fracture of his right femur. The third case involved a 43-year-old man with multiple brown tumors in both lower limbs. All three patients underwent successful parathyroidectomy for parathyroid adenomas; one case was complicated by a papillary thyroid carcinoma.   CONCLUSION Complete evaluation of the medical history and biochemical and radiographic findings is necessary to achieve a correct diagnosis and avoid unnecessary bone resections in patients with primary hyperparathyroidism.	addison disease;alveolar periostitis;bone marrow diseases;bone resorption;bone tissue;bone neoplasms;brown-sequard syndrome;hepatic capsule structure;hospital admission;hyperparathyroidism;hyperparathyroidism, primary;limb structure;malignant bone neoplasm;mandible;multiple endocrine neoplasia;offset binary;osteitis fibrosa cystica;papillary thyroid carcinoma;parathyroid adenoma;parathyroidectomy;patients;radiography	Ruibin Huang;Ruyao Zhuang;Yuan Liu;Tianti Li;Jiexiong Huang	2015		10.1186/s12880-015-0064-1	medical imaging;endocrinology;radiology;medicine;pathology;surgery	Web+IR	8.057013297594487	-70.8434884786613	133515
cb4d5e38985b85ba886f7a1b9557af9c63237f99	evolving biochemical reaction networks with stochastic attributes	modeling technique;standard deviation;stochastic simulation;biochemical network;genetic algorithm;genetic algorithms;biochemical reaction network;intracellular signalling;steady state	Biochemical networks display a wide range of behaviors. While many of these networks tend to operate in a steady-state regime, others exhibit distinctly stochastic behaviors. Fitting models to data from these systems challenges many of the linear and steady-state assumptions of typical modeling techniques. The genetic algorithm described herein seeks to generate networks which exhibit desired average/steady-state behaviors while minimizing or maximizing the standard deviation of those behaviors.	curve fitting;genetic algorithm;steady state	Thomas R. Kiehl	2009		10.1145/1570256.1570277	econometrics;mathematical optimization;genetic algorithm;computer science;machine learning;mathematics;statistics	ML	9.642802710892997	-67.52836481414506	133581
f69a41afe02ed262a2088890652cc155d8ffdb70	machine learning approaches and web-based system to the application of disease modifying therapy for sickle cell		Sickle cell disease (SCD) is a common serious genetic disease, which has a severe impact due to red blood cell (RBCs) abnormality. According to the World Health Organisation, 7 million newborn babies each year suffer either from the congenital anomaly or from an inherited disease, primarily from thalassemia and sickle cell disease. In the case of SCD, recent research has shown the beneficial effects of a drug called hydroxyurea/hydroxycarbamide in modifying the disease phenotype. The clinical management of this disease-modifying therapy is difficult and time consuming for clinical staff. This includes finding an optimal classifier that can help to solve the issues with missing values, multi-class datasets, and features selection. For the classification and discriminant analysis of SCD datasets, 7 classifiers based on machine learning models are selected representing linear and non-linear methods. After running these classifiers with a single model, the results revealed that a single classifier has provided us with effective outcomes in terms of the classification performance evaluation metric. In order to produce such an optimal outcome, this research proposed and designed combined classifiers (ensemble classifiers) among the neural network’s models, the random forest classifier, and the K-nearest neighbour classifier. In this aspect, combining the levenberg-marquardt algorithm, the voted perceptron classifier, the radial basis neural classifier, and random forest classifier obtain the highest rate of performance and accuracy. This ensemble classifier receives better results during the training set and testing set process. Recent technology advances based on smart devices have improved the medical facilities and become increasingly popular in association with real-time health monitoring and remote/personal health-care. The web-based system developed under the supervision of the haematology specialist at the Alder Hey Children’s Hospital in order to produce such an effective and useful system for both patients and clinicians. To sum up, the simulation experiment concludes that using machine learning and the web-based system platforms represents an alternative procedure that could assist healthcare professionals, particularly for the specialist nurse and junior doctor to improve the quality of care with sickle cell disorder.		Mohammed Khalaf	2018			perceptron;missing data;web application;artificial neural network;health care;machine learning;linear discriminant analysis;classifier (linguistics);random forest;artificial intelligence;computer science	ML	6.603176345114628	-77.2809779239335	133856
4483bc95162237f2d87f4709e137bf93b1399cff	validation of gene regulatory networks: scientific and inferential	genomique;genomics;validacion;systems biology;genomica;gen;gene network;biologia de sistemas;epistemology;gene;reseau de regulation genetique;validation;biologie systemique;gene regulatory network	Gene regulatory network models are a major area of study in systems and computational biology and the construction of network models is among the most important problems in these disciplines. The critical epistemological issue concerns validation. Validity can be approached from two different perspectives (i) given a hypothesized network model, its scientific validity relates to the ability to make predictions from the model that can be checked against experimental observations; and (ii) the validity of a network inference procedure must be evaluated relative to its ability to infer a network from sample points generated by the network. This article examines both perspectives in the framework of a distance function between two networks. It considers some of the obstacles to validation and provides examples of both validation paradigms.	algorithm;centrality;checking (action);computation;computational biology;concordance (publishing);cross reactions;cross-validation (statistics);data validation;dynamical system;ephrin type-b receptor 1, human;gene regulatory network;inference;inferential programming;monte carlo method;network model;petrosal sinus sampling;rectifier (neural networks);sampling (signal processing);stochastic process;systems biology;trustworthy computing	Edward R. Dougherty	2011	Briefings in bioinformatics	10.1093/bib/bbq078	biology;gene regulatory network;genomics;bioinformatics;artificial intelligence;genetics;systems biology	ML	0.4906438726523763	-66.61010660598329	134003
eccaf28ee1fe053dacc10cdf6b97cf49661a32a3	mmhg: multi-modal hypergraph learning for overall survival after d2 gastrectomy for gastric cancer		Overall survival (OS) prediction has been a central topic of oncology. Most existing OS prediction models are based on traditional statistical methods which becomes a limitation when confronted with high dimensional dataset as well complicated internal relation among features in practice. In this paper, we weaken this limitation by exploring the application of multi-modal hypergraph (MMHG) learning framework to improve the accuracy of prediction. More specifically, the proposed hypergraph model unites the demographics, pathologic characteristics and physiological indicators simultaneously to predict the overall survival after D2 gastrectomy for gastric cancer. Experiments are carried out on a real data set of West China Hospital of Sichuan University with 939 patients to evaluate the proposed approach by comparison with random forest (RF) and support vector machine (SVM). Results demonstrate that our scheme outperforms the baseline methods in overall survival classification performance.	baseline (configuration management);modal logic;operating system;radio frequency;random forest;support vector machine	Fei Lu;Zhikui Chen;Xu Yuan;Qiu Li;Zedong Du;Li Luo;Fengyi Zhang	2017	2017 IEEE 15th Intl Conf on Dependable, Autonomic and Secure Computing, 15th Intl Conf on Pervasive Intelligence and Computing, 3rd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)	10.1109/DASC-PICom-DataCom-CyberSciTec.2017.40	support vector machine;predictive modelling;machine learning;hypergraph;modal;cancer;gastrectomy;correlation;random forest;artificial intelligence;computer science	Web+IR	7.443876225655147	-77.60509035484347	134690
4e1fe8e5763f70a0d781ceab3881b732552a3eaa	feature selection on handwriting biometrics: security aspects of artificial forgeries	dynamic handwriting;security analysis;biometrics;feature selection;reverse engineering	A lot of improvements were introduced lately in or de to increase the verification performance of biometric user authenti cation systems. One method, besides many others, is the selection of specific f eatures for each user during the verification process. In this paper we present a security analysis of a user specific bit mask vector, which was originally intr oduced to improve verification performance on a Biometric Hash algorithm for d ynamic handwriting. Therefore, we use a reverse engineering attack meth od to generate artificial handwriting data and calculate error rates to exami ne the impact on the verification performance. Our goal is to study the effect o f a feature selection by a mask vector on artificial data in comparison to gen uine handwriting data. Our first experimental results show an average decrease of the equal error rate, generate by the artificial data, by approx. 64%. In comparison, equal error rates of random attacks, using verification data of another user, decreases by an average	algorithm;approximation;biometrics;bit error rate;feature selection;handwriting recognition;hash function;mask (computing);reverse engineering	Karl Kümmel;Tobias Scheidat;Claus Vielhauer;Jana Dittmann	2012		10.1007/978-3-642-32805-3_2	speech recognition;computer science;pattern recognition;data mining;security analysis;feature selection;biometrics;reverse engineering	Security	-1.2417335217796743	-72.99962849159552	134886
944af2fd787666a28b733076aa78cae50b66e22d	personalised modelling on integrated clinical and eeg spatio-temporal brain data in the neucube spiking neural network system	data models electroencephalography neurons brain models computational modeling unsupervised learning;classification integrated clinical data eeg spatio temporal brain data neucube spiking neural network system personalised modelling framework person clinical static data stbd personalised spiking neural network neucube snn architecture psnn models;spatiotemporal data;neucube;pattern classification brain electroencephalography medical computing neural nets;personalised modelling;methadone maintenance treatment personalised modelling spiking neural networks neucube spatiotemporal data eeg data opiate addict;spiking neural networks;opiate addict;eeg data	This paper introduces a novel personalised modelling framework and system for analysing Spatio-Temporal Brain Data (STBD) along with person clinical static data. For every individual, based on selected subset of similar to this individual clinical data, a subset of STBD is used for training a personalised Spiking Neural Network (PSNN) model using the recently proposed NeuCube SNN architecture. The proposed method is illustrated on a case study of personalised modelling using clinical and EEG data of two groups of subjects - drug addicts and addicts under medication. The PSNN models help to achieve a better classification accuracy compared to global SNN models or when using traditional AI methods. A PSNN model visualisation enables discovery of new knowledge about individual persons and to distinguish complex STBD across subjects.	artificial neural network;electroencephalography;spiking neural network	Maryam Gholami Doborjeh;Nikola K. Kasabov	2016	2016 International Joint Conference on Neural Networks (IJCNN)	10.1109/IJCNN.2016.7727358	computer science;artificial intelligence;machine learning;data mining;spiking neural network	AI	4.05123817566792	-77.0652150103998	135969
45f026f726ef0969c1cea28819ae82e6867d045b	a study on the dynamic signature verification system	user interface;characteristic vector;dynamic signature verification;evaluation	This paper is a research on the dynamic signature verification of error rate which are false rejection rate and false acceptance rate, the size of signature verification engine, the size of the characteristic vectors of a signature, the ability to distinguish similar signatures, the processing speed and so on. Also, we present our efficient user interface and performance results.		Jin-Whan Kim;Hyuk-Gyu Cho;Eui-Young Cha	2004	Int. J. Fuzzy Logic and Intelligent Systems	10.5391/IJFIS.2004.4.3.271	real-time computing;computer science;theoretical computer science;world wide web;signature recognition	AI	-1.3037127460650235	-73.04417928848774	136039
2cba1b0507e7392c90e59025a1f375648779f1b3	neuralization of mouse embryonic stem cells in alginate hydrogels under retinoic acid and sag treatment	spinal cord regeneration neuralization mouse embryonic stem cells alginate hydrogels retinoic acid sag treatment cgr8 cell line smoothened agonist treatment embryoid body formation stage dissociation seeding glass coverslips immunofluorescent labelling nestin neuronal marker b tubulin iii neuronal marker map2 neuronal marker cryosections;endnotes;tissue engineering cellular biophysics hydrogels neurophysiology;neurons electron tubes media mice glass;pubications	This paper examines the differentiation of a mouse embryonic stem cell line (CGR8) into neurons, under retinoic acid (RA) and smoothened agonist (SAG) treatment. When stem cells underwent through an embryoid body (EB) formation stage, dissociation and seeding on glass coverslips, immunofluorescent labelling for neuronal markers (Nestin, b-Tubulin III, MAP2) revealed the presence of both immature neural progenitors and mature neurons. Undifferentiated CGR8 were also encapsulated in tubular, alginate-gelatin hydrogels and incubated in differentiation media containing retinoic acid (RA) and smoothened agonist (SAG). Cryo-sections of the hydrogel tubes were positive for Nestin, Pax6 and b-Tubulin III, verifying the presence of neurons and neural progenitors. Provided neural induction can be more precisely directed in the tubular hydrogels, these scaffolds will become a powerful model of neural tube development in embryos and will highlight potential strategies for spinal cord regeneration.	acid;artificial neuron;cellular phone;es cell line;embryo;embryoid bodies;embryonic stem cells;encapsulation (networking);gelatin;hydrogel;hydrogels;mass effect trilogy;motor neurons;natural regeneration;neural oscillation;neuron;neurulation;polymer;polymerase chain reaction;polymers;smo protein, human;sql access group;seeding;serotonin agonists;specimen source codes - tube;spinal cord injuries;spinal cord neoplasms;spinal cord regeneration;staining method;suspensions;transcription factor;traffic collision avoidance system;transcription (software);verification and validation;verifying specimen;incubated;neural tube development;tube formation	Evangelos Delivopoulos;Kevin M. Shakesheff;Heather Peto	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7319153	biology;biochemistry;molecular biology;anatomy	Robotics	6.047553342218832	-66.28505129615503	136763
9ffef43c60ae0edaa5a749fbbe3426a661f45724	prediction-constrained topic models for antidepressant recommendation		Supervisory signals can help topic models discover low-dimensional data representations that are more interpretable for clinical tasks. We propose a framework for training supervised latent Dirichlet allocation that balances two goals: faithful generative explanations of high-dimensional data and accurate prediction of associated class labels. Existing approaches fail to balance these goals by not properly handling a fundamental asymmetry: the intended task is always predicting labels from data, not data from labels. Our new prediction-constrained objective trains models that predict labels from heldout data well while also producing good generative likelihoods and interpretable topic-word parameters. In a case study on predicting depression medications from electronic health records, we demonstrate improved recommendations compared to previous supervised topic models and high-dimensional logistic regression from words alone.	latent dirichlet allocation;logistic regression;supervised learning;topic model	Michael C. Hughes;Gabriel Hope;Leah Weiner;Thomas H. McCoy;Roy H. Perlis;Erik B. Sudderth;Finale Doshi-Velez	2017	CoRR		machine learning;artificial intelligence;latent dirichlet allocation;generative grammar;topic model;logistic regression;mathematics	ML	4.043910162039597	-74.11586667286511	136853
f6bd2b7889f0d8c9d3930b93453e457b1156b71b	mining electronic health records for adverse drug effects using regression based methods	drug effects;adverse drug event;confounding;electronic health record;large scale;regression;adverse drug events;subject matter expert;statistical techniques;electronic health records;health care	The identification of post-marketed adverse drug events (ADEs) is paramount to health care. Spontaneous reporting systems (SRS) are currently the mainstay in pharmacovigilance. Recently, electronic health records (EHRs) have emerged as a promising and effective complementary resource to SRS, as they contain a more complete record of the patient, and do not suffer from the reporting biases inherent to SRS. However, mining EHRs for potential ADEs, which typically involves identification of statistical associations between drugs and medical conditions, introduced several other challenges, the main one being the necessity for statistical techniques that account for confounding. The objective of this paper is to present and demonstrate the feasibility of a method based on regression techniques, which is designed for automated large scale mining of ADEs in EHR narratives. To the best of our knowledge this is a first of its kind approach that combines both the use of EHR data, and regression based methods in order to address confounding and identify potential ADEs. Two separate experiments are conducted. The results, which are validated by clinical subject matter experts, demonstrate great promise, as well as highlight additional challenges.	experiment;spontaneous order;subject matter expert turing test	Rave Harpaz;Krystl Haerian;Herbert S. Chase;Carol Friedman	2010		10.1145/1882992.1883008	alternative medicine;medicine;data mining;biological engineering	ML	4.249982687276904	-74.03031448607895	137174
722db40e62019a0318110f4520eba88dcfa1ca47	depression diagnosis based on ontologies and bayesian networks	belief networks;ontologies artificial intelligence belief networks inference mechanisms medical diagnostic computing multi agent systems;inference mechanisms;agent based platform depression diagnosis ontologies bayesian networks disease life quality technology development inference model depression terminology depression probability;ontologies artificial intelligence;multi agent systems;ontologies bayes methods diseases uncertainty owl fatigue;knowledge discovery depression diagnosis ontology bayesian networks;diagnosis;medical diagnostic computing;ontology;depression;bayesian networks;knowledge discovery	Recently, depression become a general disease in the world due to the promotion of life quality and technology development. Most of people are not aware of the possibility of getting depressed himself in daily life. To accurately diagnose getting depressed becomes an important issue. In this paper, we utilize ontologies and Bayesian networks techniques to build the inference model for inferring the possibility of depression. We propose an ontology model to build the terminology of depression and utilize the Bayesian networks to infer the probability of depression. In addition, the paper also proposes an agent-based platform and addresses the implementation issue. The result shows that it can be well-inferring in the depression diagnosis.	agent-based model;anytime algorithm;bayesian network;jade;mobile device;national supercomputer centre in sweden;ontology (information science);yang	Yue-Shan Chang;Wan-Chun Hung;Tong-Ying Tony Juang	2013	2013 IEEE International Conference on Systems, Man, and Cybernetics	10.1109/SMC.2013.589	computer science;artificial intelligence;data science;machine learning;ontology;medical diagnosis;bayesian network;data mining	Robotics	2.0836549238092745	-78.750259595853	137742
8116794745dad1a4ef86b41dd64cb976dae1b557	automatic energy expenditure measurement for health science	energy prediction;human energy expenditure;machine learning	BACKGROUND AND OBJECTIVE It is crucial to predict the human energy expenditure in any sports activity and health science application accurately to investigate the impact of the activity. However, measurement of the real energy expenditure is not a trivial task and involves complex steps. The objective of this work is to improve the performance of existing estimation models of energy expenditure by using machine learning algorithms and several data from different sensors and provide this estimation service in a cloud-based platform.   METHODS In this study, we used input data such as breathe rate, and hearth rate from three sensors. Inputs are received from a web form and sent to the web service which applies a regression model on Azure cloud platform. During the experiments, we assessed several machine learning models based on regression methods.   RESULTS Our experimental results showed that our novel model which applies Boosted Decision Tree Regression in conjunction with the median aggregation technique provides the best result among other five regression algorithms.   CONCLUSIONS This cloud-based energy expenditure system which uses a web service showed that cloud computing technology is a great opportunity to develop estimation systems and the new model which applies Boosted Decision Tree Regression with the median aggregation provides remarkable results.		Cagatay Catal;Akhan Akbulut	2018	Computer methods and programs in biomedicine	10.1016/j.cmpb.2018.01.015	gradient boosting;regression analysis;statistics;web service;data mining;cloud computing;computer science	Web+IR	6.121640226637344	-76.05096200284305	138199
d23d61fb774471951650f271dc458bbfe46ea0e8	algorithmic information theory may explain the pathogenic number of dna repeats in myotonic dystrophy type 1 (and in similar diseases)	myotonic dystrophy;non coding region;muscular dystrophy;g protein;algorithmic information theory;disease severity;adult onset	Myotonic Dystrophy Type 1 (DM1), the most common form of adult-onset muscular dystrophy, is caused by an abnormal number of repeats of the trinucleotide sequence CTG outside of the protein-coding part of DNA -- more than fifty repeats can cause disease. Several other diseases are caused by similar repeats in non-coding regions; in most of these diseases, up to approximately fifty repeats is normal, while pathogenic cases usually have > 50 repetitions. The fact that this level of approximately fifty repeats can be seen in many diseases, with different repeating sequences, indicates that there may be a fundamental explanation for this threshold.  In this paper, we conjecture that such an explanation may come from Algorithmic Information Theory. Crudely speaking, this threshold can be viewed as a measure of the redundancy in healthy DNA; this indirect estimate of the measure of redundancy is in good accordance with a more direct estimate { based on the fact that approximately 1=50 of DNA is directly functionally useful (e.g., protein-coding).	algorithmic information theory;chomsky hierarchy;onset (audio);redundancy (engineering)	Misha Koshelev;Luc Longpré	2010	SIGACT News	10.1145/1907450.1907534	computer science;g protein;mathematics;algorithmic information theory	Theory	8.175329351175078	-70.94805987326683	138360
1d388a830f1cf1f99c47927984053fb0f6c1b88d	reinforcement learning for closed-loop propofol anesthesia: a human volunteer study	human volunteer study;reinforcement learning;bispectral index;closed loop control of anesthesia;bis;propofol	Research has demonstrated the efficacy of closed-loop control of anesthesia using the bispectral index (BIS) of the electroencephalogram as the controlled variable, and the development of model-based, patient-adaptive systems has considerably improved anesthetic control. To further explore the use of model-based control in anesthesia, we investigated the application of reinforcement learning (RL) in the delivery of patient-specific, propofol-induced hypnosis in human volunteers. When compared to published performance metrics, RL control demonstrated accuracy and stability, indicating that further, more rigorous clinical study is warranted. When compared to population-based dosing, patientspecific drug administration is generally preferred in the clinical practice of anesthesia. Computer-controlled drug delivery systems have been investigated as a means of achieving patient-specific anesthesia, and their use is associated with a number of favorable patient outcomes, including decreased intraoperative drug consumption and shortened postoperative recovery times (Liu et al. 2006; Servin 1998; Theil et al. 1993). Historically, the application of proportional-integral-derivative (PID) control in closedloop anesthesia has demonstrated moderate success (Absalom and Kenny 2003). However, success has been constrained by limitations in the control method, as well as the complexity of human physiology (Wood 1989). To improve control performance, clinical study has broadened to include techniques commonly associated with intelligent systems, most notably fuzzy control (De Smet et al. 2008; Esmaeili et al. 2008; Carregal et al. 2000; Schaublin et al. 1996). Reinforcement learning (RL), another intelligent systems technique, has demonstrated proficiency in difficult robotic control tasks (Gullapalli 1993); however, RL has no reported application to clinical control problems, with the exception of work relating to this study (Moore et al. 2004). Despite the lack of clinical application, reinforcement learning is not far removed from medicine since its fundamental principles (dynamic programming and value function optimization) have been applied to depth of anesthesia control with favorable results (Hu, Lovejoy, and Shafer 1994). Copyright c © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. Because RL’s aptitude for specialized control tasks remains incompletely explored, the objective of this study was to investigate the application of reinforcement learning to closed-loop control of intravenous propofol anesthesia in healthy human volunteers. Accordingly, an RL agent was developed, tested in silico, and then evaluated in volunteers under an IRB-approved study protocol in the Department of Anesthesia at Stanford University School of Medicine.	asea irb;adaptive system;artificial intelligence;bellman equation;control theory;dynamic programming;electroencephalography;fuzzy control system;holographic principle;mathematical optimization;pid;population;public-key cryptography;reinforcement learning;robot;simulation;theil index;word lists by frequency;aptitude	Brett L. Moore;Periklis Panousis;Vivekanand Kulkarni;Larry D. Pyeatt;Anthony G. Doufas	2010			computer science;artificial intelligence;machine learning;reinforcement learning	AI	8.940409243096552	-73.0778833558918	138783
3f91bfffddff5e8543a41a8d36be45ed37a8163f	syndromic surveillance for health information system failures: a feasibility study		OBJECTIVE To explore the applicability of a syndromic surveillance method to the early detection of health information technology (HIT) system failures.   METHODS A syndromic surveillance system was developed to monitor a laboratory information system at a tertiary hospital. Four indices were monitored: (1) total laboratory records being created; (2) total records with missing results; (3) average serum potassium results; and (4) total duplicated tests on a patient. The goal was to detect HIT system failures causing: data loss at the record level; data loss at the field level; erroneous data; and unintended duplication of data. Time-series models of the indices were constructed, and statistical process control charts were used to detect unexpected behaviors. The ability of the models to detect HIT system failures was evaluated using simulated failures, each lasting for 24 h, with error rates ranging from 1% to 35%.   RESULTS In detecting data loss at the record level, the model achieved a sensitivity of 0.26 when the simulated error rate was 1%, while maintaining a specificity of 0.98. Detection performance improved with increasing error rates, achieving a perfect sensitivity when the error rate was 35%. In the detection of missing results, erroneous serum potassium results and unintended repetition of tests, perfect sensitivity was attained when the error rate was as small as 5%. Decreasing the error rate to 1% resulted in a drop in sensitivity to 0.65-0.85.   CONCLUSIONS Syndromic surveillance methods can potentially be applied to monitor HIT systems, to facilitate the early detection of failures.	behavior;chart;congenital abnormality;early diagnosis;ephrin type-b receptor 1, human;gene duplication abnormality;information sciences;information system;laboratory information management system;patients;sensitivity and specificity;sensor;sensorineural hearing loss (disorder);serum potassium measurement;tertiary care centers;thrombocytopenia;nervous system disorder	Mei-Sing Ong;Farah Magrabi;Enrico W. Coiera	2013	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2012-001144	data mining;computer security;statistics	ML	6.879214730906451	-79.79096632642546	139022
c3db8698028265756b243bf0952b971d96f1a1e2	modelling traditional chinese medicine therapy planning with pomdp	traditional chinese medicine;uncertainty;pomdp;clinical decision making;therapy planning;mathematical modelling;partially observable markov decision process;clinical practice;diabetes treatment;type 2 diabetes;tcm;dynamic decision making	During the traditional Chinese medicine (TCM) treatment procedure, the manifestations of patients could be observed but the health state and TCM diagnosis of patient are uncertain. Thus, the real-world TCM therapy planning is a typical kind of dynamic decision making under uncertainty. Partially observable Markov decision process (POMDP) constitutes a powerful mathematical model for planning and is suitable for TCM therapy planning. In this paper, we apply POMDP to solve TCM therapy planning problem with all the dynamics inferred from TCM clinical data for type 2 diabetes treatment. This POMDP model contains 55 health states, 67 observation variables and 414 actions, it could order prescriptions for patients with type 2 diabetes. The results demonstrate that the POMDP model for TCM therapy planning is reasonable and helpful in clinical practice.	decision theory;markov chain;mathematical model;nsa product types;partially observable markov decision process;toolkit for conceptual modeling	Qi Feng;Xuezhong Zhou;Houkuan Huang;Xiaoping Zhang;Runshun Zhang	2013	I. J. Functional Informatics and Personalised Medicine	10.1504/IJFIPM.2013.057405	traditional chinese medicine;alternative medicine;medicine;partially observable markov decision process;computer science;knowledge management;artificial intelligence;statistics	AI	2.294822340154806	-76.12935774975189	139406
2a077e66656c67d57d14fd65284eb17be5ca8f6e	using text mining to diagnose and classify epilepsy in children	patient diagnosis;medical records epilepsy classification epilepsy diagnosis children healthcare infrastructures physicians text mining approach medical decisions patient medical records ontologies named entities recognition k nearest neighbors white box lazy method;text analysis data mining diseases medical information systems ontologies artificial intelligence patient diagnosis;epilepsy medical diagnostic imaging text mining hospitals logic gates;text analysis;data mining;ontologies artificial intelligence;medical information systems;diseases;epilepsy data mining text mining electronic medical records icd codes machine learning	Epilepsy diagnosis can be an extremely complex process, demanding considerable time and effort from physicians and healthcare infrastructures. Physicians need to classify each specific type of epilepsy based on different data, e.g., types of seizures, events and exams' results. This work presents a text mining approach to support medical decisions relating to epilepsy diagnosis and classification in children. We propose a text mining process that, using patient medical records, applies ontologies and named entities recognition as preprocessing steps, then applying K-Nearest Neighbors as a white-box lazy method to classify each instance. Results on real medical records suggest that the proposed framework shows good performance and clear interpretations, albeit the reduced volume of available training data.	entity;k-nearest neighbors algorithm;lazy evaluation;named-entity recognition;ontology (information science);preprocessor;text mining	Luís Pereira;Rui Rijo;Catarina Cruz Silva;Margarida Agostinho	2013	2013 IEEE 15th International Conference on e-Health Networking, Applications and Services (Healthcom 2013)	10.1109/HealthCom.2013.6720698	natural language processing;computer science;data science;data mining	ML	3.564692584702353	-77.54892047632926	139495
66ea06a93e6dd966aa4f63fe71dcc6b43976acea	predicting mortality in the intensive care using episodes	decision support;quality assessment;severity of illness;intensive care;temporal pattern;prediction model;logistic regression model;sequential organ failure assessment	Patient outcome prediction lies at the heart of various medically relevant tasks such as quality assessment and decision support. In the intensive care (IC) there are various prognostic models in use today that predict patient mortality. All of these models are logistic regression models that predict the probability of death of an IC patient based on severity of illness scores. These scores are calculated from information that is collected within the first 24 hours of patient admission. Recently, IC units started collecting sequential organ failure assessment (SOFA) scores that quantify the degree of derangement of organs for each patient on each day of the IC stay. Although SOFA scores are primarily meant for recording incidence of organ derangement and failures, the hypothesis is that they contribute to better prediction of mortality. There is virtually no systematic way in the literature to exploit the temporal character of SOFA scores for prediction. This paper adapts ideas from temporal datamining for discovery of sequential episodes and suggests a way to put them into use in the problem of mortality prediction. In particular, we discover frequent temporal patterns, assess their suitability for prediction, and suggest a method for the integration of temporal patterns within the current logistic regression models in use today. Our results show the added value of the new predictive models.		Tudor Toma;Ameen Abu-Hanna;Robert-Jan Bosman	2005		10.1007/11499220_46	simulation;decision support system;computer science;machine learning;data mining;severity of illness;predictive modelling;logistic regression	Robotics	5.7542183339131565	-75.49768184195153	140095
f8c44834942183c85e5302402dea274a1575c14a	a multiobjective deep learning approach for predictive classification in neuroblastoma		Neuroblastoma is a strongly heterogeneous cancer with very diverse clinical courses that may vary from spontaneous regression to fatal progression; an accurate patient’s risk estimation at diagnosis is essential to design appropriate tumor treatment strategies. Neuroblastoma is a paradigm disease where different diagnostic and prognostic endpoints should be predicted from common molecular and clinical information, with increasing complexity, as shown in the FDA MAQC-II study. Here we introduce the novel multiobjective deep learning architecture CDRP (Concatenated Diagnostic Relapse Prognostic) composed by 8 layers to obtain a combined diagnostic and prognostic prediction from high-throughput transcriptomics data. Two distinct loss functions are optimized for the Event-Free Survival (EFS) and Overall Survival (OS) prognosis, respectively. We use the High-Risk (HR) diagnostic information as an additional input generated by an autoencoder embedding. The latter is used as network regulariser, based on a clinical algorithm commonly adopted for stratifying patients from cancer stage, age at insurgence of disease, and MYCN, the specific molecular marker. The architecture was applied to Illumina HiSeq2000 RNA sequencing of 498 neuroblastoma patients (176 at high risk) from the Sequencing Quality Control (SEQC) study, obtaining state-of-art on the diagnostic endpoint and improving prediction of prognosis over the HR cohort. Introduction The challenge of dealing with multiple endpoints of clinical interest is a hallmark of predictive models from high-throughput molecular data, as demonstrated in the MAQC-II (Microarray Analysis and Quality Control) Study [1]. Neuroblastoma is a paradigmatic example of disease where the medical community has adopted a clinical algorithm that defines the subtype of cancer patients with lowest expectation of response to therapy and survival, but the precision medicine approach is still failing to identify molecular profiles clearly associated to patient subtypes. Especially for High risk (HR) patients, adequate therapies are still lacking. Arising predominantly in the first two years of life, neuroblastoma is the most frequent extracranial solid tumor in infancy, accounting for about 500 new cases in Europe per year (130 in Germany), corresponding to roughly 8% of pediatric cancers and 15% of pediatric oncology deaths [2]. Neuroblastoma develops from the immature cells of the ganglionic sympathetic nervous system lineage stemming from the neural crest cells, and tumors can arise at any site where sympathetic neuroblasts are present during normal development [3], e.g., in chest. The broad variety of clinical behavior represent neuroblastoma’s major hallmark, ranging from spontaneous regression (stage 4S) to gradual maturation (stages 1-2) to aggressive and often fatal ganglioneuroma [4, 5] (stages 3-4), despite intensive multimodal treatment. Official staging is defined by the International Neuroblastoma Staging System (INSS) [6]. The current strategies used to appropriately design tumor treatment therapies use different combinations of clinical and genetic markers to discriminate patients with low or high risk of death from disease. The markers used in this diagnosis include age [7], tumor stage [8, 9] and MYCN proto-oncogene genomic amplification [10, 11]. However, this standard protocol is still imperfect, often resulting in overor undertreatment of patients with neuroblastoma [12]. Cancer genetic instability is most often studied at the genomic and gene expression levels, focusing on the effects of genomic alterations on transcription and splicing. In fact, several studies demonstrated that ar X iv :1 71 1. 08 19 8v 3 [ qbi o. Q M ] 2 2 Fe b 20 18 using messenger RNA (mRNA) expression information for molecular classification improves the diagnostic accuracy over traditional clinical markers for individual tumor behavior, enhancing the risk stratification reliability and therefore the therapy selection [13, 14, 15, 16, 17, 18, 1, 19]. Only a limited number of the published classifiers based on gene expression have been so far incorporated into clinical operative systems for a controlled validation trial: as examples, [20, 21] and the U.S. National Institutes of Health clinical trials [22, 23]. The reasons are diverse and include logistic and bureaucratic hindrances for the implementation of classifiers into clinical practice, difficulties in the setup of controlled validation trials for relatively small patient numbers, and the challenge to appropriately design the therapy according to genomic classification results. Moreover, prognostic gene expression signatures for neuroblastoma stemming from different methodologies applied to different datasets often identify diverse gene sets [24, 25]. Thus, the impact of genomic classification-induced treatment on the outcome of neuroblastoma patients is still an open issue. As a contribute, we present here a novel multi-objective deep learning [26] solution named CDRP (Concatenated Diagnostic Relapse Prognostic) that accurately classifies patients in a internationally collected neuroblastoma cohort, by combining both prognostic and diagnostic information from gene expression data. An artificial neural network (multilayer perceptron) has been used for neuroblastoma outcome prediction [27] from expression data but in a shallow learning framework. Deep learning based approaches have also appeared in the neuroblastoma literature, but using images rather than omics inputs [28]. Our architecture is built in multiple steps. We train on half of the patients a multitask net CDRP-N for classification over two distinct prognostic tasks at 5-years, namely Event-Free Survival (EFS: events are relapse, disease progression or death), and the Overall Survival (OS: partitioning patients as either dead or alive). Furthermore, the shared layer of the multitask net has additional inputs from another network modeling the high-risk (HR: high risk, non high-risk or unknown status) endpoint. In detail, we link values from the embedding of an autoencoder CDRP-A developed over the same training data for the HR diagnostic task. In order to control for selection bias, both the net CDRP-N and the autoencoder CDRP-A are trained and evaluated using a Data Analysis Protocol (DAP), based on a 10×5-fold cross validation developed within the MAQC-II and SEQC studies led by the US-FDA [1, 29]. We apply the CDRP-N–CDRP-A architecture on the RNA sequencing (RNA-Seq) dataset from the SEQC study [29, 30]. The same dataset split employed in the Neuroblastoma SEQC satellite study is adopted for comparability [30]. We obtain consistent or improved performance with CDRP-N with respect to SVM or Random Forest models. Operatively, the framework is implemented in a Keras [31] environment over a Tensorflow [32] backend, run on a nVidia Pascal-GPU Blade equipped with two GTX 1080, 8 GB dedicated RAM, 2560 CUDA cores, up to 9TFlops throughput and 8 CPU Intel Core i7–6700 with 32 GB RAM. Data description The dataset used in this study collects RNA-Seq gene expression profiles of 498 neuroblastoma patients, published as part of the SEQC initiative [29, 30]. We considered the following endpoints for classification tasks: the occurrence of an event (progression, relapse or death) (Event-Free survival, “EFS”); the occurrence of death from disease (Overall Survival, “OS”); the occurrence of an event (“EFSHR”) and death from disease (“OSHR”) in high-risk (HR) patients only. HR status was defined according to the NB2004 risk stratification criteria. The samples were split into training (NBt) and validation (NBv) sets following a published partitioning [30]. Stratification statistics for NBt and NBv are reported in Tab. 1. RNA-Seq data were preprocessed as log2 normalized expressions for 60,778 genes (“MAV-G”) [30]. Expression tables were filtered before downstream analyses by removing features without EntrezID and with interquartile range (IQR) > 0.5 using the nsFilter function in the genefilter R package, leaving 12,464 (20.5%) genes for downstream analysis. To avoid information leakage, feature filtering was performed on NBt data set and applied on both NBt and NBv sets. The deep learning architecture The architecture of two deep learning solutions CDRP-N and CDRP-A are shown in Fig. 1. For both, the neural network is developed within a DAP described in detail in the next section. The autoencoder CDRP-A is used as a regressor on the HR/non-HR task, aimed at minimizing the mean square error mse. The input layer is selected by the DAP K-best algorithm to be of dimension 250 (2% of the total number of features), corresponding to the best value mse = 0.042 with confidence interval (0.041; 0.043). This is followed by two dense layers with 128 nodes with tanh activation, ending in another dense layer with 64 nodes with linear activation. The output is later used as the HR embedding input for the shared merge layer in CDRP-N . A specular decoding structure (dotted boxes and arrows in Fig. 1) exists but is not used by CDRP-N .	algorithm;antivirus software;artificial neural network;autoencoder;binary logarithm;cuda;central processing unit;color gradient;communication endpoint;computer multitasking;concatenation;deep learning;disk staging;downstream (software development);encrypting file system;failure;geforce 10 series;gene expression programming;gene prediction;graphics processing unit;high-throughput computing;icl distributed array processor;information leakage;instability;keras;lineage (evolution);loss function;mean squared error;molecular marker;multilayer perceptron;multimodal interaction;netbios over tcp/ip;norm (social);omics;operating system;pascal (microarchitecture);precision medicine;predictive modelling;programming paradigm;random forest;random-access memory;selection bias;spectral leakage;spontaneous order;stemming;tensorflow;throughput;transcription (software)	Valerio Maggio;Marco Chierici;Giuseppe Jurman;Cesare Furlanello	2017	CoRR		autoencoder;cancer;deep learning;bioinformatics;neuroblastoma;mathematics;machine learning;disease;cohort;regression;molecular marker;artificial intelligence	ML	9.353027961961063	-75.44265676003901	140553
86f6c768f3d99d16cde45b6d33c2dc8a387cf2e0	a neural network-based interval pattern matcher	weather forecasting;interval;pattern matcher;neural network	One of the most important roles in the machine learning area is to classify, and neural networks are very important classifiers. However, traditional neural networks cannot identify intervals, let alone classify them. To improve their identification ability, we propose a neural network-based interval matcher in our paper. After summarizing the theoretical construction of the model, we take a simple and a practical weather forecasting experiment, which show that the recognizer accuracy reaches 100% and that is promising.	artificial neural network;finite-state machine;machine learning	Jing Lu;Shengjun Xue;Xiakun Zhang;Yang Han	2015	Information	10.3390/info6030388	interval;weather forecasting;computer science;machine learning;pattern recognition;data mining;time delay neural network;artificial neural network	ML	-3.904784807803459	-74.70301057160609	140928
40a1f3dc8bf2a4fc97fcc25e72b884062e685875	a new method in the long-term mortality related with heart valve replacement	cardiovascular disease long term mortality heart valve replacement lethal factors surgery logistic regressive mathematical model incomplete difference analysis reamer intercept method diabetes ejection fraction discharging blood artery block;lethal factors;artery block;heart valve replacement regressive analysis incomplete difference analysis;cardiology;regressive analysis;training;diabetes;logistic regressive mathematical model;prosthetics;logistic regression;accuracy;long term mortality;risk factors;research purpose;surgery cardiology diseases patient treatment physiological models prosthetics regression analysis;logistics;cardiovascular disease;surgery;mathematical model;diseases;patient treatment;ejection fraction;regression analysis;lethal factor;correlation;incomplete difference analysis;reamer intercept method;valves;discharging blood;heart valve;valves surgery training correlation accuracy mathematical model logistics;physiological models;heart valve replacement	This research mainly discusses the lethal factors of patients who receive the surgery of heart valve replacement for a long time. The research purpose is to provide the reference for the therapy of clinic. Logistic regressive mathematical model and a new conditioning (training) set are given. Applying incomplete difference analysis to the model and testing with reamer intercept method to ensure the correctness of the model, we obtain that the patient with diabetes, ejection fraction(the percent of discharging blood) less than 35% after surgery, the time of artery block between 61–90minutes, and the age less than 15 or more than 65 are risk factors of the long-term deaths.	correctness (computer science);mathematical model;risk factor (computing);test set;verification and validation;visual intercept	Chuanqing Xu;Lei Gong;Dehui Yuan;Shengyun Yang	2010	2010 Seventh International Conference on Fuzzy Systems and Knowledge Discovery	10.1109/FSKD.2010.5569751	logistics;mathematical model;accuracy and precision;logistic regression;risk factor;correlation;ejection fraction;regression analysis;statistics	Robotics	8.276247150823965	-75.77591485228045	142070
0688790d6423e3962d6a3adff7b369f94caa3a8b	predator-prey dynamics in p systems ruled by metabolic algorithm	lotka volterra;predator prey system;lotka volterra dynamics;predator prey systems;metabolic algorithm;p systems;predator prey dynamics;p system;stochastic simulation algorithm;runge kutta	P systems are used to compute predator-prey dynamics expressed in the traditional formulation by Lotka and Volterra. By governing the action of the transition rules in such systems using the regulatory features of the metabolic algorithm we come up with simulations of the Lotka-Volterra equations, whose robustness is comparable to that obtained using Runge-Kutta schemes and Gillespie's Stochastic Simulation Algorithm. Besides their reliability, the results obtained using the metabolic algorithm on top of P systems have a clear biochemical interpretation concerning the role, of reactants or promoters, of the species involved.	gillespie algorithm;lotka–volterra equations;metabolic process, cellular;p system;prey;production (computer science);rule (guideline);runge–kutta methods;simulation;promoter	Federico Y Fontana;Vincenzo Manca	2008	Bio Systems	10.1016/j.biosystems.2006.12.010	mathematical optimization;runge–kutta methods;computer science;bioinformatics;control theory;mathematics;ecology;p system	Logic	7.644290479472992	-67.09511831393405	142785
447c06f8e5310b6258eb9216eb178eebeecd12c7	using a data mining approach to discover behavior correlates of chronic disease: a case study of depression		The purposes of this methodological paper are: 1) to describe data mining methods for building a classification model for a chronic disease using a U.S. behavior risk factor data set, and 2) to illustrate application of the methods using a case study of depressive disorder. Methods described include: 1) six steps of data mining to build a disease model using classification techniques, 2) an innovative approach to analyzing high-dimensionality data, and 3) a visualization strategy to communicate with clinicians who are unfamiliar with advanced statistics. Our application of data mining strategies identified childhood experience living with mentally ill and sexual abuse, and limited usual activity as the strongest correlates of depression among hundreds variables. The methods that we applied may be useful to others wishing to build a classification model from complex, large volume datasets for other health conditions.		Sunmoo Yoon;Basirah Taha;Suzanne Bakken	2014	Studies in health technology and informatics	10.3233/978-1-61499-415-2-71	data mining;visualization;data science;risk factor;disease;risk assessment;medicine	ML	3.5131545240315027	-76.27667502134982	142792
5b3b121acbbd134c96852db41d777cb5d02540fc	classification and diagnosis of syndromes in chinese medicine in the context of coronary heart disease model based on data mining methods	coronary heart disease;diagnostic accuracy;statistical method;data mining;cluster analysis;myocardial ischemia;animal model	Objective: To study on the classification and diagnostic of syndromes in Chinese medicine (TCM) based on the coronary heart disease model (CHD, myocardial ischemia) by application of clustering analysis in mathematical statistics methods. Methods: By application of combining disease with syndrome model, dynamically observed and recorded pathologic signs of animal models, a total of 172 frequencies of the signs were collected, and the variables indicators were analyzed by cluster analysis. Results: The results show that CHD model can be divided into four syndromes by cluster analysis. The four categories can cover the ratio of 71.05% models; it gets a diagnostic accuracy rate of 92.11%, which can be used as key points to diagnose various syndromes in CHD. Conclusion: Cluster analysis can help to classify the TCM syndromes reasonably and objectively. What more, it also can discover the pattern of the syndrome evolution, Thus to provide a theoretical basis for the standardization of TCM research.	data mining	Yong Wang;Huihui Zhao;Jianxin Chen;Chun Li;Wenjing Chuo;Shuzhen Guo;Junda Yu;Wei Wang	2010		10.1007/978-3-642-15615-1_25	computer science;machine learning;cluster analysis	NLP	4.790659732847271	-76.34773580243603	142844
be2f58f59701699805a427042a410ddc5555ab3f	the effect of music on the level of mental concentration and its temporal change	attention;temporal pattern;music;concentration	Concentration is one of the most important factor in determining the efficiency of learning. There has not been, however, much systematic research on controlling the level of concentration. We therefore examined the effect of an external factor, namely playing music, on the performance on a task that requires much attention. We compared three conditions: music that the subject likes, music that the subject is not familiar with, and silence. The result showed that listening to music that the subject likes do increase the performance level. Also, we discovered that there exist different temporal patterns in the change of performance. The result also indicated a relationship between the temporal pattern in concentration and the external factor.	computer performance;existential quantification;experiment;frequency analysis;modulation	Fumiya Mori;Fatemeh Azadi Naghsh;Taro Tezuka	2014		10.5220/0004791100340042	psychology;speech recognition;attention;music;communication;concentration;social psychology	ML	-4.488033056362741	-79.93730431846963	143476
f283dc9bb69aa66ef7df5a5a3d2dfb66c8c2f68d	towards targeted combinatorial therapy design for the treatment of castration-resistant prostate cancer	gene regulatory networks;boolean modeling;prostate cancer;combination therapy	Prostate cancer is one of the most prevalent cancers in males in the United States and amongst the leading causes of cancer related deaths. A particularly virulent form of this disease is castration-resistant prostate cancer (CRPC), where patients no longer respond to medical or surgical castration. CRPC is a complex, multifaceted and heterogeneous malady with limited standard treatment options. The growth and progression of prostate cancer is a complicated process that involves multiple pathways. The signaling network comprising the integral constituents of the signature pathways involved in the development and progression of prostate cancer is modeled as a combinatorial circuit. The failures in the gene regulatory network that lead to cancer are abstracted as faults in the equivalent circuit and the Boolean circuit model is then used to design therapies tailored to counteract the effect of each molecular abnormality and to propose potentially efficacious combinatorial therapy regimens. Furthermore, stochastic computational modeling is utilized to identify potentially vulnerable components in the network that may serve as viable candidates for drug development. The results presented herein can aid in the design of scientifically well-grounded targeted therapies that can be employed for the treatment of prostate cancer patients.		Osama A. Arshad;Aniruddha Datta	2016		10.1145/2975167.2985671	biology;gene regulatory network;medicine;pathology;gynecology;biological engineering;genetics	EDA	8.097944119447044	-68.54348361505706	143518
3347bb7e9eb1a22cdcd1ae11b8a49bb97d5f4390	artmap-ic and medical diagnosis: instance counting and inconsistent cases	prediccion;heart disease;distributed representation;architecture systeme;prediction error;fuzzy artmap;aplicacion medical;neural networks;learning;real time;artmap;simulation;diagnostico;differential equation;simulacion;logistic regression;aprendizaje;apprentissage;voting;automated medical prediction;instance counting;prediction accuracy;arquitectura sistema;medical application;match tracking;k nearest neighbour;technical report;reseau neuronal;system architecture;diagnosis;prediction;breast cancer;red neuronal;adaptive resonance theory art;medical diagnosis;adaptive resonance theory;neural network;artmap ic;application medicale;diagnostic	For complex database prediction problems such as medical diagnosis, the ARTMAP-IC neural network adds distributed prediction and category instance counting to the basic fuzzy ARTMAP system. For the ARTMAP match tracking algorithm, which controls search following a predictive error, a new version facilitates prediction with sparse or inconsistent data. Compared to the original match tracking algorithm (MT+), the new algorithm (MT-) better approximates the real-time network differential equations and further compresses memory without loss of performance. Simulations examine predictive accuracy on four medical databases: Pima Indian diabetes, breast cancer, heart disease, and gall bladder removal. ARTMAP-IC results are equal to or better than those of logistic regression, K nearest neighbour (KNN), the ADAP preceptron, multisurface pattern separation, CLASSIT, instance-based (IBL), and C4. ARTMAP dynamics are fast, stable, and scalable. A voting strategy improves prediction by training the system several times on different orderings of an input set. Voting, instance counting, and distributed representations combine to form confidence estimates for competing predictions.	american society of hematology;artificial neural network;benchmark (computing);biological neural networks;bladder neoplasm;computer simulation;contain (action);database;estimated;gall;gallbladder;heart diseases;ibm notes;k-nearest neighbors algorithm;logistic regression;mammary neoplasms;nephrogenic systemic fibrosis;neural network simulation;paul moskowitz;real-time clock;real-time computing;scalability;sparse matrix;test set;urinary bladder	Gail A. Carpenter;Natalya Markuzon	1998	Neural networks : the official journal of the International Neural Network Society	10.1016/S0893-6080(97)00067-1	simulation;computer science;artificial intelligence;machine learning;medical diagnosis;artificial neural network	ML	7.303076316313507	-78.46607979206216	143735
f257f0f0181df82612ce1c4ad51d97c22fdf6830	using latent class analysis to model prescription medications in the measurement of falling among a community elderly population	female;health informatics;drug prescriptions;accidental falls;residence characteristics;prescription drugs;retrospective studies;models theoretical;male;information systems and communication service;management of computing and information systems;risk assessment;humans;aged;aged 80 and over	BACKGROUND Falls among the elderly are a major public health concern. Therefore, the possibility of a modeling technique which could better estimate fall probability is both timely and needed. Using biomedical, pharmacological and demographic variables as predictors, latent class analysis (LCA) is demonstrated as a tool for the prediction of falls among community dwelling elderly.   METHODS Using a retrospective data-set a two-step LCA modeling approach was employed. First, we looked for the optimal number of latent classes for the seven medical indicators, along with the patients' prescription medication and three covariates (age, gender, and number of medications). Second, the appropriate latent class structure, with the covariates, were modeled on the distal outcome (fall/no fall). The default estimator was maximum likelihood with robust standard errors. The Pearson chi-square, likelihood ratio chi-square, BIC, Lo-Mendell-Rubin Adjusted Likelihood Ratio test and the bootstrap likelihood ratio test were used for model comparisons.   RESULTS A review of the model fit indices with covariates shows that a six-class solution was preferred. The predictive probability for latent classes ranged from 84% to 97%. Entropy, a measure of classification accuracy, was good at 90%. Specific prescription medications were found to strongly influence group membership.   CONCLUSIONS In conclusion the LCA method was effective at finding relevant subgroups within a heterogenous at-risk population for falling. This study demonstrated that LCA offers researchers a valuable tool to model medical data.	accidental falls;bayesian information criterion;bootstrapping (statistics);default;latent class model;patients;pharmacology;imidazole mustard;likelihood ratio	Patrick C. Hardigan;David C. Schwartz;William D. Hardigan	2013		10.1186/1472-6947-13-60	gerontology;health informatics;risk assessment;medicine;nursing;retrospective cohort study;medical emergency	ML	7.9550986321291655	-74.75622762060088	144189
77987b3c52cf1d1b85561b8296f7340b92ad2b7e	predicting the risk of low-fetal birth weight from cardiotocographic signals using anblir system with deterministic annealing and ${\bm \varepsilon}$  -insensitive learning	algorithms fuzzy logic heart rate fetal humans infant low birth weight infant newborn learning;classification efficiency low fetal birth weight cardiotocographic signals anblir system deterministic annealing e insensitive learning artificial neural network fuzzy if then rules neurofuzzy system integrating least squares method classified cases;integrating least squares method;e insensitive learning;signal generators;fetal monitoring;least squares approximations;cardiography annealing fetal heart computerized monitoring signal processing signal generators pediatrics artificial neural networks fuzzy logic fuzzy neural networks;classification efficiency;pediatrics;deterministic annealing;annealing;learning;neural nets;birth weight;cardiotocographic signals;cardiology;least square method;fuzzy logic;computerized monitoring;artificial neural networks;monitoring system;low birth weight;heart rate fetal;condition assessment;general methods;infant newborn;signal processing;indexation;low fetal birth weight;anblir system;signal classification;signal classification fetal heart rate fhr fetal monitoring fuzzy systems risk of low fetal birth weight;fuzzy if then rules;cardiography;algorithms;decision process;humans;signal classification cardiology fuzzy systems least squares approximations medical signal processing neural nets obstetrics;risk of low fetal birth weight;fetal heart rate;fuzzy neural networks;classified cases;fetal heart;medical signal processing;fuzzy systems;fuzzy system;fetal heart rate fhr;obstetrics;artificial neural network;fuzzy if then rules neurofuzzy system;infant low birth weight	Cardiotocography (CTG) is a biophysical method of fetal condition assessment based mainly on recording and automated analysis of fetal heart activity. The computerized fetal monitoring systems provide the quantitative description of the CTG signals, but the effective conclusion generation methods for decision process support are still needed. Assessment of the fetal state can be verified only after delivery using the fetal (newborn) outcome data. One of the most important features defining the abnormal fetal outcome is low birth weight. This paper describes an application of the artificial neural network based on logical interpretation of fuzzy if-then rules neurofuzzy system to evaluate the risk of low-fetal birth weight using the quantitative description of CTG signals. We applied different learning procedures integrating least squares method, deterministic annealing (DA) algorithm, and ε-insensitive learning, as well as various methods of input dataset modification. The performance was evaluated with the number of correctly classified cases (CC) expressed as the percentage of the testing set size, and with overall index (OI) being the function of predictive indexes. The best classification efficiency (CC = 97.5% and OI = 82.7%), was achieved for integrated DA with ε-insensitive learning and dataset comprising of the CTG traces recorded as earliest for a given patient. The obtained results confirm efficiency for supporting the fetal outcome prediction using the proposed methods.	abnormal end;artificial neural network;body mass index;cardiotocography;classification;computation;computational complexity theory;database;experiment;fetal diseases;fetal heart;forecast of outcome;gestational weeks assessment;infant, newborn;inter-rater reliability;interpretation (logic);intrauterine;limited stage (cancer stage);linear least squares (mathematics);low birth weight infant;machine learning;maximal set;opportunistic infections;patients;personnameuse - assigned;randomness;registration;rule (guideline);silo (dataset);simulated annealing;solutions;spatial variability;tracing (software);algorithm;negative regulation of double-strand break repair via single-strand annealing	Robert Czabanski;Michal Jezewski;Janusz Wrobel;Janusz Jezewski;Krzysztof Horoba	2010	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2009.2039644	fuzzy logic;annealing;computer science;artificial intelligence;machine learning;least squares;birth weight;artificial neural network;algorithm;signal generator	ML	7.721784323625293	-76.63227890142258	144226
373dc939a385a0a4a13632cf519a538cd5a351a4	invited talk: encoding evolvability: the hierarchical language of polyketide synthase protein interactions		Polyketide synthases use an assembly-line mechanism to catalyse the synthesis of antibiotics and other natural products. Each member of a multiprotein complex adds a particular building block to a growing polyketide chain, so the order of the proteins determines the order of the product. In the laboratory, this property has been used to drive combinatorial chemistry; in the bacterial world, polyketide synthase pathways have been repeatedly shuffled in an arms race to generate novel poisons. I will show that the language of polyketide synthase protein interactions has been designed to facilitate this kind of innovation. I will present the interaction code in detail, and emphasize the elements it shares with high-level computer languages, including modularity, hierarchical organization, and abstraction.	coat of arms;combinatorial chemistry;computer language;high- and low-level;interaction	Mukund Thattai	2006			bioinformatics;ventilation (architecture);genetics;gas-discharge lamp;airflow;evolvability;polyketide synthase;biology	PL	3.7533456333871547	-66.33259604582895	144524
c9cfcb62eb84e0ec7505df863d10feb33629637f	biological cell communications technology: an architecture overview	molecular biophysics biocommunications cellular biophysics;chemicals;biocommunications;biological communication paradigm;nanobioscience;nanobioscience robustness chemicals optical receivers;cell communication mechanisms;molecular biophysics;cell communication;robustness;cell communication mechanisms molecular communication technology biological communication paradigm;communication technology;molecular communication technology;cellular biophysics;optical receivers	This paper provides an overview of the molecular communication technology - a biological communication paradigm inspired by cell communication mechanisms. The architecture, features, and current status of molecular communication are summarized.	cell signaling;programming paradigm	Tadashi Nakano;Tatsuya Suda	2010	The 6th International Conference on Networked Computing and Advanced Information Management		computational biology;information and communications technology;chemical industry;cell signaling;robustness;molecular biophysics	HPC	3.6346827858609685	-68.86116249720659	144685
629a309878ee0a63ff355afd2e8bb7e5cc0e5913	identifying diseases assciated with a high risk for acute kidney injury using a hospital information system database	hyperuricemia disease identification hospital information system database fluid homeostasis renal function deterioration aki patients chronic kidney disease chronic kidney mortality hospitalized patient data kochi medical school hospital serum creatinine level acute kidney injury network criteria akin criteria multivariate analysis cardiovascular diseases shock syndrome respiratory diseases pneumonia infection diseases septicemia;hospital information system database medical data mining acute kidney injury risk disease;medical information systems cardiovascular system data mining diseases kidney medical disorders	Acute kidney injury (AKI) occurs when a patient cannot maintain fluid homeostasis because of acute deterioration in renal function. Several studies have indicated that patients with AKI have higher risks for developing chronic kidney disease and mortality compared with patients without AKI. Although AKI is a serious disorder, few studies have identified the diseases that cause AKI. The purpose of this study was to identify the diseases associated with AKI. METHODS The data of 68,504 hospitalized patients at Kochi Medical School Hospital from 1981 to 2010 were analyzed. All laboratory test results were automatically processed and saved in a hospital information system database. Episodes of AKI were identified by the serum creatinine level as defined by Acute Kidney Injury Network (AKIN) criteria. All diseases that were diagnosed within 30 days prior to the development of AKI were collected from the hospital information system database. Compared with patients who were not affected with AKI, the odds ratios were calculated, and a multivariate analysis was conducted. RESULTS AND DISCUSSION The highest odds ratio was AKI (odds ratio 46.44, 95% confidence interval 36.88–58.49) This means that our method has enough usability and validity to identify the diseases associated with a high risk for AKI. In addition, cardiovascular diseases (e.g., shock syndrome), respiratory diseases (e.g., pneumonia), and infection diseases (e.g., septicemia) showed high odds ratio for AKI as expected (odds ratio 4.44, 95% confidence interval 4.07–4.84; odds ratio 2.38, 95% confidence interval 2.23–2.56; odds ratio 4.54, 95% confidence interval 4.17–4.93 respectively). Furthermore, hyperuricemia also had a significant odds ratio (odds ratio 1.79, 95% confidence interval 1.62–1.98). Few studies have shown a relationship between AKI and hyperuricemia. This study identified not only diseases expected to be risks for AKI, but also diseases that have never been regarded as risks for AKI.	database;homeostasis;information system;usability	Kazunori Otomo;Tomoaki Ishibashi;Hiromi Kataoka;Yutaka Hatakeyama;Yoshiyasu Okuhara	2012	The 6th International Conference on Soft Computing and Intelligent Systems, and The 13th International Symposium on Advanced Intelligence Systems	10.1109/SCIS-ISIS.2012.6505062	serum creatinine level;odds ratio;hyperuricemia;renal function;kidney disease;computer science;multivariate analysis;database;acute kidney injury;confidence interval	DB	4.855399479832598	-76.12732761909297	145030
82c41bf921e3e14e5798100168683f9221a91beb	a risk analysis based on a two-stage delayed diagnosis regression model with application to chronic disease progression	chronic disease;model diagnostic;disease progression;ucl;hepatitis;risk analysis;discovery;regression model;theses;conference proceedings;hepatitis c;risk factors;digital web resources;risk;ucl discovery;open access;ucl library;two stage disease progression;book chapters;delay time;open access repository;cohort study;delayed diagnosis;ucl research	This paper presents a two-stage regression model for quantifying different stages of a disease progression with delayed diagnosis time and for identifying the risk factors associated with each stage. Conventional chronic disease progression studies reported replied on the assumption that the time of the confirmation of a disease state by diagnosis is the start time of this disease state. Clearly this will lead to biased estimates of progression since the disease state should have already occurred before the diagnosis, but the true occurrence time is unknown. This later confirmation is called the delayed diagnosis in this paper and a delay-time modelling procedure is developed for the identification of the unknown stages of progression. A hazard-based regression model is also proposed for a further risk analysis. We apply the developed methods to hepatitis C data and the analysis shows that considering the delayed diagnosis significantly improved the model fit in comparison with the conventional model. We also find that the risk factors associated with each stage are more significant, particularly in the second stage of progression, than those based on the conventional model. We conclude that such delayed phenomena in diagnosis should be taken into account when modelling the chronic disease progression process and conducting related risk analysis.	color gradient;it risk management	Bo Fu;Wenbin Wang;Xin Shi	2012	European Journal of Operational Research	10.1016/j.ejor.2011.12.013	risk analysis;economics;cohort study;risk;mathematics;operations research;risk factor;regression analysis	ML	6.805405055078157	-74.23297668851005	145475
a5d8cbf8e56952eca89ae8864fa3650f2c1a5584	detection and characterization of cancer cells and pathogenic bacteria using aptamer-based nano-conjugates	nanoparticles;cancer cell;aptamer;nanomaterial;cell selex;humans;bacteria;neoplasms;nanoparticle;selex aptamer technique	Detection and characterization of cells using aptamers and aptamer-conjugated nanoprobes has evolved a great deal over the past few decades. This evolution has been driven by the easy selection of aptamers via in vitro cell-SELEX, permitting sensitive discrimination between target and normal cells, which includes pathogenic prokaryotic and cancerous eukaryotic cells. Additionally, when the aptamer-based strategies are used in conjunction with nanomaterials, there is the potential for cell targeting and therapeutic effects with improved specificity and sensitivity. Here we review recent advances in aptamer-based nano-conjugates and their applications for detecting cancer cells and pathogenic bacteria. The multidisciplinary research utilized in this field will play an increasingly significant role in clinical medicine and drug discovery.	bacteria;drug discovery;early diagnosis;gnu nano;hybrids;immunostimulating conjugate (antigen);list of wireless mice with nano receivers;nanostructured materials;neoplasms;processor affinity;reagents;receptors, cell surface;sensitivity and specificity;sensor;aptamer;cellular targeting;sensor (device)	Vinayakumar Gedi;Young-Pil Kim	2014		10.3390/s141018302	molecular biology;nanomaterials;nanotechnology;nanoparticle	ML	5.401012338404113	-66.28879936417793	145660
ee266421a00a26e44d26d4cce4d694bd7384ef4e	efficacy of clinical alerts designed to decrease the incidence of contrast induced nephropathy			incidence matrix	Mark A. Parkulo;Julia E. Crook;Launia J. White;Colleen S. Thomas;March Rucci	2015			medicine;contrast-induced nephropathy	AI	9.95808713960588	-77.38589454850928	145707
3218f27809e7c05f99ecdaf2f319dd575e4e95c3	time aggregation and model interpretation for deep multivariate longitudinal patient outcome forecasting systems in chronic ambulatory care		Clinical data for ambulatory care, which accounts for 90% of the nations healthcare spending, is characterized by relatively small sample sizes of longitudinal data, unequal spacing between visits for each patient, with unequal numbers of data points collected across patients. While deep learning has become state-of-the-art for sequence modeling, it is unknown which methods of time aggregation may be best suited for these challenging temporal use cases. Additionally, deep models are often considered uninterpretable by physicians which may prevent the clinical adoption, even of well performing models. We show that time-distributed-dense layers combined with GRUs produce the most generalizable models. Furthermore, we provide a framework for the clinical interpretation of the models.		Beau Norgeot;Dmytro Sergiiovych Lituiev;Benjamin S. Glicksberg;Atul J. Butte	2018	CoRR			ML	6.167750786587078	-75.23986655007914	145762
edcc5938e9d0c54755e755d89be2c1de536cc3f1	a prototype decision support system for mr spectroscopy-assisted diagnosis of brain tumours.	computer assisted diagnosis;decision support;case base reasoning;user interface;magnetic resonance spectroscopy;spectrum;support system;decision support system;pattern recognition;mr spectroscopy;user computer interface;brain tumour	Our objective is to develop a decision support system that improves the accuracy of non-invasive brain tumour diagnosis and grading by enabling radiologists to use data from Magnetic Resonance Spectroscopy (MRS). The system, which uses pattern recognition techniques, is trained on a validated database of spectra and associated clinical information to provide automated classification of spectra from brain tumours. An innovative user-interface presents classification results as a two-dimensional overview plot in which points representing cases of different diseases form distinct clusters. Users can inspect any cases in these plots and compare them with the new, unknown spectrum. Hence, the overview plot can both communicate the classification of a case and help provide explanation for that classification in part by supporting human case-based reasoning. This paper describes the development of a prototype system implemented in JAVA.		Joshua Underwood;Anne Rosemary Tate;Rosemary Luckin;Carles Majós;Antoni Capdevila;Franklyn A. Howe;John R. Griffiths;Caries Anús	2001	Studies in health technology and informatics	10.3233/978-1-60750-928-8-561	simulation;computer science;artificial intelligence;data mining	AI	2.4214392301955856	-77.83340041329328	145821
b15874cdc077eae02b91f3667601aec43296dcb9	isimbiosys: a discrete event simulation platform for 'in silico' study of biological systems	magnesium;databases;phopq system;system engineering;biology computing;cellular magnesium;discrete event simulation biological systems biological system modeling biological processes mathematical model systems engineering and theory system testing stochastic processes diseases databases;modeling and simulation;cellular process;cellular biophysics biology computing discrete event simulation large scale systems systems engineering;systems engineering;biological system modeling;test bed;cellular magnesium isimbiosys discrete event simulation in silico study database cataloguing complex biological system simulation tool cellular behavior system engineering biological process cellular process phopq system;database cataloguing;systems engineering and theory;cellular behavior;stochastic processes;virulence gene;mathematical model;diseases;biological systems;system testing;complex biological system;salmonella typhimurium;in silico study;simulation tool;biological processes;cellular biophysics;discrete event;isimbiosys;large scale systems;biological process;discrete event simulation;in silico	"""With the availability of huge databases cataloguing the various molecular """"parts"""" of complex biological systems, researchers from multiple disciplines have focused on developing modeling and simulation tools for studying the variability of cellular behavior at a system level - encompassing the dynamics arising from many species of interacting molecules. In this work, we present a system engineering approach to model biological processes. In this approach, a biological process is modeled as a collection of interacting functions driven in time by a set of discrete events. We focus on the discrete event simulation platform, called """"iSimBioSys"""", which we have developed for studying the dynamics of cellular processes in silico. As a test-bed for studying our approach we model the two component PhoPQ system, responsible for the expression of several virulence genes in Salmonella Typhimurium. We analyzed the effect of extra cellular magnesium on the behavioral dynamics of this pathway using our framework and compared the results with an experimental system. We also analyze the performance of iSimBioSys, based on the model biological system, in terms of system usage and response."""	biological system;database;experimental system;gene regulatory network;heart rate variability;interaction;simulation;systems engineering;testbed	Samik Ghosh;Preetam Ghosh;Kalyan Basu;Sajal K. Das;Simon Daefler	2006	39th Annual Simulation Symposium (ANSS'06)	10.1109/ANSS.2006.22	simulation;computer science;bioinformatics;theoretical computer science	Comp.	4.392578043636289	-67.13173217181503	146400
1f52d77e4b58d9c74cb34e9293ed0162e5aa510b	predicting activities of daily living for cancer patients using an ontology-guided machine learning methodology	activities of daily living;bio-ontologies;machine learning;quality of life;seer-mhos	"""BACKGROUND Bio-ontologies are becoming increasingly important in knowledge representation and in the machine learning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to the SEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities of daily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to """"understand"""" biomedical data.   RESULTS This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods were applied to create predictive models for ADL disabilities for the first year after a patient's cancer diagnosis. The first method is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods for reasoning with ontologies. The models showed that a patient's race, ethnicity, smoking preference, treatment plan and tumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performance levels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADL performance levels (P < 0.1) than methods without ontologies.   CONCLUSIONS This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for ML algorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rule learning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. The ontology-guided ML method achieved better performance than the method without ontologies. The presented method can also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledge and consistency with existing clinical expertise is critical."""	algorithm;british informatics olympiad;computable function;diagnostic neoplasm staging;disk staging;knowledge representation and reasoning;machine learning;mathematical morphology;neoplasms;ontology (information science);patients;predictive modelling;seer-sem;sgca gene;silo (dataset)	Hua Min;Hedyeh Mobahi;Katherine Irvin;Sanja Avramovic;Janusz Wojtusiak	2017		10.1186/s13326-017-0149-6	data mining;knowledge representation and reasoning;computer science;data science;activities of daily living;ontology;machine learning;artificial intelligence	AI	5.930015756081693	-76.30610020269458	146524
eea1e1ad4242a07ea9fbc6697877f9be63c94c31	improving the accuracy of existing camera based fall detection algorithms through late fusion		Fall incidents remain an important health hazard for older adults. Fall detection systems can reduce the consequences of a fall incident by insuring that timely aid is given. Currently fall detection algorithms however suffer a reduction in accuracy when introduced in real-life situations. In this paper a late fusion technique is proposed that will improve the accuracy of existing fall detection systems. It combines the confidence levels of different single camera fall detection systems. Four different aggregation methods are compared to each other based on the Area Under the Curve (AUC) of precision-recall curves. Calculating the median of the confidence levels of five cameras an increase of 218% in the AUC of the precision-recall-curves is achieved compared to the AUC of the single camera fall detector. These results show that significant improvements can be made to the accuracy of single camera fall detectors in a relatively easy way.	algorithm;area under curve;detectors;hazard (computer architecture);hypothalamic area, lateral;latent variable;real life	Greet Baldewijns;Glen Debard;Gert Mertes;Tom Croonenborghs;Bart Vanrumste	2017	2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2017.8037406	computer vision;artificial intelligence;area under the curve;algorithm;computer science	Visualization	8.66784125626312	-78.05965077216858	146797
83c751616e3d4d9251b71802bd67d206d565881d	a system dynamics approach to analyze laboratory test errors	medical errors;clinical chemistry tests;quality control;article	"""Although many researches have been carried out to analyze laboratory test errors during the last decade, it still lacks a systemic view of study, especially to trace errors during test process and evaluate potential interventions. This study implements system dynamics modeling into laboratory errors to trace the laboratory error flows and to simulate the system behaviors while changing internal variable values. The change of the variables may reflect a change in demand or a proposed intervention. A review of literature on laboratory test errors was given and provided as the main data source for the system dynamics model. Three """"what if"""" scenarios were selected for testing the model. System behaviors were observed and compared under different scenarios over a period of time. The results suggest system dynamics modeling has potential effectiveness of helping to understand laboratory errors, observe model behaviours, and provide a risk-free simulation experiments for possible strategies."""	behavior;cns disorder;experiment;flow;laboratory procedures;simulation;system dynamics;what if	Shijing Guo;Abdul V. Roudsari;Artur S. d'Avila Garcez	2015	Studies in health technology and informatics	10.3233/978-1-61499-512-8-266	quality control;simulation;computer science	SE	9.424547990325404	-70.64666958140569	146834
bb2c877242a52bdb51c5b941972dc993a4fe02a4	mining tinnitus data based on clustering and new temporal features		Tinnitus problems affect a significant portion of the population and are difficult to treat. Sound therapy for Tinnitus is a promising, expensive, and complex treatment, where the complete process may span from several months to a couple of years. The goal of this research is to explore different combinations of important factors leading to a significant recovery, and their relationships to different category of Tinnitus problems. Our findings are extracted from the data stored in a clinical database, where confidential information had been stripped off. The domain knowledge spans different disciplines such as otology as well as audiology. Complexities were encountered with temporal data and text data of certain features. New temporal features together with rule generating techniques and clustering methods are presented with a ultimate goal to explore the relationships among the treatment factors and to learn the essence of Tinnitus problems. Xin Zhang University of North Carolina at Pembroke, Dept. of Math. Comp. Science, Pembroke, NC 28372, USA e-mail: xin.zhang@uncp.edu Pamela Thompson University of North Carolina, Dept. of Computer Science, Charlotte, NC 28223, USA e-mail: pthompso@catawba.edu Zbigniew W. Raś University of North Carolina, Dept. of Computer Science, Charlotte, NC 28223, USA & Warsaw University of Technology, Institute of Comp. Science, 00-665 Warsaw, Poland e-mail: ras@uncc.edu Pawel Jastreboff Emory University School of Medicine, Dept. of Otolaryngology, Atlanta, GA 30322, USA e-mail: pjastre@emory.edu	classification chart;cluster analysis;computer science;confidentiality;data mining;decision tree learning;email;experiment;precision and recall;quantization (signal processing);resultant;software release life cycle;text corpus	Cynthia Xin Zhang;Pamela L. Thompson;Zbigniew W. Ras;Pawel J. Jastreboff	2011		10.1007/978-3-642-22913-8_11	natural language processing;domain knowledge;temporal database;cluster analysis;granular computing;hearing loss;artificial intelligence;tinnitus;computer science;otology;population	ML	-0.49655452824237056	-74.38917590418207	147345
a87bc818f7409ac97c8719aa8fae2c40d214ebbc	deep computational phenotyping	medical informatics;healthcare;multi label classification;phenotyping;deep learning;multivari ate time series	We apply deep learning to the problem of discovery and detection of characteristic patterns of physiology in clinical time series data. We propose two novel modifications to standard neural net training that address challenges and exploit properties that are peculiar, if not exclusive, to medical data. First, we examine a general framework for using prior knowledge to regularize parameters in the topmost layers. This framework can leverage priors of any form, ranging from formal ontologies (e.g., ICD9 codes) to data-derived similarity. Second, we describe a scalable procedure for training a collection of neural networks of different sizes but with partially shared architectures. Both of these innovations are well-suited to medical applications, where available data are not yet Internet scale and have many sparse outputs (e.g., rare diagnoses) but which have exploitable structure (e.g., temporal order and relationships between labels). However, both techniques are sufficiently general to be applied to other problems and domains. We demonstrate the empirical efficacy of both techniques on two real-world hospital data sets and show that the resulting neural nets learn interpretable and clinically relevant features.	artificial neural network;code;computation;deep learning;ontology (information science);scalability;sparse matrix;time series	Zhengping Che;David C. Kale;Wenzhe Li;Mohammad Taha Bahadori;Yan Liu	2015		10.1145/2783258.2783365	computer science;data science;machine learning;phenotype;data mining;deep learning;statistics	ML	2.8875524371777717	-72.78343029441857	147652
9351d31cf0e262d70147b35c80d9089fd1f696e1	a probabilistic and decision-theoretic approach to the management of infectious disease at the icu	bayesian network;medical records;intensive care unit;spectrum;probabilistic model;decision support system;intensive care;decision theory;decision theoretic;antimicrobial therapy;clinical information system;infectious diseases;probabilistic networks;temporal probabilistic models;infectious disease;medical decision support;probabilistic network;bayesian networks	The medical community is presently in a state of transition from a situation dominated by the paper medical record to a future situation where all patient data will be available on-line by an electronic clinical information system. In data-intensive clinical environments, such as intensive care units (ICUs), clinical patient data are already fully managed by such systems in a number of hospitals. However, providing facilities for storing and retrieving patient data to clinicians is not enough; clinical information systems should also offer facilities to assist clinicians in dealing with hard clinical problems. Extending an information system's capabilities by integrating it with a decision-support system may be a solution. In this paper, we describe the development of a probabilistic and decision-theoretic system that aims to assist clinicians in diagnosing and treating patients with pneumonia in the intensive-care unit. Its underlying probabilistic-network model includes temporal knowledge to diagnose pneumonia on the basis of the likelihood of laryngotracheobronchial-tree colonisation by pathogens, and symptoms and signs actually present in the patient. Optimal antimicrobial therapy is selected by balancing the expected efficacy of treatment, which is related to the likelihood of particular pathogens causing the infection, against the spectrum of antimicrobial treatment. The models were built on the basis of expert knowledge. The patient data that were available were of limited value in the initial construction of the models because of problems of incompleteness. In particular, detailed temporal information was missing. By means of a number of different techniques, among others from the theory of linear programming, these data have been used to check the probabilistic information elicited from infectious-disease experts. The results of an evaluation of a number of slightly different models using retrospective patient data are discussed as well.	communicable diseases;data-intensive computing;decision support system;decision theory;equilibrium;information systems;information system;linear programming;medical records;network model;online and offline;patients;pneumonia;signs and symptoms;intensive care unit	Peter J. F. Lucas;Nicolette de Bruijn;Karin Schurink;Andy Hoepelman	2000	Artificial intelligence in medicine	10.1016/S0933-3657(00)00048-8	decision support system;infectious disease;computer science;machine learning;bayesian network;data mining;statistics	AI	2.2762745630099626	-76.0706117620662	147654
ceb5c08de3b774cb99d592fdacf16f14c81ccd79	neural networks versus logistic regression for 30 days all-cause readmission prediction		Heart failure (HF) is one of the leading causes of hospital admissions in the US. Readmission within 30 days after a HF hospitalization is both a recognized indicator for disease progression and a source of considerable financial burden to the healthcare system. Consequently, the identification of patients at risk for readmission is a key step in improving disease management and patient outcome. In this work, we used a large administrative claims dataset to (1)explore the systematic application of neural network-based models versus logistic regression for predicting 30 days all-cause readmission after discharge from a HF admission, and (2)to examine the additive value of patients' hospitalization timelines on prediction performance. Based on data from 272,778 (49% female) patients with a mean (SD) age of 73 years (14) and 343,328 HF admissions (67% of total admissions), we trained and tested our predictive readmission models following a stratified 5-fold cross-validation scheme. Among the deep learning approaches, a recurrent neural network (RNN) combined with conditional random fields (CRF) model (RNNCRF) achieved the best performance in readmission prediction with 0.642 AUC (95% CI, 0.640-0.645). Other models, such as those based on RNN, convolutional neural networks and CRF alone had lower performance, with a non-timeline based model (MLP) performing worst. A competitive model based on logistic regression with LASSO achieved a performance of 0.643 AUC (95%CI, 0.640-0.646). We conclude that data from patient timelines improve 30 day readmission prediction for neural network-based models, that a logistic regression with LASSO has equal performance to the best neural network model and that the use of administrative data result in competitive performance compared to published approaches based on richer clinical datasets.		Ahmed Allam;Máté Nagy;G. Thoma;Michael Krauthammer	2018	CoRR			ML	6.40485054734632	-75.57634240050622	148245
5474e03617dbb226d19b010bcabf98711fe51d87	towards the design of a patient-specific virtual tumour		The design of a patient-specific virtual tumour is an important step towards Personalized Medicine. However this requires to capture the description of many key events of tumour development, including angiogenesis, matrix remodelling, hypoxia, and cell state heterogeneity that will all influence the tumour growth kinetics and degree of tumour invasiveness. To that end, an integrated hybrid and multiscale approach has been developed based on data acquired on a preclinical mouse model as a proof of concept. Fluorescence imaging is exploited to build case-specific virtual tumours. Numerical simulations show that the virtual tumour matches the characteristics and spatiotemporal evolution of its real counterpart. We achieved this by combining image analysis and physiological modelling to accurately described the evolution of different tumour cases over a month. The development of such models is essential since a dedicated virtual tumour would be the perfect tool to identify the optimum therapeutic strategies that would make Personalized Medicine truly reachable and achievable.	angiogenic process;growth factor;hypoxia;image analysis;imaging, three-dimensional;kinetics internet protocol;medicine, east asian traditional;neoplasms;numerical linear algebra;oxygen;patients;rule (guideline);simulation;telerehabilitation;cell growth;interest	Flavien Caraguel;Anne-Cécile Lesart;François Estève;Boudewijn van der Sanden;Angélique Stéphanou	2016		10.1155/2016/7851789	biology;simulation;bioinformatics	Visualization	6.570007953797698	-69.02719820176429	148327
68681b7ab9daaae98f40344c3d82038ea0303ad6	incorporating user needs into product development for improved infection detection for malaria elimination programs	user needs malaria elimination product development;user needs;innovative tools user needs product development improved infection detection malaria elimination programs subclinical infections diagnostic tools target product profile ethnographic interviewing;malaria elimination;diseases communities context interviews testing microscopy reservoirs;product development biotechnology diseases pharmaceutical technology;product development	In malaria-eliminating regions, a large proportion of ongoing transmission is attributed to low-density and subclinical infections that cannot be readily detected by currently available diagnostic tools. Accordingly, passive case detection strategies that dominate the focus of control programs need to be augmented by active infection detection (ID) tactics and more accurate diagnostic tools in an elimination context. To address this need, we are developing a target product profile (TPP) for a diagnostic test intended for use in active ID settings. To ensure the TPP incorporates the needs of users and the contexts in which active ID is implemented, we conducted field research in five representative countries across the spectrum of regional programs ranging from control to elimination. Using ethnographic interviewing, we gained an understanding of the contexts in which the test will be used and the constraints users encounter in successfully conducting active ID efforts. These findings inform the TPP and provide insight into operational conditions that must be addressed in parallel to product development. The results provide clear guidance for programmatic and technical initiatives, ensuring development and validation of innovative tools and tactics aimed at supporting successful elimination campaigns are well grounded in the needs identified by users.	field research;new product development;tpp	Kelly B. Ebels;Christine Clerk;Chris H. Crudder;Sarah McGray;Kendall Magnuson;Kathy Tietje;Paul LaBarre	2014	IEEE Global Humanitarian Technology Conference (GHTC 2014)	10.1109/GHTC.2014.6970338	biology;biotechnology;nanotechnology;immunology	HCI	0.4283304180762591	-69.0556808978417	148539
40955774ef010a12480f687d18eb8b1c5de0aef3	predicting the risk of complications in coronary artery bypass operations using neural networks	coronary artery bypass;neural network	"""Dr. David Shahian Lahey Clinic Burlington, MA 01805 Experiments demonstrated that sigmoid multilayer perceptron (MLP) networks provide slightly better risk prediction than conventional logistic regression when used to predict the risk of death, stroke, and renal failure on 1257 patients who underwent coronary artery bypass operations at the Lahey Clinic. MLP networks with no hidden layer and networks with one hidden layer were trained using stochastic gradient descent with early stopping. MLP networks and logistic regression used the same input features and were evaluated using bootstrap sampling with 50 replications. ROC areas for predicting mortality using preoperative input features were 70.5% for logistic regression and 76.0% for MLP networks. Regularization provided by early stopping was an important component of improved perfonnance. A simplified approach to generating confidence intervals for MLP risk predictions using an auxiliary """"confidence MLP"""" was developed. The confidence MLP is trained to reproduce confidence intervals that were generated during training using the outputs of 50 MLP networks trained with different bootstrap samples."""	bootstrapping (statistics);early stopping;logistic regression;memory-level parallelism;multilayer perceptron;neural networks;quad flat no-leads package;sampling (signal processing);sigmoid function;stochastic gradient descent	Richard Lippmann;Linda Kukolich;David M Shahian	1994			computer science;artificial intelligence;machine learning;pattern recognition;artificial neural network	ML	8.029090878638302	-76.40348778475067	149170
fa96778a9288307a34088407928254dd63968aac	efficient use of phase 1 information in two-phase case-control studies based on administrative databases	study design;secondary data;stratification	Population-based administrative databases are valuable data sources for scientific research. A common problem of studies based on secondary data is, however, missing confounder information. For instance in pharmacoepidemiological studies based on claims data of statutory health insurances, information on the potentially important confounders body mass index (BMI) and smoking behaviour is lacking. Obtaining additional information from another data source, e.g. from survey data, for at least a subsample of the study population resolves this problem. Two-phase designs can be employed for the combined analysis of both data sources. In logistic two-phase studies a dichotomous outcome and some covariate information are available for the whole population in phase 1 whereas the full vector of covariates is only observed for a subsample in phase 2. Information of phase 1 is utilised in the analysis of the phase 2 sample via stratification by phase 1 covariates. This approach allows for an unbiased parameter estimation and can improve efficiency of the estimation of covariates included in the stratification. An additional gain in efficiency can be achieved if the stratification is also used for sampling of the phase 2 data set and the most informative subjects are sampled with a high probability. Two-phase designs have been developed for field studies. Field studies usually comprise very few covariates in phase 1 because ascertainment of each additional covariate for the whole population is associated with additional costs. In these traditional two-phase studies, the stratification is simply defined by cross-classification of all available phase 1 covariates. Two-phase database studies include a multitude of phase 1 covariates. Cross-classification of all available covariates would therefore lead to a tremendously high number of strata and, due to the restricted size of the phase 2 sample, to empty cells. Since two-phase methods cannot be applied in presence of empty cells, new stratification strategies are needed which account for all relevant phase 1 covariates but do not result in empty cells. The aim of this thesis is the development of strategies for the efficient use of phase 1 information in logistic two-phase studies comprising a multitude of phase 1 covariates. An alternative stratification strategy is proposed, which is based on percentiles of a disease score. In this context, the disease score includes as a summary measure information on several phase 1 covariates in the stratification. The core of the thesis is the development of a design criterion which allows the identification of efficient stratifications at the planning stage of the study when only phase 1 information is available. Both the new stratification strategy and the design criterion are applied to an empirical two-phase database study investigating the risk of serious bleedings associated with phenprocoumon exposure. Additionally, the novel approaches are evaluated in a simulation study mimicking the empirical study. It is shown that those stratifications assessed as efficient by the design criterion result in the smallest standard errors for the estimates of most phase 1 covariates in the simulation study. The best stratifications regarding unbiased and efficient parameter estimation are defined by cross-classification of variables used for sampling of the phase 2 data and percentiles of the disease score. Moreover, empirically based recommendations for the planning of future two-phase studies are deduced from the results of the simulation study. An outlook on survey methodological approaches for the analysis of partially missing data completes the thesis. The multiple imputation approach is applied to the empirical study and compared to results of the two-phase analyses. It becomes evident that multiple imputation is much more efficient with respect to the estimation of coefficients for phase 1 covariates than two-phase methods. However, biased estimates are observed in a simulation study, when the imputation model does not sufficiently account for the selective sampling of the phase 2 data. A final assessment of the comparison between two-phase methods and survey methodological approaches is still missing.	brain–computer interface;coefficient;contingency table;database;estimation theory;f1 score;geo-imputation;human body weight;in-phase and quadrature components;information;microsoft outlook for mac;missing data;sampling (signal processing);simulation;two-phase commit protocol;two-phase locking	Sigrid Behr	2013			data science;data mining;database	DB	2.996299535259683	-71.89578959575395	149179
1d69b79fa896437f5255bbebc1bcfd5c8f4c0838	the implementation of growth guidance factor diffusion via octree spatial structures for neuronal systems simulation		The BioDynaMo project was created in CERN OpenLab V and aims to become a general platform for computer simulations for biological research. Important development stage was the code modernization by changing the architecture in a way to utilize multiple levels of parallelism offered by todays hardware. Individual neurons are implemented as spherical (for the soma) and cylindrical (for neurites) elements that have appropriate mechanical properties. The extracellular space is discretized, and allows for the diffusion of extracellular signaling molecules, as well as the physical interactions of the many developing neurons. This paper describes methods of the real-time growing brain dynamics simulation for BioDynaMo project, a specially the implementation of growth guidance factor diffusion via octree spatial structures.	octree;systems simulation	Almaz Sabitov;Fail Gafarov;Vlada Kugurakova;Vitaly Abramov	2017		10.1007/978-3-319-74521-3_18	architecture;extracellular;soma;discretization;theoretical computer science;octree;systems simulation;computer science	Vision	5.134486231662038	-68.45938482376323	149415
2ad51de3e4033be1974d6105477a6df9ba4f51a5	do anesthesiologists know what they are doing? mining a surgical time-series database to correlate expert assessment with outcomes	vital signs;hidden markov model hmm;ordinal regression	Anesthesiologists are taught to carefully manage patient vital signs during surgery. Unfortunately, there is little empirical evidence that vital sign management, as currently practiced, is correlated with patient outcomes. We seek to validate or repudiate current clinical practice and determine whether or not clinician evaluation of surgical vital signs correlate with outcomes. Using a database of over 90,000 cases, we attempt to determine whether those cases that anesthesiologists would subjectively decide are “low quality” are more likely to result in negative outcomes. The problem reduces to one of multi-dimensional time-series classification. Our approach is to have a set of expert anesthesiologists independently label a small number of training cases, from which we build classifiers and label all 90,000 cases. We then use the labeling to search for correlation with outcomes and compare the prevalence of important 30-day outcomes between providers.  To mimic the providers’ quality labels, we consider several standard classification methods, such as dynamic time warping in conjunction with a kNN classifier, as well as complexity invariant distance, and a regression based upon the feature extraction methods outlined by Mao et al. 2012 (using features such as time-series mean, standard deviation, skew, etc.). We also propose a new feature selection mechanism that learns a hidden Markov model to segment the time series; the fraction of time that each series spends in each state is used to label the series using a regression-based classifier. In the end, we obtain strong, empirical evidence that current best practice is correlated with reduced negative patient outcomes. We also learn that all of the experts were able to significantly separate cases by outcome, with higher prevalence of negative 30-day outcomes in the cases labeled as “low quality” for almost all of the outcomes investigated.	best practice;database;dynamic time warping;feature extraction;feature selection;hidden markov model;markov chain;non-repudiation;statistical classification;time series	Risa B. Myers;John C. Frenzel;Joseph R. Ruiz;Chris Jermaine	2016	TKDD	10.1145/2822897	ordinal regression;computer science;machine learning;pattern recognition;data mining	ML	8.262988784756798	-78.54110454120281	149688
6d3e6c0dce7bb084be4f50861c862f925b0be3b8	design strategies of fluorescent biosensors based on biological macromolecular receptors	chemically modified protein based sensors;biosensing techniques;design strategy of fluorescent biosensors;signaling aptamers;proteins;genetically encoded fluorescent biosensors;aptamers nucleotide;fluorescent dyes;affinity labels;fluorescence resonance energy transfer;biological macromolecular receptor;luminescent proteins;selex aptamer technique	Fluorescent biosensors to detect the bona fide events of biologically important molecules in living cells are increasingly demanded in the field of molecular cell biology. Recent advances in the development of fluorescent biosensors have made an outstanding contribution to elucidating not only the roles of individual biomolecules, but also the dynamic intracellular relationships between these molecules. However, rational design strategies of fluorescent biosensors are not as mature as they look. An insatiable request for the establishment of a more universal and versatile strategy continues to provide an attractive alternative, so-called modular strategy, which permits facile preparation of biosensors with tailored characteristics by a simple combination of a receptor and a signal transducer. This review describes an overview of the progress in design strategies of fluorescent biosensors, such as auto-fluorescent protein-based biosensors, protein-based biosensors covalently modified with synthetic fluorophores, and signaling aptamers, and highlights the insight into how a given receptor is converted to a fluorescent biosensor. Furthermore, we will demonstrate a significance of the modular strategy for the sensor design.	biosensors;license;smilax bona-nox;synthetic intelligence;transducer;aptamer;receptor	Kazuki Tainaka;Reiko Sakaguchi;Hironori Hayashi;Shun Nakano;Fong Fong Liew;Takashi Morii	2010		10.3390/s100201355	biochemistry;molecular biology;förster resonance energy transfer;nanotechnology	ML	5.11720944741585	-66.45963341312851	149774
4f0733a64b5b2f6ace02f577d4ab436ccc98990a	prediction of nocturnal hypoglycemia by an aggregation of previously known prediction approaches: proof of concept for clinical application	prediction of nocturnal hypoglycemia;type 1 diabetes;last before bed measurement;aggregation;lbgi	BACKGROUND AND OBJECTIVE Nocturnal hypoglycemia (NH) is common in patients with insulin-treated diabetes. Despite the risk associated with NH, there are only a few methods aiming at the prediction of such events based on intermittent blood glucose monitoring data and none has been validated for clinical use. Here we propose a method of combining several predictors into a new one that will perform at the level of the best involved one, or even outperform all individual candidates.   METHODS The idea of the method is to use a recently developed strategy for aggregating ranking algorithms. The method has been calibrated and tested on data extracted from clinical trials, performed in the European FP7-funded project DIAdvisor. Then we have tested the proposed approach on other datasets to show the portability of the method. This feature of the method allows its simple implementation in the form of a diabetic smartphone app.   RESULTS On the considered datasets the proposed approach exhibits good performance in terms of sensitivity, specificity and predictive values. Moreover, the resulting predictor automatically performs at the level of the best involved method or even outperforms it.   CONCLUSION We propose a strategy for a combination of NH predictors that leads to a method exhibiting a reliable performance and the potential for everyday use by any patient who performs self-monitoring of blood glucose.	branch predictor;calibration (statistics);congenital hyperinsulinism;diabetes mellitus;diabetic neuropathies;exhibits as topic;extraction;glucose;hematological disease;kerrison predictor;mobile app;nethack;object composition;patients;sensitivity and specificity;smartphone;algorithm	Pavlo Tkachenko;Galyna Kriukova;Marharyta Aleksandrova;Oleg Chertov;Eric Renard;Sergei V. Pereverzyev	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.07.003	simulation;delegation;computer science;artificial intelligence;data mining	NLP	7.064421289342176	-75.67134104768628	149918
5ef0a028eda8e76da975ae04ff3aa8cf01fd2eb6	impairment of speech-reading in prosopagnosia	face processing;mimica;traitement signal;identite personnelle;prosopagnosia;short term memory;reconnaissance face;reconocimiento palabra;information source;source information;mimique;speech reading;lecture parole;photography;personal identity;photographie;face recognition;complex system;perception mouvement;signal processing;information processing;fotografia;speech recognition;reconnaissance parole;facial expression;procesamiento senal;identidad personal;fuente informacion;speech perception test	The face is a source of information processed by a complex system of partly independent subsystems. The extent of the independence of processing personal identity, facial expression and facial speech remains at present unclear. We investigated the speech-reading ability of a prosopagnosic patient, LH, who is severely impaired on recognition of personal identity and recognition of facial expressions. Previous reports of such cases raised the possibility that speechreading might still be intact, even if almost all other aspects of face processing are lost. A series of speech-reading tasks were administered to LH including still photographs, video clips, short-term memory tasks for auditory and speechread materials, and tasks aimed at assessing the impact of the visual input on auditory speech recognition. LH was severely impaired on these tasks. We conclude that in LH there is a strong association between severe face processing de®cits and loss of speech-reading skills. Ó 1998 Elsevier Science B.V. All rights reserved.	complex system;information source;lh (complexity);prosopagnosia;speech recognition;video clip	Béatrice de Gelder;Jean Vroomen	1998	Speech Communication	10.1016/S0167-6393(98)00052-1	personal identity;computer vision;speech recognition;information processing;computer science;photography;signal processing;short-term memory;facial expression	AI	-3.1122136055475	-77.33390120770865	150809
fe6e80b4952fc2e553e88e066f62032b4afebb01	towards constructing a new taxonomy for psychiatry using self-reported symptoms		The Diagnostic and Statistical Manual (DSM) has served as the gold standard for psychiatric diagnosis for the past several decades in the USA, and DSM diagnoses mirror mental health and substance abuse diagnoses in ICD-9 and ICD-10. However, DSM diagnoses have severe limitations when used as phenotypes for studies of the pathophysiology underlying mental disorders, as well as for clinical treatment and research. In this paper, we use a novel approach of deconstructing DSM diagnostic criteria, and using expert knowledge to inform feature selection for unsupervised machine learning. We are able to identify clusters of symptoms that stratify subjects with the same DSM disorders into cohorts with increased clinical and biological homogeneity. These findings suggest that itemized self-report symptom data should inform a new taxonomy for psychiatry, and will enhance the bi-directional translation of knowledge from the bench to the clinic through a common terminology.		Jessica Ross;Thomas Neylan;Michael Weiner;Linda Chao;Kristin Samuelson;Ida Sim	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-736	medical diagnosis;data mining;mental health;substance abuse;terminology;psychiatry;chinese classification of mental disorders;medicine	HCI	8.409403444116373	-75.1322466977321	150834
04cbd1b1ac8ba27d479ffbecae8644d899e1433c	a real-time temporal bayesian architecture for event surveillance and its application to patient-specific multiple disease outbreak detection	bayesian network;uncertainty modeling;surveillance system;patient specific model;false negative;decision making process;temporal disease outbreak detection;network architecture;biosurveillance;disease outbreak;mining ed chief complaint data	Reliable and accurate detection of disease outbreaks remains an important research topic in disease outbreak surveillance. A temporal surveillance system bases its analysis on data not only from the most recent time period, but also on data from previous time periods. A non-temporal system only looks at data from the most recent time period. There are two difficulties with a non-temporal system when it is used to monitor real data which often contain noise. First, it is prone to produce false positive signals during non-outbreak time periods. Second, during an outbreak, it tends to release false negative signals early in the outbreak, which can adversely affect the decision making process of the user of the system. We conjecture that by converting a non-temporal system to a temporal one, we may attenuate these difficulties inherent in a non-temporal system. In this paper, we propose a Bayesian network architecture for a class of temporal event surveillance models called BayesNet-T. Using this Bayesian network architecture, we can convert certain non-temporal surveillance systems to temporal ones. We apply this architecture to a previously developed non-temporal multiple-disease outbreak detection system called PC and create a temporal system called PCT. PCT takes Emergency Department (ED) patient chief complaint data as its input. The PCT system was constructed using both data (non-outbreak diseases) and expert assessments (outbreak diseases). We compare PCT to PC using a real influenza outbreak. Furthermore, we compare PCT to both PC and the classic statistical methods CUSUM and EWMA using a total of 240 influenza and Cryptosporidium disease outbreaks created by injecting stochastically simulated outbreak cases into real ED admission data. Our results indicate that PCT has a smaller mean time to detection than PC at low false alarm rates, and that PCT is more stable than PC in that once an outbreak is detected, PCT is better at maintaining the detection signal on future days.	bayesian network;network architecture;personal computer;real-time clock	Xia Jiang;Gregory F. Cooper	2009	Data Mining and Knowledge Discovery	10.1007/s10618-009-0151-4	decision-making;simulation;network architecture;computer science;machine learning;bayesian network;data mining;outbreak;operations research	ML	6.341400584350413	-73.20377117889647	151220
67753e4f5f3ba9e28cfe5e732dbbd0dfc41c69fb	data based modelling of expired airflow clarifies chronic obstructive pulmonary disease		One of the major health challenges of the future is Chronic Obstructive Pulmonary Disease (COPD). It is characterized by airflow limitations, although current diagnosis does not give attention to the flow measurements. We aimed to develop a data-based model of the decline of the forced expiratory flow. Moreover, we analysed the relationship of model parameters with COPD presence and its severity. The data-based model was developed in 474 smoking individuals, who are at risk of having COPD, and have performed complete pulmonary function tests in order to identify whether the disease is present and at which stage. The time series of the decline of the flow was parameterised using the poles and steady state gain (SSG) of a second order transfer function model. These parameters were then linked with the presence of COPD. Observing SSG, median (IQR) in subjects with COPD was lower 3.9(2.7-5.6) compared to 8.2(7.1-9.3) in subjects without, (p<0.0001). Significant difference was also found when observing median (IQR) of two poles in subjects without disease were 0.9868(0.9810-0.9892) and 0.9333(0.9010-0.9529), respectively, compared to 0.9929(0.9901-0.9952) and 0.9082(0.8669-0.9398) in subjects with COPD (p<0.001 for both poles). Forced exhaled air can be used to expand understanding of the COPD. Moreover, the suggested parameterisation of the flow decline could be used to access COPD using spirometry.	ct pulmonary angiogram;function model;steady state;symbolic stream generator;time series;transfer function	Marko Topalovic;Vasileios Exadaktylos;Thomas Deckers;Thierry Troosters;Marc Decramer;Daniel Berckmans;Wim Janssens	2014		10.5220/0004735000050012	pulmonary function testing;spirometry;airflow;disease;physical therapy;copd;medicine	HCI	9.5680933553347	-79.25967619012091	151365
94d6e9787879dddd2449a98aa16151b9d08b3d78	tuberculosis diagnosis support analysis for precarious health information systems	artificial neural networks (ann);diagnosis support systems;multilayer perceptron (mlp);public health;self-organizing maps (som);tuberculosis diagnosis	BACKGROUND AND OBJECTIVE Pulmonary tuberculosis is a world emergency for the World Health Organization. Techniques and new diagnosis tools are important to battle this bacterial infection. There have been many advances in all those fields, but in developing countries such as Colombia, where the resources and infrastructure are limited, new fast and less expensive strategies are increasingly needed. Artificial neural networks are computational intelligence techniques that can be used in this kind of problems and offer additional support in the tuberculosis diagnosis process, providing a tool to medical staff to make decisions about management of subjects under suspicious of tuberculosis.   MATERIALS AND METHODS A database extracted from 105 subjects with precarious information of people under suspect of pulmonary tuberculosis was used in this study. Data extracted from sex, age, diabetes, homeless, AIDS status and a variable with clinical knowledge from the medical personnel were used. Models based on artificial neural networks were used, exploring supervised learning to detect the disease. Unsupervised learning was used to create three risk groups based on available information.   RESULTS Obtained results are comparable with traditional techniques for detection of tuberculosis, showing advantages such as fast and low implementation costs. Sensitivity of 97% and specificity of 71% where achieved.   CONCLUSIONS Used techniques allowed to obtain valuable information that can be useful for physicians who treat the disease in decision making processes, especially under limited infrastructure and data.		Alvaro David Orjuela-Cañón;Jorge Eliécer Camargo Mendoza;Carlos Enrique Awad García;Erika Paola Vergara Vela	2018	Computer methods and programs in biomedicine	10.1016/j.cmpb.2018.01.009	data science;supervised learning;computer vision;artificial intelligence;computer science;tuberculosis;disease;suspect;tuberculosis diagnosis;health informatics;computational intelligence;unsupervised learning	AI	5.773587704699291	-77.73367866328503	151435
6421d0c23a20ca20c8a408a2675f3a85c4653945	iso-risk air no decompression limits after scoring marginal decompression sickness cases as non-events	dcs;decompression sickness;deep sea diving;gas content;hyperbaric;inert gas;marginal	Decompression sickness (DCS) in humans is associated with reductions in ambient pressure that occur during diving, aviation, or certain manned spaceflight operations. Its signs and symptoms can include, but are not limited to, joint pain, radiating abdominal pain, paresthesia, dyspnea, general malaise, cognitive dysfunction, cardiopulmonary dysfunction, and death. Probabilistic models of DCS allow the probability of DCS incidence and time of occurrence during or after a given hyperbaric or hypobaric exposure to be predicted based on how the gas contents or gas bubble volumes vary in hypothetical tissue compartments during the exposure. These models are calibrated using data containing the pressure and respired gas histories of actual exposures, some of which resulted in DCS, some of which did not, and others in which the diagnosis of DCS was not clear. The latter are referred to as marginal DCS cases. In earlier works, a marginal DCS event was typically weighted as 0.1, with a full DCS event being weighted as 1.0, and a non-event being weighted as 0.0. Recent work has shown that marginal DCS events should be weighted as 0.0 when calibrating gas content models. We confirm this indication in the present work by showing that such models have improved performance when calibrated to data with marginal DCS events coded as non-events. Further, we investigate the ramifications of derating marginal events on model-prescribed air diving no-stop limits.		F. Gregory Murphy;Ashleigh Swingler;Wayne A. Gerth;Laurens E. Howle	2018	Computers in biology and medicine	10.1016/j.compbiomed.2017.11.012	artificial intelligence;pattern recognition;computer science;internal medicine;general malaise;decompression sickness;decompression;surgery;cardiology	NLP	7.3081185527148635	-72.9399050370005	151948
78a535cf602f95bc52868aa66422135aea00ef2b	case study of modeling immune system with b method	biology computing;formal specification;glycoprotein;b method;proteins;proteins biology computing formal specification cellular biophysics;computer aided software engineering immune system information technology;laboratory techniques;immune system;immunology immune system modeling b method immunological research cytokine network small protein molecules glycoprotein messenger molecules immune response cytokine interactions t cell cytokine networks b toolkit;veterinary medicine;cellular biophysics;immune response;dynamic behavior	An illustrative example for the current situation in immunological research is the case of cytokine networks. Cytokines are small protein or glycoprotein messenger molecules that convey information from one cell to another. Various aspects of the immune response are regulated by cytokine networks. Although many details of particular cytokine interactions have been elucidated, practically nothing is known about the behavior of the network as a whole. In our work, we have modeled aspects of T-cell cytokine networks using B method. With this model, we are able to run simulation with B-toolkit and allow us to compare the dynamic behavior of the model to actual experimental data from College of Animal Science and Veterinary Medicine. Our results show that the use of B method can help confront open questions in immunology and probably in other fields of biology, which, because of their complexity, cannot be addressed by standard laboratory techniques alone.	b-method;cell (microprocessor);interaction;simulation	Shengrong Zou	2004	The Fourth International Conference onComputer and Information Technology, 2004. CIT '04.	10.1109/CIT.2004.1357308	immune system;computer science;bioinformatics;artificial immune system	Robotics	4.461472490125964	-67.12942047786667	152383
d70ce2ce0a79f6e3b3e4fc0ab1ea507798c8677d	network-based drug discovery by integrating systems biology and computational technologies	software;drug discovery;systems biology;databases factual;proteomics;computational biology	Network-based intervention has been a trend of curing systemic diseases, but it relies on regimen optimization and valid multi-target actions of the drugs. The complex multi-component nature of medicinal herbs may serve as valuable resources for network-based multi-target drug discovery due to its potential treatment effects by synergy. Recently, robustness of multiple systems biology platforms shows powerful to uncover molecular mechanisms and connections between the drugs and their targeting dynamic network. However, optimization methods of drug combination are insufficient, owning to lacking of tighter integration across multiple '-omics' databases. The newly developed algorithm- or network-based computational models can tightly integrate '-omics' databases and optimize combinational regimens of drug development, which encourage using medicinal herbs to develop into new wave of network-based multi-target drugs. However, challenges on further integration across the databases of medicinal herbs with multiple system biology platforms for multi-target drug optimization remain to the uncertain reliability of individual data sets, width and depth and degree of standardization of herbal medicine. Standardization of the methodology and terminology of multiple system biology and herbal database would facilitate the integration. Enhance public accessible databases and the number of research using system biology platform on herbal medicine would be helpful. Further integration across various '-omics' platforms and computational tools would accelerate development of network-based drug discovery and network medicine.	algorithm;combinational logic;computation (action);computational model;drug combinations;drug delivery systems;drug discovery;lupus erythematosus, systemic;mathematical optimization;medical marijuana;medicinal herbs;network medicine;nomenclature;numerous;omics;published database;synergy;systems biology;cellular targeting;drug development;uncertain reliability;width	Elaine L. Leung;Zhi-Wei Cao;Zhi-Hong Jiang;Hua Zhou;Liang Liu	2013		10.1093/bib/bbs043	pharmacology;biology;computer science;bioinformatics;proteomics;systems biology;drug discovery	Comp.	-0.4054988358295181	-66.90937332274926	152415
efcdb4ffd6a053aa91c466eadd97317d135d2928	a visualization tool of 3-d time-varying data for the simulation of tissue growth	3 d simulation model;visualization;time varying data;cellular automata;tissue growth	Data Visualization affords us the ability to explore the spatial and temporal domains of many time-varying phenomena. In this article, we describe our application of visualization to a three-dimensional simulation model for tissue growth. We review the different components of the model where cellular automata is used to model populations of cells that execute persistent random walks, collide, and proliferate until they reach confluence. We then describe the system architecture of the developed visualization tool, the employed rendering techniques, and the related prototyping interfaces. We also discuss some of the visualization results obtained thus far that are pertinent to enhancing the validity of the computational model. This visualization tool could be useful in facilitating the research of scientists by providing them with meaningful means to interpret and analyze simulation data and to compare them to experimental results. Our objective in this work is to develop computer-aided design solutions that support the simulation of tissue growth and its design exploration.	algorithm;automata theory;cellular automaton;computation;computational model;computer-aided design;confluence;data visualization;design space exploration;emoticon;experiment;graphics processing unit;high- and low-level;interpretation (logic);population model;relevance;simulation;systems architecture;undo;usability;volume rendering	Belgacem Ben Youssef	2013	Multimedia Tools and Applications	10.1007/s11042-013-1657-8	cellular automaton;simulation;visualization;computer science;theoretical computer science;algorithm	Visualization	5.303285444906555	-68.76031966710653	152549
9883bac04ea2688931c0def7f9d7d6d93dc10aa6	a multi-agents approach to knowledge discovery	chronic disease data mining multi agent;biology computing;chronic disease;distributed processing;training;diabetes;data mining;multi agent;medical computing;accuracy;multi agent systems;classification algorithms;distributing agent based processing knowledge discovery predictive modeling complex e health system multiagent system chronic disease data modeling data partitioning heterogeneous mining homogenous data mining;multi agent systems biology computing data mining distributed processing medical computing;data mining diabetes diseases databases intelligent agent partitioning algorithms multiagent systems aging australia blood;algorithm design and analysis;multiagent systems	Over the past few years, data mining and multi-agent approach has been used successfully in the development of large complex systems. Such a hybrid approach can be considered as an effective approach for the development of predictive modeling in complex e-health systems. We propose a real time Data Mining and Multi-Agent System called DMMAS, modeling chronic disease data. DMMAS approach employs data partitioning and multiple agents with option to employ heterogeneous or homogenous data mining techniques, distributing agent based processing for modeling and combining results from all the agents to improve the efficiency.	agent-based model;complex systems;data mining;multi-agent system;predictive modelling	Cuong Tong;Dharmendra Sharma;Fariba Shadabi	2008	2008 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology	10.1109/WIIAT.2008.418	algorithm design;computer science;bioinformatics;artificial intelligence;data science;multi-agent system;data mining;accuracy and precision	AI	3.3051920264129517	-77.19406708851633	152954
f6bff4e20c4ddb40245d82c570ed038c172a64be	control-relevant erythropoiesis modeling in end-stage renal disease	proteins blood cellular biophysics diseases lung molecular biophysics patient treatment;production mathematical model diseases data models estimation physiology training;lung;proteins;blood;control relevant erythropoiesis modeling clinical data physiological relevant erythropoiesis model model based feedback control theory hemoglobin measurements anemia management protocols rhuepo therapy blood transfusions dialysis anemia patient treatment recombinant human erythropoietin end stage renal disease;molecular biophysics;diseases;patient treatment;recombinant human erythropoietin rhuepo anemia of chronic kidney disease ckd end stage renal disease esrd pharmacokinetic pharmacodynamic modeling;cellular biophysics	Anemia is prevalent in end-stage renal disease (ESRD). The discovery of recombinant human erythropoietin (rHuEPO) over 30 years ago has shifted the treatment of anemia for patients on dialysis from blood transfusions to rHuEPO therapy. Many anemia management protocols (AMPs) used by clinicians comprise a set of experience-based rules for weekly-to-monthly titration of rHuEPO doses based on hemoglobin (Hb) measurements. In order to facilitate the design of an AMP using model-based feedback control theory, we present a physiologically relevant erythropoiesis model and demonstrate its applicability using clinical data.	anemia;anemia, hemolytic;blood transfusion;control theory;erythropoiesis;feedback;hemoglobin sc disease;kidney diseases;kidney failure, chronic;patients;protocols documentation;recombinant dna;rule (guideline);titration method	Yossi Chait;Joseph Horowitz;Brendan Nichols;Rajiv P. Shrestha;Christopher V. Hollot;Michael J. Germain	2014	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2013.2286100	fox proteins;intensive care medicine;medicine;immunology;diabetes mellitus;molecular biophysics	Visualization	8.829128181880208	-69.57172173293341	153037
65cffc49605f4d26b9974314ba0247b2c122faeb	the ‘analgoscore’: a novel score to monitor intraoperative pain and its use for remifentanil closed-loop application	remifentanil infusion;hemodynamic stability;intraoperative pain monitoring;general anesthesia;closed loop systems;standard deviation;fuzzy control;haemodynamics;objective score;fuzzy logic;remifentanil closed loop;hemodynamic stability analgoscore intraoperative pain monitoring remifentanil closed loop pain measurement general anesthesia objective score mean arterial pressure heart rate fuzzy logic algorithms pain control remifentanil infusion;heart rate;analgoscore;mean arterial pressure;pain heart rate blood pressure surgery anesthesia biomedical engineering biomedical monitoring patient monitoring hemodynamics stability;pain measurement;medical control systems closed loop systems fuzzy control haemodynamics;fuzzy logic algorithms;pain control;medical control systems	Purpose: measuring pain during general anesthesia is difficult because communication with the patient is impossible. The focus of this project is the development of an objective score ('Analgoscore') of intraoperative pain based on mean arterial pressure (MAP) and heart rate (HR). The Analgoscore was used for closed-loop application of remifentanil. Methods: based on fuzzy logic algorithms, the Analgoscore ranges from -9 (too profound analgesia) to 9 (too little analgesia) in increments of 1, with -3 to + 3 representing excellent pain control, -3 to -6 and 3 to 6 good pain control, and -6 to -9 and 6 to 9 as insufficient pain control. According to the zone of pain, a remifentanil infusion was closed-loop-administered. The percentage of anesthetic time within the different control zones was recorded as well as the variability of MAP and HR Data as means plusmn standard deviation. Results: Sixteen patients (5 f, 11 m; age 49 plusmn 21 y; weight 70 plusmn 11) underwent anesthesia of a total time of 1772 min, and received a mean dose of remifentanil of 0.13 plusmn 0.08 mug/kg/min. During 84%, 14% and 0.5% of the total anesthesia time, the Analgoscore showed excellent, good or insufficient pain control, respectively. During 70% of the time, MAP ranged from -5% to 5%, during 21% of the time it ranged from - 10% to -5% and from 5% to 10% and during 9% of the time, it ranged from -20% to -10% and from 10% to 20% below or above the targeted values. HR was within 10% of target value in 99% of the total anesthesia time. Discussion: The Analgoscore is a novel score of intraoperative pain. Remifentanil was successfully closed-loop-administered and, controlled by the Analgoscore, excellent hemodynamic stability was achieved.	algorithm;fuzzy logic;hemodynamics;maxima and minima;spatial variability	Thomas M. Hemmerling;Emile Salhab;Germain Aoun;Samer Charabati;Pierre A. Mathieu	2007	2007 IEEE International Conference on Systems, Man and Cybernetics	10.1109/ICSMC.2007.4414030	fuzzy logic;computer science;artificial intelligence;hemodynamics;standard deviation;fuzzy control system;statistics	Robotics	6.477257673997142	-79.7976035843146	153239
b08c214d2a8a8f4c8743e55bf90f9e8507ae870d	prediction of stroke using deep learning model		Many predictive techniques have been widely applied in clinical decision making such as predicting occurrence of a disease or diagnosis, evaluating prognosis or outcome of diseases and assisting clinicians to recommend treatment of diseases. However, the conventional predictive models or techniques are still not effective enough in capturing the underlying knowledge because it is incapable of simulating the complexity on feature representation of the medical problem domains. This research reports predictive analytical techniques for stroke using deep learning model applied on heart disease dataset. The atrial fibrillation symptoms in heart patients are a major risk factor of stroke and share common variables to predict stroke. The outcomes of this research are more accurate than medical scoring systems currently in use for warning heart patients if they are likely to develop stroke.	deep learning	Pattanapong Chantamit-o-pas;Madhu Goyal	2017		10.1007/978-3-319-70139-4_78	heart disease;artificial intelligence;medical emergency;pattern recognition;risk factor;deep learning;computer science;disease;atrial fibrillation;stroke	NLP	5.614744873691152	-76.57811954189741	153698
2f5160de1c8ed342de83dca69fb7dd927fe1b044	finding temporal patterns in noisy longitudinal data: a study in diabetic retinopathy	frequent pattern;frequent pattern mining;temporal pattern mining;qa75 electronic computers computer science;diabetic retinopathy;temporal pattern;data warehousing;longitudinal data;r medicine general;trend mining	This paper describes an approach to temporal pattern mining using the concept of user defined temporal prototypes to define the nature of the trends of interests. The temporal patterns are defined in terms of sequences of support values associated with identified frequent patterns. The prototypes are defined mathematically so that they can be mapped onto the temporal patterns. The focus for the advocated temporal pattern mining process is a large longitudinal patient database collected as part of a diabetic retinopathy screening programme, The data set is, in itself, also of interest as it is very noisy (in common with other similar medical datasets) and does not feature a clear association between specific time stamps and subsets of the data. The diabetic retinopathy application, the data warehousing and cleaning process, and the frequent pattern mining procedure (together with the application of the prototype concept) are all described in the paper. An evaluation of the frequent pattern mining process is also presented.	data mining;expectation propagation;missing data;plasma cleaning;prototype;software prototyping;sputter cleaning	Vassiliki Somaraki;Deborah Broadbent;Frans Coenen;Simon Peter Harding	2010		10.1007/978-3-642-14400-4_32	computer science;data science;data warehouse;data mining;database	ML	2.5928016301231014	-76.89342213265616	153716
6e0dc6aa304e9730999dcaf358e08c2d8546a28b	machine learning models to predict readmission for patients with cirrhosis			machine learning	Jejo D. Koola;Aize Cao;Guanhua Chen;Amy Perkins;Samuel B. Ho;Sharon E. Davis;Michael E. Matheny	2017			cirrhosis;artificial intelligence;pattern recognition;computer science	ML	7.473984331119599	-77.36092630716826	153927
75c22b1613d4bce66c5173bd5e328ef3cb5b2187	development of a micro-simulation model for breast cancer to evaluate the impacts of personalized early detection strategies		Breast cancer screening with mammography has been shown to re- duce breast cancer mortality. However the frequency and the age range for screening eligibility has been controversial. Individual risk based screening re- gimens have recently been proposed to overcome some of the weaknesses of screening mammography. However, it is not possible to evaluate the full impact of such risk based individualized screening strategies in Canadian context. Therefore a mathematical cancer control model for breast cancer using care paths and cancer control data from the province of BC is being developed to model different early detection strategies. The model will incorporate the inci- dence, detection, diagnosis, progression, and case fatality of breast cancer in BC as baseline to make projections of the population health and economic impacts of different early detection methods for breast cancer. Once the model is vali- dated, it will be possible to test early detection pathways and strategies, frequencies and durations, as well as any health care costs associated with de- tection, diagnosis, treatment and on-going care of breast cancer patients.		Rasika Rajapakshe;Cynthia Araujo;Chelsea Vandenberg;Brent Parker;Stephen Smithbower;Chris Baliski;Susan L Ellard;Laurel Kovacic;Melanie Reed;Scott Tyldesley;Gillian Fyles;Rebecca Mlikotic	2014		10.1007/978-3-319-07887-8_52	medicine;pathology;gynecology;surgery	NLP	7.2289386205360575	-74.96860792558307	155079
b89dea7a37d248554d0d708744ac86acd8365dd5	probabilistic reasoning with a bayesian dna device based on strand displacement	genetic diagnosis;bayesian inference;dna computing	We present a computing model based on the DNA strand displacement technique, which performs Bayesian inference. The model will take single-stranded DNA as input data, that represents the presence or absence of a specific molecular signal (evidence). The program logic encodes the prior probability of a disease and the conditional probability of a signal given the disease affecting a set of different DNA complexes and their ratios. When the input and program molecules interact, they release a different pair of single-stranded DNA species whose ratio represents the application of Bayes’ law: the conditional probability of the disease given the signal. The models presented in this paper can have the potential to enable the application of probabilistic reasoning in genetic diagnosis in vitro.	autonomous robot;bayesian network;displacement mapping;encode;experiment;expert system;genetic algorithm;naive bayes classifier;scalability;strand (programming language)	Iñaki Sainz de Murieta;Alfonso Rodríguez-Patón	2012	Natural Computing	10.1007/s11047-013-9406-5	computer science;bioinformatics;machine learning;bayesian network;bayesian statistics;bayesian inference;dna computing;statistics	AI	1.8202089974768336	-66.54597548091006	155593
8beb199171c0296179c76dd6749bcc2f2ec30cc9	after turing: mathematical modelling in the biomedical and social sciences - from animal coat patterns to brain tumours to saving marriages	accurate model;reaction diffusion model;early study;mathematical modelling;modelling example;social science;gliomablastoma brain tumour;new scientific marital therapy;marital interaction;early development;current patient brain tumour	Turing's 1952 paper on reaction diffusion models for spatial pattern formation was important in the early development of the application of mathematical modelling in biology and medicine. We describe here three very different problems which have been studied in depth and which have proved informative and useful in understanding specific phenomena. We describe an early study of a reaction diffusion model which helped explain the diverse coat patterns observed on animal coats. We then describe a basic, but surprisingly informative and accurate model, currently used medically, for quantifying the growth of gliomablastoma brain tumours. It enhances imaging techniques beyond any brain scanning procedure currently available and is used to estimate patient life expectancy and explain some current patient brain tumour anomalies. Finally we describe a modelling example from the social sciences, which quantifies marital interaction which was used to predict divorce with surprising accuracy and has helped design a new scientific marital therapy which is currently used.	mathematical model;turing	James D. Murray	2012		10.1007/978-3-642-30870-3_52	computer science;artificial intelligence	Vision	6.481775335213795	-69.72693529088481	156038
4576355929d0f5385854d8f214d60e084c27440b	prognostic prediction of putaminal hemorrhage by nonlinear multivariate analysis				Yasuo Iida;Kei Takahashi;Yasunari Ohtawara;Kiyoshi Kuroda;Yasuhide Miura	2001		10.3233/978-1-60750-928-8-994	putaminal hemorrhage;multivariate analysis;radiology;medicine	ML	9.913282364742303	-79.68532981008059	156082
f953207bf63c5cbebb023d0bdf6634cf948db053	identification of risk factors in coronary bypass surgery		In quality improvement in medical care one important aim is to prevent complications after a surgery and, particularly, keep the mortality rate as small as possible. Therefore it is of great importance to identify which factors increase the risk to die in the aftermath of a surgery. Based on data of 1,163 patients who underwent an isolated coronary bypass surgery in 2007 or 2008 at the Clinic of Cardiovascular Surgery in Dusseldorf, Germany, we select predictors that affect the in-hospital-mortality. A forward search using the wrapper approach in conjunction with simple linear and also more complex classification methods such as gradient boosting and support vector machines is performed. Since the classification problem is highly imbalanced with certainly unequal but unknown misclassification costs the area under ROC curve (AUC) is used as performance criterion for hyperparameter tuning as well as for variable selection. In order to get stable results and to obtain estimates of the AUC the variable selection is repeated 25 times on different subsamples of the data set. It turns out that simple linear classification methods (linear discriminant analysis and logistic regression) are suitable for this problem since the AUC cannot be considerably increased by more complex methods. We identify the three most important predictors as the severity of cardiac insufficiency, the patient’s age as well as pulmonary hypertension. A comparison with full models trained on the same 25 subsamples shows that the classification performance in terms of AUC is not affected or only slightly decreased by variable selection.		Julia Schiffner;Erhard Godehardt;Stefanie Hillebrand;Alexander Albert;Artur Lichtenberg;Claus Weihs	2013		10.1007/978-3-319-00035-0_29	statistics;gradient boosting;support vector machine;logistic regression;random forest;feature selection;hyperparameter;mathematics;linear classifier;linear discriminant analysis	ML	6.976574527460243	-76.67059492748069	156278
c47d91ee3e345084a93ac8d003367c094ce47f17	processing neurology clinical data for knowledge discovery: scalable data flows using distributed computing	apache pig;neurology;distributed computing;epileptic seizure networks;electrophysiological signal data;clinical research	The rapidly increasing capabilities of neurotechnologies are generating massive volumes of complex multi-modal data at a rapid pace. This neurological big data can be leveraged to provide new insights into complex neurological disorders using data mining and knowledge discovery techniques. For example, electrophysiological signal data consisting of electroencephalogram (EEG) and electrocardiogram (ECG) can be analyzed for brain connectivity research, physiological associations to neural activity, diagnosis, and care of patients with epilepsy. However, existing approaches to store and model electrophysiological signal data has several limitations, which make it difficult for signal data to be used directly in data analysis, signal visualization tools, and knowledge discovery applications. Therefore, use of neurological big data for secondary analysis and potential development of personalized treatment strategies requires scalable data processing platforms. In this chapter, we describe the development of a high performance data flow system called Signal Data Cloud (SDC) to pre-process large-scale electrophysiological signal data using open source Apache Pig. The features of this neurological big data processing system are: (a) efficient partitioningof signal data into fixed size segments for easier storage in high performance distributed file system, (b) integration and semantic annotation of clinical metadata using an epilepsy domain ontology, and (c) transformation of raw signal data into an appropriate format for use in signal analysis platforms. In this chapter, we also discuss the various challenges being faced by the biomedical informatics community in the context of Big Data, especially the increasing need to ensure data quality and scientific reproducibility.		Satya S. Sahoo;Annan Wei;Curtis Tatsuoka;Kaushik Ghosh;Samden D. Lhatoo	2016		10.1007/978-3-319-50478-0_15	neuroscience;computer science;data science;theoretical computer science	ML	-2.4006915575771304	-71.27417671877343	156411
aa6777307b7d97433324244b9f2da691acc44940	possibility to use ips-technology in age-related diseases	ips technology;age 1.25 yr to 1.75 yr;specific cell types;mammal aging;age related diseases;cellular biophysics;gmcsf;diseases;elderly patients;ipsc clones;gene therapy;geriatrics;tissue engineering;regenerative therapy;chimeric mice;bone;clone pluripotent markers;molecular biophysics;c57bl-6 mice;in vitro germ layers;aged icr mice;bone marrow cells;mouse model;granulocyte macrophage colony stimulating factor;ipsc differentiation;aged ipsc-1 mice;transplantation treatments;in vivo teratoma;____________	When applying the iPSCs for regenerative therapy in elderly patients, it is necessary to establish the iPSCs from elderly patients themselves then differentiate the iPSCs to specific cell types for transplantation treatments. The mouse is a perfect model to study aging in mammals. Bone marrow (BM) cells from aged C57BL/6 mice (15 months to 21 months) were cultured with granulocyte macrophage-colony stimulating factor (GM-CSF). The efficiency to produce iPSCs from aged mice BM cells was lower than that of young mice. We established several clones of iPSCs from aged mice. All the established clones have pluripotent markers. Aged-iPSCs could differentiate to three germ layers in vitro and made teratoma in vivo. We are currently making chimeric mice between Aged-iPSC-1 and ICR mice.	bone scintigraphy;closed-circuit television;intelligent character recognition;video-in video-out	Zhao Cheng;Sachiko Ito;Naomi Nishio;Thanasegaran Suganya;Ken-ichi Isobe	2012	2012 International Symposium on Micro-NanoMechatronics and Human Science (MHS)	10.1109/MHS.2012.6492465	biology;pathology;biological engineering;immunology	Arch	5.611461507030357	-66.65572554635148	156581
805455cb4e7b03932c078371ee4cf93360917e20	an algorithm for approximating dirichlet tessellations (abstract only)	neural nets;language classification;pattern recognition;comparative linguistics	"""Dirichlet Tessellations can be defined as the partitioning of a given area with a finite set of unique points, such that each partition only has one unique point and that all locations inside each partition is closest to that unique point. Unique points that share common boundaries are considered """"neighbors"""". Dirichlet Tessellations, used in Pat tern Recognition for dot pattern analysis allows classification of dots in two ways: first, by that location of each point, and second by the neighborhood, or group of neighbors of each point. The following are discussed:"""	algorithm;pattern recognition	Joseph S. Szakas;Christian Trefftz	1991		10.1145/327164.328859	natural language processing;comparative linguistics;computer science;machine learning;pattern recognition	Vision	-4.083287058426434	-70.53798951641156	156622
72d43a4afb790d50be2c1bc55e6e2dde7f21932e	the virtual cell	arsenic;chemical kinetics;scientific method;modeling and simulation;image processing;biological process	This paper describes a computational framework for cell biological modeling and simulation that is based on the mapping of experimental biochemical and electrophysiological data onto experimental images. The framework is designed to enable the construction of complex general models that encompass the general class of problems coupling reaction and diffusion.	simulation;virtual cell	James C. Schaff;Leslie M. Loew	1999	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		biology	Embedded	5.0283808076149015	-67.84082823219279	156985
28ab04748928326422231e0ce51103200467dbbb	a genetic algorithm approach to multi-disorder diagnosis	search method;set theory;multiple disorders;genetic algorithm;differential diagnosis;genetic algorithms;medical diagnosis;knowledge base;expert system	One of the common limitations of expert systems for medical diagnosis is that they make an implicit assumption that multiple disorders do not co-occur in a single patient. The need for this simplifying assumption stems from the fact that finding minimal sets of disorders that cover all symptoms for a given patient is generally computationally intractable (NP-hard). In this paper, we explain the need for performing multi-disorder diagnosis, review previous approaches, formulate the problem using set theory notation, and propose the use of a search method based on a genetic algorithm. We test the algorithm and compare it to another approach using a simple example. The genetic algorithm performs well independently of the order of symptoms, and has the potential to perform multi-disorder diagnosis using existing or newly developed knowledge bases.	attention deficit hyperactivity disorder;base;computational complexity theory;expert systems;expert system;genetic algorithm;knowledge bases;np-hardness;numerous;patients;set theory;multiple pathologies;notation	Staal A. Vinterbo;Lucila Ohno-Machado	2000	Artificial intelligence in medicine	10.1016/S0933-3657(99)00036-6	knowledge base;genetic algorithm;computer science;artificial intelligence;machine learning;expert system;algorithm	AI	0.6341866746631614	-75.64275544435377	157240
3dc09993b94f1a79b2e776eb41360b0965d32b7e	identifying subcellular locations from images of unknown resolution	machine learning	Our group has previously used machine learning techniques to develop computational systems to automatically analyse fluorescence microscope images and classify the location of the depicted protein. Based on this work, we developed a system, the Subcellular Location Image Finder (slif), which mines images from scientific journals for analysis. For some of the images in journals, the system is able to automatically compute the pixel resolution (the physical space represented by each pixel), by identifying a scale bar and processing the caption text. However, scale bars are not always included. For those images, the pixel resolution is unknown. Blindly feeding these images into the classification pipeline results in unacceptably low accuracy. We first describe methods that minimise the impact of this problem by training resolution-insensitive classifiers. We show that these techniques are of limited use as classifiers can only be made insensitive to resolutions which are similar to each other. We then approach the problem in a different way by trying to estimate the resolution automatically and processing the image based on this prediction. Testing on digitally down-sampled images shows that the combination of these two approaches gives classification results which are essentially as good as if the resolution had been known.	image resolution;linear scale;machine learning;pixel;scale space;statistical classification	Luís Pedro Coelho;Robert F. Murphy	2008		10.1007/978-3-540-70600-7_18	computer vision;computer science;bioinformatics;machine learning;data mining	ML	-4.439448719654136	-66.27721365756125	157682
1c60e3cbd5f17bcbfd56eedcab4064f002c9fbb9	role of genetic heterogeneity and epistasis in bladder cancer susceptibility and outcome: a learning classifier system approach	urinary bladder neoplasms;epistasis genetic;individualized medicine;classification;genetic predisposition to disease;genetic heterogeneity;artificial intelligence;algorithms;humans;computational biology;polymorphism single nucleotide	BACKGROUND AND OBJECTIVE Detecting complex patterns of association between genetic or environmental risk factors and disease risk has become an important target for epidemiological research. In particular, strategies that provide multifactor interactions or heterogeneous patterns of association can offer new insights into association studies for which traditional analytic tools have had limited success.   MATERIALS AND METHODS To concurrently examine these phenomena, previous work has successfully considered the application of learning classifier systems (LCSs), a flexible class of evolutionary algorithms that distributes learned associations over a population of rules. Subsequent work dealt with the inherent problems of knowledge discovery and interpretation within these algorithms, allowing for the characterization of heterogeneous patterns of association. Whereas these previous advancements were evaluated using complex simulation studies, this study applied these collective works to a 'real-world' genetic epidemiology study of bladder cancer susceptibility.   RESULTS AND DISCUSSION We replicated the identification of previously characterized factors that modify bladder cancer risk--namely, single nucleotide polymorphisms from a DNA repair gene, and smoking. Furthermore, we identified potentially heterogeneous groups of subjects characterized by distinct patterns of association. Cox proportional hazard models comparing clinical outcome variables between the cases of the two largest groups yielded a significant, meaningful difference in survival time in years (survivorship). A marginally significant difference in recurrence time was also noted. These results support the hypothesis that an LCS approach can offer greater insight into complex patterns of association.   CONCLUSIONS This methodology appears to be well suited to the dissection of disease heterogeneity, a key component in the advancement of personalized medicine.	bladder neoplasm;continuance of life;disease ontology;evolutionary algorithm;genetic epidemiology;genetic heterogeneity;it risk;interaction;interpretation (logic);learning classifier system;mental association;personalization;poincaré recurrence theorem;precision medicine;rule (guideline);simulation;single nucleotide polymorphism	Ryan J. Urbanowicz;Angeline S. Andrew;Margaret R. Karagas;Jason H. Moore	2013		10.1136/amiajnl-2012-001574	personalized medicine;pathology;biological classification;computer science;bioinformatics;genetic heterogeneity	ML	5.396803581184466	-70.97031846357338	157937
b8f1b4d1d83d58f6555feeabe0068057ed99bf94	an intelligent approach for diabetes classification, prediction and description		A number of machine learning models have been applied to a prediction or classification task of diabetes. These models either tried to categorise patients into insulin and non-insulin, or anticipate the patients’ blood surge rate. Most medical experts have realised that there is a great relationship between patient’s symptoms with some chronic diseases and the blood sugar rate. This paper proposes a diabetes-chronic disease prediction-description model in the form of two sub-modules to verify this relationship. The first sub-module uses Artificial Neural Network (ANN) to classify the types of case and to predict the rate of fasting blood sugar (FBS) of patients. The post-process module is used to figure out the relations between the FBS and symptoms with prediction rate. The second sub-module describes the impact of the rate of FBS and the symptoms on the patient’s health. Decision Trees (DT) is used to achieve the description part of diabetes symptoms.	artificial neural network;categorization;decision tree;function-behaviour-structure ontology;intelligent agent;machine learning;requirement;statistical classification	Tarik A. Rashid;Saman M. Abdullah;Rezhna Mirza Abdullah	2015		10.1007/978-3-319-28031-8_28	machine learning	AI	5.9434658800613445	-77.71269383003988	158191
058a4daf2dc1f87953925e2c9c8d8926a1770a83	medical coding classification by leveraging inter-code relationships	medical records;multi label classification;large margin;prior knowledge;data mining;classification;l1 regularization;medical coding;large margin classifier;medical data mining	Medical coding or classification is the process of transforming information contained in patient medical records into standard predefined medical codes. There are several worldwide accepted medical coding conventions associated with diagnoses and medical procedures; however, in the United States the Ninth Revision of ICD(ICD-9) provides the standard for coding clinical records. Accurate medical coding is important since it is used by hospitals for insurance billing purposes. Since after discharge a patient can be assigned or classified to several ICD-9 codes, the coding problem can be seen as a multi-label classification problem. In this paper, we introduce a multi-label large-margin classifier that automatically learns the underlying inter-code structure and allows the controlled incorporation of prior knowledge about medical code relationships. In addition to refining and learning the code relationships, our classifier can also utilize this shared information to improve its performance. Experiments on a publicly available dataset containing clinical free text and their associated medical codes showed that our proposed multi-label classifier outperforms related multi-label models in this problem.	belief revision;code;discharger;electronic billing;margin classifier;multi-label classification;statistical classification	Yan Yan;Glenn Fung;Jennifer G. Dy;Rómer Rosales	2010		10.1145/1835804.1835831	regularization;biological classification;computer science;medical classification;data science;machine learning;pattern recognition;data mining;medical record	ML	3.561380117455031	-73.57792958422579	158455
fad37dd04b281bcf405d868755f90ca0b0ea3d9d	assessing bone loss in micro-gravity	fuzzy modeling;bone demineralization;microgravity	A prolonged stay in microgravity has various negative effects on the human body; one of these problems is a noticeable demineralization of bone tissues. Such effects are quite similar to those experienced by subjects on earth affected by osteoporosis; therefore it seems quite straightforward to adopt a similar pharmacological therapy during the stay in the space. In this paper a first step in the identification of a monitoring procedure for the bone demineralization in microgravity, as well as some guidelines for the choice of adequate therapies are given. Such a procedure is based on a mathematical model of the interaction of the most relevant blood and urine indicators of bone demineralization. Specifically, some bone metabolites have been identified, which are relevant to the phenomena and are feasible to be evaluated in the space. Moreover, a model to foresee the evolution of these parameters in the space, depending on the therapy chosen, is provided. The model is derived from the experience of doctors and experts, hence it is based mainly on linguistic information; such an information is codified by means of fuzzy numbers, in order to take into account their uncertainty.		Bruno Beomonte Zobel;Riccardo Del Vescovo;Gabriele Oliva;Valentina Russo;Roberto Setola	2012	Computer methods and programs in biomedicine	10.1016/j.cmpb.2012.05.001	pathology;micro-g environment;surgery	AI	6.086076108151154	-78.21037807113261	158549
2bc5fa7d5ccfa417ce5471ca1f692842255c39a3	roles for autonomous physiologic agents; an oxygen supply and demand example	demand example;physiologic systems control;oxygen supply;standard approach;autonomous physiologic agent;differential equation technique;heterogeneous physiologic system;differential equation;supply and demand;oxygen;differential equations;biocontrol;physiology	In the study of physiologic systems control, lumped parameter and differential equation techniques are standard approaches. Application of these techniques to the study of oxygen supply to tissues is discussed. It is then proposed that progress in dealing with heterogeneous physiologic systems is likely to proceed from the techniques of agent based modeling in the form of autonomous physiologic agents.	agent-based model;autonomous robot	Meyer Katzper	2007	2007 Winter Simulation Conference		control engineering;simulation;engineering;control theory;differential equation	Robotics	7.373318433657782	-68.00512731468054	158777
a4cce9ed4fe5a7e0eba8be6054e4798e8d25b170	compositional colored petri net approach to multiscale modeling for systems biology	systems biology;colored petri nets;multiscale modeling;compositional modeling	Colored Petri nets have been demonstrated as a powerful tool for modeling multiscale systems biology. However, the construction of colored Petri nets for biological systems requires prior knowledge about colored Petri nets and is often error-prone and cumbersome for biologists, especially when the communication between components and hierarchical organization of components in a multiscale model are an issue. To address this problem, an established way is to develop small components and then compose them into bigger models. In this paper, we present a compositional colored Petri net approach to aid automatic modeling of systems biology, and demonstrate it with two case studies. We focus on the modeling of communication between components and hierarchical organization of components as they are key to build multiscale models.	multiscale modeling;petri net;systems biology	Fei Liu;Ming Yang	2014	IJMSSC	10.1142/S1793962314500172	simulation;computer science;artificial intelligence;theoretical computer science;process architecture;petri net;systems biology;multiscale modeling	Robotics	4.114380040613725	-67.43010601633692	158812
5b4dcd4bfe738099e19c76637e3a437a2dd0ff86	non-obvious correlations to disease management unraveled by bayesian artificial intelligence analyses of cms data		OBJECTIVE Given the availability of extensive digitized healthcare data from medical records, claims and prescription information, it is now possible to use hypothesis-free, data-driven approaches to mine medical databases for novel insight. The goal of this analysis was to demonstrate the use of artificial intelligence based methods such as Bayesian networks to open up opportunities for creation of new knowledge in management of chronic conditions.   MATERIALS AND METHODS Hospital level Medicare claims data containing discharge numbers for most common diagnoses were analyzed in a hypothesis-free manner using Bayesian networks learning methodology.   RESULTS While many interactions identified between discharge rates of diagnoses using this data set are supported by current medical knowledge, a novel interaction linking asthma and renal failure was discovered. This interaction is non-obvious and had not been looked at by the research and clinical communities in epidemiological or clinical data. A plausible pharmacological explanation of this link is proposed together with a verification of the risk significance by conventional statistical analysis.   CONCLUSION Potential clinical and molecular pathways defining the relationship between commonly used asthma medications and renal disease are discussed. The study underscores the need for further epidemiological research to validate this novel hypothesis. Validation will lead to advancement in clinical treatment of asthma & bronchitis, thereby, improving patient outcomes and leading to long term cost savings. In summary, this study demonstrates that application of advanced artificial intelligence methods in healthcare has the potential to enhance the quality of care by discovering non-obvious, clinically relevant relationships and enabling timely care intervention.		Vijetha Vemulapalli;Jiaqi Qu;Jeonifer M. Garren;Leonardo O. Rodrigues;Michael A. Kiebish;Rangaprasad Sarangarajan;Niven R. Narain;Viatcheslav R. Akmaev	2016	Artificial intelligence in medicine	10.1016/j.artmed.2016.11.001	artificial intelligence;data science;data mining;statistics	AI	4.374697083487951	-73.97911420031961	158827
3ce95882fbe7d7a9863e2f8ef5cdea6d197f1d8c	belief rule-based inference for predicting trauma outcome	logistic regression;期刊论文;outcome prediction;evidential reasoning approach;belief rule base;support vector machine;artificial neural network	A belief rule-based inference methodology using the evidential reasoning approach (RIMER) is employed in this study to construct a decision support tool that helps physicians predict in-hospital death and intensive care unit admission among trauma patients in emergency departments (EDs). This study contributes to the research community by developing and validating a RIMER-based decision tool for predicting trauma outcome. To compare the prediction performance of the RIMER model with those of models derived using commonly adopted methods, such as logistic regression analysis, support vector machine (SVM), and artificial neural network (ANN), several logistic regression models, SVM models, and ANN models are constructed using the same dataset. Five-fold cross-validation is employed to train and validate the prediction models constructed using four different methods. Results indicate that the RIMER model has the best prediction performance among the four models, and its performance can be improved after knowledge base training with historical data. The RIMER tool exhibits strong potential to help ED physicians to better triage trauma, optimally utilize hospital resources, and achieve better patient outcomes. © 2015 Elsevier B.V. All rights reserved.	artificial neural network;computer emergency response team;cross-validation (statistics);decision support system;knowledge base;logic programming;logistic regression;support vector machine	Guilan Kong;Dong-Ling Xu;Jian-Bo Yang;Xiaofeng Yin;Tianbing Wang;Baoguo Jiang;Yonghua Hu	2016	Knowl.-Based Syst.	10.1016/j.knosys.2015.12.002	support vector machine;computer science;artificial intelligence;machine learning;data mining;logistic regression;evidential reasoning approach;artificial neural network	AI	6.195109363192492	-76.64351062738308	159166
449bb62beeb8e65d9060305e9deb11d1e42c8628	predicting classifier performance with a small training set: applications to computer-aided diagnosis and prognosis	histopathology computer aided diagnosis prognosis classifier cad inverse power law model statistical learning support vector machine svm c4 5 decision tree k nearest neighbor breast cancers lymphocytic infiltration mri;design automation;decision tree;computer aided diagnosis;support vector machines;cancer;application software;cad;training;biological organs;histopathology;image classification;clinical trial;inverse power law model;training data;error analysis;classifier;statistical learning;support vector machines biological organs cancer decision trees image classification medical image processing;medical image processing;application software computer aided diagnosis training data classification tree analysis support vector machines support vector machine classification decision trees breast cancer inverse problems predictive models;pixel;solid modeling;c4 5 decision tree;mri;support vector machine classification;k nearest neighbor;predictive models;svm;classification tree analysis;support vector machine;power law;decision trees;breast cancers;prognosis;breast cancer;lymphocytic infiltration;inverse problems	Selection of an appropriate classifier for computer-aided diagnosis (CAD) applications has typically been an ad hoc process. It is difficult to know a priori which classifier will yield high accuracies for a specific application, especially when well-annotated data for classifier training is scarce. In this study, we utilize an inverse power-law model of statistical learning to predict classifier performance when only limited amounts of annotated training data is available. The objectives of this study are to (a) predict classifier error in the context of different CAD problems when larger data cohorts become available, and (b) compare classifier performance and trends (both at the sample/patient level and at the pixel level) as additional data is accrued (such as in a clinical trial). In this paper we utilize a power law model to evaluate and compare various classifiers (Support Vector Machine (SVM), C4.5 decision tree, k-nearest neighbor) for four distinct CAD problems. The first two datasets deal with sample/patient-level classification for distinguishing between (1) high from low grade breast cancers and (2) high from low levels of lymphocytic infiltration in breast cancer specimens. The other two datasets are pixel-level classification problems for discriminating cancerous and non-cancerous regions on prostate (3) MRI and (4) histopathology. Our empirical results suggest that, given sufficient training data, SVMs tend to be the best classifiers. This was true for datasets (1), (2), and (3), while the C4.5 decision tree was the best classifier for dataset (4). Our results also suggest that results of classifier comparison made on small data cohorts should not be generalized as holding true when large amounts of data become available.	c4.5 algorithm;computer-aided design;decision tree;emoticon;hoc (programming language);k-nearest neighbors algorithm;machine learning;pixel;statistical classification;support vector machine;test set	Ajay Basavanhally;Scott Doyle;Anant Madabhushi	2010	2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro	10.1109/ISBI.2010.5490373	support vector machine;bayes classifier;margin;quadratic classifier;computer science;magnetic resonance imaging;machine learning;pattern recognition;data mining;histopathology	ML	8.067492349990543	-76.7359477258668	159293
f5d3aaa47e961ba5df17956dfcb8f7335b4ca391	exploring the role of pathology test results in the prediction of remaining days of hospitalisation		Accurate prediction of discharge time and identification of patients at risk of extended length of stay (LOS) can facilitate discharge planning and positively impact both the patient and the hospital in a variety of ways. To date, however, most studies only focus on the prediction of the overall LOS, which is generally estimated at admission time to hospital, emergency department or intensive care unit. This paper explores whether individual laboratory results can improve predictions of time of discharge as the tests become available. This study suggests that there is a statistically significant relationship between individual test results and remaining days in hospital and that there is a trend towards better estimates as more consecutive tests are taken into consideration. Their effect on the estimate of discharge time is generally weak. Further work integrating groups of test results into a more sophisticated dynamical model is required.	discharger;estimated;hospitalization;patients;intensive care unit	Blanca Gallego;Óscar Pérez;Frank Po-Yen Lin;Enrico W. Coiera	2012	Studies in health technology and informatics	10.3233/978-1-61499-078-9-45	medicine;pathology	HCI	6.968567614455992	-74.5679021258765	159303
44e7b960ff79a41eb70ef3bd1807732ee26f73aa	individual versus superensemble forecasts of seasonal influenza outbreaks in the united states		Recent research has produced a number of methods for forecasting seasonal influenza outbreaks. However, differences among the predicted outcomes of competing forecast methods can limit their use in decision-making. Here, we present a method for reconciling these differences using Bayesian model averaging. We generated retrospective forecasts of peak timing, peak incidence, and total incidence for seasonal influenza outbreaks in 48 states and 95 cities using 21 distinct forecast methods, and combined these individual forecasts to create weighted-average superensemble forecasts. We compared the relative performance of these individual and superensemble forecast methods by geographic location, timing of forecast, and influenza season. We find that, overall, the superensemble forecasts are more accurate than any individual forecast method and less prone to producing a poor forecast. Furthermore, we find that these advantages increase when the superensemble weights are stratified according to the characteristics of the forecast or geographic location. These findings indicate that different competing influenza prediction systems can be combined into a single more accurate forecast product for operational delivery in real time.	bayesian network;decision making;ensemble learning;geographic coordinate system;incidence matrix;projections and predictions;seasonal affective disorder;weight	Teresa K. Yamana;Sasikiran Kandula;Jeffrey Shaman	2017		10.1371/journal.pcbi.1005801	genetics;probability distribution;statistics;biology;kalman filter;location;influenza season;outbreak;bayesian inference;consensus forecast	Web+IR	7.008344261185119	-73.19159066535217	159420
052e9ae8faafa1198ef16df33f2d8544d071d1ec	a bayesian model of stereopsis depth and motion direction discrimination	model system;computer model;correspondence problem;system performance;bayesian analysis;retinal imaging;spatial frequency;bayesian model	 The extraction of stereoscopic depth from retinal disparity, and motion direction from two-frame kinematograms, requires the solution of a correspondence problem. In previous psychophysical work [Read and Eagle (2000) Vision Res 40: 3345–3358], we compared the performance of the human stereopsis and motion systems with correlated and anti-correlated stimuli. We found that, although the two systems performed similarly for narrow-band stimuli, broad-band anti-correlated kinematograms produced a strong perception of reversed motion, whereas the stereograms appeared merely rivalrous. I now model these psychophysical data with a computational model of the correspondence problem based on the known properties of visual cortical cells. Noisy retinal images are filtered through a set of Fourier channels tuned to different spatial frequencies and orientations. Within each channel, a Bayesian analysis incorporating a prior preference for small disparities is used to assess the probability of each possible match. Finally, information from the different channels is combined to arrive at a judgement of stimulus disparity. Each model system – stereopsis and motion – has two free parameters: the amount of noise they are subject to, and the strength of their preference for small disparities. By adjusting these parameters independently for each system, qualitative matches are produced to psychophysical data, for both correlated and anti-correlated stimuli, across a range of spatial frequency and orientation bandwidths. The motion model is found to require much higher noise levels and a weaker preference for small disparities. This makes the motion model more tolerant of poor-quality reverse-direction false matches encountered with anti-correlated stimuli, matching the strong perception of reversed motion that humans experience with these stimuli. In contrast, the lower noise level and tighter prior preference used with the stereopsis model means that it performs close to chance with anti-correlated stimuli, in accordance with human psychophysics. Thus, the key features of the experimental data can be reproduced assuming that the motion system experiences more effective noise than the stereoscopy system and imposes a less stringent preference for small disparities.	anti-aliasing filter;bayesian network;binocular disparity;computational model;correspondence problem;experience;humans;judgment;matching;motion system;noise (electronics);retina;spatial frequency;stereopsis;stereoscopy	Jenny C. A. Read	2002	Biological Cybernetics	10.1007/s004220100280	computer vision;bayesian probability;computer science;mathematics;computer performance;spatial frequency;optics;correspondence problem;communication;bayesian inference;physics;statistics	ML	-1.618235349382319	-77.81106309624798	159532
1f44026f932b0000f4fa2949ed8463d239ae6ad4	learning score systems for patient mortality prediction in intensive care units via orthogonal matching pursuit	matching pursuit algorithms;score systems;orthogonal matching pursuit;kernel;logistic regression;logistic regression intensive care units mortality rate prediction score systems orthogonal matching pursuit;learning systems;intensive care units;computational modeling;logistics;logistics matching pursuit algorithms learning systems kernel computational modeling sociology statistics;machine learning methods learning score systems patient mortality prediction intensive care units icu orthogonal matching pursuit omp approach critical care medicine acute physiology and chronic health evaluation system apache system simplified acute physiology score system saps system logistic regression model patient mortality probability estimation patient data set;mortality rate prediction;statistics;regression analysis data handling iterative methods learning artificial intelligence medical administrative data processing patient care probability;sociology	The problem of predicting outcome of patients in intensive care units (ICUs) is of great importance in critical care medicine, and has wide implications for quality control in ICUs. A dominant approach to this problem has been to use an ICU score system such as, for example, the Acute Physiology and Chronic Health Evaluation (APACHE) system, and the Simplified Acute Physiology Score (SAPS) system, to compute a certain severity score for a patient from a set of clinical observations, and apply a logistic regression model on this score to obtain an estimate of the probability of mortality for the patient, owing to their simplicity, these methods are widely used by clinicians. However, existing ICU score systems are built from a fixed set of patient data, and often perform poorly when applied to a patient population with different characteristics, also, with changes in patient characteristics, a score system built from a given patient data set becomes suboptimal over time. Moreover, most of these score systems are built using semi-automated procedures that require some amount of manual intervention, making it difficult to adapt them to a new patient population. Thus there is a huge need for adaptive methods that can automatically learn predictive models from a given set of patient data, tailored to perform well on similar patient populations. Indeed, there has been much work in recent years on applying various machine learning methods to this problem, however these methods learn different representations from the score systems preferred by clinicians. In this work, we develop a machine learning method based on orthogonal matching pursuit that automatically learns a score system type model, which enjoys the benefits of both worlds: like other machine learning methods, it is adaptive, like standard score systems, it uses a representation that is easy for clinicians to understand. Experiments on real-world patient data sets show that our method outperforms standard ICU score systems, and performs at least as well as other machine learning methods that employ more complex representations. As an added advantage of using the OMP approach, one can use a group-sparse variant of OMP which allows learning models with similar performance using a smaller number of clinical observations, we include experiments with this as well.	data model;experiment;international components for unicode;logistic regression;machine learning;matching pursuit;online advertising;openmp;population;predictive modelling;semiconductor industry;sparse matrix	Aadirupa Saha;Chandrahas Dewangan;Harikrishna Narasimhan;Sriram Sampath;Shivani Agarwal	2014	2014 13th International Conference on Machine Learning and Applications	10.1109/ICMLA.2014.20	logistics;econometrics;kernel;computer science;machine learning;data mining;logistic regression;computational model;statistics;matching pursuit	ML	5.943842354544981	-75.86013642104079	159564
8eb898e72dd8c94ade72f42568be5d99820889bd	simulating cancer radiotherapy on a multi-level basis: biology, oncology and image processing	radiotherapy;image processing;top down;glioblastoma;in silico oncology;in silico	Tumour growth and response to radiotherapeutic schemes is a markedly multiscale process which by no means can be reduced to only molecular or cellular events. Within this framework a new scientific area, i.e. in silico oncology has been proposed in order to address the previously mentioned hypercomplex process at essentially all levels of biocomplexity. This paper focuses on the case of imageable glioblastoma multiforme response to radiotherapy and presents the basics of an essentially top-down modelling approach, aiming at an improved undestanding of cancer and at a patient-specific optimization of treatment.	image processing	Dimitra D. Dionysiou;Georgios S. Stamatakos;Kostas Marias	2007		10.1007/978-3-540-73321-8_65	biology;pathology;bioinformatics;medical physics	Graphics	6.573571588025924	-68.86071883381688	160254
3b97ae14151f4180c27caaefb996bd0afdf97d3f	layered communication protocol for macro to nano-scale communication systems	nanoparticle signalling;communications society;drugs;telecommunications techniques;protocols;protocols biomedical communication drug delivery systems;communication system;drug delivery;nanobioscience;drug delivery systems;cancer;nanoparticle based drug delivery systems;nanotechnology;robust control;macroscale communication systems;nanoscale communication systems;receivers;layered communication protocol;protocols nanobioscience drug delivery robustness transmitters communications society nanotechnology intelligent systems robust control medical control systems;proteins;transmitters;intelligent systems;robust method;communication protocol;robustness;nanoparticle signalling layered communication protocol nanoscale communication systems macroscale communication systems nanoparticle based drug delivery systems telecommunications techniques;drug delivery system;medical control systems;biomedical communication	Nanoparticle-based drug delivery systems provide a feasible method of efficiently delivering drugs in-vivo. These drug delivery systems could greatly benefit from a robust method of nano-scale communication. Additionally, there are numerous naturally occurring nano-scale communication systems and the opportunity to develop a system based on these principles is highly promising. A layered communication protocol provides a simple, succinct method of understanding macro to nano-scale communication systems. This protocol may be used to expand and eventually apply commonly used telecommunications techniques to nanoparticle signalling. Such a protocol may begin to pave the way for future communication systems at a nano-scale.	communications protocol;gnu nano;osi model;video-in video-out	Aaron T. Sharp;Sri M. Raja;Beata J. Wysocki;Tadeusz A. Wysocki	2010	2010 IEEE International Conference on Communications	10.1109/ICC.2010.5501831	communications protocol;intelligent decision support system;telecommunications;computer science	Embedded	3.6464166196987695	-68.97349898045728	160301
094a8048e2df9c0c4bfd2cb96723a1d08b940ae2	embodied gesture learning from one-shot	training;visualization;hidden markov models;trajectory;feature extraction;production;gesture recognition	This paper discusses the problem of one shot gesture recognition. This is relevant to the field of human-robot interaction, where the user's intentions are indicated through spontaneous gesturing (one shot) to the robot. The novelty of this work consists of learning the process that leads to the creation of a gesture, rather on the gesture itself. In our case, the context involves the way in which humans produce the gestures - the kinematic and anthropometric characteristics and the users' proxemics (the use of the space around them). In the method presented, the strategy is to generate a dataset of realistic samples based on biomechanical features extracted from a single gesture sample. These features, called “the gist of a gesture”, are considered to represent what humans remember when seeing a gesture and the cognitive process involved when trying to replicate it. By adding meaningful variability to these features, a large training data set is created while preserving the fundamental structure of the original gesture. Having a large dataset of realistic samples enables training classifiers for future recognition. Three classifiers were trained and tested using a subset of ChaLearn dataset, resulting in all three classifiers showing rather similar performance around 80% recognition rate Our classification results show the feasibility and adaptability of the presented technique regardless of the classifier.	anthropometry;cognition;confusion matrix;gesture recognition;gist;heart rate variability;humans;human–robot interaction;one-shot learning;receiver operating characteristic;self-replication;spatial variability;spontaneous order;statistical classification;test set	Maria Eugenia Cabrera;Juan Pablo Wachs	2016	2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN)	10.1109/ROMAN.2016.7745244	computer vision;speech recognition;visualization;feature extraction;computer science;artificial intelligence;trajectory;gesture recognition	Vision	-3.0083866738021734	-76.46201261198117	160752
0729939c05471de5560d2fcbe118cc294ea674c7	a kinetic monte carlo simulation study of inositol 1, 4, 5-trisphosphate receptor (ip3r) calcium release channel	time dependent;ion channel;stochastisity;complex reactions;chemical master equation;gillespie algorithm;kinetic monte carlo;biological systems;simulation study;time domain;point of view;monte carlo;stochastic simulation algorithm;de young keizer model	Most of the previously theoretical studies about the stochastic nature of the IP3R calcium release channel gating use the chemical master equation (CME) approach. Because of the limitations of this approach we have used a stochastic simulation algorithm (SSA) presented by Gillespie. A single subunit of De Young-Keizer (DYK) model was simulated using Gillespie algorithm. The model has been considered in its complete form with eight states. We investigate the conditions which affect the open state of the model. Calcium concentrations were the subject of fluctuation in the previous works while in this study the population of the states is the subject of stochastic fluctuations. We found out that decreasing open probability is a function of Ca(2+) concentration in fast time domain, while in slow time domain it is a function of IP3 concentration. Studying the population of each state shows a time dependent reaction pattern in fast and medium time domains (10(-4) and 10(-3)s). In this pattern the state of X(010) has a determinative role in selecting the open state path. Also, intensity and frequency of fluctuations and Ca(2+) inhibitions have been studied. The results indicate that Gillespie algorithm can be a better choice for studying such systems, without using any approximation or elimination while having acceptable accuracy. In comparison with the chemical master equation, Gillespie algorithm is also provides a wide area for studying biological systems from other points of view.	approximation;biological system;calcium;excretory function;gillespie algorithm;inositol;interphalangeal joint 3;kinetic monte carlo;kinetics;leucaena pulverulenta;monte carlo method;quantum fluctuation;simulation;stochastic process	H. H. Haeri;S. M. Hashemianzadeh;M. Monajjemi	2007	Computational biology and chemistry	10.1016/j.compbiolchem.2007.02.009	statistical physics;biology;simulation;time domain;computer science;artificial intelligence;tau-leaping;mathematics;kinetic monte carlo;ion channel;gillespie algorithm;statistics;monte carlo method	ML	9.320364999528953	-66.52761146831433	160911
903ab90852cabe51c20eccd2c8a2532060de92b4	modular modelling of signalling pathways and their cross-talk	biological modules;signalling networks;signalling pathways;cross talk	Signalling pathways are well-known abstractions that explain the mechanisms whereby cells respond to signals. Collections of pathways form networks, and interactions between pathways in a network, known as cross-talk, enables further complex signalling behaviours. While there are several formal modelling approaches for signalling pathways, none make cross-talk explicit; the aim of this paper is to define and categorise cross-talk in a rigorous way. We define a modular approach to pathway and network modelling, based on the modules construct in the PRISM modelling language, and a set of generic signalling modules. Five different types of cross-talk are defined according to various biologically meaningful combinations of variable sharing, synchronisation labels and reaction renaming. The approach is illustrated with a case-study analysis of cross-talk between the TGF-β, WNT and MAPK pathways.	categorization;crosstalk;gene regulatory network;interaction;modeling language;prism (surveillance program)	Robin Donaldson;Muffy Calder	2012	Theor. Comput. Sci.	10.1016/j.tcs.2012.07.003	crosstalk;computer science;bioinformatics;artificial intelligence	PL	4.513241152818516	-67.34338351568343	160928
d5483104b033cb59e6703531010fdd04b29597e5	iron value classification in patients undergoing continuous ambulatory peritoneal dialysis using data mining		In this article, Data Mining classification techniques are employed, in order to classify as normal or notnormal the iron values from a patients’ blood analysis. The dataset used is relative to patients that were subjected to Continuous Ambulatory Peritoneal Dialysis (CAPD) treatment. Weka software was used for testing several classification algorithms into such data set. The main purpose is finding the best suitable classification algorithm, with a pleasing performance in classifying the instances of the data, whereas preserving low rate of false positives. The IBk algorithm achieved the best performance, being able to correctly classify 97.39% of the instances.	cgal;data mining;deus ex: human revolution;ex machina;international symposium on fundamentals of computation theory;item unique identification;k-nearest neighbors algorithm;machina (company);machine learning;weka	Catarina Peixoto;Hugo Peixoto;José Machado;António Abelha;Manuel Filipe Santos	2018		10.5220/0006820802850290	emergency medicine;continuous ambulatory peritoneal dialysis;medicine;functional testing (manufacturing)	ML	7.083104543978152	-77.48538196843282	161344
9347238ebb6d3d655d3d0759d2cf529ff1ba0baf	closing the loop from continuous m-health monitoring to fuzzy logic-based optimized recommendations	loop;closing;logic;recommendations;optimized;patient monitoring fuzzy logic learning artificial intelligence medical computing;m;monitoring;fuzzy;health;monitoring expert systems biomedical monitoring medical diagnostic imaging sensors heart rate fuzzy logic;analytic techniques continuous m health monitoring fuzzy logic based optimized recommendations continuous sensing health metrics severe health degradation learning cycles data preprocessing data analytics loopback feature improve fuzzy rules reduced data quantity improved data quality fuzzy expert system intelligent monitoring;continuous	Continuous sensing of health metrics might generate a massive amount of data. Generating clinically validated recommendations, out of these data, to patients under monitoring is of prime importance to protect them from risk of falling into severe health degradation. Physicians also can be supported with automated recommendations that gain from historical data and increasing learning cycles. In this paper, we propose a Fuzzy Expert System that relies on data collected from continuous monitoring. The monitoring scheme implements preprocessing of data for better data analytics. However, data analytics implements the loopback feature in order to constantly improve fuzzy rules, knowledge base, and generated recommendations. Both techniques reduced data quantity, improved data quality and proposed recommendations. We evaluate our solution through a series of experiments and the results we have obtained proved that our fuzzy expert system combined with the intelligent monitoring and analytic techniques provide a high accuracy of collected data and valid advices.	accidental falls;chronic disease;closing (morphology);data quality;decision making;elegant degradation;experiment;expert system;f-18 16 alpha-fluoroestradiol;fuzzy logic;iterative method;knowledge base;loopback;mhealth;oncogene, fps-fes;patients;preprocessor;rule (guideline);shin megami tensei: persona 3	Abdelghani Benharref;Mohamed Adel Serhani;Al Ramzana Nujum	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944179	loop;control engineering;fuzzy electronics;adaptive neuro fuzzy inference system;computer science;machine learning;closing;data mining;mathematics;health;logic	DB	5.4099232514758135	-78.95508155141572	161435
23821f2cdc4f815838ffd19e10fee6b787cfd0d0	mining association rules from a pediatric primary care decision support system	primary health care;algorithms;pediatrics	The purpose of this study was to apply an unsupervised data mining algorithm to a database containing data collected at the point of care for clinical decision support. The data set was taken from the Child Health Improvement Program (CHIP), a preventive services tracking and reminder system in use at the University of North Carolina. The database contains over 30,000 visits. We used a previously described pattern discovery algorithm to extract 2nd and 3rd order association rules from the data and reviewed the literature two see if the associations had been described before. The algorithm discovered 16 2nd order associations and 103 3rd order associations. The 3rd order associations contained no new information. The 2nd order associations demonstrated a covariance among a range of health risk behaviors. Additionally, the algorithm discovered that both tobacco smoke exposure and chronic cardiopulmonary disease are associated with failure on developmental screens. These relationships have been described before and have been attributed to underlying poverty. The work demonstrates the ability of unsupervised data mining by rule association on sparse clinical data to discover clinically important associations. However, many associations may be previously known or explained by confounding variables.	association rule learning;behavior;cns disorder;cardiopulmonary;chronic pulmonary heart disease;clinical data;clinical decision support system;contain (action);cor pulmonale;data mining;mental association;primary health care;rule (guideline);sparse matrix;tobacco smoke;unsupervised learning;women's health services;algorithm	Stephen M. Downs;Michael Y. Wallace	2000	Proceedings. AMIA Symposium		machine learning;data science;tobacco smoke exposure;chronic cardiopulmonary disease;clinical decision support system;association rule learning;decision support system;artificial intelligence;primary health care;medicine;confounding	ML	4.160373197852684	-75.65997887644194	162084
06e0292902b1accae55a99048abf283ddb7b10d8	selecting radiomic features from fdg-pet images for cancer treatment outcome prediction	cancer;imbalanced learning;outcome prediction;dempster shafer theory;pet images;feature selection	As a vital task in cancer therapy, accurately predicting the treatment outcome is valuable for tailoring and adapting a treatment planning. To this end, multi-sources of information (radiomics, clinical characteristics, genomic expressions, etc) gathered before and during treatment are potentially profitable. In this paper, we propose such a prediction system primarily using radiomic features (e.g., texture features) extracted from FDG-PET images. The proposed system includes a feature selection method based on Dempster-Shafer theory, a powerful tool to deal with uncertain and imprecise information. It aims to improve the prediction accuracy, and reduce the imprecision and overlaps between different classes (treatment outcomes) in a selected feature subspace. Considering that training samples are often small-sized and imbalanced in our applications, a data balancing procedure and specified prior knowledge are taken into account to improve the reliability of the selected feature subsets. Finally, the Evidential K-NN (EK-NN) classifier is used with selected features to output prediction results. Our prediction system has been evaluated by synthetic and clinical datasets, consistently showing good performance.	class;disease-free survival;encrypting file system;equilibrium;extraction;feature selection;fluorodeoxyglucose f18;forecast of outcome;functional discourse grammar;gene expression profiling;information;k-nearest neighbors algorithm;melia azedarach;neoplasms;numerous;pet therapy;polyethylene terephthalate;positron-emission tomography;radiomics;reinforcement learning;statistical classification;synthetic intelligence;tracer;cancer therapy	Chunfeng Lian;Su Ruan;Thierry Denoeux;Fabrice Jardin;Pierre Vera	2016	Medical image analysis	10.1016/j.media.2016.05.007	medicine;dempster–shafer theory;computer science;machine learning;pattern recognition;data mining;mathematics;feature selection;statistics;cancer	AI	5.570323678766462	-74.85264422025855	162936
eafc7a43cda2314cc7099c5da12a8c928ae90ec0	formalizing modularization and data hiding in synthetic biology	compartmentalization;synthetic biology;molecular computing;membrane computing;chemical synthesis	Biological systems employ compartmentalization and other co-localization strategies in order to orchestrate a multitude of biochemical processes by simultaneously enabling “data hiding” and modularization. This article presents recent research that embraces compartmentalization and co-location as an organizational programmatic principle in synthetic biological and biomimetic systems. In these systems, artificial vesicles and synthetic minimal cells are envisioned as nanoscale reactors for programmable biochemical synthesis and as chassis for molecular information processing. We present P systems, brane calculi, and the recently developed chemtainer calculus as formal frameworks providing data hiding and modularization and thus enabling the representation of highly complicated hierarchically organized compartmentalized reaction systems. We demonstrate how compartmentalization can greatly reduce the complexity required to implement computational functionality, and how addressable compartments permit the scaling-up of programmable chemical synthesis.	biomimetics;chassis;chemical database;compartmentalization (information security);image scaling;information processing;network compartment;p system;synthetic biology;synthetic intelligence	Harold Fellermann;Maik Hadorn;Rudolf M. Füchslin;Natalio Krasnogor	2014	JETC	10.1145/2667231	chemical synthesis;computer science;bioinformatics;membrane computing;artificial intelligence;theoretical computer science;nanotechnology;compartmentalization;synthetic biology	Graphics	4.16912302058241	-67.55266814642539	163204
463987228921d8cceec3bda1a70987af19e64463	inferring relationships in microbiomes from signed bayesian networks		Microbe-microbe and host-microbe interactions in a microbiome play a vital role in both health and disease. However, the structure of the microbial community and the colonization patterns are highly complex to infer even under controlled wet laboratory conditions. In this study, we investigate what information, if any, can be provided by a Bayesian Network (BN) about a microbial community. Unlike the previously proposed Co-occurrence Networks (CoNs), BNs are based on conditional dependencies and can help in revealing complex associations. In this paper, we report a surprising association between directed edges in BNs and known colonization order. Furthermore, when combined with the sign of the correlations from CoNs, BNs allow for many useful conclusions.In this paper, we ask the pertinent question: what can BNs reveal about the relationships between microbial taxa in a microbiome? In particular, we argue that BNs are able to capture temporal order (colonization order) when combined with the sign of correlation. The main contribution of this paper is to show how to carefully use BNs in conjunction with CoNs to make potential inferences about colonization order.We carried out our experiments on two datasets. The first is oral microbiome data from the Human Microbiome Project (HMP) [1], which included eight different sites all from within the oral cavity from 242 healthy adults (129 males, 113 females). The second is data from preterm infant gut microbiome samples as described in the paper by La Rosa et al. [2]. A total of 922 stool samples from 58 premature babies (each weighing ≤ 1500 g at birth) collected on different days were used for our analysis.	bayesian network;co-occurrence networks;conditional entropy;experiment;host media processing;interaction;linear algebra;relevance	Musfiqur Rahman Sazal;Daniel Ruiz-Pérez;Trevor Cickovski;Giri Narasimhan	2018	2018 IEEE 8th International Conference on Computational Advances in Bio and Medical Sciences (ICCABS)	10.1109/ICCABS.2018.8542086		DB	4.097547728212771	-71.89657645666587	163741
b2b6a2b4b24d4bb105e21f8470ba1e650912cdb3	lung nodules and beyond: approaches, challenges and opportunities in thoracic cad	computed tomography	Abstract The purpose of this manuscript is to give an overview of current activities in Computer-Aided Diagnosis (CAD) for thoracic applications. Because the large majority of applications of CAD relate to lung cancer, the detection and characterization problems in lung cancer CAD are described. Approaches to each of these problems are described. The current challenges that exist in each of the detection and characterization problems are described. Opportunities that exist in the near future are described including: efforts to create standardized databases by the Lung Image Database Consortium (LIDC) to facilitate research in lung CAD, research into other lung diseases besides lung cancer and the incorporation of other informatics sources into the computer-aided diagnosis approaches.	computer-aided design	Michael F. McNitt-Gray	2004			radiology;medicine;pathology;computed tomography	EDA	1.749223593960323	-68.9019488409291	163925
d13363e5598ba962e049f24ea5c0c34fc653166f	early warning system for financially distressed hospitals via data mining application	early warning system;hospital;data mining;financial distress	The aim of this study is to develop a Financial Early Warning System (FEWS) for hospitals by using data mining. A data mining method, Chi-Square Automatic Interaction Detector (CHAID) decision tree algorithm, was used in the study for financial profiling and developing FEWS. The study was conducted in Turkish Ministry of Health’s public hospitals which were in financial distress and in need of urgent solutions for financial issues. 839 hospitals were covered and financial data of the year 2008 was obtained from Ministry of Health. As a result of the study, it was determined that 28 hospitals (3.34%) had good financial performance, and 811 hospitals (96.66%) had poor financial performance. According to FEWS, the covered hospitals were categorized into 11 different financial risk profiles, and it was found that 6 variables affected financial risk of hospitals. According to the profiles of hospitals in financial distress, one early warning signal was detected and financial road map was developed for risk mitigation.	algorithm;chi;categorization;data mining;decision tree;detectors;distress (novel);feeling upset;hospitals, public;inventory;list of algorithms;respiratory distress syndrome, adult;united states public health service	Ali Serhan Koyuncugil;Nermin Ozgulbas	2011	Journal of Medical Systems	10.1007/s10916-011-9694-1	actuarial science;computer science;warning system	ML	4.364411001263891	-75.71374606257805	164257
ad56f90d12e6f01974fc93c51e0920851419e973	a neural network based clinical decision-support system for efficient diagnosis and fuzzy-based prescription of gynecological diseases using homoeopathic medicinal system	feed forward neural network;neural networks;prescription;homoeopathic system of medicine;gynecology;decision support system;clinical decision support system cdss;clinical decision support system;diagnosis;fuzzy systems;fuzzy system;neural network;knowledge base	As the analysis and diagnosis of gynecological diseases, especially using the homoeopathic system of medicine, gets more and more complicated, it becomes important for us to develop a decision-support system which can help a gynecologist analyze and prescribe medicines for such diseases with fewer errors. The prescription of a medicine for a gynecological disease, according to the homoeopathic system of medicine, is based on various modalities or symptoms of the disease. A medicine cannot be prescribed just by knowing the disease as is done in the case of the allopathic system of medicine or the general system of medicine. It becomes very difficult to take into account so many modalities while prescribing a medicine. A clinical decision-support system for gynecological diseases using the homoeopathic system of medicine for comprehensive diagnosis is extremely rare to find. The aim of this project is to develop a decision-support system, which will suggest a medicine for a gynecological disease based on the primary and secondary symptoms of the disease. The knowledge base of experts in this field has been utilized to develop this decision-support system. The decision-support system is based on the neural network concept. The rules for the decision-making are generated by combining a few neurons. The multi-layered weight-oriented feed forward neural network structure of the decision-support system makes it more robust, error-free and easy to program. This decision-support system, developed using Turbo prolog, will help a gynecologist to select the medicine of a particular gynecological disease quickly, easily and precisely. The primary symptoms help to evaluate a tentative medicine and the secondary symptoms confirm this tentative medicine. Fuzzy-type decision-making is used while analyzing the various symptoms. q 2005 Elsevier Ltd. All rights reserved.	artificial neural network;clinical decision support system;knowledge base;visual prolog	Ashish Mangalampalli;Srirama Moorthy Mangalampalli;Rama Chakravarthy;Ajeet K. Jain	2006	Expert Syst. Appl.	10.1016/j.eswa.2005.09.046	feedforward neural network;computer science;artificial intelligence;machine learning;medical diagnosis;medical prescription;artificial neural network;fuzzy control system	AI	4.738520863873557	-78.72452945739833	164499
c9189bfc4a170d77065da3af290de68ea6f8d3de	developing a predictive model for discharge delay in the post-anesthesia care unit.				Rodney A. Gabriel;Jihoon Kim;Lucila Ohno-Machado	2016			intensive care medicine;emergency medicine	EDA	9.237694941753727	-79.778180916856	164910
06dbea8a555005074fb5b1d3d5cfc12f1d765018	expressing complex mental states through facial expressions	virtual characters;animation system;limit set;facial expression;face to face;animal model;temporal change	A face is capable of producing about twenty thousand different facial expressions [2]. Many researchers on Virtual Characters have selected a limited set of emotional facial expressions and defined them as basic emotions, which are universally recognized facial expressions. These basic emotions have been well studied since 1969 and employed in many applications [3]. However, real life communication usually entails more complicated emotions. For instance, communicative emotions like “convinced”, “persuaded” and “bored” are difficult to describe adequately with basic emotions. Our daily face-to-face interaction is already accompanied by more complex mental states, so an empathic animation system should support them. Compared to basic emotions, complex mental states are harder to model because they require knowledge of temporal changes in facial displays and head movements as opposed to a static snapshot of the facial expression. We address this by building animation models for complex emotions based on video clips of professional actors displaying these emotions. The first step of our work is to extract the facial and head movements from video clips. We have adopted the recognition framework proposed by [4] to recognize head and facial displays, which allows facial displays to be based on not just the current facial action, but also on a pre-determined number of previous facial actions. Using the framework in [4], a commercial face tracker is used to locate and track 24 landmarks on the face. These are then mapped into facial actions and displays using Hidden Markov Models (HMMs) trained on video clips from the Mind Reading DVD compiled by researchers at the University of Cambridge Psychology Department [1]. The likelihoods of the facial displays, computed by the HMMs from the input video sequences, are used to drive the animation of our virtual characters. Six head action displays(head tilt, head turn, head forward, head backward, head shake and head nod) and three three facial displays (lip pull, lip pucker, and brow raised) are extracted for every frame and applied on the Virtual Character. The first four of the HMM outputs for head movement were translated directly into rotation about a single axis whereas head shake and head nod were translated into periodic animations. The facial displays are applied on the Virtual Characters as morph targets. As described above complex mental states differ from basic emotions in that they cannot be effectively recognized from static facial expressions, only from	apache axis;bittorrent tracker;compiler;hidden markov model;markov chain;mental state;morph target animation;real life;regular expression;snapshot (computer storage);video clip	Xueni Pan;Marco Gillies;T. Metin Sezgin;Céline Loscos	2007		10.1007/978-3-540-74889-2_79	psychology;limit set;computer vision;facial action coding system;communication;social psychology;facial expression	Graphics	-2.108999348466677	-75.77180496909322	165036
22a8993e1571c4dc214191958bf7307fdba10761	modelling cardiac patient set residuals using rough sets	information theory;myocardial infarction;probability;rough set;rough set theory	Many medical studies deal with the assessment of the prognostic or diagnostic power of some particular test with respect to some particular medical condition. However, even though a test is deemed to be powerful in this respect, the test may not be strictly needed to perform for everyone. If the test is costly or invasive, this issue is of particular interest. This paper presents a methodology based on rough set theory and Boolean reasoning that can be used to identify those patients for whom performing the test is redundant or superfluous. Furthermore, the methodology enables one to automatically construct a set of descriptive and minimal if-then rules that model the patient group in need of the test. A reanalysis of a previously published real-world dataset of patients with chest pain is used as a case study.	boolean;chest pain;description;meteorological reanalysis;patients;rough set;rule (guideline);scientific publication;set theory	Aleksander Øhrn;Staal A. Vinterbo;Piotr Szymanski;Jan Komorowski	1997	Proceedings : a conference of the American Medical Informatics Association. AMIA Fall Symposium		chest pain;information theory;data mining;computer science;rough set	EDA	1.4592805762376877	-76.23092504421582	165382
bbda32584f7695303a789015932c51e4c69c57e3	life model: a novel representation of life-long temporal sequences in health predictive analytics		Abstract Predictive analytics in healthcare can prevent patients’ emergency health conditions, and reduce costs in the long term. Moreover, accurate and timely anomaly predictions by focusing on recent events can save lives. In real-time IoT predictive analytics, modeling historical temporal health records with missing values in diagnosis prediction is a major challenge. Recent studies have started using deep learning and data abstraction techniques to model health data. However, it is difficult to train a model to predict anomalies based on temporal sparse data, especially to classify all disease diagnosis classes. Modeling a lifetime of an individual’s medical history in a short, concise sequence is a challenge. Moreover, the model should be robust and preserve the concept of time for variety of examples despite the missing values; especially in an IoT system, in which real-time prediction depends on both recent data and historical records. The proposed solution in this research for modelingtemporal pattern sequences is called as Life Model (LM) . L M provides a concise sequence to represent the history or future, using the novel intensity temporal sequence (ITS) tensors. LM algorithms and properties enable ITS tensors to train long short-term memory (LSTM) recurrent neural networks (RNN) efficiently in order to predict anomalies and diagnosis in real-time, even in the absence of some values. LM is used to predict mortality of 10,000 patients from MIMIC III dataset based on their diagnosis and procedures codes. The results show improvement in the model trained by LM-mapped data compared to fixed-sized intervals which achieved an accuracy of 99.6% with AUROC and brier score of 99.5% and of 0.00 respectively. In addition, the LM model can predict the approximate time of activities, with different granularity of seconds and up to years; tested on an activity dataset. Furthermore, a new LM-powered predictive health analytics and real-time monitoring schema (PHARMS) is proposed to enable design and implementation of predictive health analytic systems. PHARMS uses deep learning for real-time minimally-invasive intelligent activity monitoring and predictive analysis in a medical IoT scheme.		Alireza Manashty;Janet Light	2019	Future Generation Comp. Syst.	10.1016/j.future.2018.09.033	brier score;predictive analytics;real-time computing;missing data;deep learning;medical history;recurrent neural network;computer science;artificial intelligence;analytics;pattern recognition;internet of things	DB	4.4161553691112925	-77.19332725167139	165447
49c656afed7516a33183a14c63eeba6d6a1681e0	nonadherence to oral antihyperglycemic agents: subsequent hospitalization and mortality among patients with type 2 diabetes in clinical practice	survival rate;administration oral;female;retrospective studies;hospital mortality;male;data mining;treatment outcome;prevalence;indiana;mortality;diabetes mellitus type 2;health information exchange;risk assessment;humans;type 2 diabetes;hypoglycemic agents;hospitalization;medication adherence;natural language processing;electronic health records;aged	Using real-world clinical data from the Indiana Network for Patient Care, we analyzed the associations between non-adherence to oral antihyperglycemic agents (OHA) and subsequent diabetes-related hospitalization and all-cause mortality for patients with type 2 diabetes. OHA adherence was measured by the annual proportion of days covered (PDC) for 2008 and 2009. Among 24,067 eligible patients, 35,507 annual PDCs were formed. Over 90% (n=21,798) of the patients had a PDC less than 80%. In generalized linear mixed model analyses, OHA non-adherence is significantly associated with diabetes related hospitalizations (OR: 1.2; 95% CI [1.1,1.3]; p<0.0001). Older patients, white patients, or patients who had ischemic heart disease, stroke, or renal disease had higher odds of hospitalization. Similarly, OHA non-adherence increased subsequent mortality (OR: 1.3; 95% CI [1.02, 1.61]; p<0.0001). Patient age, male gender, income and presence of ischemic heart diseases, stroke, and renal disease were also significantly associated with subsequent all-cause death.	antineoplastic agents;cerebrovascular accident;cessation of life;clinical data;diabetes mellitus;diabetes mellitus, non-insulin-dependent;eighty;heart diseases;hospitalization;hypoglycemic agents;hypoxic-ischemic encephalopathy;kidney diseases;males;mental association;mixed model;myocardial ischemia;nsa product types;patients;peripheral dma controller;programme delivery control;thrombocytopenia	Vivienne J. Zhu;Wanzhu Tu;Marc B. Rosenman;J. Marc Overhage	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-60	intensive care medicine;medicine;medical emergency;diabetes mellitus	HCI	7.869194015744397	-75.79252910590445	165562
ea2473d819ad1c18f84552625ebdd0f03b40358e	analysing factors affecting hand biometrics during image capture		As more people are connected digitally, a highly automatic personal identification system is crucial. Dorsal hand vein biometric is an emerging biometric characteristic which is explored at its full swing. Although, researchers have deployed many hand biometrics using interesting techniques, it has not yet been accepted in many applications. Images capture is an important phase where the images obtained determine the performance of the biometric security system. Environmental factors and behavior of the subjects have an effect on image capture. In these work, different variables, that is, distance between camera and hand, the angle of deviation and the environmental temperature are controlled to capture images. The results are analysed and the effect of the variables have been depicted. It is deduced that image capture phase in biometric applications deserve more attention.	biometrics	Maleika Heenaye-Mamode Khan;Naushad Mamode Khan	2014		10.1016/j.procs.2014.05.456	computer vision;simulation;hand geometry	Vision	-0.9591188073135601	-71.75845608599948	166024
ee57d3f52d24d2348514c78a17cc3946c2ff8590	gene expression modular analysis: an overview from the data mining perspective		Abstract#R##N##R##N#In this review, we discuss the main problems and state-of-the-art solutions applied to the field of gene expression. Specific data analysis workflows have been developed in parallel with the technology and currently cover a very wide spectrum of methods and applications needed to give answers to a lot of scientific questions that this type of data are producing. Computer science and, more specifically, the data mining area is still benefiting from a large set of real-case scenarios to apply and develop new ideas and tools for discovering biological knowledge and new information from this experimental data. In this article, we make the reader aware of the main problems that still persist and provide a description of the methodologies that are applied for classification, clustering, and functional exploration of gene expression data. © 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 381–396 DOI: 10.1002/widm.29	data mining	Alberto D. Pascual-Montano	2011	Wiley Interdiscip. Rev. Data Min. Knowl. Discov.	10.1002/widm.29	computer science;bioinformatics;artificial intelligence;data science;machine learning;data mining	ML	-1.7155157118303233	-66.35485612650058	166167
a390161096dd8cc554942fc57fac767a5b6a28d7	early mastitis diagnosis through topological analysis of biosignals from low-voltage alternate current electrokinetics	biological patents;biomedical journals;text mining;veterinary medicine decision trees diseases medical signal processing pattern classification;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;classifier mastitis diagnosis biosignal topological analysis low voltage alternate current electrokinetics dairy cow disease dairy industry biosensing method milk quality acek device voltage level electrochemical reaction pathogen concentration level identification spectral domain gaussian based decision tree;life sciences;clinical guidelines;full text;feature extraction three dimensional displays dairy products decision trees shape pathogens robustness;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Mastitis is the most economically important disease of dairy cows worldwide, and it constantly plagues the dairy industry. A reliable biosensing method is thus imperative to detect this disease at its early stage and accurately identify the pathogen concentration level in order to better control the disease and consequently improve the quality of milk. Recent research indicates that shorter assay time and/or higher sensitivity can be achieved by integrating alternate current electrokinetics (ACEK) with biosensing. However, most existing ACEK devices use voltage levels around 10V at the risk of electrochemical reactions because a lower voltage may not effectively trigger the ACEK effect. Currently, there are no related works that can efficiently tackle the dilemma between avoiding electrochemical reaction and accelerating assay process. This paper adopts low-voltage (40~135mV) ACEK, which is safe but yields ambiguous biosignals within a short assay time, presenting great challenge to high-fidelity identification of pathogen concentration levels. This paper makes two distinctive contributions to the field of biosignal analysis. First, moving away from the traditional signal analysis in the time or spectral domain, we exploit the possibility of representing the biosignal through topological analysis that would reveal the intrinsic topological structure of point clouds generated from the biosignal. Second, in order to tackle another common challenge of biosignal analysis, i.e., limited sample size, we propose a so-called Gaussian-based decision tree (GDT), which can efficiently classify the biosignals even when the sample size is extremely small. Experimental results on the classification of five pathogen concentration levels using only 10 samples taken under various voltage levels demonstrate the robustness of the topological features as well as the advantage of GDT over some other conventional classifiers in handling small dataset. Our method reduces the voltage of ACEK to a safe level and still yields high-fidelity results in a short time.	dairy;decision tree;doppler effect;extraction;gamma-delta tocotrienol;handling (psychology);imperative programming;limited stage (cancer stage);milk, human;normal statistical distribution;pathogenic organism;plague;point cloud;signal processing;silo (dataset);voltage	Zhifei Zhang;Yang Song;Haochen Cui;Jie Wu;Fernando Schwartz;Hairong Qi	2015	2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2015.7318419	text mining;medical research;medicine;telecommunications;computer science;bioinformatics;engineering;electrical engineering;artificial intelligence;machine learning;data mining;biological engineering	Visualization	3.7699580807685233	-70.69595793457354	166379
661898921ff97b5dbe34150eb0ba4cce9e826356	a review of physiological simulation models of intracranial pressure dynamics	intracranial pressure;bepress selected works;computer model;data collection;icp computer model intracranial hypertension;model test;intracranial hypertension;simulation model;computer simulation;icp	This paper reviews the literature regarding the development, testing, and application of physiology-based computer simulation models of intracranial pressure dynamics. Detailed comparative information is provided in tabular format about the model variables and logic, any data collected, model testing and validation methods, and model results. Several syntheses are given that summarize the research carried out by influential research teams and researchers, review important findings, and discuss the methods employed, limitations, and opportunities for further research.	computer peripherals;computer hardware;computer simulation;diagram;intracranial pressure;norm (social);protocols documentation;review [publication type];simulation software;table (information);thrust;physiological aspects	Wayne W. Wakeland;Brahm Goldstein	2008	Computers in biology and medicine	10.1016/j.compbiomed.2008.07.004	computer simulation;simulation;computer science;simulation modeling;mathematics;statistics;data collection	SE	1.7449849651815537	-71.64441433790563	166523
bd3b79a8681ffa572a3ff39789c3b646fb654c84	trajectories mining in hospital information systems	information systems;trajectories mining data mining hospital management service oriented computing clustering;classification three dimensional trajectory mining hospital information systems hospital services temporal characteristics analysis temporal trajectory similarity based analysis technique clustering multidimensional scaling;hospitals;data mining;pattern classification data mining hospitals medical information systems;trajectory data mining hospitals laboratories surgery information systems kidney;trajectory;hospital management;clustering;medical information systems;service oriented computing;surgery;pattern classification;trajectories mining;kidney	This paper proposes two and three dimensional trajectories mining to analyze the temporal characteristics of hospital services. Trajectories mining method consists of the following two process. First the similarities between temporal trajectories of two or three selected variables are calculated. Second, similarity-based analysis technique such as clustering and multidimensional scaling is applied. The method was evaluated on data on the number of orders extracted from hospital information system. The results showed that the method discovered several interesting classification.	cluster analysis;data mining;image scaling;information system;multidimensional scaling	Shusaku Tsumoto;Shoji Hirano	2012	2012 IEEE International Conference on Systems, Man, and Cybernetics (SMC)	10.1109/ICSMC.2012.6377732	computer science;data science;trajectory;machine learning;service-oriented architecture;data mining;database;cluster analysis;information system	Robotics	2.494495828227241	-77.16615828643411	166661
7dd9b97575a0ea9a5aefe932a59544504c4f34d2	towards a better understanding of the evolution of senescence, apoptosis and tumour growth		Senescence (ageing) and apoptosis (programmed cell death) are phenomena that have troubled theoreticians and experimentalists. Previous research showed that the mortality curve of the yeast population followed the Gompertz-Makeham equation. We develop a generalised theoretical model which shows that the mortality of the organism can be expressed as a function of Ageing Factors such as ERCs. We use this idea to explain why senescence leads to apoptosis. Antagonistic pleiotropy and disposable soma theory suggest that senescence (and accordingly apoptosis) is a ‘side effect’. Although the altruistic benefits of apoptosis have been suggested before, we are attempting to show that in a resource-restricted environment, apoptosis can be a strategic choice. We show that the interactions between apoptotic and non-apoptotic organisms can be modelled using game theory and differential equations. We find that switching to apoptotic mode gives the organism an advantage over the non-apoptotic organisms in a resource-restricted environment. Mathematical analysis indicates that apoptosis is a stable strategy provided the conditions remain the same. We also find that one apoptotic organism can invade a population of non-apoptotic organisms. This begs the question why do tumours (which are non-apoptotic) occur if apoptosis is the best strategy? We show that apoptosis and angiogensis play a significant role in the development of tumours. We studied the effects of these two parameters on the dynamics of tumour and apoptotic populations. We find that the mixed strategy of avoidance of apoptosis and angiogenesis gives neoplasms an advantage over apoptotic organisms in certain conditions. Accordingly, the tumour organisms can invade apoptotic tissues. We also find that this strategy is not beneficial in the long-term.	game theory;gompertz function;interaction;population	Aasis Vinayak	2015			biology;cell biology;immunology	ECom	8.336789135078721	-67.13068741952593	166844
94938c95d50e632b9579352745d44175b9ef05b4	preparing, exploring and comparing cancer simulation results within a large parameter space	cancer;tumours cancer data visualisation grid computing medical computing patient treatment;subjunctive interfaces in silico oncology result space exploration multiple views;tumours;universiteitsbibliotheek;cancer simulation results;multiple views;medical computing;data visualisation;visualization;visualisation environment;in silico oncology;oncorecipesheet;parameter space;result space exploration;patient treatment;acgt oncosimulator;large parameter space;oncorecipesheet large parameter space acgt oncosimulator integrated grid based system cancer simulation results patient treatments visualisation environment;grid computing;subjunctive interfaces;integrated grid based system;patient treatments;in silico	The ACGT Oncosimulator is an integrated Grid-based system, under development within a 25-partner European-Japanese project, for patient-specific simulation of the response of a tumour and surrounding tissue to various forms of therapy. The validation of the simulation code is an activity requiring extensive human-driven visual investigation of the influence of each of the dozens of parameters to the code, and comparison of the simulation results against the known outcomes of past patient treatments. This activity therefore calls for a visualisation environment that supports users in working with an extremely large potential result space, and in rapidly setting up visualisations that highlight the differences between chosen subsets of available results. We describe the innovative features of the OncoRecipeSheet, an environment designed to meet these requirements.	grid computing;requirement;simulation	Aran Lunzer;Robert G. Belleman;Paul Melis;Georgios S. Stamatakos	2010	2010 14th International Conference Information Visualisation	10.1109/IV.2010.46	simulation;computer science;bioinformatics;theoretical computer science	Robotics	6.430567335155463	-69.17602472142428	166941
5633abb15a3954e1fe8962847cf5f435aa8c3fe3	an attention-based recurrent neural networks framework for health data analysis		(Extended Abstract of Recent Publication) Abstract In this paper we focus on prediction of health status of patients from the historical Electronic Health Records (EHR). We propose a multi-task framework that can monitor the multiple status of diagnoses. Patients’ historical records are fed into a Recurrent Neural Network (RNN) which memorizes all the past visit information, and then a task-specific layer is trained to predict multiple diagnoses. Experimental results show that prediction accuracy is reliable if compared to widely used approaches 1	computer multitasking;neural networks;random neural network;recurrent neural network	Qiuling Suo;Fenglong Ma;Giovanni Canino;Jing Gao;Aidong Zhang;Agostino Gnasso;Giuseppe Tradigo;Pierangelo Veltri	2018			recurrent neural network;machine learning;computer science;artificial intelligence	ML	4.897751075591337	-77.21624665840521	167343
ff36f2adb8733def490e08de9b986a8bb88e4eda	prediction of renal transplant rejection and acute tubular necrosis in renal transplant based on svm	support vector machines;prediction support vector machine acute tubular necrosis renal transplant rejection;support vector machines biomedical mri diseases kidney patient treatment;acute tubular necrosis cortical analysis bold mri patient classification model magnetic resonance imaging medullary analysis svm method biomarker acute renal graft rejection kidney transplant survival rate renal transplant rejection;diseases;patient treatment;kidney;biomedical mri	Prevention and proper treatment of renal transplant rejection and acute tubular necrosis in kidney are the key to improving the long-term kidney transplant survival rate. Hence, it is important to predict the acute renal graft rejection in early stage. In recent years, there emerged some biomarkers measured through non-invasive techniques that may indicate the acute rejection. In this paper, we apply SVM method to analyze biomarkers, medullary R2* (MR2*) and cortical R2* (CR2*) in transplanted kidney, acquired through BOLD MRI for classification of patients with normally functioning kidney transplants and acute rejection in kidney, including acute allograft rejection and acute tubular necrosis. Furthermore, we use the classification model to predict the acute kidney rejection. The results show that the application of SVM in the analysis of CR2* and MR2* has its potential in prediction of acute rejection in kidney.	grafting (decision trees);rejection sampling	Xun Li;Yao Wang;Chengxuan Wang;Sanqing Hu;Ying Xu;Fei Han;Jianghua Chen	2012	2012 5th International Conference on BioMedical Engineering and Informatics	10.1109/BMEI.2012.6512936	support vector machine;intensive care medicine;pathology;computer science;machine learning;diabetes mellitus	Vision	7.368107985669695	-76.79680802880556	167499
bcdb28829f4f4248019d4cb7bd9c6f8dc94e6699	personalizing lung cancer risk prediction and imaging follow-up recommendations using the national lung screening trial dataset	cancer screening;clinical decision support;data mining;lung cancer;medical informatics	Objective To demonstrate a data-driven method for personalizing lung cancer risk prediction using a large clinical dataset.   Materials and Methods An algorithm was used to categorize nodules found in the first screening year of the National Lung Screening Trial as malignant or nonmalignant. Risk of malignancy for nodules was calculated based on size criteria according to the Fleischner Society recommendations from 2005, along with the additional discriminators of pack-years smoking history, sex, and nodule location. Imaging follow-up recommendations were assigned according to Fleischner size category malignancy risk.   Results Nodule size correlated with malignancy risk as predicted by the Fleischner Society recommendations. With the additional discriminators of smoking history, sex, and nodule location, significant risk stratification was observed. For example, men with ≥60 pack-years smoking history and upper lobe nodules measuring >4 and ≤6 mm demonstrated significantly increased risk of malignancy at 12.4% compared to the mean of 3.81% for similarly sized nodules (P < .0001). Based on personalized malignancy risk, 54% of nodules >4 and ≤6 mm were reclassified to longer-term follow-up than recommended by Fleischner. Twenty-seven percent of nodules ≤4 mm were reclassified to shorter-term follow-up.   Discussion Using available clinical datasets such as the National Lung Screening Trial in conjunction with locally collected datasets can help clinicians provide more personalized malignancy risk predictions and follow-up recommendations.   Conclusion By incorporating 3 demographic data points, the risk of lung nodule malignancy within the Fleischner categories can be considerably stratified and more personalized follow-up recommendations can be made.		Jason Hostetter;James J. Morrison;Michael Morris;Jean Jeudy;Kenneth C. Wang;Eliot L. Siegel	2017	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocx012	medicine;pathology;physical therapy;data mining	ML	7.146616817645191	-75.41220212304675	167687
13f13a2768eae952972965e11180b878175ae59b	revealing determinant factors for early breast cancer recurrence by decision tree	breast cancer;recurrence;decision tree;classifier;stroma;tgfβ	Early breast cancer recurrence is indicative of poor response to adjuvant therapy and poses threats to patients’ lives. Most existing prediction models for breast cancer recurrence are regression-based models and difficult to interpret. We apply a Decision Tree algorithm to the clinical information of a cohort of non-metastatic invasive breast cancer patients, to establish a classifier that categorizes patients based on whether they develop early recurrence and on similarities of their clinical and pathological diagnoses. The classifier predicts for whether a patient developed early disease recurrence; and is estimated to be about 70% accurate. For an independent validation cohort of 65 patients, the classifier predicts correctly for 55 patients. The classifier also groups patients based on intrinsic properties of their diseases; and for each subgroup lists the disease characteristics in a hierarchal order, according to their relevance to early relapse. Overall, it identifies pathological nodal stage, percentage of intra-tumor stroma and components of TGFβ-Smad signaling pathway as highly relevant factors for early breast cancer recurrence. Since most of the disease characteristics used by this classifier are results of standardized tests, routinely collected during breast cancer diagnosis, the classifier can easily be adopted in various research and clinical settings.	algorithm;decision tree;gene regulatory network;relevance;statistical classification	Jimin Guo;Benjamin C. M. Fung;Farkhund Iqbal;Peter J. K. Kuppen;Rob A. E. M. Tollenaar;Wilma E. Mesker;Jean-Jacques Lebrun	2017	Information Systems Frontiers	10.1007/s10796-017-9764-0	knowledge management;medical diagnosis;adjuvant therapy;breast cancer;oncology;computer science;regression;cohort;pathological;disease;internal medicine;decision tree	ML	6.805482084385909	-76.24880820698667	167795
1b4f06042ea95e4065ec4cc4a4563512a1c9bf0d	controlled propagation in molecular communication using tagged liposome containers	biology computing;tagged liposome;neonatal jaundice;communication system control molecular communication containers lipidomics zinc peptides biomembranes materials science and technology nanoscale devices nanobioscience;molecular communication;nanomachines;information carrying molecules tagged liposome molecular communication communication medium nanomachines liposome containers;information carrying molecules;molecular biophysics biology computing biomedical communication;communication medium;evolutionary fuzzy neural network;molecular biophysics;liposome containers;diagnosis system;biomedical communication	Molecular communication is an emerging research area in the bio and nano science [1]. Molecular communication uses molecules as a communication medium and allows nanomachines (e.g. nano-scale biological or artificially-created devices) to communicate over a short distance. A key design challenge in molecular communication is controlling the propagation direction of information-carrying molecules towards their destination. This paper presents results of the experiment for transporting liposome containers to the designated receivers. In the experiments, molecular tags were used to identify the destination of the liposome containers.	british informatics olympiad;experiment;gnu nano;nanorobotics;software propagation	Yoshihiro Sasaki;Mineo Hashizume;Kohei Maruo;Naho Yamasaki;Junichi Kikuchi;Yuki Moritani;Satoshi Hiyama;Tatsuya Suda	2006	2006 1st Bio-Inspired Models of Network, Information and Computing Systems	10.1145/1315843.1315868	nanorobotics;molecular biophysics	HPC	3.6839650418048846	-68.97669775424346	168260
31c14041eb7b4501eb0d72a216eff0a787206f92	doubly-robust estimation of effect of imaging resource utilization on discharge decisions in emergency departments		Cluster analysis provides a data-driven multidimensional approach for identifying distinct subgroups of patients in a cohort. Each of the clusters represents a particular health condition with specific clinical trajectory and medical needs. Patients visiting emergency rooms do not share the same health condition, therefore discriminating between groups may have implications for diagnostic testing and resource utilization. We carried out this retrospective cohort study on 13825 patients who visited the emergency rooms in three Emory hospitals presenting with head trauma and non-stroke-like non-specific neurologic symptoms from January 2010 to September 2015. We utilized k-means clustering to find five distinct subgroups. Then, we investigated if getting an emergency head CT scan could have a statistically significant effect on getting discharged from the hospital. Adjusted effect estimation method was applied on each cluster to estimate the association between receiving a diagnostic test (e.g., head CT scan) on the disposition status. Out of five patient subgroups in the cohort, the chance of getting discharged for two clusters were significantly affected by getting a head CT scan. They both include comparatively older, African American or black patients who arrived in the ER with EMS, the latter suggesting critical health conditions.	accident and emergency department;arrival - action;ct scan;cluster analysis;craniocerebral trauma;diagnostic tests;discharger;emergency medical service;erdős–rényi model;k-means clustering;patients;nervous system disorder	Azade Tabaie;Falgun H. Chokshi;Andre L. Holder;Shamim Nemati	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8513076	computer vision;computed tomography;medical emergency;diagnostic test;retrospective cohort study;artificial intelligence;emergency rooms;cohort;computer science	SE	7.899369646646574	-75.57591292508198	168266
2b1975ea57623d9a9677c31a4d5842c8c2b0bc40	a system to evaluate the presence of a diarrheal epidemic caused by shigella and salmonella	epidemic effects;diarrheal epidemic;microorganisms diseases epidemics graphical user interfaces medical computing;epidemic extent;epidemic presence;shigella model;graphical user interface;medical computing;computational modeling;graphical user interfaces;basic reproduction number;sociology statistics microorganisms mathematical model pathogens computational modeling;epidemic existence diarrheal epidemic epidemic modeling disease dynamics pathogenesis shigella model salmonella model basic reproduction number r0 graphical user interface epidemic presence epidemic effects epidemic extent;pathogenesis;statistics;mathematical model;diseases;disease dynamics;epidemic existence;epidemics;salmonella model;microorganisms;sociology;epidemic modeling;pathogens	Epidemic modeling of a disease like diarrhea help in understanding the dynamics of the disease as well as preventing it. According to the pathogenesis, models for Shigella and Salmonella have been formulated and the basic reproduction number (R0) has been evaluated. These models have been built up into a user-friendly graphical user interface that displays the presence and effects of the epidemic. It was established that as long as R0<;1, there will be no epidemic. The outputs from the system are very clear and help in determining the extent and existence of the epidemic.	graphical user interface;population;usability	Ojaswita Chaturvedi;Shedden Masupe;Tiny Masupe	2014	IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)	10.1109/BHI.2014.6864385	biology;virology;immunology;microbiology	SE	8.391152692261072	-67.76934468143737	168282
1ca7b19d620607c130f0509eb7da041bec83426e	model-based glycaemic control in critical care - a review of the state of the possible	hyperglycemia;oscillations;science and technology;system approach;critically ill;dynamic systems;blood glucose;journal article;control problem;glycaemic control;clinical study;model based control;clinical outcome;insulin;control;review;nutrition;critical care;models;type 1 diabetes mellitus	Hyperglycaemia is prevalent in critical care, as patients experience stress-induced hyperglycaemia, even with no history of diabetes. Hyperglycaemia in critical care is not largely benign, as once thought, and it has a deleterious effect on outcome. Recent studies have shown that tight glucose regulation to average levels from 6.1–7.75 mmol/L can reduce mortality 25–43%, while also significantly reducing other negative clinical outcomes.#R##N##R##N#However, clinical results are highly variable and there is little agreement on what levels of performance can be achieved and how to achieve them. A typical clinical solution is to use ad-hoc protocols based primarily on experience, where large amounts of insulin, up to 50 U/h, are titrated against glucose measurements variably taken every 1–4 h. When combined with the unpredictable and sudden metabolic changes that characterise this aspect of critical illness and/or clinical changes in nutritional support, this approach results in highly variable blood glucose levels. The overall result is sustained periods of hyper- or hypo- glycaemia, characterised by oscillations between these states, which can adversely affect clinical outcomes and mortality. The situation is exacerbated by exogenous nutritional support regimes with high dextrose content. Hence, there is an emerging, strong need for the more rigorous analysis and methods that model-based control methods bring to this type of problem.#R##N##R##N#This paper reviews the state of the clinical and model-based control systems approach to the problem of managing hyperglycaemia in critical care, emphasising emerging methods and results. The overall goal is to present the fundamental problem and associated science and technologies involved. Thus, it is less a discussion of specific advantageous approaches than a presentation of the different factors that impact the problem and the different approaches taken to address them in the limited clinical engineering research done to date. These discussions are presented in the context of current and emerging clinical studies, both model-based and empirical protocol driven. Analogies to the Type 1 diabetes mellitus control problem are also noted where relevant and significant. Hence, it is more overview than specific analysis, where the overall conclusion is that there are many opportunities and unanswered questions remaining on which model-based control research can have significant clinical impact.		J. Geoffrey Chase;Geoffrey M. Shaw;Xing-Wei Wong;Thomas Lotz;Jessica Lin;Christopher E. Hann	2006	Biomed. Signal Proc. and Control	10.1016/j.bspc.2006.03.002	medicine;dynamical system;nutrition;oscillation;diabetes mellitus;surgery;proinsulin;scientific control;science, technology and society	ML	9.255355681967117	-72.65781786738351	168503
3d235d1d6ef4248ee8b7194135e539b84a3abb0d	gaze-contingent center-surround fusion of infrared images to facilitate visual search for human targets			contingency (philosophy)	Mackenzie G. Glaholt;Grace Sim	2017		10.2352/ISSN.2470-1173.2017.14.HVEI-149	computer vision	Vision	-2.7214245056432698	-79.1512981627847	168620
d0a24c05b478f9d6e8c8669d267b6db7c2609a72	a neural network model for early diagnosis of acute gvhd based on gene expression data	patient diagnosis;patient diagnosis cellular biophysics diseases genetics medical computing neural nets;dimension reduction technique artificial neural network model acute graft versus host disease early diagnosis gene expression data haematopoietic stem cell transplantation alloreactivity;acute graft versus host disease early diagnosis;neural nets;neural networks gene expression diseases artificial neural networks stem cells testing medical treatment immune system humans bioinformatics;biology;gene expression data;dimension reduction technique;genetics;medical computing;artificial neural networks;alloreactivity;feature extraction;principal component analysis;classification algorithms;diseases;haematopoietic stem cell transplantation;medical treatment;cellular biophysics;artificial neural network model	Acute graft-versus-host disease (aGVHD) is the major complication after allogeneic haematopoietic stem cell transplantation (HSCT) in which functional immune cells of donor recognize the recipient as “foreign” and mount an immunologic attack. In this paper we analyzed gene-expression profiles of 47 genes associated with alloreactivity in 59 patients submitted to HSCT. We have applied a dimension reduction technique to found the most important subset of genes to make a diagnosis of aGVHD. The composed subset has been used in order to train and test a suitable Artificial Neural Network (ANN) to detect the aGVHD at on-set of clinical signs.	artificial neural network;cell (microprocessor);dimensionality reduction;grafting (decision trees);network model	Maurizio Fiasché;Maria Cuzzola;Matteo Cacciola;Giuseppe Megali;Roberta Fedele;Pasquale Iacopino;Francesco Carlo Morabito	2009	2009 IEEE International Workshop on Genomic Signal Processing and Statistics	10.1109/GENSIPS.2009.5174360	biology;pathology;feature extraction;computer science;bioinformatics;immunology;artificial neural network;principal component analysis	Comp.	8.838601791596016	-76.49147865245278	168637
eb5336b10d229273a4ae4d8831158f26574ee763	predictive modelling based on statistical learning in biomedicine			biomedicine;machine learning;predictive modelling	Olaf Gefeller;Benjamin Hofner;Andreas Mayr;Elisabeth Waldmann	2017		10.1155/2017/4041736	data science;predictive modelling;biomedicine;machine learning;artificial intelligence;computer science	ML	0.947469658908017	-66.84406583905621	168690
0d476915e693d8f93f2c3b47b0623b93970f09e3	comparing data mining methods with logistic regression in childhood obesity prediction	young children;data mining;logistic regression;bayesian method;accuracy;machine learning;prediction accuracy;childhood obesity;prediction;public health;neural network;medical data mining	"""The epidemiological question of concern here is """" can young children at risk of obesity be identified from their early growth records? """" Pilot work using logistic regression to predict overweight and obese children demonstrated relatively limited success. Hence we investigate the incorporation of non-linear interactions to help improve accuracy of prediction; by comparing the result of logistic regression with those of six mature data mining techniques. The contributions of this paper are as follows: a) a comparison of logistic regression with six data mining techniques: specifically, for the prediction of overweight and obese children at 3 years using data recorded at birth, 6 weeks, 8 months and 2 years respectively; b) improved accuracy of prediction: prediction at 8 months accuracy is improved very slightly, in this case by using neural networks, whereas for prediction at 2 years obtained accuracy is improved by over 10%, in this case by using Bayesian methods. It has also been shown that incorporation of non-linear interactions could be important in epidemiological prediction, and that data mining techniques are becoming sufficiently well established to offer the medical research community a valid alternative to logistic regression."""	artificial neural network;data mining;emoticon;interaction;logistic regression;nonlinear system	Shaoyan Zhang;Christos Tjortjis;Xiao-Jun Zeng;Hong Qiao;Iain E. Buchan;John A. Keane	2009	Information Systems Frontiers	10.1007/s10796-009-9157-0	econometrics;prediction;public health;bayesian probability;computer science;data mining;accuracy and precision;logistic regression;statistics	ML	6.075823666455319	-76.03741516634295	169069
3b3bcdf6eaa06bf48ce60c323aa73bbad8bdddc3	the application of artificial immune systems for the prediction of premature delivery		One of the most challenging tasks currently facing the healthcare community is the identification of premature labour. Premature birth occurs when the baby is born before completion of the 37-week gestation period. The incomplete understanding of the physiology of the uterus and parturition means that premature labour prediction is a difficult task. The reason for this may be that the initial symptoms of preterm labour occur commonly in normal pregnancies. There is some misclassification in regard to recognizing full-term and preterm labour; approximately 20% of women who are identified as reaching full-term labour actually deliver prematurely. This paper explores the applicability of Artificial Immune System (AIS) technique as a new methodology to classify term and preterm records. Our AIS approach shows better results when compared with Neural Network, Decision Tree, and Support Vector Machines, achieving more than 92% accuracy overall.	artificial immune system	Rentian Huang;Hissam Tawfik;Abir Jaafar Hussain;Haya Alaskar	2014		10.1007/978-3-319-09339-0_83	artificial intelligence;gestation period;premature labour;artificial immune system;machine learning;term delivery;decision tree;artificial neural network;intensive care medicine;computer science;preterm labour;premature birth	Robotics	5.908358052452095	-77.66205171895025	169246
d13e3781134a91936e89053f9e0d3644d3351174	diagnosis code assignment using sparsity-based disease correlation embedding	embedded systems feature extraction data mining encoding medical diagnostic imaging labeling;icd code labeling;data mining;feature extraction;sparsity based regularization;diseases;disease correlation embedding icd code labeling multi label learning sparsity based regularization;correlation;multi label learning;encoding;disease correlation embedding;medical diagnostic imaging	With the latest developments in database technologies, it becomes easier to store the medical records of hospital patients from their first day of admission than was previously possible. In Intensive Care Units (ICU), modern medical information systems can record patient events in relational databases every second. Knowledge mining from these huge volumes of medical data is beneficial to both caregivers and patients. Given a set of electronic patient records, a system that effectively assigns the disease labels can facilitate medical database management and also benefit other researchers, e.g., pathologists. In this paper, we have proposed a framework to achieve that goal. Medical chart and note data of a patient are used to extract distinctive features. To encode patient features, we apply a Bag-of-Words encoding method for both chart and note data. We also propose a model that takes into account both global information and local correlations between diseases. Correlated diseases are characterized by a graph structure that is embedded in our sparsity-based framework. Our algorithm captures the disease relevance when labeling disease codes rather than making individual decision with respect to a specific disease. At the same time, the global optimal values are guaranteed by our proposed convex objective function. Extensive experiments have been conducted on a real-world large-scale ICU database. The evaluation results demonstrate that our method improves multi-label classification results by successfully incorporating disease correlations.	algorithm;bag-of-words model;code;convex function;data mining;encode;embedded system;experiment;information system;international components for unicode;mimic;multi-label classification;optimization problem;relational database;relevance;sparse matrix	Sen Wang;Xiaojun Chang;Xue Li;Guodong Long;Lina Yao;Quan Z. Sheng	2016	IEEE Transactions on Knowledge and Data Engineering	10.1109/TKDE.2016.2605687	feature extraction;computer science;data science;machine learning;pattern recognition;data mining;database;correlation;encoding	ML	3.3374655081243603	-73.54401438924414	169374
dce2530b4c65f2ac1768c4c209338fc59a22a6a5	snakes on a plane: a perfect snap for bioimage analysis	molecular biophysics biomedical optical imaging cellular biophysics data acquisition image segmentation medical image processing;snakes bioimage analysis molecular processes cellular processes biomedical research spatiotemporal relationships time lapse imaging single molecules image data manpower biomedical images data acquisition bioimage informatics biological objects imaging modality high throughput microscopy;biomedical imaging biomedical signal processing image resolution microscropy spatiotemporal phenomena complexity theory	In recent years, there has been an increasing interest in getting a proper quantitative understanding of cellular and molecular processes [1], [2]. One of the major challenges of current biomedical research is to characterize not only the spatial organization of these complex systems but also their spatiotemporal relationships [3], [4]. Microscopy has matured to the point that it enables sensitive time-lapse imaging of cells in vivo and even of single molecules [5], [6]. Making microscopy more quantitative brings important scientific benefits in the form of improved performance and reproducibility. This has been fostered by the development of technological achievements such as high-throughput microscopy. A direct consequence is that the size and complexity of image data are increasing. Time-lapse experiments commonly generate hundreds to thousands of images, each containing hundreds of objects to be analyzed [7]. These data often cannot be analyzed manually because the manpower required would be too extensive, which calls for automated methods for the analysis of biomedical images. Such computerized extraction of quantitative information out of the rapidly expanding amount of acquired data remains a major challenge. The development of the related algorithms is nontrivial and is one of the most active fronts in the new field of bioimage informatics [8]?[11]. Segmenting thousands of individual biological objects and tracking them over time is remarkably difficult. A typical algorithm will need to be tuned to the imaging modality and will have to cope with the fact that cells can be tightly packed and may appear in various configurations, making them difficult to segregate.	algorithm;bioimage informatics;complex systems;experiment;high-throughput computing;modality (human–computer interaction);spatial organization;throughput;video-in video-out	Ricard Delgado-Gonzalo;Virginie Uhlmann;Daniel Schmitter;Michael Unser	2015	IEEE Signal Processing Magazine	10.1109/MSP.2014.2344552	computer vision;computer science;bioinformatics;data science	Visualization	2.0804868287452853	-68.71259431039134	169389
52a61631838f0599c0e832ed09d8bf633cb9ea49	capturing biological frequency control of circadian clocks by reaction system modularization				Thomas Hinze;Christian Bodenstein;Ines Heiland;Stefan Schuster	2011	ERCIM News			OS	7.243127446913398	-66.93471678635959	169757
85850cf2274af1bc2783e2f1c4eb0757d8b2afb2	the necker-zeno model for bistable perception	non commuting operations;cognitive time scales;temporal nonlocality;bistable perception;bell inequality;acategorial states;necker zeno model	A novel conceptual framework for theoretical psychology is presented and illustrated for the example of bistable perception. A basic formal feature of this framework is the non-commutativity of operations acting on mental states. A corresponding model for the bistable perception of ambiguous stimuli, the Necker-Zeno model, is sketched and some empirical evidence for it so far is described. It is discussed how a temporal non-locality of mental states, predicted by the model, can be understood and tested.	2'-deoxythymidine;bell's theorem;chap protocol;challenge-handshake authentication protocol;coexist (image);consciousness;decision making;design rationale;emoticon;generalization (psychology);hl7publishingsubsection <operations>;hemoglobin necker enfants-malades;high- and low-level;illusions;information processing;koch snowflake;lexm gene;locality of reference;machine perception;markov chain;markov model;mental state;note (document);ontic;principle of locality;quantum mechanics;quantum state;social inequality;spontaneous order;unconscious personality factor;detection of temperature stimulus involved in sensory perception of pain;non-t, non-b childhood acute lymphoblastic leukemia	Harald Atmanspacher;Thomas Filk	2013	Topics in cognitive science	10.1111/tops.12044	psychology;artificial intelligence;bell's theorem;communication;social psychology	ML	-4.445239508611109	-77.76772140306554	170108
58ac8025a7d28d76e3ef7afd6601a6f1aed2b702	call for papers: deep phenotyping for precision medicine				Chunhua Weng;Nigam Haresh Shah;George Hripcsak	2018	Journal of biomedical informatics	10.1016/j.jbi.2018.09.017	precision medicine;information retrieval;computer science	Crypto	0.13720746643812876	-67.98965827727586	170398
333f5cfe0714535a2916b47739fa6efe87c28b37	predicting the likelihood of falls among the elderly using likelihood basis pursuit technique	risk factors;data mining;basis pursuit	This study reports on the application of the knowledge discovery in database process to generate models that can predict the likelihood of falls among the elderly who reside in long-term care facilities. This process was applied to data held in the Minimum Data Set, a comprehensive resident assessment instrument being used in all Medicare and Medicaid supported nursing homes in the United States. For this study, we incorporated a new data mining technique, Likelihood Basis Pursuit, into the process. Using this technique, we were able to correctly identify which of the variables in this data set were associated with falls and generate models that could make fall likelihood predictions based upon those variables. Because the model provides probabilities based upon the exact combination of variables present in a particular resident, models constructed using this new data mining technique have the potential to be more useful for assessing fall risk.	accidental falls;basis pursuit;data mining;nursing homes;probability;long-term care	Kanittha Volrathongchia;Patricia Flatley Brennan;Michael C. Ferris	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		long-term care;knowledge extraction;injury prevention;data mining;minimum data set;suicide prevention;occupational safety and health;basis pursuit;medicaid;medicine	SE	4.436911774950311	-75.71599420608595	170448
5a50a537aa435c7b59ec6c8d3b2eb1fb79eedfeb	computer-aided diagnosis of parkinson’s disease using enhanced probabilistic neural network	computer aided diagnosis;parkinson s disease;enhanced probabilistic neural networks	Early and accurate diagnosis of Parkinson’s disease (PD) remains challenging. Neuropathological studies using brain bank specimens have estimated that a large percentages of clinical diagnoses of PD may be incorrect especially in the early stages. In this paper, a comprehensive computer model is presented for the diagnosis of PD based on motor, non-motor, and neuroimaging features using the recently-developed enhanced probabilistic neural network (EPNN). The model is tested for differentiating PD patients from those with scans without evidence of dopaminergic deficit (SWEDDs) using the Parkinson’s Progression Markers Initiative (PPMI) database, an observational, multi-center study designed to identify PD biomarkers for diagnosis and disease progression. The results are compared to four other commonly-used machine learning algorithms: the probabilistic neural network (PNN), support vector machine (SVM), k-nearest neighbors (k-NN) algorithm, and classification tree (CT). The EPNN had the highest classification accuracy at 92.5 % followed by the PNN (91.6 %), k-NN (90.8 %) and CT (90.2 %). The EPNN exhibited an accuracy of 98.6 % when classifying healthy control (HC) versus PD, higher than any previous studies.	artificial neural network;attention deficit hyperactivity disorder;biological markers;biological neural networks;classification;color gradient;computer simulation;decision tree learning;dopamine hydrochloride;k-nearest neighbors algorithm;machine learning;multiclass classification;neuroimaging;numerical weather prediction;numerous;parkinson disease;parkinsonian disorders;patients;probabilistic neural network;progressive disease;somatosensory discrimination disorder;specimen;support vector machine;perineuronal net (cell component)	Thomas J. Hirschauer;Hojjat Adeli;John A. Buford	2015	Journal of Medical Systems	10.1007/s10916-015-0353-9	medicine;pathology;artificial intelligence;machine learning	ML	7.396728791119638	-76.80203855130621	170521
da556835f4209f63f5f47543b38f076fb0fb7bba	development of sensor cells using nf-κb pathway activation for detection of nanoparticle-induced inflammation	animals;sensor cells;nanoparticles;nf kappa b;mice;tio nanoparticles agglomerates;plasmids;nf κb;toll like receptor 4;inflammation;genes reporter;toll like receptor4;transfection;humans;tio 2 nanoparticles agglomerates;titanium;3t3 cells	The increasing use of nanomaterials in consumer and industrial products has aroused concerns regarding their fate in biological systems. An effective detection method to evaluate the safety of bio-nanomaterials is therefore very important. Titanium dioxide (TiO(2)), which is manufactured worldwide in large quantities for use in a wide range of applications, including pigment and cosmetic manufacturing, was once thought to be an inert material, but recently, more and more studies have indicated that TiO(2) nanoparticles (TiO(2) NPs) can cause inflammation and be harmful to humans by causing lung and brain problems. In order to evaluate the safety of TiO(2) NPs for the environment and for humans, sensor cells for inflammation detection were developed, and these were transfected with the Toll-like receptor 4 (TLR4) gene and Nuclear Factor Kappa B (NF-κB) reporter gene. NF-κB as a primary cause of inflammation has received a lot of attention, and it can be activated by a wide variety of external stimuli. Our data show that TiO(2) NPs-induced inflammation can be detected by our sensor cells through NF-κB pathway activation. This may lead to our sensor cells being used for bio-nanomaterial safety evaluation.	artificial nanoparticles;biological system;british informatics olympiad;concurrent computing;cosmetics;gene regulatory network;inflammation;lightweight portable security;nf-kappa b;nanostructured materials;pigment;quantity;sensor web;structure of parenchyma of lung;tlr4 protein, human;wakefulness;titanium dioxide	Peng Chen;Satoshi Migita;Koki Kanehira;Shuji Sonezaki;Akiyoshi Taniguchi	2011		10.3390/s110707219	titanium;nfkb1;nanotechnology;nanoparticle;transfection;plasmid	Mobile	5.376998036984326	-66.46292521923814	170622
21d48d116dcac3b0484b002dc15005fd21424ee2	mining causal relationships among clinical variables for cancer diagnosis based on bayesian analysis	biological patents;biomedical journals;causal relationship;text mining;europe pubmed central;citation search;data mining and knowledge discovery;citation networks;computational biology bioinformatics;research articles;abstracts;cancer diagnosis;restricted bayesian classifier;open access;life sciences;clinical guidelines;algorithms;full text;computer appl in life sciences;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Cancer is the second leading cause of death around the world after cardiovascular diseases. Over the past decades, various data mining studies have tried to predict the outcome of cancer. However, only a few reports describe the causal relationships among clinical variables or attributes, which may provide theoretical guidance for cancer diagnosis and therapy. Different restricted Bayesian classifiers have been used to discover information from numerous domains. This research work designed a novel Bayesian learning strategy to predict cause-specific death classes and proposed a graphical structure of key attributes to clarify the implicit relationships implicated in the data set. The working mechanisms of 3 classical restricted Bayesian classifiers, namely, NB, TAN and KDB, were analysed and summarised. To retain the properties of global optimisation and high-order dependency representation, the proposed learning algorithm, i.e., flexible K-dependence Bayesian network (FKBN), applies the greedy search of conditional mutual information space to identify the globally optimal ordering of the attributes and to allow the classifiers to be constructed at arbitrary points (values of K) along the attribute dependence spectrum. This method represents the relationships between different attributes by using a directed acyclic graph (DAG) model. A total of 12 data sets were selected from the SEER database and KRBM repository by 10-fold cross-validation for evaluation purposes. The findings revealed that the FKBN model outperformed NB, TAN and KDB. A Bayesian classifier can graphically describe the conditional dependency among attributes. The proposed algorithm offers a trade-off between probability estimation and network structure complexity. The direct and indirect relationships between the predictive attributes and class variable should be considered simultaneously to achieve global optimisation and high-order dependency representation. By analysing the DAG inferred from the breast cancer data set of the SEER database we divided the attributes into two subgroups, namely, key attributes that should be considered first for cancer diagnosis and those that are independent of each other but are closely related to key attributes. The statistical analysis results clarify some of the causal relationships implicated in the DAG.	bayesian network;cardiovascular diseases;causality;cessation of life;class;conditional mutual information;cross reactions;cross-validation (statistics);data mining;directed acyclic graph;global optimization;graph - visual representation;graphical user interface;greedy algorithm;inference;mammary neoplasms;mathematical optimization;maxima and minima;seer-sem;triacetoneamine-n-oxyl;cancer diagnosis;emotional dependency	LiMin Wang	2015		10.1186/s13040-015-0046-4	text mining;causality;computer science;bioinformatics;data science;data mining	ML	5.006545333533732	-71.02158223350575	170947
44f79b16599dccc46cb1c72ef0ab21106e6c948d	decision tree induction to prediction of prognosis in severe traumatic brain injury of brazilian patients from florianopolis city	brain;pattern classification brain data mining decision trees medical computing;mortality decision tree induction method prognosis prediction traumatic brain injury brazilian patients florianopolis city data mining c4 5 algorithm public health problem morbidity;data mining;medical computing;pattern classification;decision trees;data mining classification algorithms databases algorithm design and analysis decision trees brain injuries data models	The data mining consists in identification of characteristics and relationships between data, aiming the transformation of these into useful knowledge. The obtainment of knowledge occurs trough tasks, methods and algorithms that have specific purposes, and that are applied according to the goals of analysis. The analysis showed in this article consists in application of data mining by task of classification by the method of decision tree induction by the C4.5 algorithm for the prediction of prognosis in severe traumatic brain injury. The traumatic brain injury is a public health problem, constituting in one of the main causes of morbidity and mortality. In the development were performed the steps of preprocessing, the application of data mining and the evaluation of generated model, obtaining the accuracy of 87%.	c4.5 algorithm;data mining;decision tree;preprocessor	Merisandra C. M. Garcia;Evandro T. Martins;Fernando M. Azevedo	2013	13th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2013.6701601	computer science;data science;decision tree;data mining;operations research	ML	4.3572301004184535	-76.66484423804405	171394
deecb65b898ab3f3db381f432536bf3eba306bbb	a crowdsourcing approach to developing and assessing prediction algorithms for aml prognosis	acute myeloid leukemia;systems biology;cancer treatment;machine learning;human genetics;proteomics;prognosis;proteomic databases	Acute Myeloid Leukemia (AML) is a fatal hematological cancer. The genetic abnormalities underlying AML are extremely heterogeneous among patients, making prognosis and treatment selection very difficult. While clinical proteomics data has the potential to improve prognosis accuracy, thus far, the quantitative means to do so have yet to be developed. Here we report the results and insights gained from the DREAM 9 Acute Myeloid Prediction Outcome Prediction Challenge (AML-OPC), a crowdsourcing effort designed to promote the development of quantitative methods for AML prognosis prediction. We identify the most accurate and robust models in predicting patient response to therapy, remission duration, and overall survival. We further investigate patient response to therapy, a clinically actionable prediction, and find that patients that are classified as resistant to therapy are harder to predict than responsive patients across the 31 models submitted to the challenge. The top two performing models, which held a high sensitivity to these patients, substantially utilized the proteomics data to make predictions. Using these models, we also identify which signaling proteins were useful in predicting patient therapeutic response.	cell signaling;classification;congenital abnormality;crowdsourcing;disease-free survival;forecast of outcome;gain;genetic heterogeneity;hematologic neoplasms;leukemia, myelocytic, acute;myeloid leukemia;open platform communications;patients;proteomics;algorithm;pediatric acute myeloblastic leukemia	David Noren;Byron Long;Raquel Norel;Kahn Rrhissorrakrai;Kenneth R. Hess;Chenyue W. Hu;Alex Bisberg;André Schultz;Erik Engquist;Li Liu;Xihui Lin;Gregory M. Chen;Honglei Xie;Geoffrey A. M. Hunter;Paul C. Boutros;Oleg Stepanov;Dream 9 AML-OPC Consortium;Thea Norman;Stephen H. Friend	2016		10.1371/journal.pcbi.1004890	biology;bioinformatics;proteomics;systems biology;human genetics	ML	6.883696396020688	-75.58809504906944	171608
c42c5349b8a0d0063a191a8d65bce462c68b0bbf	molecular robotics approach for constructing an artificial cell model		Prototype artificial cell models with designed functional molecules are presented here. Artificial molecular devices based on a giant liposome were prepared to obtain specific properties that cannot be obtained from natural cells. In this context, artificial cell research is seen an extension of “molecular robotics” research. Cooperative and integrated chemical systems will be constructed from the molecular devices. Here, we present the 3 aspects of the study model: (1) geneexpressing cell model encapsulated in the liposome to simulate membrane protein synthesis, (2) multirole molecular device with a designed DNA nanostructure on the cellular membrane, and (3) designed membrane peptide device for surface recognition. Although these devices are inspired by living cell functions, such goaloriented systems are free from the constraints of natural history and evolution. These artificial devices may be integrated to develop novel tools for living systems.	artificial cell;living systems;prototype;robotics;simulation	Shin-ichiro M. Nomura;Yusuke Sato;Kei Fujiwara	2013		10.7551/978-0-262-31709-2-ch070	biology	Robotics	3.859297922105131	-66.27318615413009	171712
fad32499aea5c6d4086dcfb87c4a8bfb23ea0d27	a novel approach for cancer outcome prediction using personalized classifier	subtype;outcome prediction;personalized classification	Cancer patients can be classifier into different subtypes but an obvious question can ask, whether these predefined subtypes can be helpful to detect the risk of disease in advance which is a well known problem in cancer biology. In this article we address these issues of using subtypes for disease outcome prediction and propose a personalized classification approach by relaxing the predefined subtype idea. Finally, we find that our proposed method is helpful to develop prediction model which can be useful in clinical practice to predict disease outcome. Thus the idea proposed here has better accuracy and application than conventional approaches for cancer outcome prediction.	personalization;statistical classification	Md. Jamiul Jahid;Jianhua Ruan	2012		10.1145/2401603.2401619	medicine;pathology;bioinformatics;data mining	ML	6.249695359698862	-76.79853410215267	172037
3af44aeb39a545a9fbf00d65906588670abc2d2f	applying risk models on patients with unknown predictor values: an incremental learning approach	cluster analysis;risk;theoretical models	In clinical practice, many patients may have unknown or missing values for some predictors, causing that the developed risk models cannot be directly applied on these patients. In this paper, we propose an incremental learning approach to apply a developed risk model on new patients with unknown predictor values, which imputes a patient's unknown values based on his/her k-nearest neighbors (k-NN) from the incremental population. We perform a real world case study by developing a risk prediction model of stroke for patients with Type 2 diabetes mellitus from EHR data, and incrementally applying the risk model on a sequence of new patients. The experimental results show that our risk prediction model of stroke has good prediction performance. And the k-nearest neighbors based incremental learning approach for data imputation can gradually increase the prediction performance when the model is applied on new patients.	cerebrovascular accident;diabetes mellitus;diabetes mellitus, insulin-dependent;financial risk modeling;geo-imputation;increment;k-nearest neighbors algorithm;kerrison predictor;missing data;patients;statistical imputation	Enliang Xu;Xiang Li;Jing Mei;Shiwan Zhao;Gang Hu;Eryu Xia;Haifeng Liu;Guo Tong Xie;Meilin Xu;Xuejun Li	2017	Studies in health technology and informatics	10.3233/978-1-61499-830-3-639	incremental learning;computer science;artificial intelligence;pattern recognition	ML	5.29378753029305	-76.43664140560814	172315
92c543bd7eae8275c859fc13b108f0255c0dd391	practical approaches to mining of clinical datasets : from frameworks to novel feature selection	computer science	....................................................................................................................... ii ACKNOWLEDGEMENT ................................................................................................. iv DECLARATION ................................................................................................................ v LIST OF TABLES ............................................................................................................. xi LIST OF FIGURES ......................................................................................................... xiii LIST OF ALGORITHMS ................................................................................................. xv LIST OF ABBREVIATIONS .......................................................................................... xvi NOTATIONS .................................................................................................................... xx INTRODUCTION ........................................................................................ 1 CHAPTER 1 1.1 Clinical dataset ...................................................................................... 3 1.1.1 Incomplete, errors and noisy data ................................................... 7 1.1.2 Diverse clinical features and their scales ........................................ 8 1.1.3 Class imbalance ............................................................................... 8 1.1.4 Large dimensionality ....................................................................... 8 1.2 Motivation and research problem.......................................................... 9 1.3 Research aim and objectives ............................................................... 11 1.4 Thesis structure ................................................................................... 12 FRAMEWORKS FOR DATA MINING ................................................... 13 CHAPTER 2 2.1 Introduction ......................................................................................... 13 2.2 Data mining ......................................................................................... 15 2.2.1 Predictive modelling ..................................................................... 15 2.2.2 Descriptive modelling ................................................................... 16 2.2.3 Supervised and unsupervised learning .......................................... 17 2.3 Data mining frameworks ..................................................................... 19 2.3.1 CRISP-DM .................................................................................... 20 2.3.2 SEMMA ........................................................................................ 22 viii 2.3.3 Handling clinical data framework (HCDF)................................... 24 2.3.3.1 Data analysis ........................................................................... 25 2.3.3.2 Imputation ............................................................................... 27 2.3.3.3 Data sampling.......................................................................... 28 2.3.3.4 Dimensionality reduction ........................................................ 29 2.3.3.5 Classification ........................................................................... 30 2.3.3.6 Evaluation ............................................................................... 30 2.4 Summary ............................................................................................. 32 HANDLING MISSING VALUES ............................................................. 34 CHAPTER 3 3.1 Introduction ......................................................................................... 34 3.2 Types of missing values ...................................................................... 36 3.3 Discarding method .............................................................................. 37 3.3.1 Complete case analysis  ............................................................. 38 3.3.2 Available case analysis ............................................................... 38 3.4 Techniques for imputing ..................................................................... 39 3.4.1 Single imputation .......................................................................... 41 3.4.2 Multiple Imputation ...................................................................... 42 3.4.3 Imputation methods ....................................................................... 43 3.4.3.1 Most common value imputation (MCI) .................................. 43 3.4.3.2 Concept most common value imputation (CMCI) .................. 44 3.4.3.3 K-nearest neighbour imputation (KNNI) ................................ 44 3.4.3.4 K-means clustering imputation (KMI) .................................... 45 3.4.3.5 Fuzzy K-means clustering imputation (FKMI) ....................... 47 3.4.3.6 Expectation-maximization imputation (EMI) ......................... 48 3.4.3.7 Support vector machine imputation (SVMI)........................... 49 3.5 Imputation for clinical data ................................................................. 50 3.5.1 Missing values imputation for clinical datasets ............................ 51 3.5.2 Metrics for imputation................................................................... 52 3.5.2.1 Statistics .................................................................................. 52 3.5.2.2 Data distribution ...................................................................... 55 3.5.2.3 Performance indicators............................................................ 59 3.6 Summary ............................................................................................. 62 PRINCIPLES FOR DIMENSIONALITY REDUCTION CHAPTER 4 TECHNIQUES ............................................................................................... 64 4.		Nongnuch Poolsawad	2014			computer science;bioinformatics;data science;data mining	ML	5.265490046225229	-75.2737723095229	172539
46719983aef462292f6f163454fb154e503d5b94	evaluation of classification methods for the prediction of hospital length of stay using medicare claims data	healthcare;naive bayes;length of stay;machine learning;decision trees;prediction	In this paper, we investigate the performance of a series of classification methods for the prediction of the hospital Length of Stay (LOS), based on two temporally sequential clinical scenarios. We used a 2012 Medicare Provider Analysis and Review (MedPar) dataset, which contains records of Medicare beneficiaries who used inpatient hospital services. Our subset included 300,000 randomly selected cases. During the prepossessing we added new features and linked our data with external datasets, using common key identifiers. In the first scenario our goal was to predict the LOS using a subset of information which is readily available to the clinician upon the patient admission, while the second scenario assumes that there is available additional data (information on the patient diagnosis and clinical procedures). For our experiments we used three different classifiers: Naïve Bayes, AdaBoost and C4.5 Decision tree, for two different LOS cut-off points (4 day and 12 day hospital stay). The overall performance of our classifiers was ranging from fair to very good. On the other hand the true positive rate, that is the correct classification of the long hospital stays, was low, with an exception of Naïve Bayes, which demonstrated significantly better performance in the second scenario. Our results indicate that Naïve Bayes may be used for the prediction of the in-hospital LOS. Our analysis also indicates that the MedPar data combined with other data resources has the potential to provide a good basis for robust prediction analytics in hospitals.	adaboost;c4.5 algorithm;decision tree;experiment;identifier;naive bayes classifier;randomness;sensitivity and specificity;temporal logic	Dimitrios Zikos;Konstantinos Tsiakas;Fadiah Qudah;Vassilis Athitsos;Fillia Makedon	2014		10.1145/2674396.2674430	naive bayes classifier;prediction;computer science;data science;machine learning;decision tree;data mining	ML	6.7439335448544755	-75.55263721934244	172682
a71e843b0b27f3c34505b0b8551b5b86ab23eb7d	towards the applied hybrid model in decision making: support the early diagnosis of type 2 diabetes	bayesian network;expert systems;multicriteria;diabetes;early diagnosis	A hybrid model, combining Bayesian network and a multicriteria method, is presented in order to assist with the decision making process about which questions would be more attractive to the definition of the diagnosis of Diabetes type 2. We have proposed the application of an expert system structured in probability rules and structured representations of knowledge in production rules and probabilities (Artificial Intelligence - AI). The importance of the early diagnosis associated with the appropriate treatment is to decrease the chance of developing the complications of diabetes, reducing the impact on our society. Diabetes is a group of metabolic diseases characterized by hyperglycemia resulting from defects in insulin secretion, insulin action, or both.	nsa product types	Andrea Carvalho Menezes;Plácido Rogério Pinheiro;Mirian Calíope Dantas Pinheiro;Tarcísio Pequeno Cavalcante	2012		10.1007/978-3-642-34062-8_84	systems engineering;engineering;artificial intelligence;machine learning	AI	4.585423281538645	-78.36699007765118	172850
acd9bbdca75092c8e0d962730c6847a14b5b559f	ensemble of classifiers for length of stay prediction in colorectal cancer		The paper puts forward an ensemble of state-of-the-art clas- sifiers - support vector machines, neural networks and decision trees - to estimate the length of stay after surgery in patients diagnosed with colorectal cancer. The three paradigms are brought together in order to achieve both a more accurate prediction through a voting scheme and transparency of the discriminative guidelines through visual rules. The results support the theoretical assumptions and are confirmed by the physicians.		Ruxandra Stoean;Catalin Stoean;Adrian Sandita;Daniela Ciobanu;Cristian Mesina	2015		10.1007/978-3-319-19258-1_37	machine learning;pattern recognition;data mining	NLP	6.797887467189155	-76.99034933219221	173154
1680e41318c4bc7c0ea147c40c88b15c206c6d26	prescriptive analytics through constrained bayesian optimization		Prescriptive analytics leverages predictive data mining algorithms to prescribe appropriate changes to alter a predicted outcome of undesired class to a desired one. As an example, based on the conversation of a reformed addict on a message board, prescriptive analytics may predict the intervention required. We develop a novel prescriptive analytics solution by formulating a constrained Bayesian optimization problem to find the smallest change that we need to make on an actionable set of features so that with sufficient confidence an instance can be changed from an undesirable class to the desirable class. We use two public health dataset, multi-year CDC dataset on disease prevalence across the 50 states of USA and alcohol related data from Reddit to demonstrate the usefulness of our results.	bayesian optimization;program optimization	Haripriya Harikumar;Santu Rana;Sunil Gupta;Thin Nguyen;Ramachandra Kaimal;Svetha Venkatesh	2018		10.1007/978-3-319-93034-3_27	machine learning;artificial intelligence;bayesian optimization;computer science;data mining;conversation;prescriptive analytics;constrained optimization	ML	3.6408305434060573	-74.67659272752157	173968
2fbe224d35e0a8ac42b0c44dcb30fe6abb7ee98d	prediction of lung cancer incidence on the low-dose computed tomography arm of the national lung screening trial: a dynamic bayesian network	structure learning;expert driven networks;cancer incidence;dynamic bayesian networks;annual nlst cancer risk;individualized lung cancer screening;lung stage cancer state space	INTRODUCTION Identifying high-risk lung cancer individuals at an early disease stage is the most effective way of improving survival. The landmark National Lung Screening Trial (NLST) demonstrated the utility of low-dose computed tomography (LDCT) imaging to reduce mortality (relative to X-ray screening). As a result of the NLST and other studies, imaging-based lung cancer screening programs are now being implemented. However, LDCT interpretation results in a high number of false positives. A set of dynamic Bayesian networks (DBN) were designed and evaluated to provide insight into how longitudinal data can be used to help inform lung cancer screening decisions.   METHODS The LDCT arm of the NLST dataset was used to build and explore five DBNs for high-risk individuals. Three of these DBNs were built using a backward construction process, and two using structure learning methods. All models employ demographics, smoking status, cancer history, family lung cancer history, exposure risk factors, comorbidities related to lung cancer, and LDCT screening outcome information. Given the uncertainty arising from lung cancer screening, a cancer state-space model based on lung cancer staging was utilized to characterize the cancer status of an individual over time. The models were evaluated on balanced training and test sets of cancer and non-cancer cases to deal with data imbalance and overfitting.   RESULTS Results were comparable to expert decisions. The average area under the curve (AUC) of the receiver operating characteristic (ROC) for the three intervention points of the NLST trial was higher than 0.75 for all models. Evaluation of the models on the complete LDCT arm of the NLST dataset (N=25,486) demonstrated satisfactory generalization. Consensus of predictions over similar cases is reported in concordance statistics between the models' and the physicians' predictions. The models' predictive ability with respect to missing data was also evaluated with the sample of cases that missed the second screening exam of the trial (N=417). The DBNs outperformed comparison models such as logistic regression and naïve Bayes.   CONCLUSION The lung cancer screening DBNs demonstrated high discrimination and predictive power with the majority of cancer and non-cancer cases.		Panayiotis Petousis;Simon X. Han;Denise R. Aberle;Alex A. T. Bui	2016	Artificial intelligence in medicine	10.1016/j.artmed.2016.07.001	computer science;machine learning;dynamic bayesian network;statistics	ML	7.074222046009013	-75.33045927739792	174401
3bf9aac2af4711dff6268514f6eafdc6d23b383c	exploratory characterization of outliers in a multi-centre 1h-mrs brain tumour dataset	high dimensionality;outlier detection;proton magnetic resonance spectroscopy;dimensionality reduction;data visualization;medical decision support systems;expert opinion;data exploration;brain tumours;dimensional reduction;article;brain tumour	As part of the AIDTumour research project, the analysis of MRS data corresponding to various tumour pathologies is used to assist expert diagnosis. The high dimensionality of the MR spectra might obscure atypical aspects of the data that would jeopardize their automated classification and, as a result, the process of computer-based diagnostic assistance. In this paper, we put forward a method to overcome this potential problem that combines automatic outlier detection, visualization through dimensionality reduction, and expert opinion.	anomaly detection;dimensionality reduction;exploratory testing;minimal recursion semantics	Alfredo Vellido;Margarida Julià-Sapé;Enrique Romero;Carles Arús	2008		10.1007/978-3-540-85565-1_24	computer science;data science;machine learning;data mining;data visualization;dimensionality reduction	ML	2.8356155281625504	-77.29092704089278	174981
21974db20892c30913e2fcc32155407ed4fae2b4	a machine learning approach to differentiating bacterial from viral meningitis	antibiotic treatment;school of no longer in use;electronics and computer science;rough set theory diseases learning artificial intelligence medical diagnostic computing neural nets probability;bacterial meningitis;probability;neural nets;automated diagnosis;rough set theory;viral meningitis;automated diagnosis machine learning bacterial meningitis viral meningitis blood cerebrospinal fluid gram stain antibiotic treatment rough sets probabilistic neural network clinical dataset dimensionality reduction;clinical dataset;machine learning microorganisms rough sets testing performance evaluation neural networks blood antibiotics laboratories costs;dimensionality reduction;machine learning;blood;diseases;rough sets;learning artificial intelligence;probabilistic neural network;medical diagnostic computing;cerebrospinal fluid;gram stain	Clinical reports indicate that differentiating bacterial from viral (aseptic) meningitis is still a difficult issue, compounded by factors such as age and time of presentation. Clinicians routinely rely on the results from blood and cerebrospinal fluid (CSF) to discriminate bacterial from viral meningitis. Tests such as the CSF Gram stain performed prior to broad-spectrum antibiotic treatment yield sensitivities between 60 and 92%. Sensitivity can be increased by performing additional laboratory testing, but the results are never completely accurate and are not cost effective in many cases. In this study, we wished to determine if a machine learning approach, based on rough sets and a probabilistic neural network could be used to differentiate between viral and bacterial meningitis. We analysed a clinical dataset containing records for 581 cases of acute bacterial or viral meningitis. The rough sets approach was used to perform dimensionality reduction in addition to classification. The results were validated using a probabilistic neural network. With an overall accuracy of 98%, these results indicate rough sets is a useful approach to differentiating bacterial from viral meningitis	artificial neural network;computational fluid dynamics;dimensionality reduction;machine learning;n-gram;probabilistic neural network;rough set	Kenneth Revett;Florin Gorunescu;Marius Ene	2006	IEEE John Vincent Atanasoff 2006 International Symposium on Modern Computing (JVA'06)	10.1109/JVA.2006.2	medicine;pathology;artificial intelligence;machine learning	SE	7.628465173336462	-76.50152425883101	177218
423fb07f73f8dc48a8834a2a5533f960b7b53594	expert system based on neuro-fuzzy rules for diagnosis breast cancer	diagnostic tool;fuzzy rules;medical diagnosis breast cancer;artificial intelligent;negative predictive value;neuro fuzzy;artificial intelligence;medical application;mammography;breast cancer;medical diagnosis;health care system;expert system	Recent advances in the field of artificial intelligence have led to the emergence of expert systems for medical applications. Moreover, in the last few decades computational tools have been designed to improve the experiences and abilities of physicians for making decisions about their patients. Breast cancer is the commonest cancer in women and is the second leading cause of cancer death (Jemal et al., 2003). Although it is curable when detected early, about one third of women with breast cancer die of the disease (Scheidhauer, Walter, & Seemann, 2004). In this study, we have developed an expert system that we called as an Ex-DBC (Expert system for Diagnosis of Breast Cancer), because differentiating between benign and malignant mammographic findings, however, is quite difficult. Only 15–30% of biopsies performed on nonpalpable but mammographically suspicious lesions prove malignant (Hall, Storella, Silverstone, & Wyshak, 1988). The golden standard for diagnosis of breast cancer is biopsy. But, biopsy can be a source of patient discomfort, bleeding and infection, and can burden the health care system with extra costs. Thus, to reduce unnecessary biopsy rate have acquired big importance. The fuzzy rules which will be use in inference engine of Ex-DBC system were found by using neurofuzzy method. Ex-DBC can be used as a strong diagnostic tool with 97% specificity, 76% sensitivity, 96% positive and 81% negative predictive values for diagnosing of breast cancer. That the developed system’s positive predictive is high is very important. By means of this system can be prevented unnecessary biopsy. Beside it can be benefited from this system for training of students in medicine. 2010 Elsevier Ltd. All rights reserved.	artificial intelligence;casio exilim;computation;emergence;experience;expert system;hall effect;inference engine;neuro-fuzzy;sensitivity and specificity	Ali Keleş;Aytürk Keles;Ugur Yavuz	2011	Expert Syst. Appl.	10.1016/j.eswa.2010.10.061	computer science;artificial intelligence;neuro-fuzzy;breast cancer;medical diagnosis;expert system	AI	5.574962076958905	-78.3849190087498	177393
f9d113eb37d2a135e7f6818e4684dc2f1fb0e88a	artificial neural network medical decision support tool: predicting transfusion requirements of er patients	6 to 24 h artificial neural network medical decision support tool emergency room patients blood product transfusion patient care statistical model transfusion decisions transfusion planning patient transfusion requirement trauma patient readily available information patient record backpropagation neural network training backpropagation neural network testing sensitivity analysis specificity analysis blood units 2 to 6 h;sensitivity and specificity;backpropagation neural network;neural nets;trauma backpropagation neural network transfusion;bepress selected works;trauma;haemodynamics;backpropagation;patient care;statistical model;medical expert systems;mean absolute difference;statistical analysis;emergency room;medical information systems;blood transfusion critical care decision support systems management diagnosis computer assisted emergency service hospital humans neural networks computer reproducibility of results sensitivity and specificity therapy computer assisted wounds and injuries;decision support systems;transfusion;surgery;rapid assessment;backpropagation neural network transfusion trauma;surgery medical information systems backpropagation neural nets decision support systems medical expert systems patient care haemodynamics statistical analysis;artificial neural networks erbium blood predictive models hospitals costs surgery medical diagnostic imaging testing backpropagation;medical decision support;artificial neural network;neural network	Blood product transfusion is a financial concern for hospitals and patients. Efficient utilization of this dwindling resource is a critical problem if hospitals are to maximize patient care while minimizing costs. Traditional statistical models do not perform well in this domain. An additional concern is the speed with which transfusion decisions and planning can be made. Rapid assessment in the emergency room (ER) necessarily limits the amount of usable information available (with respect to independent variables available). This study evaluates the efficacy of using artificial neural networks (ANNs) to predict the transfusion requirements of trauma patients using readily available information. A total of 1016 patient records are used to train and test a backpropagation neural network for predicting the transfusion requirements of these patients during the first 2, 2-6, and 6-24 h, and for total transfusions. Sensitivity and specificity analysis are used along with the mean absolute difference between blood units predicted and units transfused to demonstrate that ANNs can accurately predict most ER patient transfusion requirements, while only using information available at the time of entry into the ER.	accident and emergency department;artificial neural network;backpropagation;beds;blood product;decision support system;erdős–rényi model;financial cost;hematological disease;patients;projections and predictions;requirement;resource allocation;sensitivity and specificity;statistical model;wounds and injuries	Stanislaw Walczak	2005	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2005.847510	statistical model;medicine;computer science;backpropagation;machine learning;hemodynamics;mean difference;medical emergency;artificial neural network;surgery;statistics	ML	6.5309761995624	-75.18958689167192	177887
6275aa21331a2712222b7ab2116e9589e21ae82c	prediction of manipulation actions	hand motions;online action recognition;journal article;forces on the hand;action prediction	By looking at a person’s hands, one can often tell what the person is going to do next, how his/her hands are moving and where they will be, because an actor’s intentions shape his/her movement kinematics during action execution. Similarly, active systems with real-time constraints must not simply rely on passive video-segment classification, but they have to continuously update their estimates and predict future actions. In this paper, we study the prediction of dexterous actions. We recorded videos of subjects performing different manipulation actions on the same object, such as “squeezing”, “flipping”, “washing”, “wiping” and “scratching” with a sponge. In psychophysical experiments, we evaluated human observers’ skills in predicting actions from video sequences of different length, depicting the hand movement in the preparation and execution of actions before and after contact with the object. We then developed a recurrent neural network based method for action prediction using as input image patches around the hand. We also used the same formalism to predict the forces on the finger tips using for training synchronized video and force data streams. Evaluations on two new datasets show that our system closely matches human performance in the recognition task, and demonstrate the ability of our algorithms to predict in real time what and how a dexterous action is performed.	algorithm;artificial neural network;experiment;human reliability;real-time locating system;recurrent neural network;semantics (computer science)	Cornelia Fermüller;Fang Wang;Yezhou Yang;Konstantinos Zampogiannis;Yi Zhang;Francisco Barranco;Michael Pfeiffer	2017	International Journal of Computer Vision	10.1007/s11263-017-0992-z	artificial intelligence;computer vision;machine learning;computer science;data stream mining;formalism (philosophy);kinematics;recurrent neural network	Vision	-1.6052690171536113	-75.51414621444634	178195
5736791b4ae53db9c6d81438900229f3ec6fd6ac	multimodal latent variable analysis		Consider a set of multiple, multimodal sensors capturing a complex system or a physical phenomenon of interest. Our primary goal is to distinguish the underlying sources of variability manifested in the measured data. The first step in our analysis is to find the common source of variability present in all sensor measurements. We base our work on a recent paper, which tackles this problem with alternating diffusion (AD). In this work, we suggest to further the analysis by extracting the sensor-specific variables in addition to the common source. We propose an algorithm, which we analyze theoretically, and then demonstrate on three different applications: a synthetic example, a toy problem, and the task of fetal ECG extraction.	algorithm;complex system;heart rate variability;latent variable;multimodal interaction;sensor;spatial variability;synthetic intelligence;toy problem	Vardan Papyan;Ronen Talmon	2018	Signal Processing	10.1016/j.sigpro.2017.07.016	complex system;diffusion map;machine learning;nonlinear dimensionality reduction;sensor fusion;computer science;artificial intelligence;toy problem;phenomenon;latent variable	ML	-0.9341429940755256	-75.6581163102434	178260
3ca7d93ea60ef1c90288b01e7f74059873db2d07	adjusting outbreak detection algorithms for surveillance during epidemic and non-epidemic periods	sensitivity and specificity;qr355 virology;population surveillance;ra public aspects of medicine;algorithms;humans;disease outbreaks	Many aberration detection algorithms are used in infectious disease surveillance systems to assist in the early detection of potential outbreaks. In this study, we explored a novel approach to adjusting aberration detection algorithms to account for the impact of seasonality inherent in some surveillance data. By using surveillance data for hand-foot-and-mouth disease in Shandong province, China, we evaluated the use of seasonally-adjusted alerting thresholds with three aberration detection methods (C1, C2, and C3). We found that the optimal thresholds of C1, C2, and C3 varied between the epidemic and non-epidemic seasons of hand-foot-and-mouth disease, and the application of seasonally adjusted thresholds improved the performance of outbreak detection by maintaining the same sensitivity and timeliness while decreasing by nearly half the false alert rate during the non-epidemic season. Our preliminary findings suggest a general approach to improving aberration detection for outbreaks of infectious disease with seasonally variable incidence.	alert:type:point in time:^patient:nominal;communicable diseases;foot-and-mouth disease;hand, foot and mouth disease;incidence matrix;mouth diseases;seasonality;state or province of exposure to illness:loc:pt:^patient:nom;algorithm	Zhongjie Li;Shengjie Lai;David L. Buckeridge;Honglong Zhang;Yajia Lan;Weizhong Yang	2012	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2011-000126	simulation;medicine;pathology;computer science;operations research	ML	9.784780988453285	-77.3482525004803	178268
70e927528d40f2a4a4603fa869aeec6cf11a5eeb	exploratory data analysis of insulin therapy in the elderly type 2 diabetic patients		The ultimate goal of diabetes treatment is to improve the health of diabetics as well as to reduce the risk of macrovascular and microvascular complications associated with diabetes by maintaining the quality of life of the patients. For this purpose, we conducted a study in which we assessed the insulin needs in the elderly type 2 diabetic patients in order to develop an insulin treatment model from physiological parameters. At this time, the needs of this specific population are not well known because of the combination of persistent insulin resistance and reduced capacity of insulin secretion. Therefore, we considered a cohort of patients older than 65 years. We used data mining methods to replace the empirical and typically used patterns at the initiation of insulin therapy. This process is meant for better assistance during the adjustment of the insulin treatment of the type 2 diabetes patients. This study is part of a PhD thesis funded by the company AXON’ and conducted in Reims Hospital University and research	exploratory testing;nsa product types	Afshan Nourizadeh;Frédéric Blanchard;Amine Aït Younes;Brigitte Delemer;Michel Herbin	2013	Stud. Inform. Univ.		type 2 diabetes;pattern recognition;artificial intelligence;exploratory data analysis;computer science;cohort;insulin resistance;insulin;population;quality of life;internal medicine;diabetes mellitus	ML	7.15373431482004	-75.82085941426566	178434
63816259608c4ee3ae2fafa555ffa85e6baebb2e	an approach of soft computing applications in clinical neurology	neural networks;soft computing;neurology;fuzzy logic	This paper briefly introduces various soft computing techniques and presents miscellaneous applications in clinical neurology domain. The aim is to present the large possibilities of applying soft computing to neurology related problems. Recently published data about use of soft computing in neurology are observed from the literature, surveyed and reviewed. This study detects which methodology or methodologies of soft computing are frequently used together to solve the specific problems of medicine. Recent developments in medicine show that diagnostic expert systems can help physicians make a definitive diagnosis. Automated diagnostic systems are important applications of pattern recognition, aiming at assisting physicians in making diagnostics decisions. Soft computing models have been researched and implemented in neurology for a very long time. This paper presents applications of soft computing models of the cutting edge researches in neurology domain.	soft computing	Dragan Simic;Svetlana Simic;Ilija Tanackov	2011		10.1007/978-3-642-21222-2_52	fuzzy logic;neurology;computer science;artificial intelligence;machine learning;soft computing;artificial neural network	HPC	5.190164541953086	-78.1830568663095	178475
833887f9e4d550ed46512419e03197d838f97646	mlcaf: multi-level cross-domain semantic context fusioning for behavior identification	context-awareness;fusioning;human behavior identification;ontologies;reasoning	The emerging research on automatic identification of user's contexts from the cross-domain environment in ubiquitous and pervasive computing systems has proved to be successful. Monitoring the diversified user's contexts and behaviors can help in controlling lifestyle associated to chronic diseases using context-aware applications. However, availability of cross-domain heterogeneous contexts provides a challenging opportunity for their fusion to obtain abstract information for further analysis. This work demonstrates extension of our previous work from a single domain (i.e., physical activity) to multiple domains (physical activity, nutrition and clinical) for context-awareness. We propose multi-level Context-aware Framework (mlCAF), which fuses the multi-level cross-domain contexts in order to arbitrate richer behavioral contexts. This work explicitly focuses on key challenges linked to multi-level context modeling, reasoning and fusioning based on the mlCAF open-source ontology. More specifically, it addresses the interpretation of contexts from three different domains, their fusioning conforming to richer contextual information. This paper contributes in terms of ontology evolution with additional domains, context definitions, rules and inclusion of semantic queries. For the framework evaluation, multi-level cross-domain contexts collected from 20 users were used to ascertain abstract contexts, which served as basis for behavior modeling and lifestyle identification. The experimental results indicate a context recognition average accuracy of around 92.65% for the collected cross-domain contexts.	addresses (publication format);automatic identification and data capture;behavior model;chronic disease;computation (action);conformity;context awareness;exercise;genetic heterogeneity;high- and low-level;inductive reasoning;inference;mental association;numerous;ontology;open-source software;pa-risc;real-time locating system;relevance;rule (guideline);semantic web rule language;silo (dataset);triplestore;ubiquitous computing	Muhammad Asif Razzaq;Claudia Villalonga;Sungyoung Lee;Usman Akhtar;Maqbool Ali;Eun-Soo Kim;Asad Masood Khattak;Hyonwoo Seung;Tae Ho Hur;Jae Hun Bang;Dohyeong Kim;Wajahat Ali Khan	2017		10.3390/s17102433	ubiquitous computing;data mining;ontology (information science);ontology;context model;computer science;context awareness;knowledge management	HCI	-2.860668907651786	-69.91919485103497	178586
e525f21de3eebd35dfc2749eda5614fb47c9b2a3	adjoint systems for models of cell signaling pathways and their application to parameter fitting	innate immune response;molecular biophysics;ordinary differential equations;proteins;sensitivity analysis;mathematical model;time measurement;biochemistry;ordinary differential equation;discrete time;backpropagation;immune response;sensitivity;genetics;cell signaling;performance index;fitting;modeling	The paper concerns the problem of fitting mathematical models of cell signaling pathways. Such models frequently take the form of sets of nonlinear ordinary differential equations. While the model is continuous in time, the performance index used in the fitting procedure involves measurements taken at discrete time moments. Adjoint sensitivity analysis is a tool which can be used for finding the gradient of a performance index in the space of parameters of the model. In the paper, a structural formulation of adjoint sensitivity analysis called the generalized backpropagation through time (GBPTT) is used. The method is especially suited for hybrid, continuous-discrete time systems. As an example, we use the mathematical model NF-kappaB of the regulatory module, which plays a major role in the innate immune response in animals.	backpropagation through time;cell signaling;curve fitting;gradient;immunity, innate;mathematical model;mathematics;nonlinear system	Krzysztof Fujarewicz;Marek Kimmel;Tomasz Lipniacki;Andrzej Swierniak	2007	IEEE/ACM Transactions on Computational Biology and Bioinformatics	10.1145/1299023.1299025	biology;ordinary differential equation;biochemistry;econometrics;mathematical optimization;adjoint equation;computer science;mathematics;statistics;molecular biophysics	Metrics	9.812446746497429	-67.9248295605172	178623
39c7eb52c49268530b0e4520f713e0847e6e1c08	face extraction using skin color and pca face recognition in a mobile cloudlet environment	vm synthesis;cyber foraging face recognition principal component analysis cloud computing cloudlets vm synthesis mobile cloud computing;mobile cloud computing;cloudlets;cyber foraging;principal component analysis cloud computing computer vision face recognition feature extraction image colour analysis mobile computing;servers;face recognition;principal component analysis;mobile communication;mobile handsets;face;cloud computing;face recognition cloud computing face servers mobile handsets principal component analysis mobile communication;face detection face extraction skin color pca face recognition mobile cloudlet environment mobile device mobile computing computer vision cyber foraging principal component analysis vmbased cloudlet	Mobile devices and mobile computing play an important role in the day to day life of people. Nowadays mobile devices are capable of doing very complex applications. But because of the resource scarcity of mobile devices, the compute intensive applications, like computer vision applications, are still found to be very difficult to execute on mobile devices. As a solution cyber foraging, where mobile devices offload either computation or data or both to a device within their network, can be applied. This paper explains our work in which we have successfully implemented a principal component analysis(PCA) based face recognition using VMBased cloudlet. In the face detection phase we are using a skin color based approach to extract the face region by which the accuracy and speed of the application is found to be increased.	cloudlet;computation;computer vision;cyber foraging;face detection;facial recognition system;mobile computing;mobile device;principal component analysis	M. V. PraseethaV.;S. Vadivel	2016	2016 4th IEEE International Conference on Mobile Cloud Computing, Services, and Engineering (MobileCloud)	10.1109/MobileCloud.2016.11	facial recognition system;face;embedded system;computer vision;mobile search;simulation;mobile telephony;cloud computing;computer science;operating system;mobile computing;server;principal component analysis	Mobile	0.8770705410349298	-71.03826658214365	179017
4f9ab83b6fc131f6398e73960847bff77eaccbb5	physical understanding via reduction of complex multiscale models: glycolysis in saccharomyces cerevisiae	eigenvalues and eigenfunctions;complex multiscale model reduction;biology computing;time scale;saccharomyces cerevisiae;manifolds;saccharomyces cerevisiae complex multiscale model reduction glycolysis;biological system modeling;spectrum;multiscale modeling;evolution biology;accuracy;limit cycle;mathematical model;sugar;glycolysis;microorganisms;algorithm design and analysis;sugar biology computing microorganisms reduced order systems;reduced order systems;mathematical model biological system modeling iterative algorithms fungi algorithm design and analysis evolution biology limit cycles computational biology genetics size measurement	We consider complex mathematical models that are characterized by a wide spectrum of time scales, the fastest of which are operative during the initial state only, leaving the slower ones to drive the system at later times. It is shown that very useful physical understanding can be acquired if the fast and slow dynamics are first separated and then analyzed. Existing algorithmic methodologies can be applied for this purpose. A demonstration of this approach is presented for a glycolysis model, the solution of which asymptotically evolves around a limit cycle.	fastest;limit cycle;mathematical model;multiscale modeling	Panayotis D. Kourdis;Dimitris A. Goussis;Ralf Steuer	2008	2008 8th IEEE International Conference on BioInformatics and BioEngineering	10.1109/BIBE.2008.4696657	biology;spectrum;algorithm design;glycolysis;manifold;bioinformatics;mathematical model;accuracy and precision;microorganism;limit cycle;ecology;multiscale modeling;statistics	Robotics	10.032038837540831	-67.87527445270825	179169
1eab38b5b14568efabe041e80227191107bb160a	intrinsic dynamics and pharmacometric model discrimination	hysteresis;medical computing;visual programming;graphical user interfaces;drugs plasmas hysteresis predictive models pain clinical trials safety blood delay effects open loop systems;clinical response intrinsic dynamics pharmacometric model discrimination graphic representations effect compartment model indirect response model simulation dynamic characteristics visual tools hysteresis curves differentiation parameter space;graphical representation;graphical user interfaces patient treatment hysteresis visual programming medical computing;parameter space;patient treatment;compartment model;technical report;dynamic characteristic	Comparison of two competing pharmacometric models is undertaken using graphic representations to elucidate their similarities and differences. The effect compartment model and the indirect response model are simulated and compared for a range of parameters to determine to what extent their dynamic characteristics can be matched. Visual tools such as hysteresis curves are used to examine model dynamics in a variety of ways. Differentiation is facilitated to the extent that competing models have distinctly defined dynamics. For the models studied, the degree of difference depends on the region in parameter space which in turn relates to clinical response.	hysteresis;multi-compartment model	Meyer Katzper	1995		10.1145/224401.224774	control engineering;simulation;hysteresis;computer science;technical report;graphical user interface;parameter space;visual programming language;world wide web;multi-compartment model;statistics	Web+IR	9.571367735959985	-69.83344752098398	179533
43487f73225e5da6f4e34ca1473b563c0fc7a9da	an optimization model for sequential decision-making applied to risk prediction after liver resection and transplantation	predictive value of tests;discriminant analysis;risk assessment	The paper demonstrates a sequential decision procedure, which allows for optimal cost-effective or early decision-making while maintaining given error constraints. As a specific example the construction of a sequential decision procedure to determine if a patient was a high risk patient or not is used. The advantages of the procedure are demonstrated by a surgical problem of risk prediction from a clinical study on liver resection and transplantation. Data are available pre, peri- and postoperative, and form the basis of three clinical scores. The quality of the procedure is measured in terms of sensitivity and specificity, and the procedure will be optimized in a way, that a priori given error constraints will be maintained. A decision theoretic model is introduced and a robust procedure is developed. The approach is feasible for any fixed number of continuous clinical test scores obtained in a time sequence. Two sets of scores derived form linear discriminant analysis and artificial neural networks are compared.	artificial neural network;decision making;decision problem;excision;hepatectomy;linear discriminant analysis;mathematical optimization;patients;sensitivity and specificity;theory;time series	Günter Tusch	1999	Proceedings. AMIA Symposium		liver transplantation;machine learning;artificial neural network;predictive value of tests;surgery;a priori and a posteriori;risk assessment;artificial intelligence;transplantation;linear discriminant analysis;medicine;resection	ML	7.219777001803041	-76.41341438939456	181040
6a541035817e087c08b21e361506bb73de63b758	analysis on meteorological conditions and health factors based on c4.5 algorithm	prediction algorithms;data mining;atmospheric impact health factors c4 5 algorithm boundary science biological conditions chemical conditions physical conditions health care services disease treatments decision tree algorithm medical meteorology prediction weather elements forecasting algorithm data mining algorithm;visualization medical meteorology decision tree neural network upper respiratory tract infection prediction of incidence;meteorology decision trees prediction algorithms algorithm design and analysis predictive models data mining data models;predictive models;decision trees;meteorology;algorithm design and analysis;patient treatment data mining decision trees diseases health care medical computing;data models	Medical Meteorology is a rising Boundary Science. It is the study of atmospheric impact on physical, chemical and biological conditions of human bodies for treatments of disease and services of health care. This paper analyzes current situation in the field of medical meteorology and discusses the performance and the characteristic of Decision Tree algorithm on medical meteorology prediction based on existing forecasting algorithm. This paper also builds models according to weather elements to predict the incidence of the day, and further validations will test the accuracy of the predictions from the model. Advantages and disadvantages will also be then analyzed. Finally, strengths and weaknesses of other data mining algorithm are discussed based on the problems of Decision Tree Algorithm.	artificial neural network;backtracking;big data;bivariate data;c4.5 algorithm;data mining;decision tree model;incidence matrix;list of algorithms;machine learning;predictive modelling;requirement;test set	Xu Han;Han Jiang;Yanfeng Jiang;Heng Gu;Siyue Zhang;Tao Lv;Cong Wang	2014	2014 IEEE 3rd International Conference on Cloud Computing and Intelligence Systems	10.1109/CCIS.2014.7175804	data modeling;algorithm design;prediction;decision tree learning;computer science;data science;decision tree;data mining;predictive modelling;operations research	Robotics	4.357800749396071	-76.44003199971912	181342
b2dfdfa067c5bb3e924dfa51e78d8e2b18744d34	chunking phenomenon in complex sequential skill learning in humans	teoria cognitiva;reponse temporelle;analyse amas;apprentissage sequence;skill learning;classification non supervisee;human behaviour;skill acquisition;memorizacion por bloque;cognitive theory;theorie cognitive;sequence learning;skill;cluster analysis;time response;clustering;habilidad;clasificacion no supervisada;habilete;unsupervised classification;analisis cluster;memorisation par bloc;respuesta temporal;chunking	Sequential skill learning is central to much of human behaviour. It is known that sequences are hierarchically organized into several chunks of information that enables efficient performance of the acquired skill. We present clustering analysis on response times as subjects learn finger movement sequences of length 24 arranged in two ways - 12 sets of two movements each and 6 sets of four movements each. The experimental results and the analysis point out that greater amount of reorganization of sequences into chunks is more likely when the set-size is kept lower and discuss the cognitive implications of these findings.	humans;shallow parsing	Chandrasekhar V. S. Pammi;Krishna P. Miyapuram;Raju S. Bapi;Kenji Doya	2004		10.1007/978-3-540-30499-9_44	sequence learning;computer science;artificial intelligence;machine learning;cluster analysis	ML	-0.932507343880032	-78.98131226394418	181357
1ffa47f9b4be5b11e7e1323b77e8febfed9caccc	virtual high throughput screening (vhts) on an optical high speed testbed		Malaria remains a global health concern, which kills over a million people each year. In this paper we present work extending the approach of the WISDOM initiative by focusing on the problems noticed during the first WISDOM challenge against malaria and test the newly established, high bandwidth optical Grid environment VIOLA for advanced bioinformatics applications using the UNICORE middleware service. In addition we present an approach to reduce the size of the compound database to improve the efficiency of the screening.		Mohammad Shahid;Wolfgang Ziegler;Vinod Kasam;Marc Zimmermann;Martin Hofmann-Apitius	2008	Studies in health technology and informatics		data mining;testbed;computer network;high-throughput screening;medicine	HPC	1.4373338121383088	-66.21234621086961	181383
fb3bd5c9f52d51d0cf41e7e6cb6a65895ee00048	computationally generated cardiac biomarkers: heart rate patterns to predict death following coronary attacks		Heart disease is the leading cause of death in the United States, claiming over 830,000 lives each year (34% of all deaths or roughly one death every 38 seconds). A similar situation exists in other parts of the world, where an estimated 40% of all deaths in the developing world by the year 2020 are expected to be due to heart disease. The risk of death in these patients can be substantially lowered through the delivery of appropriate treatments (e.g., pharmacological and surgical interventions). However, matching patients to treatments that are appropriate for their risk remains challenging. In this paper, we aim to address this challenge by developing novel computational biomarkers that can be used to risk stratify patients. Our focus is on identifying high risk behavior in large datasets of electrocardiographic (ECG) signals from patients who experienced mortality following coronary attacks and those that remained event free. We frame the problem of finding risk markers as the discovery of approximately conserved heart rate sequences that are significantly overrepresented in either high or low risk patients. We propose a randomized hashingand greedy centroid selection-based algorithm to efficiently discover such heart rate patterns in large highresolution ECG datasets captured continuously over long periods from thousands of patients. When evaluated on data from 3,067 patients in two separate cohorts, our pattern discovery algorithm was able to correctly identify patients at high risk of death, even after adjusting for information in existing heart rate-based risk stratification metrics. Moreover, our approach can be easily extended to other clinical and non-clinical applications focused on approximate sequential patterns discovery in massive time-	approximation algorithm;greedy algorithm;randomized algorithm	Chih-Chun Chia;Zeeshan Syed	2011		10.1137/1.9781611972818.63	computer science;artificial intelligence;machine learning;heart disease;heart rate;biomarker (medicine);cardiac biomarkers;cause of death;psychological intervention;internal medicine;cardiology	ML	4.314737950933659	-75.10098559420217	181425
95901468d16b2a2c36e3c78b850827b7355b68cc	new diagnostic criteria for polycythemia rubra vera: artificial neural network approach	model system;diagnostic criteria;feature extraction;clinical practice;missing data;polycythemia vera;artificial neural network	An ANN-based approach in optimal selection of input parameters for Polycythemia Vera (PV) diagnostics and classification based on this new input set is presented in this paper. Results showed that a trained artificial neural network, known as a robust modeling system, gives the solution for classification of “gray” PV diagnostic zones that occur in practice, and therefore it obtain high quality decisions not only for PV diagnostics, but also for selection of lab data as PV diagnostic symptoms in clinical practice. Ten lab and other clinical findings on 431 PV patients from original PVSG cohort and records on 91 patients with other myeloproliferative or secondary polycythemia were included in this study. Significant differences (p<0.001) were found in comparison of correct diagnostic classification of patients using a trained ANN (97.1%), and using PVSG diagnostic criteria (73.2%). A new set of lab parameters defined in our study as a set of ANN inputs gives a basis for better prediction of PV diagnosis than the set used in PVSG criteria. These results are an important step in specification of new PVSG diagnostic criteria applicable in clinical practice.	artificial neural network;display resolution;page view	Mehmed M. Kantardzic;Hazem M. Hamdan;Benjamin Djulbegovic;Adel Said Elmaghraby	2000			missing data;feature extraction;computer science;artificial intelligence;machine learning;data mining;artificial neural network	ML	6.680227493847009	-76.86393709845557	181616
622fc98c06857e52a77bb1246d4b06e33d3e2183	a generic framework for enhancing the interpretability of granular computing-based information	granular computing;medical computing data mining electric impedance imaging fuzzy logic gaussian processes;pragmatics;impedance;biomedical measurements;systematic data granulation algorithm;gaussian processes;fuzzy logic data mining humans fuzzy systems tomography biomedical measurements physics computing systems engineering and theory hospitals impedance;gaussian fuzzy membership functions;fuzzy rules elicitation;lungs;hospitals;data mining;physics computing;electric impedance imaging;systems engineering and theory;granular computing based information;medical computing;fuzzy logic;ventilation;granular cardinality;physiological expectations;electrical impedance tomography granular computing data mining fuzzy logic system transparency system interpretability knowledge discovery;feature extraction;interpretability features;linguistic hedges;lung ventilation;system transparency;electrical impedance tomography;biomedical dataset;humans;physiological expectations granular computing based information fuzzy logic interpretability features systematic data granulation algorithm fuzzy rules elicitation data mining granular cardinality gaussian fuzzy membership functions linguistic hedges biomedical dataset electrical impedance tomography lung ventilation;system interpretability;tomography;fuzzy systems;knowledge discovery	One of the main advantages of Granular Computing and Fuzzy Logic is the transparency and interpretability features that are available to the user. In this paper we present a systematic data granulation algorithm for the elicitation of Fuzzy rules and show how the granular data and relational information extracted during the data mining process can be translated into Fuzzy Logic statements with enhanced interpretability. Notions of granular cardinality, distribution and distance are used to apply linguistic hedges to two-sided Gaussian Fuzzy membership functions. The proposed methodology is applied to a biomedical dataset relating to Electrical Impedance Tomography (EIT) measurements of lung ventilation showing good agreement and interpretability between the captured knowledge and the theoretical and physiological expectations.	algorithm;data mining;electromagnetically induced transparency;fuzzy logic;granular computing;nominal impedance;tomography	George Panoutsos;Mahdi Mahfouf;Gary H. Mills;Brian H. Brown	2010	2010 5th IEEE International Conference Intelligent Systems	10.1109/IS.2010.5548394	artificial intelligence;machine learning;data mining;mathematics	Robotics	2.9492687552000176	-79.09606735369806	181952
51de9ac105df1d296be3e622790052f411e97a6c	epidemiological consequences of imperfect vaccines for immunizing infections	biological patents;biomedical journals;text mining;europe pubmed central;citation search;infectious disease dynamics;citation networks;imperfect vaccines;92b05;research articles;abstracts;open access;life sciences;clinical guidelines;92d25;full text;age structured population models;rest apis;orcids;europe pmc;37n25;biomedical research;bioinformatics;literature search	"""The control of some childhood diseases has proven to be difficult even in countries that maintain high vaccination coverage. This may be due to the use of imperfect vaccines and there has been much discussion on the different modes by which vaccines might fail. To understand the epidemiological implications of some of these different modes, we performed a systematic analysis of a model based on the standard SIR equations with a vaccinated component that permits vaccine failure in degree (""""leakiness""""), take (""""all-or-nothingness"""") and duration (waning of vaccine-derived immunity). The model was first considered as a system of ordinary differential equations, then extended to a system of partial differential equations to accommodate age structure. We derived analytic expressions for the steady states of the system and the final age distributions in the case of homogenous contact rates. The stability of these equilibria are determined by a threshold parameter Rp , a function of the vaccine failure parameters and the coverage p. The value of p for which Rp = 1 yields the critical vaccination ratio, a measure of herd immunity. Using this concept we can compare vaccines that confer the same level of herd immunity to the population but may fail at the individual level in different ways. For any fixed Rp > 1, the leaky model results in the highest prevalence of infection, while the all-or-nothing and waning models have the same steady state prevalence. The actual composition of a vaccine cannot be determined on the basis of steady state levels alone, however the distinctions can be made by looking at transient dynamics (such as after the onset of vaccination), the mean age of infection, the age distributions at steady state of the infected class, and the effect of age-specific contact rates."""	age distribution;bloc1s3 gene;degree (graph theory);epidemiology;immunity, herd;infection;license;liver failure, acute;onset (audio);population parameter;rp (complexity);steady state	F. M. G. Magpantay;M. A. Riolo;M. Domenech de Cellès;Aaron A. King;Pejman Rohani	2014	SIAM journal on applied mathematics	10.1137/140956695	text mining;medical research;data mining;operations research	ML	8.493366506110402	-67.96949466992564	182342
6bc4642c2b8108067914abccea3f48e18aec5200	medical diagnosis from dental x-ray images: a novel approach using clustering combined with fuzzy rule-based systems		In practical dentistry, dentists use their experience to examine dental X-ray images to identify patients symptoms for the diagnosis of possible diseases. However, this method is based solely on experts experience which varies from dentist to dentist. The idea of dental diagnosis from X-Ray images is to support dentists in making a more valid conclusion. In this paper, we propose a unified framework using Clustering and Fuzzy Rule-based systems for the diagnosis of dental problems. This framework is modeled under real dental cases of Hanoi Medical University, Vietnam including 56 dental images of 5 diseases in the period 2014 2015. Improvements of the standalone problems especially in the side of classification and decision making are demonstrated. Empirical results reveal the best method in terms of accuracy.	cluster analysis;computation;fuzzy clustering;fuzzy rule;inference engine;rule-based system;semiconductor industry;serial ata;time complexity;unified framework;x-ray (amazon kindle)	Tran Manh Tuan;Nguyen Hai Minh;Van Thien Nguyen;Tran Thi Ngan;Huu Nguyen To	2016	2016 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS)	10.1109/NAFIPS.2016.7851622	medicine;biological engineering;dentistry	AI	3.723673529148655	-79.11778918411586	182379
5a6520672d036ee1e261959e963a1bc64738e850	prediction of radical hysterectomy complications for cervical cancer using computational intelligence methods	diagnostic accuracy;machine learning classifiers;complications prediction;cervical cancer;cross validation	In this work, eleven classifiers were tested in the prediction of intra- and post-operative complications in women with cervical cancer. For the real data set the normalization of the input variables was applied, the feature selection was performed and the original data set was binarized. The simulation showed the best model satisfying the quality criteria such as: the average value and the standard deviation of the error, the area under ROC curve, sensitivity and specificity. The results can be useful in clinical practice.	computation;computational intelligence	Jacek Kluska;Maciej Kusy;Bogdan Obrzut	2012		10.1007/978-3-642-29350-4_31	computer science;machine learning;cross-validation	AI	7.167978394721565	-77.12347136373745	182453
70acaf7102c09ba5f5bbead32f2561b87dc81d53	computer algebra in systems biology	formal language;biological network;system biology;human genome;chip;dynamic system;molecular biology;computer algebra;dna microarray	Systems biology focuses on the study of entire biological systems rather than on their individual components. With the emergence of high-throughput data generation technologies for molecular biology and the development of advanced mathematical modeling techniques, this field promises to provide important new insights. At the same time, with the availability of increasingly powerful computers, computer algebra has developed into a useful tool for many applications. This article illustrates the use of computer algebra in systems biology by way of a well-known gene regulatory network, the Lac Operon in the bacterium E. coli.	computer;emergence;gene regulatory network;high-throughput computing;mathematical model;symbolic computation;systems biology;throughput	Reinhard C. Laubenbacher;Bernd Sturmfels	2009	The American Mathematical Monthly		computational biology;chip;modelling biological systems;biological network;human genome;formal language;biological computation;dna microarray;dynamical system;systems biology;complex systems biology	Comp.	4.093642661052678	-66.91959469657122	182496
169459f39bc4a9e44f901d5a0ccac2f7ec9db337	optimal sequence of tests for the mediastinal staging of non-small cell lung cancer	health informatics;information systems and communication service;management of computing and information systems	BACKGROUND Non-small cell lung cancer (NSCLC) is the most prevalent type of lung cancer and the most difficult to predict. When there are no distant metastases, the optimal therapy depends mainly on whether there are malignant lymph nodes in the mediastinum. Given the vigorous debate among specialists about which tests should be used, our goal was to determine the optimal sequence of tests for each patient.   METHODS We have built an influence diagram (ID) that represents the possible tests, their costs, and their outcomes. This model is equivalent to a decision tree containing millions of branches. In the first evaluation, we only took into account the clinical outcomes (effectiveness). In the second, we used a willingness-to-pay of € 30,000 per quality adjusted life year (QALY) to convert economic costs into effectiveness. We assigned a second-order probability distribution to each parameter in order to conduct several types of sensitivity analysis.   RESULTS Two strategies were obtained using two different criteria. When considering only effectiveness, a positive computed tomography (CT) scan must be followed by a transbronchial needle aspiration (TBNA), an endobronchial ultrasound (EBUS), and an endoscopic ultrasound (EUS). When the CT scan is negative, a positron emission tomography (PET), EBUS, and EUS are performed. If the TBNA or the PET is positive, then a mediastinoscopy is performed only if the EBUS and EUS are negative. If the TBNA or the PET is negative, then a mediastinoscopy is performed only if the EBUS and the EUS give contradictory results. When taking into account economic costs, a positive CT scan is followed by a TBNA; an EBUS is done only when the CT scan or the TBNA is negative. This recommendation of performing a TBNA in certain cases should be discussed by the pneumology community because TBNA is a cheap technique that could avoid an EBUS, an expensive test, for many patients.   CONCLUSIONS We have determined the optimal sequence of tests for the mediastinal staging of NSCLC by considering sensitivity, specificity, and the economic cost of each test. The main novelty of our study is the recommendation of performing TBNA whenever the CT scan is positive. Our model is publicly available so that different experts can populate it with their own parameters and re-examine its conclusions. It is therefore proposed as an evidence-based instrument for reaching a consensus.	aspiration pneumonia;ct scan;cell (microprocessor);consensus (computer science);decision tree;disk staging;endoscopic ultrasound;how true feel vigorous right now;influence diagram;mediastinoscopy;mediastinum;neoplasm metastasis;non-small cell lung carcinoma;patients;personnameuse - assigned;polyethylene terephthalate;population parameter;positrons;pulmonary medicine;rigid needle adapter;sensitivity and specificity;small cell carcinoma of lung;ultrasonography;lymph nodes	Manuel Luque;Francisco Javier Díez;Carlos Disdier	2015		10.1186/s12911-016-0246-y	health informatics;intensive care medicine;medicine;pathology;nursing;surgery	AI	9.734286223831496	-74.55635805596643	182597
35304f87221ffc148830bc6b053ab29b618798fd	a multi-level spatial clustering algorithm for detection of disease outbreaks	incidence;communicable diseases;proportional hazards models;population surveillance;risk factors;cluster analysis;risk assessment;algorithms;disease outbreaks	In this paper, we proposed a Multi-level Spatial Clustering (MSC) algorithm for rapid detection of emerging disease outbreaks prospectively. We used the semi-synthetic data for algorithm evaluation. We applied BARD algorithm [1] to generate outbreak counts for simulation of aerosol release of Anthrax. We compared MSC with two spatial clustering algorithms: Kulldorff's spatial scan statistic [2] and Bayesian spatial scan statistic [3]. The evaluation results showed that the areas under ROC had no significant difference among the three algorithms, so did the areas under AMOC. MSC demonstrated significant computational efficiency (100 + times faster) and higher PPV. However, MSC showed 2-6 hours delay on average for outbreak detection when the false alarm rate was lower than 1 false alarm per 4 weeks. We concluded that the MSC algorithm is computationally efficient and it is able to provide more precise and compact clusters in a timely manner while keeping high detection accuracy (cluster sensitivity) and low false alarm rates.	aerosol dose form;algorithm;algorithmic efficiency;chamaecyparis lawsoniana;cluster analysis;computation;parkinson disease;semiconductor industry;simulation;statistic (data);synthetic data;statistical cluster	Jialan Que;Fu-Chiang Tsui	2008	AMIA ... Annual Symposium proceedings. AMIA Symposium		simulation;geography;data mining;statistics	Metrics	6.834320475830001	-72.63276461792702	182878
9392f6486f0ceda839cbfa61ed93ed21eca8535d	tumor growth rate determines the timing of optimal chronomodulated treatment schedules	chronobiology;g2 phase;time dependent;circadian rhythms;cell cycle and cell division;drug metabolism;drug chronotherapy;models biological;circadian rhythm;circadian time;tumor growth;treatment outcome;kinetic parameter;cancer treatment;antineoplastic agents;drug therapy;drug administration;theoretical analysis;cell proliferation;outcome assessment health care;cell cycle;humans;neoplasms;synthesis phase;kinetics;computer simulation	In host and cancer tissues, drug metabolism and susceptibility to drugs vary in a circadian (24 h) manner. In particular, the efficacy of a cell cycle specific (CCS) cytotoxic agent is affected by the daily modulation of cell cycle activity in the target tissues. Anti-cancer chronotherapy, in which treatments are administered at a particular time each day, aims at exploiting these biological rhythms to reduce toxicity and improve efficacy of the treatment. The circadian status, which is the timing of physiological and behavioral activity relative to daily environmental cues, largely determines the best timing of treatments. However, the influence of variations in tumor kinetics has not been considered in determining appropriate treatment schedules. We used a simple model for cell populations under chronomodulated treatment to identify which biological parameters are important for the successful design of a chronotherapy strategy. We show that the duration of the phase of the cell cycle targeted by the treatment and the cell proliferation rate are crucial in determining the best times to administer CCS drugs. Thus, optimal treatment times depend not only on the circadian status of the patient but also on the cell cycle kinetics of the tumor. Then, we developed a theoretical analysis of treatment outcome (TATO) to relate the circadian status and cell cycle kinetic parameters to the treatment outcomes. We show that the best and the worst CCS drug administration schedules are those with 24 h intervals, implying that 24 h chronomodulated treatments can be ineffective or even harmful if administered at wrong circadian times. We show that for certain tumors, administration times at intervals different from 24 h may reduce these risks without compromising overall efficacy.	adverse reaction to drug;biological factors;body tissue;cell cycle kinetics (discipline);cell proliferation;chronotherapy;kinetics internet protocol;modulation;neoplasms;non-small cell lung carcinoma;patients;population;schedule (computer science);schedule (document type);drug metabolism	Samuel Bernard;Branka Cajavec Bernard;Francis Lévi;Hanspeter Herzel	2010		10.1371/journal.pcbi.1000712	computer simulation;pharmacology;biology;endocrinology;toxicology;chronobiology;genetics;circadian rhythm	Metrics	8.376818126310289	-68.75900452182941	182984
e8ecdad6456fe65f0d445c27b001b823d5872ff1	3-d biofabrication using stereolithography for biology and medicine	stereolithography tissue engineering polymers resins fabrication electrodes organizations;biological tissues;stereolithography;electrophoresis;scaffolds 3 d biofabrication stereolithography sl review tissue engineering neovessel formation cell matrix interactions cell cell interaction dielectrophoresis dep;cellular biophysics;tissue engineering biological tissues blood vessels cellular biophysics electrophoresis stereolithography;blood vessels;animals biocompatible materials cell communication drug delivery systems electrophoresis equipment design extracellular matrix humans hydrogels imaging three dimensional mice polymers regenerative medicine software time factors tissue engineering;tissue engineering	In this paper, we review our recent work on the potential of stereolithography (SL) for different biomedical applications including tissue engineering, neovessel formation, investigating cell-cell and cell matrix interactions, and development of cellular systems. Also, we show that SL technology can be combined with dielectrophoresis (DEP) to create scaffolds with micro-scale organization, a hallmark of in vivo tissues.	body tissue;cross link;electroactive polymers;executable space protection;hydrogels;interaction;kinetics internet protocol;sl (complexity);sleep polysomnography domain;tissue engineering;video-in video-out	Piyush Bajaj;Vincent Chan;Jae Hyun Jeong;Pinar Zorlutuna;Hyunjoon Kong;Rashid Bashir	2012	2012 Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2012.6347557	electrophoresis;nanotechnology;biological engineering;tissue engineering	Visualization	3.7408554396349345	-68.98023569196744	183099
a562ba969f1b8f43a87a36e3fc953567c5481cd8	measuring distance through dense weighted networks: the case of hospital-associated pathogens		Hospital networks, formed by patients visiting multiple hospitals, affect the spread of hospital-associated infections, resulting in differences in risks for hospitals depending on their network position. These networks are increasingly used to inform strategies to prevent and control the spread of hospital-associated pathogens. However, many studies only consider patients that are received directly from the initial hospital, without considering the effect of indirect trajectories through the network. We determine the optimal way to measure the distance between hospitals within the network, by reconstructing the English hospital network based on shared patients in 2014-2015, and simulating the spread of a hospital-associated pathogen between hospitals, taking into consideration that each intermediate hospital conveys a delay in the further spread of the pathogen. While the risk of transferring a hospital-associated pathogen between directly neighbouring hospitals is a direct reflection of the number of shared patients, the distance between two hospitals far-away in the network is determined largely by the number of intermediate hospitals in the network. Because the network is dense, most long distance transmission chains in fact involve only few intermediate steps, spreading along the many weak links. The dense connectivity of hospital networks, together with a strong regional structure, causes hospital-associated pathogens to spread from the initial outbreak in a two-step process: first, the directly surrounding hospitals are affected through the strong connections, second all other hospitals receive introductions through the multitude of weaker links. Although the strong connections matter for local spread, weak links in the network can offer ideal routes for hospital-associated pathogens to travel further faster. This hold important implications for infection prevention and control efforts: if a local outbreak is not controlled in time, colonised patients will appear in other regions, irrespective of the distance to the initial outbreak, making import screening ever more difficult.	aplnr gene;absolute probability judgement;basal ganglia diseases;betweenness centrality;closeness centrality;computation;computational biology;conceptualization (information science);cytology;data curation;digital curation;encapsulated postscript;estimated;heart rate variability;infection;iteration;marginal model;markov chain;one thousand;optimization problem;pdgfb wt allele;pathogenic organism;patients;randomness;simulation;spatial variability;statistical prevalence;temporal difference learning;tetanus and diphtheria toxoid adsorbed for adult use;weighted network;autologous immunoglobulin idiotype-klh conjugate vaccine;keyhole-limpet hemocyanin	Tjibbe Donker;Timo Smieszek;Katherine L. Henderson;Alan P. Johnson;A. Sarah Walker;Julie V. Robotham	2017		10.1371/journal.pcbi.1005622	infection control;bioinformatics;computer security;biology;centrality;local spread;outbreak;telecommunications;transmission (mechanics)	ML	6.697256459572325	-70.57333754094985	183261
93822087e73f9bdb9a49a48f9af96ecc10c52a0a	support vector machine based epilepsy prediction using textural features of mri	diagnostic test;texture features;magnetic resonance image;radial basis function;machine learning;rheumatic heart disease;prediction accuracy;prediction model;support vector machine;central nervous system	Epilepsy is a disorder of the central nervous system, specifically the brain. It is a neurological malfunction affecting about 1% of the population and is the third most common neurological disorder following rheumatic heart disease and Alzheimer‘s disease, but it imposes higher costs on society. Magnetic Resonance Imaging (MRI) is one of the most common diagnostic tests used for patients for epilepsy prediction. Shortage of radiologists and the large volume of MRI scan images that need to be analyzed may lead to labor intensive, expensive and inaccurate prediction. Hence there is a need to generate an efficient prediction model for making a correct diagnosis of epilepsy and accurate prediction of its type. This paper describes the modeling of epilepsy prediction using Support Vector Machines (SVM), a machine learning algorithm. The prediction model has been generated by training the support vector machine with descriptive features derived from MRI data of 350 patients and observed that the SVM based model with a Radial Basis Function (RBF) kernel produces 93.87% of prediction accuracy.	algorithm;experiment;machine learning;nonlinear system;radial (radio);radial basis function kernel;radiology;resonance;supervised learning;support vector machine	V. Sujitha;P. Sivagami;M. S. Vijaya	2010		10.1016/j.procs.2010.11.036	support vector machine;radial basis function;computer science;artificial intelligence;central nervous system;magnetic resonance imaging;machine learning;predictive modelling;diagnostic test	ML	6.731101613341371	-78.14373248863401	183486
f697928fe04ed580f3fe34c3f22b892379f3ba8e	hierarchical representation of differential diagnosis lists for clinical decision support systems	expert systems;differential diagnosis;user interfaces;clinical decision support systems;hierarchical representation	Clinical decision support systems (CDSSs) can generate differential diagnosis lists that may contain hundreds of diseases. These lists grow in size as coverage expands to rare diseases, but large lists can easily become a burden on user cognition. To address this issue, we first outline the representations of differential diagnosis lists on current CDSSs, and then propose a novel approach that represents these differential diagnosis lists hierarchically, coupled with an algorithm for optimal initialization. Preliminary evaluation suggested that our proposed approach outperforms existing approaches with respect to search costs, particularly for large lists. This hierarchical representation should alleviate the cognitive load on user physicians and provide an efficient means to search through very large lists.	clinical decision support system	Takashi Okumura;Masaki Tagawa	2014			clinical decision support system;computer science;theoretical computer science;machine learning;data mining;expert system	ECom	-3.038038694977751	-72.44130845986449	183540
89476ee849ba93fc666c91cbe19be68c0aff11f4	adaboost algorithm with random forests for predicting breast cancer survivability	performance measure;sensitivity and specificity;8903 information services;1117 public health and health services;mathematics;support vector machines;cancer;receiver operator characteristic;classification algorithms accuracy prediction algorithms breast cancer support vector machines artificial neural networks training;training;prediction algorithms;school of engineering and science;data mining;random forests;receiver operating characteristic curve;medical computing;stability;accuracy;artificial neural networks;combining classifier;hybrid method;mathematical models;sensitivity analysis;random forest;classification algorithms;roc curve;diseases;artificial intelligence;receiver operating characteristic curve adaboost algorithm random forests breast cancer survivability prediction model stability overfitting problems;predictive models;stability cancer learning artificial intelligence medical computing sensitivity analysis;prediction model;overfitting problems;computer science;learning artificial intelligence;signal processing algorithms;breast cancer survivability prediction model;decision trees;respubid14757;prediction;breast cancer;educational technologies;conference proceeding;adaboost algorithm;medical diagnostic imaging	In this paper we propose a combination of the AdaBoost and random forests algorithms for constructing a breast cancer survivability prediction model. We use random forests as a weak learner of AdaBoost for selecting the high weight instances during the boosting process to improve accuracy, stability and to reduce overfitting problems. The capability of this hybrid method is evaluated using basic performance measurements (e.g., accuracy, sensitivity, and specificity), Receiver Operating Characteristic (ROC) curve and Area Under the receiver operating characteristic Curve (AUC). Experimental results indicate that the proposed method outperforms a single classifier and other combined classifiers for the breast cancer survivability prediction.	adaboost;algorithm;cross-validation (statistics);ensemble kalman filter;ensemble learning;experiment;logic programming;overfitting;random forest;receiver operating characteristic;sensitivity and specificity;statistical classification	Jaree Thongkam;Guandong Xu;Yanchun Zhang	2008	2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)	10.1109/IJCNN.2008.4634231	statistical classification;random forest;prediction;computer science;machine learning;pattern recognition;data mining;receiver operating characteristic;artificial neural network	Vision	7.030713656787517	-77.226057783649	183563
3109cca971521f44eab2078efb5f88167637b1ff	an imbalanced learning based mdr-tb early warning system	early warning system;disease prediction;imbalanced learning;mdr tb	As a man-made disease, multidrug-resistant tuberculosis (MDR-TB) is mainly caused by improper treatment programs and poor patient supervision, most of which could be prevented. According to the daily treatment and inspection records of tuberculosis (TB) cases, this study focuses on establishing a warning system which could early evaluate the risk of TB patients converting to MDR-TB using machine learning methods. Different imbalanced sampling strategies and classification methods were compared due to the disparity between the number of TB cases and MDR-TB cases in historical data. The final results show that the relative optimal predictions results can be obtained by adopting CART-USBagg classification model in the first 90 days of half of a standardized treatment process.	algorithm;binocular disparity;decision tree learning;entity name part qualifier - adopted;machine learning;memory data register;mobile device;multi-drug resistance;patients;physical therapy exercises;risk assessment;sampling (signal processing);terabyte;test set;tuberculosis;tuberculosis, meningeal;tuberculosis, multidrug-resistant;tuberculosis, osteoarticular;tuberculosis, pulmonary	Sheng Li;Bo Tang;Haibo He	2016	Journal of Medical Systems	10.1007/s10916-016-0517-2	simulation;medicine;data science;warning system;data mining	ML	5.835520361861771	-77.19602634724214	183616
ff15ade161bd4b7ecfa02885b1e3a673bfd6c72a	optimized detection of tar content in the manufacturing process using adaptive neuro-fuzzy inference systems	high performance liquid chromatography;fuzzy logic;fuzzy inference;neuro fuzzy system;adaptive neuro fuzzy inference system	The purpose of this study is to model and optimize the detection of tar in cigarettes during the manufacturing process and show that low yield cigarettes contain similar levels of nicotine as compared to high yield cigarettes while B (Benzene), T(toluene) and X (xylene) (BTX) levels increase with increasing tar yields. A neuro-fuzzy system which comprises a fuzzy inference structure is used to model such a system. Given a training set of samples, the Adaptive Neuro-Fuzzy Inference System (ANFIS) classifiers learned how to differentiate a new case in the domain. The ANFIS classifiers were used to detect the tar in smoke condensate when five basic features defining cigarette classes indications were used as inputs. A classical method by High Performance Liquid Chromatography (HPLC) is also introduced to solve this problem. At last the performances of these two methods are compared.	adaptive neuro fuzzy inference system;btx (form factor);cns disorder;cigarette;class;fuzzy control system;fuzzy logic;neuro-fuzzy;nicotine;performance;tar dosage form;tars;test set;xylene	Zikrija Avdagic;Lejla Begic Fazlic;Samim Konjicija	2009	Studies in health technology and informatics	10.3233/978-1-60750-044-5-615	fuzzy logic;adaptive neuro fuzzy inference system;computer science;artificial intelligence;machine learning;data mining;high-performance liquid chromatography	AI	8.401379906227758	-77.38453117391377	183719
f894d3cbfe775c6bfb59de868eced299ada76d82	linear analysis near a steady-state of biochemical networks: control analysis, correlation metrics and circuit theory	simulation and modeling;proteome;flux balance analysis;systems biology;signal transduction;physiological cellular and medical topics;models biological;dynamic stability;metabolic control analysis;chemical potential;computational biology bioinformatics;nonequilibrium steady state;statistics as topic;biochemical network;quantitative analysis;system analysis;thermodynamics;algorithms;rate limiting;biochemical reaction network;linear models;computer simulation;biochemistry;steady state;bioinformatics	Several approaches, including metabolic control analysis (MCA), flux balance analysis (FBA), correlation metric construction (CMC), and biochemical circuit theory (BCT), have been developed for the quantitative analysis of complex biochemical networks. Here, we present a comprehensive theory of linear analysis for nonequilibrium steady-state (NESS) biochemical reaction networks that unites these disparate approaches in a common mathematical framework and thermodynamic basis. In this theory a number of relationships between key matrices are introduced: the matrix A obtained in the standard, linear-dynamic-stability analysis of the steady-state can be decomposed as A = SR T where R and S are directly related to the elasticity-coefficient matrix for the fluxes and chemical potentials in MCA, respectively; the control-coefficients for the fluxes and chemical potentials can be written in terms of R T BS and S T BS respectively where matrix B is the inverse of A; the matrix S is precisely the stoichiometric matrix in FBA; and the matrix e A t plays a central role in CMC. One key finding that emerges from this analysis is that the well-known summation theorems in MCA take different forms depending on whether metabolic steady-state is maintained by flux injection or concentration clamping. We demonstrate that if rate-limiting steps exist in a biochemical pathway, they are the steps with smallest biochemical conductances and largest flux control-coefficients. We hypothesize that biochemical networks for cellular signaling have a different strategy for minimizing energy waste and being efficient than do biochemical networks for biosynthesis. We also discuss the intimate relationship between MCA and biochemical systems analysis (BSA).	20-methylcholanthrene;anabolism;biochemical reaction;bond issues, construction;breast-conserving surgery;carbon dioxide:ppres:pt:ventilator airway circuit:qn:infrared absorption;cell signaling;coefficient;control theory;elasticity (data store);flux balance analysis;gene regulatory network;largest;mathematics;metabolic process, cellular;metabolic control analysis;network analysis (electrical circuits);rate limiting;steady state;summation (document);the matrix;thermodynamics;whole earth 'lectronic link	William J. Heuett;Daniel A. Beard;Hong Qian	2008	BMC Systems Biology	10.1186/1752-0509-2-44	computer simulation;metabolic control analysis;biology;biophysics;computer science;bioinformatics;quantitative analysis;flux balance analysis;linear model;rate limiting;chemical potential;proteome;system analysis;steady state;systems biology;signal transduction	ML	9.531118500423863	-67.14405144313024	183873
4814314a672486e9d687cc6cdf0c43b07a7ecf93	age and gender effects on 15 platelet phenotypes in a spanish population	immature platelet fraction ipf;platelet indices;platelet function assay pfa;age;gender;platelet counts	INTRODUCTION Several studies have analysed the platelet parameters in human blood, nevertheless there are no extensive analyses on the less common platelet phenotypes. The main objective of our study is to evaluate the age and gender effects on 15 platelet phenotypes.   METHODS We studied 804 individuals, ranging in age from 2 to 93 years, included in the Genetic Analysis of Idiopathic Thrombophilia 2 (GAIT 2) Project. The 15 platelet phenotypes analysed were the platelets counts, platelet volumes, plateletcrits, immature platelet fraction (IPF) and platelet function assay (PFA). A regression-based method was used to evaluate the age and gender effects on these phenotypes.   RESULTS Our results were consistent with the previously reported results regarding platelet counts and plateletcrit (PCT). They showed a decrease with increasing age. The mean platelet volume (MPV), platelet distribution width (PDW) and platelet-large cell ratio (P-LCR) increased with age, but did not present any gender effect. All the IPF phenotypes increased with age, whereas the PFA phenotypes did not show any relation to age or gender.   DISCUSSION To sum up, our study provides a comprehensive analysis of the age and gender effects on the platelet phenotypes in a family-base sample. Our results suggest more reasonable age stratification into two distinct groups: childhood, ranging from 2 to 12 years, and the mature group, from 13 to 93 years. Moreover, the PFA phenotypes were maintained constant while the platelet counts, the MPV and IPF levels vary with age.	blood platelet disorders;blood platelets;cloud fraction;gray platelet syndrome;hypersomnolence, idiopathic;idiopathic pulmonary fibrosis;phenotype;platelet count measurement;platelet distribution width measurement;platelet mean volume determination (procedure);predictive failure analysis;stratification;thrombophilia;vincristine;mpv	Miquel Vázquez-Santiago;Andrey Ziyatdinov;Núria Pujol-Moix;Helena Brunel;Agnès Morera;José Manuel Soria;Juan Carlos Souto	2016	Computers in biology and medicine	10.1016/j.compbiomed.2015.12.023	ageing;immunology;diabetes mellitus;surgery	NLP	8.469697463591814	-73.46774213257811	184141
7c45480cbf6d5e751413b1a86e9cf7b2c6b657eb	estimation of a neonatal cardiovascular risk score by biomedical discriminant analysis	pediatrics cardiology heart rate genetics production facilities blood pressure biomedical monitoring pregnancy computerized monitoring condition monitoring;cardiology;haemodynamics;blood pressure;statistical analysis cardiology computerised monitoring haemodynamics medical computing patient monitoring;genetics;medical computing;discriminant analysis;variable selection;heart rate;family history;statistical analysis;indexation;patient monitoring;computerised monitoring;biomedical discriminate analysis genetics neonatal cardiovascular risk score elevated blood pressure preventive interventions heart rate newborns family history descriptive statistics circulatory variables all subsets variable selection technique;cardiovascular risk;pregnant women	Genetics is a primary factory for the predisposition of a newborn to elevated blood pressure (BP) later in life. The search for an index for this factor, needed to assess, on the neonate, the success of failure of preventive interventions instituted on pregnant women, is discussed. This index could be based on characteristics of BP and heart rate (HR) variability during the first days after birth. In the search for such an index, the systolic and diastolic BP and HR of 150 newborns were automatically monitored at about 30-min intervals for 48 h starting early after birth. The newborns were assigned to a group of either a negative or positive family history of high BP. Circadian characteristics and descriptive statistics for the three circulatory variables were used for classification by a 'monotest', an all-subsets variable selection technique for biomedical discriminate analysis. The monotest is discussed in detail. When the 90% range of systolic BP was used as classifier, the monotest yielded a 69% total classification equivalent to prior criteria. >	linear discriminant analysis	Ramón C. Hermida;Fernando Aguado;José R. Fernandez;Diane E. Ayala;José Rodríguez-Cervilla;José M. Fraga	1992		10.1109/CBMS.1992.245003	intensive care medicine;medicine;computer science;blood pressure;machine learning;hemodynamics;remote patient monitoring;linear discriminant analysis;feature selection;surgery	Vision	8.528839788033007	-76.88736247964356	184584
30afa15e7d8b58a28ce999d2af6e166dcf542b4c	blood glucose prediction using stochastic modeling in neonatal intensive care	hyperglycemia;modelizacion;stress;forecasting;clinical data;cuidado intensivo;glycemia;stochastic modeling;time varying;modelo prevision;insulina;pediatrics;modele mathematique;metabolic problem;genie biomedical;stochastic systems biochemistry biomedical measurement blood diseases molecular biophysics paediatrics patient care proteins;hypoglycemia;surveillance;real time control;hiperglicemia;2d kernel model stochastic modeling neonatal intensive care hyperglycemia metabolic problem homeostasis endogenous regulatory systems insulin sensitivity hypoglycemia neonatal patients birth gestational age blood glucose prediction;analisis cuantitativo;real time;birth gestational age;dynamic model;stochastic approximation forecasting human factors;hospitals;nouveau ne;insulin sensitivity;hombre;hormona pancreatica;glucemia;modelo matematico;blood glucose;homeostasis;2d kernel model;probability forecasts;patient care;algorithms blood glucose cohort studies humans infant newborn infant premature infant very low birth weight intensive care neonatal models biological predictive value of tests stochastic processes;newborn;forecast model;glucose;modelisation;stochastic;vigilancia;low birth weight;human factors;hyperglycemie;biomedical engineering;proteins;hormone pancreatique;stochastic processes;paediatrics;endogenous regulatory systems;analyse quantitative;intensive care;glycemie;neonatal patients;glucosa;blood;stochastic approximation;molecular biophysics;safety;human;mathematical model;insuline;diseases;quantitative analysis;insulin;pancreatic hormone;predictive models;gestational age;ingenieria biomedica;blood glucose prediction;cross validation;variance estimation;neonatal intensive care;stochastic model;sugar;stochastic systems;modeling;conditional probability;insulin sensitive;modelo estocastico;neonatal;biomedical measurement;recien nacido;modele stochastique;biochemistry;soin intensif;homme;modele prevision	Hyperglycemia is a common metabolic problem in premature, low-birth-weight infants. Blood glucose homeostasis in this group is often disturbed by immaturity of endogenous regulatory systems and the stress of their condition in intensive care. A dynamic model capturing the fundamental dynamics of the glucose regulatory system provides a measure of insulin sensitivity (SI). Forecasting the most probable future SI can significantly enhance real-time glucose control by providing a clinically validated/proven level of confidence on the outcome of an intervention, and thus, increased safety against hypoglycemia. A 2-D kernel model of SI is fitted to 3567 h of identified, time-varying SI from retrospective clinical data of 25 neonatal patients with birth gestational age 23 to 28.9 weeks. Conditional probability estimates are used to determine SI probability intervals. A lag-2 stochastic model and adjustments of the variance estimator are used to explore the bias-variance tradeoff in the hour-to-hour variation of SI. The model captured 62.6% and 93.4% of in-sample SI predictions within the (25th-75th) and (5th-95th) probability forecast intervals. This overconservative result is also present on the cross-validation cohorts and in the lag-2 model. Adjustments to the variance estimator found a reduction to 10%-50% of the original value provided optimal coverage with 54.7% and 90.9% in the (25th-75th) and (5th-95th) intervals. A stochastic model of SI provided conservative forecasts, which can add a layer of safety to real-time control. Adjusting the variance estimator provides a more accurate, cohort-specific stochastic model of SI dynamics in the neonate.	areal density (computer storage);bias–variance tradeoff;blood glucose;ccl4 protein, human;choose (action);cross reactions;cross-validation (statistics);estimated;glucose metabolism disorders;homeostasis;hyperglycemia;image scaling;infant, newborn;large;low birth weight infant;mathematical model;metabolic process, cellular;neonatal intensive care;patients;probability;projections and predictions;real-time clock;real-time transcription;sample variance;seizures;sensitivity and specificity;stochastic modelling (insurance);stochastic process;test scaling;insulin, isophane	Aaron J. Le Compte;Dominic S. Lee;J. Geoffrey Chase;Jessica Lin;Adrienne Lynn;Geoffrey M. Shaw	2010	IEEE Transactions on Biomedical Engineering	10.1109/TBME.2009.2035517	stochastic approximation;endocrinology;medicine;computer science;mathematics;stochastic;diabetes mellitus;surgery;statistics;molecular biophysics	ML	7.389970273012126	-73.43270586917302	184758
48ed44b8197a1c92b314c55d02bd0514545cd5b3	accelerating rare disease diagnosis with collaborative filtering.				Feichen Shen;Sijia Liu;Yanshan Wang;Liwei Wang;Naveed Afzal;Hongfang Liu	2017			data mining;collaborative filtering;rare disease;computer science	AI	2.8234010647880488	-74.99876162711162	184770
3b5ba6291e8b3558740f85b7f002bd1090a747aa	data-driven clinical phenotyping of denosumab exposure in a large united states cohort		Denosumab is a therapeutic monoclonal antibody originally developed for alleviation of osteoporosis symptoms. Due to known anti-osteoclastic effect via RANKL inhibition, indication for denosumab was expanded to include treatment for various cancers. Phenotyping for denosumab exposure occurred within a large (~50 million individuals) US-based insurance claims cohort between 2007 and 2016. Denosumab exposed patients (n=70,610) were randomly assigned to 20% training (n=14,122) and 80% analysis (n=56,488) cohorts. CCS level 2 hierarchies annotated phenotype labels (n=146) for training, analysis, and control cohorts, including data from inpatient and outpatient diagnosis (ICD9&10) codes 12 months prior and post index date. To reduce the dimension of our phenotypic feature space of 144 CCS categories, we applied an efficient and powerful gradient boosting ML algorithm named XGBoost on the training cohort to identify individuals who have been exposed to denosumab more than once. Relevant phenotypic features (n=31) were identified with 0.988 cross-validation balanced accuracy. Within the analysis cohort, our model replicated with balanced accuracy of 0.846. Several therapeutic indications were predictive of denosumab exposure such as in treatment of osteoporosis and various cancers, hypertension, eye disorders, disorders of lipid metabolism, disease of urinary system, bone disease and spondylosis (Table 1). Removing the primary indication (osteoporosis) from the denosumab model only slightly modified feature importance scores (Table 1 - FI2). Independent statistical characterization of features corroborated all associations between denosumab exposure and phenotype prevalence as significant (all adjusted p-values < 0.001). Interdependent effect between primary or secondary indications of denosumab exposure and phenotype prevalence are needed to elucidate potential disease co-occurrence or opportunities for therapeutic repositioning.	algorithm;cpu cache;code;cross-validation (statistics);feature vector;gradient boosting;interdependence;randomness;xgboost	Trang Le;Matthew K. Breitenstein	2018	2018 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2018.00086	cancer;bone disease;denosumab;osteoporosis;eye disorder;oncology;informatics;disease;cohort;internal medicine;medicine	ML	8.366579559961929	-75.08801253896448	184881
d0671970d1fbf5e1d57eaae7106dc54a9e5793ae	a review of experimental opportunities for molecular communication	molecular communication;body area nanonetworks;neuronal networks;bacteria communication;membrane nanotube	The growth of nanotechnology has led to miniature devices that are able to perform limited functionalities in hard to access areas. Example nanodevice applications in the healthcare domain include early detection of harmful diseases. The current field of molecular communication is aiming to increase the functionalities of nanodevices, by enabling communication to be performed. Since its first introduction, communication researchers have been proposing various solutions that could possibly realize molecular communications (e.g., molecular diffusion and bacteria nanonetworks). These solutions have largely been limited to theoretical simulation modeling. However, to fully realize a future for real deployments and developments of molecular communication, a strong synergy will be required with molecular biologists. The aim of this paper is to create this link, and at the same time provide guidance for current molecular communication researchers of possible real developments of molecular communication based on the current state-of-the-art experimental work. In particular, we present a review on bacteria communication and membrane nanotubes, as well as neuronal networks. We also discuss possible applications in the future focusing in particular on Body Area NanoNetworks (BAN2). © 2013 Elsevier Ltd.		Sasitharan Balasubramaniam;Sigal Ben-Yehuda;Sophie Pautot;Aldo Jesorka;Pietro Liò;Yevgeni Koucheryavy	2013	Nano Comm. Netw.	10.1016/j.nancom.2013.02.002	biology;telecommunications;nanotechnology;biological engineering	EDA	3.8159152197838613	-68.5999734904611	185274
c14a54417d5940ee40964a01778969cf7729f060	logical and semantic modeling of complex biomolecular networks		Systems biology models aim to describe and understand the behaviour of a cell. This living organism is represented by a complex biomolecular network. In the literature, most researches focus only on modeling isolated parts of this network, such as the metabolic network or the gene regulatory network. However, to fully understand the behaviour of a cell we should model and analyze the biomolecular network as a whole.Towards this goal, we firstly present a formalization for describing the logical structure, function and behaviour of complex biomolecular networks. In addition, we propose a semantic approach based on four ontologies to provide a rich description for modeling a biomolecular network and its state changes. This approach contributes to propose to the biologist a platform where to simulate the state changes of biomolecular networks with the hope of steering their behaviours.		Ali Ayadi;Cecilia Zanni-Merk;François de Bertrand de Beuvron;Saoussen Krichen	2016		10.1016/j.procs.2016.08.108	computer science;bioinformatics;artificial intelligence	ML	4.388088171427816	-67.25420987846132	185419
5fbc400871b541a804e29bce0fac2917e5d430b1	supporting an early detection of diabetic neuropathy by visual analytics	general;i 3 8 computer graphics;j 3 computer applications;h 5 0 information interfaces and presentation;life and medical sciences;applications	In this paper, we describe a step-wise approach to utilize ophthalmic markers for detecting early diabetic neuropathy (DN), the most common long-term complication of diabetes mellitus. Our approach is based on the Visual Analytics Mantra: First, we statistically analyze the data to identify those variables that separate DN patients from a control group. Afterwards, we show the important separating variables individually, but also in the context of all variables regarding a pre-defined classification. By doing so, we support the understanding of the categorization in respect of the value distribution of variables. This allows for zooming, filtering and further analysis like deleting non-relevant variables that do not contribute to the definition of markers as well as deleting data records with false data values or false classifications. Finally, outliers are observed and investigated in detail. So, a third group of potential DN patients can be introduced. In this way, the detection of early DN can be effectively supported.	carpal tunnel syndrome;categorization;content-control software;design marker;false precision;sensor;visual analytics;zooming user interface	Martin Luboschik;Martin Röhlig;Günther Kundt;Oliver Stachs;Sabine Peschel;Andrey Zhivov;Rudolf F. Guthoff;Karsten Winter;Heidrun Schumann	2014		10.2312/eurova.20141145	speech recognition;computer science;artificial intelligence;data mining	ML	1.807012856522823	-77.28233772464644	185707
419964e842f4284b794b4773f00d2eec266dcacc	a markov chain model to evaluate patient transitions in small community hospitals	probability;hospitals;mathematical model;markov processes;organizations;steady state	A patient journey in the hospital may include many departments or units. Making safe and smooth transitions within the hospital is of significant importance. This paper introduces a Markov chain model to study patient transitions between emergency department, intensive or critical care unit, and hospital ward. An iteration method is presented to evaluate the performance of transition process. It is shown that such a method has a high accuracy of estimation and can be used to study patient transitions in small community hospitals.	estimation theory;iteration;markov chain	Hyo-Kyung Lee;Jingshan Li;Albert J. Musa;Philip A. Bain	2016	2016 IEEE International Conference on Automation Science and Engineering (CASE)	10.1109/COASE.2016.7743468	intensive care medicine;medicine;operations management;pediatrics	Robotics	5.574794459424131	-72.93272696980002	186230
455add514d784683a77b7ad5a5f60d54679b1924	machine learning for characterization of insect vector feeding		Insects that feed by ingesting plant and animal fluids cause devastating damage to humans, livestock, and agriculture worldwide, primarily by transmitting pathogens of plants and animals. The feeding processes required for successful pathogen transmission by sucking insects can be recorded by monitoring voltage changes across an insect-food source feeding circuit. The output from such monitoring has traditionally been examined manually, a slow and onerous process. We taught a computer program to automatically classify previously described insect feeding patterns involved in transmission of the pathogen causing citrus greening disease. We also show how such analysis contributes to discovery of previously unrecognized feeding states and can be used to characterize plant resistance mechanisms. This advance greatly reduces the time and effort required to analyze insect feeding, and should facilitate developing, screening, and testing of novel intervention strategies to disrupt pathogen transmission affecting agriculture, livestock and human health.	citrus plant;computer program;disease vectors;ingestion;liquid substance;livestock;machine learning;paget's disease, mammary;pathogenic organism;transmitter;vertical disease transmission;voltage	Denis S. Willett;Justin George;Nora S. Willett;Lukasz L. Stelinski;Stephen L. Lapointe	2016		10.1371/journal.pcbi.1005158	bioinformatics;biology;citrus greening disease;vector (epidemiology);pathogen;agriculture;livestock;agronomy	ML	4.691300486696295	-66.18915184370458	186475
4f509f121e12e96db5ab79598147fba3a85df655	lab-on-chip for exosomes and microvesicles detection and characterization		Interest in extracellular vesicles and in particular microvesicles and exosomes, which are constitutively produced by cells, is on the rise for their huge potential as biomarkers in a high number of disorders and pathologies as they are considered as carriers of information among cells, as well as being responsible for the spreading of diseases. Current methods of analysis of microvesicles and exosomes do not fulfill the requirements for their in-depth investigation and the complete exploitation of their diagnostic and prognostic value. Lab-on-chip methods have the potential and capabilities to bridge this gap and the technology is mature enough to provide all the necessary steps for a completely automated analysis of extracellular vesicles in body fluids. In this paper we provide an overview of the biological role of extracellular vesicles, standard biochemical methods of analysis and their limits, and a survey of lab-on-chip methods that are able to meet the needs of a deeper exploitation of these biological entities to drive their use in common clinical practice.	biological markers;body fluids;entity;exosomes;extracellular vesicles;liquid substance;requirement;vesicle (morphologic abnormality)	Maria Serena Chiriacò;Monica Bianco;Annamaria Nigro;Elisabetta Primiceri;Francesco Ferrara;Alessandro Romano;Angelo Quattrini;Roberto Furlan;Valentina Arima;Giuseppe Maruccio	2018		10.3390/s18103175	electronic engineering;lab-on-a-chip;engineering;microvesicles;nanotechnology	SE	2.6596991101690604	-68.56821373324631	186792
551dd6e42667f67438eae33acc07c104d9c38e35	a machine learning model for triage in lean pediatric emergency departments		High demand periods and under-staffing due to financial constraints cause Emergency Departments (EDs) to frequently exhibit over-crowding and slow response times to provide adequate patient care. In response, Lean Thinking has been applied to help alleviate some of these issues and improve patient handling, with success. Lean approaches in EDs include separate patient streams, with low-complexity patients treated in a so-called Fast Track, in order to reduce total waiting time and to free-up capacity to treat more complicated patients in a timely manner. In this work we propose the use of Machine Learning techniques in a Lean Pediatric ED to correctly predict which patients should be admitted to the Fast Track, given their signs and symptoms. Charts from 1205 patients of the emergency department of Hospital Napoleon Franco Pareja in Cartagena - Colombia, were used to construct a dataset and build several predictive models. Validation and test results are promising and support the validity of this approach and further research on the subject.	machine learning	William Caicedo-Torres;Gisela García;Hernando Pinzón	2016		10.1007/978-3-319-47955-2_18	intensive care medicine;medicine;emergency medicine;medical emergency	AI	6.306052125685749	-75.77320452945175	187182
b3ec60d363b390c3acad7168857cfa6479a1c145	early prostate cancer diagnosis by using artificial neural networks and support vector machines	levenberg marquardt;relational data;risk map;prostate specific antigen;broyden fletcher goldfarb shanno;kernel function;scaled conjugate gradient;statistical significance;family health;family physician;benign prostate hyperplasia;early diagnosis;bmi body mass index;gleason score;chronic prostatitis;feature selection;support vector machine;training algorithm;artificial neural network;prostate cancer;expert system	The aim of this study is to design a classifier based expert system for early diagnosis of the organ in constraint phase to reach informed decision making without biopsy by using some selected features. The other purpose is to investigate a relationship between BMI (body mass index), smoking factor, and prostate cancer. The data used in this study were collected from 300 men (100: prostate adenocarcinoma, 200: chronic prostatism or benign prostatic hyperplasia). Weight, height, BMI, PSA (prostate specific antigen), Free PSA, age, prostate volume, density, smoking, systolic, diastolic, pulse, and Gleason score features were used and independent sample t-test was applied for feature selection. In order to classify related data, we have used following classifiers; scaled conjugate gradient (SCG), Broyden-Fletcher-Goldfarb-Shanno (BFGS), and Levenberg-Marquardt (LM) training algorithms of artificial neural networks (ANN) and linear, polynomial, and radial based kernel functions of support vector machine (SVM). It was determined that smoking is a factor increases the prostate cancer risk whereas BMI is not affected the prostate cancer. Since PSA, volume, density, and smoking features were to be statistically significant, they were chosen for classification. The proposed system was designed with polynomial based kernel function, which had the best performance (accuracy: 79%). In Turkish Family Health System, family physician to whom patients are applied firstly, would contribute to extract the risk map of illness and direct patients to correct treatments by using expert system such proposed.		Murat Çinar;Mehmet Engin;Erkan Zeki Engin;Y. Ziya Atesçi	2009	Expert Syst. Appl.	10.1016/j.eswa.2008.08.010	kernel;support vector machine;levenberg–marquardt algorithm;relational database;computer science;artificial intelligence;machine learning;statistical significance;feature selection;expert system;artificial neural network	ML	7.216154214264597	-77.4795324159896	187525
33d38128cead77695c005b417d4630ff5efcc984	markov modeling of conformational kinetics of cardiac ion channel proteins	time constant;ion channel;generic algorithm;conformation;statistical evaluation;markov model;mathematical model;monte carlo simulation;kinetics;mutation	Markov modeling of conformational kinetics of cardiac ion channels is a prospective means to correlate the molecular defects of channel proteins to their electrophysiological dysfunction. However, both the identifiability of the microscopic conformations and the estimation of the transition rates are challenging. In this paper, we present a new method in which the distribution space of the time constants of exponential components of mathematical models are searched as an alternative to the consideration of transition rates. Transition rate patterns were defined and quasi random seed sequences for each pattern were generated by using a multiple recursive generator algorithm. Cluster-wide Monte Carlo simulation was performed to investigate various schemes of Markov models. It was found that by increasing the number of closed conformations the time constants were shifted to larger magnitudes. With the inclusion of inactivation conformation the time distribution was altered depending on the topology of the schemes. Further results demonstrated the stability of the morphology of time distributions. Our study provides the statistical evaluation of the time constant space of Markov schemes. The method facilities the identification of the underlying models and the estimation of parameters, hence is proposed for use in investigating the functional consequences of defective genes responsible for ion channel diseases.	kinesiology;markov chain	Chong Wang;Antje Krause;Chris D. Nugent;Werner Dubitzky	2006		10.1007/11946465_11	mathematical optimization;simulation;markov chain monte carlo;mathematics;statistics;variable-order markov model	Vision	9.008431879052887	-66.52698008158679	187733
b7b8abf52bebe289bfc7378c6ea4426afefbe020	machine learning techniques for survival time prediction in breast cancer		The use of machine learning in disease prediction and prognosis is part of a growing trend of personalized and predictive medicine. Cancer studies are domain of active machine learning implementation in particular in sense of accuracy of cancer prognosis and prediction. The accuracy of survival time prediction in breast cancer is the main object of the study. Two major features for survival time prediction, based on clinical data are used: the created in the study tumor integrated clinical feature and Nottingham prognostic index. The applied machine learning methods aside with data normalisation and classification provide promising results for accuracy of survival time prediction. Results showed prepotency of the support vector regression modles - linear and decision tree regression models, for more accurate prediction of the survival time in breast cancer. Cross-validation, based on four parameters for error evaluation, confirms the results of the model performance concerning the accuracy of survival time prediction in breast cancer.	machine learning	Iliyan Mihaylov;Maria Nisheva;Dimitar Vassilev	2018		10.1007/978-3-319-99344-7_17	breast cancer;regression analysis;support vector machine;computer science;machine learning;artificial intelligence;decision tree;predictive medicine;cancer;nottingham prognostic index;disease	Theory	6.088518587725248	-76.24393049365094	187910
630a0979c8d28aba8e7f33552669ec8a98d0e1cd	learning bayesisan networks by genetic algorithms: a case study in the prediction of survival in malignant skin melanoma	bayesian network;genetic algorithm	In this work we introduce a methodology based on Genetic Algorithms for the automatic induction of Bayesian Networks from a le containing cases and variables related to the problem. The methodology is applied to the problem of predicting survival of people after one, three and ve years of being diagnosed as having malignant skin melanoma. The accuracy of the obtained model, measured in terms of the percentage of well-classi ed subjects, is compared to that obtained by the called Naive-Bayes. In both cases, the estimation of the model accuracy is obtained from the 10-fold cross-validation method.	bayesian network;cross-validation (statistics);genetic algorithm;naive bayes classifier	Pedro Larrañaga;Basilio Sierra;Miren J. Gallego;Maria J. Michelena;Juan Manuel Pikatza Atxa	1997		10.1007/BFb0029459	genetic algorithm;computer science;bioinformatics;machine learning;pattern recognition;bayesian network	AI	7.305223119211669	-77.35349682818553	188301
204c4e554f5f1703f16b6d7d52d40af86fa63836	a predictive model for guillain-barré syndrome based on single learning algorithms		Background. Guillain-Barré Syndrome (GBS) is a potentially fatal autoimmune neurological disorder. The severity varies among the four main subtypes, named as Acute Inflammatory Demyelinating Polyneuropathy (AIDP), Acute Motor Axonal Neuropathy (AMAN), Acute Motor Sensory Axonal Neuropathy (AMSAN), and Miller-Fisher Syndrome (MF). A proper subtype identification may help to promptly carry out adequate treatment in patients. Method. We perform experiments with 15 single classifiers in two scenarios: four subtypes' classification and One versus All (OvA) classification. We used a dataset with the 16 relevant features identified in a previous phase. Performance evaluation is made by 10-fold cross validation (10-FCV). Typical classification performance measures are used. A statistical test is conducted in order to identify the top five classifiers for each case. Results. In four GBS subtypes' classification, half of the classifiers investigated in this study obtained an average accuracy above 0.90. In OvA classification, the two subtypes with the largest number of instances resulted in the best classification results. Conclusions. This study represents a comprehensive effort on creating a predictive model for Guillain-Barré Syndrome subtypes. Also, the analysis performed in this work provides insight about the best single classifiers for each classification case.	2',3'-dideoxyadenosine;acute motor axonal neuropathy;acute inflammatory demyelinating polyneuropathy;artificial neural network;association rule learning;c4.5 algorithm;conflict (psychology);cross reactions;decision trees;decision tree;demyelinating diseases;ensemble learning;experiment;guillain-barre syndrome;machine learning;memory-level parallelism;name;neural network simulation;not invented here;patients;performance evaluation;philodinidae sp. qsa;predictive modelling;qtscript;radial basis function;rule induction;silo (dataset);statistical test;statistical classification;subtype (attribute);superword level parallelism;trees (plant);algorithm;nervous system disorder;triangulation	Juana Canul-Reich;Juan Frausto Solís;José Hernández-Torruco	2017		10.1155/2017/8424198	medicine;pathology;artificial intelligence;machine learning	ML	7.8164434759552535	-77.69844979601315	188343
f928529e7c0bb1010f6196e4b4a29fa494bc196b	automatic image processing in developmental testing of visual-motor integration	image processing;image analysis	Learning related vision problems are deficits in visual efficiency and visual information processing that can interfere with the ability to perform to one’s full learning potential. The prevalence of visual efficiency problems in the school-aged population is thought to be in the 15-20 percent range. Estimates of the prevalence of learning problems among school-aged children range from 2-10 percent. Nearly one-half of these children have been diagnosed with a learning disability, of which as many as 75 percent have particular difficulty with reading.	image processing;information processing	Michael C. Fairhurst;N. Higson;C. Clar;Russell J. Bradford;W. Clark;E. Pringle	1994		10.1007/3-540-58476-5_148	image analysis;image processing;digital image processing;automatic image annotation	ML	-3.1382880756115874	-76.67054859044602	189662
5ec677d9d5e5504d2cc29b4f63c6fe9bd492f93d	application of critical condition scoring scale of traditional chinese medicine in predicting operative risks for elder patients with orthopedic surgery	geriatrics;orthopaedics;possum critical condition scoring scale of traditional chinese medicine orthopedic surgery complication morbidity mortahty;mathematical model equations logistics sensitivity decision support systems orthopedic surgery constitution;blood;surgery;regression analysis;complication mortality elder patients orthopedic surgery possum data analysis spss logistic regression analysis statistical significant equation blood syndrome differentiation score misclassification rate omission classification rate youden index csstcm model c index critical condition scoring scale of traditional chinese medicine complication morbidity;surgery blood geriatrics orthopaedics regression analysis	Objective: To evaluate the capability of Critical Condition Scoring Scale of Traditional Chinese Medicine (CSSTCM) in predicting complication morbidity and mortality among elder patients who received orthopedic surgery. Methods: Evaluate patients in second orthopedic department of Guangdong Provincial Hospital of Chinese Medicine from September 2012 to March 2013 by CSSTCM and POSSUM. All data were analyzed in SPSS 19.0. Results: Logistic regression analysis yielded a statistically significant equation for complication morbidity: In R/(1 - R) = - 7.821+0.495×FS+0.335×CS+0.625×QS (FS: four examinations and eight principles score; CS: constitution score; QS: qi & blood syndrome differentiation score). The cutting point of the model was 48.32%, sensitivity 78.13%, specificity 91.30%, misclassification rate 8.70%, omission classification rate 21.87%, Youden index 0.69 and coincidence rate 85.1% The data did not produce an effective Logistic regression equation for mortality. CSSTCM model had better C-Index (0.902 to 0.757), sensitivity (0.781 to 0.656), specificity (0.913 to 0.812) and coincidence rate (0.851 to 0.762) than POSSUM. Conclusion: CSSTCM can generate a significant and effective regression model for complication morbidity, but fails to establish a significant equation for mortality due to limited sample volume. CSSTCM model in predicting complication morbidity has better fitness for the observed sample than POSSUM does.	logistic regression;spss;sensitivity and specificity	Haiyun Chen;Yingyu Hu;Zhe Li;Wanxi Deng;Yi Liu;Qiang Lin;Zehui He	2013	2013 IEEE International Conference on Bioinformatics and Biomedicine	10.1109/BIBM.2013.6732745	geriatrics;regression analysis	AI	7.9588778616583635	-75.72842751936915	189742
246a9f54fcbaf55290c1a0b7426f4987617dff11	cellular factories - emerging technologies for fabrication of nanomedicines?		The development of innovative nanomedicines requires the implementation of new biocompatible materials and their efficient assembly into defined nanostructures. Complex and costly synthesis of these materials can be coped with biological fabrication using microorganism factories, recombinant DNA and metabolic engineering. Modern bioprocess technologies may have the key for the implementation of tomorrow’s nanomedicines. This paper specifically focuses on the current state of the art of nanopharmaceuticals and their future perspectives.	recombinant dna	V. Ramos;X. Turon;Salvador Borrós	2013			nanotechnology	Graphics	3.5712855712140206	-68.02836148437001	189998
481535f3ae0ac83045a1278a59b22f21a3a712c6	towards trustworthy predictions of conversion from mild cognitive impairment to dementia: a conformal prediction approach		Predicting progression from a stage of Mild Cognitive Impairment to Alzheimer’s disease is a major pursuit in current dementia research. As a result, many prognostic models have emerged with the goal of supporting clinical decisions. Despite the efforts, the lack of a reliable assessment of the uncertainty of each prediction has hampered its application in practise. It is paramount for clinicians to know how much they can rely upon the prediction made for a given patient, in order to adjust treatments to the patient based on that information. In this exploratory study, we evaluated the Conformal Prediction approach on the task of making predictions with precise levels of confidence. Conformal prediction showed promising results. Using high confidence levels have the drawback of leaving a large number of MCI patients without prognostic (the classifier is not confident enough to give a single class). When using forced predictions, conformal predictors achieved classification performances as good as standard classifiers, with the advantage of complementing each prediction with a confidence value.	trustworthy computing	Telma Pereira;Sandra Cardoso;Dina Silva;Alexandre de Mendonça;Manuela Guerreiro;Sara C. Madeira	2017		10.1007/978-3-319-60816-7_19	exploratory research;trustworthiness;psychology;artificial intelligence;cognitive psychology;pattern recognition;dementia;cognition	HCI	6.313771472995107	-76.62539180456513	190077
17fbe817ee3d3109d2461fd4ced8f858a0a06bb8	latent force models for human action recognition		Human action recognition is a key process for robots when targeting natural and effective interactions with humans. Such systems need solving the challenging task of designing robust algorithms handling intra and inter-personal variability: for a given action, people do never reproduce the same movements, preventing from having stable and reliable models for recognition. In our work, we use the latent force model (LFM [2]) to introduce mechanistic criteria in explaining the time series describing human actions in terms actual forces. According to LFM’s, the human body can be seen as a dynamic system driven by latent forces. In addition, the hidden structure of these forces can be captured through Gaussian processes (GP) modeling. Accordingly, regression processes are able to give suitable models for both classification and prediction. We applied this formalism to daily life actions recognition and tested it successfully on a collection of real activities. The obtained results show the effectiveness of the approach. We discuss also our future developments in addressing intention recognition, which can be seen as the early detection facet of human activities recognition.		Zhi-Chao Li;Ryad Chellali;Yi Yang	2016		10.1007/978-3-319-43506-0_20	machine learning;control engineering;engineering;facet (geometry);gaussian process;artificial intelligence;formalism (philosophy)	Vision	-1.0885072858114788	-75.54335753919942	191171
d809da87e9e5f58d6da05893714f9173af78fd9a	identifiability and online estimation of diagnostic parameters with in the glucose insulin homeostasis	observability;glucose insulin homeostasis;identifiability;unscented kalman filter	Today, diagnostic decisions about pre-diabetes or diabetes are made using static threshold rules for the measured plasma glucose. In order to develop an alternative diagnostic approach, dynamic models as the Minimal Model may be deployed. We present a novel method to analyze the identifiability of model parameters based on the interpretation of the empirical observability Gramian. This allows a unifying view of both, the observability of the system's states (with dynamics) and the identifiability of the system's parameters (without dynamics). We give an iterative algorithm, in order to find an optimized set of states and parameters to be estimated. For this set, estimation results using an Unscented Kalman Filter (UKF) are presented. Two parameters are of special interest for diagnostic purposes: the glucose effectiveness S(G) characterizes the ability of plasma glucose clearance, and the insulin sensitivity S(I) quantifies the impact from the plasma insulin to the interstitial insulin subsystem. Applying the identifiability analysis to the trajectories of the insulin glucose system during an intravenous glucose tolerance test (IVGTT) shows the following result: (1) if only plasma glucose G(t) is measured, plasma insulin I(t) and S(G) can be estimated, but not S(I). (2) If plasma insulin I(t) is captured additionally, identifiability is improved significantly such that up to four model parameters can be estimated including S(I). (3) The situation of the first case can be improved, if a controlled external dosage of insulin is applied. Then, parameters of the insulin subsystem can be identified approximately from measurement of plasma glucose G(t) only.	diabetes mellitus;glucose metabolism disorders;gramian matrix;homeostasis;interstitial webpage;intravenous glucose tolerance test;iterative method;kalman filter;plasma active;prediabetes syndrome;rule (guideline);algorithm;insulin, isophane	Claudia Eberle;Christoph Ament	2012	Bio Systems	10.1016/j.biosystems.2011.11.003	kalman filter;econometrics;observability;identifiability;computer science;control theory	ML	7.317387944726376	-73.36961583469723	191351
ba9cf0ab0fbf9fbd6340c075ddc2feb7575cdbb1	analysis of melody roots in hungarian folk music using self-organizing maps with adaptively weighted dynamic time warping	distance function;self organized map;dynamic time warping	"""In the present paper, we describe a method for extracting certain basic note sequences that should function as stable starting points for variations in Hungarian folk music. The database of our study contains melody section contours constructed from digitized notes of 2323 Hungarian folk songs. Using these contour sequences, a self-organizing map (SOM) was trained to determine typical note sequences appearing in many individual melody sections. To identify these """"roots"""" in different parts of melody sections, we worked out a version of dynamic time warping with adaptive weights for the purpose of the distance function of the SOM. The resulting melody roots proved to be of clear and self-contained musical meaning, and they correspond to the results of classical ethnomusicology. The system of the roots can be interpreted as a condensed form of the main information of the studied musical culture."""	dynamic time warping;map;organizing (structure);roots	Zoltan Juhasz	2007	Applied Artificial Intelligence	10.1080/08839510600940116	speech recognition;metric;computer science;artificial intelligence;dynamic time warping	AI	-4.499307653570764	-78.78667640313937	191683
b4360a50e5ff9d72dcf240e33c95684eadf2e295	high-specificity neurological localization using a connectionist model	connectionist models	Most previous connectionist models for diagnosis have been developed using error backpropagation. While these systems function reasonably well, they have been limited by their need for a large database of test cases, to situations where a single disorder is present, and by the large number of connections required between fully-connected sets of processing units. Here we describe a recently developed connectionist model that overcomes these limitations. This approach can reuse existing causal knowledge bases, works well in situations where multiple disorders can occur simultaneously, and does not require fully-connected sets of processing units. We demonstrate that the accuracy of this model is comparable to that of more conventional AI programs using the same knowledge base in determining precisely the site of brain damage in a group of 50 stroke patients. These results support the conclusion that connectionist models can effectively use pre-existing causal knowledge bases from AI systems, and that they can function accurately when handling actual clinical problems.		Stanley Tuhrim;James A. Reggia;Yun Peng	1994	Artificial intelligence in medicine	10.1016/0933-3657(94)90028-0	computer science;artificial intelligence;machine learning;algorithm	AI	4.685048685194033	-78.5711639817132	191734
24d0de650f7449cd92627ca983e7f18a75f2b2aa	lost in localization? the focus is meta-analysis	brain;meta analysis;terminology as topic;brain mapping;databases as topic;humans;meta analysis as topic	The recent commentary by Derrfuss J, Mar RA. (2009). Lost in localization: the need for a universal coordinate database. Neuroimage, In Press proposed a universal coordinate database to archive functional neuroimaging results. In this response, we discuss our strategy in developing the BrainMap database, which was created as a mechanism to promote coordinate-based meta-analysis methods.	functional neuroimaging;traffic collision avoidance system;archive	Angela R. Laird;Jack L. Lancaster;Peter T. Fox	2009	NeuroImage	10.1016/j.neuroimage.2009.06.047	psychology;neuroscience;meta-analysis;computer science;bioinformatics;data mining;brain mapping;information retrieval	Vision	-1.2345018890746453	-69.92780514755964	191739
692942f2b8c0b1dbce813b849a007945810606b9	a method for a real-time novel premature infant pain profile using high rate, high volume physiological data streams	clinical decision support system premature infant pain profile physiological data streams neonatal population pain management neonatal intensive care unit newborn infant artemis premature infant pain profile nicu the hospital for sick children toronto pipp scores apipp algorithm;paediatrics decision support systems medical information systems	Management of pain in the neonatal population is one of the most challenging problems in the Neonatal Intensive Care Unit (NICU). There are many gaps in our knowledge of the optimal utilization of the techniques available to assess pain in a valid and reliable manner. Pain that is not treated may cause significant clinical complications in the newborn infant. An initial start was made in a previous study to design a novel Premature Infant Pain Profile with a substantial focus on physiological factors. In this study, the novel Artemis Premature Infant Pain Profile (APIPP) was retrospectively compared with the Premature Infant Pain Profile (PIPP) that is used in the NICU at The Hospital for Sick Children, Toronto, to evaluate the effectiveness and validity of the novel scale being proposed. The main objective of this research was to use a preliminary case study approach by using retrospective data that were assigned PIPP scores to compare the performance of the novel APIPP with the current standard for measuring pain. The experiment was performed with two surgical subjects' data retrospectively with the APIPP algorithm. We demonstrated this study using an environment known as Artemis, a clinical decision support system.	algorithm;clinical decision support system;real-time transcription	Tanvi Naik;Anirudh Thommandram;K. E. Shiron Fernando;Nadja Bressan;Andrew James;Carolyn McGregor	2014	2014 IEEE 27th International Symposium on Computer-Based Medical Systems	10.1109/CBMS.2014.29	intensive care medicine;medicine;computer science;pediatrics;surgery	DB	6.09102530169699	-79.39543845380808	193256
0bbb1c23c2327a77aa403bc2310fc198b12ddddc	surgical models for computer-assisted neurosurgery	operating room;surgical planning;multimodal image guided neurosurgery;decision tree;patient specific model;data mining;brain tumor;surgical ontology;surgical models;surgical workflows;decision making process;surgical procedure;human computer interface;computer assisted surgery	In this paper, we outline a way to improve computer-assisted neurosurgery using surgical models along with patient-specific models built from multimodal images. We propose a methodological framework for surgical models that include the definition of a surgical ontology, the development of software for describing surgical procedures based on this ontology and the analysis of these descriptions to generate knowledge about surgical practice. Knowledge generation is illustrated by two studies. One hundred fifty-nine patients who underwent brain tumor surgery were described from postoperative reports using the surgical ontology. First, from a subset of 106 surgical cases, we computed a decision tree using a prediction approach that gave probability in terms of operating room patient positioning percentages and according to tumor location within one or more lobes. Second, from the whole set of 159 surgical cases, we identified 6 clusters describing families of cases according to pathology-related parameters. Results from both studies showed possible prediction of parts of the surgical procedure from pathology-related characteristics of the patient. Surgical models enable surgical knowledge to be made explicit, facilitating the surgical decision-making process and surgical planning and improving the human-computer interface during surgery.		Pierre Jannin;Xavier Morandi	2007	NeuroImage	10.1016/j.neuroimage.2007.05.034	decision-making;medicine;pathology;decision tree;surgery	AI	4.084814586524873	-79.41805085411903	193570
80870309855439e0c440f001150c306376adf38e	the neuroelectromagnetic inverse problem and  the zero dipole localization error	health research;uk clinical guidelines;biological patents;europe pubmed central;citation search;serveur institutionnel;inverse problem;archive institutionnelle;uk phd theses thesis;open access;life sciences;archive ouverte unige;cybertheses;uk research reports;medical journals;institutional repository;europe pmc;biomedical research;bioinformatics	A tomography of neural sources could be constructed from EEG/MEG recordings once the neuroelectromagnetic inverse problem (NIP) is solved. Unfortunately the NIP lacks a unique solution and therefore additional constraints are needed to achieve uniqueness. Researchers are then confronted with the dilemma of choosing one solution on the basis of the advantages publicized by their authors. This study aims to help researchers to better guide their choices by clarifying what is hidden behind inverse solutions oversold by their apparently optimal properties to localize single sources. Here, we introduce an inverse solution (ANA) attaining perfect localization of single sources to illustrate how spurious sources emerge and destroy the reconstruction of simultaneously active sources. Although ANA is probably the simplest and robust alternative for data generated by a single dominant source plus noise, the main contribution of this manuscript is to show that zero localization error of single sources is a trivial and largely uninformative property unable to predict the performance of an inverse solution in presence of simultaneously active sources. We recommend as the most logical strategy for solving the NIP the incorporation of sound additional a priori information about neural generators that supplements the information contained in the data.	american nurses' association;antibodies, antinuclear;arabic numeral 0;choice behavior;choose (action);computation;contain (action);electroencephalography;endeavour (supercomputer);epilepsy;image noise;internationalization and localization;magnetoencephalography;manuscripts;medical records, problem-oriented;relevance;semantics (computer science);tomography	Rolando Grave de Peralta Menendez;Olaf Hauk;Sara González Andino	2009		10.1155/2009/659247	psychology;simulation;medicine;computer science;bioinformatics;inverse problem;artificial intelligence;machine learning;data mining;mathematics;operations research;algorithm;statistics	Vision	-0.7519166388244646	-69.55504025653727	193743
c5f46c84a451b6e1587ee1ca5bfc1b8c1653b5f6	molecular diagnostics lims functions in an anatomic pathology lims environment: experience and challenges (a case study)				James R. Davie;James H. Harrison;Jeffrey A. Kant	2001			medicine;pathology;bioinformatics;biological engineering	SE	2.3458177350148666	-67.9228880481992	194388
bc123bfbf5daa3134b6fde4b07bafb752455d0b0	sensitivity analysis on effect of biomechanical factors for classifying vertebral deformities		Classification of degenerations prevalent in human population is considered to be a crucial task which is performed by a physician or the radiologist. With numerous data being generated and innumerable features getting extracted, identification of normal and pathological case becomes a daunting process. Data learning techniques provide valuable resources in automating the entire procedure easing the burden on the consultant physician. However, since the inception of various machine learning techniques, feasible solution at the cost of computational expense needs to be evaluated. Factors considered for classification play a significant role in defining the accuracy of a system. The current study aims at demonstrating the trade off achieved at the expense of accuracy amongst the number of features and instances. In this article, vertebral column dataset from UCI repository is used for training and testing. Effect of various data pre-processing techniques are presented alongside an extensive study on feature selection method. For validation, breast tissue dataset from the former repository is considered and analyzed.		Jiyo S. Athertya;Gurunathan Saravana Kumar	2016		10.1007/978-3-319-60618-7_2	artificial intelligence;consultant physician;machine learning;feature selection;computer science;population	HCI	6.567725313613305	-77.36612650392105	195051
207ad50542b57a4de091bc64bd5f65f2332cfe0d	patient-specific explanation in models of chronic disease	explanation;chronic disease;decision theory;patient education;bayesian networks	Clinical models of chronic disease characteristically must represent significant uncertainty in both the data input and inferences. This lack of determinism makes it especially difficult for system users to understand and have confidence in the models. This paper presents a representation for uncertainty and patient preferences that serves as a framework for graphical summary and computer-generated explanation of patient-specific clinical decision models. The implementation described is a computer decision aid designed to enhance the clinician-patient consultation process for patients with suspected angina pectoris. The generic angina model is represented as a Bayesian decision network, where the patient descriptors, probabilities, and preferences are treated as random variables. The initial distributions for these variables represent information on the population of patients with anginal symptoms, and the approach provides a method for efficiently tailoring the distributions to an individual patient. This framework also provides metrics for judging the importance of each variable in the model. The graphical interface uses this information to augment the display of a network representation of the model. Variables that are important for clinician-patient communication are highlighted in the graphical display of the network and included in the text explanation in printed patient-education materials. These techniques serve to keep the explanation of the patient's decision model concise, allowing the communication with the patient to focus on the most important aspects of the treatment decision.		Holly Brügge Jimison;Lawrence M. Fagan;Ross D. Shachter;Edward H. Shortliffe	1992	Artificial Intelligence in Medicine	10.1016/0933-3657(92)90027-M	decision theory;computer science;artificial intelligence;machine learning;bayesian network;data mining;statistics	AI	1.394449278538598	-76.15064279103737	195058
a2702e72b2a99b365bc152a847710acc68bda6e3	diagnosis of hypoglycemic episodes using a neural network based rule discovery system	hypoglycemic episodes;fuzzy regression;genetic program;qt interval;neural networks;blood glucose;journal article;heart rate;department of health;rule discovery;genetic algorithm;western australia;medical diagnosis;production rule;electrocardiogram;type 1 diabetes mellitus;neural network	Hypoglycemia or low blood glucose is dangerous and can result in unconsciousness, seizures and even death for Type 1 diabetes mellitus (T1DM) patients. Based on the T1DM patients’ physiological parameters, corrected QT interval of the electrocardiogram (ECG) signal, change of heart rate, and the change of corrected QT interval, we have developed a neural network based rule discovery system with hybridizing the approaches of neural networks and genetic algorithm to identify the presences of hypoglycemic episodes for TIDM patients. The proposed neural network based rule discovery system is built and is validated by using the real T1DM patients’ data sets collected from Department of Health, Government of Western Australia. Experimental results show that the proposed neural network based rule discovery system can achieve more accurate results on both trained and unseen T1DM patients’ data sets compared with those developed based on the commonly used classification methods for medical diagnosis, statistical regression, fuzzy regression and genetic programming. Apart from the achievement of these better results, the proposed neural network based rule discovery system can provide explicit information in the form of production rules which compensate for the deficiency of traditional neural network method which do not provide a clear understanding of how they work in prediction as they are in an implicit black-box structure. This explicit information provided by the product rules can convince medical doctors to use the neural networks to perform diagnosis of hypoglycemia on T1DM patients.	angular defect;artificial neural network;association rule learning;black box;chomsky hierarchy;discovery system;fuzzy concept;genetic algorithm;genetic programming;sensitivity and specificity	Kit Yan Chan;Sai-Ho Ling;Tharam S. Dillon;Hung T. Nguyen	2011	Expert Syst. Appl.	10.1016/j.eswa.2011.02.020	qt interval;genetic algorithm;computer science;artificial intelligence;machine learning;medical diagnosis;data mining;artificial neural network	ML	6.1817358931425614	-77.98247626864425	195218
99e269d7011f8b9848ad7459e8e827f39fc24aed	cyclic causal discovery from continuous equilibrium data	universiteitsbibliotheek;article in monograph or in proceedings	We propose a method for learning cyclic causal models from a combination of observational and interventional equilibrium data. Novel aspects of the proposed method are its ability to work with continuous data (without assuming linearity) and to deal with feedback loops. Within the context of biochemical reactions, we also propose a novel way of modeling interventions that modify the activity of compounds instead of their abundance. For computational reasons, we approximate the nonlinear causal mechanisms by (coupled) local linearizations, one for each experimental condition. We apply the method to reconstruct a cellular signaling network from the flow cytometry data measured by Sachs et al. (2005). We show that our method finds evidence in the data for feedback loops and that it gives a more accurate quantitative description of the data at comparable model complexity.	approximation algorithm;causal filter;cell signaling;feedback;nonlinear system	Joris M. Mooij;Tom Heskes	2013	CoRR		econometrics;computer science;artificial intelligence;machine learning;data mining;mathematics;statistics	ML	9.424797101211606	-68.12345995450359	195248
7aab46e5364ba89087d54927d2a0e14b52ce13e7	intelligent optimization models for disease diagnosis using a service-oriented architecture and management science	patient diagnosis;iom;integrated approach;pattern clustering;information retrieval;linear network;soa;health risk;service oriented architecture diseases genetic algorithms health care information retrieval linear programming management science medical computing multi agent systems patient diagnosis pattern clustering;medical computing;accuracy;multi agent systems;health risk reduction;intelligent agents;disease diagnosis;stem academic environment;intelligent optimization models;diseases accuracy service oriented architecture genetic algorithms optimization;intelligent agent;stem academic environment intelligent optimization models disease diagnosis service oriented architecture management science medical care industries health care industries iom soa ms health risk reduction linear programming linear network search methodologies information retrieval clustering extended genetic algorithm intelligent agents stem undergraduate degree programs;linear programming;diseases;linear program;genetic algorithm;search methodologies;genetic algorithms;stem undergraduate degree programs;optimization;clustering extended genetic algorithm;ms;medical error;service oriented architecture;health care industries;medical care industries;optimization model;disease diagnosis soa management science clustering extended genetic algorithm;management science;health care	The accuracy of disease diagnosis remains a significant challenge that medical and health care industries experience due to a relative lack of misdiagnosis studies and a difficulty of retrieving patients' information. Validation of diagnosis and certainty of its accuracy is the goal of this research. The research, as reported in this paper introduces an innovative solution to determine the accuracy of disease diagnosis. The solution is based on Intelligent Optimization Models (IOM) using integration of Service-Oriented Architecture (SOA) and Management Science (MS). These models enable medical doctors to make inference about disease diagnosis and allow a quick diagnosis of diseases at higher level of accuracy. The models also have the advantage of reducing health risk associated with experimenting with real patients. In particular, bad decisions that cause death or wrong treatment can be avoided. About 44,000 to 98,000 Americans die annually as the result of medical errors. Experimenting with these models requires less time and is less expensive than experimenting with studying patient's condition. In a SOA environment, the study of this research develops new intelligent concepts. These concepts integrate approaches of management science models including linear programming and network, search methodologies, information retrieval, clustering extended genetic algorithm, and intelligent agents. A prototype is created and examined in order to validate the concepts. The proposed concepts strengthen the capacity and quality of STEM undergraduate degree programs. The concepts also promote a vigorous STEM academic environment to increase the number of students entering STEM careers.	cluster analysis;experiment;genetic algorithm;information retrieval;intelligent agent;linear programming;management science;mathematical optimization;prototype;service-oriented architecture;service-oriented device architecture	Naser El-Bathy;Clay S. Gloster;Ghassan Azar;Mohammed El-Bathy;Gordon Stein	2012	2012 IEEE International Conference on Electro/Information Technology	10.1109/EIT.2012.6220716	simulation;genetic algorithm;computer science;engineering;linear programming;electrical engineering;machine learning;service-oriented architecture;data mining;management science;computer security;intelligent agent	Robotics	2.268288037283464	-78.4407882991963	195413
ac4e2697c2b10c3404e2b604484a6d9cf93a04c4	a change in vaccine efficacy and duration of protection explains recent rises in pertussis incidence in the united states	aged 19 35 months;female;incidence;08 information and computing sciences;vaccine potency;adolescent;child preschool;male;infant;biochemical research methods;treatment outcome;journal article;epidemiology;booster doses;vaccines;age distribution;population surveillance;whooping cough;immunity;prevention;bordetella pertussis;science technology;vaccination;infant newborn;01 mathematical sciences;immunization schedule;adult;case control studies;life sciences biomedicine;resurgence;children;child;models statistical;biochemistry molecular biology;risk assessment;mathematical computational biology;pertussis;humans;coverage;adolescents;completeness;young adult;diphtheria tetanus pertussis vaccine;diagnosis;computer simulation;age groups;infectious disease epidemiology;06 biological sciences;vaccination and immunization;bioinformatics	Over the past ten years the incidence of pertussis in the United States (U.S.) has risen steadily, with 2012 seeing the highest case number since 1955. There has also been a shift over the same time period in the age group reporting the largest number of cases (aside from infants), from adolescents to 7-11 year olds. We use epidemiological modelling and a large case incidence dataset to explain the upsurge. We investigate several hypotheses for the upsurge in pertussis cases by fitting a suite of dynamic epidemiological models to incidence data from the National Notifiable Disease Surveillance System (NNDSS) between 1990-2009, as well as incidence data from a variety of sources from 1950-1989. We find that: the best-fitting model is one in which vaccine efficacy and duration of protection of the acellular pertussis (aP) vaccine is lower than that of the whole-cell (wP) vaccine, (efficacy of the first three doses 80% [95% CI: 78%, 82%] versus 90% [95% CI: 87%, 94%]), increasing the rate at which disease is reported to NNDSS is not sufficient to explain the upsurge and 3) 2010-2012 disease incidence is predicted well. In this study, we use all available U.S. surveillance data to: 1) fit a set of mathematical models and determine which best explains these data and 2) determine the epidemiological and vaccine-related parameter values of this model. We find evidence of a difference in efficacy and duration of protection between the two vaccine types, wP and aP (aP efficacy and duration lower than wP). Future refinement of the model presented here will allow for an exploration of alternative vaccination strategies such as different age-spacings, further booster doses, and cocooning.	adolescent (age group);best practice;booster (electric power);booster immunization - actsubstanceadministrationcode;eighty;epidemiology;incidence matrix;mathematical model;mathematics;pertussis vaccine;population parameter;refinement (computing);silo (dataset)	Manoj Gambhir;Thomas A. Clark;Simon Cauchemez;Sara Y. Tartof;David L. Swerdlow;Neil M. Ferguson	2015		10.1371/journal.pcbi.1004138	computer simulation;risk assessment;incidence;case-control study;epidemiology;young adult;population pyramid;toxicology;completeness;vaccination;immunology;demographic profile	ML	8.35048524204394	-73.36215329751656	195646
a3694a0de12cef2abe6913587b9e38e5df04efae	the dependence of machine learning on electronic medical record quality		There is growing interest in applying machine learning methods to Electronic Medical Records (EMR). Across different institutions, however, EMR quality can vary widely. This work investigated the impact of this disparity on the performance of three advanced machine learning algorithms: logistic regression, multilayer perceptron, and recurrent neural network. The EMR disparity was emulated using different permutations of the EMR collected at Children's Hospital Los Angeles (CHLA) Pediatric Intensive Care Unit (PICU) and Cardiothoracic Intensive Care Unit (CTICU). The algorithms were trained using patients from the PICU to predict in-ICU mortality for patients on a held out set of PICU and CTICU patients. The disparate patient populations between the PICU and CTICU provide an estimate of generalization errors across different ICUs. We quantified and evaluated the generalization of these algorithms on varying EMR size, input types, and fidelity of data.		Long Van Ho;David Ledbetter;Melissa D. Aczon;Randall C. Wetzel	2017	AMIA ... Annual Symposium proceedings. AMIA Symposium		artificial intelligence;mathematics;machine learning;medical record;logistic regression;pediatric intensive care unit;recurrent neural network;multilayer perceptron;intensive care unit	ML	6.142545269008065	-75.52826337622199	195652
6a17d5ddb2cb15346343b6ac17fcda98a8222042	a potential causal association mining algorithm for screening adverse drug reactions in postmarketing surveillance	databases;icd 9;international classification of diseases;surveillance biochemistry causality data mining drugs medical information systems;drugs;experience based fuzzy recognition primed decision;unknown adverse drug reactions;association mining;recognition primed decision model rpd;surveillance;computer model;psychology originated qualitative rpd model;electronic medical data;recognition primed decision model rpd adverse drug reactions adrs data mining fuzzy logic postmarketing surveillance potential causal association rules pcars;recognition primed decision;harmful consequences;adverse drug reaction screening;interestingness measure;enalapril potential causal association mining algorithm adverse drug reaction screening postmarketing surveillance unknown adverse drug reactions harmful consequences data mining approach electronic health databases potential causal association rules icd 9 international classification of diseases interestingness measure experience based fuzzy recognition primed decision psychology originated qualitative rpd model drug symptom pair electronic medical data veterans affairs medical center detroit;association rules;frequency measurement;product surveillance postmarketing;data mining;drug symptom pair;electronic health databases;drug toxicity;fuzzy logic;risk ratio;veterans affairs medical center detroit;signal processing computer assisted;potential causal association rules;computational modeling;potential causal association mining algorithm;association rule;computer experiment;medical information systems;adverse drug reactions adrs;enalapril;algorithms;electric potential;drugs association rules electric potential frequency measurement databases computational modeling;pattern recognition automated;humans;postmarketing surveillance;data mining approach;potential causal association rules pcars;databases factual;early detection;adverse drug reaction;veterans affairs;biochemistry;electronic health records	Early detection of unknown adverse drug reactions (ADRs) in postmarketing surveillance saves lives and prevents harmful consequences. We propose a novel data mining approach to signaling potential ADRs from electronic health databases. More specifically, we introduce potential causal association rules (PCARs) to represent the potential causal relationship between a drug and ICD-9 (CDC. (2010). International Classification of Diseases, Ninth Revision (ICD-9). [Online]. Available: http://www.cdc.gov/nchs/icd/icd9.html) coded signs or symptoms representing potential ADRs. Due to the infrequent nature of ADRs, the existing frequency-based data mining methods cannot effectively discover PCARs. We introduce a new interestingness measure, potential causal leverage, to quantify the degree of association of a PCAR. This measure is based on the computational, experience-based fuzzy recognition-primed decision (RPD) model that we developed previously (Y. Ji, R. M. Massanari, J. Ager, J. Yen, R. E. Miller, and H. Ying, “A fuzzy logic-based computational recognition-primed decision model,” Inf. Sci., vol. 177, pp. 4338-4353, 2007) on the basis of the well-known, psychology-originated qualitative RPD model (G. A. Klein, “A recognition-primed decision making model of rapid decision making,” in Decision Making in Action: Models and Methods, 1993, pp. 138-147). The potential causal leverage assesses the strength of the association of a drug-symptom pair given a collection of patient cases. To test our data mining approach, we retrieved electronic medical data for 16 206 patients treated by one or more than eight drugs of our interest at the Veterans Affairs Medical Center in Detroit between 2007 and 2009. We selected enalapril as the target drug for this ADR signal generation study. We used our algorithm to preliminarily evaluate the associations between enalapril and all the ICD-9 codes associated with it. The experimental results indicate that our approach has a potential to better signal potential ADRs than risk ratio and leverage, two traditional frequency-based measures. Among the top 50 signal pairs (i.e., enalapril versus symptoms) ranked by the potential causal-leverage measure, the physicians on the project determined that eight of them probably represent true causal associations.	advanced glycosylation end product-specific receptor;adverse reaction to drug;association rule learning;belief revision;causal filter;causality;code;computation;data mining;decision making;disruptive, impulse control, and conduct disorders;doxorubicin;drug delivery systems;enalapril;find-a-drug;fuzzy logic;knowledge representation and reasoning;mental association;patients;published database;rule (guideline);volume;whole earth 'lectronic link;algorithm	Yanqing Ji;Hao Ying;Peter Dews;Ayman Mansour;John Tran;Richard E. Miller;R. Michael Massanari	2011	IEEE Transactions on Information Technology in Biomedicine	10.1109/TITB.2011.2131669	computer simulation;association rule learning;medicine;computer science;data science;machine learning;data mining;computer security	ML	2.284756576937018	-76.03410089334858	195672
aeabf8b11c607f3503ab5649a7f63488bbc1d283	precise prediction for managing chronic disease readmissions	public health system precise prediction chronic disease readmission managing hospital readmissions health disease patients chronic disease patients healthcare funding resource utilization prediction models moderate predictive power discriminative ability cohort population clinical data rehospitalization cross validation receiver operating characteristic curve analysis candidate prediction algorithms;sensitivity analysis diseases health care medical information systems patient treatment	Potentially preventable hospital readmissions have a crippling effect on the health of chronic disease patients and on healthcare funding and resource utilization. While several prediction models have been proposed to help identify and manage high risk patients, most offer only moderate predictive power and discriminative ability. We develop and validate several models that utilize cohort population and clinical data and are capable of precisely identifying chronic disease patients with a high risk of rehospitalization within 30 days. Cross validation and receiver operating characteristic curve analysis are used to examine the predictive power of the models. The developed models offer high precision and discrimination and outperform current state of the art models. Delivering between 73% and 79% sensitivity at 93% specificity, the models offer excellent candidate prediction algorithms for the battle against the burden of chronic disease on the public health system.	algorithm;chronic disease;cross reactions;patients;preventable;receiver operating characteristic;sensitivity and specificity;seventy nine	Sankalp Khanna;Justin Boyle;Norm Good	2014	2014 36th Annual International Conference of the IEEE Engineering in Medicine and Biology Society	10.1109/EMBC.2014.6944188	intensive care medicine;medicine;computer science;physical therapy;medical emergency	SE	6.826657768713091	-75.91224611249332	195789
8c9bdd2dc0b05bad8f40a32885cde2764de03e78	on the dynamical properties of a model of cell differentiation	signal image and speech processing;health research;uk clinical guidelines;biological patents;europe pubmed central;systems biology;citation search;computational biology bioinformatics;biomedical engineering;uk phd theses thesis;life sciences;uk research reports;medical journals;europe pmc;biomedical research;bioinformatics	One of the major challenges in complex systems biology is that of providing a general theoretical framework to describe the phenomena involved in cell differentiation, i.e., the process whereby stem cells, which can develop into different types, become progressively more specialized. The aim of this study is to briefly review a dynamical model of cell differentiation which is able to cover a broad spectrum of experimentally observed phenomena and to present some novel results.	cell differentiation process;complex systems biology;experiment;stem cells	Marco Villani;Roberto Serra	2013		10.1186/1687-4153-2013-4	biology;medical research;computer science;bioinformatics;engineering;data science;biological engineering;systems biology	ML	5.003465249018117	-67.60073712817565	195930
098b1b3d368a0eddb0d6f7161df10c1791721a1f	sas macros for estimation of direct adjusted cumulative incidence curves under proportional subdistribution hazards models	subdistribution;software;treatment effect;hazard ratio;genie biomedical;cumulative incidence function;regression model;proportional hazards models;proportional hazard model;cumulative incidence;standard error;hazard models;bone marrow transplant;prognostic factor;biomedical engineering;bone marrow transplantation;cox model;models statistical;regression analysis;ingenieria biomedica;humans;hazard function;competing risks	The cumulative incidence function is commonly reported in studies with competing risks. The aim of this paper is to compute the treatment-specific cumulative incidence functions, adjusting for potentially imbalanced prognostic factors among treatment groups. The underlying regression model considered in this study is the proportional hazards model for a subdistribution function [1]. We propose estimating the direct adjusted cumulative incidences for each treatment using the pooled samples as the reference population. We develop two SAS macros for estimating the direct adjusted cumulative incidence function for each treatment based on two regression models. One model assumes the constant subdistribution hazard ratios between the treatments and the alternative model allows each treatment to have its own baseline subdistribution hazard function. The macros compute the standard errors for the direct adjusted cumulative incidence estimates, as well as the standard errors for the differences of adjusted cumulative incidence functions between any two treatments. Based on the macros' output, one can assess treatment effects at predetermined time points. A real bone marrow transplant data example illustrates the practical utility of the SAS macros.	baseline (configuration management);bone marrow transplantation;estimated;failure rate;incidence matrix;pooled sample;proportional hazards model;sas	Xu Zhang;Mei-Jie Zhang	2011	Computer methods and programs in biomedicine	10.1016/j.cmpb.2010.07.005	econometrics;mathematics;proportional hazards model;regression analysis;statistics	Vision	8.74066645513985	-74.41666797633876	196501
8b5fc8bb6f165806a3b86944a503ddb318507db3	construction of a semi-naive model to predict early readmission of copd patients by using quality care information	databases;bayes methods;niobium;hospitals;mathematical model;diseases;predictive models	The quality of inpatient care in emergency rooms has proved a major factor to be taken into account upon hospital admission. Several studies show that early readmission of Chronic Obstructive Pulmonary Disease patients after discharge can be predicted according to information on the assistance quality received during the patient's hospital stay, however, these studies use many variables to be captured, from admission to discharge dates, which makes it difficult, or even impossible, to follow the track and record all the information. We propose an algorithm to construct a Semi-Naive model capable of both selecting and aggregating attributes related to inpatient care quality to result in a selection of very few attributes, while overcoming the performance of other early readmission prediction models.	algorithm;discharger;feature selection;money;naive bayes classifier;search algorithm;semiconductor industry	Pablo Bermejo;José A. Gámez;Jose Miguel Puerta;Marco A. Esquivias;Pedro J. Tárraga	2016	2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)	10.1109/ICDMW.2016.0040	niobium;computer science;machine learning;mathematical model;predictive modelling;statistics	DB	4.514529925978134	-77.44269866729287	196606
ef3dd08f2f7890c919d8ad90960e14170601fec8	classifying and validating intermittent eeg patterns with syntactic methods	grammar;systeme nerveux pathologie;electrodiagnostic;biomedical data processing;electroencefalografia;computerized processing;tratamiento informatico;informatique biomedicale;hombre;classification;attribute grammar;electroencephalographie;electrodiagnostico;analyse syntaxique;nervous system diseases;analisis sintaxico;grammaire;syntactic analysis;sistema nervioso patologia;human;pattern recognition;reconnaissance forme;informatica biomedica;electroencephalography;reconocimiento patron;electrodiagnosis;traitement informatique;gramatica;clasificacion;homme;grammaire attribut	For diagnostic purposes the EEG is recorded as a multichannel signal. For classifying and validating intermittent EEG patterns, the temporal and spatial relations between the constituent basic patterns are important. When attempting syntactic pattern recognition from these patterns, the absence of a natural string representation causes problems. They were solved by using a type of attributed grammar and putting all spatial and temporal information into the attributes. Three of these grammars are used in a production system together with a scheduling algorithm. The first results from this system are given.	electroencephalography;syntactic methods	Georg Ferber	1986	Pattern Recognition	10.1016/0031-3203(86)90054-3	speech recognition;electroencephalography;electrodiagnosis;biological classification;computer science;artificial intelligence;parsing;grammar;attribute grammar	Vision	-4.1052148495224	-73.23721805856913	196760
23eea89bf549f1f1ee35ecd06c0ad104e4047ea1	seeing the wood for the trees: a forest of methods for optimization and omic-network integration in metabolic modelling	article	Metabolic modelling has entered a mature phase with dozens of methods and software implementations available to the practitioner and the theoretician. It is not easy for a modeller to be able to see the wood (or the forest) for the trees. Driven by this analogy, we here present a 'forest' of principal methods used for constraint-based modelling in systems biology. This provides a tree-based view of methods available to prospective modellers, also available in interactive version at http://modellingmetabolism.net, where it will be kept updated with new methods after the publication of the present manuscript. Our updated classification of existing methods and tools highlights the most promising in the different branches, with the aim to develop a vision of how existing methods could hybridize and become more complex. We then provide the first hands-on tutorial for multi-objective optimization of metabolic models in R. We finally discuss the implementation of multi-view machine learning approaches in poly-omic integration. Throughout this work, we demonstrate the optimization of trade-offs between multiple metabolic objectives, with a focus on omic data integration through machine learning. We anticipate that the combination of a survey, a perspective on multi-view machine learning and a step-by-step R tutorial should be of interest for both the beginner and the advanced user.	dna integration;database validation plan;ephrin type-b receptor 1, human;manuscripts;metabolic process, cellular;numerous;poly a;post-translational protein processing;seizures;silo (dataset);trees (plant);algorithm;anatomical layer;statistical cluster	Supreeta Vijayakumar;Max Conway;Pietro Liò;Claudio Angione	2018	Briefings in bioinformatics	10.1093/bib/bbx053	biology;botany;computer science	AI	-0.2355243602947407	-67.11825510776335	197171
75358af84d159f32716e33d43bc46fccfeb3e772	length of stay prediction and analysis through a growing neural gas model	business intelligence in health care;computer science all;los prediction;self organizing networks	Length of stay (LoS) prediction is considered an important research field in Healthcare Informatics as it can help to improve hospital bed and resource management. The health cost containment process carried out in Italian local healthcare systems makes this problem particularly challenging in healthcare services management. In this work a novel unsupervised LoS prediction model is presented which performs better than other ones commonly used in this kind of problem. The developed model detects autonomously the subset of non-class attributes to be considered in these classification tasks, and the structure of the trained selforganizing network can be analysed in order to extract the main factors leading to the overcoming of regional LoS threshold.	informatics;neural gas;unsupervised learning	Luigi Lella;Antonio Di Giorgio;Aldo Franco Dragoni	2015			simulation;engineering;artificial intelligence;data science	AI	5.08250139191909	-77.2928885385417	197296
3b6649c2b6fc8fab3bd841d793527972c4f085b4	computing pathways to systems biology: key contributions of computational methods in pathway identification		Understanding large molecular networks consisting of entities such as genes, proteins or RNAs that interact in complex ways to drive the cellular machinery has been an active focus of systems biology. Computational approaches have played a key role in systems biology by complementing theoretical and experimental approaches. Here we roadmap some key contributions of computational methods developed over the last decade in the reconstruction of biological pathways. We position these contributions in a ‘systems biology perspective’ to reemphasize their roles in unravelling cellular mechanisms and to understand ‘systems biology diseases’ including cancer.	computation;entity;gene regulatory network;kripke semantics;systems biology	Sriganesh Srihari;Mark A. Ragan	2013	CoRR		computational biology;biology;cell biology;bioinformatics;systems biology;complex systems biology	Comp.	4.263273159552847	-67.20230014997533	197507
1a2019d50584c7923b3523451bb3c6fd38a61f2e	optimal icu admission control with premature discharge		The intensive care unit (ICU) delivers care to critically ill patients with high resource intensity. Stochastic patient arrival and uncertain length of stay in ICUs present tremendous challenge to establish the admission and discharge policy to maximize the number of surviving patients and improve operational efficiency. In practice, ICU schedulers reserve some beds for potential patients with most critical conditions. Moreover, they prematurely discharge current ICU patients who are in stable status to accommodate for new and more urgent arrivals. We develop an analytical framework to quantify the impact of the number of reserved beds and suggest when to prematurely discharge current patients. A Markov decision process model is established to strike a balance between the rejection of incoming patient and the premature discharge in the near future. Monotonicity, concavity, and structural properties of the optimal control policy are presented. Using the inpatient record at a tertiary-level hospital in China, we conduct a case study and propose an effective threshold policy. Extensive numerical experiments are performed to analyze the effect of each parameter on the total survival benefits and compare different policies. Note to Practitioners—The intensive care unit (ICU) is the most resource intensive unit in the hospital, which has a significant impact on patient safety and care quality. Due to the increasing and aging population, the contradiction between the increasing demand and the insufficient ICU resources puts eligible ICU candidates at risk of lacking timely ICU care. Given the important role of the ICU in surviving lives of most critical patients, it is of paramount significance to improve ICU resource utilization and maximize the number of ICU survivors. Therefore, the objective of this paper is to develop a mathematical model to study ICU admission policies and premature discharge decisions. Based on our analytical framework, the ICU admission director can evaluate performance measures of ICU admission control system and investigate the impact of different policies on care quality of patients. By using an effective threshold admission policy proposed in this paper, the ICU admission director can be equipped with a decision support tool for the ICU admission control decisions with quantitative measures of potential impact.		Xuanjing Li;Dacheng Liu;Na Geng;Xiaolei Xie	2019	IEEE Transactions on Automation Science and Engineering	10.1109/TASE.2018.2827664	population ageing;computer science;resource management;mathematical optimization;operational efficiency;intensive care medicine;decision support system;admission control;markov decision process;patient safety;intensive care unit	SE	5.7507208520011215	-73.66688832480177	198151
0229158e3d01356a2398db1d5b5a855dfc471c66	the use of artificial neural networks to stratify the length of stay of cardiac patients based on preoperative and initial postoperative factors	length of stay prediction;length of stay;artificial neural networks;outcome prediction;postoperative cardiac surgical patients;artificial neural network	BACKGROUND The limitations of current prognostic models in identifying postoperative cardiac patients at risk of experiencing morbidity and subsequently an extended intensive care unit length of stay (ICU LOS) is well recognized. This coupled with the desire for risk stratification in order to prioritize medical intervention has lead to the need for the development of a system that can accurately predict individual patient outcome based on both preoperative and immediate postoperative clinical factors. The usefulness of artificial neural networks (ANNs) as an outcome prediction tool in the critical care environment has been previously demonstrated for medical intensive care unit (ICU) patients and it is the aim of this study to apply this methodology to postoperative cardiac patients.   METHODS A review of contemporary literature revealed 15 preoperative risk factors and 17 operative and postoperative variables that have a determining effect on LOS. An integrated, multi-functional software package was developed to automate the ANN development process. The efficacy of the resultant individual ANNs as well as groupings or ensembles of ANNs were measured by calculating sensitivity and specificity estimates as well as the area under the receiver operating curve (AUC) when the ANN is applied to an independent test dataset.   RESULTS The individual ANN with the highest discriminating ability produced an AUC of 0.819. The use of the ensembles of networks technique significantly improved the classification accuracy. Consolidating the output of three ANNs improved the AUC to 0.90.   CONCLUSIONS This study demonstrates the suitability of ANNs, in particular ensembles of ANNs, to outcome prediction tasks in postoperative cardiac patients.	area under curve;artificial cardiac pacemaker;artificial neural network;boosting (machine learning);bootstrap aggregating;cardiopulmonary bypass;care of intensive care unit patient;database;databases;estimated;forecast of outcome;generalization (psychology);large;morbidity - disease rate;numerous;patients;principal component analysis;receiver operating characteristic;resultant;sensitivity and specificity;silo (dataset);small;solutions;stratification	Michael Rowan;Thomas Ryan;Francis Hegarty;Neil O'Hare	2007	Artificial intelligence in medicine	10.1016/j.artmed.2007.04.005	computer science;machine learning;artificial neural network	ML	6.849325501294467	-76.37939680570193	198814
54af43addaada5737b669314aca8d6fddb02f323	clinical prediction from structural brain mri scans: a large-scale empirical study		Multivariate pattern analysis (MVPA) methods have become an important tool in neuroimaging, revealing complex associations and yielding powerful prediction models. Despite methodological developments and novel application domains, there has been little effort to compile benchmark results that researchers can reference and compare against. This study takes a significant step in this direction. We employed three classes of state-of-the-art MVPA algorithms and common types of structural measurements from brain Magnetic Resonance Imaging (MRI) scans to predict an array of clinically relevant variables (diagnosis of Alzheimer’s, schizophrenia, autism, and attention deficit and hyperactivity disorder; age, cerebrospinal fluid derived amyloid-β levels and mini-mental state exam score). We analyzed data from over 2,800 subjects, compiled from six publicly available datasets. The employed data and computational tools are freely distributed ( https://www.nmr.mgh.harvard.edu/lab/mripredict ), making this the largest, most comprehensive, reproducible benchmark image-based prediction experiment to date in structural neuroimaging. Finally, we make several observations regarding the factors that influence prediction performance and point to future research directions. Unsurprisingly, our results suggest that the biological footprint (effect size) has a dramatic influence on prediction performance. Though the choice of image measurement and MVPA algorithm can impact the result, there was no universally optimal selection. Intriguingly, the choice of algorithm seemed to be less critical than the choice of measurement type. Finally, our results showed that cross-validation estimates of performance, while generally optimistic, correlate well with generalization accuracy on a new dataset.	application domain;attention deficit hyperactivity disorder;autistic disorder;benchmark (computing);cerebrospinal fluid;class;compiler;computation;consortium;cross reactions;cross-validation (statistics);data collection;estimated;freesurfer;generalization (psychology);hyperactive behavior;largest;machine learning;magnetic resonance imaging;manuscripts;mental association;mental state;mood disorders;nato architecture framework;naloxone;neuroimaging;pet/ct scan;pattern recognition;region of interest;scanning;schizophrenia;scicrunch;scientific publication;self-replication;silo (dataset);sodium fluoride;the 100;tinyurl;webserver directory index;algorithm;format;triangulation	Mert R. Sabuncu;Ender Konukoglu	2014	Neuroinformatics	10.1007/s12021-014-9238-1	simulation;computer science;artificial intelligence;machine learning;data mining;statistics	ML	9.518943123279088	-73.78596214515483	198911
92518e18f0fa7645c2fb8800434f33b01150f68d	evaluation of syndromic algorithms for detecting patients with potentially transmissible infectious diseases based on computerised emergency-department data	sensitivity and specificity;female;health informatics;retrospective studies;middle aged;communicable diseases;male;france;information systems and communication service;medical records systems computerized;population surveillance;early diagnosis;adult;management of computing and information systems;algorithms;emergency service hospital;humans;aged	BACKGROUND The objective of this study was to ascertain the performance of syndromic algorithms for the early detection of patients in healthcare facilities who have potentially transmissible infectious diseases, using computerised emergency department (ED) data.   METHODS A retrospective cohort in an 810-bed University of Lyon hospital in France was analysed. Adults who were admitted to the ED and hospitalised between June 1, 2007, and March 31, 2010 were included (N=10895). Different algorithms were built to detect patients with infectious respiratory, cutaneous or gastrointestinal syndromes. The performance parameters of these algorithms were assessed with regard to the capacity of our infection-control team to investigate the detected cases.   RESULTS For respiratory syndromes, the sensitivity of the detection algorithms was 82.70%, and the specificity was 82.37%. For cutaneous syndromes, the sensitivity of the detection algorithms was 78.08%, and the specificity was 95.93%. For gastrointestinal syndromes, the sensitivity of the detection algorithms was 79.41%, and the specificity was 81.97%.   CONCLUSIONS This assessment permitted us to detect patients with potentially transmissible infectious diseases, while striking a reasonable balance between true positives and false positives, for both respiratory and cutaneous syndromes. The algorithms for gastrointestinal syndromes were not specific enough for routine use, because they generated a large number of false positives relative to the number of infected patients. Detection of patients with potentially transmissible infectious diseases will enable us to take precautions to prevent transmission as soon as these patients come in contact with healthcare facilities.	accident and emergency department;communicable diseases;early diagnosis;erectile dysfunction;gastrointestinal diseases;hospital admission;patients;respiratory insufficiency;sensitivity and specificity;sensor;syndrome;vasculitis, leukocytoclastic, cutaneous;algorithm	Solweig Gerbier;Quentin Gicquel;Anne-Laure Millet;Christophe Riou;Jacqueline Grando;Stéfan Jacques Darmoni;Véronique Pagliaroli;Marie Hélène Metzger	2013		10.1186/1472-6947-13-101	health informatics;medicine;pathology;nursing;retrospective cohort study	ML	9.597519883605777	-77.31537749556244	199052
cdf7b2a7f27215028d514af3162b7d31cd411094	enisi multiscale modeling of mucosal immune responses driven by high performance computing	inflammatory bowel disease;parallel processing cellular biophysics diseases medical computing;enisi;multiscale modeling;hpc;computational modeling immune system load modeling biological system modeling heating;inflammatory bowel disease multiscale modeling enisi hpc cd4 t cell differentiation;gut mucosa enisi multiscale modeling high performance computing computational modeling tool biological process enteric immunity simulator multiscale modeling platform enisi msmv2 mucosal immune response modeling hpc simulation molecular pathway t cell differentiation tissue level interaction immunoregulation mechanisms;cd4 t cell differentiation	Computational modeling tools have increasingly important roles in our understanding of biological processes. Computer simulations guide experimental and clinical efforts in an unprecedented rate. This study presents ENteric Immunity Simulator - multiscale modeling (MSM) platform developed for high performance computing (HPC). ENISI MSMv2 is designed for modeling mucosal immune responses. The system scales to 109 agents in HPC simulations. This is an important step towards building large scale information processing representations of immune responses that integrate multiple modeling technologies and spatiotemporal scales ranging from nanoseconds to years and from molecules to systems. Our HPC-driven ENISI MSM platform combines the study of molecular pathways controlling T cell differentiation and tissue level interactions between cells to characterize novel mechanisms of immunoregulation at the gut mucosa.	computation;computer simulation;information processing;interaction;markov switching multifractal;multiscale modeling;supercomputer	Vida Abedi;Raquel Hontecillas;Stefan Hoops;Nathan Liles;Adria Carbo;Pinyi Lu;Casandra W. Philipson;Josep Bassaganya-Riera	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359768	biology;supercomputer;computer science;bioinformatics;immunology;multiscale modeling	HPC	5.8474565667878196	-68.21102598997007	199059
aa97138a398ab21a7cf02cb666b7c5482bfda45d	comparative study of joint analysis of microarray gene expression data in survival prediction and risk assessment of breast cancer patients	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;gene expression;research articles;abstracts;open access;survival analysis;life sciences;clinical guidelines;risk assessment;microarray;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Microarray gene expression data sets are jointly analyzed to increase statistical power. They could either be merged together or analyzed by meta-analysis. For a given ensemble of data sets, it cannot be foreseen which of these paradigms, merging or meta-analysis, works better. In this article, three joint analysis methods, Z-score normalization, ComBat and the inverse normal method (meta-analysis) were selected for survival prognosis and risk assessment of breast cancer patients. The methods were applied to eight microarray gene expression data sets, totaling 1324 patients with two clinical endpoints, overall survival and relapse-free survival. The performance derived from the joint analysis methods was evaluated using Cox regression for survival analysis and independent validation used as bias estimation. Overall, Z-score normalization had a better performance than ComBat and meta-analysis. Higher Area Under the Receiver Operating Characteristic curve and hazard ratio were also obtained when independent validation was used as bias estimation. With a lower time and memory complexity, Z-score normalization is a simple method for joint analysis of microarray gene expression data sets. The derived findings suggest further assessment of this method in future survival prediction and cancer classification applications.	algorithmic efficiency;forecast of outcome;gene expression;hazard ratio;mammary neoplasms;merge;microarray;overall survival;patients;proportional hazards model;receiver operating characteristic;risk assessment;survival analysis	Haleh Yasrebi	2016		10.1093/bib/bbv092	risk assessment;biology;text mining;gene expression;computer science;bioinformatics;data science;microarray;data mining;survival analysis	Comp.	5.781307689158763	-75.17950071091278	199236
19a91d3ab32ceb522943d024d956fbf6c20a20c8	diagnesia: a prototype of a decision support system for anesthetists	biomedical monitoring;decision support;diagnesia;medical computing decision making decision support systems;prototypes;medical computing;support system;decision support system;heart rate;monitoring;anesthetists;decision support systems;smart alarm capacity;decision making diagnesia decision support system anesthetists smart alarm capacity;surgery;situation awareness;prototypes decision support systems anesthesia surgery patient monitoring anesthetic drugs pain decision making biomedical monitoring surges;anesthesia;diagnesia decision support anesthesia	The complexity of modern anesthesia procedures requires the development of sophisticated workstations with built-in decision support systems having smart-alarm capacity. In this paper, methods used by a prototype (Diagnesia) of a decision support system for anesthetists are presented. During surgery, Diagnesia uses patient data recorded to continuously estimate likelihood and unlikelihood of diagnoses, applying arguments for and against the different diagnoses, and presents the most probable diagnoses to the anesthetist. The intended system is not meant to replace the current monitors or anesthetists but rather to facilitate decision making by improving situation awareness.	decision support system;prototype;workstation	John Kizito	2008	2008 Third International Conference on Broadband Communications, Information Technology & Biomedical Applications	10.1109/BROADCOM.2008.39	intensive care medicine;medicine;biological engineering;anesthesia	Robotics	4.505688689440059	-79.67290303180272	199274
9996a905ced3767a1351e8e33126cf1c9814ca05	classifiers accuracy improvement based on missing data imputation		In this paper we investigate further and extend our previous work on radar signal identification and classification based on a data set which comprises continuous, discrete and categorical data that represent radar pulse train characteristics such as signal frequencies, pulse repetition, type of modulation, intervals, scan period, scanning type, etc. As the most of the real world datasets, it also contains high percentage of missing values and to deal with this problem we investigate three imputation techniques: Multiple Imputation (MI); K-Nearest Neighbour Imputation (KNNI); and Bagged Tree Imputation (BTI). We apply these methods to data samples with up to 60% missingness, this way doubling the number of instances with complete values in the resulting dataset. The imputation models performance is assessed with Wilcoxon’s test for statistical significance and Cohen’s effect size metrics. To solve the classification task, we employ three intelligent approaches: Neural Networks (NN); Support Vector Machines (SVM); and Random Forests (RF). Subsequently, we critically analyse which imputation method influences most the classifiers’ performance, using a multiclass classification accuracy metric, based on the area under the ROC curves. We consider two superclasses (‘military’ and ‘civil’), each containing several ‘subclasses’, and introduce and propose two new metrics: inner class accuracy (IA); and outer class accuracy (OA), in addition to the overall classification accuracy (OCA) metric. We conclude that they can be used as complementary to the OCA when choosing the best classifier for the problem at hand.		Ivan Jordanov;Nedyalko Petrov;Alessio Petrozziello	2018	J. Artif. Intell. Soft Comput. Res.	10.1515/jaiscr-2018-0002	missing data;artificial intelligence;support vector machine;machine learning;imputation (statistics);artificial neural network;computer science;random forest	ML	8.789768637520263	-78.24698426453025	199413
e8d0cb9fa871cd3c7e20db5d82e408fd86e4138c	drted: deep learning recommendation of treatment from electronic data			deep learning	Melissa Aczon;David Ledbetter;Long Van Ho;Alec M. Gunny;Randall C. Wetzel	2016			data mining;deep learning;information retrieval;computer science;artificial intelligence	ECom	0.08583513043073086	-70.56529560426974	199489
0dddbc446aa83eea66a5c21404bd5acfae2d0b1e	sensitivity of diabetic retinopathy associated vision loss to screening interval in an agent-based/discrete event simulation model	agent based modeling;diabetic eye screen;diabetic retinopathy;discrete event simulation	OBJECTIVE To examine the effect of changes to screening interval on the incidence of vision loss in a simulated cohort of Veterans with diabetic retinopathy (DR). This simulation allows us to examine potential interventions without putting patients at risk.   METHODS Simulated randomized controlled trial. We develop a hybrid agent-based/discrete event simulation which incorporates a population of simulated Veterans--using abstracted data from a retrospective cohort of real-world diabetic Veterans--with a discrete event simulation (DES) eye clinic at which it seeks treatment for DR. We compare vision loss under varying screening policies, in a simulated population of 5000 Veterans over 50 independent ten-year simulation runs for each group.   RESULTS Diabetic Retinopathy associated vision loss increased as the screening interval was extended from one to five years (p<0.0001). This increase was concentrated in the third year of the screening interval (p<0.01). There was no increase in vision loss associated with increasing the screening interval from one year to two years (p=0.98).   CONCLUSIONS Increasing the screening interval for diabetic patients who have not yet developed diabetic retinopathy from 1 to 2 years appears safe, while increasing the interval to 3 years heightens risk for vision loss.	agent-based model;complex systems;concentrate dosage form;delivery of health care;diabetes mellitus;diabetic retinopathy;incidence matrix;patients;phenotype;randomized algorithm;retinal diseases;simulation;systems simulation;unspecified visual loss;hearing impairment	T. Eugene Day;Nathan Ravi;Hong Xian;Ann Brugh	2014	Computers in biology and medicine	10.1016/j.compbiomed.2014.01.007	simulation;medicine;computer science;discrete event simulation;diabetes mellitus;surgery	Vision	7.326815189909043	-74.51243146675439	199554
eee2956ffcd4291de1d0b9a102300ba20c5f3176	potential application of biosensor in food nutrition labeling		Biosensor technology is one of the hot research topics in recent years, and has been researched earlier in food analysis scope, but its application in the scope of food nutrition labels is immature. This paper summarizes the application of biosensors in the analysis of food nutrition labels, including carbohydrate, amino acid, vitamin C, fat and so on, and forecasts its development prospect in food ingredient analysis.		Yi Zhai;Tao Zhang;Na Li;Wenqian Qi;Meng Zhou	2017	22017 IEEE International Conference on Computational Science and Engineering (CSE) and IEEE International Conference on Embedded and Ubiquitous Computing (EUC)	10.1109/CSE-EUC.2017.265	computer network;nutrition labeling;food science;food analysis;ingredient;nutrition facts label;computer science;biosensor;sugar	Robotics	2.2334731525692426	-68.0093990886033	199886
