id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
7cce619d34d1737a6429c501b23ba771c089ff2e	modélisation uml/b pour la validation des exigences de sécurité des règles d'exploitation ferroviaires. (uml/b modeling for the safety requirements validation of railway operating rules)		ions de modélisation communes. Cela permet un accès homogène aux modèles dans les différents niveaux (ou différentes couches) grâce à la réflexion. En outre, cela permet de standardiser l’accès aux outils à travers une API (Application Programming Interface) commune et de sérialiser les modèles grâce au standard XMI (XML Metadata Interchange) [Sprinkle et al., 2014]. Le MOF a contribué de manière significative aux principes fondamentaux de l’architecture dirigée par les modèles MDA de l’OMG. 2.3.3 L’approche MDA L’architecture dirigée par les modèles ou MDA (en anglais Model Driven Architecture) est une approche de l’IDM proposée par l’OMG en 2000 (Figure 2.5). Cette approche vise la promulgation des bonnes pratiques de modélisation en exploitant les avantages de considérer le modèle au centre du développement logiciel. Elle se base sur la séparation des préoccupations entre la logique métier des systèmes informatiques et les plate-formes utilisées. Cette séparation est conçue, entre autres, dans le but d’éviter de mêler les spécifications fonctionnelles d’un système avec des spécifications techniques de la phase de mise en production sur une plate-forme donnée. Plusieurs formalismes standards de modélisation sont définis par MDA, notamment UML (Unified Modeling Language), MOF et XMI, afin de promouvoir les qualités intrinsèques des modèles telles que pérennité, productivité et prise en compte des plateformes d’exécution [Combemale, 2008b]. Le MDA place le modèle au centre du cycle de développement. En effet, en s’appuyant sur le standard UML, le principe clé du MDA consiste en l’utilisation des modèles séparément aux différentes phases de développement d’une application : de la phase d’élicitation des exigences jusqu’au codage (Figure 2.6). À cet effet, le MDA préconise l’élaboration des modèles suivants [Combemale, 2008b] : — CIM (Computation Independant Model) qui est un modèle d’exigences indépenChapitre 2. Contexte scientifique 31 Figure 2.5 – MDA par l’OMG dant de l’information, — PIM (Platform Independant Model) qui est un modèle d’analyse et de conception indépendant de tout détail technique de la plate-forme, — PSM (Platform Specific Model) qui est un modèle de code, combinant les spécifications du PIM avec les détails propres de la plate-forme, et — PDM (Platform Description Model) qui est un modèle de description de la plate-forme.		Rahma Yangui	2016				Crypto	-108.14590837163875	20.001802914144996	97861
aeae88cba65e0e23dc46a54a3d0bceea8ff19697	ein intuitives verfahren zur adaptiven merkmalsgestützten segmentierung	004 informatik	Bei der Automatisierung einer Objekterkennungsaufgabe wird eine Prozeskette bestehend aus Vorverarbeitung, Segmentierung, Merkmalsextraktion, Merkmalsselektion und Klassifikation aufgebaut. Im Allgemeinen werden hierfur manuell die geeigneten Algorithmen und ihre Parameter ausgewahlt und konfiguriert, bis man mit den Ergebnissen zufrieden ist. Als Parameter werden meist spezifische Eigenschaften oder Merkmale der Objekte verwendet. Die Auswahl insbesondere der Parameter der Algorithmen sollte auf einer ausreichend grosen, reprasentativen Stichprobe stattfinden, um fur alle zu erkennenden Objekte einer Domane gultig zu sein. Dies ist durch die manuelle Auswahl meist nicht gegeben. Zwar kann der Experte intuitiv gut segmentieren und klassifizieren, jedoch die benotigten Parameter nicht mathematisch prazise formulieren. Ein Algorithmus kann ohne den Experten nicht so wie von diesem gewunscht segmentieren, die Parameter aus einer vorliegenden Segmentierung aber einfach ermitteln - er ist ihm in der Extraktion und statistischen Selektion quantitativ mesbarer Merkmale uberlegen. Im Rahmen dieser Arbeit wurde eine Methodik entwickelt, bei der ein Experte intuitiv Beispiele prasentiert, aus denen der Algorithmus problembezogene Segmentierungsparameter ableitet. Indem ausreichend Beispiele gegeben werden, lernt der Algorithmus, die zu erkennenden Objekte zur Zufriedenheit des Experten zu segmentieren und ihn bei der Automatisierung zu unterstutzen. Diese neue Methodik wurde erfolgreich auf verschiedene Probleme angewendet. Fur kunstliche Bilder ergaben sich Ubereinstimmungen der Segmentierungen zu Referenzsegmentierungen von im Mittel 95%. Fur die Brustkrebserkennung konnten Detektionsraten von bis zu 97% erzielt werden. Fur die Segmentierung von Kaumuskeln auf MRT-Daten konnten automatisch Parameter gefunden werden, mit denen die manuelle Segmentierung zu 99% reproduziert werden konnte. Weiterhin wurde die Methode zur Segmentierung naturlicher und kunstlicher Szenen verwendet und bei der Detektion von Lunkern in Spritzguswerkstucken getestet. Die Ergebnisse konnten hier nur subjektiv bewertet werden. Bisher kommen nur lokale Merkmale von Grauwertbildern zum Einsatz. Aufgrund ihres generalisierenden Ansatzes konnen mit der entwickelten Methodik beliebige lokale Merkmale (z.B. Farbe) der Objekte als Parameter verwendet werden.		Michael Beller	2005			performance art;art	Vision	-107.48197902323693	31.70114158564983	99956
efe74a20cc596995f75bcd748c11da1c71f36ce4	posicionamento de circuitos 3d considerando o planejamento de 3d-vias		This article explores methods to perform placement of 3D circuits considering issues related to vertical interconnects (3D-Vias). A complete flow, starting from the I/O pins handling, global placement, detailed placement and 3D-Via placement is presented. The I/O pins algorithm spreads the I/Os evenly and aids the placer to obtain a reduced number of 3D-Vias. The global placement engine based on Quadratic algorithm considers the technology information and 3D-Via pitch to reduce wire length and balance the cells distribution in 3D. Critical connections can be handled with the insertion of artificial nets that lead to 3D-Via avoidance for those nets. Finally, 3D-Vias are placed by a fast algorithm based on Tetris legalization. The whole framework enforces the potential benefits of 3D-Circuits on wire length improvement and demonstrates efficient algorithms designed for 3D placement that can be incorporated into new CAD (Computer Aided Design) tools. 1 Introdução Um dos problemas mais importantes no projeto de circuitos VLSI imposto pelas tecnologias recentes está relacionado com os fios do circuito. Primeiro, consideremos o aumento do tamanho dos projetos enquanto que o tamanho dos componentes do circuito está se tornando drasticamente menor. Este cenário produz redes de fios cada vez maiores, densas e complexas. Segundo, o atraso dos componentes ativos reduziu-se mais rapidamente que o atraso dos fios. Hoje em dia, a resistência dos fios é extremamente relevante, enquanto que isto era ignorado no passado quando se calculava o atraso de um circuito. O atraso das conexões é responsável por mais da metade do atraso do circuito como um todo em tecnologias recentes. Terceiro, o processo de fabricação e os efeitos elétricos parasitas das tecnologias mais modernas são fatores que basicamente introduzem inúmeras regras de projeto que devem ser respeitadas para que o circuito funcione. A regularidade de conexões é cada vez mais difícil de ser obtida devido à topologia aleatória das redes de conexão em um circuito. Finalmente, o consumo de potência é fortemente afetado pela capacitância dos nodos de um circuito. Os fios criam grandes capacitâncias que são carregadas e descarregadas a cada transição do sinal. Em conclusão, a qualidade de um circuito está fortemente ligada à complexidade de conexões do mesmo e à capacidade que as ferramentas de CAD têm para lidar com o problema. É necessário um esforço de pesquisa muito significativo para diminuir o comprimento dos fios com a finalidade de reduzir os problemas relacionados à fiação do circuito. Fios mais curtos são mais rápidos, dissipam menos potência e são mais facilmente roteáveis e fabricáveis. Entre as técnicas propostas, o uso de algoritmos de síntese que visem a redução do comprimento dos fios é uma das maneiras mais efetivas para obter melhores circuitos. Os vários estágios de um fluxo de projeto podem ser agrupados em quatro passos consecutivos principais: síntese de sistema, síntese de alto nível, síntese lógica e finalmente RITA • Volume 19 • Número 1 • 2012 29 Posicionamento de Circuitos 3D Considerando o Planejamento de 3D-Vias síntese física. A síntese física, responsável pela transição de uma descrição do circuito em nível lógico para o leiaute, é uma tarefa bastante complexa e sensível à complexidade da fiação. Ela é composta de sub-tarefas como posicionamento das células e roteamento das conexões. O leitor pode se referir a [26] para obter mais informações básicas sobre estas etapas. Um dos principais objetivos da etapa de síntese física é lidar com problemas relacionados a conexões. Por esta razão, os algoritmos que a incorporam devem necessariamente objetivar a redução do tamanho das conexões. Apesar da existência de boas técnicas para redução do comprimento médio dos fios neste nível, a constante evolução tecnológica exige que estes algoritmos sejam revistos regularmente. Particularmente, atraso e potência são problemas importantes nos projetos atuais que impactam significativamente as etapas de posicionamento e roteamento. Análise de atraso e potência podem identificar componentes mais críticos do circuito que deveriam receber atenção especial dos algoritmos de síntese física e receber maior peso na minimização do comprimento dos fios. A redução do tamanho dos componentes, que costumava ser uma importante ferramenta para melhorar o atraso e a dissipação de potência de circuitos, é hoje uma fonte de novos problemas relacionados aos fios dos circuitos. Hoje em dia os componentes têm dimensões próximas a dimensões atômicas, indicando que a possibilidade de se continuar reduzindo seus tamanhos seja cada vez menor; a limitação física exige que novas alternativas sejam buscadas para seguir o progresso da indústria de semicondutores. Recentemente, a tecnologia de fabricação de circuitos 3D foi proposta. Mais detalhes sobre esta tecnologia são apresentados na próxima sessão. Ela aparece como uma possível solução para a estrutura de fios de um circuito. É esperado que arranjando os elementos de um circuito em 3D obtenham-se fios mais curtos. De fato, trabalhos de pesquisa recentes como [24], [5], [29], [18] e muitos outros demonstram que circuitos 3D podem de fato levar a redução do tamanho dos fios. Também é demonstrado que a melhora na fiação é proporcional ao tamanho do circuito [19]. Os trabalhos [2] e [3] especificam o existente interesse da indústria e da academia nesta tecnologia. A possibilidade de desenvolvemento de projetos de microeletrônica em 3D abre um imenso espaço de pesquisa na área de ferramentas de CAD, principalmente no nível de síntese física. Novos algoritmos devem ser criados para lidar com o arranjo 3D dos elementos e tirar o máximo de benefício desta tecnologia enquanto consideram limitações e restrições da mesma. Hoje em dia, a pesquisa neste campo ainda está no seu início. Considere o problema de posicionamento 3D, por exemplo. A riqueza e variedade de técnicas que levaram a uma melhora expressiva dos algoritmos de posicionamento através de décadas devem também propiciar uma maturidade similar para a área de posicionamento 3D. Este artigo trata-se do resumo da tese de doutorado [14] e diversos detalhes e resulta30 RITA • Volume 19 • Número 1 • 2012 Posicionamento de Circuitos 3D Considerando o Planejamento de 3D-Vias dos omitidos no artigo estão detalhados na tese. Apresentam-se algoritmos para posicionamento 3D visando à redução do tamanho dos fios e considerando elementos críticos (portas lógicas e fios). Exploram-se métodos para realizar posicionamento de células em circuitos 3D considerando problemas relacionados às conexões verticais (conhecidas como 3D-Vias) enquanto procura-se obter um tamanho reduzido de conexões aproveitando o arranjo 3D dos elementos. Um volume significativo de novos métodos e contribuições são apresentados neste texto, sendo totalmente validados numa ferramenta de posicionamento chamada Z-Place. 2 Circuitos 3D Como Um Novo Paradigma de Projeto Um circuito 3D pode ser definido como um chip VLSI com camadas ativas empilhadas chamadas de tiers. A figura 1 demonstra uma visão didática de um circuito 3D composto por camadas ativas e níveis de metal. A comunicação entre duas camadas adjacentes é dada por uma via especial chamada de 3D-Via. Active Area (transistors) Metal Layers Active Area (transistors) Active Area (transistors) Metal Layers Metal Layers Figura 1. Uma visão didática de um circuito 3D composto por camadas ativas e de metal 2.1 Dados de tecnologia de circuitos 3D Esta sessão resume algums dados tecnológicos obtidos através de vários artigos publicados na literatura, especialmente [29], [20], [27], [24] e [25]. As 3D-Vias são classificadas de acordo com as seguintes características: RITA • Volume 19 • Número 1 • 2012 31 Posicionamento de Circuitos 3D Considerando o Planejamento de 3D-Vias • A estratégia usada para integrar os tiers conectados por uma dada 3D-Via que pode ser face-to-face (as camadas do circuito são postas frente a frente sendo a conexão entre elas efetuada por apenas um contato entre o nível superior de metal de cada camada), face-to-back (a parte superior da primeira camada é exposta à parte inferior da segunda camada, fazendo com que a conexão entre elas necessite perfurar área ativa) ou backto-back (a área ativa de ambas camadas precisa ser perfurada). A figura 2 ilustra o efeito da estratégia de integração no particionamento das células. • A distância entre dois tiers adjacentes (conhecida também como espaçamento entre tiers). • O espaçamento mínimo requirido entre 3D-Vias colocadas lado a lado. • O fato de alguns tipos de 3D-Vias ocuparem área ativa e outros tipos não.	3d printing;algorithm;computer-aided design;dr-dos;dia;em intermediate language;electrical connection;em (typography);fire emblem: path of radiance;input/output;lo que tú quieras oír;multimedia exchange network over satellite;non-uniform memory access;numerical aperture;operations research;pitch (music);power-on reset;seja online;tetris;transistor;unified model;very-large-scale integration;via (electronics)	Renato Fernandes Hentschke;Marcelo de Oliveira Johann;Ricardo Augusto da Luz Reis	2012	RITA		computer science;artificial intelligence;operating system;computer vision	Vision	-107.74147363983963	21.330677440666587	100095
294be267d3dd66b110ac806488e93e149e1fc726	usos del smartphone en actividades académicas realizadas por estudiantes de licenciatura del área computacional de la uabc		Resumen. Se presenta un estudio sobre los usos educativos del smartphone realizados por estudiantes de licenciatura del Área Computacional de la Universidad Autónoma de Baja California (UABC). La muestra fue de 1,073 estudiantes de 30 programas de licenciatura, de los cuales, se seleccionó una muestra representativa de 73 estudiantes del Área Computacional (licenciaturas de Ciencias Computacionales, Ingeniería en Computación e Informática). Los resultados sugieren que los principales usos educativos fueron similares tanto en las licenciaturas del Área Computacional como en las otras licenciaturas. Destacan los usos de comunicación e interacción para lograr acuerdos, realizar tareas y para trabajo en equipo. En cuanto al manejo y acceso a la información, destacan las actividades de búsqueda, intercambio y consulta de contenidos educativos. Mientras que los apoyos organizacionales se enfocaron a consulta fecha/hora, manejo de contactos y recordatorios. Se encontraron diferencias en la proporción de uso, destacando el Área Computacional en la mayoría de ellas.	las vegas algorithm;linear algebra;lo que tú quieras oír;power-on reset;research institute of computer science and random systems;smartphone;unique name assumption	Sandra Macías-Maldonado;Javier Organista-Sandoval	2014	Research in Computing Science		history;performance art	Crypto	-106.7365549144252	18.40381772909694	100444
a72e858857419f134d12ff1ef8e6e2e5ca189619	um estudo comparativo da analise de curto-circuito probabilistico em ambientes paralelo e distribuido	curtos circuitos;sistemas de energia eletrica;processamento paralelo computadores;probabilidades;metodo de monte carlo;constitutional and consumerists proving garantees	Este trabalho apresenta a paralelizacao de um programa de analise de curto-circuito probabilistico utilizando o metodo de Monte Cado para sistemas de potencia. O programa, originariamente desenvolvido e implementado em computadores sequenciais, foi codificado para dois ambientes distintos de alto desempenho (paralelo e distribuido), tendo como um dos objetivos a verificacao de alguns itens importantes concernentes ao processamento paralelo, tais como: portabilidade, desempenho, escalabilidade e comunicacao. As implementacoes paralela e distribuida desta aplicacao foram feitas com dois modelos de programacao concorrente: o SP M D (Single Process Multiple Data) e o Mestre/Escravo. Os resultados foram obtidos atraves de testes em quatro sistemas eletricos da regiao Sul-Sudeste do Sistema Interligado brasileiro #R##N#Abstract	unified model	Fujio Sato	1995				NLP	-106.71001913735334	18.52392043163826	102997
a3a6540996897ff1781bcd1992e5f33d9aa3df19	protocol engineering using uml	communication protocol;semantics.;validation;distributed system;object oriented method;software engineering;simulation language;validation and verification;semantic model;formal method;operational semantics;software component;standard model;partially ordered set;software life cycle;object oriented	En depit de l’interet croissant pour les methodes formelles et leurs outils de validation et verification asso cies, le developpement des systemes repartis les ignore le plus souvent. Cela est du, d’apres les auteurs, principalement a leur non-integration dans les cycles modernes de developpement logiciel. La construction et la maintenance des systemes repartis ouverts sont pour la plupart fondes sur un developpement. L’article etudie un cadre de conception UML (unified modeling language) pour equiper le processus de developpement objet avec des outils de validation formelle et montre comment des techniques de validation classiques peuvent etre utilisees des maintenant sur des modeles UML en exploitant des informations contenues dans les diagrammes de classes et de deploiement, et en utilisant une semantique operationnelle des diagrammes d’etat. Il presente aussi comment les vues comportementales de UML, incluant les diagrammes de sequence et de collaboration, pourraient etre traitees de facon coherente en utilisant un modele semantique commun appele BDL permettant les traductions des differentes vues entre elles. BDL est un langage reactif synchrone fonde sur une semantique de vrai parallelisme. Les interactions entre objets de base sont representees en BDL par des ensembles partiellement ordonnes. Le comportement global est obtenu par composition des interactions de base. BDL ouvre de nouvelles perspectives de validation de systemes formes de composants locaux synchrones, mis en interaction asynchrone.	unified modeling language	Claude Jard;Jean-Marc Jézéquel;Alain Le Guennec;Benoît Caillaud	1999	Annales des Télécommunications	10.1007/BF03004068	electronic engineering;linguistics;mathematics;algorithm	SE	-108.8977268267668	18.260616536431296	106073
b80bc648180b270527755bc3b218998228703d17	integrando aspectos de sustentabilidade à engenharia de sistemas		Resumo. Sustentabilidade é uma das principais forças motoras de nossa sociedade. Entre as muitas iniciativas para alcançar esta meta estão as de TI, que se preocupam principalmente com o consumo responsável de recursos durante o desenvolvimento e a operação de sistemas de software. Software, no entanto, é parte de um contexto mais amplo, sistemas sócio-técnicos, cujo projeto tem uma grande influência no consumo de recursos. Uma maior compreensão do papel da sustentabilidade e a incorporação deste aspecto à engenharia de sistemas sócio-técnicos pode representar uma importante contribuição para vencer este desafio. Este artigo descreve a proposta de uma pesquisa para compreender o estado-da-prática e desenvolver métodos e técnicas que deem suporte aos diferentes aspectos de sustentabilidade durante a engenharia de sistemas.	tree-meta;unified model	Camilla Bomfim;Wesley Nunes;Leticia Duboc;Carina Frota Alves;Xavier Franch;Renata S. S. Guizzardi	2013			performance art;philosophy	Vision	-106.7675503323611	18.33283131946728	107279
b30f5b49c1a4d975938de08d322857e435c60303	modellierung und visualisierung digitaler pflanzen: methoden und software		The extraction of the realistic geometry of a plant enables it’s use not only for the scientific study of plant growth, but for teaching in botany and above all for computer-assisted garden & landscape architecture. Moreover the spatial arrangement of plants and their development over time can be visualised. A variety of different methods for modelling such natural looking plants exist and powerful software is available. 1 Einführung und Problemstellung In der Computergrafik werden schon seit den sechziger Jahren algorithmische Beschreibungsverfahren für Pflanzen, die möglichst realistisch aussehen sollen, untersucht. Zur Erstellung der Pflanzengeometrie existieren verschiedene Ansätze. Bei einem regelbasierten Vorgehen wird eine Pflanze durch iterative Anwendung eines formalen Regelsystems auf einen Anfangszustand erzeugt, während bei einem prozeduralen Ansatz eine Pflanze durch Anwendung eines parametrisierbaren Erzeugungsalgorithmus erstellt wird. Ziel solcher Pflanzenmodellierung ist, einerseits die Erstellung von digitalen Pflanzen für die virtuelle Welt (Filme, Computerspiele, etc.) zu ermöglichen und andererseits Computer-gestützte Gartenund Landschaftsplanung zu betreiben, insbesondere da längst auch leistungsfähige Software verfügbar ist. 2 Modellierungsmethoden 2.1 Botanische Gesetzmäßigkeiten Eines der Ziele der Pflanzenmorphologie ist, in der Formenvielfalt der Pflanzen bestimmte Grundmuster zu erkennen und eine Klassifikation anhand charakteristischer Merkmale zu erstellen. Hilfreich für die Pflanzenmodellierung sind alle Regeln, die sich in irgendeiner Form algorithmisch umsetzen lassen, um die Wuchsform der Pflanzenachse, die Verzweigungsart bzw. Seitensprossbildung, die Formen von Blättern, Blüten und Früchten zu beschreiben. Ausführliche Darstellungen solcher Gesetzmäßigkeiten finden sich in [De94] und [Ka94].	eine and zwei;iterative method;unified model;volume rendering	Georg Ohmayer	2008			scientific study;art;performance art		-108.08246162203731	30.674782614427784	107600
4bc14e1db05198969d2595eacbf9e444169aeb78	expressando atributos não-funcionais em workflows científicos		In this paper we present OSC, a scientific workflow specification language based on software architecture principles. In contrast with other approaches, OSC employs connectors as first-class constructs. In this way, we leverage reusability and compositionality in the workflow modeling process, specially in the configuration of mechanisms that manage non-functional attributes. Resumo. Este artigo apresenta OSC, uma linguagem de especificação de workflows cientı́ficos baseada em princı́pios de arquitetura de software. Em contraposição a outras abordagens, OSC emprega conectores como construções de primeira classe. Desse modo, propicia-se uma maior capacidade de reuso e composicionalidade na modelagem de workflows, particularmente nas configurações dos mecanismos que lidam com atributos não-funcionais. 1. Introdução Trabalhos recentes sobre sistemas gerenciadores de workflows cientı́ficos (SGWfCs) têm demonstrado que a comunidade cientı́fica vem se preocupando em adicionar suporte a atributos não-funcionais a esses sistemas [Ludäscher et al, 2006; Mouallem et al., 2010; Gadelha Jr. et al., 2011]. No entanto, tais atributos comumente não podem ser especificados nos modelos dos workflows, pelo fato das linguagens de especificação de workflows existentes possuı́rem expressividade em geral limitada para esse fim. Essa caracterı́stica torna a modelagem dos workflows mais simples, porém oferece menor flexibilidade na configuração dos mecanismos associados a esses atributos. Quando o sistema oferece suporte à configuração desses atributos, a mesma é concentrada na especificação das tarefas (componentes computacionais), ou é associada ao workflow como um todo. Essa caracterı́stica dificulta ou impossibilita a configuração desses mecanismos nas comunicações e coordenações empregadas entre tarefas. Em busca do aprimoramento dessa expressividade, este trabalho apresenta a linguagem OSC, uma evolução do trabalho preliminar apresentado como resumo estendido em [Medeiros e Gomes, 2011]. OSC é definida sobre a linguagem de descrição arquitetural Acme [Garlan et al., 1997]. Em contraposição a outras abordagens, OSC emprega conectores como construções de primeira classe para a modelagem tanto de tipos quanto de instâncias de interações entre tarefas quanto de regras que governam essas interações. Com essa abordagem, OSC propicia uma maior capacidade de reuso, composicionalidade e configurabilidade na modelagem de workflows, beneficiando particularmente o tratamento de atributos não-funcionais. O restante deste artigo está estruturado como se segue. A Seção 2 apresenta os atributos não-funcionais tratados neste trabalho. A Seção 3 apresenta os elementos de modelagem de OSC. A Seção 4 apresenta um exemplo de uso de OSC, que é comparada a trabalhos relacionados na Seção 5. Por fim, na Seção 6 são apresentadas as conclusões. 2. Atributos não-funcionais em workflows cientı́ficos O levantamento dos atributos não-funcionais tratados neste trabalho foi realizado a partir da análise de workflows cientı́ficos existentes (como o OrthoMCL [Fischer et al, 2011] e o ProFrager, este último apresentado na Seção 4) e de alguns dos SGWfCs (vide Seção 5) mais populares na literatura dentre aqueles que permitem a composição e configuração destes atributos em uma linguagem de modelagem (seja ela gráfica ou textual). Neste trabalho são tratados ar X iv :1 30 4. 50 99 v1 [ cs .C E ] 1 8 A pr 2 01 3 como atributos não-funcionais: (i) os atributos de qualidade relacionados a confiabilidade e rastreabilidade, (ii) o paralelismo de tarefas, e (iii) o paralelismo de dados. Nesse levantamento, o escalonamento de tarefas também se mostra um atributo não-funcional importante. Porém, como o mesmo deve ser tratado fim-a-fim em qualquer workflow e sua configuração depende de informações contidas na descrição das tarefas e conectores, escolheu-se tratá-lo diretamente no SGWfC que executa workflows OSC. A implementação de um SGWfC para OSC existe e está disponı́vel (vide Seção 4), mas seu detalhamento foge ao escopo deste trabalho. Falhas podem ocorrer em diversas partes de um workflow, podendo ser falhas tanto nas tarefas quanto em suas interações, e por diversos motivos, como falhas nas transferências de dados ou falta de bibliotecas necessárias à execução de tarefas. Essa caracterı́stica ressalta a importância da adoção de mecanismos de tolerância a falhas em SGWfCs de forma a adicionar confiabilidade às execuções. Já os mecanismos de rastreamento de proveniência de dados são utilizados por SGWfCs para uma melhor gerência dos metadados que podem ser gerados em cada execução de um workflow. Workflows podem gerar uma quantidade significativa de metadados, o que tem estimulado a comunidade cientı́fica a buscar soluções que facilitem essa gerência em SGWfCs [Gadelha Jr. et al., 2011]. Ambientes para execução paralela de software têm sido crescentemente associados a SGWfCs. Dois tipos principais de paralelismo de tarefas são em geral considerados: memória compartilhada e memória distribuı́da. Apesar de aceleradores (como GPUs) serem uma tendência, optou-se por não abordá-los inicialmente neste trabalho, pois os exemplos de workflows estudados não apresentaram nenhuma tarefa que dependesse deste tipo de paralelismo. Workflows podem ser usados para o processamento de grandes massas de dados. Os esquemas de varredura de parâmetros e MapReduce são interessantes para esse tipo de processamento quando os dados podem ser divididos para o processamento (em geral, paralelo) de conjuntos menores de dados. A varredura de parâmetros consiste em invocações repetidas de uma tarefa utilizando diferentes dados de entrada para cada invocação, podendo portanto ser usada também em simulações computacionais baseadas em métodos como o de Monte Carlo. Já no MapReduce [Dean e Ghemawat, 2008] uma função map processa um par {chave,valor} e gera um conjunto intermediário de pares {chave,valor}. Uma função reduce processa todos os pares gerados pela função map com uma mesma chave. Para gerar os pares de entrada da função reduce, após a função map é executada uma fase intermediária de ordenação das chaves e fusão dos valores regidos pela mesma chave. 3. OSC: Open Scientific Connectors OSC é definida sobre a linguagem Acme [Garlan et al., 1997]. Em Acme é possı́vel descrever estilos arquiteturais que permitem o reuso de elementos de modelagem em diferentes arquiteturas de software, bem como a definição de regras de composição desses elementos. Um estilo foi definido em Acme para a descrição dos elementos – tarefas, conectores, portas, e papéis – e regras de modelagem de workflows em OSC. Em OSC, tarefas só se comunicam por meio de conectores. Tarefas e conectores possuem interfaces denominadas, respectivamente, de portas e papéis. O modelo de um workflow em OSC envolve a ligação de portas de entrada/saı́da de tarefas a papéis de origem/destino de conectores. Ligações entre portas e papéis podem representar dependências de controle ou de dados entre tarefas. OSC considera a existência de dois tipos de usuários no processo de especificação de workflows cientı́ficos: cientistas e projetistas. O usuário cientista descreve workflows em termos de relações entre instâncias de tipos pré-definidos de tarefas e de conectores (desenvolvimento com reuso). O usuário projetista pode estender OSC definindo novos tipos de tarefas e conectores com base nos tipos pré-definidos pela linguagem (desenvolvimento para reuso). OSC predefine um conjunto de tipos básicos para tarefas, portas, conectores e papéis. Esses tipos básicos associam o elemento de modelagem abstrato no workflow com sua implementação concreta. Por exemplo, uma tarefa pode ser um executável ou um “fluxo” (um workflow encapsulado como uma tarefa), enquanto um conector pode ser um pipe de caracteres ou o transporte de um arquivo. Associado a esses tipos básicos são predefinidos também tipos especı́ficos para representar a configuração de atributos não-funcionais. A Figura 1 apresenta diagramas UML que representam os tipos básicos de tarefa e alguns de seus tipos especı́ficos. Os diagramas usam o formato proposto por Hnatkowska et al. [2005]. Optou-se por usar generalizações, restrições e powertypes da UML 2.0 para retratar esses tipos neste artigo, ao invés das especificações Acme correspondentes, devido ao espaço disponı́vel. Contudo, para ilustrar o uso de Acme alguns trechos dessas especificações são apresentados nas subseções que se seguem. (a) Tipos básicos de Tarefa (b) Extensão do tipo Executavel (c) Extensão do tipo Fluxo Figura 1. Tipos básicos e especı́ficos de tarefas em OSC A Figura 1(a) define o powertype TipoEstrutura para englobar os tipos básicos de tarefas, os quais não podem ser combinados por se tratarem de tipos disjuntos. A Figura 1(b) apresenta os tipos OSC referentes ao atributo não-funcional de paralelismo de tarefas. A Figura 1(c) apresenta os tipos OSC referentes ao atributo não-funcional de paralelismo de dados. Com exceção do tipo VarreduraDeParametros, todos os outros atributos não-funcionais representados na Figura 1 são exclusivos para o tipo Tarefa. Portas também possuem tipos para a varredura de parâmetros, de forma a permitir a configuração de bifurcações e junções. As Figuras 2 e 3 mostram diagramas UML que representam os tipos de atributos de qualidade para tarefas e conectores. Em OSC os atributos de qualidade são classificados pelo powertype TipoAtributoDeQualidade. Todos os elementos OSC estão associados a esse powertype, porém nem todos os atributos de qualidade são tratados em todos os elementos e o tratamento é distinto em cada elemento. Os parágrafos a seguir apresentam mais detalhes sobre como os atributos não-funcionais são tratados em OSC. Figura 2. Diagrama de tipos de tipos atributos de qualidade para tarefas Figura 3. Diagrama de tipos atributos de qualidade para conectores Paralelismo de tarefas. Os tipos MemoriaCompartilhada e MemoriaDistribuida (vide Figuras 1(b) e 4(a)) permitem que tarefas parale	acme;em intermediate language;em (typography);emoticon;flow-following, finite-volume icosahedral model;graphics processing unit;lo que tú quieras oír;modo (software);mapreduce;michael j. fischer;monte carlo method;multi-agent system;nem (cryptocurrency);network-attached storage;numerical aperture;power-on reset;software architecture;specification language;unified model;unified modeling language;vem	Vivian Medeiros;Antônio Tadeu A. Gomes	2013	CoRR		database;computer science	Security	-107.62831864437763	19.436627702007875	115278
99caf2c69867d135ab1fbea1180e75b32f8bdc06	klassifikation und implementierung von location based airport services zur situierten unterstützung von passagierprozessen an flughäfen		Im Folgenden werden prozessunterstützende LBAS benannt und klassifiziert. Der Betrachtung liegt ein in Zusammenarbeit mit dem Siemens Airport Center erstelltes Prozesshaus der im Flughafen-umfeld relevanten Prozesse zugrunde (vgl. Abbildung 1).		Stefan Hausmann	2009			computer vision;data mining;visualization;angiography;computer science;artificial intelligence	ML	-106.99742510083178	30.131263099241753	116858
0775d0b4bff5bb844bb7448d974dd6b4f229f050	interoperabilidade e portabilidade de documentos digitais usando oontologias		Our purpose is to enable interoperability of documents and achieve portability of digital documents through the reuse of content and format in different plausible combinations. We propose the characterization of digital documents using ontologies as a solution to the problem of lack of interoperability in the implementations of document formats. As proof of concept we consider the portability between ODF (Open Document Format) and OOXML (Office Open XML) document formats. Resumo. Nosso objetivo é possibilitar a interoperabilidade de documentos e atingir a portabilidade simples e confiável de documentos digitais através da reutilização de formatos e conteúdos, em diferentes combinações plausı́veis. Propomos a caracterização de documentos digitais usando ontologias como solução ao problema da falta de interoperabilidade nas implementações de formatos de documentos. Como prova de conceito, será considerada a portabilidade entre os formatos de documentos ODF (Open Document Format) e OOXML (Office Open XML). 1. Introdução As organizações precisam trocar informação através de documentos. Muitas vezes esses documentos são apresentados com formato e conteúdos pré-definidos, que podem ser equivalentes ou quase equivalentes entre si, porém bastantes distintos em diferentes organizações (ou em uma mesma organização em diferentes contextos históricos). Como recurso importante para gerenciar seu conhecimento de forma efetiva e preservar seu capital intelectual, as organizações precisam disponibilizar documentos independentemente do software com que foram criados. Propomos a caracterização dos formatos de documentos digitais usando ontologias para favorecer a portabilidade e superar o problema de falta de interoperabilidade de documentos nas organizações. O trabalho está organizado da seguinte forma: na Seção 2 introduzimos o problema da preservação dos documentos digitais; na Seção 3 apresentamos os conceitos fundamentais da interoperabilidade e portabilidade de documentos; na Seção 4 mostramos os principais conceitos das ontologias; na Seção 5 resumimos alguns trabalhos relacionados. Na Seção 6 explicamos nossa proposta; finalmente, na Seção 7 fazemos as considerações finais. 2. Preservação dos documentos digitais Um documento digital é um documento codificado em formato binário, acessı́vel por meio de um sistema computacional [Gouget et al. 2005]. Nosso trabalho está focado em	interoperability;numerical aperture;office open xml;ontology (information science);power-on reset;prova;single event upset;social capital;software portability;unified model	Erika Guetti Suca;Flávio S. Corrêa da Silva	2011				Security	-106.82035218135083	18.624959499432475	121600
a55ba8bbb5466d2921c05d01a0a0e51707c3c454	improvements on printing mice in labyrinths	labyrinthe;laberinto;automata estado finito;algorithme;algorithm;finite automaton;automate fini;maze;labyrinth;algoritmo	Mice with printing ability mastering every labyrinth are considered in respect to the number of states and print symbols needed. A mouse with four states and three print symbols is given, which is used for a simplified construction of a mouse with only one print symbol. Furthermore a labyrinth mastering mouse with only one state and four print symbols is given. Es werden Varianten von Druckmäusen zur Labyrinth-Bewältigung untersucht, insbesondere im Hinblick auf die Anzahl der Zustände und Drucksymbole. Eine Maus mit vier Zuständen und drei Drucksymbolen wird angegeben, die Grundlage einer einfacheren Konstruktion eine alle Labyrinthe bewältigenden Maus mit nur einem Drucksymbol ist. Ausserdem wird eine Maus mit nur einem Zustand und vier Drucksymbolen angegeben.	bus mastering;computer mouse;eine and zwei;ken's labyrinth;printing	Horst Müller	1992	Computing	10.1007/BF02320194	computer science;artificial intelligence;mathematics;finite-state machine;algorithm	OS	-107.47485668920116	30.663015448878863	127682
795f077673568cf0e974937032816841b9769cd2	"""el proyecto """"bibliotècnica a tu medida"""""""		Los recientes avances en el ámbito de las bibliotecas digitales han propiciado el rápido desarrollo de las mismas y el incremento de proyectos realizados en bibliotecas universitarias relacionados con la organización, gestión y accesibilidad a colecciones de documentos en distintos formatos a través de Internet, así como la proliferación de servicios virtuales ofrecidos a usuarios no presenciales. Parece lógico que los desarrollos más destacados se den en el ámbito de la organización y acceso a colecciones de documentos a través de navegadores web, más que en el desarrollo de servicios que deberían dar un valor añadido a estas colecciones, ya que lógicamente se debe garantizar la accesibilidad como paso previo al desarrollo de servicios que aumenten su manejabilidad y ofrezcan a los usuarios finales de estas bibliotecas herramientas propias de gestión y trabajo. En este sentido, en el marco del programa estratégico del ‘Servei de Biblioteques y Documentació’ (SBD) de la ‘Universitat Politècnica de Catalunya’ (UPC) Paideia (2000-2005), durante los años 2000 y 2001 se desarrolla el proyecto ‘Bibliotècnica: la biblioteca digital de la UPC’1 con el objetivo en una primera fase de mejorar la organización y la gestión de las colecciones de documentos digitales que se ofrecen a la comunidad universitaria para después ampliar los servicios mediante Internet.	bibliothèque de l'école des chartes;han unification;iso 8601;linear algebra;naruto shippuden: clash of ninja revolution 3;smart battery;unique name assumption	Marta García;Mònica Medina;Montserrat Mendez Planell;Jordi Prats;Oriol Rico Millán;Imma Suy;Pep Torn Poch;Joan Tur;Anna Viñas	2001				Security	-106.69480880986686	18.47797557749218	130571
20521947887f0791c030c6b883aef2eaab53f400	j2ee deployment: the jonas case study	application server;java 2 platform enterprise edition;point of view	"""RÉSUMÉ. La spécification J2EE (Java 2 platform Enterprise Edition) définit une architecture de serveur d'application Java. Jusqu'à J2EE 1.3, seuls les aspects de déploiement concernant le développeur d'applications étaient adressés. Avec J2EE 1.4, les interfaces et les étapes de déploiement ont été plus précisément spécifiées dans la spécification """"J2EE Deployment"""". JOnAS (Java Open Application Server) est une plate-forme J2EE développée au sein du consortium ObjectWeb. Les aspects déploiement sont en cours de développement. Cet article décrit les concepts liés au déploiement dans J2EE, ainsi que les problématiques levées lors de leur mise en œuvre pour JOnAS. Il n'a pas pour but de présenter un travail abouti, mais illustre le déploiement par un cas concret et ébauche une liste de besoins non encore satisfaits dans le domaine."""	application server;encore computer;jonas;java platform, enterprise edition;linear algebra;software deployment	François Exertier	2004	CoRR		computer science;operating system;database;algorithm;application server	PL	-108.16543342022992	19.285583063518764	133907
46a90a10baad9ce42d6026d4eba6b799d69e69dd	una estrategia para elevar la competitividad de las industrias de software pymes			linear algebra;unique name assumption	Raquel Anaya;Luís Fernando Londoño;Julio Ariel Hurtado Alegria	2006			systems engineering;software engineering;software;computer science	Logic	-107.685952174285	18.403609870593865	135270
eb450e2c29ffbe5dacd953ba4c4eac0b5f9d6b0b	uma abordagem de recomendação sensível ao contexto para apoio a autenticação implícita em ambientes móveis e pervasivos baseado em conhecimento comportamental do usuário	tese doutorado	Tese (doutorado) - Universidade Federal de Santa Catarina, Centro Tecnologico, Programa de Pos-graduacao em Engenharia e Gestao do Conhecimento, Florianopolis, 2013	uma acceleration architecture	João Carlos D. Lima	2013				NLP	-106.68835531616062	20.65486286107879	136843
79ef9d9e2887a02ed7d495ed4d9f4c2c17341391	avaliando com usuários um método de representação, extração e mensuração de interações sociais		RESUMO Uma tarefa explorada nas pesquisas com mídias sociais está relacionada a atribuição de significado para as interações que ocorrem entre esses usuários dentro desses sistemas. Essa tarefa é geralmente realizada a partir do cálculo e da interpretação de medições estatísticas e/ou baseadas em grafos. Em tal cenário existe a busca por um modelo descritivo de como as interações sociais estão ocorrendo dentro do sistema, em especial, de um modelo capaz de detalhar as ações realizadas pelos usuários, as mídias compartilhadas, as aplicações e os tipos de dispositivos utilizados. Neste trabalho apresentamos um método capaz de guiar a aplicação de uma técnica computacional que permite a construção de artefatos de software capazes de viabilizar a representação, mineração e mensuração de interações sociais. O método é utilizado na avaliação de interações sociais entre os usuários do Facebook. Como resultado, demonstramos a viabilidade do cômputo do comportamento coletivo de usuários de Redes Sociais Online a partir da aplicação da técnica e utilização do método. Em uma etapa seguinte do trabalho, uma avaliação é feita com usuários potenciais que expressaram sua opinião sobre as potencialidades de uso do método.	lo que tú quieras oír;numerical aperture;power-on reset;unified model	Alan Keller Gomes;Maria da Graça Campos Pimentel	2016			computer science	Security	-106.83801418940945	18.25650496516638	146346
0403e4044ce72571073d5bd526dc07389d0f0976	untersuchungen zur nutzung von teleteaching im universitären bereich	breitbandubertragung;telekonferenz;hochschule;kooperation;vorlesung;mpeg standard	Zusammenfassung: : #R##N#Universitaten und Fachhochschulen sind heute bestrebt, ihren Studenten eine immer bessere Ausbildung zu ermoglichen und die Zusammenarbeit auf dem Gebiet der Forschung und Entwicklung weiter zu intensivieren. Ein umfassendes Vorlesungsangebot, einschlieslich optionalen Lehrveranstaltungen von internationalen Wissenschaftlern wird erwartet. Um dieses Angebot weiter auszubauen und damit einen grosen Kreis von Interessenten zu erreichen, sollten wichtige Vorlesungen von anderen Hochschulen importiert und eigene Lehrveranstaltungen exportiert werden.#R##N#Die Basis fur eine derartige Zusammenarbeit bilden die breitbandigen Datennetze, an die alle Universitaten und Fachhochschulen in Deutschland angeschlossen sind. Uber dieses Kommunikationsmedium konnen unterschiedliche Daten in hoher Geschwindigkeit ubertragen werden. Der Schwerpunkt dieser Arbeit bestand in der Entwicklung und Implementierung eines Teleteaching-Systems zur Ubertragung von Vorlesungen in hoher Qualitat uber das Breit-band-#R##N#Wissenschaftsnetz in entfernte Horsale. Dazu wurden bereits bestehende Losungsansatze analysiert und auf ihren moglichen Einsatz untersucht. Das Ergebnis zeigte jedoch, dass es zum gegenwartigen Zeitpunkt kein System#R##N#gab, welches den gestellten hohen Qualitatsanforderungen genugte. Aus diesem Grund wurde auf Basis der Modellierung von realen und entfernten Horsalen ein Konzept erarbeitet, welches ein Vorlesungs-Sharing ermoglichte. Als Ergebnis der Modellierung wurden die unterschiedlichen Datentypen und die damit erforderlichen Ubertragungs-technologien ermittelt. Auf Basis dieser Analysen und der gestellten Qualitatskriterien erfolgten verschiedene Untersuchungen, um eine technische Losungsvarianten zu finden. Das aus dem Modell und den technischen Untersuchungen entwickelte neue System fur Teleteaching bietet die Moglichkeit der Ubertragung von Vorlesungen in hoher Qualitat und in Echtzeit. Die Grundlage bildet eine bidirektionale Video- und Audiokommunikation#R##N#zwischen den beiden Horsalen und damit zwischen Dozent und den Studenten im entfernten Horsaal. Die Audio- und Videodaten werden in Echtzeit als MPEG-2 komprimierter  Datenstrom im IP-Netz, speziell dem Breitband Wissenschaftsnetz ubertragen. Fur die Ubertragung der Prasentationsgrafiken und der Tafelinhalte wird dem Dozenten ein  elektronisches Whiteboard zur Verfugung gestellt. Von hier aus kann er unabhangig von jeglicher Geratetechnik seine Prasentation steuern, Annotationen darin ausfuhren und eine simulierte Tafel nutzen. Diese Daten werden mit einem Application-Sharing Tool in den entfernten Horsaal ubertragen und mittels eines zweiten Projektors an die Medienwand projiziert. Durch diese Technologie der zweikanaligen Ubertragung, verbunden mit den zugehorigen Ubertragungsqualitaten ist es gelungen, ein System zu entwickeln, welches seit 4 Jahren im praktischen Studienbetrieb eingesetzt und von den Studenten und Dozenten akzeptiert wird. Das haben entsprechende statistische Untersuchungen bewiesen. Dieses System kann fur unterschiedliche Lehrgebiete eingesetzt werden. Von den Dozenten werden keine speziellen Kenntnisse erwartet und es ist keine gesonderte Einarbeitung notwendig. Es mussen lediglich alle Prasentationsgrafiken in elektronischer Form vorliegen.		Olaf Götz	2001			art;performance art	NLP	-107.00819625851216	32.08759910428581	147726
d4940bf5e4bab89635e4ae1e1ef71b5a8b380764	un enfoque basado en valor para la refactorización software			linear algebra	Emanuel Irrazábal;Juan M. Vara;Javier Garzás;Esperanza Marcos	2010			software;software engineering;computer science	SE	-107.4971171637563	18.452941611776453	150761
19932d22a66c07c5649216c704bfe3d87cfc1740	texture features based on gabor phase	filtering;texture;filtrage;filtrado;phase;segmentation;texture features;fase;textura;lts1;discriminacion;segmentacion;discrimination;filtrage gabor	Local phase information obtained by Gabor filtering may be used for texture discrimination and image segmentation purposes. Explored are the densities of isolated zero points, discontinuities and zero contours. The phase gradient and local demodulation methods are studied as well. It is shown that most of these methods can render useful boundary and region information if simple test images are considered. However, the quality of this information degrades if the textures are disturbed by a small amount of jitter or by band-limited additive noise. Zusammenfassung. Die lokale Phaseninformation, die man bei einer Gabor-Filterung erh~ilt, kann zur Texturbestimmung und zur Bildsegmentierung verwendet werden. Dabei werden die Dichte isolierter Nullstellen, Unstetigkeiten und Konturen mit Amplitude Null gefunden. Der Phasengradient und lokale Demodulationsmethoden werden ebenfalls untersucht. Es wird gezeigt, dab die meisten dieser Methoden niitzliche Rand und Gebietsinformationen erzeugen, falls einfache Testbilder betrachtet w.erden. Die Giite dieser Information nimmt ab, wenn die Texturen dutch einen kleinen Jitter oder bandbegrenztes additives Rauschen gest~rt sind. R6sum6. L'information locale de phase obtenue par filtrage de Gabor peut ~tre utilis6e pour la discrimination de textures et la segmentation d'images. Nous explorons les densit6s des z6ros isol6s, des discontinuit6s et des contours de z6ros. Nous 6tudions 6galement les m6thodes de gradient de phase et de d6modulation locale. Nous montrons que la plupart de ces m6thodes peuvent fournir une information sur les fronti~res et les r6gions utile si des images de test simples sont consid6r6es. La qualit6 de cette information se d6grade toutefois si les textures sont perturb6es par une petite quantit6 de bruit additif de type jigue ou h bande limit6e.	additive white gaussian noise;bandlimiting;bibliothèque de l'école des chartes;gabor filter;gradient;image segmentation;linear algebra;null (sql);rand index;utility functions on indivisible goods	J. M. Hans du Buf;Peter Heitkämper	1991	Signal Processing	10.1016/0165-1684(91)90002-Z	filter;computer vision;discrimination;speech recognition;phase;texture;segmentation;computer graphics (images)	Vision	-106.80734811049373	31.612156714439468	152540
350580063b0e95581cc82bd1b21eeb0d171d04d1	are: ada rendering engine		E' ormai pratica diffusa, nello sviluppo di applicazioni web, l'utilizzo di template e di potenti template engine per automatizzare la generazione dei contenuti da presentare all'utente. Tuttavia a volte la potenza di tali engine è ottenuta mescolando logica e interfaccia, introducendo linguaggi diversi da quelli di descrizione della pagina, o addirittura inventando nuovi linguaggi dedicati. ARE (ADA Rendering Engine) è pensato per gestire l'intero flusso di creazione del contenuto HTML/XHTML dinamico, la selezione del corretto template, CSS, JavaScript e la produzione dell'output separando completamente logica e interfaccia. I templates utilizzati sono puro HTML senza parti in altri linguaggi, e possono quindi essere gestiti e visualizzati autonomamente. Il codice HTML generato è uniforme e parametrizzato. E' composto da due moduli, CORE (Common Output Rendering Engine) e ALE (ADA Layout Engine). Il primo (CORE) viene utilizzato per la generazione OO degli elementi del DOM ed è pensato per aiutare lo sviluppatore nella produzione di codice valido rispetto al DTD utilizzato. CORE genera automaticamente gli elementi del DOM in base al DTD impostato nella configurazione Il secondo (ALE) viene utilizzato come template engine per selezionare automaticamente in base ad alcuni parametri (modulo, profilo utente, tipologia del nodo, del corso, preferenze di installazione) il template HTML, i CSS e i file JavaScript appropriati. ALE permette di usare templates di default e microtemplates ricorsivi per semplificare il lavoro del grafico. I due moduli possono in ogni caso essere utilizzati indipendentemente l'uno dall'altro. E' possibile generare e renderizzare una pagina HTML utilizzando solo CORE oppure inviare gli oggetti CORE al template engine ALE che provvede a renderizzare la pagina HTML. Viceversa è possibile generare HTML senza utilizzare CORE ed inviarlo al template engine ALE CORE è alla prima release ed è già utilizzato all'interno dei progetti ADA e MAKO. Tra gli sviluppi previsti: il completamento della libreria per diverse DTD; la creazione di classi di livello superiori che automatizzino compiti ripetitivi (creazione di form, tabelle, etc). ARE è software libero, rilasciato sotto licenza GPL 2, ed è scaricabile all'indirizzo: con l'obiettivo di rendere possibile una personalizzazione dell'interfaccia a molti livelli e per esigenze diverse: dalla personalizzazione gestita autonomamente dall'utente a quella legata a contenuti specifici o ai diversi device di interazione. E' ormai pratica diffusa, nello sviluppo di applicazioni web, l'utilizzo di template e di potenti template engine per automatizzare la generazione dei contenuti da presentare all'utente. L'idea di base è quella di dividere l'interfaccia in …	ada;cascading style sheets;genera;html;javascript;layout engine;linear algebra;modulo operation;naruto shippuden: clash of ninja revolution 3;primos;rendering (computer graphics);structure of observed learning outcome;tali'zorah;unique name assumption;xhtml	Stefano Penge;Maurizio Mazzoneschi;Vito Modena	2009	IxD&A		computer graphics (images);rendering (computer graphics);computer science	DB	-108.86085290829651	31.1667088884867	153610
72d046c01a26495b6a3848b6672a81714385c1db	graphische kommunikations- und präsentationsformen für komplexe wissens- und textstrukturen: zur konzeption eines graphischen interface für ein wissensbesiertes textkondensierungssystem	und textstrukturen;graphische kommunikations;und pr	Der Bericht gibt einen Uberblick uber die Konzeption der Systemoberflache der graphisch-interaktiven Benutzerschnittstelle TOPOGRAPHIC. Ausgehend von den Anforderungen an das Interface, das komplexe, stark vernetzte Datenstrukturen visualisieren und einheitliche, aber multifunktionale Zugangsmoglichkeiten fur verschiedene Benutzertypen bereitstellen soll, werden graphische Kommunikations- und Prasentationsformen unter dem Aspekt ihrer kognitiven Adaquanz diskutiert.		Rainer Hammwöhner;Ulrich Thiel	1984		10.1007/978-3-642-46577-2_15	art;performance art	HCI	-106.71373238621612	32.103664980607014	156198
8f16dad88c0c0886fec44c5ecc43451f2cc47c4c	ein offenes hypermediasystem für graphische applikationen	graphische applikationen;ein offenes hypermediasystem	Offene Hypermediasysteme bieten neben der Unterstutzung der Organisation und Manipulation heterogener Datenobjekte wie Text, Rasterbilder, Vektorbilder, Video, Animation und Ton vor allen Dingen eine leichte Erweiterbarkeit auf neue Datenobjekte und Tools. In dieser Arbeit wird eine Basisarchitektur fur derartige Systeme vorgestellt. Ausgehend von einer Anforderungsdefinition an diese Systeme werden die Aufgabenbereiche in einem offenen Hypermediasystem definiert und in eine konzeptionelle Architektur eingeordnet. Auf der Basis dieser Architektur werden die einzelnen Komponenten (Massenspeicherverwaltung, Objektverwaltung, Sitzungsverwaltung und Prasentations-/Interaktionsverwaltung) sowie die Beschreibungselemente (Informationsobjekte, Funktionen, Bereiche, Bindungen, Ereignisse und Aktionen) eines offenen Hypermediasystems fur graphische Applikationen definiert und deren Interaktion beschrieben. Anschliesend wird eine Basisrealisierung des vorgestellten Konzeptes, das HyperPicture-Toolkit, vorgestellt.		Thomas Kirste	1991		10.1007/978-3-642-77060-9_51	animation;art;performance art	Vision	-107.09231449950464	31.991473566031583	157511
509fbc045ccc63f67e8c4b190b86481ed58d3dee	suppressing the spread of email malcode using short-term message recall	damage control;computer viruses;mobile code;behavior analysis	Outbreaks of computer viruses and worms have established a pressing need for developing proactive antivirus solutions. A proactive antivirus solution is one that reliably and accurately detects novel malicious mobile code and one that either prevents damage or recovers systems from the damage that such code inflicts. Research has indicated that behavioral analysis, though provably imprecise, can feasibly predict whether novel behavior poses a threat. Nevertheless, even the most reliable detection methods can conceivably misclassify malicious code or deem it harmful only after substantial damage has taken place. The study of damage control and recovery mechanisms is, therefore, clearly essential to the development of better proactive systems. Earlier work has demonstrated that undoing the damage of malicious code is possible with an appropriate behavior monitoring and recording mechanism. However, it remains that even if a system is recovered, the virulent code may have already propagated to other systems, some of which may not be well-equipped in terms of proactive defenses. Curbing the propagation of undesired code once it has left the boundaries of a system is a hard problem and one that has not received much attention. This work focuses on a specific instance of this difficult problem: viruses and worms that spread by email. In this paper, we explore how advantageous it would be to have a short-term email undo mechanism whose purpose is to recall infected messages. Simulation results demonstrate that such ability can substantially curb the damage of email viruses on a global scale. The results are encouraging because they only assume technology that is either readily available or that is otherwise clearly practical today Les épidémies de virus et vers informatiques ont rendu nécessaire, de manière urgent, le développement de solutions antivirales proactives. Une solution antivirale proactive consiste à détecter de manière précise et fiable des codes malicieux nouveaux mais elle doit également permettre de prévenir et de réparer les dommages que de tels codes occasionnent. Les recherches ont montré que l’analyse antivirale comportementale, bien qu’il ait été prouvé qu’elle était imprécise, pouvait quelquefois déterminer si un nouveau comportement, pour un code, constitue une menace ou non. Néanmoins, même la plus fiable des méthodes de détection peut échouer ou bien décider de son caractère malveillant seulement après que des dommages aient été constatés. L’étude du contrôle des dommages au système et/ou aux données, ainsi que des mécanismes de restauration, est, par conséquent, essentielle pour le développement de meilleurs systèmes proactifs. Des travaux antérieurs ont prouvé que la réparation des dommages provoqués par des codes mailcieux est possible à l’aide d’une surveillance comportementale appropriée et des mécanismes de sauvegarde adéquats. Toutefois, certains problèmes subsistent. Si un système est réparé, le code virulent peut s’être déjà propagé vers d’autres systèmes, certains d’entre eux pouvant ne pas être convenablement équipés en terme de défense proactive. Juguler la propagation de codes non désirés une fois qu’il s’est propagé au-delà des limites d’un système, est un problème difficile, qui n’a, à ce jour, pas fait l’objet de beaucoup d’études. Cet article considère une instance particulière de ce problème difficile : les virus et les vers se propageant par l’intermèdiaire du courrier électronique. Nous étudions ici les avantages de disposer de mécanismes de réparation à court terme des messages éléctroniques, dont le but est de rappeler les-dits messages lorsqu’ils ont été déclarés infectés. Les simulations ont démontré qu’une telle capacité peut substantiellement limiter les dommages des virus et vers de courrier électronique, sur une large échelle. Ces résultats sont encourageants car ils reposent seulement sur des technologies qui sont disponibles très rapidement de nos jours ou faciles à mettre en pratique. Kalamita počítačových virů a červů vyvolala naléhavou potřebu vývoje proaktivních antivirových řešení. Proaktivní antivirové řešení je takové, které spolehlivě a přesně detekuje nový škodlivý mobilní kód a také předchází škodám, nebo obnovuje poškozené systémy, jež takový kód způsobil. Výzkum dal najevo, že analýza chování, byt’ pravděpodobně nepřesná, může vhodně předvídat nová chování předložených hrozeb. Nicméně pokud nejspolehlivější detekční metody mohou myslitelně špatně klasifikovat škodlivý kód, nebo pokládat ho za škodlivý jen po podstatném poškození, má svůj význam. Studium řízení poškození a mechanismu obnovy je proto zřejmě podstatné pro vývoj lepších proaktivních systémů. Předchozí práce ukázala, že zkáza poškození škodlivého kódu je možná pomocí vhodného chování monitorovacího a záznamového mechanismu. Nicméně zbývá to, že i když je systém obnoven, může být virulentní kód již rozšířen do dalších systémů, z nichž některé nemusejí být dobře vybaveny ve výrazech proaktivní obrany. Omezování šíření nežádoucího kódu jednou ponechaného v hranicích systému je těžký problém a často mu není věnováno dost pozornosti. Tato práce se zaměřuje na specifickou situaci tohoto obtížného problému: viry a červi šířené emailem. V tomto článku zjištujeme, jaké výhody by mohl mít krátkodobý zpětný mechanismus jehož účelem je zpětné volání infikovaných zpráv. Výsledky simulace ukázaly že taková schopnost může podstatně omezit škody emailových virů v globálním měřítku. Výsledky jsou povzbudivé, protože předpokládají pouze technologie bud’ snadno dostupné, nebo takové, které jsou jinak zřetelně účinné dnes. Tietokonevirusten ja -matojen nopea leviäminen on saanut aikaan tarpeen kehittää proaktiivisia virustentorjuntamenetelmiä. Proaktiivinen torjuntamenetelmä on sellainen, joka luotettavasti ja tarkasti tunnistaa uusia haitallisia koodeja ja joko estää vahinkoa tai palauttaa järjestelmiä haitallisen koodin aiheuttamalta tuholta. Tutkimukset ovat osoittaneet, että ohjelman käyttäytymisen tarkkaileminen, joskin todistettavasti epätarkkaa, pystyy ennustamaan, aiheuttaako uudenlainen käyttäytyminen vahinkoa. Kaikesta huolimatta jopa kaikkein luotettavimmat tunnistamismenetelmät voivat luokitella väärin haitallista koodia tai pystyvät todentamaan koodin haitallisuuden vasta merkittävän tuhon jälkeen. Tutkimusta vahingon säätelystä ja palautusmekanismeista siis tarvitaan, jotta parempia proaktiivisia järjestelmiä voidaan kehittää. Aikaisempi tutkimusalueen työ on osoittanut, että palautuminen haitallisen koodin aiheuttamista vahingoista on mahdollista sopivien käyttäytymisen tarkkailun sekä tallennusmenetelmien avulla. Vaikka järjestelmä pystyttäisiin palauttamaan, haitallinen koodi on saattanut levitä muihin järjestelmiin. Osa näistä muista järjestelmistä ei ehkä sisällä proaktiivista suojausta. Järjestelmän rajojen ulkopuolelle levinneen haitallisen koodin leviämisen rajoittaminen on vaikea ongelma, joka ei ole saanut paljon huomiota. Tämä tutkimus keskittyy kuvatun ongelman erityisalueeseen: viruksiin ja matoihin, jotka leviävät sähköpostin avulla. Tutkimme, miten hyödyllinen olisi sellainen lyhytaikainen sähköpostin lähetyksen peruminen, jonka tarkoituksena on vetää pois saastuneet viestit. Simulaatiolla saadut tulokset osoittavat, että menetelmä voi merkittävästi rajoittaa sähköpostivirusten aiheuttamaa maailmanlaajuista ongelmaa. Tulokset ovat rohkaisevia, koska teknologia on jo joko saatavilla tai muuten käytännöllistä. Le epidemie di virus e worm richiedono urgentemente lo sviluppo di soluzioni antivirus proattive. Una soluzione proattiva e’ una soluzione che puo’ individuare accuratamente e affidabilmente nuovi tipi di codice maligno, e che puo’ prevenire o curare i danni che simili codici infliggono. La ricerca ha indicato che l’analisi comportamentale, anche se dimostratamente imprecisa, puo’ predire se dei nuovi tipi di comportamento sono una minaccia. Cionondimeno, anche i piu’ affidabili metodi di individuazione possono misclassificare il codice, o individuarlo come maligno solo dopo che ha compiuto dei danni. Lo studio di meccanismi di controllo dei danni e di ricupero è quindi chiaramente essenziale per lo sviluppo di sistemi proattivi migliori. Precedenti lavori hanno dimostrato che riparare i danni del codice maligno è possibile con un appropriato controllo e registrazione dei loro comportamtni. Tuttavia, anche se un sistema viene recuperato, il codice virale potrebbe essersi gia’ propagato ad altri sistemi, che potrebbero essere meno protetti da difese proattive. Rallentare la propagazione di codice virale che ha gia’ lasciato un sistema e’ un problema difficile e poco studiato. Questo lavoro si concentra su una specifica istanza del problema: virus e worm che si diffondono via e-mail. In questo articolo esploriamo i vantaggi che si otterrebbero creando un meccanismo di richiamo, a breve termine, per le email, al solo scopo di richiamare eventuali messaggi infetti. Risultati di simulazione dimostrano che questa tecnologia puo’ diminuire fortemente il danno dei virus via posta elettronica su scala globale. I risultati sono particolarmente incoraggianti perche’ utilizzano solo tecnologie che sono gia’ disponibili o realizzabili con facilita’ al giorno d’oggi. Ausbrüche von Computerviren und Würmern haben den Bedarf an proaktiven Antivirus Lösungen signifikant gesteigert. Im Sinne dieses Papiers wird eine proaktive Antivirus Lösung als System verstanden, welches zuverlässig und akkurat neuen mobilen maliziösen Code erkennt und entweder Schaden verhindert oder Systeme, die durch den entsprechenden Schadcode angegriffen worden sind, wieder herstellt. Untersuchungen haben ergeben, daß Verhaltensanalyse, losgelöst von dem Vorwurf ungenau zu sein, brauchbare Ergebnisse liefert, wenn es gilt, ein neuartiges Verhalten hinsichtlich seiner (potentiellen) Gefährlichkeit einzustu-fen. Nichtsdestotrotz können selbst die zuverlässigsten Erkennungsmethoden maliziösen Code beizeiten falsch klassif	antivirus software;bibliothèque de l'école des chartes;code mobility;computer virus;cranial electrotherapy stimulation;déjà vu;eine and zwei;email;flux limiter;large eddy simulation;linear algebra;malware;menace;naruto shippuden: clash of ninja revolution 3;numerical aperture;point of interest;proactive parallel suite;scala;software propagation;structure of observed learning outcome;threat (computer);undo;unique name assumption;breve;nouveau	Ibrahim K. El-Far;Richard Ford;Attila Ondi;Manan Pancholi	2005	Journal in Computer Virology	10.1007/s11416-005-0003-8	simulation;computer science;operating system;computer security;computer virus	Security	-106.80106248107529	19.81126783082751	157706
0a01649fd3e2332d57c9d453bca8834a2881c6a0	interaktive 3d segmentierung auf basis einer optimierten oberflächeninterpolation mittels radialer basisfunktionen		In dieser Arbeit wird ein Verfahren zur effizienten und interaktiven Erstellung einer 3D Segmentierung vorgestellt. Dafur wird die Oberflache des zu segmentierenden Bereiches auf Basis manuell gezeichneter Konturen mittels radialer Basisfunktionen interpoliert. Um die Auswirkung zusatzlicher Konturen auf die interpolierten Oberflache in wenigen Sekunden visualisieren zu konnen, wurden gezielte Masnahmen getroffen, um den Algorithmus auf Geschwindigkeit zu optimieren. Eine Evaluation auf Basis von Expertensegmentierungen ergab, dass sich der Aufwand fur die Erstellung einer manuellen Segmentierung deutlich reduzieren lies, wobei gleichzeitig eine hohe Genauigkeit erzielt werden konnte.		Andreas Fetzer;Hans-Peter Meinzer;Tobias Heimann	2012		10.1007/978-3-642-28502-8_33	computer graphics (images);art	HCI	-107.2114044574637	31.774565115770923	161069
af482949a29cb79f65774791ccfd238c7ac5e71c	govmobile: uma proposta para disponibilizar dados abertos georreferenciados para governo eletrônico		The advancement of social media technologies has enhanced the Brazilian experiences in e-gov, developing the government processes and providing the creation of new policies to connect the citizens with govern. In Brazil, one of these initiatives is the Brazilian Open Data Policy that aims to make information (and its use) by citizens feasible. In parallel, the growth of mobile devices popularity and wireless networks had allowed the access to the Brazilian government open data in a faster and easier way as well as extending the social web potential. In this context, this paper discusses a preliminary proposal to provide open georeferenced data called GovMobile. Additionally, it is shown a system composed by a Web application (where governmental institutions publish open georeferenced data) and also a mobile application (where these data are available). The contribution is preliminarily to show how the access to information by society is being planned by the government and motivate the publication of open data by government organizations to integrate citizens in the process of political and social change. RESUMO O avanço nas tecnologias de mídia social tem possibilitado experiências em e-gov, trazendo melhoria aos processos internos do governo bem como à criação de novas políticas de interação entre governo e sociedade. No Brasil, uma das políticas que vem ganhando força é a Política Brasileira de Dados Abertos que tem por objetivo ampliar o acesso e a utilização dos dados públicos pelos cidadãos. Diante de um cenário de crescente popularização dos dispositivos móveis e modernização das redes sem fio, a utilização destes dispositivos se torna um importante canal de acesso aos dados abertos governamentais a fim de aumentar rapidez e facilidade no acesso a estas informações, além de estender o potencial da Web social. Neste contexto, este artigo discute uma proposta preliminar para disponibilizar dados abertos georreferenciados, denominada GovMobile. Além disso, um sistema composto por uma aplicação Web (onde as organizações governamentais publicarão dados abertos georreferenciados) e por uma aplicação móvel (onde estes dados estarão disponíveis) é apresentado. A contribuição ainda inicial desta pesquisa é mostrar como o acesso à informação por parte da sociedade vem sendo planejado pelo governo e motivar a publicação de dados abertos pelas organizações governamentais visando integrar o cidadão no processo de mudança política e social.	e-government;fire emblem: path of radiance;flow-following, finite-volume icosahedral model;freedom of information laws by country;mobile app;mobile device;power-on reset;social media;unified model;vem;web application	Evandro Rocha;Miriam Chaves;Rodrigo Magalhães dos Santos;Sérgio Assis Rodrigues;Jano Moreira de Souza	2013			operating system;business	DB	-106.91236332029315	19.586725781983514	163116
07a7ca2ea7883cd9cb53ac1cc51c71fb5cc31b16	mixing scrum-psp: combinación de scrum y psp para mejorar la calidad del proceso de software		El desarrollo de software en un entorno real es muy agresivo con respecto a los tiempos de entrega, presupuesto y el contrato con el cliente. La combinación de métodos ágiles y procesos disciplinados es una opción prometedora que apoya al desarrollo de software a mejorar la calidad. En este trabajo se presenta un modelo de integración de procesos combinando Scrum y PSP (Personal Software Process) para mejorar el proceso de software, teniendo en cuenta las características primordiales de cada marco de trabajo. El resultado de esta investigación es un intento para adoptar la agilidad y disciplina de los procesos en diferentes ambientes de trabajo.	bibliothèque de l'école des chartes;linear algebra;naruto shippuden: clash of ninja revolution 3;personal software process;scrum (software development);unique name assumption	Mauricio Leonardo Urbina Delgadillo;María Antonieta Abud-Figueroa;Silvestre Gustavo Peláez-Camarena;Giner Alor-Hernández;Alma Ivonne Sánchez García	2016	Research in Computing Science		cartography	Logic	-107.54616301775773	18.447564783561646	163755
1b406c0bd37aa82879f483641f90c3359d75a955	hurome: entorno para modelado de coreografías y modernización de código para un robot humanoide	language robobasic;info eu repo semantics conferenceobject;codigo robobasic;hurome robot humanoide de modelado para el medio ambiente;robobasic code;lenguajes y sistemas informaticos;hurome humanoid robot modeling environment;lenguaje robobasic;robot humanoide robonova;robonova humanoid robot	"""Resumen El entorno de modelado HuRoME (Humanoid Robot Modeling Environment) integra un conjunto de herramientas diseñadas para facilitar el modelado de coreografías y la modernización del software existente para el robot humanoide Robonova (www.robonova.com). HuRoME permite a los numerosos usuarios de Robonova: (1) modelar gráficamente y validar formalmente las secuencias de movimientos del robot (coreografías), (2) generar automáticamente la implementación asociada a cada coreografía en el lenguaje RoboBASIC, y (3) modernizar y reutilizar el software ya existente, permitiendo la obtención de los modelos equivalentes a cualquier programa RoboBASIC. El Desarrollo de Software Dirigido por Modelos (DSDM) [5] permite elevar el nivel de abstracción con el que se diseña el software y ofrece mecanismos para automatizar, en gran medida, el proceso de generación de código. Este paradigma gira entorno a la definición y el uso sistemático de modelos y transformaciones de modelos a lo largo de todo el ciclo de vida del software. El DSDM se ha aplicado de forma exitosa en muchos dominios, algunos relativamente cercanos a la robótica como los sistemas empotrados [4] o las redes de sensores [6]. Sin embargo, las referencias al DSDM en el ámbito de la robótica son recientes y aún escasas [1-2], aunque algunos autores identifican este nuevo paradigma como una de las claves para solventar la """" falta crónica de normalización, interoperabilidad y reutilización del software en robótica """" [3]. El entorno HuRoME, que se presenta en este trabajo, ofrece una primera aproximación al desarrollo de software para robótica utilizando un enfoque dirigido por modelos. Como se detalla en las siguientes secciones, HuRoME no sólo facilita la generación automática de código a partir de modelos, sino también el proceso inverso, permitiendo la obtención de modelos a partir de programas ya existentes (legacy code), facilitando así su tratamiento (depuración, extensión, reutilización, análisis, simulación, etc.) con un nivel de abstracción mayor que el que proporciona el código fuente. En su versión actual, HuRoME sólo da soporte a la plataforma Robonova (robot humanoide de bajo coste utilizando fundamen-talmente con fines docentes), si bien se están barajando posibles extensiones para dar soporte a otras plataformas robóticas. 2. Descripción de HuRoME A continuación se describen las tres herramientas que conforman HuRoME (ver Figura 1), todas ellas desarrolladas utilizando las facilidades para DSDM que ofrece la plataforma Eclipse. En primer lugar, la Sección 2.1 describe la herramienta gráfica de modelado y validación de coreografías, implementada con GMF (Graphical Modeling …"""	bibliothèque de l'école des chartes;dynamic systems development method;eclipse;el-fish;graphical modeling framework;graphical user interface;humanoid robot;legacy code;linear algebra;naruto shippuden: clash of ninja revolution 3;primer;power-on reset;unique name assumption;ver (command)	Juan F. Inglés-Romero;Cristina Vicente-Chicote;Diego Alonso	2010			simulation;geography;artificial intelligence;cartography	ML	-107.37959896641546	18.272900901270024	165969
6cacf05ab56382c80c906ec6ef03884e146bcdae	proposta de utilização de ferramentas cae no planejamento do processo de moldagem por injeção de termoplásticos em moldes de estereolitografia	tese doutorado	Tese (doutorado) - Universidade Federal de Santa Catarina, Centro Tecnologico. Programa de Pos-Graduacao em Engenharia Mecânica.	power-on reset	Armando Sa Ribeiro Junior	2003				Crypto	-106.67291557195148	20.78715524784043	167610
d1917b40531d0b4f973f7923e9f8864f131203c2	desenvolvimento e aplicação de um modelo para relacionar diferentes sistemas de informação na àrea da saúde	tese doutorado	Tese (doutorado) - Universidade Federal de Santa Catarina, Centro Tecnologico. Programa de Pos-graduacao em Engenharia de Producao.	numerical aperture;unified model	Ivana Teresinha Corrêa de Oliveira	2007				Crypto	-106.67935011588783	20.35902482083685	171004
407a4ac7ff765f756d9cf9ebfeabbae43b71ffe1	requisitos arquiteturais como base para a qualidade de ambientes de engenharia de software		Resumo — Nos ultimos anos, uma proliferacao de ferramentas e Ambientes de Engenharia de Software (AESs) tem sido observada, com impactos positivos na producao de software. Apesar do reuso ser foco de muitas pesquisas em desenvolvimento de software, ferramentas e ambientes tem sido, muitas vezes, construidos individualmente, sem atencao ao reuso dos esforcos de desenvolvimento desses sistemas. Alem disso, as arquiteturas desses sistemas, bem como as arquiteturas de referencia, nao tem sido largamente investigadas, o que pode estar influenciando nas dificuldades de evolucao e integracao que esses sistemas tem sofrido. Dessa forma, este trabalho tem o objetivo de apresentar uma investigacao que resultou em um conjunto de requisitos arquiteturais de AESs e que deu origem ao estabelecimento de uma arquitetura de referencia para esses ambientes. Resultados parciais tem mostrado a viabilidade dessa arquitetura como base para o desenvolvimento incremental e evolutivo de AESs, evidenciando que os requisitos estabelecidos tem sido atendidos. Palavras chave — Ambiente de engenharia de software (software engineering environment), arquitetura de referencia (reference architecture), requisito arquitetural (architectural requirement).		Elisa Yumi Nakagawa;José Carlos Maldonado	2008			operating system;systems engineering;software;computer science	Logic	-107.8575699499486	18.921046105208642	180222
bf625929a9ffe1879798555446da00f62127b495	walkthrough vs. videokonfrontation - vergleich zweier methoden zur formativen software-evaluation	poster	Für die iterative Software-Entwicklung spielt die formative Evaluation unter Einbeziehung (potenzieller) Benutzer eine wichtige Rolle. Um gute Erfolge zu erzielen und gleichzeitig den Kostenaufwand der formativen Evaluation zu rechtfertigen, ist es notwendig, möglichst effektive und effiziente Verfahren zu entwickeln bzw. zu identifizieren. Vor diesem Hintergrund wurden im Rahmen eines Kooperationsprojektes zwischen der Universität Osnabrück und einem E-Commerce-Anbieter die Methoden Videokonfrontation und Walkthrough bei der Evaluation einer Website angewendet und verglichen. Sowohl Videokonfrontation als auch Walkthrough können eingesetzt werden, um Benutzbarkeitsprobleme in interaktiven Systemen zu identifizieren. Teilnehmer sind bei beiden Methoden potenzielle Benutzer des Systems. Der Vergleich wurde als Evaluationsstudie angelegt; im Vordergrund stand die Bewertung unter realistischen Bedingungen und nicht die Untersuchung der Wirkweisen der Methoden.	cognitive walkthrough;e-commerce;eine and zwei;iterative method;unified model;vhf omnidirectional range	Meike Döhl	2001				OS	-107.11204278990026	32.151542290971555	181138
e3138a19f09f514af7e4de11ccae40cc0d0745f6	uso de sgbds nosql na gerência da proveniência distribuída em workflows científicos		Resumo. Um fator fundamental na gerência de experimentos modelados como workflows científicos são seus dados de proveniência. Esses dados basicamente são usados para garantir a reprodutibilidade, porém nos últimos anos eles também vêm sendo usados para tarefas de monitoramento e escalonamento de atividades. Como essas tarefas demandam consultas em tempo real, conforme a quantidade de dados de proveniência aumenta, mecanismos eficazes para armazenamento e consulta se fazem necessários. Uma das opções mais comuns é utilizar os SGBDs relacionais para gerenciar a proveniência, dada a tradição da tecnologia. Porém, novas tecnologias como os SGBDs NoSQL tem ganhado bastante atenção nos últimos anos e podem ser de grande valia nesse cenário, principalmente em ambientes distribuídos onde escalabilidade é essencial. Este artigo realiza um estudo comparativo entre SGBDs relacionais e um SGBD NoSQL (Cassandra) no que tange a gerência dos dados de proveniência. Apresentamos um estudo com um workflow real de bioinformática usando a máquina de workflows para nuvens SciCumulus.	em intermediate language;nosql;numerical aperture;unified model	Carlos Filipe Júnior;Guilherme Ferreira;Daniel de Oliveira	2014			nosql;database;workflow;computer science	Security	-107.35888721124267	19.435340683197133	182510
78ba927955a8dbf850f6124423b15b185fe5d6b3	die anwendung der b-spline-approximation in computer graphics	computer graphic	Seit den ersten Anfangen auf dem Gebiet Computer Graphics in den sechziger Jahren bemuht man sich, grafische Sichtgerate (graphic displays) im computergestutzten Entwurf (CAD) einzusetzen. Besondere Anstrengungen wurden unternommen um dreidimensionale Gebilde darstellen, interaktiv entwerfen und verandern zu konnen. Die Grundlage fur die notwendigen mathematischen Verfahren wurde von Coons 1964 mit der Veroffentlichung seiner Arbeit „Surfaces for computer-aided design of space figures“ /1/ geliefert. Flachen werden danach in Parameterform definiert. Durch Zusammensetzen von Teilflachen — auch patches oder Pflaster genannt — erhalt man grossere Flachen. Die Darstellung der Flachen erfolgt durch Projektion einiger ihrer Parameterlinien auf den Bildschirm.	approximation;b-spline;computer graphics	Wolfgang Straßer	1974		10.1007/3-540-07141-5_264	graphics software	Visualization	-108.1384265581716	31.091912174290357	186118
0d266860ee22ad63404601a522327978beb3fbfa	um estudo empírico e analítico do impacto da governança de ti no desempenho organizacional	avaliacao de desempenho;governanca corporativa;it governance;sistemas de informacao;bovepa;investimento e mudanca tecnologica;event study;it management;tecnologia da informacao aplicabilidade nas organizacoes;survey;tecnologia da informacao aspectos economicos;organizational performance;investimentos tomada de decisao;desempenho organizacional;tese		unified model	Guilherme Lerch Lunardi	2008			organizational performance;business administration;business	Crypto	-107.80959322014834	19.88821194377088	186824
83d126ff53f111af70e03f3fd8f211ea46e11c85	pde-based image compression based on edges and optimal data	partial differential equation;optimisation;kante;bildkompression;inpainting;image compression;partielle differentialgleichung;edge;kompression;optimierungsproblem;codec;bildrekonstruktion;diffusion;bildverarbeitung;bildkante	This thesis investigates image compression with partial differential equations (PDEs) based on edges and optimal data.#R##N##R##N#It first presents a lossy compression method for cartoon-like images. Edges together with some adjacent pixel values are extracted and encoded. During decoding, information not covered by this data is reconstructed by PDE-based inpainting with homogeneous diffusion. The result is a compression codec based on perceptual meaningful image features which is able to outperform JPEG and JPEG2000.#R##N##R##N#In contrast, the second part of the thesis focuses on the optimal selection of inpainting data. The proposed methods allow to recover a general image from only 4% of all pixels almost perfectly, even with homogeneous diffusion inpainting. A simple conceptual encoding shows the potential of an optimal data selection for image compression: The results beat the quality of JPEG2000 when anisotropic diffusion is used for inpainting.#R##N##R##N#Finally, the thesis shows that the combination of the concepts allows for further improvements. #N#Die vorliegende Arbeit untersucht die Bildkompression mit partiellen Differentialgleichungen (PDEs), basierend auf Kanten und optimalen Daten.#R##N##R##N#Sie stellt zunachst ein verlustbehaftetes Kompressionsverfahren fur cartoonartige Bilder vor. Dazu werden Kanten zusammen mit einigen benachbarten Pixelwerten extrahiert und anschliesend kodiert. Wahrend der Dekodierung, werden Informationen, die durch die gespeicherten Daten nicht abgedeckt sind, mittels PDE-basiertem Inpainting mit homogenener Diffusion rekonstruiert. Das Ergebnis ist ein Kompressionscodec, der auf visuell bedeutsamen Bildmerkmalen basiert und in der Lage ist, die Qualitat von JPEG und JPEG2000 zu ubertreffen.#R##N##R##N#Im Gegensatz dazu konzentriert sich der zweite Teil der Arbeit auf die optimale Auswahl von Inpaintingdaten. Die vorgeschlagenen Methoden ermoglichen es, ein gewohnliches Bild aus nur 4%  aller Pixel nahezu perfekt wiederherzustellen, selbst mit homogenem Diffusionsinpainting. Eine einfache konzeptuelle Kodierung zeigt das Potential einer optimierten Datenauswahl auf: Die Ergebnisse ubersteigen die Qualitat von JPEG2000, sofern das Inpainting mit einem anisotropen Diffusionsprozess erfolgt. #R##N##R##N#Schlieslich zeigt die Arbeit, dass weitere Verbesserungen durch die Kombination der Konzepte erreicht werden konnen.		Markus Mainberger	2014		10.22028/D291-26608	telecommunications;computer science;electrical engineering;computer graphics (images)	ML	-107.58385849994389	31.231221288439198	187676
57ef35f3295d2f5fed51532874d6a70232b03903	estimation of arma(p, q) parameters	filtering;filtrage;filtro kalman;filtrado;filtre kalman;kalman filter;identification;estimacion parametro;identificacion;parameter estimation;estimation parametre	The need for estimating the parameters of an ARMA(p, q) process arises in many applications both in signal processing and in automatic control. Recently, we proposed an estimation procedure to get the ARMA parameters. The method is based on a 2-step approach: first the AR parameters are estimated using a transient Kalman gain, then the MA parameters are estimated by a fast filtering algorithm. This short paper shows the results of simulations conducted to evaluate this method. We study the consistency and the efficiency of the proposed estimator and we give some examples for ARMA(2, 2), ARMA(4, 2) and ARMA(4, 4) processes. Zusammenfassung. Eine Sch~itzung der Parameter eines ARMA(p, q)-Prozesses braucht man fiir zahlreiche Anwendungen der Signalverarbeitung wie der selbstt~itigen Regelung. Kiirzlich haben wir eine Sch~itzprozedur hierfiir vorgeschlagen. Sie beruht auf einem zweistufigen Ansatz: Zun~ichst werden die AR-Parameter mit Hilfe einer transienten Kalman-Verst~irkung, danach die MA-Parameter mit einem schnellen Filter-Algorithmus gesch~itzt. Die Ergebnisse von Simulationen zur Bewertung des Verfahrens werden gezeigt. Wir untersuchen die Konsistenz und die Wirksamkeit des vorgeschlagenen Sch~itzers, und wir geben einige Beispiele an fiir ARMA(2, 2)-, ARMA(4, 2)und ARMA(4, 4)-Prozesse. R~sumr. L'estimation des param/~tres d 'un module ARMA(p, q) est un probl~me qui se pose h la fois en automatique et en traitement du signal. Nous avons prrsent6 une mrthode d'estimation des param~tres d 'un module ARMA. La procrdure d'estimation s'effectue en 2 temps: tout d 'abord, l 'estimation de la partie AR est faite en utilisant un gain de Kalman transitoire. Puis, la pattie MA est estimre grace b. un algorithme de filtrage rapide. Cet article prrsente les rrsultats de tests effecturs pour 6valuer cette mrthode. Nous 6tudions la consistance et I'efficacit6 de l'estimateur propos6 et prrsentons plusieurs exemples pour des processus ARMA(2, 2), ARMA(4, 2) et ARMA(4, 4).	algorithm;automatic control;die (integrated circuit);eine and zwei;internet explorer;kalman filter;linear algebra;param;recueil des historiens des croisades;sie (file format);signal processing;simulation;triple des	Josiane Zerubia;Gérard Alengrin	1991	Signal Processing	10.1016/0165-1684(91)90028-H	filter;identification;kalman filter;econometrics;computer science;control theory;estimation theory;statistics	ML	-108.4205215624911	28.60763906416368	190345
0331afa2f5860cfe9ee66be331ecd51370cd8237	bias na representação de assunto: uma discussão de oposições binárias nos functional requirements for subject authority data (frsad)	ciencia da informacao;organizacao da informacao;tese de doutorado;organization of information;representacao do conhecimento teoria da informacao;linguagem documentaria	Como são poucos os estudos brasileiros que assumem declaradamente seus posicionamentos sobre as biases, este artigo propõe-se a apresentar uma discussão crítica sobre as biases na representação de assunto a partir dos Functional Requirements for Subject Authority Data: A Conceptual Model (FRSAD). Para esse exercício, adotamos uma postura epistêmica pós-estruturalista, pois não estamos em busca de verdades universais, mas de uma outra maneira de entender a representação de assunto e os substitutos documentais construídos pelos bibliotecários. Elegemos o método Desconstrução (OLSON, 1996) para descentralizar a oposição binária neutralidade versus interesses especiais, isto é, de um lado temos a crença do bibliotecário em evitar a interferência de seus valores morais na busca de uma suposta imparcialidade e, do outro, a necessidade deste mesmo profissional realizar julgamentos de valor visando melhor representar comunidades usuárias específicas. Os binários foram descentralizados a partir dos FRSAD ressaltando-se as relações de poder percebidas, assim como as premissas e as presunções subjacentes a elas.		Suellen Oliveira Milani	2014			library science;functional requirement;frsad;political science	Crypto	-107.11158241769782	18.775870094556083	192088
e1109fd6a590ec382f52c5a6e7f67b405a55f3a5	algorithms and data structures for interactive ray tracing on commodity hardware	nvidia geforce 8800 gtx;paper;raytracing;bilderzeugung;cuda;thesis;gpu ray tracing;ray tracing;kd tree;nvidia;algorithms;acceleration structures;computer science;3d graphics and realism;beschleunigungs strukturen;rendering	Rendering methods based on ray tracing provide high image realism, but have been historically regarded as offline only. This has changed in the past decade, due to significant advances in the construction and traversal performance of acceleration structures and the efficient use of data-parallel processing. Today, all major graphics companies offer real-time ray tracing solutions. The following work has contributed to this development with some key insights. We first address the limited support of dynamic scenes in previous work, by proposing two new parallel-friendly construction algorithms for KD-trees and BVHs. By approximating the cost function, we accelerate construction by up to an order of magnitude (especially for BVHs), at the expense of only tiny degradation to traversal performance. For the static portions of the scene, we also address the topic of creating the “perfect” acceleration structure. We develop a polynomial time non-greedy BVH construction algorithm. We then modify it to produce a new type of acceleration structure that inherits both the high performance of KD-trees and the small size of BVHs. Finally, we focus on bringing real-time ray tracing to commodity desktop computers. We develop several new KD-tree and BVH traversal algorithms specifically tailored for the GPU. With them, we show for the first time that GPU ray tracing is indeed feasible, and it can outperform CPU ray tracing by almost an order of magnitude, even on large CAD models. #N#Ray-Tracing basierte Bildsynthese-Verfahren bieten einen hohen Grad an Realismus, wurden allerdings in der Vergangenheit ausschlieslich als nicht echtzeitfahig betrachtet. Dies hat sich innerhalb des letzten Jahrzehnts geandert durch signifikante Fortschritte sowohl im Bereich der Erstellung und Traversierung von Beschleunigungs-Strukturen, als auch im effizienten Einsatz paralleler Berechnung. Heute bieten alle grosen Grafik-Firmen Echtzeit-Ray-Tracing Losungen an. Die vorliegende Dissertation behandelt Betrage zu dieser Entwicklung in mehreren Kernaspekten. Der erste Teil beschaftigt sich mit der eingeschrankten Unterstutzung von dynamischen Szenen in bisherigen Verfahren. Hierbei behandeln wir zwei zur Parallelisierung geeignete Algorithmen zur Erstellung von KD-Baumen und Bounding-Volume-Hierarchien. Durch Approximation von Kosten-Funktionen kann eine Verbesserung der Konstruktionszeit von bis zu einer Grosenordnung erreicht werden (speziell fur BVH-Strukturen), bei nur geringem Verlust von Traversierungs-Effizienz. Mit Blick auf den statischen Teil einer Szene beschaftigen wir uns mit der Erstellung “perfekter” Beschleunigungs-Strukturen. Wir entwickeln einen Algorithmus zur BVH-Erstellung, der ein globales Optimum in polynomialer Zeit liefert. Dies fuhrt zu einer neuartigen Beschleunigungs-Struktur, welche sowohl die hohe Leistung von KD-Baumen, als auch den geringen Platzbedarf von BVH-Strukturen in sich vereinigt. Abschliesend betrachten wir Echtzeit-Ray-Tracing auf Desktop-Computern. Wir entwickeln neuartige KD-Baum- und BVH-Traversierungs-Algorithmen, die speziell auf den Einsatz von Grafikprozessoren zugeschnitten sind. Wir zeigen damit zum ersten Mal, dass GPU-Ray-Tracing nicht nur praktikabel ist, sondern auch mehr als eine Grosenordnung effizienter sein kann als CPU basierte Ray-Tracing-Verfahren, selbst bei der Darstellung groser CAD Modelle.	algorithm;commodity computing;data structure;ray tracing (graphics)	Stefan Popov	2012			computer science;electrical engineering;computer graphics (images)	Graphics	-108.21459371990159	31.11374391560491	194678
12cf0bf54393185ff11f79c0f2b20e87f3acd764	automatische rasterkoordinatenermittlung mit hilfe digitaler filter		Zur beruhrungslosen Ermittlung von Oberflachenverformungen gewinnen die Rasterverfahren zunehmend an Bedeutung, da deren regelmasige Struktur besonders gut fur die automatische Bildverarbeitung geeignet ist. Ein Raster, auf die Oberflache aufgebracht und mit dieser verformt, wird bei unterschiedlichen Laststufen aufgenommen. Aus der Verschiebung der Rasterpunkte erhalt man ein Mas fur die Dehnung von ebenen Oberflachen. Zeichnet man den Raster aus mehreren unterschiedlichen Richtungen auf, so liefert die Stereophotogrammetrie auch die 3-dimensionale Kontur der Oberflache. Aufgabe der Bildverarbeitung ist es nun, die Koordinaten des Rasters mit hoher Genauigkeit und weitgehend automatisch zu bestimmen.		K. Andresen;R. Helsch	1987		10.1007/978-3-662-22205-8_58	computer graphics (images);art	Robotics	-107.10610327623564	31.747205121371344	194945
2fc859fb0efd5a489d5241b4e731944a741b6fc1	impulsbasierte dynamiksimulation von mehrkörpersystemen in der virtuellen realität		Die dynamische Simulation gewinnt im Bereich der virtuellen Realität immer mehr an Bedeutung. Sie ist ein wichtiges Hilfsmittel, um den Grad der Immersion des Benutzers in eine virtuelle Welt zu erhöhen. In diesem Anwendungsbereich ist die Geschwindigkeit des verwendeten Simulationsverfahrens entscheidend. Weitere Anforderungen an das Verfahren sind unter anderem Genauigkeit, Stabilität und eine einfache Implementierung. In dieser Arbeit wird ein neues impulsbasiertes Verfahren für die dynamische Simulation von Mehrkörpersystemen vorgestellt. Dieses erfüllt, im Gegensatz zu klassischen Verfahren, alle Anforderungen der virtuellen Realität. Das vorgestellte Verfahren arbeitet ausschließlich mit Impulsen, um mechanische Gelenke, Kollisionen und bleibende Kontakte mit Reibung zu simulieren.	eine and zwei;immersion (virtual reality);sie (file format);simulation;the grid analysis and display system (grads);unified model	Jan Bender	2007			physics	OS	-107.1032489226472	31.910618886941183	196458
