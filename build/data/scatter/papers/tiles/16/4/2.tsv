id	title	keywords	abstract	entities	authors	year	journal	doi	fos	area	x	y	ix
f9418e8168af7d492b8192e816c326414136e884	evaluation study for an iso 13606 archetype based medical data visualization method	graphical user interface;iso 13606;ehr;evaluation;archetype	The objective of this evaluation study is to assess a method for standard based medical data visualization. The method allows flexible and customizable visualization for ISO 13606 archetype based medical data. The chosen evaluation concept is based the Guideline for Good Evaluation Practice in Health Informatics (GEP-HI). The stages of the study were identified. Each stage got a detailed description. We also identified the participants and their required qualifications and responsibilities. The evaluation location was described in details. The evaluation metrics were defined. The questionnaires for doctors, patients and experts were developed to fulfill the requirements of the evaluation study. The study was performed in Tomsk, Russia. 30 patients and 5 doctors participated in the study. The overall performance of the users reached the expert level by the end of the study. Patients as well as medical staff stated in their comments that the usability of the system was high, and they preferred it to the previously used paper-based and computer based systems. This was also shown by the high level of satisfaction measured within our study. The visualization approach, integrated into the electronic health record, was well accepted in our pilot setting with high usability scores from patients and doctors alike. The results showed the efficiency for both modeling and visualization part of the system	aspartate transaminase;data visualization;electronic health records;evaluation function;gene expression programming;high-level programming language;imagery;informatics;isoproterenol;laboratory certification document;patients;requirement;usability;responsibility	Georgy Kopanitsa	2015	Journal of Medical Systems	10.1007/s10916-015-0270-y	simulation;computer science;evaluation;data mining;graphical user interface;multimedia;archetype	Visualization	-56.27497011765909	-67.0721556885777	92126
19cc836868e6146a4c2d552b162597826aab43ca	clinical timelines development from textual medical reports in italian		Patients diagnosed with chronic conditions are visited multiple times over the years. The medical reports produced during these visits often include valuable knowledge in the form of free text. To help physician access and review this knowledge, natural language processing and aggregation techniques are needed. In this work, we propose a system that extracts and summarizes information from medical reports written in Italian. For each patient, the system builds and visualizes a timeline of the extracted events. The proposed approach has the potential to enhance the process of reviewing patient clinical histories, reducing the time needed to access large amounts of data. In the future, an extension of the visualized timeline and an extrinsic evaluation will be performed.	aggregate data;natural language processing;relationship extraction;timeline;usability	Natalia Viani;Lucia Sacchi;Valentina Tibollo;Carlo Napolitano;Silvia G. Priori;Riccardo Bellazzi	2017	2017 IEEE 3rd International Forum on Research and Technologies for Society and Industry (RTSI)	10.1109/RTSI.2017.8065897	information extraction;data mining;timeline;medicine	NLP	-50.225175935280845	-68.62443440137694	92238
4de94f5b39e650780104f95d1d19c3e1902947e2	a searchable patient record database for decision support	patient record database;decision support;patterns;. similarity search;feature vectors;pattern names;feature vector;similarity search;medical history	"""UNLABELLED We describe a searchable patient record database for decision support. It contains medical histories of real but pseudonymous patients with patterns of diagnosis, chosen treatment, and outcome. To be searchable, the patterns contain a feature vector (for similarity search by calculating distances) and a globally unique """"pattern name"""" which identifies the kind of data which are represented by the feature vector. Patterns with the same pattern name are directly comparable; they represent the same kind of data. For pattern selection the database provides a growing well-structured list of initial diagnoses with associated input masks.   PROCEDURE The doctor can assume that the database contains patients similar to the current patient if he finds his initial diagnosis in the list. Clicking on it opens an associated input mask which requests specific further data for finer differentiation. After input a searchable pattern group is built from the provided data, and used to search for histories of patients with similar fine diagnostics, and for the most successful treatment decisions at these patients. This information can be very valuable for deciding the treatment of the current patient. Because the database can collect patient histories from all countries, in the long run this could open access to a wealth of experience which by far exceeds the capacity of a today's doctor."""	decision support systems, clinical;decision support system;dioscorea trifida;distance;feature vector;input mask;masks;mathematical concepts;mathematical model;mathematics;medical records;patients;pseudonymity;signs and symptoms;similarity search	Wolfgang Orthuber;Thorsten Sommer	2009	Studies in health technology and informatics	10.3233/978-1-60750-044-5-584	data mining;medical diagnosis;medical history;decision support system;database;feature vector;computer science;nearest neighbor search	ML	-49.18461780245016	-66.34739823715582	92345
af0b99fbfbcfbe6b2e8f58ab77de680dc1d948a2	a causal modeling framework for generating clinical practice guidelines from data	clinical practice guideline;evidence based medicine;causal discovery;machine learning;causal models	The practice of medicine is becoming increasingly evidencebased and clinical practice guidelines (CPGs) are necessary for advancing evidence-based medicine (EBM). We hypothesize that machine learning methods can play an important role in learning CPGs automatically from data . Automatically induced CPGs can then be used for further manual refinement and deployment, for automated guideline compliance checking, for better understanding of disease processes, and for improved physician education. We discuss why learning CPGs is a special form of computational causal discovery and why simply predictive (i.e., noncausal) methods may not be appropriate for this task.	causal filter;causality;machine learning;refinement (computing);software deployment	Subramani Mani;Constantin F. Aliferis	2007		10.1007/978-3-540-73599-1_59	evidence-based medicine;computer science;knowledge management;machine learning;data mining;causal model	AI	-53.87438940841939	-67.41864951603736	92606
45c01128e9d538ad839b2ab2b87dbefb9f386a29	knowledge management system in falling risk for physiotherapy care of elderly	medical treatment knowledge management system kms falling risk physiotherapy care elderly healthcare decision support system knowledge engineering motion capture technology case based reasoning;senior citizens;decision support systems senior citizens knowledge management knowledge engineering cognition abstracts;knowledge management;conference paper;abstracts;cognition;decision support systems;patient treatment case based reasoning decision support systems geriatrics health care knowledge engineering knowledge management;knowledge engineering	This paper describes the elderly healthcare research project affected by a fall. The decision support system is proposed as knowledge management method, including knowledge engineering to acquiring the expert's heuristically diagnostic knowledge and sharing this knowledge to the physiotherapist in the form of tool and application at the right time. This paper outlines a Knowledge Management System (KMS) to diagnose falling patterns in elderly people using Motion Capture Technology. The idea is to integrate an appropriate procedure including case based reasoning and motion capture to provide a decision support system. The diagnosis information derived from the process of KMS helps support the physiotherapist to determine serious falling risks in the elderly and recommend guidelines for medical treatment. The evaluation result shows an efficient performance with 80.95% of precision when using the Assumption Attribute category criteria with KNNR=3. Furthermore, the result of KMS-EUCS shows a high satisfaction from the users with 97.50% of satisfaction in a community of practice scenario. This can confirm the successful of KMS approach within the falling risk screening procedure.	care-of address;case-based reasoning;decision support system;heuristic;knowledge engineering;knowledge management;management system;motion capture	Worasak Rueangsirarak;Nopasit Chakpitak;Komsak Meksamoot;Prapas Pothongsunun	2014	Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific	10.1109/APSIPA.2014.7041812	decision support system;intelligent decision support system;systems engineering;engineering;knowledge management;management science;personal knowledge management	AI	-54.4447484061831	-66.93284256352904	92779
060852fe16484d5a1ffe33dd1e16a25612b17967	research paper: prescribers' responses to alerts during medication ordering in the long term care se	relative risk;selected works;adverse drug event;randomized controlled trial;long term care;laboratory tests;computerized physician order entry;confidence interval;decision support system;side effect;drug therapy;clinical decision support;renal insufficiency;life sciences;health science;bepress;clinical decision support system;user computer interface;medical error;central nervous system	Objective: Computerized physician order entry with clinical decision support has been shown to improve medication safety in adult inpatients, but few data are available regarding its usefulness in the long- term care setting. The objective of this study was to examine opportunities for improving medication safety in that clinical setting by determining the proportion of medication orders that would generate a warning message to the prescriber via a computerized clinical decision support system and assessing the extent to which these alerts would affect prescribers' actions. Design: The study was set within a randomized controlled trial of computerized clinical decision support conducted in the long-stay units of a large, academically-affiliated long-term care facility. In March 2002, a computer-based clinical decision support system (CDSS) was added to an existing computerized physician order entry (CPOE) system. Over a subsequent one-year study period, prescribers ordering drugs for residents on three resident-care units of the facility were presented with alerts; these alerts were not displayed to prescribers in the four control units. Measurements: We assessed the frequency of drug orders associated with various categories of alerts across all participating units of the facility. To assess the impact of actually receiving an alert on prescriber behavior during drug ordering, we calculated separately for the intervention and control units the proportion of the alerts, within each category, that were followed by an appropriate action and estimated the relative risk of an appropriate action in the intervention units compared to the control units. Results: During the 12 months of the study, there were 445 residents on the participating units of the facility, contributing 3,726 resident-months of observation time. During this period, 47,997 medication orders were entered through the CPOE system—approximately 9 medication orders per resident per month. 9,414 alerts were triggered (2.5 alerts per resident-month). The alert categories most often triggered were related to risks of central nervous system side-effects such as over-sedation (20% ). Alerts for risk of drug-associated constipation (13%) or renal insufficiency/electrolyte imbalance (12%) were also common. Twelve percent of the alerts were related to orders for warfarin. Overall, prescribers who received alerts were only slightly more likely to take an appropriate action (relative risk 1.11, 95% confidence interval 1.00, 1.22). Alerts related to orders for warfarin or central nervous system side effects were most likely to engender an appropriate action, such as ordering a recommended laboratory test or canceling an ordered drug. Conclusion: Long-term care facilities must implement new system-level approaches with the potential to improve medication safety for their residents. The number of medication orders that triggered a warning message in this study suggests that CPOE with a clinical decision support system may represent one such tool. However, the relatively low rate of response to these alerts suggests that further refinements to such systems are required, and that their impact on medication errors and adverse drug events must be carefully assessed.		James Judge;Terry S. Field;Martin DeFlorio;Jane Laprino;Jill Auger;Paula Rochon;David W. Bates;Jerry H. Gurwitz	2006	JAMIA	10.1197/jamia.M1945	relative risk;clinical decision support system;intensive care medicine;confidence interval;medicine;pathology;computer science;artificial intelligence;central nervous system;nursing;medical emergency;side effect;randomized controlled trial;statistics	NLP	-60.03775931235068	-66.39306546218671	95698
10d55967830a7b18b74f7a1a251943622abf3b4f	remote afterloading brachytherapy: human factors in a partially automated treatment system	automatic control;treatment planning;radiation dose;human factors standpoint remote afterloading brachytherapy radioactive source prescribed dose treatment delivery system repeatable control source placement delivery problems radiation dose human error rab treatment delivery sites systems analysis rab partially automated treatment systems;source placement;control systems;user interfaces human factors medical computing radiation therapy;delivery system;radioactive source;treatment delivery system;nuclear regulatory commission;human factors standpoint;medical computing;member and geographic activities board committees;protection;rab;human factors;systems analysis;brachytherapy human factors member and geographic activities board committees performance analysis automatic control control systems neoplasms automation medical treatment protection;brachytherapy;task analysis;team performance;performance analysis;system analysis;partially automated treatment systems;repeatable control;rab treatment delivery sites;delivery problems;radiation therapy;remote afterloading brachytherapy;prescribed dose;neoplasms;medical treatment;human error;user interfaces;automation	Remote ajierloading brachytherapy (RABJ introdures a radioactive source close to a target (or tumor) in the body with the object of delivering a prescribed dose of radiation to that target. Partial automation of the treatment delivery system using a computer to control the position of the source produces precise and repeatable control of source placement and has eliminated staff exposure to radiation during most RAB procedures. Some treatment planning and delivery problems affecting the radiation dose to the patient have been reported with these devices. A few of these problems have been traced to faulty equipment, but most have been attributed to human error. As purt of a research study sponsored by The United States Nuclear Regulatory Commission, a human factors team has visited 23 RAB treatment delivery sites in the United States. The team performed a function and task analysis of RAB and used the results of that analysis to provide a framework for a systems analysis of the potential causes of human error in RAB. This paper presents some of the error protection and detection methods used in these partially automated treutment systems and discusses ways in which they can be expected to succeed or fail from n human factors standpoint.	human error;human factors and ergonomics;task analysis	Michael L. Quinn;Isabelle Schoenfeld;Dennis Serig	1993		10.1109/CBMS.1993.263008	systems analysis;radiation therapy;simulation;human error;radiology;automation;absorbed dose;automatic control;task analysis;system analysis;user interface;rab	Robotics	-59.32099319670755	-67.31756536856763	96250
f7235e0dc6042293f5ff00c0e84e4e0502753707	incremental revision of biological networks from texts		This work focuses on automatic extraction of relations between biological components from the literature to support incremental development of biological models. The incremental approach is illustrated by automatic expansion of a partial plant defence response network with 36 components and 50 relations, which was created manually by merging three existing structural models. Two incremental steps of automated extraction with the Bio3graph tool yielded the final model with 36 components and 237 relations. The results show that the existing biological networks can be considerably extended by mining publicly available biomedical articles, and that numerous valid relations can be extracted automatically.	automatic control;belief revision;biological network;iterative and incremental development	Dragana Miljkovic;Vid Podpecan;Tjaša Stare;Igor Mozetic;Kristina Gruden;Nada Lavrac	2013			computer science;artificial intelligence;data mining;algorithm	NLP	-49.92142044528978	-69.62195364861437	97061
3d6965122a36e947c74c56fb7555b25ce32d4a8a	a concept-based framework for retrieving evidence to support emergency physician decision making at the point of care	workflow management;medical information retrieval;information retrieval;evidence based medicine;software agent;clinical decision support;clinical practice;scientific research;article;point of care	The goal of evidence-based medicine is to uniformly apply evidence gained from scientific research to aspects of clinical practice. In order to achieve this goal, new applications that integrate increasingly disparate health care information resources are required. Access to and provision of evidence must be seamlessly integrated with existing clinical workflow and evidence should be made available where it is most often required at the point of care. In this paper we address these requirements and outline a concept-based framework that captures the context of a current patient-physician encounter by combining disease and patientspecific information into a logical query mechanism for retrieving relevant evidence from the Cochrane Library. Returned documents are organized by automatically extracting concepts from the evidence-based query to create meaningful clusters of documents which are presented in a manner appropriate for point of care support. The framework is currently being implemented as a prototype software agent that operates within the larger context of a multi-agent application for supporting workflow management of emergency pediatric asthma exacerbations.	cochrane library;document;multi-agent system;prototype;requirement;software agent	Dympna O'Sullivan;Ken Farion;Stan Matwin;Wojtek Michalowski;Szymon Wilk	2007		10.1007/978-3-540-78624-5_9	evidence-based medicine;workflow;clinical decision support system;point of care;scientific method;medicine;computer science;knowledge management;artificial intelligence;software agent;data mining;management science	DB	-51.95508194614011	-66.37418126833727	97132
39afee4bf7c64be003d95f8aa7cf71bc5e1bcc82	a comparison of medication administrations errors using cpoe orders vs. handwritten orders for pediatric continuous drug infusions.	medical order entry systems;critical illness;infusions intravenous;nursing staff hospital;intensive care units pediatric;medication errors;child;writing;humans	Continuous drug infusions in critically ill patients are associated with a high error rate. Although CPOE systems have shown to reduce prescribing errors, the effect on administration errors has not been studied in pediatric ICU patients. We studied this by measuring the ability of nurses to detect medication administration errors using CPOE orders versus handwritten orders for continuous drug infusions.		Azizeh Khaled Sowan;Mohamed Gaffoor;Karen L. Soeken;Mary Etta Mills;Meg Johantgen;Vinay U. Vaidya	2006	AMIA ... Annual Symposium proceedings. AMIA Symposium		intensive care medicine;medicine;pediatrics;medical emergency	Logic	-59.75746260824657	-66.3218208330046	97199
e21f901cb03eab0162c3d215db3855c6290c29bc	analysis of operating theatre utilisation to drive efficiency and productivity improvements.		There is an urgent need in the acute health system to use resources as efficiently as possible. One such group of resources are operating theatres, which have an important impact on patient flow through a hospital. Data-driven insights into the use of operating theatres can suggest improvements to minimise wastage and improve theatre availability. In this paper, a short extract of surgical data from participating Queensland public hospitals was statistically analysed to examine the effects of session type, session specialty, scheduling the longest case first and day of the week on theatre utilisation. It was found that day-long sessions (as opposed to separate morning or afternoon sessions), mid-week sessions, certain specialties (eg. neurosurgery sessions) and not doing the longest case first were most beneficial to theatre utilisation. Awareness of these findings is important in any redesign activity aimed at improving flow performance.		Hamish Thorburn;Sankalp Khanna;Justin Boyle;Norm Good;Michael Steyn	2014	Studies in health technology and informatics	10.3233/978-1-61499-427-5-163	simulation;operations management;industrial engineering	HCI	-60.835941808327924	-66.86589258242492	99764
b70512bea161f1170648f9259e7cb789e6978844	modelling temporal, data-centric medical processes	emergency department;clinical guideline;cardiology;temporal data;structured workflow;modeling language;process design;temporal workflow;temporal constraints;temporal controllability;data dependence;myocardial infarct;clinical process;workflow management system;time constraint	Workflow technology has emerged as one of the leading technologies in modeling, redesigning, and executing medical processes and some interesting workflow management systems are available. Nevertheless, such systems are lacking in an effective management of two key aspects: data dependencies and temporal constraints. In clinical/health context these two aspects are of paramount importance. For example, a surgery intervention could need the results of the concurrent bioptic analysis to be properly concluded; on the other hand, to successfully apply a fibrinolytic therapy to patients with ST-segment Elevation Myocardial Infarction (STEMI) a maximum delay of 30 minutes must be considered w.r.t. the emergency department admission. In this paper, we propose TNest, a new advanced, structured and highly modular workflow modeling language that allows one to easily express data dependencies and time constraints during process design. All the features of TNest have been considered to model the process related to classical clinical guidelines, i.e. those for the management of STEMI patients, published by the American College of Cardiology/American Heart Association.	data dependency;modeling language	Carlo Combi;Mauro Gambini;Sara Migliorini;Roberto Posenato	2012		10.1145/2110363.2110382	simulation;computer science;knowledge management;biological engineering	ML	-52.443724566731525	-67.35512928906694	99978
168827680845bc5f6bd45722bd98debbdde4433b	ontological and epistemological views of 'headings' in clinical records		This paper presents a theoretical framework for analyzing the structure and use of headings in clinical narrative. We are researching the proposed UK national standard of headings for communicating clinical information. We investigate three topics with respect to these headings) conformance to standards and systems) coverage and reliability) navigation and interpretation. The method described here is used to analyse the three distinct topics in a coherent and systematic way deploying a variant of functional analysis to consider the meaningfulness and interpretation of headings from both the clinician' and system' perspectives.	code coverage;coherence (physics);conformance testing;interpretation (logic);standards characteristics	Stephen Kay	2001	Studies in health technology and informatics	10.3233/978-1-60750-928-8-104	knowledge management;ontology;medicine	SE	-54.72877379198613	-70.0026360123624	100437
c224ecd139b08e3a1d559ea09c15792e5e658755	automated identification of antibiotic overdoses and adverse drug events via analysis of prescribing alerts and medication administration records	cpoe;adverse drug event;clinical;decision support systems;electronic health record;electronic medical record;medical order entry system;patient safety;risk management	Objectives Electronic trigger detection tools hold promise to reduce Adverse drug event (ADEs) through efficiencies of scale and real-time reporting. We hypothesized that such a tool could automatically detect medication dosing errors as well as manage and evaluate dosing rule modifications.   Materials and Methods We created an order and alert analysis system that identified antibiotic medication orders and evaluated user response to dosing alerts. Orders associated with overridden alerts were examined for evidence of administration and the delivered dose was compared to pharmacy-derived dosing rules to confirm true overdoses. True overdose cases were reviewed for association with known ADEs.   Results Of 55 546 orders reviewed, 539 were true overdose orders, which lead to 1965 known overdose administrations. Documentation of loose stools and diarrhea was significantly increased following drug administration in the overdose group. Dosing rule thresholds were altered to reflect clinically accurate dosing. These rule changes decreased overall alert burden and improved the salience of alerts.   Discussion Electronic algorithm-based detection systems can identify antibiotic overdoses that are clinically relevant and are associated with known ADEs. The system also serves as a platform for evaluating the effects of modifying electronic dosing rules. These modifications lead to decreased alert burden and improvements in response to decision support alerts.   Conclusion The success of this test case suggests that gains are possible in reducing medication errors and improving patient safety with automated algorithm-based detection systems. Follow-up studies will determine if the positive effects of the system persist and if these changes lead to improved safety outcomes.		Eric S. Kirkendall;Michal Kouril;Judith W. Dexheimer;Joshua D. Courter;Philip Hagedorn;Rhonda D. Szczesniak;Dan Li;Rahul Damania;Thomas Minich;S. Andrew Spooner	2017	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocw086	medicine;toxicology;medical emergency	SE	-59.380408841060785	-66.84809304752604	101030
d76bec176b3230657e55a4ea3c1d1046622f67c9	high-risk drug–drug interactions between clinical practice guidelines for management of chronic conditions		Clinicians and clinical decision-support systems often follow pharmacotherapy recommendations for patients based on clinical practice guidelines (CPGs). In multimorbid patients, these recommendations can potentially have clinically significant drug-drug interactions (DDIs). In this study, we describe and validate a method for programmatically detecting DDIs among CPG recommendations. The system extracts pharmacotherapy intervention recommendations from narrative CPGs, normalizes the terms, creates a mapping of drugs and drug classes, and then identifies occurrences of DDIs between CPG pairs. We used this system to analyze 75 CPGs written by authoring entities in the United States that discuss outpatient management of common chronic diseases. Using a reference list of high-risk DDIs, we identified 2198 of these DDIs in 638 CPG pairs (46 unique CPGs). Only 9 high-risk DDIs were discussed by both CPGs in a pairing. In 69 of the pairings, neither CPG had a pharmacologic reference or a warning of the possibility of a DDI.	adverse reaction to drug;bibliographic index;central pattern generator;chronic disease;class;clinical practice guideline;clinical decision support system;didanosine;drug interactions;entity;infinite impulse response;informatics (discipline);interaction;patients;pharmaceutical preparations;pharmacology;practice guidelines as topic;sensor;sixty nine;support system;united states department of veterans affairs;cytidylyl-3'-5'-guanosine	Geoffrey J. Tso;Samson W. Tu;Mark A. Musen;Mary Kane Goldstein	2017				ML	-55.15723437973276	-66.61978105426688	102001
419ae923c04a82c04f16a84f7d7d2afa02acc59c	analyzing relationship between patient and doctor in public dental health using particle memetic multivariable logistic regression analysis approach (mlra2)	dental problem;dental surgical details;particle-memetic multivariable logistic regression;patient and doctor relationship;public dental health aspects	In the developing technology managing patient and doctor relationship and communication process is one of the critical factors because the patient may take high risk surgery, treatment and diagnosis. Due to the importance of the patient-doctor relationship, in this paper investigate the various aspects of relation between the doctor and dental problem related patients. During the analyzing process, data has been collected from adults who are facing dental problems and other dental injuries in which data has been gathered from 423 individuals by conducting interviews. The gathered data is investigated with the help of particle-memetic multivariable logistic regression analysis method which examines the patient income, dental surgical details, injuries and other factors relationship has been investigated. From the analyzed data, how the patients are treated by doctors examined for improving the relationship between patient and doctor in public dental health aspects.	amendment;behavior;coefficient;consent forms;declaration (computer programming);helsinki declaration;high risk acute leukemia;logistic regression;memetics;oral health;patients;regression analysis;sodium monofluorophosphate 0.0076 mg/mg toothpaste;interest;standards characteristics	Sajith Vellappally;Abdulaziz A. Al Kheraif;Sukumaran Anil;Mansour K. Assery;K. Aswini Kumar;Darshan Devang Divakar	2018	Journal of Medical Systems	10.1007/s10916-018-1037-z	data mining;multivariable calculus;logistic regression;family medicine;medicine	HCI	-61.80967186031118	-66.14820423873626	102472
059c281cf63e594faf0b9d6082c103b264807ebf	ontology-based modeling of clinical practice guidelines: a clinical decision support system for breast cancer follow-up interventions at primary care settings	online resources;clinical practice guideline;information resources;scholarly research;information sources;academic research;online databases;education resources;publishing;family physician;research databases;support system;decision support system;nova scotia;australasian research information;south east asian information;guideline elements model;information databases;full content;semantic web;education databases;australian databases;clinical decision support system;commissioning;electronic publisher;article;breast cancer;online;e titles;primary care;library resources	Breast cancer follow-up care can be provided by family physicians after specialists complete the primary treatment. Cancer Care Nova Scotia has developed a breast cancer follow-up Clinical Practice Guideline (CPG) targeting family physicians. In this paper we present a project to computerize and deploy the said CPG in a Breast Cancer Follow-up Decision Support System (BCF-DSS) for use by family physicians in a primary care setting. We present a semantic web approach to model the CPG knowledge and employ a logic-based proof engine to execute the CPG in order to infer patient-specific recommendations. We present the three stages of the development of BCF-DSS--i.e., (a) Computerization of the paper-based CPG for Breast Cancer follow-up; (b) Development of three ontologies--i.e., the Breast Cancer Ontology, the CPG ontology based on the Guideline Element Model (GEM) and a Patient Ontology; and (c) Execution of the Breast Cancer follow-up CPG through a logic-based CPG execution engine.	aftercare;breast carcinoma;cns disorder;central pattern generator;clinical practice guideline;clinical decision support system;dss brand of docusate sodium;decision support systems, clinical;decision support techniques;emoticon;follow-up report;independent practice associations;inference;malignant neoplasm of breast;mammary neoplasms;niemann-pick disease, type d;ontology (information science);patients;practice guidelines as topic;primary health care;semantic web;cellular targeting;positive regulation of methylenetetrahydrofolate reductase (nad(p)h) activity	Samina Raza Abidi;Syed Sibte Raza Abidi;Sajjad Hussain;Mike Shepherd	2007	Studies in health technology and informatics	10.3233/978-1-58603-774-1-845	medicine;knowledge management;data mining	HCI	-55.12678021796611	-66.29152871956242	104558
c8d86dcef9a3567c1d4a87e41b4bc49259739fcc	exploiting natural language processing for improving health processes		In the medical world, high quality digital registration in an Electronic Patient Dossier (EPD) of symptoms, diagnoses, treatments, test results, images, interpretations, and outcomes becomes commonplace. Together with a shortage of medical professionals, means that they experience pressure at the expense of actual ‘hands on the bed’. On the other hand, EPDs contain a wealth of largely unused, unstructured textual information. Clinicians primarily communicate with each other through letters and reports. Our main question is: Can Natural Language Processing (NLP) exploit this wealth? By extracting structured data and using it as features for machine learning, a wide variety of process improvements become possible. Furthermore, it may contribute to the desire of government and health stakeholders to simplify registration and relieve pressure. This paper sketches a few prominent process improvements that we plan to research.	display resolution;epd;machine learning;natural language processing	Maurice van Keulen;Jeroen Geerdink;Gerard Linssen;Riemer H J A Slart;Onno Vijlbrief	2017			medical diagnosis;economic shortage;natural language processing;data model;exploit;government;artificial intelligence;computer science	AI	-52.2330339149783	-69.33002194671555	106650
5f9c430718d48351c01f8bdc894e5ba54732b13d	conclusions from a naive bayes operator predicting the medicare 2011 transaction data set		Introduction: The United States Federal Government operates one of the world's largest medical insurance programs, Medicare, to ensure payment for clinical services for the elderly, illegal aliens and those without the ability to pay for their care directly. The program is mired in controversy over its costs and consequences. This paper evaluates the Medicare 2011 Transaction Data Set which details the transfer of funds from Medicare to private and public clinical care facilities for specific clinical services (DRG) for the operational year 2011.	data structure;digital raster graphic;naive bayes classifier;transaction data	Nick Williams	2014	CoRR		actuarial science;data mining	AI	-58.395739036367395	-69.37435582955428	106949
45af8a84e7ffb2c7c932843ca21a33f8921407f8	analysis of eligibility criteria representation in industry-standard clinical trial protocols	controlled vocabulary;information retrieval;eligibility determination;natural language processing;clinical trials	"""Previous research on standardization of eligibility criteria and its feasibility has traditionally been conducted on clinical trial protocols from ClinicalTrials.gov (CT). The portability and use of such standardization for full-text industry-standard protocols has not been studied in-depth. Towards this end, in this study we first compare the representation characteristics and textual complexity of a set of Pfizer's internal full-text protocols to their corresponding entries in CT. Next, we identify clusters of similar criteria sentences from both full-text and CT protocols and outline methods for standardized representation of eligibility criteria. We also study the distribution of eligibility criteria in full-text and CT protocols with respect to pre-defined semantic classes used for eligibility criteria classification. We find that in comparison to full-text protocols, CT protocols are not only more condensed but also convey less information. We also find no correlation between the variations in word-counts of the ClinicalTrials.gov and full-text protocols. While we identify 65 and 103 clusters of inclusion and exclusion criteria from full text protocols, our methods found only 36 and 63 corresponding clusters from CT protocols. For both the full-text and CT protocols we are able to identify 'templates' for standardized representations with full-text standardization being more challenging of the two. In our exploration of the semantic class distributions we find that the majority of the inclusion criteria from both full-text and CT protocols belong to the semantic class """"Diagnostic and Lab Results"""" while """"Disease, Sign or Symptom"""" forms the majority for exclusion criteria. Overall, we show that developing a template set of eligibility criteria for clinical trials, specifically in their full-text form, is feasible and could lead to more efficient clinical trial protocol design."""		Sanmitra Bhattacharya;Michael N. Cantor	2013	Journal of biomedical informatics	10.1016/j.jbi.2013.06.001	controlled vocabulary;computer science;artificial intelligence;data mining;database;algorithm	Web+IR	-48.453761336887986	-68.51791817387587	107633
79828bfb1e59e202f8b0b3751db6ecff3a53cb47	an approach to automatic process deviation detection in a time-critical clinical process	human error;process deviations;process mining;trauma resuscitation;workflow compliance	"""MOTIVATION Prior research has shown that minor errors and deviations from recommended guidelines in complex medical processes can accumulate to increase the likelihood that a major error will go uncorrected and lead to an adverse outcome. Real-time automatic and accurate detection of process deviations may help medical teams better prevent or mitigate the effect of errors and improve patient outcomes. Our goal was to develop an approach for automatic detection of errors and process deviations in trauma resuscitation.   METHODS Using video review, we coded activity traces of 95 pediatric trauma resuscitations collected in a Level 1 trauma center over two years (2014-2016). Twenty-four randomly selected activity traces were compared with a knowledge-driven model of trauma resuscitation workflow using a phase-based conformance checking algorithm for detecting true and false deviations (alarms). An analysis of false alarms identified three types of causes: (1) model gaps or discrepancies between the model (""""work as imagined"""") and actual practice (""""work as done""""), (2) errors in activity traces coding, and (3) algorithm limitations. We repaired the system to remove model gaps, reduce coding errors, and address algorithm limitations. The repaired system was first evaluated with another 20 traces and then applied to the entire dataset of 95 traces.   RESULTS During the training, we detected 573 process deviations in 24 activity traces that include 1099 activities. Among these deviations, only 27% represented true deviations and the remaining 73% were false alarms. This initial deviation detection accuracy was only 66.6%, with a F1-score of 0.42. Detection accuracy of the repaired system increased to 95.2% (0.85 F1-score) during system validation and to 98.5% (0.96 F1-score) during testing. After deploying the repaired deviation detection system to all 95 activity traces, we detected 1060 process deviations in 5659 activities (11.2 deviations per resuscitation). Among the 5659 activities in these traces, 4893 fit the repaired knowledge-driven workflow model, 294 were errors of omission, 538 were errors of commission, and 228 were scheduling errors.   CONCLUSION Our approach to automatic deviation detection provides a method for identifying repeated, omitted and out-of-sequence activities that can be included in the design of decision support systems for complex medical processes. Our findings show the importance of assessing detected deviations for repairing a knowledge-driven model that best represents """"work as done."""""""		Sen Yang;Aleksandra Sarcevic;Richard A. Farneth;Shuhong Chen;Omar Z. Ahmed;Ivan Marsic;Randall S. Burd	2018	Journal of biomedical informatics	10.1016/j.jbi.2018.07.022	conformance checking;data mining;scheduling (computing);decision support system;computer science;workflow;resuscitation;artificial intelligence;pattern recognition	Metrics	-59.383123690611214	-67.33470879158324	108541
8c63fe287d15f5be60c8c9fc9f57db0c9c288530	automatic identification & classification of surgical margin status from pathology reports following prostate cancer surgery		Prostate cancer removal surgeries result in tumor found at the surgical margin, otherwise known as a positive surgical margin, have a significantly higher chance of biochemical recurrence and clinical progression. To support clinical outcomes assessment a system was designed to automatically identify, extract, and classify key phrases from pathology reports describing this outcome. Heuristics and boundary detection were used to extract phrases. Phrases were then classified using support vector machines into one of three classes: 'positive (involved) margins,' 'negative (uninvolved) margins,' and 'not-applicable or definitive.' A total of 851 key phrases were extracted from a sample of 782 reports produced between 1996 and 2006 from two major hospitals. Despite differences in reporting style, at least 1 sentence containing a diagnosis was extracted from 780 of the 782 reports (99.74%). Of the 851 sentences extracted, 97.3% contained diagnoses. Overall accuracy of automated classification of extracted sentences into the three categories was 97.18%.	automatic identification and data capture;categories;class;classification;color gradient;contain (action);extraction;gleason's theorem;heuristic;holographic principle;informatics (discipline);kearns-sayre syndrome;malignant neoplasm of prostate;missing data;national library of medicine (u.s.);netware loadable module;operative surgical procedures;phrases;prostate carcinoma;prostatic neoplasms;sayre's paradox;speech-language pathology;support vector machine;surgical margins;tumor stage;pathology report;sentence	Leonard W. D'Avolio;Mark S. Litwin;Selwyn O. Rogers;Alex A. T. Bui	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium			NLP	-48.677675396766816	-69.91380148300773	108734
d2d09ce7e06a10c2b135e13716bf21941aaf2fac	enable (exportable notation and bookmark list engine): an interface to manage tumor measurement data from pacs to cancer databases	clinical oncology;data collection;data management;efficiency;multimedia;pacs;pacs metadata;recist	Oncologists evaluate therapeutic response in cancer trials based on tumor quantification following selected “target” lesions over time. At our cancer center, a majority of oncologists use Response Evaluation Criteria in Solid Tumors (RECIST) v1.1 quantifying tumor progression based on lesion measurements on imaging. Currently, our oncologists handwrite tumor measurements, followed by multiple manual data transfers; however, our Picture Archiving Communication System (PACS) (Carestream Health, Rochester, NY) has the ability to export tumor measurements, making it possible to manage tumor metadata digitally. We developed an interface, “Exportable Notation and Bookmark List Engine” (ENABLE), which produces prepopulated RECIST v1.1 worksheets and compiles cohort data and data models from PACS measurement data, thus eliminating handwriting and manual data transcription. We compared RECIST v1.1 data from eight patients (16 computed tomography exams) enrolled in an IRB-approved therapeutic trial with ENABLE outputs: 10 data fields with a total of 194 data points. All data in ENABLE’s output matched with the existing data. Seven staff were taught how to use the interface with a 5-min explanatory instructional video. All were able to use ENABLE successfully without additional guidance. We additionally assessed 42 metastatic genitourinary cancer patients with available RECIST data within PACS to produce a best response waterfall plot. ENABLE manages tumor measurements and associated metadata exported from PACS, producing forms and data models compatible with cancer databases, obviating handwriting and the manual re-entry of data. Automation should reduce transcription errors and improve efficiency and the auditing process.	asea irb;adobe streamline;automation;cns disorder;ct scan;color gradient;data model;data point;efficiency;file archiver;interface device component;metastatic neoplasm;nci-designated cancer center;neoplasms;neuroectodermal tumor, primitive;non-small cell lung carcinoma;p1 bacteriophage artificial chromosomes;patients;phil bernstein;picture archiving and communication system;published database;quantitation;radiology;response evaluation criteria in solid tumors;scalability;single source of truth;transcription (software);tumor burden;tumor progression;waterfall plot;x-ray computed tomography;explanation;notation;pediatric intracranial germ cell brain tumor	Nikhil Goyal;Andrea B. Apolo;Eliana D. Berman;Mohammad Hadi Bagheri;Jason E. Levine;John W. Glod;Rosandra N. Kaplan;Laura B. Machado;Les R. Folio	2016	Journal of Digital Imaging	10.1007/s10278-016-9938-1	medicine;computer science;data mining;multimedia;world wide web	Visualization	-56.34423748650873	-66.68318584103783	109542
862b6c5bfca7fc4e01bca0f936cb5c662d4923f5	scope of consumer responses to an e-mail health newsletter	biomedical research;bioinformatics	Background. Health care providers and patients are using the Internet at an ever-increasing rate. There are an estimated 25,000 health information sites on the World Wide Web [1], making it the dh largest topic on the Internet [2]. The demand for consumer health information is fueled in part by the problem of insufficient communication between doctors and patients. E-mail is seen as a partial solution, but fewer than 2% of US physicians communicate with patients via e-mail [2].		Ann W. Stodder;Gary Barnas;Charles E. Kahn	2000			data science;health care;internet privacy;the internet;medicine	HCI	-57.99749307692546	-67.71707574726884	109783
751f809d174b4057493a857b67b83bf9a37a77c4	use of digital whole slide imaging in dermatopathology	melanocytic lesions;digital whole slide imaging;dermatopathology	Digital whole slide imaging (WSI) is an emerging technology for pathology interpretation, with specific challenges for dermatopathology, yet little is known about pathologists’ practice patterns or perceptions regarding WSI for interpretation of melanocytic lesions. A national sample of pathologists (N = 207) was recruited from 864 invited pathologists from ten US states (CA, CT, HI, IA, KY, LA, NJ, NM, UT, and WA). Pathologists who had interpreted melanocytic lesions in the past year were surveyed in this cross-sectional study. The survey included questions on pathologists’ experience, WSI practice patterns and perceptions using a 6-point Likert scale. Agreement was summarized with descriptive statistics to characterize pathologists’ use and perceptions of WSI. The majority of participating pathologists were between 40 and 59 years of age (62 %) and not affiliated with an academic medical center (71 %). Use of WSI was seen more often among dermatopathologists and participants affiliated with an academic medical center. Experience with WSI was reported by 41 %, with the most common type of use being for education and testing (CME, board exams, and teaching in general, 71 %), and clinical use at tumor boards and conferences (44 %). Most respondents (77 %) agreed that accurate diagnoses can be made with this technology, and 59 % agreed that benefits of WSI outweigh concerns. However, 78 % of pathologists reported that digital slides are too slow for routine clinical interpretation. The respondents were equally split as to whether they would like to adopt WSI (49 %) or not (51 %). The majority of pathologists who interpret melanocytic lesions do not use WSI, but among pathologists who do, use is largely for CME, licensure/board exams, and teaching. Positive perceptions regarding WSI slightly outweigh negative perceptions. Understanding practice patterns with WSI as dissemination advances may facilitate concordance of perceptions with adoption of the technology.	academic medical centers;central nervous system melanocytic neoplasm;concordance (publishing);conferences;cross-sectional data;description;fifty nine;forty nine;lactic acid;neoplasms;primary melanocytic lesion of meninges;slide (glass microscope);wafer-scale integration;whole body imaging;benefit	Tracy Onega;Lisa M. Reisch;Paul D. Frederick;Berta M. Geller;Heidi D. Nelson;Jason P. Lott;Andrea C. Radick;David E. Elder;Raymond L. Barnhill;Michael W. Piepkorn;Joann G. Elmore	2015	Journal of Digital Imaging	10.1007/s10278-015-9836-y	medicine;pathology;multimedia	HCI	-61.86607196272031	-66.61100326532028	109922
d3366037e36a3d7b365e0fd6e8266b75dc24237f	natural language processing of radiology reports for the detection of thromboembolic diseases and clinically relevant incidental findings	radiology;computational biology bioinformatics;algorithms;humans;research report;incidental findings;combinatorial libraries;computational biology;computer appl in life sciences;pulmonary embolism;natural language processing;tomography x ray computed;microarrays;bioinformatics	Natural Language Processing (NLP) has been shown effective to analyze the content of radiology reports and identify diagnosis or patient characteristics. We evaluate the combination of NLP and machine learning to detect thromboembolic disease diagnosis and incidental clinically relevant findings from angiography and venography reports written in French. We model thromboembolic diagnosis and incidental findings as a set of concepts, modalities and relations between concepts that can be used as features by a supervised machine learning algorithm. A corpus of 573 radiology reports was de-identified and manually annotated with the support of NLP tools by a physician for relevant concepts, modalities and relations. A machine learning classifier was trained on the dataset interpreted by a physician for diagnosis of deep-vein thrombosis, pulmonary embolism and clinically relevant incidental findings. Decision models accounted for the imbalanced nature of the data and exploited the structure of the reports. The best model achieved an F measure of 0.98 for pulmonary embolism identification, 1.00 for deep vein thrombosis, and 0.80 for incidental clinically relevant findings. The use of concepts, modalities and relations improved performances in all cases. This study demonstrates the benefits of developing an automated method to identify medical concepts, modality and relations from radiology reports in French. An end-to-end automatic system for annotation and classification which could be applied to other radiology reports databases would be valuable for epidemiological surveillance, performance monitoring, and accreditation in French hospitals.	accreditation;algorithm;body of uterus;database;deep vein thrombosis;end-to-end principle;epidemiology;immune system diseases;modality (human–computer interaction);natural language processing;performance;pulmonary embolism;radiology;structure of deep vein;supervised learning;thromboembolism;venography - procedure;venous thrombosis;angiogram;benefit	Anne-Dominique Pham;Aurélie Névéol;Thomas Lavergne;Daisuke Yasunaga;Olivier Clément;Guy Meyer;Rémy Morello;Anita Burgun-Parenthoine	2014		10.1186/1471-2105-15-266	biology;dna microarray;computer science;bioinformatics	NLP	-48.63216120247057	-69.84911433972064	110531
70dadd25128e5d083548ff686f78948894320ce6	"""primary care provider adherence to an alert for intensification of diabetes blood pressure medications before and after the addition of a """"chart closure"""" hard stop"""		"""Objective To evaluate provider responses to a narrowly targeted """"Best Practice Advisory"""" (BPA) alert for the intensification of blood pressure medications for persons with diabetes before and after implementation of a """"chart closure"""" hard stop, which is non-interruptive but demands an action or dismissal before the chart can be closed.   Materials and Methods We designed a BPA that fired alerts within an electronic health record (EHR) system during outpatient encounters for patients with diabetes when they had elevated blood pressures and were not on angiotensin receptor blocking medications. The BPA alerts were implemented in eight primary care practices within UCLA Health. We compared data on provider responses to the alerts before and after implementing a """"chart closure"""" hard stop, and we conducted chart reviews to adjudicate each alert's appropriateness.   Results Providers responded to alerts more often after the """"chart closure"""" hard stop was implemented (P < .001). Among 284 alert firings over 16 months, we judged 107 (37.7%) to be clinically unnecessary or inappropriate based on chart review. Among the remainder, which represent clear opportunities for treatment, providers ordered the indicated medication more often (41% vs 75%) after the """"chart closure"""" hard stop was implemented (P = .001).   Discussion The BPA alerts for diabetes and blood pressure control achieved relatively high specificity. The """"chart closure"""" hard stop improved provider attention to the alerts and was effective at getting patients treated when they needed it.   Conclusion Targeting specific omitted medication classes can produce relatively specific alerts that may reduce alert fatigue, and using a """"chart closure"""" hard stop may prompt providers to take action without excessively disrupting their workflow."""	alert:type:point in time:^patient:nominal;angiotensin receptor;angiotensins;bell palsy;best practice;blocking (computing);chart evaluation by healthcare professional;class;closure;creatinine;dr-dos;data acquisition;diabetes mellitus;electronic health records;entity–relationship model;execution unit;fatigue;how true feel alert right now;manuscripts;medicare;menopause;mentored patient-oriented research career development award;national center for advancing translational sciences (u.s.);norm (social);oracle bpa suite;patients;pregnancy;primary health care;review [publication type];revision procedure;scientific publication;sensitivity and specificity;interest;medicare/medicaid	Magaly Ramirez;Richard Maranon;Jeffery Fu;Janet S. Chon;Kimberly Chen;Carol M. Mangione;Gerardo Moreno;Douglas S. Bell	2018	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocy073	data mining;blood pressure;medical record;remainder;dismissal;medical emergency;chart;medicine	HCI	-58.87742392431778	-66.1940323086754	110886
0e4624c943417852894828942da730459237f101	annotation schemes to encode domain knowledge in medical narratives	broad goal;unique annotation scheme;verbal diagnostic narrative;annotation scheme;diagnostic process;cognitive reasoning;diagnostic style;medical narrative;diagnostic correctness;domain knowledge;annotation study;relevant diagnostic step	The broad goal of this study is to further the understanding of doctors’ diagnostic styles and reasoning processes. We analyze and validate methods for annotating verbal diagnostic narratives collected together with eyemovement data. The long-term goal is to understand the cognitive reasoning and decisionmaking processes of medical experts, which could be useful for clinical information systems. The linguistic data set consists of transcribed recordings. Dermatologists were shown images of cutaneous conditions and asked to explain their observations aloud as they proceeded towards a diagnosis. We report on two linked annotation studies. In the first study, a subset of narratives were annotated by experts using a unique annotation scheme developed specifically for capturing decision-making components in the diagnostic process of dermatologists. We analyze annotator agreement as well as compare this annotation scheme to semantic types of the Unified Medical Language System as validation. In the second study, we explore the annotation of diagnostic correctness in the narratives at three relevant diagnostic steps, and we also explore the relationship between the two annotation schemes.	cognition;cognitive computing;correctness (computer science);decision support system;encode;ibm notes;information system;multimodal interaction	Wilson McCoy;Cecilia Ovesdotter Alm;Cara Calvelli;Rui Li;Jeff B. Pelz;Pengcheng Shi;Anne R. Haake	2012			computer science;bioinformatics;data mining;information retrieval	AI	-50.18648421196438	-69.12065762628606	110896
c6ab068ce45d2273fc7ea96c91a7c1aba67eb94f	user needs analysis and usability assessment of datamed - a biomedical data discovery index	data discovery;information retrieval;metadata;usability;user needs	Objective To present user needs and usability evaluations of DataMed, a Data Discovery Index (DDI) that allows searching for biomedical data from multiple sources.   Materials and Methods We conducted 2 phases of user studies. Phase 1 was a user needs analysis conducted before the development of DataMed, consisting of interviews with researchers. Phase 2 involved iterative usability evaluations of DataMed prototypes. We analyzed data qualitatively to document researchers' information and user interface needs.   Results Biomedical researchers' information needs in data discovery are complex, multidimensional, and shaped by their context, domain knowledge, and technical experience. User needs analyses validate the need for a DDI, while usability evaluations of DataMed show that even though aggregating metadata into a common search engine and applying traditional information retrieval tools are promising first steps, there remain challenges for DataMed due to incomplete metadata and the complexity of data discovery.   Discussion Biomedical data poses distinct problems for search when compared to websites or publications. Making data available is not enough to facilitate biomedical data discovery: new retrieval techniques and user interfaces are necessary for dataset exploration. Consistent, complete, and high-quality metadata are vital to enable this process.   Conclusion While available data and researchers' information needs are complex and heterogeneous, a successful DDI must meet those needs and fit into the processes of biomedical researchers. Research directions include formalizing researchers' information needs, standardizing overviews of data to facilitate relevance judgments, implementing user interfaces for concept-based searching, and developing evaluation methods for open-ended discovery systems such as DDIs.		Ram Dixit;Deevakar Rogith;Vidya Narayana;Mandana Salimi;Anupama E. Gururaj;Lucila Ohno-Machado;Hua Xu;Todd R. Johnson	2017	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocx134		Web+IR	-50.44741622081166	-67.27146800824615	111618
cedf72c9a09356c39b39a5bbf2c92b4e6b265ceb	role-preserving redaction of medical records to enable ontology-driven processing		Electronic medical records (EMR) have largely replaced hand-written patient files in healthcare. The growing pool of EMR data presents a significant resource in medical research, but the U.S. Health Insurance Portability and Accountability Act (HIPAA) mandates redacting medical records before performing any analysis on the same. This process complicates obtaining medical data and can remove much useful information from the record. As part of a larger project involving ontologydriven medical processing, we employ a method of recognizing protected health information (PHI) that maps to ontological terms. We then use the relationships defined in the ontology to redact medical texts so that roles and semantics of terms are retained without compromising anonymity. The method is evaluated by clinical experts on several hundred medical documents, achieving up to a 98.8% f-score, and has already shown promise for retaining semantic information in later processing.	data science;data structure;de-identification;excalibur: morgana's revenge;f1 score;health insurance portability and accountability act;map;pipeline (computing);protected health information;pseudonymity;requirements analysis	Seth Polsley;Atif Tahir;Muppala Raju;Akintayo Akinleye;Duane Steward	2017		10.18653/v1/W17-2324	medical record;information retrieval;ontology;redaction;computer science	ML	-49.432562793096714	-67.34975529256735	112423
7345a27ce2bac2bdfccd9656b50d4d215cd04f79	validating the extract, transform, load process used to populate a large clinical research database	electronic health record;correctness;informatics;clinical data warehouse;extract transform load	BACKGROUND Informaticians at any institution that are developing clinical research support infrastructure are tasked with populating research databases with data extracted and transformed from their institution's operational databases, such as electronic health records (EHRs). These data must be properly extracted from these source systems, transformed into a standard data structure, and then loaded into the data warehouse while maintaining the integrity of these data. We validated the correctness of the extract, load, and transform (ETL) process of the extracted data of West Virginia Clinical and Translational Science Institute's Integrated Data Repository, a clinical data warehouse that includes data extracted from two EHR systems.   METHODS Four hundred ninety-eight observations were randomly selected from the integrated data repository and compared with the two source EHR systems.   RESULTS Of the 498 observations, there were 479 concordant and 19 discordant observations. The discordant observations fell into three general categories: a) design decision differences between the IDR and source EHRs, b) timing differences, and c) user interface settings. After resolving apparent discordances, our integrated data repository was found to be 100% accurate relative to its source EHR systems.   CONCLUSION Any institution that uses a clinical data warehouse that is developed based on extraction processes from operational databases, such as EHRs, employs some form of an ETL process. As secondary use of EHR data begins to transform the research landscape, the importance of the basic validation of the extracted EHR data cannot be underestimated and should start with the validation of the extraction process itself.		Michael J. Denney;Dustin M. Long;Matthew G. Armistead;Jamie L. Anderson;Baqiyyah N. Conway	2016	International journal of medical informatics	10.1016/j.ijmedinf.2016.07.009	correctness;computer science;data science;data warehouse;data mining;database;informatics	DB	-54.88465617066367	-68.06363761298337	113342
dd183d33c793f8a7c100653e4eadf9f6aa0563ad	towards morphologically annotated corpus of hospital discharge reports in polish	polish clinical data;annotated corpus;domain specific rule;paper discuses problem;low level linguistic information;manual verification;hospital discharge report;automatic morphologic annotation	The paper discuses problems in annotating a corpus containing Polish clinical data with low level linguistic information. We propose an approach to tokenization and automatic morphologic annotation of data that uses existing programs combined with a set of domain specific rules and vocabulary. Finally we present the results of manual verification of the annotation for a subset of data.	tokenization (data security);vocabulary	Malgorzata Marciniak;Agnieszka Mykowiecka	2011			natural language processing;computer science;data mining;information retrieval	NLP	-49.269600794959814	-68.62091494897047	114199
2cc9f7698e74a92eee86f4e0e4b8eba07dd9dd4b	a data-driven approach for quality assessment of radiologic interpretations	information storage and retrieval;mammography;pathology;quality improvement;radiology	"""Given the increasing emphasis on delivering high-quality, cost-efficient healthcare, improved methodologies are needed to measure the accuracy and utility of ordered diagnostic examinations in achieving the appropriate diagnosis. Here, we present a data-driven approach for performing automated quality assessment of radiologic interpretations using other clinical information (e.g., pathology) as a reference standard for individual radiologists, subspecialty sections, imaging modalities, and entire departments. Downstream diagnostic conclusions from the electronic medical record are utilized as """"truth"""" to which upstream diagnoses generated by radiology are compared. The described system automatically extracts and compares patient medical data to characterize concordance between clinical sources. Initial results are presented in the context of breast imaging, matching 18 101 radiologic interpretations with 301 pathology diagnoses and achieving a precision and recall of 84% and 92%, respectively. The presented data-driven method highlights the challenges of integrating multiple data sources and the application of information extraction tools to facilitate healthcare quality improvement."""	101 mouse;concordance (publishing);cost efficiency;downstream (software development);electronic health records;electronics, medical;information extraction;interpretation process;medical records;patients;precision and recall;quality of health care;radiology;reference standards	William Hsu;Simon X. Han;Corey W. Arnold;Alex A. T. Bui;Dieter R. Enzmann	2016	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocv161	medicine;pathology	ML	-50.78830914053747	-68.23814096184229	114569
fb98bf5a274404ff01873907d6249a2a41ba84a2	big data, big problems: a healthcare perspective	big data;challenges;costs;healthcare;opportunities	"""Much has been written on the benefits of big data for healthcare such as improving patient outcomes, public health surveillance, and healthcare policy decisions. Over the past five years, Big Data, and the data sciences field in general, has been hyped as the """"Holy Grail"""" for the healthcare industry promising a more efficient healthcare system with the promise of improved healthcare outcomes. However, more recently, healthcare researchers are exposing the potential and harmful effects Big Data can have on patient care associating it with increased medical costs, patient mortality, and misguided decision making by clinicians and healthcare policy makers. In this paper, we review the current Big Data trends with a specific focus on the inadvertent negative impacts that Big Data could have on healthcare, in general, and specifically, as it relates to patient and clinical care. Our study results show that although Big Data is built up to be as a the """"Holy Grail"""" for healthcare, small data techniques using traditional statistical methods are, in many cases, more accurate and can lead to more improved healthcare outcomes than Big Data methods. In sum, Big Data for healthcare may cause more problems for the healthcare industry than solutions, and in short, when it comes to the use of data in healthcare, """"size isn't everything."""""""	big data;cns disorder;decision making;financial cost;health care;patients;public health surveillance;science;benefit	Mowafa Said Househ;Bakheet Aldosari;Abdullah Alanazi;André Kushniruk;Elizabeth M. Borycki	2017	Studies in health technology and informatics	10.3233/978-1-61499-781-8-36	data science;data mining;health care;big data;medicine	ML	-58.82909612624232	-68.18620033701818	115267
564f2864febd076cd190651118d6cd09cbe66b1c	using natural language processing to facilitate medical record abstraction in epidemiological studies			natural language processing	Carlton Moore;Kyle Shaffer;Anna Kucharska-Newton;Stephanie W. Haas;Gerardo Heiss	2015			medical record;natural language processing;abstraction;artificial intelligence;computer science;epidemiology	AI	-49.86000248916545	-67.78486377212592	116318
765b70e6a3ce2cd1151d5dd21258fe0f3ab072d7	smash: a data-driven informatics method to assist experts in characterizing semantic heterogeneity among data elements.		"""Semantic heterogeneity (SH) is detrimental to data interoperability and integration in healthcare. Assessing SH is difficult, yet fundamental to addressing the problem. Using expert-based and data-driven methods we assessed SH among HIV-associated data elements (DEs). Using Clinicaltrials.gov, we identified and obtained eight data dictionaries, and created a DE inventory. We vectorized DEs by study, and developed a new method, String Metric-assisted Assessment of Semantic Heterogeneity (SMASH), to find DEs: similar in An and Bn, unique to An, and unique to Bn. An HIV expert assessed pairs for semantic equivalence. Heterogeneous DEs were either semantically-equivalent/syntactically-different (HIV-positive/HIV+/Seropositive), or syntactically-equivalent/semantically-different (""""Partner"""" [sexual]/""""Partner""""[relationship]). Context of usage was considered. SMASH aided identification of SH. Of 1,175 DE from pairs, 1,048 (87%) were semantically heterogeneous and 127 (13%) were homogeneous. Most heterogeneous pairs (97%) were semantically-equivalent/syntactically-different. Expert-based and data-driven methods are complementary for assessing SH, especially among semantically-equivalent/syntactically-different DE. Similar expert-based/data-driven solutions are recommended for resolving SH."""		William Brown;Chunhua Weng;David K. Vawdrey;Alex Carballo-Dieguez;Suzanne Bakken	2016	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;data mining;database;information retrieval	Comp.	-49.12611796576807	-66.9635441472606	116837
9b8743e756a9a82f33f1fa6538d0245339882efe	nlp based congestive heart failure case finding: a prospective analysis on statewide electronic medical records	sensitivity and specificity;vocabulary controlled;data mining;random forests;prevalence;heart failure;prospective studies;decision support systems clinical;reproducibility of results;prospective validation;algorithms;pattern recognition automated;humans;congestive heart failure;electronic medical record;natural language processing;electronic health records;maine	BACKGROUND In order to proactively manage congestive heart failure (CHF) patients, an effective CHF case finding algorithm is required to process both structured and unstructured electronic medical records (EMR) to allow complementary and cost-efficient identification of CHF patients.   METHODS AND RESULTS We set to identify CHF cases from both EMR codified and natural language processing (NLP) found cases. Using narrative clinical notes from all Maine Health Information Exchange (HIE) patients, the NLP case finding algorithm was retrospectively (July 1, 2012-June 30, 2013) developed with a random subset of HIE associated facilities, and blind-tested with the remaining facilities. The NLP based method was integrated into a live HIE population exploration system and validated prospectively (July 1, 2013-June 30, 2014). Total of 18,295 codified CHF patients were included in Maine HIE. Among the 253,803 subjects without CHF codings, our case finding algorithm prospectively identified 2411 uncodified CHF cases. The positive predictive value (PPV) is 0.914, and 70.1% of these 2411 cases were found to be with CHF histories in the clinical notes.   CONCLUSIONS A CHF case finding algorithm was developed, tested and prospectively validated. The successful integration of the CHF case findings algorithm into the Maine HIE live system is expected to improve the Maine CHF care.	algorithm;appendix;b. j. fogg;bourne shell;cdisc sdtm case finding terminology;compactflash;congestive heart failure;consultation;cost efficiency;dd (unix);deploy;electronic health records;electronics, medical;excalibur: morgana's revenge;experiment;gcy gene;google analytics;haplogroup cz (mtdna);haxe;health information exchange;host-based intrusion detection system;hypoxic-ischemic encephalopathy;kgs go server;lz77 and lz78;lib sh;limewire;manuscripts;nx bit;natural language processing;note (document);numerical methods for ordinary differential equations;patients;positive predictive value of diagnostic test;prospective search;reln gene;rl (complexity);revision procedure;software deployment;subgroup;tooth agenesis, selective, 5;xbl;holder;interest;population health	Yue Wang;Jin Luo;Shiying Hao;Haihua Xu;Andrew Young Shin;Bo Jin;Rui Liu;Xiaohong Deng;Lijuan Wang;Le Zheng;Yifan Zhao;Chunqing Zhu;Zhongkai Hu;Changlin Fu;Yanpeng Hao;Yingzhen Zhao;Yunliang Jiang;Dorothy Dai;Devore S. Culver;Shaun T. Alfreds;Todd Rogow;Frank Stearns;Karl G. Sylvester;Eric Widen	2015	International journal of medical informatics	10.1016/j.ijmedinf.2015.06.007	prospective cohort study;random forest;medicine;pathology;prevalence;computer science;data mining	ML	-49.119223604383976	-69.92065394239403	117342
eac0f29318a165282dae123dda7eb5653c8e51a7	post-coordination in practice: evaluating compositional terminological system-based registration of icu reasons for admission	post coordination;electronic patient record;electronic medical records;terminological system;evaluation;electronic medical record	BACKGROUND Re-use of patient data from an electronic patient record heavily relies on structured, coded data. Terminological systems (TSs) are meant to support this. TSs are more commonly implemented nowadays, especially those that support post-coordination. The effect of using a compositional TS on correctness and specificity of captured patient data is unknown.   OBJECTIVE To evaluate the agreement between free-text reasons for admission to intensive care captured in a patient data management system (PDMS) and reasons for admission that were recorded using a compositional TS embedded in the PDMS.   METHODS Observational study comparing pairs of free-text reasons for admission to intensive care with reasons for admission that were recorded using a compositional TS. Both reasons for admission were captured in the PDMS by clinicians during regular care practice. Each pair was judged as exact match, partial match or mismatch by two independent raters. Partial matches were further analyzed to investigate whether free-text or TS-based reasons for admissions included more detail and whether these differences could be explained by the content, the interface of the TS or by user or usability characteristics.   RESULTS Eleven percent of the pairs matched exactly, 79% of the pairs matched partially and 10% of the pairs did not match. Compared to free-text registration TS-based registration resulted in more detail for 21% of the partial matches, in less detail for 43% of the partial matches and in 36% of the partial matches some detail was added while at the same time other detail was lacking. In 65% of the cases in which the TS-based registration lacked some detail, this detail was available in the content of the TS. Physicians who used the TS occasionally had a significantly higher percentage of mismatches.   CONCLUSION In practice, post-coordination leads to information with different detail but a level of detail comparable to free-text registration of reasons for admission. Details missing in the TS-based reasons for admission were most often available in the TS, indicating that user interaction with the system is more of an impediment than the contents of the TS.		Nicolette de Keizer;Ferishta Bakhshi-Raiez;Evert de Jonge;Ronald Cornet	2008	International journal of medical informatics	10.1016/j.ijmedinf.2008.05.002	simulation;medicine;evaluation;data mining;management	Web+IR	-51.714982390384165	-68.14739020047573	119619
91bf37141962d02ab89b42a21addf370728cd159	about the language of hungarian discharge reports		Concerning use of terminological systems translated to any particular language or computer assisted translation of patient records the daily medical language has to be taken into account rather than the pure academic language. The aim of this study to investigate the language of the diagnosis of the Hungarian hospital discharge reports. Word of collection of discharge diagnoses were categorised according to the language and form (normal words, abbreviations, acronyms). It was found that Latin is still a dominant, but the language is rather a mixture, where local jargon, non standardised abbreviations occur frequently. Some signs show, that discharge reports are not used primarily as a tool of communication of physicians, rather serve local administrative purposes.		György Surján;Gergely Héja	2003	Studies in health technology and informatics	10.3233/978-1-60750-939-4-869	natural language processing;artificial intelligence;medicine	NLP	-48.814504159501524	-68.91344198876493	119802
6e36e2093288a3f0dfa609667ac20e3c4748d3d5	development and evaluation of an ensemble resource linking medications to their indications	medlineplus;rxnorm;pharmaceutical preparations;internet;drug therapy;dictionaries as topic;natural language processing;electronic health records	OBJECTIVE To create a computable MEDication Indication resource (MEDI) to support primary and secondary use of electronic medical records (EMRs).   MATERIALS AND METHODS We processed four public medication resources, RxNorm, Side Effect Resource (SIDER) 2, MedlinePlus, and Wikipedia, to create MEDI. We applied natural language processing and ontology relationships to extract indications for prescribable, single-ingredient medication concepts and all ingredient concepts as defined by RxNorm. Indications were coded as Unified Medical Language System (UMLS) concepts and International Classification of Diseases, 9th edition (ICD9) codes. A total of 689 extracted indications were randomly selected for manual review for accuracy using dual-physician review. We identified a subset of medication-indication pairs that optimizes recall while maintaining high precision.   RESULTS MEDI contains 3112 medications and 63 343 medication-indication pairs. Wikipedia was the largest resource, with 2608 medications and 34 911 pairs. For each resource, estimated precision and recall, respectively, were 94% and 20% for RxNorm, 75% and 33% for MedlinePlus, 67% and 31% for SIDER 2, and 56% and 51% for Wikipedia. The MEDI high-precision subset (MEDI-HPS) includes indications found within either RxNorm or at least two of the three other resources. MEDI-HPS contains 13 304 unique indication pairs regarding 2136 medications. The mean±SD number of indications for each medication in MEDI-HPS is 6.22 ± 6.09. The estimated precision of MEDI-HPS is 92%.   CONCLUSIONS MEDI is a publicly available, computable resource that links medications with their indications as represented by concepts and billing codes. MEDI may benefit clinical EMR applications and reuse of EMR data for research.	code;computable function;dual;electronic health records;electronic billing;electronics, medical;excalibur: morgana's revenge;extraction;hantavirus infections;haxe;hermanski-pudlak syndrome;holder device component;international classification of diseases;lrsam1 gene;largest;lattice boltzmann methods;license;mandibular right second molar tooth;manuscripts;medlineplus health topics;natural language processing;objective-c;ontology;peer review;precision and recall;quantum well;randomness;refinement (computing);reuse (action);reverse monte carlo;revision procedure;rxnorm;subgroup;typed assembly language;unified medical language system;united states national institutes of health;wikipedia;interest	Wei-Qi Wei;Robert Michael Cronin;Hua Xu;Thomas A. Lasko;Lisa Bastarache;Joshua C. Denny	2013		10.1136/amiajnl-2012-001431	pharmacotherapy;natural language processing;the internet;medicine;computer science;data mining;database;rxnorm;world wide web;information retrieval	Web+IR	-48.925043414487064	-69.98249402961271	119956
a19e389d89837d34a6a58e796a0d5e142997af16	computer-assisted expert case definition in electronic health records	insurance claims data;safety surveillance;case definition;health outcomes;electronic health records	PURPOSE To describe how computer-assisted presentation of case data can lead experts to infer machine-implementable rules for case definition in electronic health records. As an illustration the technique has been applied to obtain a definition of acute liver dysfunction (ALD) in persons with inflammatory bowel disease (IBD).   METHODS The technique consists of repeatedly sampling new batches of case candidates from an enriched pool of persons meeting presumed minimal inclusion criteria, classifying the candidates by a machine-implementable candidate rule and by a human expert, and then updating the rule so that it captures new distinctions introduced by the expert. Iteration continues until an update results in an acceptably small number of changes to form a final case definition.   RESULTS The technique was applied to structured data and terms derived by natural language processing from text records in 29,336 adults with IBD. Over three rounds the technique led to rules with increasing predictive value, as the experts identified exceptions, and increasing sensitivity, as the experts identified missing inclusion criteria. In the final rule inclusion and exclusion terms were often keyed to an ALD onset date. When compared against clinical review in an independent test round, the derived final case definition had a sensitivity of 92% and a positive predictive value of 79%.   CONCLUSION An iterative technique of machine-supported expert review can yield a case definition that accommodates available data, incorporates pre-existing medical knowledge, is transparent and is open to continuous improvement. The expert updates to rules may be informative in themselves. In this limited setting, the final case definition for ALD performed better than previous, published attempts using expert definitions.	adrenoleukodystrophy;atomic layer deposition;classification;electronic health records;exception handling;exclusion;inference;inflammatory bowel diseases;information;intestinal diseases;intestines;ion beam deposition;irritable bowel syndrome;iteration;liver diseases;natural language processing;onset (audio);positive predictive value of diagnostic test;rule (guideline);sampling (signal processing);scientific publication	Alexander Muir Walker;Xiaofeng Zhou;Ashwin N. Ananthakrishnan;Lisa S. Weiss;Rongjun Shen;Rachel E. Sobel;Andrew Bate;Robert F. Reynolds	2016	International journal of medical informatics	10.1016/j.ijmedinf.2015.10.005	medicine;artificial intelligence;data science;data mining;database;case definition;management;statistics	ML	-53.13139252517313	-70.26937881453556	120680
735b9176a862456bc13d4740d3e74427669c451b	arthritis quality indicators for the veterans administration: implications for electronic data collection, storage format, quality assessment, and clinical decision support	vocabulary controlled;data collection;quality of health care;medical records systems computerized;quality indicators health care;medical audit;humans;user computer interface;automatic data processing;arthritis rheumatoid;united states department of veterans affairs	The Veterans Administration (VA) uses information technology and performance measures to improve quality and efficiency. The VA stores all patient data electronically. Manual quality assessment audits are performed every three months. They are time consuming and expensive. Automated reviews would be more efficient. But the patient records are neither sufficiently coded nor structured to allow for full machine interpretability. Evidence-based rheumatology quality indicators have been proposed for inclusion in the quality data set. Automated reviews for some conditions would be possible with modification to some VA electronic data entry screens and to the underlying data repository. This effort would risk the imposition of untenable data entry and workflow burdens upon clinicians. This paper outlines some specific considerations for one disease, rheumatoid arthritis.	chart evaluation by healthcare professional;clinical decision support system;conferences;data collection;evaluation procedure;humans;mind;open system (computing);outlines (document);patients;research data archiving;review [publication type];rheumatoid arthritis;rheumatology specialty;traffic collision avoidance system;united states department of veterans affairs	Carl A. Williams;Angelia D. Mosley-Williams;J. Marc Overhage	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;data mining;biological engineering	Embedded	-56.229348586760146	-67.39815025820602	120708
3f6912dedd32ab920c7547428e1b9a10f5107bae	rating quality in metadata harvesting	tool;metadata;open archive initiative;quality;harvesting	The quality of the data and metadata affects the interoperability of the collections and the quality of all processing. Our metadata quality metric helps the metadata harvester collection administrators detecting and improving the weaknesses of their metadata, and harvesters locating the most problematic collections, in terms of metadata quality, and prompt their administrators to improve their metadata. We extended and used an adaptive quantitative metadata quality metric and a tool to implement it. In controlled values, their value distribution is considered, and in free text values the length of their description. Moreover, we also consider additional information in the OAI-PMH XML responces, that is not normally mapped in metadata elements, but still contains metadata information, such as XML attributes. We used the tool to make quality observations, to examine collections for patterns and irregularities and to produce the appropriate advice for the collection administrators. Some of these observations are demonstrated here. We compared the reported quality over a 3-year period, to get a general quantitative and qualitative feeling of the diversity in the record descriptions, and the changes in their quality during their lifetime. We verified the assumption that the quality increases over time: usually by a tiny amount, in every collection, and by a lot on a small number of collections. Also, the lower quality collections are the ones that stop responding and vanish.	interoperability;sensor;vanish (computer science);xml	Sarantos Kapidakis	2015		10.1145/2769493.2769512	computer science;database;metadata;world wide web;information retrieval;metadata repository	Web+IR	-59.61598843528494	-70.79393699889611	120979
23f70b6c5e8e3c2e2cc08d9c53b5d6e46bae355a	whose voices are heard in patient safety incident reports?	bioinformatics;biomedical research	Patient safety incident reporting systems are used to monitor adverse events, generate information for risk management and to improve patient safety. A number of electronic reporting systems have been developed, but their data elements appear relatively similar. An inductive data analysis was carried out to find out especially what is the content of descriptions of contributing factors of adverse events. The data consisted of incident reports entered in a hospital based reporting system in the years 2008-2010. Overall, 82 reports of 785 contained free text information about patients' and relatives' involvement in the events reported by staff. We found that patients themselves noticed almost half of these incidents. Of the incidents they noticed, most resulted in moderate harm.	adverse event domain;contain (action);contribution;description;patients;risk management;voice	Kaija Saranto;David W. Bates;Minna Mykkänen;Mikko Härkönen;Merja Miettinen	2012	NI 2012 : 11th International Congress on Nursing Informatics, June 23-27, 2012, Montreal, Canada. International Congress in Nursing Informatics			DB	-59.837232001118494	-66.1496507028431	121482
481f43d76f86923bbe0b36ddf75d23e68f4e3b09	evaluating a medical error taxonomy	product development;human factors;classification;controlled vocabulary	Healthcare has been slow in using human factors principles to reduce medical errors. The Center for Devices and Radiological Health (CDRH) recognizes that a lack of attention to human factors during product development may lead to errors that have the potential for patient injury, or even death. In response to the need for reducing medication errors, the National Coordinating Council for Medication Errors Reporting and Prevention (NCC MERP) released the NCC MERP taxonomy that provides a standard language for reporting medication errors. This project maps the NCC MERP taxonomy of medication error to MedWatch medical errors involving infusion pumps. Of particular interest are human factors associated with medical device errors. The NCC MERP taxonomy of medication errors is limited in mapping information from MEDWATCH because of the focus on the medical device and the format of reporting.	cessation of life;clinical trial expedited safety report;human factors and ergonomics;medical devices;neural correlates of consciousness;new product development;patients;taxonomy;pump (device)	Juliana J. Brixey;Todd R. Johnson;Jiajie Zhang	2002	Proceedings. AMIA Symposium		new product development;medwatch;data mining;controlled vocabulary;health care;medicine	Visualization	-58.00811036297872	-66.81168569119076	122057
f8e49dca171a0ebc9cd38f6b30363a62c380e174	measuring the quality of medical records: a method for comparing completeness and correctness of clinical encounter data.	methods;quality assurance health care;physician patient relations;medical records systems computerized;humans;pilot projects	This paper explores the attributes of quality in recorded clinical encounter data, examines issues in measuring these attributes, and describes a method for measuring two attributes, completeness and correctness. The method is defined in the context of computer-based records and is demonstrated in a pilot study. Videotaped physician-patient encounters and an empiric process of determining a gold standard for content are used. The methodology was found to be feasible. Problems encountered during the pilot study can be remedied.		Judith R. Logan;Paul N. Gorman;Blackford Middleton	2001	Proceedings. AMIA Symposium		medicine;data mining;database;management science	SE	-54.13468262733033	-67.40365307557299	122150
94bb60942afa52658a74b4875cc86a685c9b5704	automatic classification of mammography reports by bi-rads breast tissue composition class		Because breast tissue composition partially predicts breast cancer risk, classification of mammography reports by breast tissue composition is important from both a scientific and clinical perspective. A method is presented for using the unstructured text of mammography reports to classify them into BI-RADS breast tissue composition categories. An algorithm that uses regular expressions to automatically determine BI-RADS breast tissue composition classes for unstructured mammography reports was developed. The algorithm assigns each report to a single BI-RADS composition class: 'fatty', 'fibroglandular', 'heterogeneously dense', 'dense', or 'unspecified'. We evaluated its performance on mammography reports from two different institutions. The method achieves >99% classification accuracy on a test set of reports from the Marshfield Clinic (Wisconsin) and Stanford University. Since large-scale studies of breast cancer rely heavily on breast tissue composition information, this method could facilitate this research by helping mine large datasets to correlate breast composition with other covariates.	bi-rads;breast fibroglandular tissue;categories;class;collections (publication);mammary gland parenchyma;mammary neoplasms;mammography;ninety nine;regular expression;test set;algorithm	Bethany Percha;Houssam Nassif;Jafi A. Lipson;Elizabeth S. Burnside;Daniel L. Rubin	2012	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2011-000607	medicine;pathology;gynecology	AI	-48.7163255059857	-70.07584218633808	122471
afd0f02054fdf4490443755cb56bb1a331f52b2f	developing ontological background knowledge for biomedicine	004 informatik	Biomedicine is an impressively fast developing, interdisciplinary field of research. To control the growing volumes of biomedical data, ontologies are increasingly used as common organization structures. Biomedical ontologies describe domain knowledge in a formal, computationally accessible way. They serve as controlled vocabularies and background knowledge in applications dealing with the integration, analysis and retrieval of heterogeneous types of data. The development of biomedical ontologies, however, is hampered by specific challenges. They include the lack of quality standards, resulting in very heterogeneous resources, and the decentralized development of biomedical ontologies, causing the increasing fragmentation of domain knowledge across them. In the first part of this thesis, a life cycle model for biomedical ontologies is developed, which is intended to cope with these challenges. It comprises the stages “requirements analysis”, “design and implementation”, “evaluation”, “documentation and release” and “maintenance”. For each stage, associated subtasks and activities are specified. To promote quality standards for biomedical ontology development, an emphasis is set on the evaluation stage. As part of it, comprehensive evaluation procedures are specified, which allow to assess the quality of ontologies on various levels. To tackle the issue of knowledge fragmentation, the life cycle model is extended to also cover ontology alignments. Ontology alignments specify mappings between related elements of different ontologies. By making potential overlaps and similarities between ontologies explicit, they support the integration of ontologies and help reduce the fragmentation of knowledge. In the second part of this thesis, the life cycle model for biomedical ontologies and alignments is validated by means of five case studies. As a result, they confirm that the model is effective. Four of the case studies demonstrate that it is able to support the development of useful new ontologies and alignments. The latter facilitate novel natural language processing and bioinformatics applications, and in one case constitute the basis of a task of the “BioNLP shared task 2013”, an international challenge on biomedical information extraction. The fifth case study shows that the presented evaluation procedures are an effective means to check and improve the quality of ontology alignments. Hence, they support the crucial task of quality assurance of alignments, which are themselves increasingly used as reference standards in evaluations of automatic ontology alignment systems. Both, the presented life cycle model and the ontologies and alignments that have resulted from its validation improve information and knowledge management in biomedicine and thus promote biomedical research.	bioinformatics;biomedical text mining;controlled vocabulary;documentation;fork (software development);fragmentation (computing);information extraction;knowledge management;natural language processing;ontology (information science);ontology alignment;requirement;requirements analysis;sequence alignment;trusted computer system evaluation criteria;web ontology language	Elena Beisswanger	2013			idef5;computer science;bioinformatics;knowledge management;data mining	Web+IR	-50.457998811130494	-67.37739264915383	122855
ed912a23574f2e5c1a00ea133a0e61e4816ef250	research paper: developing optimal search strategies for detecting clinically sound studies in medline	internal medicine;clinical application;search strategy;gold standard;indexing terms;selection combining;clinical study;operating characteristic	"""OBJECTIVE To develop optimal MEDLINE search strategies for retrieving sound clinical studies of the etiology, prognosis, diagnosis, prevention, or treatment of disorders in adult general medicine.   DESIGN Analytic survey of operating characteristics of search strategies developed by computerized combinations of terms selected to detect studies meeting basic methodologic criteria for direct clinical use in adult general medicine.   MEASURES The sensitivities, specificities, precision, and accuracy of 134,264 unique combinations of search terms were determined by comparison with a manual review of all articles (the """"gold standard"""") in ten internal medicine and general medicine journals for 1986 and 1991.   RESULTS Less than half of the studies of the topics of interest met basic criteria for scientific merit for testing clinical applications. Combinations of search terms reached peak sensitivities of 82% for sound studies of etiology, 92% for prognosis, 92% for diagnosis, and 99% for therapy in 1991. Compared with the best single terms, multiple terms increased sensitivity for sound studies by over 30% (absolute increase), but with some loss of specificity when sensitivity was maximized. For 1986, combinations reached peak sensitivities of 72% for etiology, 95% for prognosis, 86% for diagnosis, and 98% for therapy. When search terms were combined to maximize specificity, over 93% specificity was achieved for all purpose categories in both years. Compared with individual terms, combined terms achieved near-perfect specificity that was maintained with modest increases in sensitivity in all purpose categories except therapy. Increases in accuracy were achieved by combining terms for all purpose categories, with peak accuracies reaching over 90% for therapy in 1986 and 1991.   CONCLUSIONS The retrieval of studies of important clinical topics cited in MEDLINE can be substantially enhanced by selected combinations of indexing terms and textwords."""	categories;forecast of outcome;indexes;internal medicine specialty;journal;medline;ninety nine;sensitivity and specificity;sensor;general practice (field)	R. Brian Haynes;Nancy L. Wilczynski;K. Ann McKibbon;Cynthia J. Walker-Dilks;John C. Sinclair	1994	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.1994.95153434	alternative medicine;index term;medicine;pathology;gold standard;data mining;statistics	Web+IR	-56.83406405329126	-69.22778922476988	123033
ad6264cd81d90fce80b5a0d3e4a01556d66eea09	validation for accuracy of cancer diagnosis in electronic medical records using a text mining method		To validate the accuracy of data in electronic medical record, we compared cancer diagnosis and key words in pathologic reports of cancer patients in a tertiary hospital, using text mining method. We investigated in fourteen kinds of cancers that had highest incidence rates in Korea. Approximately two-third (71.0%) of total patients had right match in cancer diagnosis with pathologic report. The ratio of concurrence was the highest (86.3%) in thyroid cancer patients, however, the ratio was the lowest (49.9%) in liver cancer patients. To prevent the errors in data input, a systematic alarm and feedback to clinicians should be required.	concurrence (quantum computing);electronic health records;electronics, medical;feedback;fourteen;incidence matrix;liver and intrahepatic biliary tract carcinoma;medical records;neoplasms;patients;tertiary care centers;text mining;thyroid carcinoma;cancer diagnosis	Yura Lee;Soo-Yong Shin;Sung-Min Ahn;Jae Ho Lee;Woo Sung Kim	2015	Studies in health technology and informatics	10.3233/978-1-61499-564-7-882	data mining;medical record;incidence (epidemiology);thyroid cancer;cancer;liver cancer;medicine;text mining	HCI	-56.83591083492414	-67.63850080015601	124108
33a60df1f10353faade430c6ac95c5552af20959	developing optimal search strategies for detecting sound clinical prediction studies in medline	sensitivity and specificity;medline;evidence based practice;search strategy;gold standard;medical subject headings;risk assessment;humans;diagnosis;prognosis;information storage and retrieval;main outcome measure;causality;literature search;health care	"""BACKGROUND The gaining interest in the use of clinical prediction guides as an aid for helping clinicians make effective front-line decisions, together with the increasing emphasis on evidence-based practice, underscores the need for accurate identification of sound clinical prediction studies. Despite the growing use of clinical prediction guides, little work has been done on identifying optimal literature search filters for retrieving these types of studies. The current study extends our earlier work, on developing optimal search strategies, to include clinical prediction guides.   OBJECTIVE To develop optimal search strategies for detecting methodologically sound clinical prediction studies in MEDLINE in the publishing year 2000.   DESIGN Comparison of the retrieval performance of methodologic search strategies in MEDLINE with a manual review (""""gold standard"""") of each article for each issue of 162 core health care journals for the year 2000.   METHODS 6 experienced research assistants who had been trained and intensively calibrated reviewed all issues of 162 journals for the publishing year 2000. Each article was classified for format, interest, purpose, and methodologic rigor. Search strategies were developed for all purpose categories, including studies of clinical prediction guides.   MAIN OUTCOME MEASURES The sensitivity (recall), specificity, precision, and accuracy of single and combinations of search terms.   RESULTS 39% of original studies classified as a clinical prediction guide were methodologically sound. Combinations of terms reached peak sensitivities of 95%. Compared with the best single term, a three-term strategy increased sensitivity for sound studies by 17% (absolute increase), but with some loss of specificity when sensitivity was maximized. When search terms were combined to optimize sensitivity and specificity, these values reached or were close to 90%.   CONCLUSION Several search strategies can enhance the retrieval of sound clinical prediction studies."""	categories;classification;journal;medline;rigor - temperature-associated observation;sensitivity and specificity;sensor	Sharon S.-L. Wong;Nancy L. Wilczynski;R. Brian Haynes;Ravi Ramkissoonsingh	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;data science;data mining;information retrieval	Comp.	-56.834987748247094	-69.21872217555735	124797
1063df5a25a0eab224c35bba95e17c500a588652	galen based formal representation of icd10	international classification of diseases;computer assisted coding;galen;description logic;ontology;icd10	OBJECTIVES The main objective is to create a knowledge-intensive coding support tool for the International Classification of Diseases (ICD10), which is based on formal representation of ICD10 categories. Beyond this task the resulting ontology could be reused in various ways. Decidability is an important issue for computer-assisted coding; consequently the ontology should be represented in description logic.   METHODS The meaning of the ICD10 categories is represented using the GALEN Core Reference Model. Due to the deficiencies of its representation language (GRAIL) the ontology is transformed to the quasi-standard OWL. A test system which extracts disease concepts and classifies them to ICD10 categories has been implemented in Prolog to verify the feasibility of the approach.   RESULTS The formal representation of the first two chapters of ICD10 (infectious diseases and neoplasms) has been almost completed. The constructed ontology has been converted to OWL DL. The test system successfully identified diseases in medical records from gastrointestinal oncology (84% recall, however precision is only 45%). The classifier module is still under development. Due to the experiences gained during the modelling, in the future work FMA is going to be used as anatomical reference ontology.		Gergely Héja;György Surján;Gergely Lukácsy;Peter Pallinger;Miklós Gergely	2007	International journal of medical informatics	10.1016/j.ijmedinf.2006.07.008	upper ontology;description logic;bibliographic ontology;computer science;ontology;artificial intelligence;theoretical computer science;ontology;ontology-based data integration;process ontology;algorithm;suggested upper merged ontology	Web+IR	-50.268425239148144	-66.25672730996936	125178
248d47c701595e25dcea325c48129153a51d8e17	seasonal variation in an at-home telemonitoring trial		This paper aims to present findings on seasonal variation in a recently completed Commonwealth Scientific and Industrial Research Organization (CSIRO) national trial of home telemonitoring of patients with chronic conditions, carried out at five locations along the east coast of Australia. Patients in this trial were selected from a list of eligible patients living with a range of chronic conditions. Each test patient was case matched with at least one control patient. A total of 114 test patients and 173 control patients were available in this trial. However, of the 287 patients, we only considered subjects who had one or more admissions in the years 2010–2012. Three different groups were analyzed because of substantially different climates, i.e., Queensland (QLD), Australian Capital Territory & Victoria (ACT + VIC), and Tasmania (TAS). Time series data were analyzed using linear regression for a period of 3 years before the intervention in order to obtain an average seasonal variation pattern.	capital punishment;climate;hospital admission;mos technology vic-ii;patients;pentalogy of cantrell;seasonality;thermal-assisted switching;time series;victoria (3d figure);ifosfamide/lomustine/vinblastine	Ahmadreza Argha;Branko George Celler	2018	2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)	10.1109/EMBC.2018.8512495	computer vision;optometry;artificial intelligence;commonwealth;seasonality;linear regression;computer science	Visualization	-60.667493481422404	-67.3397146064092	126689
01e327b5e3c9a3b60f2c9079244ad66d36568d80	using ontologies to identify patients with diabetes in electronic health records	knowledgebase management;diabetes mellitus;ehealth;ontology_based database access;ontology;electronic health records	This paper describes a work in progress that explores the applicability of ontologies to solve problems in the medical domain. We investigate whether it is feasible to use ontologies and ontology-based data access (OBDA) to automate common clinical tasks faced by general practitioners (GPs), which are labor-intensive and error prone in terms of relevant information retrieved from electronic health records (EHRs). Our study aims to improve the selection of diabetes patients for clinical trials or medical research. The biggest impediment to automating such clinical tasks is the essential bridging of the semantic gaps between existing patient data in EHRs, such as reasons for visit, chronic conditions and diagnoses, pathology tests and prescriptions stored in general practice EHRs (GPEHR), and the ways which medical researchers or GPs interpret those records. Our current understanding is that automated identification of diabetes patients can be specified systematically as a solution supported by semantic retrieval. We detail the challenges to building a realistic case study, which consists of solving issues related to conceptualization of data and domain context, integration of different datasets, ontology creation based on the SNOMED CT-AU® standard, mapping between existing data and ontology, and the challenge of data fitness for research use. Our prototype is based on data which scale to thirteen years of approximately 100,000 anonymous patient records from four general practices in south western Sydney.	bridging (networking);cognitive dimensions of notations;conceptualization (information science);data access;ontology (information science);prototype;systematized nomenclature of medicine	Hairong Yu;Siaw-Teng Liaw;Jane Taggart;Alireza Rahimi Khorzoughi	2013			epistemology;data science;ontology;data mining;ehealth;database;world wide web	HCI	-52.34601732891994	-66.61259023380028	127321
d05c7ffa70d313975b8ccd39a9c94aa5eb510b51	crowd control: effectively utilizing unscreened crowd workers for biomedical data annotation	logistic regression;ehr data;sentence classification;text annotations;crowdsourcing	Annotating unstructured texts in Electronic Health Records data is usually a necessary step for conducting machine learning research on such datasets. Manual annotation by domain experts provides data of the best quality, but has become increasingly impractical given the rapid increase in the volume of EHR data. In this article, we examine the effectiveness of crowdsourcing with unscreened online workers as an alternative for transforming unstructured texts in EHRs into annotated data that are directly usable in supervised learning models. We find the crowdsourced annotation data to be just as effective as expert data in training a sentence classification model to detect the mentioning of abnormal ear anatomy in radiology reports of audiology. Furthermore, we have discovered that enabling workers to self-report a confidence level associated with each annotation can help researchers pinpoint less-accurate annotations requiring expert scrutiny. Our findings suggest that even crowd workers without specific domain knowledge can contribute effectively to the task of annotating unstructured EHR datasets.	anatomic structures;annotation;behavior;crowdsourcing;ear diseases;electronic health records;machine learning;radiology;supervised learning	Anne Cocos;Ting Qian;Chris Callison-Burch;Aaron J. Masino	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.04.003	natural language processing;computer science;bioinformatics;data science;machine learning;data mining;database;logistic regression;world wide web;crowdsourcing	Web+IR	-49.790646259720674	-68.97243627138756	127821
9372655c991dfc0cefe71fa800f007f0f5a33a30	snap 診断 : 分析的な診断推論とは異なる直観的診断法について	hyperventilation;function decline;adverse drug events;treatable dementia	A training pack for A&E staff in spotting and treating whiplash injury has been issued by the British Association for A&E Medicine. Sponsored by the Association of British Insurers, it is being made available free of charge to A&E departments. Whiplash injuries cost insurers an estimated £700 million a year.		市川 元啓;山中 克郎	2003	Nursing standard (Royal College of Nursing (Great Britain) : 1987)	10.1007/978-0-387-35973-1_1226	psychiatry;anesthesia;diabetes mellitus	Vision	-60.48454789000865	-67.62700068668674	128379
357873ff2de008eae457744f1c86bb981ec797f8	auditing description-logic-based medical terminological systems by detecting equivalent concept definitions	controlled vocabulary;description logics;methods;algorithms;description logic;knowledge representation	OBJECTIVE To specify and evaluate a method for auditing medical terminological systems (TSs) based on detecting concepts with equivalent definitions. This method addresses two important problems: redundancy, where the same concept is represented more than once (described by different terms), and underspecification, where different concepts have the same representation and hence appear indistinguishable from each other.   DESIGN The auditing method is applicable for TSs that are or can be represented in a description logic (DL). The method relies on the assumption that concept definitions are non-primitive (i.e. they are regarded as providing necessary and sufficient conditions). Whereas this assumption may not hold for many definitions, it does serve the purpose of detecting sets of logically equivalent concepts by a DL reasoner. Such a set may include the same concept which is defined more than once and/or different concepts that are underspecified as they appear indistinguishable from each other by their represented properties. Analysis of these sets provides insight into the representation quality of concepts and provides hints at improving the TS.   MEASUREMENTS In our case study the method is applied to the DICE TS, a comprehensive TS in intensive care. It comprises about 2500 concepts and 40 properties and relations.   RESULTS In DICE we found four concepts that were defined twice. Furthermore, 100 sets were found containing more than 300 underspecified concepts. The sizes of these sets ranged from 2 to 13. Analysis revealed that many concepts can be more completely defined, either by adding existing relations, or by the introduction of new relations into the terminological system.   CONCLUSION The method proved both usable and valuable for auditing TSs. DL reasoning is fully automated and all equivalent concept definitions are systematically found. The resulting sets of equivalent concepts clearly point out which concept definitions are to be reviewed, as they contain duplicate definitions of a concept, and (inherently or unnecessarily) underspecified concepts.	addresses (publication format);definition;description logic;greater than;mpeg transport stream;medical records systems, computerized;semantic reasoner;sensor	Ronald Cornet;Ameen Abu-Hanna	2008	International journal of medical informatics	10.1016/j.ijmedinf.2007.06.008	knowledge representation and reasoning;description logic;computer science;artificial intelligence;theoretical computer science;data mining;algorithm	DB	-51.1747709661193	-67.5299095890106	128538
61e9342c62b99437eeeed9155d0ecef4b6ec36da	herald (health economics using routine anonymised linked data)	chronic disease;female;health informatics;outcome and process assessment health care;pedestrian safety;referral and consultation;poison control;retrospective studies;diagnostic tests routine;middle aged;injury prevention;data collection;male;general practitioners;safety literature;traffic safety;injury control;costs and cost analysis;information systems and communication service;home safety;injury research;safety abstracts;human factors;emergency medical services;adult;management of computing and information systems;occupational safety;safety;safety research;accident prevention;violence prevention;humans;bicycle safety;questionnaires;cost of illness;poisoning prevention;physician s practice patterns;falls;wales;ergonomics;hospitalization;suicide prevention;aged;spondylitis ankylosing	BACKGROUND Health economic analysis traditionally relies on patient derived questionnaire data, routine datasets, and outcomes data from experimental randomised control trials and other clinical studies, which are generally used as stand-alone datasets. Herein, we outline the potential implications of linking these datasets to give one single joined up data-resource for health economic analysis.   METHOD The linkage of individual level data from questionnaires with routinely-captured health care data allows the entire patient journey to be mapped both retrospectively and prospectively. We illustrate this with examples from an Ankylosing Spondylitis (AS) cohort by linking patient reported study dataset with the routinely collected general practitioner (GP) data, inpatient (IP) and outpatient (OP) datasets, and Accident and Emergency department data in Wales. The linked data system allows: (1) retrospective and prospective tracking of patient pathways through multiple healthcare facilities; (2) validation and clarification of patient-reported recall data, complementing the questionnaire/routine data information; (3) obtaining objective measure of the costs of chronic conditions for a longer time horizon, and during the pre-diagnosis period; (4) assessment of health service usage, referral histories, prescribed drugs and co-morbidities; and (5) profiling and stratification of patients relating to disease manifestation, lifestyles, co-morbidities, and associated costs.   RESULTS Using the GP data system we tracked about 183 AS patients retrospectively and prospectively from the date of questionnaire completion to gather the following information: (a) number of GP events; (b) presence of a GP 'drug' read codes; and (c) the presence of a GP 'diagnostic' read codes. We tracked 236 and 296 AS patients through the OP and IP data systems respectively to count the number of OP visits; and IP admissions and duration. The results are presented under several patient stratification schemes based on disease severity, functions, age, sex, and the onset of disease symptoms.   CONCLUSION The linked data system offers unique opportunities for enhanced longitudinal health economic analysis not possible through the use of traditional isolated datasets. Additionally, this data linkage provides important information to improve diagnostic and referral pathways, and thus helps maximise clinical efficiency and efficiency in the use of resources.	ankylosing spondylitis;clarify;code;data system;emoticon;health care;health services;hospital admission;linkage (software);linked data;medical economics;morbidity - disease rate;onset (audio);paget's disease, mammary;patient referral;patients;prospective search;questionnaire domain;silo (dataset);stratification;genetic linkage;inpatient	Muhammad J. Husain;Sinead Brophy;Steven Macey;Leila M. Pinder;Mark D. Atkinson;Roxanne Cooksey;Ceri J. Phillips;Stefan Siebert	2011		10.1186/1472-6947-12-24	questionnaire;medicine;suicide prevention;human factors and ergonomics;injury prevention;nursing;retrospective cohort study;emergency medicine;emergency medical services;medical emergency;statistics;data collection	ML	-58.765111384469364	-66.51354916422369	128682
4a7b82f57bf55e31cb8bba3b196cfa468c69f7fc	medical and transmission vector vocabulary alignment with schema.org	vocabulary alignment;querying;research vocabulary;ontology;question answering	Available biomedical ontologies and knowledge bases currently lack formal and standards-based interconnections between disease, disease vector, and drug treatment vocabularies. The PNNL Medical Linked Dataset (PNNL-MLD) addresses this gap. This paper describes the PNNLMLD, which provides a unified vocabulary and dataset of drug, disease, side effect, and vector transmission background information. Currently, the PNNL-MLD combines and curates data from the following research projects: DrugBank, DailyMed, Diseasome, DisGeNet, Wikipedia Infobox, Sider, and PharmGKB. The main outcomes of this effort are a dataset aligned to Schema.org, including a parsing framework, and extensible hooks ready for integration with selected medical ontologies. The PNNLMLD enables researchers more quickly and easily to query distinct datasets. Future extensions to the PNNL-MLD may include Traditional Chinese Medicine, broader interlinks across genetic structures, a larger thesaurus of synonyms and hypernyms, explicit coding of diseases and drugs across research systems, and incorporating vector-borne transmission vocabularies.	best practice;conceptualization (information science);data point;drugbank;global variable;knowledge base;linked data;ontology (information science);parsing;pharmgkb;schema.org;thesaurus;vocabulary;wikipedia	William Smith;Alan Chappell;Courtney Corley	2015			computer science;data science;data mining;information retrieval	AI	-48.98487015350117	-67.09093834974202	128905
0c0b238742d9c3bfcaf8eae721fdb7a3b1b942e0	development and preliminary evaluation of a prototype of a learning electronic medical record system	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	Electronic medical records (EMRs) are capturing increasing amounts of data per patient. For clinicians to efficiently and accurately understand a patient's clinical state, better ways are needed to determine when and how to display EMR data. We built a prototype system that records how physicians view EMR data, which we used to train models that predict which EMR data will be relevant in a given patient. We call this approach a Learning EMR (LEMR). A physician used the prototype to review 59 intensive care unit (ICU) patient cases. We used the data-access patterns from these cases to train logistic regression models that, when evaluated, had AUROC values as high as 0.92 and that averaged 0.73, supporting that the approach is promising. A preliminary usability study identified advantages of the system and a few concerns about implementation. Overall, 3 of 4 ICU physicians were enthusiastic about features of the prototype.	academic medical centers;electronic health records;excalibur: morgana's revenge;fifty nine;learning disorders;logistic regression;medical records systems, computerized;medical records, problem-oriented;patients;prototype;receiver operating characteristic;usability testing;intensive care unit	Andrew J. King;Gregory F. Cooper;Harry Hochheiser;Gilles Clermont;Shyam Visweswaran	2015	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;data science;data mining;biological engineering	Security	-55.55937924956443	-66.47653352307958	129482
66906bfebc5b91292e575c14271ee5f0938b7c66	strategies for handling missing clinical data for automated surgical site infection detection from the electronic health record	surgical site infections;missing data;electronic health records	Proper handling of missing data is important for many secondary uses of electronic health record (EHR) data. Data imputation methods can be used to handle missing data, but their use for analyzing EHR data is limited and specific efficacy for postoperative complication detection is unclear. Several data imputation methods were used to develop data models for automated detection of three types (i.e., superficial, deep, and organ space) of surgical site infection (SSI) and overall SSI using American College of Surgeons National Surgical Quality Improvement Project (NSQIP) Registry 30-day SSI occurrence data as a reference standard. Overall, models with missing data imputation almost always outperformed reference models without imputation that included only cases with complete data for detection of SSI overall achieving very good average area under the curve values. Missing data imputation appears to be an effective means for improving postoperative SSI detection using EHR clinical data.	area under curve;computer mouse;data collection;data model;dummy variable (statistics);electronic health records;experiment;geo-imputation;handling (psychology);missing data;occur (action);patients;reference standards;registries;silo (dataset);statistical imputation;surgical wound infection;the superficial	Zhen Hu;Genevieve B. Melton;Elliot G. Arsoniadis;Yan Wang;Mary R. Kwaan;György J. Simon	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.03.009	medicine;pathology;missing data;data mining;statistics	ML	-57.77807982279634	-67.17007731081478	130147
2478a2c1da807c9f34dd4f1dc74f4b524fc6654f	viewpoint paper: identifying patient smoking status from medical discharge records		"""The authors organized a Natural Language Processing (NLP) challenge on automatically determining the smoking status of patients from information found in their discharge records. This challenge was issued as a part of the i2b2 (Informatics for Integrating Biology to the Bedside) project, to survey, facilitate, and examine studies in medical language understanding for clinical narratives. This article describes the smoking challenge, details the data and the annotation process, explains the evaluation metrics, discusses the characteristics of the systems developed for the challenge, presents an analysis of the results of received system runs, draws conclusions about the state of the art, and identifies directions for future research. A total of 11 teams participated in the smoking challenge. Each team submitted up to three system runs, providing a total of 23 submissions. The submitted system runs were evaluated with microaveraged and macroaveraged precision, recall, and F-measure. The systems submitted to the smoking challenge represented a variety of machine learning and rule-based algorithms. Despite the differences in their approaches to smoking status identification, many of these systems provided good results. There were 12 system runs with microaveraged F-measures above 0.84. Analysis of the results highlighted the fact that discharge summaries express smoking status using a limited number of textual features (e.g., """"smok"""", """"tobac"""", """"cigar"""", Social History, etc.). Many of the effective smoking status identifiers benefit from these features."""	algorithm;annotation;discharger;f1 score;identifier;informatics (discipline);logic programming;machine learning;natural language processing;natural language understanding;patients;smoke;smoking status;status epilepticus;status register;viewpoint	Özlem Uzuner;Ira Goldstein;Yuan Luo;Isaac S. Kohane	2008	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2408	medicine;pathology;artificial intelligence;data science;nursing;data mining	NLP	-49.71422477705539	-69.10620274621084	130416
5d0abaeee4a329370221407d4f2f8925bd6fa0f9	automatic icd-10 classification of cancers from free-text death certificates	machine learning;cancer classification;natural language processing;death certificates	OBJECTIVE Death certificates provide an invaluable source for cancer mortality statistics; however, this value can only be realised if accurate, quantitative data can be extracted from certificates--an aim hampered by both the volume and variable nature of certificates written in natural language. This paper proposes an automatic classification system for identifying cancer related causes of death from death certificates.   METHODS Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. These features were used to train Support Vector Machine classifiers (one classifier for each cancer type). The classifiers were deployed in a cascaded architecture: the first level identified the presence of cancer (i.e., binary cancer/nocancer) and the second level identified the type of cancer (according to the ICD-10 classification system). A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure. In addition, detailed feature analysis was performed to reveal the characteristics of a successful cancer classification model.   RESULTS The system was highly effective at identifying cancer as the underlying cause of death (F-measure 0.94). The system was also effective at determining the type of cancer for common cancers (F-measure 0.7). Rare cancers, for which there was little training data, were difficult to classify accurately (F-measure 0.12). Factors influencing performance were the amount of training data and certain ambiguous cancers (e.g., those in the stomach region). The feature analysis revealed a combination of features were important for cancer type classification, with SNOMED CT concept and oncology specific morphology features proving the most valuable.   CONCLUSION The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.	automatic identification and data capture;ct scan;cancer death rate;certificate (record artifact);collections (publication);death certificates;extraction;f1 score;galaxy morphological classification;gastric tissue;grams;malignant neoplasms;n-gram;natural language;pet/ct scan;support vector machine;systematized nomenclature of medicine;test set;x-ray computed tomography;cancer classification;gram	Bevan Koopman;Guido Zuccon;Anthony N. Nguyen;Anton Bergheim;Narelle Grayson	2015	International journal of medical informatics	10.1016/j.ijmedinf.2015.08.004	speech recognition;computer science;data science;data mining	Comp.	-48.34557600079858	-69.78855343440611	130611
f39cbd3b7a45d06689363028d3bfc6998567472e	improving electronic health record (ehr) accuracy and increasing compliance with health maintenance clinical guidelines through patient access and input	clinical guideline;quality measurement;health maintenance;electronic health record;quality of care;quality measures;personal health record	BACKGROUND Health maintenance is crucial for preventing morbidity and premature mortality, but many patients do not receive preventive services at recommended intervals. One reason for this is the lack of up-to-date information accurately reflecting patients' history. Electronic health records (EHRs) can be useful, but are often incomplete. Patient input has the potential to improve the accuracy of this information. In this study, we assessed the current state of EHR completeness for preventive services and the added value of patient reported information.   METHODS Participants were sent a survey, pre-populated with health maintenance procedure information from their EHRs. They were asked to review this information and indicate whether it was accurate or if they had a procedure done more recently. Of 1098 patients recruited from a primary care practice, 163 returned the survey. When a patient reported a more recent test than was noted in the EHR, researchers updated the EHR to reflect the additional information. Data were also gathered from the EHR 6 months after surveys were completed to analyze whether providing due test information encouraged patients to get tested and vaccinated. A review of medical records was performed on a control group to analyze differences in adherence to preventive guidelines between those that were notified of their overdue status and those who were not notified.   RESULTS The EHR was frequently incomplete when compared to patient report. In particular, many patients were misidentified as being overdue for health maintenance procedures when they had obtained them in other places. Showing patients their information resulted in little impact on overall adherence. However, with the cumulative effects of additional patient-reported procedures and procedures performed after the survey, intervention patients had higher documented adherence rates for every procedure than the control group.   CONCLUSIONS Health maintenance data in EHRs were often incomplete. Patients were often able to provide useful information, demonstrating the value of patient contributions in keeping records up-to-date.	appendix;beauveria australis;boubovia sp. m.h. 80813;cervix carcinoma;choice behavior;colorectal carcinoma;comment (computer programming);data mining;document completion status - documented;electronic health records;emoticon;ephrin type-b receptor 1, human;experience;geographic information systems;health promotion;interventional procedure;measles-rubella vaccine (live);medical records;morbidity - disease rate;neck;neoplasms;online and offline;partial;patients;population;primary health care;randomized algorithm;regulatory submission;virtual screening;adenotonsillectomy	Maria Staroselsky;Lynn A. Volk;Ruslana Tsurikova;Lisa Pizziferri;Margaret Lippincott;Jonathan S. Wald;David W. Bates	2006	International journal of medical informatics	10.1016/j.ijmedinf.2005.10.004	medicine;nursing;medical emergency	HCI	-58.612869549609265	-66.51013713704745	131096
5bdb92b3773c1b424a7b0bb869cfc94f8cb6ae46	a model of ambiguity and vagueness in clinical practice guideline recommendations	practice guidelines as topic;semantics;terminology as topic;fuzzy logic;decision making computer assisted;humans;linguistics	Ambiguity and vagueness in clinical practice guidelines reduce the likelihood of clinician adherence. They lead to inconsistent interpretation and, in turn, to inappropriate practice variation and medical errors. Resolving ambiguity and vagueness is an essential step in the computerized implementation of clinical practice guidelines. Successful resolution of ambiguity and vagueness requires an understanding of their characteristics, yet ambiguity and vagueness have not been differentiated, classified and described in medical context. In this paper, we propose a tri-axial model to describe ambiguity and vagueness in clinical practice guidelines: differentiation of true ambiguity from vagueness, classification of ambiguity and vagueness, intentionality and components involved. Our goals in introducing this model are: (a) to provide guidance to guideline authors to enable them to reduce inadvertent use of ambiguous or vague language, (b) to improve transparency when vague language is used deliberately and (c) to create a framework for the development of tools to apply the model during authoring and implementation of clinical practice guidelines.	ambiguous grammar;classification;emoticon;intentionality;practice guidelines as topic;practice management, medical;recommender system;triangular function;trimipramine;vagueness	Shlomi Codish;Richard N. Shiffman	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;knowledge management;data mining;social psychology	SE	-53.642960101063906	-68.53204552137623	131650
4e1ccb06a1a6115b9d7db86533eb1121dc866b87	review and evaluation of electronic health records-driven phenotype algorithm authoring tools for clinical and translational research	phenotype algorithm authoring tool;phenotyping;clinical research;review;electronic health records	OBJECTIVE To review and evaluate available software tools for electronic health record-driven phenotype authoring in order to identify gaps and needs for future development.   MATERIALS AND METHODS Candidate phenotype authoring tools were identified through (1) literature search in four publication databases (PubMed, Embase, Web of Science, and Scopus) and (2) a web search. A collection of tools was compiled and reviewed after the searches. A survey was designed and distributed to the developers of the reviewed tools to discover their functionalities and features.   RESULTS Twenty-four different phenotype authoring tools were identified and reviewed. Developers of 16 of these identified tools completed the evaluation survey (67% response rate). The surveyed tools showed commonalities but also varied in their capabilities in algorithm representation, logic functions, data support and software extensibility, search functions, user interface, and data outputs.   DISCUSSION Positive trends identified in the evaluation included: algorithms can be represented in both computable and human readable formats; and most tools offer a web interface for easy access. However, issues were also identified: many tools were lacking advanced logic functions for authoring complex algorithms; the ability to construct queries that leveraged un-structured data was not widely implemented; and many tools had limited support for plug-ins or external analytic software.   CONCLUSIONS Existing phenotype authoring tools could enable clinical researchers to work with electronic health record data more efficiently, but gaps still exist in terms of the functionalities of such tools. The present work can serve as a reference point for the future development of similar tools.	accessibility;algorithm;compiler;computable function;database;electronic health records;embase;extensibility;genetic translation process;human-readable medium;phenotype determination;plug (physical object);plug-in (computing);pubmed;review [publication type];scopus;social network analysis software;translational research;user interface device component;web of science;web search engine;format	Jie Xu;Luke V. Rasmussen;Pamela L. Shaw;Guoqian Jiang;Richard C. Kiefer;Huan Mo;Jennifer A. Pacheco;Peter Speltz;Qian Zhu;Joshua C. Denny;Jyotishman Pathak;William K. Thompson;Enid N. H. Montague	2015	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocv070	clinical research;computer science;bioinformatics;phenotype;data mining;database;world wide web	HCI	-51.270624807779264	-66.24925909006197	132146
d6e0aad3b962566085e5eeea4e7d527470fdc411	a spatio-anatomical medical ontology and automatic plausibility checks		In this paper, we explain the peculiarities of medical knowledge management and propose a way to augment medical domain ontologies by spatial relations in order to perform automatic plausibility checks. Our approach uses medical expert knowledge represented in formal ontologies to check the results of automatic medical object recognition algorithms for spatial plausibility. It is based on the comprehensive Foundation Model of Anatomy ontology which we extend with spatial relations between a number of anatomical entities. These relations are learned inductively from an annotated corpus of 3D volume data sets. The induction process is split into two parts. First, we generate a quantitative anatomical atlas using fuzzy sets to represent inherent imprecision. From this atlas we then abstract the information further onto a purely symbolic level to generate a generic qualitative model of the spatial relations in human anatomy. In our evaluation we describe how this model can be used to check the results of a state-of-the-art medical object recognition system for 3D CT volume data sets for spatial plausibility. Our results show that the combination of medical domain knowledge in formal ontologies and sub-symbolic object recognition yields improved overall recognition precision.	automatic taxonomy construction;plausibility structure	Manuel Möller;Daniel Sonntag;Patrick Ernst	2010		10.1007/978-3-642-29764-9_3	spatial intelligence;data mining;semantic technology;ontology (information science);fuzzy set;domain knowledge;medical imaging;data set;computer science;spatial relation	NLP	-49.22668989031162	-66.82619800118503	132763
5b8224cf5d25fd6c33c2c5dbabb3605406ac4aec	ontological approach for safe and effective polypharmacy prescription.	chronic disease;drug therapy computer assisted;drug prescriptions;pharmaceutical preparations;terminology as topic;decision support systems clinical;artificial intelligence;polypharmacy;humans;formularies as topic	The intake of multiple medications in patients with various medical conditions challenges the delivery of medical care. Initial empirical studies and pilot implementations seem to indicate that generic safe and effective multi-drug prescription principles could be defined and reused to reduce adverse drug events and to support compliance with medical guidelines and drug formularies. Given that ontologies are known to provide well-principled, sharable, setting-independent and machine-interpretable declarative specification frameworks for modeling and reasoning on biomedical problems, we explore here their use in the context of multi-drug prescription. We propose an ontology for modeling drug-related knowledge and a repository of safe and effective generic prescription principles. To test the usability and the level of granularity of the developed ontology-based specification models and heuristic we implemented a tool that computes the complexity of multi-drug treatments, and a decision aid to check the safeness and effectiveness of prescribed multi-drug treatments.		María Adela Grando;Susan Farrish;Cynthia Boyd;Aziz A. Boxwala	2012	AMIA ... Annual Symposium proceedings. AMIA Symposium		pharmacology;medicine;data mining;biological engineering	SE	-52.492513941096846	-66.98173752790716	133158
122dd51acee4396daae0ba1a241adf079e211eb6	evidence-based validation and improvement of electronic health record systems	clinical data;health informatics;evidence based software validation and improvement;electronic health record;software engineering;electronic health record systems;efficacy;safety;health information system;software validation	"""A program of research on methodology for """"evidence-based"""" validation and improvement of electronic health record systems and related health information systems is proposed. This program would integrate existing ideas from software engineering and health informatics with new techniques for analyzing a combination of usage data, clinical data, and execution data, to support scientifically sound methods for measuring and enhancing the safety and efficacy of EHR systems."""	informatics;information system;software engineering;usage data	Andy Podgurski	2010		10.1145/1882362.1882422	reliability engineering;health informatics;public health informatics;clinical decision support system;verification and validation;systems engineering;data mining;efficacy	SE	-53.30550084666382	-66.9950812188548	134389
08316ef4d711ee89daab3daf5ed4b560b9b0b0b4	designing devices with the task in mind: which numbers are really used in hospitals?	medical devices;interface design;digit distributions;number entry;heuristic evaluation	OBJECTIVE We studied the patterns of digits and numbers used when programming infusion pumps with the aim of informing the design of number entry interfaces.   BACKGROUND Number entry systems on medical devices are designed with little thought given to the numbers that will be entered. In other fields, text and number entry interfaces are designed specifically for the task that they will be used for. Doing so allows for faster and more accurate interaction.   METHOD In Study 1, logs were taken from infusion pumps used in a hospital. Information about the numbers being typed was extracted. For Study 2, three common number entry interfaces were evaluated in light of these results to determine which were best suited to the task of programming infusions.   RESULTS There are clear patterns in the numbers being used in hospitals. The digit 0 is used far more frequently than any other digit. The numbers 1,000, 100, and 50 are used in nearly half of all infusions. Study 2 demonstrates that interfaces are not optimized for entering such data.   CONCLUSION Changes could be made to the design of the number entry interface on infusion pumps, leading to a reduction in the number of key presses necessary to program a device. We offer a set of four heuristics to guide the design of number entry interfaces on infusion devices.   APPLICATION Improving the design of the number entry interface of medical devices, such as infusion pumps, would lead to improved efficiency and a reduction in the likelihood of errors.		Sarah Wiseman;Anna Louise Cox;Duncan P. Brumby	2013	Human factors	10.1177/0018720812471988	simulation;human–computer interaction;computer science;engineering;theoretical computer science;interface design;heuristic evaluation;engineering drawing	HCI	-59.825579486127175	-68.22174711500934	134561
f2aa632df5d8d32acc3f7d414cce87d20470d48e	retrieving cases for treatment advice in nursing using text representation and structured text retrieval.	health informatics;decision support;case base reasoning;linear regression model;rule based;combining evidence;retrieval functions;text retrieval;case based reasoning	A nursing database which records patient details and treatments as fields in a standard database format is transformed into a collection, in text form, of patient case days with history. Each case is represented as text strings encoding the patient details, the current problems, treatments and their associated history. The cosine measure of similarity is used to compute a whole case similarity between a text query and the cases in text form. This standard text retrieval technique is used and compared to a simple rule base. In case-based reasoning, the similarity of cases is often computed by combining similarities of the case features involved. In this work the standard text retrieval function is modified to incorporate this case structure by combining individual matches of case components based on the cosine measure. The combination is based on a linear regression model for learning the weights assigned to the components of this retrieval function. For the 1355 records two tasks were tried: predicting the treatment for a new problem and predicting the treatment for a continuing problem when a change of treatment is required. Simple text retrieval was better than the rule base for one task and case structured retrieval was at least 18% better on both tasks. Further techniques are discussed.		John Yearwood;Ross Wilkinson	1997	Artificial intelligence in medicine	10.1016/S0933-3657(96)00362-4	document retrieval;health informatics;case-based reasoning;full text search;computer science;artificial intelligence;machine learning;pattern recognition;data mining;data retrieval;information retrieval	Web+IR	-48.574762123369986	-66.99643267593862	135922
422c83d0b414cd210fc2743cc3ed2db37439dc6c	document ontology: supporting narrative documents in electronic health records		Electronic health records (EHRs) are beginning to manage an increasing volume of narrative data, such as clinical notes pertaining to admission, patient progress, shift change, follow-up, consultation, procedures, etc. These documents fall into a wide variety of classes, based on who is writing them, for what purpose, and in which location, suggesting the need for a document ontology (DO) to model our knowledge of health care documents and their properties. This paper focuses on one aspect of the Health Level 7 (HL7)/ Logical Observation Identifiers, Names, and Codes (LOINC) DO, the Subject Matter Domain (SMD). We created a new polyhierarchical structure for the SMD that combines the current value lists from the LOINC database with another value list from the American Board of Medical Specialties (ABMS). We refined and evaluated the new structure through expert review of the ontology, a survey of medical specialty boards, and specification of SMDs for a corpus of clinical notes.	body of uterus;class;document ontology;electronic health records;health level 7;identifier;medical specialities;name;note (document);patients;service mapping description	Jason S. Shapiro;Suzanne Bakken;Sookyung Hyun;Genevieve B. Melton;Cara Schlegel;Stephen B. Johnson	2005	AMIA ... Annual Symposium proceedings. AMIA Symposium		loinc;specialty;world wide web;data science;identifier;ontology;health care;narrative;delegation (computing);document ontology;computer science	PL	-51.192963981712545	-67.0756270221462	135929
c8e91384473133bab230f25612234bbe2ade294a	use of order sets in inpatient computerized provider order entry systems: a comparative analysis of usage patterns at seven sites	medical order entry systems;inpatients;clinical decision support;humans;user computer interface;safety management;physician s practice patterns;systems integration;electronic health records;order sets;computerized physician order entry system	BACKGROUND Many computerized provider order entry (CPOE) systems include the ability to create electronic order sets: collections of clinically related orders grouped by purpose. Order sets promise to make CPOE systems more efficient, improve care quality and increase adherence to evidence-based guidelines. However, the development and implementation of order sets can be expensive and time-consuming and limited literature exists about their utilization.   METHODS Based on analysis of order set usage logs from a diverse purposive sample of seven sites with commercially and internally developed inpatient CPOE systems, we developed an original order set classification system. Order sets were categorized across seven non-mutually exclusive axes: admission/discharge/transfer (ADT), perioperative, condition-specific, task-specific, service-specific, convenience, and personal. In addition, 731 unique subtypes were identified within five axes: four in ADT (S=4), three in perioperative, 144 in condition-specific, 513 in task-specific, and 67 in service-specific.   RESULTS Order sets (n=1914) were used a total of 676,142 times at the participating sites during a one-year period. ADT and perioperative order sets accounted for 27.6% and 24.2% of usage respectively. Peripartum/labor, chest pain/acute coronary syndrome/myocardial infarction and diabetes order sets accounted for 51.6% of condition-specific usage. Insulin, angiography/angioplasty and arthroplasty order sets accounted for 19.4% of task-specific usage. Emergency/trauma, obstetrics/gynecology/labor delivery and anesthesia accounted for 32.4% of service-specific usage. Overall, the top 20% of order sets accounted for 90.1% of all usage. Additional salient patterns are identified and described.   CONCLUSION We observed recurrent patterns in order set usage across multiple sites as well as meaningful variations between sites. Vendors and institutional developers should identify high-value order set types through concrete data analysis in order to optimize the resources devoted to development and implementation.		Adam Wright;Joshua Feblowitz;Justine E. Pang;James D. Carpenter;Michael Krall;Blackford Middleton;Dean F. Sittig	2012	International journal of medical informatics	10.1016/j.ijmedinf.2012.04.003	clinical decision support system;medicine;computer science;artificial intelligence;nursing;data mining;law;system integration	HCI	-55.412590475156776	-67.78554487656801	136909
0721196ae3c45625281baf9ed42ac7639ceb0b7c	the capture and use of detailed process information in the dialogix system for structured web-based interactions	epidemiology;internet;data collection;questionnaires	Information gathering tools, such as questionnaires, surveys, and structured interviews, are ubiquitously used in evaluating patients and systems. Despite their common use, there is a desperate need for better questionnaires in medical research and epidemiology, and an infrastructure that lets them be publicly scrutinized. Unfortunately, there has been no common platform that supports the deployment of arbitrary information gathering tools. Some psychiatric diagnostic interviews and epidemiological trials require sophisticated structured interviews containing complex branching logic, dynamic phrase composition, and multiple languages. The Dialogix system was developed to meet this need and facilitate the rapid definition and web-based deployment of structured human-computer interactions. This paper describes the content and process-related information captured by Dialogix, and how that information has been used in the development and deployment of two large epidemiological studies.	common platform;deploy;epidemiology;human–computer interaction;interviews;mental disorders;patients;programming languages;web application	Thomas M. White;Michael J. Hauan	2001	Proceedings. AMIA Symposium		data collection;the internet;software deployment;structured interview;web application;medical research;branching (version control);data mining;software;computer science	HCI	-52.15286947961301	-66.65186887838469	138225
049688f9704e8a6e7010f354eeadcf2a476d8b06	htnsystem: hypertension information extraction system for unstructured clinical notes		Hypertension (HTN) relevant information has great application potential in cohort discovery and building predictive models for prevention and surveillance. Unfortunately most of this valuable patient information is buried in the form of unstructured clinical notes. In this study we present HTN information extraction system called HTNSystem which is capable of extracting mentions of HTN and inferring HTN from BP lab values. HTNSystem is a rule based system which implements MetaMap as a core component together with custom built BP value extractor and post processing components. It is evaluated on a corpus of 514 clinical notes (82.92% F-measure). HTNSystem is distributed as an open source command line tool available at https://github.com/TCRNBioinformatics/HTNSystem .	information extraction	Jitendra Jonnagaddala;Siaw-Teng Liaw;Pradeep Kumar Ray;Manish Kumar;Hong-Jie Dai	2014		10.1007/978-3-319-13987-6_21	medicine;data science;data mining;database	NLP	-49.819354027652814	-68.87937733830975	138795
c4f94f04a29de8168b819e7264a3bee2681b9cc5	portals to wonderland: health portals lead to confusing information about the effects of health care	health informatics;systematic review;web pages;information access;information services;information systems and communication service;delivery of health care;internet;health information;management of computing and information systems;humans;review literature as topic;information storage and retrieval;health care	BACKGROUND The Internet offers a seemingly endless amount of health information of varying quality. Health portals, which provide entry points to quality-controlled collections of websites, have been hailed as a solution to this problem. The objective of this study is to assess the extent to which government-run health portals provide access to relevant, valid and understandable information about the effects of health care.   METHODS We selected eight clinically relevant questions for which there was a systematic review, searched four portals for answers, and compared the answers we found to the results of the systematic reviews.   RESULTS Our searches resulted in 3400 hits, 155 of which mentioned both the condition and the intervention in one of the eight questions. Sixty-three of the 155 web pages did not give any information about the effect of the intervention. Seventy-seven qualitatively described the effects of the intervention. Twenty-six of these had information that was too unclear to be categorised; 15 were not consistent with the systematic review; and 36 were consistent with the review, but usually did not mention what happens without the intervention, what outcomes have been measured or when they were measured. Fifteen web pages quantitatively described effects. Four of these were abstracts from the systematic review, nine had information that was incomplete and potentially misleading because of a lack of information about people not receiving the intervention and the length of follow-up; one had information that was consistent with the review, but only referred to three trials whereas the review included six; and one was consistent with the review.   CONCLUSION Information accessible through health portals is unlikely to be based on systematic reviews and is often unclear, incomplete and misleading. Portals are only as good as the websites they lead to. Investments in national health portals are unlikely to benefit consumers without investments in the production and maintenance of relevant, valid and understandable information to which the portals lead.	abstract summary;ambient occlusion;cg (programming language);cochrane library;collections (publication);entry point;expectation propagation;health care;information retrieval;internet;manuscripts;page (document);partial;portals;review [publication type];systematic review;weakness;web page;citation;interest	Claire Glenton;Elizabeth J. Paulsen;Andrew D. Oxman	2005	BMC Medical Informatics and Decision Making	10.1186/1472-6947-5-7	health informatics;clinical decision support system;the internet;systematic review;medicine;hrhis;nursing;web page;data mining;world wide web;information retrieval;information system;health care	HCI	-58.51515693465318	-67.70380411432363	139284
29b7526d4aed949980c88c68427ae107ec90c2f2	analysis of temporal abstraction in medical databases	clinical research	Physicians and medical decision-support applications, such as for diagnosis, therapy, monitoring, quality assessment, and clinical research, reason about patients in terms of abstract, clinically meaningful concepts, typically over significant time periods. Clinical databases, however, store only raw, time-stamped data. Thus, there is a need to bridge this gap. We introduce the Temporal Abstraction Language (TAR) which enables specification of abstract relations involving raw data and abstract concepts, and use it for defining typical medical abstraction patterns. For each pattern we further analyze finiteness properties of the answer set.	database;decision support system;stable model semantics	Mira Balaban;David Boaz;Yuval Shahar	2003			data science;raw data;data mining;clinical research;database;abstraction;computer science	AI	-52.26504076544367	-67.54978454145304	140043
5de770ac53af3a6c1c4ec6eb1a437b5afd77cd4a	integrating va's ndf-rt drug terminology with pharmgkb: preliminary results		Biomedical terminology and vocabulary standards play an important role in enabling consistent, comparable, and meaningful sharing of data within and across institutional boundaries, as well as ensuring semantic interoperability. The Veterans Affairs (VA) National Drug File Reference Terminology (NDF-RT) is a federally recommended standardized terminology resource encompassing medications, ingredients, and a hierarchy for high-level drug classes. In this study, we investigate the drug-disease relationships in NDF-RT and determine how PharmGKB can be leveraged to augment NDF-RT, and vice-versa. Our preliminary results indicate that with additional curation and analyses, information contained in both knowledge resources can be mutually integrated.	class;contain (action);digital curation;high- and low-level;nomenclature;pharmgkb;semantic interoperability;united states department of veterans affairs;vocabulary;standards characteristics	Jyotishman Pathak;Laura C. Weiss;Matthew J. Durski;Qian Zhu;Robert R. Freimuth;Christopher G. Chute	2012	Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing		semantic interoperability;drug;veterans affairs;pharmgkb;data mining;terminology;vocabulary;hierarchy;biology	Metrics	-50.50239934054266	-67.08510594246367	140567
5f76e9b0ddea024014dbd203275624272da9281e	analysis of annotated data models for improving data quality	cdisc odm;natural language processing;semantic interoperability;umls	The public Medical Data Models (MDM) portal with more than 9.000 annotated forms from clinical trials and other sources provides many research opportunities for the medical informatics community. It is mainly used to address the problem of heterogeneity by searching, mediating, reusing, and assessing data models, e. g. the semi-interactive curation of core data records in a special domain. Furthermore, it can be used as a benchmark for evaluating algorithms that create, transform, annotate, and analyse structured patient data. Using CDISC ODM for syntactically representing all data models in the MDM portal, there are semi-automatically added UMLS CUIs at several ODM levels like ItemGroupDef, ItemDef, or CodeList item. This can improve the interpretability and processability of the received information, but only if the coded information is correct and reliable. This raises the question how to assure that semantically similar datasets are also processed and classified similarly. In this work, a (semi-)automatic approach to analyse and assess items, questions, and data elements in clinical studies is described. The approach uses a hybrid evaluation process to rate and propose semantic annotations for under-specified trial items. The evaluation algorithm operates with the commonly used NLM MetaMap to provide UMLS support and corpus-based proposal algorithms to link datasets from the provided CDISC ODM item pool.		Hannes Ulrich;Ann-Kristin Kock-Schoppenhauer;Björn Andersen;Josef Ingenerf	2017	Studies in health technology and informatics	10.3233/978-1-61499-808-2-190	data modeling;data mining;data quality;computer science	DB	-49.31711583145365	-67.1698544583063	140999
e81eb61942a71aa577ade2a5d46d1a480d2b4804	does domain knowledge matter for assertion annotation in clinical texts?	natural language processing assertion annotation clinical texts statistical difference clinical domain knowledge;information retrieval;text analysis;medical computing;statistical analysis;medical diagnostic imaging medical services standards biomedical informatics educational institutions natural language processing;natural language processing;text analysis information retrieval medical computing natural language processing statistical analysis	This pilot study aims to determine how well subjects annotate assertions about problem mentions in clinical text and determine if a statistical difference exists between subjects with and without clinical domain knowledge.		Danielle L. Mowery;Pamela W. Jordan;Janyce Wiebe;Wendy W. Chapman;Lin Liu	2012	2012 IEEE Second International Conference on Healthcare Informatics, Imaging and Systems Biology	10.1109/HISB.2012.61	natural language processing;language identification;question answering;computer science;data science;temporal annotation;information extraction;information retrieval	Robotics	-48.931665759014564	-68.5665198467261	141156
6232b4275edbf76963a43c30e1aac77105fb1f7d	mining typical order sequences from ehr for building clinical pathways		Clinical pathway is one of the key tools for providing standardized treatment for patients. However, building a new pathway from scratch is a time-consuming task for medical staffs, as it involves optimization of the treatment plan while preserving operability in a hospital. In this paper, we present a method for mining typical treatment processes from electric health records (EHRs) for facilitating creation of new pathways by providing base blocks. Firstly, we constitute occurrence and transition frequency matrices of clinical orders using all cases. Next, we compute the typicalness index for each order sequence based on the occurrence and transition frequencies. After that we perform clustering of all cases according to the similarity defined on the typicalness index. Experimental results on two disease datasets demonstrate that the method is capable of producing clusters that reflect differences of treatment processes without a priori information about order types.		Shoji Hirano;Shusaku Tsumoto	2014		10.1007/978-3-319-13186-3_5	bioinformatics;data mining	Crypto	-52.84652001970451	-68.48797384380279	141179
d30decdd38a31476a01524b040d69687ea46f934	a computational framework for converting textual clinical diagnostic criteria into the quality data model	ctakes;quality data model;diagnostic criteria;conditional random fields;natural language processing	BACKGROUND Constructing standard and computable clinical diagnostic criteria is an important but challenging research field in the clinical informatics community. The Quality Data Model (QDM) is emerging as a promising information model for standardizing clinical diagnostic criteria.   OBJECTIVE To develop and evaluate automated methods for converting textual clinical diagnostic criteria in a structured format using QDM.   METHODS We used a clinical Natural Language Processing (NLP) tool known as cTAKES to detect sentences and annotate events in diagnostic criteria. We developed a rule-based approach for assigning the QDM datatype(s) to an individual criterion, whereas we invoked a machine learning algorithm based on the Conditional Random Fields (CRFs) for annotating attributes belonging to each particular QDM datatype. We manually developed an annotated corpus as the gold standard and used standard measures (precision, recall and f-measure) for the performance evaluation.   RESULTS We harvested 267 individual criteria with the datatypes of Symptom and Laboratory Test from 63 textual diagnostic criteria. We manually annotated attributes and values in 142 individual Laboratory Test criteria. The average performance of our rule-based approach was 0.84 of precision, 0.86 of recall, and 0.85 of f-measure; the performance of CRFs-based classification was 0.95 of precision, 0.88 of recall and 0.91 of f-measure. We also implemented a web-based tool that automatically translates textual Laboratory Test criteria into the QDM XML template format. The results indicated that our approaches leveraging cTAKES and CRFs are effective in facilitating diagnostic criteria annotation and classification.   CONCLUSION Our NLP-based computational framework is a feasible and useful solution in developing diagnostic criteria representation and computerization.		Na Hong;Dingcheng Li;Yue Yu;Qiongying Xiu;Hongfang Liu;Guoqian Jiang	2016	Journal of biomedical informatics	10.1016/j.jbi.2016.07.016	natural language processing;computer science;bioinformatics;artificial intelligence;data science;machine learning;data mining;database;conditional random field;information retrieval	NLP	-50.17779661319897	-68.11521987973197	141839
ab14a499e4235e8a2e55413cbff6d6d2f0f9556e	emerse: the electronic medical record search engine	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;medical records systems computerized;research articles;abstracts;open access;life sciences;clinical guidelines;full text;information storage and retrieval;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	EMERSE (The Electronic Medical Record Search Engine) is an intuitive, powerful search engine for free-text documents in the electronic medical record. It offers multiple options for creating complex search queries yet has an interface that is easy enough to be used by those with minimal computer experience. EMERSE is ideal for retrospective chart reviews and data abstraction and may have potential for clinical care as well.	abstraction (software engineering);academic medical centers;bug search subtest (wppsi-iv);drug utilization review;electronic health records;electronics, medical;medical records systems, computerized;web search engine;web search query;search a word	David A. Hanauer	2006	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;data science;data mining;information retrieval	Arch	-51.16407350620962	-66.36678220963887	141896
dec86f9dae8621a10b25ca59d4da7876c64ca7d6	indicateurs en biologie et en imagerie au sein des systèmes d'information de santé		Introduction: Health information system (HIS) has been widely studied in order to evaluate their effectiveness. However the results are heterogeneous and the methods are rarely adapted to the french health system. This work aims at developing a tool for HIS description and listing indicators frequently used to assess its benefits in the fields of laboratory and radiology. Those tools will allow inter HIS comparison. Methods: Processes were finely described in a university hospital in order to create a processes description tool. A short literature review allowed us to list the most commonly used indicators in the field of research concerning the HIS benefits. Results: Articles longs des 15es Journées francophones d’informatique médicale, JFIM 2014, pages 127–138 Fès, Maroc, 12–13 juin 2014 The process describing tool was successfully used by four hospitals to describe their HIS. Several indicators have been selected in the international literature and some of them have been modified to describe more precisely the sub processes. Conclusion: All the items for the evaluation of the benefits of the HIS are ready. Mots clés : Dossiers médicaux électroniques ; Systèmes d'information hospitaliers ; Systèmes d'information de laboratoire d'analyses médicales ; Systèmes d'information de radiologie ; Indicateurs qualité santé.	information system;radiology	Mher B. Joulakian;Nicolas Griffon;Matthieu Schuers;Éric Lepage;Céline Savoye-Collet;Soumaya Skalli;Philippe Massari;Stéfan Jacques Darmoni	2014				SE	-55.359311001558616	-69.91965433247032	142437
d0fcb72749d4a2ae6a7491dc7ee54071d74d7f02	segmenting and merging domain-specific ontology modules for clinical informatics	snomed ct;segment;module;ontology;domain specificity;clinical informatics	A significant set of challenges to the use of large, source ontologies in the medical domain include: automated translation, customization of source ontologies, and performance issues associated with the use of logical reasoning systems to interpret the meaning of a domain captured in a formal knowledge representation. SNOMED-CT and FMA are two reference ontologies that cover much of the domain of clinical informatics and motivate a better means for re-use. In this paper, we present a method for segmenting and merging modules from these ontologies for a specific domain that preserve the meaning of the anatomy terms they have in	ct scan;fma instruction set;informatics;knowledge representation and reasoning;machine translation;ontology (information science);systematized nomenclature of medicine;upper ontology	Chimezie Ogbuji;Sivaram Arabandi;Songmao Zhang;Guo-Qiang Zhang	2010		10.3233/978-1-60750-535-8-414	module;idef5;computer science;knowledge management;data science;ontology;snomed ct;data mining	AI	-49.143061224189026	-67.41960233076185	142508
fef87417cf0939618e70db7d54b240b700900596	an integrated electronic medical record system (iemrs) with decision support capability in medical prescription	electronic medical records;information technology;data mining;decision support systems;medical prescriptions;hong kong;journal magazine article	Purpose: This study presents how a clinical decision support system can help in prescription and knowledge acquisition processes. Design/methodology/approach: An integrated Electronic Medical Records System (iEMRS) is designed to enhance the decision support quality in prescription. Findings: By evaluating the system performance through 135 prescription records collected from a Hong Kong medical organization, iEMRS shows a satisfactory result in suggesting medicines that is properly the same as the decisions made by the physicians. Originality/value: Compared with the static clinical guidelines built (manually) in the traditional clinical decision support system, knowledge in iEMRS is generated by the knowledge discovery result from professional experiences of various physicians and patient histories, which are more dynamic in nature. A treatment algorithm, designed in data mining technique, is introduced to improve information management in medical organization by integration of decision support capability and EMRS, and supplement the deficiencies of traditional clinical decision support system.	clinical decision support system;data mining;experience;information management;knowledge acquisition;medical algorithm	Jacky S. L. Ting;Andrew W. H. Ip;Albert H. C. Tsang;George T. S. Ho	2012	J. Systems and IT	10.1108/13287261211255347	clinical decision support system;decision support system;computer science;knowledge management;data science;data mining;docle;information technology;medical algorithm	ML	-54.81387052034619	-66.41260581460956	143154
504dd246d0d3648350db7e0d8a9b0b9c86c0ae0c	a systematic review of predictive models for asthma development in children	health informatics;models theoretical;asthma;bronchiolitis;information systems and communication service;machine learning;management of computing and information systems;child;humans;asthma development;predictive model	BACKGROUND Asthma is the most common pediatric chronic disease affecting 9.6 % of American children. Delay in asthma diagnosis is prevalent, resulting in suboptimal asthma management. To help avoid delay in asthma diagnosis and advance asthma prevention research, researchers have proposed various models to predict asthma development in children. This paper reviews these models.   METHODS A systematic review was conducted through searching in PubMed, EMBASE, CINAHL, Scopus, the Cochrane Library, the ACM Digital Library, IEEE Xplore, and OpenGrey up to June 3, 2015. The literature on predictive models for asthma development in children was retrieved, with search results limited to human subjects and children (birth to 18 years). Two independent reviewers screened the literature, performed data extraction, and assessed article quality.   RESULTS The literature search returned 13,101 references in total. After manual review, 32 of these references were determined to be relevant and are discussed in the paper. We identify several limitations of existing predictive models for asthma development in children, and provide preliminary thoughts on how to address these limitations.   CONCLUSIONS Existing predictive models for asthma development in children have inadequate accuracy. Efforts to improve these models' performance are needed, but are limited by a lack of a gold standard for asthma development in children.	abbreviations;aclarubicin;area under curve;bibliographic reference;casp;cinahl;chronic disease;cochrane library;database;digital library;embase;expiration, function;feedback;ieee xplore;immunoglobulin e;informatics;intelligent platform management interface;manuscripts;nitric oxide;office open xml;predictive modelling;projection screen;pubmed;receiver operating characteristic;respiratory syncytial virus;review [publication type];revision procedure;scopus;systematic review;thinking, function;interest;mecarzole	Gang Luo;Flory Nkoy;Bryan L. Stone;Darell Schmick;Michael D. Johnson	2015		10.1186/s12911-015-0224-9	health informatics;medicine;pathology;physical therapy;predictive modelling;pediatrics	HCI	-56.99313883353233	-69.09655826861349	143391
4d5a3f888b4a916dd7ddad07354f0234aea68855	analyzing differences between chinese and english clinical text: a cross-institution comparison of discharge summaries in two languages	cross language;discharge summary;clinical notes	Worldwide adoption of Electronic Medical Records (EMRs) databases in health care have generated an unprecedented amount of clinical data available electronically. There has been an increasing trend in US and western institutions towards collaborating with China on medical research using EMR data. However, few studies have investigated characteristics of EMR data in China and their differences with the data in US hospitals. As an initial step towards differentiating EMR data in Chinese and US systems, this study attempts to understand system and cultural differences that may exist between Chinese and English clinical documents. We collected inpatient discharge summaries from one Chinese and from three US institutions and manually analyzed three major clinical components in text: medical problems, tests, and treatments. We reported comparison results at the document level and section level and discussed potential reasons for observed differences. Documenting and understanding differences in clinical reports from the US and China EMRs are important for cross-country collaborations. Our study also provided valuable insights for developing natural language processing tools for Chinese clinical text.	clinical data;database;discharger;documented;electronic health records;excalibur: morgana's revenge;natural language processing;programming languages;software documentation;inpatient	Yonghui Wu;Jianbo Lei;Wei-Qi Wei;Buzhou Tang;Joshua C. Denny;S. Trent Rosenbloom;Randolph A. Miller;Dario A. Giuse;Kai Zheng;Hua Xu	2013		10.3233/978-1-61499-289-9-662	medicine;data science;data mining;genealogy	NLP	-49.85923085557122	-68.29266917445308	143540
117e0e140fb25e57b805ae2c89a765d9af85d515	comparison of measures to assess change in diagnostic performance due to a decision support system	physicians	Little has been done to examine the relative merit of measures used to assess the impact of diagnostic decision support systems (DDSS) on physician performance. In this study, 10 different single-measures of diagnostic performance were compared empirically. The measures were of three types: rank-order, all-or-none, and appropriateness. The responsiveness (RESP) of each measure was estimated under two repeated-measures experimental conditions. RESP is the degree to which a measure could detect differences between conditions of low and high performance. The diagnostic performance of 108 physicians was compared on medical cases of varying diagnostic difficulty and with or without a high level of assistance from a DDSS. The results showed that the RESP among the measures varied nearly tenfold. The rank-order measures tended to provide the highest RESP values (maximum = 2.14) while appropriateness measures provided the lowest RESP values (maximum = 1.41). The most responsive measures were rank-orders of the correct diagnosis within the top 5 to 10 listed diagnoses.	cns disorder;computer performance;decision support systems, clinical;decision support system;degree (graph theory);diagnostic techniques, otological;high-level programming language;norm (social);respiratory rate;responsiveness	Richard S. Maisiak;Eta S. Berner	2000	Proceedings. AMIA Symposium		statistics;medical diagnosis;decision support system;data mining;business	SE	-61.21601646440886	-67.1048291334984	143558
ba889b7d4e275afbf1f8943c50ff09cdc4f6b01f	a new method for readability testing of drug consumer information to increase drug safety	drug safety;readability indices;medical thesaurus;readability test;readability criteria	This document presents a new readability test designed to optimise the readability of package leaflets for drugs. It discusses the existing methods and the additional properties that the new method offers. A newly developed catalogue of criteria forms the foundation for the new method. In addition, the problems associated with current package leaflets are discussed.		Fabian Merges;Madjid Fathi	2011		10.1145/2024288.2024327	engineering;data mining;multimedia;world wide web	SE	-54.985671213141266	-68.84713031629478	145925
66899bb47d6369037e7dc65020d8d34b4212be89	a value set for documenting adverse reactions in electronic health records	allergy and immunology;controlled;drug-related side effects and adverse reactions;electronic health records;hypersensitivity;natural language processing;vocabulary	Objective To develop a comprehensive value set for documenting and encoding adverse reactions in the allergy module of an electronic health record.   Materials and Methods We analyzed 2 471 004 adverse reactions stored in Partners Healthcare's Enterprise-wide Allergy Repository (PEAR) of 2.7 million patients. Using the Medical Text Extraction, Reasoning, and Mapping System, we processed both structured and free-text reaction entries and mapped them to Systematized Nomenclature of Medicine - Clinical Terms. We calculated the frequencies of reaction concepts, including rare, severe, and hypersensitivity reactions. We compared PEAR concepts to a Federal Health Information Modeling and Standards value set and University of Nebraska Medical Center data, and then created an integrated value set.   Results We identified 787 reaction concepts in PEAR. Frequently reported reactions included: rash (14.0%), hives (8.2%), gastrointestinal irritation (5.5%), itching (3.2%), and anaphylaxis (2.5%). We identified an additional 320 concepts from Federal Health Information Modeling and Standards and the University of Nebraska Medical Center to resolve gaps due to missing and partial matches when comparing these external resources to PEAR. This yielded 1106 concepts in our final integrated value set. The presence of rare, severe, and hypersensitivity reactions was limited in both external datasets. Hypersensitivity reactions represented roughly 20% of the reactions within our data.   Discussion We developed a value set for encoding adverse reactions using a large dataset from one health system, enriched by reactions from 2 large external resources. This integrated value set includes clinically important severe and hypersensitivity reactions.   Conclusion This work contributes a value set, harmonized with existing data, to improve the consistency and accuracy of reaction documentation in electronic health records, providing the necessary building blocks for more intelligent clinical decision support for allergies and adverse reactions.	clinical decision support system;documented;drug allergy;electronic health records;exanthema;hypersensitivity;information model;one health initiative;pear;patients;pruritus;rodent nomenclature name;silo (dataset);software documentation;systematized nomenclature of medicine;terminology value set;urticaria;anaphylaxis;gastrointestinal upset;standards characteristics	Foster R. Goss;Kenneth H. Lai;Maxim Topaz;Warren W. Acker;Leigh Kowalski;Joseph M. Plasek;Kimberly G. Blumenthal;Diane L. Seger;Sarah Patricia Slight;Kin Wah Fung;Frank Y. Chang;David W. Bates;Li Zhou	2018	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocx139	data mining;medicine	ML	-50.250563118445164	-66.73263481820105	147711
6069b2b162220b8827ed01e5b9a286979fc13b70	risk factor detection for heart disease by applying text analytics in electronic medical records	female;vocabulary controlled;incidence;medical records;middle aged;narration;male;diabetes complications;data mining;comorbidity;confidentiality;cardiovascular diseases;text classification;computer security;california;risk assessment;pattern recognition automated;longitudinal studies;humans;natural language processing;electronic health records;cohort studies;aged	In the United States, about 600,000 people die of heart disease every year. The annual cost of care services, medications, and lost productivity reportedly exceeds 108.9 billion dollars. Effective disease risk assessment is critical to prevention, care, and treatment planning. Recent advancements in text analytics have opened up new possibilities of using the rich information in electronic medical records (EMRs) to identify relevant risk factors. The 2014 i2b2/UTHealth Challenge brought together researchers and practitioners of clinical natural language processing (NLP) to tackle the identification of heart disease risk factors reported in EMRs. We participated in this track and developed an NLP system by leveraging existing tools and resources, both public and proprietary. Our system was a hybrid of several machine-learning and rule-based components. The system achieved an overall F1 score of 0.9185, with a recall of 0.9409 and a precision of 0.8972.		Manabu Torii;Jung-Wei Fan;Weili Yang;Theodore Lee;Matthew T. Wiley;Daniel Zisook;Yang Huang	2015	Journal of biomedical informatics	10.1016/j.jbi.2015.08.011	risk assessment;incidence;confidentiality;medicine;pathology;cohort study;computer science;bioinformatics;artificial intelligence;data science;data mining;database;narrative;medical record	ML	-56.58770668553357	-67.72886502597167	149237
c3e377500705df75f8364e15c54835dcdacf6061	a new generation of primary care service areas or general practice catchment areas		Primary Care is fundamental to a well-functioning health system. Various geographical small areas including specialized geographies such as Primary Care Service Areas (PCSAs) are used to measure primary care relevant outcomes and services, or to target interventions. PCSAs are small areas, the majority of patients resident in which obtain their primary care services from within the geography. The extent of this self-sufficiency of use is measured by the Localization Index (LI). PCSAs have been built in the US, Australia and Switzerland using an allocation algorithm, which, while simple and easy to implement, may require the use of various ad-hoc parameters. In this article we propose an optimization based approach to creating PCSAs, - an approach which has previously been used to generate labour flow regions in Ireland. The approach is data driven, thus requiring a minimal number of ad-hoc parameters. We compared the resulting PCSAs (or `rPCSAsu0027) with PCSAs generated using the traditional allocation algorithm. We found that rPCSAs were generally larger, offered greater LIs and reflected patient travel patterns better than traditional PCSAs. Accounting for the larger size of rPCSAs showed that rPCSAs offered better LIs than similar sized traditional PCSAs.		Soumya Mazumdar;Ludovico Pinzari;Nasser Bagheri;Paul Konings;Federico Girosi;Ian McRae	2017	Trans. GIS	10.1111/tgis.12287	data mining;general practice;computer science	HCI	-62.19062677618822	-67.75082586872317	151034
42f0590f6b6630b5e424a27b94cd7c92bf16b23e	multiple attributes for patient care data: toward a multiaxial, combinatorial vocabulary	biomedical research;bioinformatics	Background. Henry and Meadu0027 have proposed that to capture patient care data adequately for studies of intervention effectiveness and outcomes, multiaxial, combinatorial vocabularies are required. Simultaneously, Health Level 7 (HL7) has released standards for patient care data2 that accommodate the need for multiaxial, combinatorial vocabularies by defining detailed elements of a patient problem message, a patient goal message, and a patient plan message and by specifying how these elements can be combined. These events raise a question: Can terms commonly used by nurses and other nonphysicians to record patient care be readily structured into a multiaxial, combinatorial vocabulary?	vocabulary	Judy G. Ozbolt	1997			data science;data mining;computer science;vocabulary;text mining	ML	-52.74934883307447	-66.48584087220449	151125
b1c8a38b06b0b749ce12065b12dee5b58cc160d4	the evaluation of an expert system for the analysis of umbilical cord blood	expert systems;umbilical cord acid base balance;umbilical cord;acid base balance;clinical evaluation;neonatal outcome;umbilical cord blood;expert system	An assessment of neonatal outcome may be obtained from analysis of blood in the umbilical cord of an infant immediately after delivery. This can provide information on the health of the new-born infant, guide requirements for neonatal care, but there are problems with the technique. Samples frequently contain errors in one or more of the important parameters, preventing accurate interpretation and many clinical staff lack the expert knowledge required to interpret error-free results. The development and implementation of an expert system to overcome these difficulties has previously been described. This expert system validates the raw data, provides an interpretation of the results for clinicians and archives all the results, including the quality control and calibration data, for permanent storage. Issues regarding the clinical evaluation of this system are now detailed further, along with some clinical results illustrating the potential of such a system.	calibration;expert system;requirement;umbilical cord blood;umbilicus (anatomy);archive	Jonathan M. Garibaldi;Jennifer A. Westgate;Emmanuel C. Ifeachor	1999	Artificial intelligence in medicine	10.1016/S0933-3657(99)00020-2	acid–base homeostasis;computer science;artificial intelligence;expert system	AI	-57.76147948515546	-66.34137804688078	152152
0bf384409d619d9a86d33b8ad932f789c0571ee9	biomedical informatics - how to choose the best tool for each task	biomedical software;quality control;software benchmarking	The ever-increasing number of bioinformatics software tools that are publicly available, is leading to greater expectations about its regular use in clinical practice. However, from the end-users' perspective, they face many time the challenge of choosing the right tool for each task, from a panoply of solutions that have been developed over the years. In this paper, we propose a benchmarking methodology, based on a set of performance indicators, which can be used to identify the best methods and tools for each particular use case, both in research as in clinical practice.		Fábio Maia;Luís Bastião;José Luís Oliveira	2018	Studies in health technology and informatics	10.3233/978-1-61499-852-5-406	data mining;health informatics;medicine	SE	-51.3441761177087	-66.59670192323767	152788
be875296a8a791b8c8c69615ae865a6387df5d5e	architecture of a decision support system to improve clinicians' interpretation of abnormal liver function tests.	serveur institutionnel;archive institutionnelle;open access;archive ouverte unige;cybertheses;institutional repository	The objective of this work was to create a self-working computerized clinical decision support system (CDSS) able to analyze liver function tests (LFTs) in order to provide diagnostic suggestions and helpful care support to clinicians. We developed an expert system that processes exclusively para-clinical information to provide diagnostic propositions. Drugs are a major issue in dealing with abnormal LFTs, therefore we created a drug-disease causality assessment tool to include drugs in the differential diagnosis. Along with the results, the CDSS will guide clinicians in the care process offering them case-specific support in the form of guidelines, order sets and references to recent articles. The CDSS will be implemented in Geneva University Hospitals clinical information system (CIS) during year 2011. For the time being, preliminary tests have been conducted on case reports chosen randomly on Pubmed. Considered as medical challenges, case reports were nevertheless processed correctly by the program to the extent that 18 cases out of 20 were diagnosed accurately.		Raphaël D. Chevrier;David Jaques;Christian Lovis	2011	Studies in health technology and informatics	10.3233/978-1-60750-806-9-195	medicine;pathology;artificial intelligence;data mining;database;management;operations research	AI	-58.16993882762791	-66.18176524135902	152896
4a84361932e04ce422d3c642ca6ce298ca865684	clinical code set engineering for reusing ehr data for research: a review		INTRODUCTION The construction of reliable, reusable clinical code sets is essential when re-using Electronic Health Record (EHR) data for research. Yet code set definitions are rarely transparent and their sharing is almost non-existent. There is a lack of methodological standards for the management (construction, sharing, revision and reuse) of clinical code sets which needs to be addressed to ensure the reliability and credibility of studies which use code sets.   OBJECTIVE To review methodological literature on the management of sets of clinical codes used in research on clinical databases and to provide a list of best practice recommendations for future studies and software tools.   METHODS We performed an exhaustive search for methodological papers about clinical code set engineering for re-using EHR data in research. This was supplemented with papers identified by snowball sampling. In addition, a list of e-phenotyping systems was constructed by merging references from several systematic reviews on this topic, and the processes adopted by those systems for code set management was reviewed.   RESULTS Thirty methodological papers were reviewed. Common approaches included: creating an initial list of synonyms for the condition of interest (n=20); making use of the hierarchical nature of coding terminologies during searching (n=23); reviewing sets with clinician input (n=20); and reusing and updating an existing code set (n=20). Several open source software tools (n=3) were discovered.   DISCUSSION There is a need for software tools that enable users to easily and quickly create, revise, extend, review and share code sets and we provide a list of recommendations for their design and implementation.   CONCLUSION Research re-using EHR data could be improved through the further development, more widespread use and routine reporting of the methods by which clinical codes were selected.	best practice;brute-force search;code;database;electronic health records;electronic health record system component:type:pt:electronic health record:nom:ahrq;entity name part qualifier - adopted;futures studies;objective-c;open-source software;paper;phenotype determination;reuse (action);review [publication type];revision procedure;sampling (signal processing);surgical revision;systematic review;standards characteristics	Richard Williams;Evangelos Kontopantelis;Iain E. Buchan;Niels Peek	2017	Journal of biomedical informatics	10.1016/j.jbi.2017.04.010	computer science;data mining;database	SE	-52.87328588991819	-66.42265357088738	154257
8b3cf587c78c39cbba74bae76a15f3910d09ea7f	an end-to-end hybrid algorithm for automated medication discrepancy detection	medication reconciliation;automated medication reconciliation;health informatics;drug prescriptions;information systems and communication service;machine learning;adult;management of computing and information systems;algorithms;humans;natural language processing;patient discharge;medication discrepancy detection	BACKGROUND In this study we implemented and developed state-of-the-art machine learning (ML) and natural language processing (NLP) technologies and built a computerized algorithm for medication reconciliation. Our specific aims are: (1) to develop a computerized algorithm for medication discrepancy detection between patients' discharge prescriptions (structured data) and medications documented in free-text clinical notes (unstructured data); and (2) to assess the performance of the algorithm on real-world medication reconciliation data.   METHODS We collected clinical notes and discharge prescription lists for all 271 patients enrolled in the Complex Care Medical Home Program at Cincinnati Children's Hospital Medical Center between 1/1/2010 and 12/31/2013. A double-annotated, gold-standard set of medication reconciliation data was created for this collection. We then developed a hybrid algorithm consisting of three processes: (1) a ML algorithm to identify medication entities from clinical notes, (2) a rule-based method to link medication names with their attributes, and (3) a NLP-based, hybrid approach to match medications with structured prescriptions in order to detect medication discrepancies. The performance was validated on the gold-standard medication reconciliation data, where precision (P), recall (R), F-value (F) and workload were assessed.   RESULTS The hybrid algorithm achieved 95.0%/91.6%/93.3% of P/R/F on medication entity detection and 98.7%/99.4%/99.1% of P/R/F on attribute linkage. The medication matching achieved 92.4%/90.7%/91.5% (P/R/F) on identifying matched medications in the gold-standard and 88.6%/82.5%/85.5% (P/R/F) on discrepant medications. By combining all processes, the algorithm achieved 92.4%/90.7%/91.5% (P/R/F) and 71.5%/65.2%/68.2% (P/R/F) on identifying the matched and the discrepant medications, respectively. The error analysis on algorithm outputs identified challenges to be addressed in order to improve medication discrepancy detection.   CONCLUSION By leveraging ML and NLP technologies, an end-to-end, computerized algorithm achieves promising outcome in reconciling medications between clinical notes and discharge prescriptions.	cell hybridization;discharger;discrepancy function;document completion status - documented;end-to-end principle;entity;error analysis (mathematics);hybrid algorithm;linkage (software);logic programming;matching;machine learning;name;natural language processing;note (document);patients	Qi Li;Stephen Spooner;Megan Kaiser;Nataline Lingren;Jessica Robbins;Todd Lingren;Huaxiu Tang;Imre Solti;Yizhao Ni	2015		10.1186/s12911-015-0160-8	health informatics;medicine;computer science;theoretical computer science;nursing;data mining;database	NLP	-48.76426401860802	-69.48141709207442	156023
f01d0e78ff949e52aef9857e46da258ed21a1338	expanding snomed-ct through spanish drug summaries of product characteristics		Terminologies in the biomedical field are one of the main resources used in the clinical practice. Keeping them up-to-date to meet real-world use cases is a critical operation that even in the case of well maintained terminologies such as SNOMED-CT involves much effort from domain experts. Pharmacological products or drugs are constantly being approved and made available in the market and their clinical information should be also updated in terminologies. Each new drug is provided with its Summary of Product Characteristics (SPC), a document in natural language that contains its essential information.  This paper proposes a method for populating the Spanish extension of SNOMED-CT with drug names using SPCs and representing their clinical data sections in the terminology. More precisely, the method has been applied to the therapeutic indication and the adverse reaction sections, in which disease names are recognized as named entities in the document and mapped to the terminology. The relations between the drug name and the mapped entities are also represented in the terminology based on the specific roles that they have in the document.	algorithm;ct scan;distributional semantics;latent semantic analysis;named entity;natural language;population;sensor;systematized nomenclature of medicine	Pablo Calleja;Raúl García-Castro;Guadalupe Aguado de Cea;Asunción Gómez-Pérez	2017		10.1145/3148011.3148028	drug;computer science;information retrieval;data mining;natural language;clinical practice;summary of product characteristics;use case;named-entity recognition;terminology;snomed ct	Web+IR	-48.72658029501282	-67.45190047540562	156336
cf3c030364b42b99049896daeb49679871e8959e	clinical workflow modeling in obstetrics: hepatitis b in pregnancy	clinical;decision support systems;practice guidelines;pregnancy complications	Evidence-based clinical guidelines positively effect physician decision-making. Actionable clinical guidelines that actively trigger alerts, reminders, and instructive texts will increase effectiveness. We applied Activiti, a Business Process Model and Notation language system to model a clinical guideline for the prevention of mother-to-child transmission of hepatitis B as a computerized clinical workflow. Furthermore, we implemented an interconnected Arden-Syntax-based medical rule engine, which is part of the ARDENSUITE software.	activiti;business process model and notation;business rules engine;decision making;hepatitis b	Fadi Shamoon;Harald Leitich;Jeroen S. de Bruin;Andrea Rappelsberger;Klaus-Peter Adlassnig	2017	Studies in health technology and informatics	10.3233/978-1-61499-830-3-1336	hepatitis b;pregnancy;obstetrics;workflow;computer science	SE	-55.04063222445203	-66.33529840353107	158675
5f3f20e544703018dba9bd71001aef9a9f88902d	using natural language processing to provide personalized learning opportunities from trainee clinical notes	decision support;advanced directives;altered mental status;geriatric education;natural language processing;medical education	OBJECTIVE Assessment of medical trainee learning through pre-defined competencies is now commonplace in schools of medicine. We describe a novel electronic advisor system using natural language processing (NLP) to identify two geriatric medicine competencies from medical student clinical notes in the electronic medical record: advance directives (AD) and altered mental status (AMS).   MATERIALS AND METHODS Clinical notes from third year medical students were processed using a general-purpose NLP system to identify biomedical concepts and their section context. The system analyzed these notes for relevance to AD or AMS and generated custom email alerts to students with embedded supplemental learning material customized to their notes. Recall and precision of the two advisors were evaluated by physician review. Students were given pre and post multiple choice question tests broadly covering geriatrics.   RESULTS Of 102 students approached, 66 students consented and enrolled. The system sent 393 email alerts to 54 students (82%), including 270 for AD and 123 for AMS. Precision was 100% for AD and 93% for AMS. Recall was 69% for AD and 100% for AMS. Students mentioned ADs for 43 patients, with all mentions occurring after first having received an AD reminder. Students accessed educational links 34 times from the 393 email alerts. There was no difference in pre (mean 62%) and post (mean 60%) test scores.   CONCLUSIONS The system effectively identified two educational opportunities using NLP applied to clinical notes and demonstrated a small change in student behavior. Use of electronic advisors such as these may provide a scalable model to assess specific competency elements and deliver educational opportunities.	ablepharon-macrostomia syndrome;abnormal mental state;altered level of consciousness;attitude;conflict (psychology);customize;e-mail address;education, medical;electronic health records;electronics, medical;email;embedded system;embedding;evaluation;general-purpose modeling;geriatrics;medical records;mental disorders;natural language processing;note (document);patients;personalization;precision and recall;relevance;scalability;school;sixty nine;systems design;vhdl-ams	Joshua C. Denny;Anderson Spickard;Peter Speltz;Renee Porier;Donna E. Rosenstiel;James S. Powers	2015	Journal of biomedical informatics	10.1016/j.jbi.2015.06.004	natural language processing;simulation;medicine;decision support system;pathology;computer science;bioinformatics;artificial intelligence;nursing;machine learning;data mining;database;multimedia	HCI	-55.74023084320452	-68.23863368158895	158838
c442947452bf3674c9742a54cd74643f2a730491	clinical calculators in hospital medicine: availability, classification, and needs	clinical score;decision support tool;clinical calculator;medical calculator;electronic medical record	OBJECTIVE Clinical calculators are widely used in modern clinical practice, but are not generally applied to electronic health record (EHR) systems. Important barriers to the application of these clinical calculators into existing EHR systems include the need for real-time calculation, human-calculator interaction, and data source requirements. The objective of this study was to identify, classify, and evaluate the use of available clinical calculators for clinicians in the hospital setting.   METHODS Dedicated online resources with medical calculators and providers of aggregated medical information were queried for readily available clinical calculators. Calculators were mapped by clinical categories, mechanism of calculation, and the goal of calculation. Online statistics from selected Internet resources and clinician opinion were used to assess the use of clinical calculators.   RESULTS One hundred seventy-six readily available calculators in 4 categories, 6 primary specialties, and 40 subspecialties were identified. The goals of calculation included prediction, severity, risk estimation, diagnostic, and decision-making aid. A combination of summation logic with cutoffs or rules was the most frequent mechanism of computation. Combined results, online resources, statistics, and clinician opinion identified 13 most utilized calculators.   CONCLUSION Although not an exhaustive list, a total of 176 validated calculators were identified, classified, and evaluated for usefulness. Most of these calculators are used for adult patients in the critical care or internal medicine settings. Thirteen of 176 clinical calculators were determined to be useful in our institution. All of these calculators have an interface for manual input.		Mikhail A. Dziadzko;Ognjen Gajic;Brian W. Pickering;Vitaly Herasevich	2016	Computer methods and programs in biomedicine	10.1016/j.cmpb.2016.05.006	medicine;data science;data mining	ML	-55.82906469731878	-67.5606986117255	158985
1f5fc1c748ec808a895f79acdf04e465089ff6fb	a knowledge-based scheduling system for emergency departments	emergency department;knowledge based system;interactive system;waiting time;reactive scheduling;patient priorities;health care system;knowledge base	A knowledge-based reactive scheduling system is proposed to answer the requirements of Emergency Departments (EDs). The algorithm includes detailed patient priority, arrival time, flow time and doctor load. The main aim is to determine the patients who have higher priorities initially, and then minimize their waiting times. To achieve this aim, physicians and the other related workers can use an interactive system. In this study, we evaluated the existing system by comparing the proposed system. Also, reactive scheduling cases were evaluated for some items such as decreasing the number of doctors, changing durations and entering of an urgent patient to the system. All experiments were performed with proposed algorithm and right shift rescheduling approach. 2010 Elsevier B.V. All rights reserved.		Safak Kiris;Nihat Yüzügüllü;Nurdan Ergün;Arif Alper Cevik	2010	Knowl.-Based Syst.	10.1016/j.knosys.2010.06.005	knowledge base;simulation;computer science;artificial intelligence;knowledge-based systems	Robotics	-60.781009491388666	-67.31481504721943	159299
7ed558526f261066b6f3f343f8c3353e1082592a	locating relevant patient information in electronic health record data using representations of clinical concepts and database structures		Clinicians and clinical researchers often seek information in electronic health records (EHRs) that are relevant to some concept of interest, such as a disease or finding. The heterogeneous nature of EHRs can complicate retrieval, risking incomplete results. We frame this problem as the presence of two gaps: 1) a gap between clinical concepts and their representations in EHR data and 2) a gap between data representations and their locations within EHR data structures. We bridge these gaps with a knowledge structure that comprises relationships among clinical concepts (including concepts of interest and concepts that may be instantiated in EHR data) and relationships between clinical concepts and the database structures. We make use of available knowledge resources to develop a reproducible, scalable process for creating a knowledge base that can support automated query expansion from a clinical concept to all relevant EHR data.	data structure;database storage structures;electronic health records;genetic heterogeneity;knowledge base;partial;query expansion;question (inquiry);scalability	Xuequn Pan;James J. Cimino	2014	AMIA ... Annual Symposium proceedings. AMIA Symposium		scalability;query expansion;biological ontologies;medical record;databases as topic;information retrieval;knowledge base;database;data structure;systematized nomenclature of medicine;computer science	Theory	-51.94027633505353	-66.26564908360302	159458
44e4061020198cde30acc3257351da08a88af1f1	normalization of relative and incomplete temporal expressions in clinical narratives		OBJECTIVE To improve the normalization of relative and incomplete temporal expressions (RI-TIMEXes) in clinical narratives.   METHODS We analyzed the RI-TIMEXes in temporally annotated corpora and propose two hypotheses regarding the normalization of RI-TIMEXes in the clinical narrative domain: the anchor point hypothesis and the anchor relation hypothesis. We annotated the RI-TIMEXes in three corpora to study the characteristics of RI-TMEXes in different domains. This informed the design of our RI-TIMEX normalization system for the clinical domain, which consists of an anchor point classifier, an anchor relation classifier, and a rule-based RI-TIMEX text span parser. We experimented with different feature sets and performed an error analysis for each system component.   RESULTS The annotation confirmed the hypotheses that we can simplify the RI-TIMEXes normalization task using two multi-label classifiers. Our system achieves anchor point classification, anchor relation classification, and rule-based parsing accuracy of 74.68%, 87.71%, and 57.2% (82.09% under relaxed matching criteria), respectively, on the held-out test set of the 2012 i2b2 temporal relation challenge.   DISCUSSION Experiments with feature sets reveal some interesting findings, such as: the verbal tense feature does not inform the anchor relation classification in clinical narratives as much as the tokens near the RI-TIMEX. Error analysis showed that underrepresented anchor point and anchor relation classes are difficult to detect.   CONCLUSIONS We formulate the RI-TIMEX normalization problem as a pair of multi-label classification problems. Considering only RI-TIMEX extraction and normalization, the system achieves statistically significant improvement over the RI-TIMEX results of the best systems in the 2012 i2b2 challenge.	ansi escape code;annotation;behavior;blood;body of uterus;class;corpus linguistics;error analysis (mathematics);informatics (discipline);java architecture for xml binding;large;logic programming;multi-label classification;numerous;parser;parsing;partial;rs-232;reasoning;relaxation;structure of parenchyma of lung;temporal expressions;tension;test data;test set;text corpus;united states national institutes of health	Weiyi Sun;Anna Rumshisky;Özlem Uzuner	2015	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocu004	pattern recognition;data mining;mathematics;information retrieval	NLP	-48.56177283298851	-69.81769162708144	159631
0fcf50e8874b1e88a7524eb0f41ce07c0d1fdb62	enabling synergy between psychology and natural language processing for e-government: crime reporting and investigative interview system	information extraction;e government;cognitive interview;crime;e goverment;natural language processing	We are developing an automated crime reporting and investigative interview system. The system incorporates cognitive interview techniques to maximize witness memory recall, and information extraction technology to extract and annotate crime entities from witness narratives and interview responses. Evaluations of the IE components of the system show that it captures 70 to 77% of information from witness narratives with 93 to 100% precision. Our development goal is for the system to approximate progressively the performance effectiveness of a human investigative interviewer and to generate graphical visualizations of crime report information.	approximation algorithm;e-government;entity;graphical user interface;information extraction;natural language processing;synergy	Alicia Iriberri;Chih Hao Ku;Gondy Leroy	2008			psychology;data mining;social psychology;computer security	AI	-50.3932115816693	-69.59701385016123	160038
b932094f4553b82234d03e244d93f6b667dc3c16	application of artificial intelligence to aphasia treatment	intelligent tutoring system;artificial intelligent;knowledge base;expert system	In this paper we describe SARAH a system devoted to the rehabilitation of aphasic patients. The system has many features of an intelligent tutoring system. Its origininality lies in the patient modelling method used to closely represent the patients' impairments. The patient model provide a means to adapt the rehabilitation sessions to a particular patient. The tests used to diagnose the patient's impairments and then to train him in order to make him avoid his troubles are generated and not retrieved from some knowledge base. This insures the flexibility such a system needs. Finally an expert system is used to schedule the tests during a session.	applications of artificial intelligence;expert system;knowledge base	Véronique Masson;Rene Quiniou	1990		10.1145/98894.98846	knowledge base;simulation;computer science;artificial intelligence;expert system	AI	-54.28010640125977	-67.08807449391618	160278
30821bedf8b69131750dad96d6f54f34eca98b42	methodology to build medical ontology from textual resources	software;vocabulary controlled;terminology as topic;forms and records control;pulmonary medicine;humans;natural language processing;patient discharge	In the medical field, it is now established that the maintenance of unambiguous thesauri goes through ontologies. Our research task is to help pneumologists code acts and diagnoses with a software that represents medical knowledge through a domain ontology. In this paper, we describe our general methodology aimed at knowledge engineers in order to build various types of medical ontologies based on terminology extraction from texts. The hypothesis is to apply natural language processing tools to textual patient discharge summaries to develop the resources needed to build an ontology in pneumology. Results indicate that the joint use of distributional analysis and lexico-syntactic patterns performed satisfactorily for building such ontologies.	discharger;engineering;knowledge engineer;lexico;natural language processing;nomenclature;ontology (information science);patient discharge;pulmonary medicine;terminology extraction;thesaurus	Audrey Baneyx;Jean Charlet;Marie-Christine Jaulent	2006	AMIA ... Annual Symposium proceedings. AMIA Symposium		natural language processing;upper ontology;idef5;computer science;data mining;information retrieval;process ontology	Logic	-49.313477219415354	-68.39218334071977	162657
6db949a142b06d1900d5d0f6d0eaf31d98b180c7	use of super-concepts to customize electronic medical records data display	electronic medical record	Patient medical record systems (MRS) merely offer static applications, in which mostly unstructured text is linked to coded data. In these applications the more common presentation is a time oriented one, which does not allow easily for data and information retrieval. Concept oriented views based on supper-concepts (metaterms) initially defined in CISMeF to optimize Web medical search, was implemented in our MRS as specialties views. This work shows that these terminological tools are able to facilitate information retrieval.	customize;dinner;information retrieval;linker (computing);medical records systems, computerized;medical records, problem-oriented;minimal recursion semantics	Philippe Massari;Suzanne Pereira;Benoît Thirion;A. Derville;Stéfan Jacques Darmoni	2008	Studies in health technology and informatics	10.3233/978-1-58603-864-9-845	computer science	Web+IR	-51.433741351847985	-66.47863146832704	162747
d8c5cc741e9f06ac79ca3a794a6c68ba27632cc7	a hand hygiene compliance check system: brief communication on a system to improve hand hygiene compliance in hospitals and reduce infection	rreadmittance rates;hand washing;compliance rate tracking;reinforcement system;hand hygiene compliance;hand sanitation;nosocomial infections	Hand hygiene compliance is the most significant, modifiable cause of hospital-acquired infections, yet national averages for compliance rates remain unsatisfactory. Noncompliance can contribute to patient mortality, extended hospital stays, higher re-admission rates, and lower reimbursement for hospitals under the Patient Protection and Affordable Care Act. Although several hand sanitizing tracking systems currently exist, they pose problems of personal tracking, workflow interference, system maintenance concerns, among others. Considering these barriers, we created a prototype system that includes compliance rate tracking, real-time sanitization reminders, and a data archive for future studies.	acquired immunodeficiency syndrome;archive;ethanol 0.62 ml/ml topical gel;futures studies;hygiene;interference (communication);patients;prototype;real-time transcription;research data archiving;sanitization (classified information);tracking system	Tracey S. Hong;Emily C. Bush;Morgan F. Hauenstein;Alec Lafontant;Chen Li;Jonathan P. Wanderer;Jesse M. Ehrenfeld	2015	Journal of Medical Systems	10.1007/s10916-015-0253-z	intensive care medicine;medicine;surgery	OS	-58.427330734898675	-66.18593697507512	163074
1098bdfa2e108e2cd94fa06de139d0a7ce248497	reasoning and discovering novel treatments in linked social health records		Discovering novel, alternative treatment options that may have the same efficacy and patient safety as existing drugs is a challenging task for clinicians. Through research and observations, clinicians can form a hypothesis about a possible compatible option, but it is difficult to support or refute it. In this study, we present an approach that utilizes the Semantically Linked Data of different Social Health Records (SHR), which contain patient-generated, health-related contents. The SHRs can provide information about the crowd of online patients' health practices that is lacking within specific Electronic Health Records (EHR) systems. We present the Linked Data framework for building an SHR knowledge base, and describe methods for reasoning and discovering potential novel alternative treatment options, as well as an approach for gathering support that an alternative treatment can indeed be substituted for standard treatment options.	knowledge base;linked data;shr	Paolo Cappellari;Soon Ae Chun;Dennis Shpitz	2017		10.1145/3019612.3019839	rdf;social determinants of health;linked data;knowledge base;data mining;decision support system;patient safety;business;standard treatment;knowledge management	AI	-52.61352515438178	-66.84384550579665	163485
16a972540448e15b034095d07f25a46a050b16c8	information retrieval using umls-based structured queries	biological patents;biomedical journals;text mining;europe pubmed central;citation search;citation networks;research articles;abstracts;open access;life sciences;clinical guidelines;full text;rest apis;orcids;europe pmc;biomedical research;bioinformatics;literature search	"""During the last three years, we have developed and described components of ELBook, a semantically based information-retrieval system [14]. Using these components, domain experts can specify a query model, indexers can use the query model to index documents, and end-users can search these documents for instances of indexed queries. Project Goals and Design The goal of our project is to make it easier to find crucial information in online medical resources. As more full text material becomes available on the World Wide Web, there is an increased need to develop better search mechanisms. In time-critical medical situations, clinicians need to be able to pinpoint answers to diagnostic and treatment questions. We have developed novel methods for indexing and searching medical textbooks. The methods work by describing questions that the material was written to answer. An example question is """"What antimicrobial agent can be used in the treatment of organism <E.Coli> in patients with infection <meningitis> with underlying disease <renal failure> or special condition <allergy to penicillin>?"""" Our goal was to associate these questions with passages of textbooks (i.e., index documents with these questions), and then retrieve appropriate passages depending on specific questions that users pose to ELBook. We first built a specialized search engine that matches the questions asked by the user to indexed questions in documents. Next, we developed ISAID, an indexing tool that is able to suggest which questions are being answered by specific portions of the text of documents. Users of this tool can fill in appropriate values for concepts such as organism or infection in such questions. The indexing process is semi-automated because of the difficulties of computer-based natural language understanding: a human is required to review automatically generated, provisional markup for correctness and completeness. We have reported the performance of knowledge-based natural language processing techniques to identify domain concepts and relations in full text sources [3,4]. These methods used the UMLS to correctly identify semantic types (e.g., amoxicillin is a drug) for about 2/3 of the medical concepts in the medical textbook chapters we have tested. In addition to the specialized search engine and ISAID, we developed QueryEditor, a knowledgeacquisition tool that creates and maintains the description of the queries. Once the queries are described, the user can generate the templates for the indexing tool and the HTML queries for the search engine. Using the query editor, we have described a set of core medical questions that can be used as the basis for deriving question sets for more specialized areas of medicine, such as infectious disease, patient education, or military medicine."""	correctness (computer science);document;html editor;indexed grammar;information retrieval;list of academic databases and search engines;markup language;natural language processing;natural language understanding;question answering;semiconductor industry;subject-matter expert;web search engine;window of opportunity;world wide web	Lawrence M. Fagan;Daniel C. Berrios;Albert Chan;Russell J. Cucina;Anupam Datta;Maulik K. Shah;Sujith Surendran	2001			query expansion;ranking;computer science;data science;data mining;information retrieval;query language	DB	-50.37477992108909	-66.83646412145175	163615
19b658a61d9875a8fdd4080e16781110b5998f1b	hybrid curation of gene–mutation relations combining automated extraction and crowdsourcing	genomics;databases genetic;genetic predisposition to disease;humans;data curation;computational biology;information storage and retrieval;natural language processing;crowdsourcing;mutation	BACKGROUND This article describes capture of biological information using a hybrid approach that combines natural language processing to extract biological entities and crowdsourcing with annotators recruited via Amazon Mechanical Turk to judge correctness of candidate biological relations. These techniques were applied to extract gene- mutation relations from biomedical abstracts with the goal of supporting production scale capture of gene-mutation-disease findings as an open source resource for personalized medicine.   RESULTS The hybrid system could be configured to provide good performance for gene-mutation extraction (precision ∼82%; recall ∼70% against an expert-generated gold standard) at a cost of $0.76 per abstract. This demonstrates that crowd labor platforms such as Amazon Mechanical Turk can be used to recruit quality annotators, even in an application requiring subject matter expertise; aggregated Turker judgments for gene-mutation relations exceeded 90% accuracy. Over half of the precision errors were due to mismatches against the gold standard hidden from annotator view (e.g., incorrect EntrezGene identifier or incorrect mutation position extracted), or incomplete task instructions (e.g., the need to exclude nonhuman mutations).   CONCLUSIONS The hybrid curation model provides a readily scalable cost-effective approach to curation, particularly if coupled with expert human review to filter precision errors. We plan to generalize the framework and make it available as open source software.   DATABASE URL http://www.mitre.org/publications/technical-papers/hybrid-curation-of-gene-mutation-relations-combining-automated.	abstract summary;access network;aggregate data;amazon mechanical turk;amazona;appendiceal neoplasms;appendix;correctness (computer science);crowdsourcing;digital curation;elfacos ow 100;end-to-end principle;entity;epidemiologic research design;experiment;extraction;hybrid system;identifier;ideographic rapporteur group;judgment;mutation;natural language processing;nonlocal lagrangian;open-source software;partial;personalization;pool (computer science);precision medicine;preparation;scalability;small;sol-gel;stage level 1;stage level 2;subject matter expert turing test;torg-winchester syndrome;the turk;uniform resource locator;united states national institutes of health;waardenburg syndrome, type 4a;corporation;interest	John D. Burger;Emily Doughty;Ritu Khare;Chih-Hsuan Wei;Rajashree Mishra;John S. Aberdeen;David Tresner-Kirsch;Ben Wellner;Maricel G. Kann;Zhiyong Lu;Lynette Hirschman	2014		10.1093/database/bau094	mutation;genomics;data curation;computer science;bioinformatics;data science;data mining;database;world wide web;genetics;crowdsourcing;information retrieval	NLP	-48.955126229810105	-70.23599771557224	164134
0c3e512fd1e1b1586efdb3511ed8c88c71ad64df	design problems encountered in the development of a medical expert system	system design medical expert system doc congenital heart defects diagnosis training tool pediatric cardiologists royal children s hospital melbourne;expert systems;cardiology;computer aided instruction;training;training cardiology computer aided instruction expert systems medical diagnostic computing;medical expert systems pediatrics cardiology hospitals birth disorders management training australia guidelines heart management information systems;medical expert system;system design;medical diagnostic computing;congenital heart defect	lk is paper describes design problems encountered in the development of D.O.C., a medical expert system developed as a training tool for Paediatric Cardiologists at the Royal Children’s Hospital in Melbourne. Ihe major objective of D.O.C. is to assist trainees in the diagnosis of congenital heart defects in neonates. Ihe paper outlines the specijications and guidelines for the system: describes the design problems which were confronted, and then describes the features of the system design which were developed to solve those di#iculties. 1: Rationale for the system Dr. J. Wilkinson, Director of Paediatric Cardiology at the Royal Children’s Hospital in Melbourne expressed an interest in the development of technological tools to assist his profession. Paediatric Cardiology is a speciality within a speciality: a doctor must be not only a Paediatrician but also have a demonstrable comprehension of the field of Cardiology as it relates to children. The breadth and depth of knowledge required is formidable, and the training is correspondingly long and intensive. The training, in the hospital, takes the form of a MasterlApprentice relationship between the trainee and the experienced Paediatric Cardiologist. There are only 17 Paediatric Cardiologists in Australia, and it thus constitutes an exceedingly rare expertise. Time spent in training new cardiologists is time spent away from what may be literally life-saving work. D.O.C. (Diagnosis Of Cardiac defects) was envisaged as an adjunct tool to the standard training, one that would be constantly available, and could be used as a consultant for specific real cases, or for imaginary but puzzling cases, and would provide access to expertise even when the expert was not available. 2: Specifications and guidelines for the development of D.O.C. The major agreed goals of the system were: to develop a readily accessible training tool for Paediatric Cardiologists at the Children ’s Hospital. Ihe major purpose of the system would be to assist the trainee to accurately diagnose Cardiac Congenital Defecth in neonates Cfirst 12 months of l f e ) on the basis of clinical features and some limited test results. to accurately encapsulate the expertise of Dr. Wilkinson to allow the reasoning process used in reaching a diagnosis to be as transparent as possible to the 232 0-8186-4260-2/93 $03.00	design rationale;expert system;imaginary time;systems design	A. Parr	1993		10.1109/ANNES.1993.323035	medicine;pediatrics;biological engineering	HCI	-58.678506511231305	-67.23549281598682	164383
a7d10bcd058a199d8b5fb936b4acf2b168541e66	automated tuberculosis detection.	clinical data;case report;center for disease control;rule based;chest radiograph;positive predictive value;electronic medical record;natural language processor;tuberculosis	OBJECTIVE To measure the accuracy of automated tuberculosis case detection.   SETTING An inner-city medical center.   INTERVENTION An electronic medical record and a clinical event monitor with a natural language processor were used to detect tuberculosis cases according to Centers for Disease Control criteria.   MEASUREMENT Cases identified by the automated system were compared to the local health department's tuberculosis registry, and positive predictive value and sensitivity were calculated.   RESULTS The best automated rule was based on tuberculosis cultures; it had a sensitivity of .89 (95% CI.75-.96) and a positive predictive value of .96 (.89-.99). All other rules had a positive predictive value less than .20. A rule based on chest radiographs had a sensitivity of .41 (.26-.57) and a positive predictive value of .03 (.02-.05), and rule the represented the overall Centers for Disease Control criteria had a sensitivity of .91 (.78-.97) and a positive predictive value of .15 (.12-.18). The culture-based rule was the most useful rule for automated case reporting to the health department, and the chest radiograph-based rule was the most useful rule for improving tuberculosis respiratory isolation compliance.   CONCLUSIONS Automated tuberculosis case detection is feasible and useful, although the predictive value of most of the clinical rules was low. The usefulness of an individual rule depends on the context in which it is used. The major challenge facing automated detection is the availability and accuracy of electronic clinical data.		George Hripcsak;Charles Knirsch;Nilesh L. Jain;Ariel Pablos-Méndez	1997	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.1997.0040376	rule-based system;medicine;pathology;computer science;artificial intelligence;surgery	AI	-55.59539760896302	-67.55750512987758	164725
023d7147647172418d23b4276a266ce63df88be5	construction of semantic analysis system for traditional chinese medicine unstructured medical records	programming language semantics;medical records;latent semantic analysis corpus database of traditional chinese medicine chinese word segmentation;traditional chinese medicine;random sampling;programming language semantics knowledge based systems medical diagnostic computing medical information systems;data mining;corpus database of traditional chinese medicine;kidney disease;chinese word segmentation;medical information systems;system design;intelligence analysis;data mining method traditional chinese medicine medical records unstructured medical records intelligent analysis system language description characteristics corpus database training chinese word segmentation latent semantic analysis modeling language characteristics kidney disease symptom descriptions;latent semantic analysis;medical diagnostic computing;knowledge based systems;semantics databases tagging humans training resource description framework kidney;semantic analysis	Purpose: To develop an intelligent analysis system applying in Traditional Chinese Medicine (TCM) medical records, according to the language description characteristics of TCM; Methodology: Constructing semantic analysis system by several relevant technique, such as corpus database training, Chinese word segmentation and latent semantic analysis modeling etc, through the study of the language characteristics of medical records and the demands in research of TCM; Findings: A corpus database for specialties has been established by processing over 50,000 medical records of TCM; Over 1,400 records regarding to kidney disease have been evaluated automatically by their symptoms descriptions; The result shows a high consistency compared with the human analysis of random sample; Conclusion: It displays the feasibility and advantage of the whole system designing for unstructured medical records, and provides data mining method for massive unstructured data of TCM in integrated solution.	semantic analysis (compilers)	Heng Weng;Chuanliang Yi;Yingzi Lin;Yuping Zeng;Yang Li;Siliang Wu;Dacan Chen;Haoyang Fu	2011		10.1109/BIBMW.2011.6112479	natural language processing;traditional chinese medicine;sampling;intelligence analysis;latent semantic analysis;computer science;data science;data mining;medical record;systems design	NLP	-49.15730887042393	-68.29185818788669	165146
22b961b1002598c93c613f23197a290ae44a6f67	towards extraction of conceptual structures from electronic health records	case history;language technology;electronic health record;hybrid approach;conceptual graph	This paper presents the general framework and the current results of a project that aims to develop a system for knowledge discovery and extraction from the texts of Electronic Health Records in Bulgarian language. The proposed hybrid approach integrates language technologies and conceptual processing. The system generates conceptual graphs encoding the patient case history, which contains templates for the patient's diseases, symptoms and treatments. We describe simple inference in the generated graphs resource bank. Some experiments and their evaluation are presented in the article.		Svetla Boytcheva;Galia Angelova	2009		10.1007/978-3-642-03079-6_8	natural language processing;conceptual graph;computer science;artificial intelligence;data science;data mining;language technology	HCI	-49.68772075294805	-67.93699350689256	166753
118c386da06a9ce552b3893aeb69d23f8ae27584	medical knowledge graph construction by aligning large biomedical datasets		Building large Knowledge Bases can be realised by aligning and integrating existing data sources. To support AI-based digital healthcare services within Babylon Health significant effort to build a large medical KB was recently undertaken. To realise this goal a highly configurable and modular ontology integration pipeline has been created which works as follows: an initial ontology is used as a seed KB (KB0) and additional data sources are integrated into it creating new extended versions of KB0. The integration process is based on a Matching phase, an Aggregation phrase, and a final PostProcessing phase. In the Matching phase the following matchers can be used:	babylon;base;data sources;graph - visual representation;kilobyte;knowledge bases;knowledge graph;ontology;version	Giorgos Stoilos;David Geleta;Jetendr Shamdasani;Mohammad Khodadadi	2018				AI	-49.4137556881293	-66.86502052833914	167849
1985a54cb0628da6fe74cf0e0230e491472c3c41	designing and evaluating a clustering system for organizing and integrating patient drug outcomes in personal health messages.	support vector machines;health education;data mining;terminology as topic;cluster analysis;drug therapy;outcome assessment health care;adverse drug reaction reporting systems;humans;mathematical concepts;pubmed;automatic data processing	Patient outcomes to drugs vary, but physicians currently have little data about individual responses. We designed a comprehensive system to organize and integrate patient outcomes utilizing semantic analysis, which groups large collections of personal comments into a series of topics. A prototype implementation was built to extract situational evidences by filtering and digesting user comments provided by patients. Our methods do not require extensive training or dictionaries, while categorizing comments based on expert opinions from standard source, or patient-specified categories. This system has been tested with sample health messages from our unique dataset from Yahoo! Groups, containing 12M personal messages from 27K public groups in Health and Wellness. We have performed an extensive evaluation of the clustering results with medical students. Evaluated results show high quality of labeled clustering, promising an effective automatic system for discovering patient outcomes from large volumes of health information.	aspirin;categories;categorization;chronic obstructive airway disease;cluster analysis;collections (publication);comparative effectiveness research;dictionaries as topic;dictionary;display resolution;document completion status - documented;f1 score;headache;ibuprofen;information needs;interactivity;lung diseases, obstructive;lung diseases;migraine disorders;organizing (structure);patients;primary lateral sclerosis, adult, 1;principle of good enough;probabilistic latent semantic analysis;programming paradigm;prototype;semantic analysis (compilers);sentiment analysis;silo (dataset);social network;tracer;united states department of agriculture;whole earth 'lectronic link;benefit;contents - htmllinktype;message;statistical cluster	Yunliang Jiang;Qingzi Liao;Qian Cheng;Richard Berlin;Bruce R. Schatz	2012	AMIA ... Annual Symposium proceedings. AMIA Symposium		medicine;knowledge management;data science;data mining	Web+IR	-48.83445570488541	-69.9939022240667	167864
8d4d0ec9bb53537c303caf4a1761f3c9a7d75bd1	variance analysis in task-time matrix clinical pathways	databases;hospitals;electrocardiography;computational modeling;hidden markov models;logic gates;surgery	Clinical pathways are popular healthcare management tools to standardise care and ensure quality. Measuring pathway conformance and analysing variances gives valuable feedback in the context of care improvement trajectories. The Business Process Model and Notation (BPMN) language and Task-Time matrices are popular ways to model clinical pathways. A key step in variance analysis involves the computation of optimal alignments between the pathway model and patient-specific traces. This paper presents for this step a new algorithm which reduces the time for finding deviations from hours to minutes. A case study on variance analysis is undertaken, where a clinical pathway from the practice and a large set of patients data from an EMR database are used. The results demonstrate that automated variance analysis between BPMN Task-Time models and real-life EMR data is feasible. We also provide meaningful insights for further improvement.	algorithm;business process model and notation;computation;conformance testing;excalibur: morgana's revenge;gene regulatory network;real life;tracing (software)	Hui Yan;Pieter Van Gorp;Uzay Kaymak;Lei Ji;Xudong Lu;Choo Chiap Chiau;Hendrikus H. M. Korsten;Huilong Duan	2017	2017 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI)	10.1109/BHI.2017.7897253	simulation;computer science;data science;data mining	SE	-53.910549207812856	-69.07660465238813	170126
cccdcc3468ab1b3542040a67a325ea28071c2a71	a filmless radiology department in a full digital regional hospital: quantitative evaluation of the increased quality and efficiency	internal medicine;workload;diagnostic imaging;pacs;patient stay;financial management;hospitals district;computer user training;radiology information systems;cost saving;efficiency;performance;quality assurance health care;statistical test;information access;hospital information systems;picture archiving and communication system;patient care;system performance;length of stay;business model;total quality management;process monitoring;time factors;internet;pacs system;diagnostic techniques neurological;italy;thoracic surgical procedures;cost effectiveness;computer terminals;humans;cost savings;work simplification;quantitative evaluation;efficiency organizational	Reggio Emilia hospital installed Picture Archiving and Communications Systems (PACS) as the final step towards a completely digital clinical environment completing the HIS/EMR and 1,400 web/terminals for patient information access. Financial benefits throughout the hospital were assessed upfront and measured periodically. Key indicators (radiology exam turnaround time, number of radiology procedures performed, inpatients length of stay before and after the PACS implementation, etc.) were analyzed and values were statistically tested to assess workflow and productivity improvements. The hospital went “filmless” in 28 weeks. Between the half of 2004 and the respective period in 2003, overall Radiology Department productivity increased by 12%, TAT improved by more than 60%. Timelier patient care resulted in decreased lengths of stay. Neurology alone experienced a 12% improvement in average patient stay. To quantify the impact of PACS on the average hospital stays and the expected productivity benefits to inpatient productivity were used a “high level” and a “detailed” business model. Annual financial upsides have exceeded $1.9 millions/year. A well-planned PACS deployment simplifies imaging workflow and improves patient care throughout the hospital while delivering substantial financial benefits. Staff buy-in was the key in this process and on-going training and process monitoring are a must.	deploy;excalibur: morgana's revenge;file archiver;health care;high-level programming language;information access;mathematical optimization;neurology speciality;picture archiving and communication system;radiology;speech disorders;speech recognition;thematic apperception test;benefit;inpatient	Andrea Nitrosi;Giovanni Borasi;Franco Nicoli;Gino Modigliani;Andrea Botti;Marco Bertolini;Pietro Notari	2007		10.1007/s10278-007-9006-y	medical imaging;financial management;simulation;radiology;medicine;computer performance;picture archiving and communication system	HCI	-60.802664683895465	-66.56259435047484	171020
496472007350412a8c53f3f5524ae98e69391d1d	a computational system based on ontologies to automate the mapping process of medical reports into structured databases		Abstract We have developed, in collaboration with medical and computer experts, the ontology-based Medical Report Mapping Process to support the transformation of unstructured reports into a structured representation. Nevertheless, the techniques employed in this two-phase process must be performed individually and manually by computer instructions, which hinder their use by users not familiar with such language. Thereby, this work proposes a tool to automate and optimize this process by integrating its techniques in a computational system, which was built using a software engineering prototyping approach. This system was experimentally evaluated by applying it to a set of 100 textual reports. The first phase decreased the total number of phrases (853) and words (2520) by 82.25% (48) and 92.70% (184), respectively. In the second phase, 100% of the relevant pieces of information (previously established) present in the reports were transcribed. Also, the second phase was applied, using the same configuration as the first study, in another set with 250 textual reports, resulting in a mapping rate of 99.74%. The unprocessed and unmapped words, regarding both experimental evaluations, were recorded for later inclusion into the ontology. By using this system, efficient and scalable investigations can be performed from medical reports, contributing to generate new knowledge. Also, the system facilitates the definition of these structures due to the feasibility to analyze different sentences in unique phrase sets.	database;model of computation;ontology (information science)	Jefferson Tales Oliva;Huei Diana Lee;Newton Spolaôr;Weber Shoity Resende Takaki;Cláudio Saddy Rodrigues Coy;João José Fagundes;Feng Chung Wu	2019	Expert Syst. Appl.	10.1016/j.eswa.2018.08.004	data mining;scalability;ontology (information science);ontology;phrase;computer science	DB	-50.06634247942718	-67.62858115952893	172098
8f5fd1d2a8b34d3d77445061e0ab5755ab1d658a	assessing data relevance for automated generation of a clinical summary		Clinicians perform many tasks in their daily work requiring summarization of clinical data. However, as technology makes more data available, the challenges of data overload become ever more significant. As interoperable data exchange between hospitals becomes more common, there is an increased need for tools to summarize information. Our goal is to develop automated tools to aid clinical data summarization. Structured interviews were conducted on physicians to identify information from an electronic health record they considered relevant to explaining the patients medical history. Desirable data types were systematically evaluated using qualitative and quantitative analysis to assess data categories and patterns of data use. We report here on the implications of these results for the design of automated tools for summarization of patient history.	categories;clinical data;electronic health records;interoperability;patients;relevance	Tielman Van Vleck;Daniel M. Stein;Peter D. Stetson;Stephen B. Johnson	2007	AMIA ... Annual Symposium proceedings. AMIA Symposium		documentation;data type;data science;automatic summarization;structured interview;medical record;information retrieval;interoperability;data exchange;information overload;computer science	SE	-52.247101191537226	-66.68074819088305	173954
b389b1b11ab103de52f21e1ba07134dbc29bf0bf	review: medical diagnostic decision support systems - past, present, and future: a threaded bibliography and brief commentary	decision support;large scale;decision support system;electronic medical record;primary care	"""Articles about medical diagnostic decision support (MDDS) systems often begin with a disclaimer such as, """"despite many years of research and millions of dollars of expenditures on medical diagnostic systems, none is in widespread use at the present time."""" While this statement remains true in the sense that no single diagnostic system is in widespread use, it is misleading with regard to the state of the art of these systems. Diagnostic systems, many simple and some complex, are now ubiquitous, and research on MDDS systems is growing. The nature of MDDS systems has diversified over time. The prospects for adoption of large-scale diagnostic systems are better now than ever before, due to enthusiasm for implementation of the electronic medical record in academic, commercial, and primary care settings. Diagnostic decision support systems have become an established component of medical technology. This paper provides a review and a threaded bibliography for some of the important work on MDDS systems over the years from 1954 to 1993."""	bibliography;decision support system;electronic health records;electronics, medical;medical records;primary health care;published comment	Randolph A. Miller	1994	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.1994.95236141	medicine;decision support system;computer science;artificial intelligence;data mining;operations research	HCI	-58.78339785697551	-68.35281570135469	174118
90f813474cf612ff5eee5d1f2b8c92f269d426d2	a systematic review of the types and causes of prescribing errors generated from using computerized provider order entry systems in primary and secondary care	clinical decision support;medication errors;computerized provider order entry;patient safety;alerts	Objective To understand the different types and causes of prescribing errors associated with computerized provider order entry (CPOE) systems, and recommend improvements in these systems.   Materials and Methods We conducted a systematic review of the literature published between January 2004 and June 2015 using three large databases: the Cumulative Index to Nursing and Allied Health Literature, Embase, and Medline. Studies that reported qualitative data about the types and causes of these errors were included. A narrative synthesis of all eligible studies was undertaken.   Results A total of 1185 publications were identified, of which 34 were included in the review. We identified 8 key themes associated with CPOE-related prescribing errors: computer screen display, drop-down menus and auto-population, wording, default settings, nonintuitive or inflexible ordering, repeat prescriptions and automated processes, users' work processes, and clinical decision support systems. Displaying an incomplete list of a patient's medications on the computer screen often contributed to prescribing errors. Lack of system flexibility resulted in users employing error-prone workarounds, such as the addition of contradictory free-text comments. Users' misinterpretations of how text was presented in CPOE systems were also linked with the occurrence of prescribing errors.   Discussion and Conclusions Human factors design is important to reduce error rates. Drop-down menus should be designed with safeguards to decrease the likelihood of selection errors. Development of more sophisticated clinical decision support, which can perform checks on free-text, may also prevent errors. Further research is needed to ensure that systems minimize error likelihood and meet users' workflow expectations.		Clare L. Brown;Helen L. Mulcaster;Katherine L. Triffitt;Dean F. Sittig;Joan S. Ash;Katie Reygate;Andrew K. Husband;David W. Bates;Sarah Patricia Slight	2016	Journal of the American Medical Informatics Association : JAMIA	10.1093/jamia/ocw119	clinical decision support system;medicine;computer science;knowledge management;artificial intelligence;data mining	HCI	-59.73421579970463	-66.38502238652393	174456
3055f55f1b19d0580d7bcae60b3a63d9595110b0	ontological analysis of snomed ct	health informatics;disease;information systems and communication service;terminology as topic;snomed ct;formal reasoning;management of computing and information systems;hungary;systematized nomenclature of medicine;evaluation studies as topic;point of view;systems integration;knowledge engineering	BACKGROUND SNOMED CT is the most comprehensive medical terminology. However, its use for intelligent services based on formal reasoning is questionable.   METHODS The analysis of the structure of SNOMED CT is based on the formal top-level ontology DOLCE.   RESULTS The analysis revealed several ontological and knowledge-engineering errors, the most important are errors in the hierarchy (mostly from an ontological point of view, but also regarding medical aspects) and the mixing of subsumption relations with other types (mostly 'part of').   CONCLUSION The found errors impede formal reasoning. The paper presents a possible way to correct these problems.	ct scan;knowledge engineering;pet/ct scan;reason;subsumption architecture;systematized nomenclature of medicine clinical terms;upper ontology;x-ray computed tomography	Gergely Héja;György Surján;Péter Varga	2008	BMC Medical Informatics and Decision Making	10.1186/1472-6947-8-S1-S8	medcin;health informatics;medicine;systematized nomenclature of medicine;computer science;knowledge management;nursing;knowledge engineering;snomed ct;data mining;information retrieval;system integration	AI	-51.19249817191115	-67.46862115697624	175021
1b82b434004754659044831f041d17873c240108	tracing the formalization steps of textual guidelines.	clinical guideline;authoring tool	This paper presents a new guideline authoring tool, called Guideline Markup Tool (GMT). It proposes two useful features, which are missing in existing tools. First, it facilitates the translation of a free-text guideline into a formal representation, providing special XML macros. Second, it can be used to create links between the original guideline and its formal representation. Therefore, the GMT eases the implementation of clinical guidelines in a formal representation, which can be used in monitoring and therapy planning systems.	xml	Peter Votruba;Silvia Miksch;Andreas Seyfang;Robert Kosara	2004	Studies in health technology and informatics	10.3233/978-1-60750-944-8-172	knowledge management;software engineering;data mining	HCI	-51.55091148826048	-67.08551254200135	175425
72e325d6d55c4d52fbcac0ed7eb5b3da40c97016	understanding and preventing wrong-patient electronic orders: a randomized controlled trial		OBJECTIVE To evaluate systems for estimating and preventing wrong-patient electronic orders in computerized physician order entry systems with a two-phase study.   MATERIALS AND METHODS In phase 1, from May to August 2010, the effectiveness of a 'retract-and-reorder' measurement tool was assessed that identified orders placed on a patient, promptly retracted, and then reordered by the same provider on a different patient as a marker for wrong-patient electronic orders. This tool was then used to estimate the frequency of wrong-patient electronic orders in four hospitals in 2009. In phase 2, from December 2010 to June 2011, a three-armed randomized controlled trial was conducted to evaluate the efficacy of two distinct interventions aimed at preventing these errors by reverifying patient identification: an 'ID-verify alert', and an 'ID-reentry function'.   RESULTS The retract-and-reorder measurement tool effectively identified 170 of 223 events as wrong-patient electronic orders, resulting in a positive predictive value of 76.2% (95% CI 70.6% to 81.9%). Using this tool it was estimated that 5246 electronic orders were placed on wrong patients in 2009. In phase 2, 901 776 ordering sessions among 4028 providers were examined. Compared with control, the ID-verify alert reduced the odds of a retract-and-reorder event (OR 0.84, 95% CI 0.72 to 0.98), but the ID-reentry function reduced the odds by a larger magnitude (OR 0.60, 95% CI 0.50 to 0.71).   DISCUSSION AND CONCLUSION Wrong-patient electronic orders occur frequently with computerized provider order entry systems, and electronic interventions can reduce the risk of these errors occurring.	alert:type:point in time:^patient:nominal;estimated;id-wsf;large;order management system;patients;positive predictive value of diagnostic test;randomized algorithm;two-phase commit protocol;negative resolution <retracted>;orders - hl7publishingdomain	Jason S. Adelman;Gary E. Kalkut;Clyde B. Schechter;Jeffrey M. Weiss;Matthew A. Berger;Stan H. Reissman;Hillel W. Cohen;Stephen J. Lorenzen;Daniel A. Burack;William N. Southern	2013	Journal of the American Medical Informatics Association : JAMIA	10.1136/amiajnl-2012-001055	medicine;data mining	AI	-59.86023978491174	-66.55097678758715	177701
07277a1539c1d39207c1c3f75f267fbabbde32e0	toward a veterinary informatics research agenda: an analysis of the pubmed-indexed literature	medical informatics;research agenda;medical subject headings;indexation;veterinary medicine;medical informatic;subject headings;information storage and retrieval;pubmed;veterinary	PURPOSE Veterinary medicine and human health are inextricably intertwined. Effective tracking of veterinary information - veterinary informatics - impacts not only veterinary medicine, but also public health, informatics research, and clinical care. However, veterinary informatics has received little attention from the general biomedical informatics community.   METHODS To identify both active and under-researched areas in veterinary informatics, we retrieved Medical Subject Heading (MeSH) descriptors for veterinary informatics-related citations and analyzed them by topic category, animal type, and journal.   RESULTS We found that the categories of veterinary informatics with the most growth were information/bibliographical retrieval, hardware/programming, and radiology/imaging. Less than two articles per year were published in the areas of computerized veterinary medical records, clinical decision support, standards, and controlled vocabularies. Veterinary informatics articles primarily address production animals such as cattle and sheep, and companion animals such as cats and dogs. Six journals account for 31% of the veterinary informatics literature, 35 journals account for 66%.   CONCLUSIONS Veterinary informatics remains an embryonic field with relatively few publications. With the exception of radiology/imaging, published articles are primarily focused on non-clinical areas such as hardware/programming and information retrieval. There are very few publications on controlled vocabularies, standards, methodologies for integrating disparate systems, computerized medical records, clinical decision support systems, and system usability. The lack of publications in these areas may hamper efforts to collect and track animal health data at a time when such data are potentially critical to human health.		Kimberly A. Smith-Akin;Charles F. Bearden;Stephen T. Pittenger;Elmer V. Bernstam	2007	International journal of medical informatics	10.1016/j.ijmedinf.2006.02.009	health administration informatics;health informatics;public health informatics;controlled vocabulary;medicine;pathology;data science	HCI	-54.66242653199795	-68.78543266292964	177811
58250fdc7515fb4827353c19c81d0f3a8f2e028a	research paper: an experiment comparing lexical and statistical methods for extracting mesh terms from clinical free text	statistical method;indexation;hybrid system;electronic medical record	OBJECTIVE A primary goal of the University of Pittsburgh's 1990-94 UMLS-sponsored effort was to develop and evaluate PostDoc (a lexical indexing system) and Pindex (a statistical indexing system) comparatively, and then in combination as a hybrid system. Each system takes as input a portion of the free text from a narrative part of a patient's electronic medical record and returns a list of suggested MeSH terms to use in formulating a Medline search that includes concepts in the text. This paper describes the systems and reports an evaluation. The intent is for this evaluation to serve as a step toward the eventual realization of systems that assist healthcare personnel in using the electronic medical record to construct patient-specific searches of Medline.   DESIGN The authors tested the performances of PostDoc, Pindex, and a hybrid system, using text taken from randomly selected clinical records, which were stratified to include six radiology reports, six pathology reports, and six discharge summaries. They identified concepts in the clinical records that might conceivably be used in performing a patient-specific Medline search. Each system was given the free text of each record as an input. The extent to which a system-derived list of MeSH terms captured the relevant concepts in these documents was determined based on blinded assessments by the authors.   RESULTS PostDoc output a mean of approximately 19 MeSH terms per report, which included about 40% of the relevant report concepts. Pindex output a mean of approximately 57 terms per report and captured about 45% of the relevant report concepts. A hybrid system captured approximately 66% of the relevant concepts and output about 71 terms per report.   CONCLUSION The outputs of PostDoc and Pindex are complementary in capturing MeSH terms from clinical free text. The results suggest possible approaches to reduce the number of terms output while maintaining the percentage of terms captured, including the use of UMLS semantic types to constrain the output list to contain only clinically relevant MeSH terms.	cell hybridization;discharger;electronic health records;electronics, medical;evaluation procedure;hybrid system;indexes;medline;medical records systems, computerized;patients;performance;radiology;randomness;unified medical language system;pathology report	Gregory F. Cooper;Randolph A. Miller	1998	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.1998.0050062	natural language processing;medicine;computer science;artificial intelligence;data science;machine learning;data mining;database;information retrieval;statistics;hybrid system	Web+IR	-48.87484980022719	-68.69847972705475	179939
44ef9aa9a561c0196b7689eb2fe411200baf1e57	developing a knowledge base to support the annotation of ultrasound images of ectopic pregnancy	data mining and knowledge discovery;computational biology bioinformatics;ectopic pregnancy;algorithms;combinatorial libraries;computer appl in life sciences;application ontology;bioinformatics;knowledge base	BACKGROUND Ectopic pregnancy is a frequent early complication of pregnancy associated with significant rates of morbidly and mortality. The positive diagnosis of this condition is established through transvaginal ultrasound scanning. The timing of diagnosis depends on the operator expertise in identifying the signs of ectopic pregnancy, which varies dramatically among medical staff with heterogeneous training. Developing decision support systems in this context is expected to improve the identification of these signs and subsequently improve the quality of care. In this article, we present a new knowledge base for ectopic pregnancy, and we demonstrate its use on the annotation of clinical images.   RESULTS The knowledge base is supported by an application ontology, which provides the taxonomy, the vocabulary and definitions for 24 types and 81 signs of ectopic pregnancy, 484 anatomical structures and 32 technical elements for image acquisition. The knowledge base provides a sign-centric model of the domain, with the relations of signs to ectopic pregnancy types, anatomical structures and the technical elements. The evaluation of the ontology and knowledge base demonstrated a positive feedback from a panel of 17 medical users. Leveraging these semantic resources, we developed an application for the annotation of ultrasound images. Using this application, 6 operators achieved a precision of 0.83 for the identification of signs in 208 ultrasound images corresponding to 35 clinical cases of ectopic pregnancy.   CONCLUSIONS We developed a new ectopic pregnancy knowledge base for the annotation of ultrasound images. The use of this knowledge base for the annotation of ultrasound images of ectopic pregnancy showed promising results from the perspective of clinical decision support system development. Other gynecological disorders and fetal anomalies may benefit from our approach.	anatomic structures;annotation;cns disorder;clinical decision support system;decision support systems, clinical;ectopic pregnancy;female genital diseases;fetal diseases;genetic heterogeneity;gynecology;knowledge base;medical ultrasound;positive feedback;pregnancy complications;taxonomy;ultrasonography;vocabulary;transvaginal ultrasound	Ferdinand Dhombres;Paul Maurice;Stéphanie Friszer;Lucie Guilbaud;Nathalie Lelong;Babak Khoshnood;Jean Charlet;Nicolas Perrot;Eric Jauniaux;Davor Jurkovic;Jean-Marie Jouannic	2017		10.1186/s13326-017-0117-1	data mining;clinical decision support system;knowledge base;computer science;decision support system;ontology;ectopic pregnancy;pregnancy;annotation;vocabulary	NLP	-55.28320255325071	-67.76219434642647	180839
17942439bcd62542809a913e66195674d3ab2184	experiments to create ontology-based disease models for diabetic retinopathy from different biomedical resources		According to the World Health Organisation diabetic retinopathy (DR) is a high priority eye disease. This paper investigates a method for creating disease models for DR using the ontologies BioTopLite2 and SNOMED CT and different biomedical resources: 1) consultation notes from anonymised electronic health records; 2) the clinical practice guideline for DR by the American Academy of Ophthalmology; 3) the BMJ Best Practice for DR; and 4) neural language models from Deep Learning (CBOW and Skip-gram) using a 14M PubMed dataset. As SNOMED CT does not contain disease models, the novelty of this study is twofold: a) evaluation of the utility of CBOW and Skip-gram for obtaining DR disease models from the biomedical literature; and b) the proposed method for building ontology-based disease models exploiting SNOMED CT reference sets. In our method, we first propose a representation of SNOMED CT reference sets for DR in OWL by extracting upper modules from SNOMED CT. Secondly, we use content ontology design patterns with BioTopLite2 and SNOMED CT that act as templates to semantically represent clinical content in OWL. We report on the effectiveness of the method.	academy;best practice;deep learning;design pattern;emoticon;language model;ontology (information science);pubmed;skip list;systematized nomenclature of medicine	Mercedes Argüello Casteleiro;Catalina Martínez-Costa;Jose Julio Des Diz;Maria Jesus Fernandez Prieto;Chris Wroe;Diego Maseda-Fernandez;George Demetriou;Goran Nenadic;John A. Keane;Stefan Schulz;Robert Stevens	2017			ontology;diabetic retinopathy;disease;computer science;bioinformatics	Web+IR	-50.17998225195233	-67.25251994368776	180876
9f7dec26c10a361dbb9352cf4253a74eac315d1d	mapping equivalence of german emergency department medical record concepts with snomed ct after implementation with hl7 cda	emergency medicine;health level seven;logical observation identifiers names and codes;snomed ct;health information exchange	INTRODUCTION The German Emergency Department Medical Record (GEDMR) was created by medical domain experts and healthcare providers providing a dataset as well as a form. The trauma module of GEDMR was syntactically standardized using HL7 CDA and semantically standardized using different terminologies including SNOMED CT, LOINC and proprietary coding systems. This study depicts the mapping accuracy with aforementioned syntactical and semantical standards in general and especially the content coverage of SNOMED CT.   METHODS The specification of GEDMR (V2015.1) concepts with eHealth-standards HL7-CDA, LOINC, SNOMED CT was analyzed. A content coverage assessment was made using the ISO TR 12300 rating scheme, following descriptive analysis.   RESULTS The trauma module of GEDMR contains 489 concepts, with 202 concepts expressed via HL7 CDA structure. It is possible to code 89 % of the remaining concepts via SNOMED CT. 79 % provide an advanced level of semantic interoperability, as they represent the source information either lexically or as an approved synonym.   DISCUSSION The terminology binding problem is relevant when combining different standards for syntactic and semantic interoperability with best practice documents and reference specifications providing guidance. A national license and extension for SNOMED CT in Germany as well as an ongoing effort in contributing to the International Version of SNOMED CT would be necessary to gain full coverage for concepts in German Emergency Medicine and to leverage the associated standardization process.	.cda file;best practice;binding problem;ct scan;congenital dyserythropoietic anemia;eighty nine;health level 7;health level seven;isoproterenol;medical records, problem-oriented;name binding;pet/ct scan;semantic interoperability;seventy nine;silo (dataset);specification;systematized nomenclature of medicine clinical terms;turing completeness;standards characteristics	Dominik Brammen;Heike Dewenter;Kai U. Heitmann;Volker Thiemann;Raphael W. Majeed;Felix Walcher;Rainer Röhrig;Sylvia Thun	2017	Studies in health technology and informatics	10.3233/978-1-61499-808-2-175	equivalence (measure theory);medcin;medical record;emergency department;emergency medicine;medicine;german;snomed ct	AI	-50.90495666748627	-67.3732573274008	182310
71975f2a27e010970e43e5d61d1d4ae32af4a367	research paper: the utility of adding retrospective medication profiling to computerized provider order entry in an ambulatory care population	study design;adverse drug event;drug utilization;research paper;negative binomial regression;usual care;computerized provider order entry;drug interaction;adverse drug reaction;ambulatory care	BACKGROUND We assessed whether medication safety improved when a medication profiling program was added to a computerized provider order entry system.   DESIGN Between June 2001 and January 2002 we profiled outpatients with potential prescribing errors using computerized retrospective drug utilization software. We focused primarily on drug interactions. Patients were randomly assigned either to Provider Feedback or to Usual Care. Subsequent adverse drug event (ADE) incidence and other outcomes, including ADE preventability and severity, occurring up to 1 year following the last profiling date were evaluated retrospectively by a pharmacist blinded to patient assignment.   MEASUREMENTS Data were abstracted using a study-designed instrument. An ADE was defined by an Adverse Drug Reaction Probability scale score of 1 or more. Statistical analyses included negative binomial regression for comparing ADE incidence.   RESULTS Of 913 patients in the analytic sample, 371 patients (41%) had one or more ADEs. Incidence, by individual, was not significantly different between Usual Care and Provider Feedback groups (37% vs. 45%; p = 0.06; Coefficient, 0.19; 95% CI: -0.008, 0.390). ADE severity was also similar. For example, 51% of ADEs in the Usual Care and 58% in the Provider Feedback groups involved symptoms that were not serious (95% CI for the difference, -15%, 2%). Finally, ADE preventability did not differ. For example, 16% in the Usual Care group and 17% in the Provider Feedback group had an associated warning (95% CI for the difference, -7 to 5%; p = 0.79).   CONCLUSION Medications safety did not improve with the addition of a medication profiling program to an electronic prescribing system.	adverse reaction to drug;blinded;coefficient;dav regimen;drug utilization;incidence matrix;interaction;negative base;order management system;outpatients;patients;personnameuse - assigned;profiling (computer programming);randomness	Peter A. Glassman;Pamela S. Belperio;Andrew Lanto;Barbara Simon;Robert Valuck;Jeffrey Sayers;Martin Lee	2007	Journal of the American Medical Informatics Association : JAMIA	10.1197/jamia.M2313	medicine;ambulatory care;medical emergency;negative binomial distribution;drug interaction;clinical study design;statistics	ML	-60.39547335798939	-66.35313496193916	183473
166ac0ca40125528eb7fd581de2fde8990e813d1	building medical ontologies based on terminology extraction from texts: an experimentation in pneumology	ontology;terminology extraction;pneumology.;knowledge engineering;differential semantics;natural language processing	Pathologies and acts are classified in thesauri to help physicians to code their activity. In practice, the use of thesauri is not sufficient to reduce variability in coding and thesauri do not fit computer processing. We think the automation of the coding task requires a conceptual modelling of medical items: an ontology. Our objective is to help pneumologists code acts and diagnoses with a software that represents medical knowledge by an ontology of the concerned specialty. The main research hypothesis is to apply natural language processing tools to corpora to develop the resources needed to build the ontology. In this paper, our objective is twofold: we have to build the ontology of pneumology and we want to develop a methodology for the knowledge engineer to build various types of medical ontologies based on terminology extraction from texts.	classification;knowledge engineer;natural language processing;nomenclature;ontology (information science);pulmonary medicine;spatial variability;terminology extraction;text corpus;thesaurus	Audrey Baneyx;Jean Charlet;Marie-Christine Jaulent	2005	Studies in health technology and informatics		data mining;ontology (information science);terminology extraction;medicine	AI	-49.15832152005822	-68.38836547441386	185554
c51b90a27a445d5f475cf5cfdcf25b6e06d9f455	informatics can identify systemic sclerosis (ssc) patients at risk for scleroderma renal crisis	prednisone;hypertension;blood pressure;renal crisis;systemic sclerosis;informatics;scleroderma;steroid;management;natural language processing	BACKGROUND Electronic medical records (EMR) provide an ideal opportunity for the detection, diagnosis, and management of systemic sclerosis (SSc) patients within the Veterans Health Administration (VHA). The objective of this project was to use informatics to identify potential SSc patients in the VHA that were on prednisone, in order to inform an outreach project to prevent scleroderma renal crisis (SRC).   METHODS The electronic medical data for this study came from Veterans Informatics and Computing Infrastructure (VINCI). For natural language processing (NLP) analysis, a set of retrieval criteria was developed for documents expected to have a high correlation to SSc. The two annotators reviewed the ratings to assemble a single adjudicated set of ratings, from which a support vector machine (SVM) based document classifier was trained. Any patient having at least one document positively classified for SSc was considered positive for SSc and the use of prednisone≥10mg in the clinical document was reviewed to determine whether it was an active medication on the prescription list.   RESULTS In the VHA, there were 4272 patients that have a diagnosis of SSc determined by the presence of an ICD-9 code. From these patients, 1118 patients (21%) had the use of prednisone≥10mg. Of these patients, 26 had a concurrent diagnosis of hypertension, thus these patients should not be on prednisone. By the use of natural language processing (NLP) an additional 16,522 patients were identified as possible SSc, highlighting that cases of SSc in the VHA may exist that are unidentified by ICD-9. A 10-fold cross validation of the classifier resulted in a precision (positive predictive value) of 0.814, recall (sensitivity) of 0.973, and f-measure of 0.873.   CONCLUSIONS Our study demonstrated that current clinical practice in the VHA includes the potentially dangerous use of prednisone for veterans with SSc. This present study also suggests there may be many undetected cases of SSc and NLP can successfully identify these patients.	classification;cross reactions;cross-validation (statistics);electronic health records;excalibur: morgana's revenge;f1 score;hypertensive disease;informatics (discipline);natural language processing;patients;positive predictive value of diagnostic test;prednisone;scleroderma renal crisis;semiconductor research corporation;support vector machine;systemic scleroderma;systemic therapy (psychotherapy);systemics	Doug Redd;Tracy M. Frech;Maureen A. Murtaugh;Julia Rhiannon;Qing Zeng-Treitler	2014	Computers in biology and medicine	10.1016/j.compbiomed.2014.07.022	medicine;pathology;computer science;physical therapy;blood pressure;informatics	ML	-57.47403633450357	-66.62221193836456	185790
11b70e12da23e02fa38faa49537b5c641e500f50	which barriers affect morbidity registration performance of gp trainees and trainers?	medical informatics;general practitioners;graduate medical education;electronic health records;primary care	BACKGROUND Diagnosis coding percentages in the specialty training of general practitioners (GPs) are generally high, but not perfect, indicating barriers against coding still exist, possibly influencing the validity of data based on electronic patient records (EPRs).   OBJECTIVE To study the relationship between barriers to coding diagnoses with the International Classification of Primary Care (ICPC) of GP trainees and trainers and their self-reported and actual coding performance.   METHODS A questionnaire was developed, and returned by 71 (of 73, 97%) GP trainees and 103 (of 108, 95%) GP trainers, affiliated to the GP Specialty Training of the Academic Medical Center, University of Amsterdam. Their barriers to ICPC coding and self-reported coding performance were compared with EPR-derived data extractions that were collected during one year.   RESULTS Mean coding percentages were 88.3 (SD=11.5) and 82.3% (SD=19.0) (trainees/trainers). Most participants reported always registering ICPC codes for consultations and home visits, specifically in those situations pre-specified in the questionnaire. Telephone consultations, repeat prescriptions and administrative actions were coded less frequently. Most participants never or rarely experienced coding barriers, an exception being 'insufficient refinement of the ICPC system'. Most motivation and ICPC-related barriers correlated with self-reported and actual coding performance. Regression analyses showed that 'ICPC coding is unpleasant to use' predicted both trainees' and trainers' coding percentages. The trainers' coding percentage was also predicted by 'no personal gain from ICPC' and 'coding is difficult'.   CONCLUSION The mean coding percentages we found were high, but could further be improved by increasing GPs' motivation and by making ICPC coding more user-friendly. EPR-derived data seem biased by non-coded telephone consultations only.	acm international collegiate programming contest;code;consultation;data validation;epr paradox;extraction;home visit (procedure);international classification of primary care;morbidity - disease rate;patients;refinement (computing);registration;spatio-temporal analysis;usability	Jip de Jong;Mechteld R. M. Visser;Margreet Wieringa-de Waard	2013	International journal of medical informatics	10.1016/j.ijmedinf.2013.02.002	health informatics;family medicine;medicine;nursing	HCI	-60.392586455068084	-66.1771161466148	186260
b71cf769d64a30b0de64e1184a486a2e5d82cdfb	knowledge-based query expansion to support scenario-specific retrieval of medical free text	lung cancer;knowledge based approach;medical free text retrieval;statistical method;text retrieval;differential diagnosis;automatic query expansion;query expansion;knowledge base	"""In retrieving medical free text, users are often interested in answers relevant to certain scenarios, scenarios that correspond to common tasks in medical practice, e.g., """"treatment"""" or """"diagnosis"""" of a disease. Consequently, the queries they pose are often scenario-specific, e.g., """"lung cancer, treatment."""" A fundamental challenge in handling such queries is that scenario terms in the query (e.g. """"treatment"""") are too general to match specialized terms in relevant documents (e.g. """"lung excision""""). In this paper we propose a knowledge-based query expansion method that exploits the UMLS knowledge source to append the original query with additional terms that are specifically relevant to the query's scenario(s). We compare the proposed method with statistical expansion that only explores statistical term correlation and expands terms that are not necessarily scenario specific. Our study on the OHSUMED testbed shows that the knowledge-based method which results in scenario-specific expansion is able to improve more than 5% over the statistical method on average, and about 10% for queries that mention certain scenarios, such as """"treatment of a disease"""" and """"differential diagnosis of a symptom/disease."""""""	append;document;query expansion;scenario planning;statistical model;testbed	Zhenyu Liu;Wesley W. Chu	2005		10.1145/1066677.1066922	knowledge base;query expansion;ranking;computer science;artificial intelligence;data mining;database;world wide web;information retrieval;query language	NLP	-48.378132943094144	-67.35579627342645	187651
3e086cbaa4f24dccca182179a8a5881c61239a1b	semi-automated entry of clinical temporal-abstraction knowledge	clinical data;knowledge based system;temporal abstraction;knowledge acquisition;knowledge base;knowledge engineering	OBJECTIVES The authors discuss the usability of an automated tool that supports entry, by clinical experts, of the knowledge necessary for forming high-level concepts and patterns from raw time-oriented clinical data.   DESIGN Based on their previous work on the RESUME system for forming high-level concepts from raw time-oriented clinical data, the authors designed a graphical knowledge acquisition (KA) tool that acquires the knowledge required by RESUME. This tool was designed using Protégé, a general framework and set of tools for the construction of knowledge-based systems. The usability of the KA tool was evaluated by three expert physicians and three knowledge engineers in three domains-the monitoring of children's growth, the care of patients with diabetes, and protocol-based care in oncology and in experimental therapy for AIDS. The study evaluated the usability of the KA tool for the entry of previously elicited knowledge.   MEASUREMENTS The authors recorded the time required to understand the methodology and the KA tool and to enter the knowledge; they examined the subjects' qualitative comments; and they compared the output abstractions with benchmark abstractions computed from the same data and a version of the same knowledge entered manually by RESUME experts.   RESULTS Understanding RESUME required 6 to 20 hours (median, 15 to 20 hours); learning to use the KA tool required 2 to 6 hours (median, 3 to 4 hours). Entry times for physicians varied by domain-2 to 20 hours for growth monitoring (median, 3 hours), 6 and 12 hours for diabetes care, and 5 to 60 hours for protocol-based care (median, 10 hours). An increase in speed of up to 25 times (median, 3 times) was demonstrated for all participants when the KA process was repeated. On their first attempt at using the tool to enter the knowledge, the knowledge engineers recorded entry times similar to those of the expert physicians' second attempt at entering the same knowledge. In all cases RESUME, using knowledge entered by means of the KA tool, generated abstractions that were almost identical to those generated using the same knowledge entered manually.   CONCLUSION The authors demonstrate that the KA tool is usable and effective for expert physicians and knowledge engineers to enter clinical temporal-abstraction knowledge and that the resulting knowledge bases are as valid as those produced by manual entry.		Yuval Shahar;Hai Chen;Daniel P. Stites;Lawrence V. Basso;Herbert Kaizer;Darrell M. Wilson;Mark A. Musen	1999	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.1999.0060494	knowledge survey;knowledge base;computer science;knowledge management;artificial intelligence;knowledge engineering;data mining;database;domain knowledge	HCI	-50.46800284725291	-68.90737403168467	187863
2192c0fe1ad9c61f3f956109e18daf7855604957	development and evaluation of a common data model enabling active drug safety surveillance using disparate healthcare databases	drug safety;acute myocardial infarction;electronic health record;higher order;data model;drug use;medical history;electronic medical record	OBJECTIVE Active drug safety surveillance may be enhanced by analysis of multiple observational healthcare databases, including administrative claims and electronic health records. The objective of this study was to develop and evaluate a common data model (CDM) enabling rapid, comparable, systematic analyses across disparate observational data sources to identify and evaluate the effects of medicines.   DESIGN The CDM uses a person-centric design, with attributes for demographics, drug exposures, and condition occurrence. Drug eras, constructed to represent periods of persistent drug use, are derived from available elements from pharmacy dispensings, prescriptions written, and other medication history. Condition eras aggregate diagnoses that occur within a single episode of care. Drugs and conditions from source data are mapped to biomedical ontologies to standardize terminologies and enable analyses of higher-order effects.   MEASUREMENTS The CDM was applied to two source types: an administrative claims and an electronic medical record database. Descriptive statistics were used to evaluate transformation rules. Two case studies demonstrate the ability of the CDM to enable standard analyses across disparate sources: analyses of persons exposed to rofecoxib and persons with an acute myocardial infarction.   RESULTS Over 43 million persons, with nearly 1 billion drug exposures and 3.7 billion condition occurrences from both databases were successfully transformed into the CDM. An analysis routine applied to transformed data from each database produced consistent, comparable results.   CONCLUSION A CDM can normalize the structure and content of disparate observational data, enabling standardized analyses that are meaningfully comparable when assessing the effects of medicines.	aggregate data;aggregate function;centralized computing;conceptual schema;data model;databases;demography;description;dictionary;doxorubicin;electronic health records;electronics, medical;global variable;health planning;healthcare encounters domain;medical records;myocardial infarction;nomenclature;normalize;ontology (information science);patients;peer review;persistence (computer science);pharmacovigilance;published database;rule (guideline);source data;timeline fluoride releasing resin;trionic;vocabulary;disease registry;interest;rofecoxib	Stephanie J. Reisinger;Patrick B. Ryan;Donald J. O'Hara;Gregory E. Powell;Jeffrey L. Painter;Edward N. Pattishall;Jonathan A. Morris	2010	Journal of the American Medical Informatics Association : JAMIA	10.1136/jamia.2009.002477	higher-order logic;medicine;data model;computer science;medical history;data mining;database;pharmacovigilance	ML	-52.97567421536901	-66.77064952008494	188180
b1c72e286d7aa5dc10f3948bade01c94eedeb9b3	a model-based analysis of semiautomated data discovery and entry using automated content-extraction	errors;information systems;data bases;identification;models;knowledge based systems;extraction	JADE GOLDSTEIN-STEWART and JUSTIN GROSSMAN United States Department of Defense, Washington, DC ________________________________________________________________________ Content extraction systems can automatically extract entities and relations from raw text and use the information to populate knowledge bases, potentially eliminating the need for manual data discovery and entry. Unfortunately, content extraction is not sufficiently accurate for end-users who require high trust in the information uploaded to their databases, creating a need for human validation and correction of extracted content. In this paper, we examine content extraction errors and explore their influence on a prototype semi-automated system that allows a human reviewer to correct and validate extracted information before uploading it, focusing on the identification and correction of precision errors. We applied content extraction to six different corpora and used a Goals, Operators, Methods, and Selection rules Language (GOMSL) model to simulate the activities of a human using the prototype system to review extraction results, correct precision errors, ignore spurious instances, and validate information. We compared the simulated task completion rate of the semi-automated system model with that of a second GOMSL model that simulates the steps required for finding and entering information manually. Results quantify the efficiency advantage of the semi-automated workflow and illustrate the value of employing multidisciplinary quantitative methods to calculate system-level measures of technology utility.	database;entity;jade;population;prototype;semiconductor industry;simulation;text corpus;upload	Ransom K. Winder;Craig Haimson;Jade Goldstein-Stewart;Justin Grossman	2013	Int. J. Hum. Comput. Interaction	10.1080/10447318.2012.758528	identification;extraction;computer science;data science;knowledge-based systems;data mining;information retrieval;information system	AI	-50.05494389655263	-68.87512548868825	189170
55757f5e624c20fd3a85c4508502366b939c0c89	the diagnosis related groups enhanced electronic medical record	hospital information system;clinical guideline;search engine;information sources;expert systems;diagnosis related group;user interface;medical records systems;computerized hospital information systems;medical informatics applications;electronic medical record;medical record linkage;diagnosis related groups;administrative data;medical record systems;web based service;expert system	PROBLEM The introduction of Diagnosis Related Groups as a basis for hospital payment in Germany announced essential changes in the hospital reimbursement practice. A hospital's economical survival will depend vitally on the accuracy and completeness of the documentation of DRG relevant data like diagnosis and procedure codes. In order to enhance physicians' coding compliance, an easy-to-use interface integrating coding tasks seamlessly into clinical routine had to be developed. A generic approach should access coding and clinical guidelines from different information sources.   METHODS Within the Electronic Medical Record (EMR) a user interface ('DRG Control Center') for all DRG relevant clinical and administrative data has been built. A comprehensive DRG-related web site gives online access to DRG grouping software and an electronic coding expert. Both components are linked together using an application supporting bi-directional communication. Other web based services like a guideline search engine can be integrated as well.   RESULTS With the proposed method, the clinician gains quick access to context sensitive clinical guidelines for appropriate treatment of his/her patient and administrative guidelines for the adequate coding of the diagnoses and procedures. This paper describes the design and current implementation and discusses our experiences.		Marcel Lucas Müller;Thomas Bürkle;Sebastian Irps;Norbert Roeder;Hans-Ulrich Prokosch	2003	International journal of medical informatics	10.1016/S1386-5056(03)00050-9	medicine;computer science;artificial intelligence;data mining;user interface;expert system;search engine	HCI	-55.95963406881659	-66.19506752969976	190084
95fa2343ffa0c8fb8f88894e5932f90084f7dd34	ascot: assisting search and creation of clinical trials	text mining;facet;clinical trial;data mining;cluster labels;clustering;eligibility criteria;term extraction;algorithms;design;terms;clinical trials	In this paper we present ASCOT, an efficient search application for clinical trials that is designed to aid writing new trials. Clinical trials are protocols describing medical research on humans. Although they are a valuable source of medical practice evidence, search is laborious due to the immense number of existing protocols. Writing a new trial includes composing detailed eligibility criteria, which might be time-consuming, especially for new researchers. ASCOT uses text mining and data mining methods to enrich clinical trials with metadata, that can be employed to narrow down search. In addition, ASCOT integrates an eligibility criteria recommendation component.	communications protocol;data mining;text mining	Ioannis Korkontzelos;Sophia Ananiadou	2012		10.1145/2110363.2110472	computer science;data science;data mining;information retrieval	NLP	-50.69181455838781	-67.18339217136668	190399
00ac8a7fdb8e16962f0ae008bc417d4a7fe80570	indexing anatomical concepts to omim clinical synopsis using umls metathesaurus		As a first step toward the quantitative comparison of clinical features of diseases, we indexed the text descriptions in the Clinical Synopsis section of the Online Mendelian Inheritance in Man (OMIM) with concepts for the body parts, organs, and tissues contained in the Metathesaurus of the Unified Medical Language System (UMLS). We also indexed the text with the diseases and disorders having links to body parts specified in the thesaurus. The vocabulary size was approximately 177,540 representations for 81,435 concepts, and 2,161 concepts were indexed to 3,779 OMIM entries. The indexed concepts included 134 concepts for the noun forms of anatomical concepts and 985 indexed concepts for diseases and disorders that were linked to 132 and 408 anatomical concepts, respectively. We report herein that the retrieval of OMIM entries for diseases affecting specific organs can be made more comprehensive through the anatomical concepts indexed to the Clinical Synopsis or linked to the indexed concepts, as compared to simply matching organ names to the Clinical Synopsis text. The recall and precision of identifying relevant body parts in the Clinical Synopsis were calculated as 78% and 92.5%, respectively, based on random sampling. The examination of the unidentified body parts due to lack of indexed diseases and disorders showed that although most of the concepts for diseases and disorders were contained in the Metathesaurus, their relations to body parts were not. The indexing result proved the effectiveness of the Metathesaurus as a resource for the identification of concepts indicating body parts, diseases, and disorders.	body part;body tissue;contain (action);description;disease;index;indexes;matching;name;online mendelian inheritance in man;organ;part dosing unit;petrosal sinus sampling;precision and recall;sampling (signal processing);sampling - surgical action;thesaurus;umls metathesaurus;unified medical language system;video synopsis;vocabulary	Teruyoshi Hishiki;Osamu Ogasawara;Yoshimasa Tsuruoka;Kousaku Okubo	2003	In silico biology			AI	-48.55545195714124	-67.56411157675024	190425
ca2e346c16f4811314dbfabad5915f86038a8ccd	a distributional approach to summarization of radiology reports	patient care distributional approach diagnostic radiology report summarization;radiology document handling medical diagnostic computing natural language processing patient care;radiology medical diagnostic imaging filtering computed tomography radio frequency yttrium	Diagnostic radiology reports contain a summary written by a radiologist for communication with primary care providers. Information not included in the summary may be lost in the communication, resulting in substandard patient care. We introduce a notion of salience based on distributional characteristics of term usage in a corpus of radiology reports and present an algorithm for generation of suggestions for inclusion of additional information in report summaries. We evaluate our method on a corpus of 98,913 reports and show our method suggests additions to 11-28% of reports, depending on body location and imaging modality.	algorithm;modality (human–computer interaction);radiology;text corpus	Eamon Johnson;W. Christopher Baughman;Gultekin Özsoyoglu	2015	2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)	10.1109/BIBM.2015.7359815	multimedia	Visualization	-49.52796236480634	-70.1059230862927	190647
a758d7673631fc58bc59fd1cc1d533449094baf4	note highlights: surfacing relevant concepts from unstructured notes for health professionals		Health and social care professionals are under increasing pressure to assimilate the ever-growing volume of data from case notes and electronic medical records. In this paper, we propose and evaluate with domain experts a cognitive system for patient-centric care that leverages and combines natural language processing, semantics, and learning from users over time to support care professionals making informed and timely decisions while reducing the burden of interacting with large volumes of unstructured patient notes. We propose methods for highlighting the entities embedded in the unstructured data and providing a personalized view of an individual. We evaluate through a user study and show a consensus between what the domain experts and the system consider relevant and discuss early feedback on the value of our Note Highlights methods to domain experts.	artificial intelligence;baseline (configuration management);consumability;embedded system;entity;expect;interaction;natural language processing;personalization;screenshot;usability testing	Vanessa López;Joao H. Bettencourt-Silva;Grace McCarthy;Natasha Mulligan;Fabrizio Cucci;Stéphane Deparis;Marco Luca Sbodio;Pierpaolo Tommasi;John Segrave-Daly;Conor Cullen;Ciaran Hennessy;Beth McKeon;Karie Kelly;Russell Olsen;John Dinsmore;Anne-Marie Brady;Nagesh Yadav;Spyros Kotoulas	2017	2017 IEEE International Conference on Healthcare Informatics (ICHI)	10.1109/ICHI.2017.28	medical record;data mining;terminology;semantics;cognition;unified modeling language;knowledge management;unstructured data;computer science	Robotics	-50.650356688427664	-69.25800502385853	190692
4b5203203f64f0cdf95970b04b078de7ebe3542d	are health videos from hospitals, health organizations, and active users available to health consumers? an analysis of diabetes health video ranking in youtube		Health consumers are increasingly using the Internet to search for health information. The existence of overloaded, inaccurate, obsolete, or simply incorrect health information available on the Internet is a serious obstacle for finding relevant and good-quality data that actually helps patients. Search engines of multimedia Internet platforms are thought to help users to find relevant information according to their search. But, is the information recovered by those search engines from quality sources? Is the health information uploaded from reliable sources, such as hospitals and health organizations, easily available to patients? The availability of videos is directly related to the ranking position in YouTube search. The higher the ranking of the information is, the more accessible it is. The aim of this study is to analyze the ranking evolution of diabetes health videos on YouTube in order to discover how videos from reliable channels, such as hospitals and health organizations, are evolving in the ranking. The analysis was done by tracking the ranking of 2372 videos on a daily basis during a 30-day period using 20 diabetes-related queries. Our conclusions are that the current YouTube algorithm favors the presence of reliable videos in upper rank positions in diabetes-related searches.	dependability;diabetes mellitus;health literacy;imagery;information needs;information source;internet;investments;mathematical optimization;multimedia;obsolete - editstatus;page (document);patients;personnameuse - assigned;scientific publication;search algorithm;used quit cigarette smoking videos;web search engine;whitelist;algorithm;interest;videocassette	Carlos Fernández-Llatas;Vicente Traver;Jose Enrique Borrás Morell;Antonio Martinez-Millana;Randi Karlsen	2017		10.1155/2017/8194940	medicine;multimedia;internet privacy;world wide web	Web+IR	-58.38995250541346	-68.3097840664992	191077
3a9abba80e8668102625e6e85da1635746b3601e	automated knowledge extraction for decision model construction: a data mining approach	medline;semantics;medical subject headings;decision making computer assisted;decision support techniques;information management;algorithms;humans;information storage and retrieval;automatic data processing;colorectal neoplasms	Combinations of Medical Subject Headings (MeSH) and Subheadings in MEDLINE citations may be used to infer relationships among medical concepts. To facilitate clinical decision model construction, we propose an approach to automatically extract semantic relations among medical terms from MEDLINE citations. We use the Apriori association rule mining algorithm to generate the co-occurrences of medical concepts, which are then filtered through a set of predefined semantic templates to instantiate useful relations. From such semantic relations, decision elements and possible relationships among them may be derived for clinical decision model construction. To evaluate the proposed method, we have conducted a case study in colorectal cancer management; preliminary results have shown that useful causal relations and decision alternatives can be extracted.	algorithm;association rule learning;causal filter;clinical use template;colorectal carcinoma;data mining;extraction;inference;medline;medical subject headings;citation	Ai-Ling Zhu;Tze-Yun Leong	2003	AMIA ... Annual Symposium proceedings. AMIA Symposium		computer science;data science;data mining;information retrieval	Logic	-49.91425572471234	-68.06004469359101	191976
b8452240427c7ea18a65161f588832213835af31	towards a metadata registry for evaluating augmented medical interventions		Quality evaluation in the field of Augmented Surgery is strategic for public health policies. It implies to be able to effectively perform evaluation of Quality in term of Expected Medical Benefit (EMB). The notion of EMB is complex and not standardized in this field. To define and to evaluate EMB, it is necessary to discover the knowledge on the domain targeted by the device and to structure it. This paper presents first parts of this work. Focused on navigated knee surgery, it led us to obtain two main results: the identification of a new criterion for evaluating EMB obtained thanks to the formalization of a new kind of metadata. These encouraging results seem to offer new perspectives for the evaluation of devices from the field of augmented surgery.	health policy;registries	Anne-Sophie Silvent;Alexandre Moreau-Gaudry;Philippe Cinquin	2011	Studies in health technology and informatics	10.3233/978-1-60750-806-9-175	data mining;metadata registry;psychological intervention;medicine	HCI	-52.47350862222493	-66.60913758214726	192729
134f1c5b32b8502a491327f8d20d594ee9559777	intelligent data processing in training evaluation practices	biological measures;data processing knowledge representation data mining radio access networks australia bridges performance evaluation intelligent systems performance analysis knowledge engineering;evaluation system;knowledge fragments;performance evaluation;performance grading;data processing;bridges;royal australian navy;physiological evaluation intelligent data processing naval training evaluation practices data mining knowledge representation processes data preprocessing evaluation system ran bridge simulator royal australian navy performance grading biological measures knowledge fragments intelligent evaluation techniques;performance grade;data mining;knowledge representation processes;ran bridge simulator;medical expert systems;physiology;medical expert systems naval engineering computing military computing knowledge representation computer based training data mining intelligent tutoring systems digital simulation physiology;intelligent tutoring systems;intelligent evaluation techniques;computer based training;performance analysis;intelligent systems;naval training evaluation practices;naval engineering computing;knowledge representation;intelligent data processing;data preprocessing;physiological evaluation;digital simulation;australia;military computing;radio access networks;knowledge engineering	Data mining and knowledge representation processes are explored for use in pre-processing data for an evaluation system. The RAN (Royal Australian Navy) Bridge Simulator provides the evaluation environment. Both performance grading and biological measures are analysed and processed to provide a basic level of knowledge representation. These knowledge fragments can then be passed to an evaluation system.		M. Ostrognay;Lakhmi C. Jain;A. Cropley;V. Puri;D. Filippidis	1999		10.1109/KES.1999.820110	simulation;engineering;knowledge management;data science	NLP	-54.48234745495179	-66.99420201728017	192979
13da2165476bb7c7f3e336ba0146e3ea9d6971f4	an approach to multidimensional medical data analysis based on the skyline operator		Over-treatments such as high medicine fees, repeated physical examinations, overuse of antibiotics and hormones are very common in some hospitals in China. In order to reduce these over-treatment phenomena, two multidimensional data analysis methods are used to analyze historical medical data in this paper. Then, for different situations of outpatients and inpatients, the best treatment cases for recommendation and the worst treatment cases to avoid are found out by using two algorithms based on the Skyline operator to sort multidimensional historical medical data. The results provide patients with appropriate prescriptions and provide doctors with reasonable recommendations of treatments.		Min Che;Liya Wang;Zhibin Jiang	2018	2018 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)	10.1109/IEEM.2018.8607324		DB	-58.81106765132223	-67.26568193009898	193393
0ae0a479e11f7c59b9151226869d094826db5f51	assessing the reliability of the radiation therapy care delivery process using discrete event simulation	radiation therapy;health care;discrete event simulation;qa checklist;quality assurance;patient safety events;nonhuman error;incident rates;radiation therapy care delivery process reliability;patient care;reliability;human error;radiation oncology;safety	"""This paper presents a discrete event simulation-based analysis of the Radiation Therapy (RT) care delivery process at the Radiation Oncology Center of the University of North Carolina (UNC) at Chapel Hill with the goal of assessing process reliability and patient safety. The use of quality assurance (QA) checklists in radiation oncology is a widely recognized method for detecting potential human and non-human errors before they reach the patient. In this study, data on patient safety events (""""an incident that reached the patient, whether or not the patient was harmed"""") and near misses (""""an incident that comes close to reaching the patient but is caught and corrected beforehand"""") were collected through a comprehensive safety program and used to estimate incident rates and the reliability score for each QA checklist."""	chapel;sensor;simulation;software quality assurance	Pegah Pooya;Julie S. Ivy;Lukasz Mazur;Katharin Deschesne;Prithima Mosaly;Gregg Tracton;Nishant Singh	2014	Proceedings of the Winter Simulation Conference 2014		reliability engineering;biological engineering	Visualization	-59.27254439426414	-67.24338351929437	194712
d7e5167a70b57b81d3528b6e256c1e5f763bfe25	instruments to assess the quality of health information on the world wide web: what can our patients actually use?	medical informatics;internet;health information;world wide web;patient education;medical informatic	OBJECTIVE To find and assess quality-rating instruments that can be used by health care consumers to assess websites displaying health information.   DATA SOURCES Searches of PubMed, the World Wide Web (using five different search engines), reference tracing from identified articles, and a review of the of the American Medical Informatics Association's annual symposium proceedings.   REVIEW METHODS Sources were examined for availability, number of elements, objectivity, and readability.   RESULTS A total of 273 distinct instruments were found and analyzed. Of these, 80 (29%) made evaluation criteria publicly available and 24 (8.7%) had 10 or fewer elements (items that a user has to assess to evaluate a website). Seven instruments consisted of elements that could all be evaluated objectively. Of these seven, one instrument consisted entirely of criteria with acceptable interobserver reliability (kappa> or =0.6); another instrument met readability standards.   CONCLUSIONS There are many quality-rating instruments, but few are likely to be practically usable by the intended audience.	common criteria;conferences;eighty;health care;informatics;instrument - device;inter-rater reliability;objective-c;objectivity/db;patients;pubmed;web site;web search engine;world wide web;standards characteristics	Elmer V. Bernstam;Dawn M. Shelton;Muhammad F. Walji;Funda Meric-Bernstam	2005	International journal of medical informatics	10.1016/j.ijmedinf.2004.10.001	health informatics;the internet;medicine;computer science;nursing;data mining;multimedia;world wide web	SE	-55.83364070529583	-68.61195608872728	195623
4752a80e8b6d5411f9f22324795eb884f63f8f75	ontology-based specification, identification and analysis of perioperative risks	agent system;ontology;perioperative risks;risk analysis;risk definition;risk identification;risk specification	BACKGROUND Medical personnel in hospitals often works under great physical and mental strain. In medical decision-making, errors can never be completely ruled out. Several studies have shown that between 50 and 60% of adverse events could have been avoided through better organization, more attention or more effective security procedures. Critical situations especially arise during interdisciplinary collaboration and the use of complex medical technology, for example during surgical interventions and in perioperative settings (the period of time before, during and after surgical intervention).   METHODS In this paper, we present an ontology and an ontology-based software system, which can identify risks across medical processes and supports the avoidance of errors in particular in the perioperative setting. We developed a practicable definition of the risk notion, which is easily understandable by the medical staff and is usable for the software tools. Based on this definition, we developed a Risk Identification Ontology (RIO) and used it for the specification and the identification of perioperative risks.   RESULTS An agent system was developed, which gathers risk-relevant data during the whole perioperative treatment process from various sources and provides it for risk identification and analysis in a centralized fashion. The results of such an analysis are provided to the medical personnel in form of context-sensitive hints and alerts. For the identification of the ontologically specified risks, we developed an ontology-based software module, called Ontology-based Risk Detector (OntoRiDe).   CONCLUSIONS About 20 risks relating to cochlear implantation (CI) have already been implemented. Comprehensive testing has indicated the correctness of the data acquisition, risk identification and analysis components, as well as the web-based visualization of results.	adverse event;alert brand of caffeine;centralized computing;clinical decision-making;cochlear implant procedure;cochlear structure;context-sensitive help;correctness (computer science);data acquisition;decision making;imagery;ion implantation;ontology;ribostamycin;software system;specification;web application;interdisciplinary collaboration	Alexandr Uciteli;Juliane Neumann;Kais Tahar;Kutaiba Saleh;Stephan Stucke;Sebastian Faulbrück-Röhr;André Kaeding;Martin Specht;Tobias M. Schmidt;Thomas Neumuth;Andreas Besting;Dominik Stegemann;Frank Portheine;Heinrich Herre	2017		10.1186/s13326-017-0147-8	computer science;health technology;data mining;ontology;psychological intervention;risk analysis (business);adverse effect;perioperative;text mining	SE	-54.973202432875354	-66.7321668003435	195869
9002a1ba4b8a5090e9b1945ab4651e26508ab4bf	total cost of high volume multi-detector ct data management	total cost of ownership;data management;picture archiving and communication system;cost analysis;activity based costing;indirect cost;cost of data management;cost effectiveness;information lifecycle management;digital image;multi detector ct;picture archiving and communication system pacs	The number of CT images increased significantly when 16-row multi-detector CT scanners (MDCT) were introduced. The purpose of this study is to quantify the total data management cost of MDCTand to identify cost drivers. An activity-based costing (ABC) method was used to analyze cost drivers in MDCT data management. Changing from 4-row to 16-row CT, mean image count per exam was increased from 699 to 1465 images per exam, reaching up to 18,000 images per exam. Cost analysis was implemented in four steps: (1) identify activities; (2) assign direct costs to activities; (3) define bases for indirect costs; (4) assign indirect cost to the activities. Four major activities were identified in MDCT data management: PACS check-in, data handling during digital reporting, storage, and hospital-wide digital image distribution. Total direct and indirect costs were $49 per mean exam. With $15.27 per exam, storage turned out to be the main cost driver. Therefore, two scenarios for selective storage are discussed. When 50% of the images are deleted before long-term storage, savings of $7.63 per exam are achieved, while additional labor cost for data selection is only $4.55 per exam. When 80% of the images are deleted in a labor-intensive process, savings of $9.11 per examwill result. Awhat-if-analysis shows that long-term storage ofMDCT data is cost-effective below a storage cost of $8.40 per exam. Given the actual total cost of ownership (TCO) of $53 per GB, high performance storage is not cost-effective for long-term storage of high volume MDCT data. If the storage price decay of annually 45% continues, the break-even cost of $8.40 per examwill be reached in the next 1– 2 years. In themeantime, only the most relevant parts ofMDCTexams should be kept, or low cost mass storage media should be used to optimize the cost–value relation.D 2004 CARS and Elsevier B.V. All rights reserved.	ct scan;digital image;mass storage;modified discrete cosine transform;picture archiving and communication system;total cost of ownership	D. R. Voellmy;O. Handgrätinger;Simon Wildermuth;B. Fröhlich;Borut Marincek	2004		10.1016/j.ics.2004.03.173	embedded system;simulation;cost-effectiveness analysis;radiology;data management;computer science;cost–benefit analysis;picture archiving and communication system;indirect costs;digital image;activity-based costing	AI	-61.2770798648117	-66.95779200723598	196180
5943b6c092cf80fd9700a715acdffdc332032fbc	loinc®: a universal catalogue of individual clinical observations and uniform representation of enumerated collections	patient data;biological patents;health information technology;biomedical journals;variables;standards;text mining;europe pubmed central;clinical observations;citation search;healthcare technology;citation networks;data collection forms;research articles;abstracts;patient assessments;research systems;open access;public health forms;life sciences;clinical guidelines;terminology;interoperability;full text;data sets;answer lists;rest apis;clinical care;orcids;public health;europe pmc;uniform representation;biomedical research;framework;bioinformatics;literature search	In many areas of practice and research, clinical observations are recorded on data collection forms by asking and answering questions, yet without being represented in accepted terminology standards these results cannot be easily shared among clinical care and research systems. LOINC contains a well-developed model for representing variables, answer lists, and the collections that contain them. We have successfully added many assessments and other collections of variables to LOINC in this model. By creating a uniform representation and distributing it worldwide at no cost, LOINC aims to lower the barriers to interoperability among systems and make this valuable data available across settings when and where it is needed.	collections (publication);contract agreement;evaluation procedure;interoperability;loinc code;nomenclature;open-source software;prosencephalon;run-time infrastructure (simulation);standards characteristics	Daniel J. Vreeman;Clement J. McDonald;Stanley M. Huff	2010	International journal of functional informatics and personalised medicine	10.1504/IJFIPM.2010.040211	variables;interoperability;text mining;pathology;public health;computer science;bioinformatics;data science;software framework;data mining;terminology;information retrieval;data set	Web+IR	-52.881566824845414	-66.26239469642262	196206
0b74aa5b0f5c8d12b9afcbd06ef6523bc54b537b	using point of service clinical documentation to reduce variability in charge capture.		Data collected at bedside to document patient care can also be used to generate an itemized summary of charges including activity-based clinician charges. This approach becomes advantageous when the charge capture operation is transparent to the clinician who would otherwise have to review the care documentation, recall the appropriate charging rules, and exercise discretion in capturing charges. Documented procedures and supplies convert directly into patient charge rules. Documented patient care is more difficult to translate into activity-based charges because nursing care can vary in intensity and duration depending on the patient's needs. The problem can be overcome by embedding time or data-driven logic into the charging rules. Using this approach in the labor and delivery units of 7 IHC hospitals (114 beds), we generated consistent charge summaries. We improved the accuracy of patient charges from 65% to over 98% of our charge summaries having no missed charges.	design of the fat file system;discipline of nursing;documentation;embedding;heart rate variability;immunohistochemistry;manufactured supplies;patients;point of sale;rule (guideline)	Sidney N. Thornton;Hong Yu;Reed M. Gardner	2002	Proceedings. AMIA Symposium		embedded system;real-time computing;simulation;engineering	ML	-59.17326708051552	-68.89655173837296	196911
a994c69c1a45bdd1eee6874184b10c724f5b3cd8	an approach to decision support in heart failure	ontologies;decision support systems;reasoning.;decision support;decision support system;clinical decision support system;knowledge representation	Chronic heart failure is a severe clinical syndrome among the most remarkable for prevalence and morbidity in the developed western countries. The European STREP project HEARTFAID aims at realizing an innovative platform of services which will improve the processes of diagnosis, prognosis and therapy provision in the heart failure domain. The core of the platform intelligence is a Clinical Decision Support System, designed by integrating innovative knowledge representation techniques and hybrid reasoning methods, and including advanced tools for the analysis of diagnostic data. In this paper we discuss how we are using semantic web technologies for implementing a real, significant clinical scenario, covering the clinical course of a heart failure patient.	clinical decision support system;failure domain;knowledge representation and reasoning;semantic web	Sara Colantonio;Massimo Martinelli;Davide Moroni;Ovidio Salvetti;Francesco Perticone;Angela Sciacqua;Domenico Conforti;Antonio Gualtieri	2007			semantic web;clinical decision support system;computer science;knowledge representation and reasoning;decision support system;data mining;heart failure;knowledge management	AI	-54.31693724422547	-66.30041494536306	197274
afab14848a7e27097108838ebc80bc9bb3254b17	identification of anatomical terminology in medical text	unified medical language system;natural language processing	We report on an experiment to use the natural language processing tools being developed in the SPECIALIST system to accurately identify terminology associated with the coronary arteries as expressed in coronary catheterization reports. The ultimate goal is to map from any anatomically-oriented medical text to online images, using the UMLS as an intermediate knowledge source. We describe some of the problems encountered when processing coronary artery terminology and report on the results of a formative evaluation of a tool for addressing these problems.	arterial system;coronary artery;natural language processing;nomenclature;unified medical language system	Charles Sneiderman;Thomas C. Rindflesch;Carol A. Bean	1998	Proceedings. AMIA Symposium		formative assessment;coronary arteries;anatomical terminology;terminology;artificial intelligence;natural language processing;unified medical language system;medicine	NLP	-49.68225029757384	-68.5361766417573	198167
878bf68026abe3c40dcaa2c31ceb9917e72e2dff	assisting the translation of the core subset of snomed ct into french		BACKGROUND the Core Subset of SNOMED CT is part of the UMLS-Core Project dedicated to study problem list vocabularies. SNOMED CT is not yet translated into French.   OBJECTIVE to propose an automated method to assist the translation of the CORE Subset of SNOMED CT into French.   MATERIAL the 2009 AA versions of the CORE Subset of SNOMED CT and UMLS; use of four French-language terminologies integrated into the UMLS Metathesaurus: SNOMED International, ICD10, MedDRA, and MeSH.   METHOD an exact mapping completed by a close mapping between preferred terms of the CORE Subset of SNOMED CT and those of the four terminologies.   RESULTS 89% of the preferred terms of the CORE Subset of SNOMED CT are mapped with at least one preferred term in one of the four terminologies.   DISCUSSION if needed, synonymous terms could be added by the means of synonyms in the terminologies; the proposed method is independent from French and could be applied to other natural languages.	amino acids;ct scan;four-dimensional computed tomography;genetic translation process;languages;meddra;nanda-international terminology;natural language;pet/ct scan;subgroup;systematized nomenclature of medicine clinical terms;unified medical language system;version;vocabulary;x-ray computed tomography	Hocine Abdoune;Tayeb Merabti;Stéfan Jacques Darmoni;Michel Joubert	2011	Studies in health technology and informatics	10.3233/978-1-60750-806-9-819	data mining;information retrieval;snomed ct;computer science	SE	-48.53037790870975	-68.4519764669455	198812
831f241e96a25043229242c7c95118f5fc4d9903	integrating natural language processing expertise with patient safety event review committees to improve the analysis of medication events	patient safety events;visualization;machine learning;medication;natural language processing	OBJECTIVES Many healthcare providers have implemented patient safety event reporting systems to better understand and improve patient safety. Reviewing and analyzing these reports is often time consuming and resource intensive because of both the quantity of reports and length of free-text descriptions in the reports.   METHODS Natural language processing (NLP) experts collaborated with clinical experts on a patient safety committee to assist in the identification and analysis of medication related patient safety events. Different NLP algorithmic approaches were developed to identify four types of medication related patient safety events and the models were compared.   RESULTS Well performing NLP models were generated to categorize medication related events into pharmacy delivery delays, dispensing errors, Pyxis discrepancies, and prescriber errors with receiver operating characteristic areas under the curve of 0.96, 0.87, 0.96, and 0.81 respectively. We also found that modeling the brief without the resolution text generally improved model performance. These models were integrated into a dashboard visualization to support the patient safety committee review process.   CONCLUSIONS We demonstrate the capabilities of various NLP models and the use of two text inclusion strategies at categorizing medication related patient safety events. The NLP models and visualization could be used to improve the efficiency of patient safety event data review and analysis.		Allan Fong;Nicole Harriott;Donna M. Walters;Hanan Foley;Richard Morrissey;Raj M. Ratwani	2017	International journal of medical informatics	10.1016/j.ijmedinf.2017.05.005	visualization;medicine;computer science;knowledge management;data science;machine learning;data mining	HCI	-49.523995300881204	-69.4756055532906	198835
